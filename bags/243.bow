discourse aware neural extractive text summarization jiacheng zhe yu jingjing university of texas at austin dynamics ai research utexas edu zhe gan yu cheng com r a l c s c v v i x r a abstract recently bert has been adopted for ument encoding in state of the art text marization models however sentence based extractive models often result in redundant or uninformative phrases in the extracted summaries also long range dependencies throughout a document are not well tured by bert which is pre trained on tence pairs instead of documents to address these issues we present a discourse aware neural summarization model discobert extracts sub sentential discourse units instead of sentences as candidates for extractive selection on a ner granularity to capture the long range dependencies among discourse units structural discourse graphs are constructed based on rst trees and ence mentions encoded with graph tional networks experiments show that the proposed model outperforms state of the art methods by a signicant margin on popular summarization benchmarks compared to other bert base models introduction neural networks have achieved great success in the task of text summarization nenkova et al yao et al there are two main lines of research abstractive and extractive while the abstractive paradigm rush et al see et al celikyilmaz et al sharma et al focuses on generating a summary word by word after encoding the full document the extractive approach cheng and lapata zhou et al narayan et al directly selects tences from the document to assemble into a mary the abstractive approach is more exible most of this work was done when the rst author was an intern at microsoft illustration and datasets are available at com jiacheng xu discobert illustration of discobert for text figure marization sentence based bert model baseline selects whole sentences and the proposed discourse aware model discobert selects edus the right side of the gure illustrates the two discourse graphs we use i graph with the mentions of pulitzer prizes highlighted as examples and rst graph induced by rst discourse trees and generally produces less redundant summaries while the extractive approach enjoys better ity and efciency cao et al recently some hybrid methods have been posed to take advantage of both by designing a two stage pipeline to rst select and then rewrite or compress candidate sentences chen and bansal gehrmann et al zhang et al xu and durrett compression or rewriting aims to discard uninformative phrases in the lected sentences however most of these hybrid systems suffer from the inevitable disconnection between the two stages in the pipeline meanwhile modeling long range context for document summarization remains a challenge xu it is one of the most prestigious bestowed upon journalists and people in the arts and today the pulitzer prize for journalism went to the post and courier newspaper of charleston south which has a tiny staff of just and a daily circulation of winner this iconic photo by new york times photographer daniel berehulak was part of a winning and shows james dorbor suspected of being infected with being carried by medical staff to an ebola treatment center in monrovia liberia the pulitzer awarded annually by columbia recognize extraordinary work in u s journalism literature drama and other categories other winners of the coveted award included the st louis post dispatch coref graphrst graphdocument of pulitzer winner this iconic photo by new york times photographer daniel berehulak was part of a winning and shows james dorbor suspected of being infected with being carried by medical staff to an ebola treatment center in monrovia liberia it is one of the most prestigious bestowed upon journalists and people in the arts and today the pulitzer prize for journalism went to the post and courier newspaper of charleston south which has a tiny staff of just and a daily circulation of selection et al pre trained language models vlin et al are designed mostly for sentences or a short paragraph thus poor at capturing range dependencies throughout a document pirical observations liu and lapata show that adding standard encoders such as lstm or transformer vaswani et al on top of bert to model inter sentential relations does not bring in much performance gain in this paper we present discobert a discourse aware neural extractive summarization model built upon bert to perform compression with extraction simultaneously and reduce dancy across sentences we take elementary course unit edu a sub sentence phrase unit inating from rst mann and thompson carlson et al as the minimal selection unit instead of sentence for extractive summarization figure shows an example of discourse tion with sentences broken down into edus tated with brackets by operating on the discourse unit level our model can discard redundant details in sub sentences therefore retaining additional pacity to include more concepts or events leading to more concise and informative summaries furthermore we netune the representations of discourse units with the injection of prior edge to leverage intra sentence discourse relations more specically two discourse oriented graphs are proposed rst graph gr and coreference graph gc over these discourse graphs graph convolutional network gcn kipf and welling is imposed to capture long range interactions among edus rst graph is constructed from rst parse trees over edus of the document on the other hand coreference graph connects entities and their coreference clusters mentions across the document the path of coreference navigates the model from the core event to other occurrences of that event and in parallel explores its interactions with other concepts or events the main contribution is threefold we pose a discourse aware extractive summarization model discobert which operates on a sentential discourse unit level to generate cise and informative summary with low we propose to structurally model dancy adopt rst as the discourse framework due to the availability of existing tools the nature of the rst tree ture for compression and the observations from louis et al other alternatives includes graph bank wolf and gibson and pdtb miltsakaki et al figure example of discourse segmentation and rst tree conversion the original sentence is segmented into edus in box a and then parsed into an rst discourse tree in box the converted based rst discourse tree is shown in box c cleus nodes including and and satellite nodes including and are denoted by solid lines and dashed lines respectively relations are in italic the edu is the head of the whole tree span while the edu is the head of the span inter sentential context with two types of discourse graph iii discobert achieves new state of the art on two popular newswire text summarization datasets outperforming other bert base models discourse graph construction in this section we rst introduce the cal structure theory rst mann and son a linguistic theory for discourse ysis and then explain how we construct discourse graphs used in discobert two types of course graph are considered rst graph and erence graph all edges are initialized as nected and connections are later added for a subset of nodes based on rst discourse parse tree or coreference mentions discourse analysis discourse analysis focuses on inter sentential tions in a document or conversation in the rst framework the discourse structure of text can be represented in a tree format the whole document can be segmented into contiguous adjacent and non overlapping text spans called elementary course units edus each edu is tagged as either nucleus or satellite which characterizes its arity or saliency nucleus nodes are generally more central and satellite nodes are more peripheral and less important in terms of content and grammatical reliance there are dependencies among edus that represent their rhetorical relations in this work we treat edu as the minimal unit for content selection in text summarization rst discourse tree converted rst discourse winner this iconic photo by new york times photographer daniel berehulak was part of a winning series and shows james dorbor suspected of being infected with ebola being carried by medical staff to an ebola treatment center in monrovia liberia conversion sec ure shows an example of discourse segmentation and the parse tree of a sentence among these edus rhetorical relations represent the functions of different discourse units as observed in louis et al the rst tree structure already serves as a strong indicator for content selection on the other hand the agreement between rhetorical tions tends to be lower and more ambiguous thus we do not encode rhetorical relations explicitly in our model in content selection for text summarization we expect the model to select the most concise and pivotal concept in the document with low dancy however in traditional extractive rization methods the model is required to select a whole sentence even though some parts of the sentence are not necessary our proposed approach can select one or several ne grained edus to der the generated summaries less redundant this serves as the foundation of our discobert model rst graph when selecting sentences as candidates for tive summarization we assume each sentence is grammatically self contained but for edus some restrictions need to be considered to ensure maticality for example figure illustrates an rst discourse parse tree of a sentence where this iconic series is a grammatical sentence but and shows is not we need to stand the dependencies between edus to ensure the grammaticality of the selected combinations the detail of the derivation of the dependencies could be found in sec the construction of the rst graph aims to vide not only local paragraph level but also range document level connections among edus we use the converted dependency version of the tree to build the rst graph gr by initializing an empty graph and treating every discourse dency from the i th edu to the j th edu as a directed edge i e coreference graph text summarization especially news tion usually suffers from the well known position bias issue kedzie et al where most of the key information is described at the very beginning example in figure details such as the name of the suspected child in the exact location of the photo in and who was carrying the child in are unlikely to be reected in the nal summary algorithm construction of the coreference graph gc require coreference clusters c cn mentions for each cluster ci eim initialize the graph gc without any edge gc for i to n do collect the location of all occurences eim to l lm for j to m k to m do gc end for end for return constructed graph gc of the document however there is still a decent amount of information spread in the middle or at the end of the document which is often ignored by summarization models we observe that around of oracle sentences appear after the rst sentences in the cnndm dataset besides in long news articles there are often multiple core acters and events throughout the whole document however existing neural models are poor at ing such long range context especially when there are multiple ambiguous coreferences to resolve to encourage and guide the model to capture the long range context in the document we pose a coreference graph built upon discourse units algorithm describes how to construct the coreference graph we rst use stanford corenlp manning et al to detect all the coreference clusters in an article for each coreference cluster all the discourse units containing the mention of the same cluster will be connected this process is iterated over all the coreference mention clusters to create the nal coreference graph figure provides an example where pulitzer prizes is an important entity and has occurred multiple times in multiple discourse units the constructed coreference graph is shown on the right side of the when graph gc is constructed edges among and are all connected due to the mentions of pulitzer prizes discobert model overview figure provides an overview of the proposed model consisting of a document encoder and a graph encoder for the document encoder a trained bert model is rst used to encode the intentionally ignore other entities and mentions in this example for simplicity figure left model architecture of discobert the stacked discourse graph encoders contain k stacked dge blocks right the architecture of each discourse graph encoder dge block whole document on the token level then a attentive span extractor is used to obtain the edu representations from the corresponding text spans the graph encoder takes the output of the ment encoder as input and updates the edu resentations with graph convolutional network based on the constructed discourse graphs which are then used to predict the oracle labels assume that document d is segmented into n edus in total i e d dn where denotes the i th edu following liu and lapata we formulate extractive summarization as a sequential labeling task where each edu di is scored by neural networks and decisions are made based on the scores of all edus the oracle labels are a sequence of binary labels where stands for being selected and for not we denote the labels as y y y n during training we aim to predict the sequence of labels y given the document d during inference we need to further consider discourse dependency to ensure the coherence and grammaticality of the output summary y document encoder bert is a pre trained deep bidirectional former encoder vaswani et al devlin et al following liu and lapata we code the whole document with bert and netune the bert model for summarization bert is originally trained to encode a single sentence or sentence pair however a news article typically contains more than words hence we need to make some adaptation to apply bert for document encoding specically we insert and tokens at the beginning and the end of each sentence respectively in order to encode long documents such as news articles we also tend the maximum sequence length that bert can take from to in all our experiments the input document after tokenization is denoted as d dn and di where is the number of bpe tokens in the i th edu if is the rst edu in a sentence there is also a token prepended to di if dj is the last edu in a sentence there is a token appended to see figure the schema of insertion of and is an approach used in liu and lapata for simplicity these two tokens are not shown in the equations bert model is then used to encode the document hb hb where hb whole document in the same length as the input is the bert output of the hb after the bert encoder the representation of the token can be used as sentence tation however this approach does not work in our setting since we need to extract the tation for edus instead therefore we adopt a also tried inserting and at the beginning and the end of every edu and treating the corresponding representation as the representation for each edu but the performance drops drastically it is one of the most prestigious honors cls sep bestowed upon the arts and today the pulitzer prize for south carolina which has a tiny cls discourse graph convolutional network self attentive span extractor spanext proposed in lee et al to learn edu representation for the i th edu with words with the output we from the bert encoder hb obtain edu representation as follows hb hb ij ij aij hs aij hb ij where ij is the score of the j th word in the edu aij is the normalized attention of the j th word w t all the words in the span hs is a weighted sum i of the bert output hidden states throughout the paper all the w matrices and b vectors are eters to learn we abstract the above self attentive span extractor as hs i hb after the span extraction step the whole ment is represented as a sequence of edu n rdhn which hs sentations hs hs will be sent to the graph encoder graph encoder given the constructed graph g v e nodes v correspond to the edus in a document and edges e correspond to either rst discourse relations or coreference mentions we then use graph volutional network to update the representations of all the edus to capture long range cies missed by bert for better summarization to modularize architecture design we present a single discourse graph encoder dge layer multiple dge layers are stacked in our experiments assume that the input for the k th dge layer is denoted as n rdhn and the corresponding output is denoted as rdhn the k th dge layer is designed as follows n i i relu i j i i jni i i i where ln represents layer normalization ni denotes the neighorhood of the i th edu node is the output of the i th edu in the k th i dge layer and hs which is the output from the document encoder after k layers of dataset document sum e in graph sent edu tok tok gr cnndm nyt gc table statistics of the datasets the rst block shows the average number of sentences edus and tokens in the documents the second block shows the average number of tokens in the reference summaries the third block shows the average number of edges in the constructed rst graphs gr and coreference graphs gc respectively graph propagation we obtain hg rdhn which is the nal representation of all the edus after the stacked dge layers for different graphs the parameter of dges are not shared if we use both graphs their output are concatenated hg r c hg training inference during training hg is used for predicting the acle labels specically yi i where represents the logistic function and yi is the prediction probability ranging from to the training loss of the model is binary cross entropy loss given the predictions and oracles l i y yi for discobert without graphs the output from document encoder hs is used for prediction stead the creation of oracle is operated on edu level we greedily pick up edus with their sary dependencies until drops during inference given an input document ter obtaining the prediction probabilities of all the edus i e y yn we sort y in scending order and select edus accordingly note that the dependencies between edus are also forced in prediction to ensure grammacality of erated summaries experiments in this section we present experimental results on two popular news summarization datasets we compare our proposed model with state of the art baselines and conduct detailed analysis to validate the effectiveness of discobert datasets we evaluate the models on two datasets new york times nyt sandhaus cnn and mail cnndm hermann et al we use the script from see et al to extract summaries from raw data and stanford corenlp for sentence boundary detection tokenization and parsing ning et al due to the limitation of bert we only encode up to bert bpes table provides statistics of the datasets the edges in gc are undirected while those in gr are directional for cnndm there are and samples for training validation and test respectively we use the un anonymized version as in previous summarization work nyt is licensed by following previous work zhang et al xu and durrett we use and samples for training validation and test respectively state of the art baselines we compare our model with the following state the art neural text summarization models extractive models banditsum treats tive summarization as a contextual bandit lem trained with policy gradient methods dong et al neusum is an extractive model with architecture where the attention nism scores the document and emits the index as the selection zhou et al compressive models jecs is a neural compression based summarization model using blstm as the encoder xu and durrett the rst stage is selecting sentences and the ond stage is sentence compression by pruning stituency parsing tree bert based models bert based models have achieved signicant improvement on cnndm and nyt when compared with lstm counterparts bertsum is the rst bert based extractive marization model liu and lapata our baseline model bert is the re implementation of bertsum pnbert proposed a bert based model with various training strategies including ment learning and pointer networks zhong et al hibert is a hierarchical bert based model for document encoding which is further pretrained with unlabeled data zhang et al implementation details we use allennlp gardner et al as the code framework the implementation of graph model oracle sentence oracle discourse r l neusum zhou et al banditsum dong et al jecs xu and durrett pnbert zhong et al pnbert w rl bert zhang et al hiberts hibert s hibert m bertsum liu and lapata base raffel et al bert discobert discobert w gc discobert w gr discobert w gr gc table results on the test set of the cnndm dataset and are reported models with the asterisk symbol used extra data for pre training and are shorthands for unigram and bigram lap r l is the longest common subsequence encoding is based on dgl wang et al periments are conducted on a single nvidia card and the mini batch size is set to due to gpu memory capacity the length of each document is truncated to bpes we use the pre trained bert base uncased model and ne tune it for all periments we train all our models for up to steps rouge lin is used as the evaluation metrics and is used as the validation criteria the realization of discourse units and structure is a critical part of edu pre processing which quires two steps discourse segmentation and rst parsing in the segmentation phase we use a neural discourse segmenter based on the bilstm crf framework wang et al the segmenter achieved score on the rst dt test set in which the human performance is in the ing phase we use a shift reduce discourse parser to extract relations and identify neuclrity ji and eisenstein the dependencies among edus are crucial to the grammaticality of selected edus here are the two steps to learn the derivation of cies head inheritance and tree conversion head inheritance denes the head node for each valid non terminal tree node for each leaf node the ldc upenn neuraleduseg com pku com jiyfeng dplp head is itself we determine the head of non terminal nodes based on their nuclearity for example in figure the heads of text spans and need to be grounded to a gle edu we propose a simple yet effective schema to convert rst discourse tree to a based discourse tree we always consider the dependency restriction such as the reliance of lite on nucleus when we create oracle during processing and when the model makes the tion for the example in figure if the model selects being carried liberia as a date span we will enforce the model to select and shows and this series as well the number of chosen edus depends on the average length of the reference summaries dencies across edus as mentioned above and the length of the existing content the optimal average number of edus selected is tuned on the ment set experimental results results on cnndm table shows results on cnndm the rst section includes baseline sentence based oracle and discourse based oracle the second section lists the performance of line models including non bert based and based variants the performance of our proposed model is listed in the third section bert is our implementation of sentence based bert model discobert is our discourse based bert model without discourse graph encoder discobert w gc and discobert w gr are the based bert model with coreference graph and rst graph respectively discobert w gr gc is the fusion model encoding both graphs the proposed discobert beats the based counterpart and all the competitor els with the help of discourse graph coder the graph based discobert beats the of the art bert model by a signicant margin on on ablation study with individual graphs shows that the rst graph is slightly more helpful than the coreference both children are then the head of the current node inherits the head of the left child otherwise when one child is n and the other is s the head of the current node inherits the head of the n child one child node is n and the other is s the head of the s node depends on the head of the n node if both children are n and the right child does not contain a subject in the discourse the head of the right n node depends on the head of the left n node model oracle sentence oracle discourse r l jecs xu and durrett bert zhang et al hiberts hibertm hibert s hibert m bert discobert discobert w gc discobert w gr discobert w gr gc table results on the test set of the nyt dataset models with the asterisk symbol used extra data for pre training graph while the combination of both achieves ter performance overall results on nyt results are summarized in ble the proposed model surpasses previous state of the art bert based model by a signicant s and hibert margin hibert m used extra data for pre training the model we notice that in the nyt dataset most of the improvement comes from the use of edus as minimal selection units cobert provides gain on over the bert baseline however the use of course graphs does not help much in this case grammaticality due to segmentation and partial selection of tence the output of our model might not be as grammatical as the original sentence we ally examined and automatically evaluated model output and observed that overall the generated summaries are still grammatical given the rst dependency tree constraining the rhetorical tions among edus a set of simple yet effective post processing rules helps to complete the edus in some cases automatic grammar checking we followed xu and durrett to perform automatic mar checking using grammarly table shows the grammar checking results where the average number of errors in every characters on ndm and nyt datasets is reported we compare discobert with sentence based bert model all shows the summation of the number of rors in all categories as shown in the table the source m cr pv pt cnndm nyt sent disco sent disco all o table number of errors per characters based on automatic grammaticality checking with marly on cnndm and nyt lower values are ter detailed error categories including correctness cr passive voice pv misuse punctuation pt in compound complex sentences and others o are listed from left to right model sent disco ref all coherence grammaticality table human evaluation results we ask turkers to grade the overall preference coherence and icality from to mean values along with standard deviations are reported clare hines who lives in brisbane was diagnosed with a brain tumour after suffering epileptic seizures after a number of tests doctors discovered she had a benign tumour that had wrapped itself around her acoustic facial and balance nerve and told her she had have it surgically removed or she risked the tumour turning malignant one week before brain surgery she found out she was pregnant jordan henderson in action against aston villa at wembley on sunday has agreed a new liverpool deal the club s vice captain puts pen to paper on a deal which will keep him at liverpool until rodgers will sider henderson for the role of club captain after steven gerrard moves to la galaxy at the end of the campaign but for now the england international is delighted to have agreed terms on a contract that will take him through the peak years of his career table example outputs from cnndm by cobert strikethrough indicates discarded edus originates from missing or improper or missing anaphora resolution in this example johnny is believed to have but actually he is the police say only selecting the ond edu yields a sentence actually he is ne which is not clear who is he mentioned here summaries generated by our model have retained the quality of the original text related work human evaluation we sampled documents from the test set of cnndm and for each sample we asked two turkers to grade three summaries from to results are shown in table bert model the original bertsum model lects sentences from the document hence providing the best overall readability coherence and maticality in some cases reference summaries are just long phrases so the scores are slightly lower than those from the sentence model discobert model is slightly worse than sent bert model but is fully comparable to the other two variants examples analysis we show some examples of model output in table we notice that a decent amount of irrelevant details are removed from the extracted summary despite the success we further conducted ror analysis and found that the errors mostly inated from the rst dependency resolution and the upstream parsing error of the discourse parser the misclassication of rst dependencies and the hand crafted rules for dependency resolution hurted the grammaticality and coherence of the generated outputs common punctuation issues include extra or missing commas as well as ing quotation marks some of the coherence issue neural extractive summarization neural works have been widely used in extractive rization various decoding approaches including ranking narayan et al index prediction zhou et al and sequential labelling lapati et al zhang et al dong et al have been applied to content selection our model uses a similar conguration to encode the document with bert as liu and lapata did but we use discourse graph structure and graph encoder to handle the long range dependency issue neural compressive summarization text summarization with compression and deletion has been explored in some recent work xu and durrett presented a two stage neural model for selection and compression based on constituency tree pruning dong et al presented a neural sentence compression model with discrete operations including deletion and addition different from these studies as we use edus as minimal selection basis sentence compression is achieved automatically in our model discourse summarization the use of course theory for text summarization has been plored before louis et al examined the benet of graph structure provided by discourse lations for text summarization hirao et al yoshida et al formulated the tion problem as the trimming of the document course tree durrett et al presented a system of sentence extraction and compression with ilp methods using discourse structure li et al demonstrated that using edus as units of content selection leads to stronger summarization mance compared with them our proposed method is the rst neural end to end summarization model using edus as the selection basis graph based summarization graph approach has been explored in text summarization over decades lexrank introduced a stochastic based method for computing relative importance of textual units erkan and radev sunaga et al employed a gcn on the lation graphs with sentence embeddings obtained from rnn tan et al also proposed based attention in abstractive summarization model fernandes et al developed a framework to reason long distance relationships for text rization conclusion in this paper we present discobert which uses discourse unit as the minimal selection basis to reduce summarization redundancy and leverages two types of discourse graphs as inductive bias to capture long range dependencies among discourse units we validate the proposed approach on two popular summarization datasets and observe sistent improvement over baseline models for future work we will explore better graph encoding methods and apply discourse graphs to other tasks that require long document encoding acknowledgement thanks to junyi jessy li greg durrett yen chun chen and to the other members of the microsoft dynamics ai research team for the reading feedback and suggestions references ziqiang cao furu wei wenjie li and sujian li faithful to the original fact aware neural tive summarization in aaai conference on cial intelligence lynn carlson daniel marcu and mary ellen okurovsky building a discourse tagged pus in the framework of rhetorical structure theory in proceedings of the second sigdial workshop on discourse and dialogue asli celikyilmaz antoine bosselut xiaodong he and yejin choi deep communicating agents for in proceedings of the abstractive summarization conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages new orleans louisiana association for computational linguistics yen chun chen and mohit bansal fast tive summarization with reinforce selected tence rewriting in proceedings of the annual meeting of the association for computational guistics volume long papers pages association for computational linguistics jianpeng cheng and mirella lapata neural marization by extracting sentences and words in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages berlin germany sociation for computational linguistics jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language in proceedings of the conference standing of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota ation for computational linguistics yue dong zichao li mehdi rezagholizadeh and jackie chi kit cheung editnts an neural programmer interpreter model for sentence in proceedings of cation through explicit editing the annual meeting of the association for putational linguistics pages florence italy association for computational linguistics yue dong yikang shen eric crawford herke van hoof and jackie chi kit cheung banditsum extractive summarization as a contextual bandit in proceedings of the conference on cal methods in natural language processing pages association for computational tics greg durrett taylor berg kirkpatrick and dan klein learning based single document rization with compression and anaphoricity in proceedings of the annual straints ing of the association for computational linguistics volume long papers pages ation for computational linguistics gunes erkan and dragomir r radev lexrank graph based lexical centrality as salience in text summarization journal of articial intelligence search dialogue pages association for tional linguistics patrick fernandes miltiadis allamanis and marc brockschmidt structured neural tion arxiv preprint matt gardner joel grus mark neumann oyvind tafjord pradeep dasigi nelson f liu matthew ters michael schmitz and luke zettlemoyer a deep semantic natural language in proceedings of workshop for cessing platform nlp open source software nlp oss pages melbourne australia association for tional linguistics sebastian gehrmann yuntian deng and alexander rush bottom up abstractive summarization in proceedings of the conference on cal methods in natural language processing pages association for computational tics karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa man and phil blunsom teaching machines to read and comprehend in c cortes n d lawrence d d lee m sugiyama and r nett editors advances in neural information cessing systems pages curran ciates inc tsutomu hirao yasuhisa yoshida masaaki nishino norihito yasuda and masaaki nagata document summarization as a tree knapsack in proceedings of the conference on lem empirical methods in natural language processing seattle washington usa yangfeng ji and jacob eisenstein tion learning for text level discourse parsing in ceedings of the annual meeting of the tion for computational linguistics volume long papers pages baltimore maryland ation for computational linguistics chris kedzie kathleen mckeown and hal daume iii content selection in deep learning models of in proceedings of the summarization ference on empirical methods in natural language processing pages thomas n kipf and max welling supervised classication with graph convolutional networks in proceedings of iclr kenton lee luheng he mike lewis and luke moyer end to end neural coreference in proceedings of the conference on lution empirical methods in natural language processing pages copenhagen denmark association for computational linguistics junyi jessy li kapil thadani and amanda stent the role of discourse units in near extractive in proceedings of the annual rization ing of the special interest group on discourse and chin yew lin rouge a package for matic evaluation of summaries in text tion branches out yang liu and mirella lapata text in proceedings of tion with pretrained encoders the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china association for computational linguistics annie louis aravind joshi and ani nenkova discourse indicators for content selection in rization in proceedings of the sigdial ference pages tokyo japan association for computational linguistics william c mann and sandra a thompson rhetorical structure theory toward a functional ory of text organization text interdisciplinary nal for the study of discourse christopher manning mihai surdeanu john bauer jenny finkel steven bethard and david mcclosky the stanford corenlp natural language processing toolkit in proceedings of annual meeting of the association for computational guistics system demonstrations pages sociation for computational linguistics eleni miltsakaki rashmi prasad aravind joshi and bonnie webber the penn discourse in proceedings of the fourth international bank conference on language resources and evaluation ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural work based sequence model for extractive rization of documents in aaai conference on cial intelligence shashi narayan shay b cohen and mirella lapata ranking sentences for extractive tion with reinforcement learning in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long pers pages association for tional linguistics ani nenkova kathleen mckeown al matic summarization foundations and trends in information retrieval colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu exploring the limits of transfer learning with a unied text to text former arxiv preprint alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of the sentence summarization conference on empirical methods in natural language processing pages association for computational linguistics evan sandhaus the new york times annotated corpus linguistic data consortium philadelphia abigail see peter j liu and christopher d ning get to the point summarization with pointer generator networks in proceedings of the annual meeting of the association for tational linguistics volume long papers pages association for computational tics eva sharma luyang huang zhe hu and lu wang an entity driven framework for abstractive in proceedings of the summarization ference on empirical methods in natural language processing and the international joint ence on natural language processing ijcnlp pages jiwei tan xiaojun wan and jianguo xiao abstractive document summarization with a in proceedings based attentional neural model of the annual meeting of the association for computational linguistics volume long papers pages vancouver canada association for computational linguistics ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all in advances in neural information you need cessing systems pages minjie wang lingfan yu da zheng quan gan yu gai zihao ye mufei li jinjing zhou qi huang chao ma ziyue huang qipeng guo hao zhang haibin lin junbo zhao jinyang li alexander j smola and zheng zhang deep graph brary towards efcient and scalable deep iclr workshop on representation ing on graphs learning on graphs and manifolds yizhong wang sujian li and jingfeng yang toward fast and accurate neural discourse in proceedings of the conference on tation empirical methods in natural language processing pages brussels belgium association for computational linguistics florian wolf and edward gibson representing discourse coherence a corpus based study tational linguistics jiacheng xu danlu chen xipeng qiu and xuanjing huang cached long short term memory ral networks for document level sentiment in proceedings of the conference on cation empirical methods in natural language ing pages austin texas association for computational linguistics jiacheng xu and greg durrett neural tive text summarization with syntactic compression in proceedings of the conference on cal methods in natural language processing hong kong china association for computational guistics jin ge yao xiaojun wan and jianguo xiao cent advances in document summarization edge and information systems michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan and dragomir radev graph based neural multi document summarization in proceedings of the ence on computational natural language learning conll pages vancouver canada association for computational linguistics yasuhisa yoshida jun suzuki tsutomu hirao and masaaki nagata dependency based course parser for single document summarization in proceedings of the conference on pirical methods in natural language processing emnlp pages doha qatar ation for computational linguistics xingxing zhang mirella lapata furu wei and ming zhou neural latent extractive document in proceedings of the summarization ference on empirical methods in natural language processing pages association for tational linguistics xingxing zhang furu wei and ming zhou bert document level pre training of hierarchical bidirectional transformers for document in proceedings of the annual meeting tion of the association for computational linguistics pages florence italy association for computational linguistics ming zhong pengfei liu danqing wang xipeng qiu and xuanjing huang searching for tive neural extractive summarization what works and what s next in proceedings of the annual meeting of the association for computational guistics pages florence italy tion for computational linguistics qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao neural ument summarization by jointly learning to score and select sentences in proceedings of the nual meeting of the association for computational linguistics volume long papers pages association for computational linguistics a appendix figure provides three sets of examples of the constructed graphs from cnndm specically gc is strictly symmetric and self loop is added to all the nodes to prevent the graph from growing too sparse on the other hand all of the on diagonal entries in gr are zero because the node from rst graph never points to itself figure examples of the adjacent matrix of ence graphs gc and rst graphs gr
