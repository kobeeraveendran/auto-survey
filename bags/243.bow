discourse aware neural extractive text summarization jiacheng zhe jingjing university texas austin dynamics research utexas edu zhe gan cheng com abstract recently bert adopted ument encoding state art text marization models sentence based extractive models result redundant uninformative phrases extracted summaries long range dependencies document tured bert pre trained tence pairs instead documents address issues present discourse aware neural summarization model discobert extracts sub sentential discourse units instead sentences candidates extractive selection ner granularity capture long range dependencies discourse units structural discourse graphs constructed based rst trees ence mentions encoded graph tional networks experiments proposed model outperforms state art methods signicant margin popular summarization benchmarks compared bert base models introduction neural networks achieved great success task text summarization nenkova yao main lines research abstractive extractive abstractive paradigm rush celikyilmaz sharma focuses generating summary word word encoding document extractive approach cheng lapata zhou narayan directly selects tences document assemble mary abstractive approach exible work rst author intern microsoft illustration datasets available com jiacheng discobert illustration discobert text figure marization sentence based bert model baseline selects sentences proposed discourse aware model discobert selects edus right gure illustrates discourse graphs use graph mentions pulitzer prizes highlighted examples rst graph induced rst discourse trees generally produces redundant summaries extractive approach enjoys better ity efciency cao recently hybrid methods posed advantage designing stage pipeline rst select rewrite compress candidate sentences chen bansal gehrmann zhang durrett compression rewriting aims discard uninformative phrases lected sentences hybrid systems suffer inevitable disconnection stages pipeline modeling long range context document summarization remains challenge prestigious bestowed journalists people arts today pulitzer prize journalism went post courier newspaper charleston south tiny staff daily circulation winner iconic photo new york times photographer daniel berehulak winning shows james dorbor suspected infected carried medical staff ebola treatment center monrovia liberia pulitzer awarded annually columbia recognize extraordinary work journalism literature drama categories winners coveted award included louis post dispatch coref graphrst graphdocument pulitzer winner iconic photo new york times photographer daniel berehulak winning shows james dorbor suspected infected carried medical staff ebola treatment center monrovia liberia prestigious bestowed journalists people arts today pulitzer prize journalism went post courier newspaper charleston south tiny staff daily circulation selection pre trained language models vlin designed sentences short paragraph poor capturing range dependencies document pirical observations liu lapata adding standard encoders lstm transformer vaswani bert model inter sentential relations bring performance gain paper present discobert discourse aware neural extractive summarization model built bert perform compression extraction simultaneously reduce dancy sentences elementary course unit edu sub sentence phrase unit inating rst mann thompson carlson minimal selection unit instead sentence extractive summarization figure shows example discourse tion sentences broken edus tated brackets operating discourse unit level model discard redundant details sub sentences retaining additional pacity include concepts events leading concise informative summaries furthermore netune representations discourse units injection prior edge leverage intra sentence discourse relations specically discourse oriented graphs proposed rst graph coreference graph discourse graphs graph convolutional network gcn kipf welling imposed capture long range interactions edus rst graph constructed rst parse trees edus document hand coreference graph connects entities coreference clusters mentions document path coreference navigates model core event occurrences event parallel explores interactions concepts events main contribution threefold pose discourse aware extractive summarization model discobert operates sentential discourse unit level generate cise informative summary low propose structurally model dancy adopt rst discourse framework availability existing tools nature rst tree ture compression observations louis alternatives includes graph bank wolf gibson pdtb miltsakaki figure example discourse segmentation rst tree conversion original sentence segmented edus box parsed rst discourse tree box converted based rst discourse tree shown box cleus nodes including satellite nodes including denoted solid lines dashed lines respectively relations italic edu head tree span edu head span inter sentential context types discourse graph iii discobert achieves new state art popular newswire text summarization datasets outperforming bert base models discourse graph construction section rst introduce cal structure theory rst mann son linguistic theory discourse ysis explain construct discourse graphs discobert types course graph considered rst graph erence graph edges initialized nected connections later added subset nodes based rst discourse parse tree coreference mentions discourse analysis discourse analysis focuses inter sentential tions document conversation rst framework discourse structure text represented tree format document segmented contiguous adjacent non overlapping text spans called elementary course units edus edu tagged nucleus satellite characterizes arity saliency nucleus nodes generally central satellite nodes peripheral important terms content grammatical reliance dependencies edus represent rhetorical relations work treat edu minimal unit content selection text summarization rst discourse tree converted rst discourse winner iconic photo new york times photographer daniel berehulak winning series shows james dorbor suspected infected ebola carried medical staff ebola treatment center monrovia liberia conversion sec ure shows example discourse segmentation parse tree sentence edus rhetorical relations represent functions different discourse units observed louis rst tree structure serves strong indicator content selection hand agreement rhetorical tions tends lower ambiguous encode rhetorical relations explicitly model content selection text summarization expect model select concise pivotal concept document low dancy traditional extractive rization methods model required select sentence parts sentence necessary proposed approach select grained edus der generated summaries redundant serves foundation discobert model rst graph selecting sentences candidates tive summarization assume sentence grammatically self contained edus restrictions need considered ensure maticality example figure illustrates rst discourse parse tree sentence iconic series grammatical sentence shows need stand dependencies edus ensure grammaticality selected combinations detail derivation dependencies found sec construction rst graph aims vide local paragraph level range document level connections edus use converted dependency version tree build rst graph initializing graph treating discourse dency edu edu directed edge coreference graph text summarization especially news tion usually suffers known position bias issue kedzie key information described beginning example figure details suspected child exact location photo carrying child unlikely reected nal summary algorithm construction coreference graph require coreference clusters mentions cluster eim initialize graph edge collect location occurences eim end end return constructed graph document decent information spread middle end document ignored summarization models observe oracle sentences appear rst sentences cnndm dataset long news articles multiple core acters events document existing neural models poor ing long range context especially multiple ambiguous coreferences resolve encourage guide model capture long range context document pose coreference graph built discourse units algorithm describes construct coreference graph rst use stanford corenlp manning detect coreference clusters article coreference cluster discourse units containing mention cluster connected process iterated coreference mention clusters create nal coreference graph figure provides example pulitzer prizes important entity occurred multiple times multiple discourse units constructed coreference graph shown right graph constructed edges connected mentions pulitzer prizes discobert model overview figure provides overview proposed model consisting document encoder graph encoder document encoder trained bert model rst encode intentionally ignore entities mentions example simplicity figure left model architecture discobert stacked discourse graph encoders contain stacked dge blocks right architecture discourse graph encoder dge block document token level attentive span extractor obtain edu representations corresponding text spans graph encoder takes output ment encoder input updates edu resentations graph convolutional network based constructed discourse graphs predict oracle labels assume document segmented edus total denotes edu following liu lapata formulate extractive summarization sequential labeling task edu scored neural networks decisions based scores edus oracle labels sequence binary labels stands selected denote labels training aim predict sequence labels given document inference need consider discourse dependency ensure coherence grammaticality output summary document encoder bert pre trained deep bidirectional encoder vaswani devlin following liu lapata code document bert netune bert model summarization bert originally trained encode single sentence sentence pair news article typically contains words need adaptation apply bert document encoding specically insert tokens beginning end sentence respectively order encode long documents news articles tend maximum sequence length bert experiments input document tokenization denoted number bpe tokens edu rst edu sentence token prepended edu sentence token appended figure schema insertion approach liu lapata simplicity tokens shown equations bert model encode document document length input bert output bert encoder representation token sentence tation approach work setting need extract tation edus instead adopt tried inserting beginning end edu treating corresponding representation representation edu performance drops drastically prestigious honors cls sep bestowed arts today pulitzer prize south carolina tiny cls discourse graph convolutional network self attentive span extractor spanext proposed lee learn edu representation edu words output bert encoder obtain edu representation follows aij aij score word edu aij normalized attention word words span weighted sum bert output hidden states paper matrices vectors eters learn abstract self attentive span extractor span extraction step ment represented sequence edu rdhn sentations sent graph encoder graph encoder given constructed graph nodes correspond edus document edges correspond rst discourse relations coreference mentions use graph volutional network update representations edus capture long range cies missed bert better summarization modularize architecture design present single discourse graph encoder dge layer multiple dge layers stacked experiments assume input dge layer denoted rdhn corresponding output denoted rdhn dge layer designed follows relu jni represents layer normalization denotes neighorhood edu node output edu dge layer output document encoder layers dataset document sum graph sent edu tok tok cnndm nyt table statistics datasets rst block shows average number sentences edus tokens documents second block shows average number tokens reference summaries block shows average number edges constructed rst graphs coreference graphs respectively graph propagation obtain rdhn nal representation edus stacked dge layers different graphs parameter dges shared use graphs output concatenated training inference training predicting acle labels specically represents logistic function prediction probability ranging training loss model binary cross entropy loss given predictions oracles discobert graphs output document encoder prediction stead creation oracle operated edu level greedily pick edus sary dependencies drops inference given input document ter obtaining prediction probabilities edus sort scending order select edus accordingly note dependencies edus forced prediction ensure grammacality erated summaries experiments section present experimental results popular news summarization datasets compare proposed model state art baselines conduct detailed analysis validate effectiveness discobert datasets evaluate models datasets new york times nyt sandhaus cnn mail cnndm hermann use script extract summaries raw data stanford corenlp sentence boundary detection tokenization parsing ning limitation bert encode bert bpes table provides statistics datasets edges undirected directional cnndm samples training validation test respectively use anonymized version previous summarization work nyt licensed following previous work zhang durrett use samples training validation test respectively state art baselines compare model following state art neural text summarization models extractive models banditsum treats tive summarization contextual bandit lem trained policy gradient methods dong neusum extractive model architecture attention nism scores document emits index selection zhou compressive models jecs neural compression based summarization model blstm encoder durrett rst stage selecting sentences ond stage sentence compression pruning stituency parsing tree bert based models bert based models achieved signicant improvement cnndm nyt compared lstm counterparts bertsum rst bert based extractive marization model liu lapata baseline model bert implementation bertsum pnbert proposed bert based model training strategies including ment learning pointer networks zhong hibert hierarchical bert based model document encoding pretrained unlabeled data zhang implementation details use allennlp gardner code framework implementation graph model oracle sentence oracle discourse neusum zhou banditsum dong jecs durrett pnbert zhong pnbert bert zhang hiberts hibert hibert bertsum liu lapata base raffel bert discobert discobert discobert discobert table results test set cnndm dataset reported models asterisk symbol extra data pre training shorthands unigram bigram lap longest common subsequence encoding based dgl wang periments conducted single nvidia card mini batch size set gpu memory capacity length document truncated bpes use pre trained bert base uncased model tune periments train models steps rouge lin evaluation metrics validation criteria realization discourse units structure critical edu pre processing quires steps discourse segmentation rst parsing segmentation phase use neural discourse segmenter based bilstm crf framework wang segmenter achieved score rst test set human performance ing phase use shift reduce discourse parser extract relations identify neuclrity eisenstein dependencies edus crucial grammaticality selected edus steps learn derivation cies head inheritance tree conversion head inheritance denes head node valid non terminal tree node leaf node ldc upenn neuraleduseg com pku com jiyfeng dplp head determine head non terminal nodes based nuclearity example figure heads text spans need grounded gle edu propose simple effective schema convert rst discourse tree based discourse tree consider dependency restriction reliance lite nucleus create oracle processing model makes tion example figure model selects carried liberia date span enforce model select shows series number chosen edus depends average length reference summaries dencies edus mentioned length existing content optimal average number edus selected tuned ment set experimental results results cnndm table shows results cnndm rst section includes baseline sentence based oracle discourse based oracle second section lists performance line models including non bert based based variants performance proposed model listed section bert implementation sentence based bert model discobert discourse based bert model discourse graph encoder discobert discobert based bert model coreference graph rst graph respectively discobert fusion model encoding graphs proposed discobert beats based counterpart competitor els help discourse graph coder graph based discobert beats art bert model signicant margin ablation study individual graphs shows rst graph slightly helpful coreference children head current node inherits head left child child head current node inherits head child child node head node depends head node children right child contain subject discourse head right node depends head left node model oracle sentence oracle discourse jecs durrett bert zhang hiberts hibertm hibert hibert bert discobert discobert discobert discobert table results test set nyt dataset models asterisk symbol extra data pre training graph combination achieves ter performance overall results nyt results summarized ble proposed model surpasses previous state art bert based model signicant hibert margin hibert extra data pre training model notice nyt dataset improvement comes use edus minimal selection units cobert provides gain bert baseline use course graphs help case grammaticality segmentation partial selection tence output model grammatical original sentence ally examined automatically evaluated model output observed overall generated summaries grammatical given rst dependency tree constraining rhetorical tions edus set simple effective post processing rules helps complete edus cases automatic grammar checking followed durrett perform automatic mar checking grammarly table shows grammar checking results average number errors characters ndm nyt datasets reported compare discobert sentence based bert model shows summation number rors categories shown table source cnndm nyt sent disco sent disco table number errors characters based automatic grammaticality checking marly cnndm nyt lower values ter detailed error categories including correctness passive voice misuse punctuation compound complex sentences listed left right model sent disco ref coherence grammaticality table human evaluation results ask turkers grade overall preference coherence icality mean values standard deviations reported clare hines lives brisbane diagnosed brain tumour suffering epileptic seizures number tests doctors discovered benign tumour wrapped acoustic facial balance nerve told surgically removed risked tumour turning malignant week brain surgery found pregnant jordan henderson action aston villa wembley sunday agreed new liverpool deal club vice captain puts pen paper deal liverpool rodgers sider henderson role club captain steven gerrard moves galaxy end campaign england international delighted agreed terms contract peak years career table example outputs cnndm cobert strikethrough indicates discarded edus originates missing improper missing anaphora resolution example johnny believed actually police selecting ond edu yields sentence actually clear mentioned summaries generated model retained quality original text related work human evaluation sampled documents test set cnndm sample asked turkers grade summaries results shown table bert model original bertsum model lects sentences document providing best overall readability coherence maticality cases reference summaries long phrases scores slightly lower sentence model discobert model slightly worse sent bert model fully comparable variants examples analysis examples model output table notice decent irrelevant details removed extracted summary despite success conducted ror analysis found errors inated rst dependency resolution upstream parsing error discourse parser misclassication rst dependencies hand crafted rules dependency resolution hurted grammaticality coherence generated outputs common punctuation issues include extra missing commas ing quotation marks coherence issue neural extractive summarization neural works widely extractive rization decoding approaches including ranking narayan index prediction zhou sequential labelling lapati zhang dong applied content selection model uses similar conguration encode document bert liu lapata use discourse graph structure graph encoder handle long range dependency issue neural compressive summarization text summarization compression deletion explored recent work durrett presented stage neural model selection compression based constituency tree pruning dong presented neural sentence compression model discrete operations including deletion addition different studies use edus minimal selection basis sentence compression achieved automatically model discourse summarization use course theory text summarization plored louis examined benet graph structure provided discourse lations text summarization hirao yoshida formulated tion problem trimming document course tree durrett presented system sentence extraction compression ilp methods discourse structure demonstrated edus units content selection leads stronger summarization mance compared proposed method rst neural end end summarization model edus selection basis graph based summarization graph approach explored text summarization decades lexrank introduced stochastic based method computing relative importance textual units erkan radev sunaga employed gcn lation graphs sentence embeddings obtained rnn tan proposed based attention abstractive summarization model fernandes developed framework reason long distance relationships text rization conclusion paper present discobert uses discourse unit minimal selection basis reduce summarization redundancy leverages types discourse graphs inductive bias capture long range dependencies discourse units validate proposed approach popular summarization datasets observe sistent improvement baseline models future work explore better graph encoding methods apply discourse graphs tasks require long document encoding acknowledgement thanks junyi jessy greg durrett yen chun chen members microsoft dynamics research team reading feedback suggestions references ziqiang cao furu wei wenjie sujian faithful original fact aware neural tive summarization aaai conference cial intelligence lynn carlson daniel marcu mary ellen okurovsky building discourse tagged pus framework rhetorical structure theory proceedings second sigdial workshop discourse dialogue asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents proceedings abstractive summarization conference north american chapter association computational linguistics man language technologies volume long pers pages new orleans louisiana association computational linguistics yen chun chen mohit bansal fast tive summarization reinforce selected tence rewriting proceedings annual meeting association computational guistics volume long papers pages association computational linguistics jianpeng cheng mirella lapata neural marization extracting sentences words proceedings annual meeting sociation computational linguistics volume long papers pages berlin germany sociation computational linguistics jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language proceedings conference standing north american chapter association computational linguistics human language technologies volume long short papers pages minneapolis minnesota ation computational linguistics yue dong zichao mehdi rezagholizadeh jackie chi kit cheung editnts neural programmer interpreter model sentence proceedings cation explicit editing annual meeting association putational linguistics pages florence italy association computational linguistics yue dong yikang shen eric crawford herke van hoof jackie chi kit cheung banditsum extractive summarization contextual bandit proceedings conference cal methods natural language processing pages association computational tics greg durrett taylor berg kirkpatrick dan klein learning based single document rization compression anaphoricity proceedings annual straints ing association computational linguistics volume long papers pages ation computational linguistics gunes erkan dragomir radev lexrank graph based lexical centrality salience text summarization journal articial intelligence search dialogue pages association tional linguistics patrick fernandes miltiadis allamanis marc brockschmidt structured neural tion arxiv preprint matt gardner joel grus mark neumann oyvind tafjord pradeep dasigi nelson liu matthew ters michael schmitz luke zettlemoyer deep semantic natural language proceedings workshop cessing platform nlp open source software nlp oss pages melbourne australia association tional linguistics sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference cal methods natural language processing pages association computational tics karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa man phil blunsom teaching machines read comprehend cortes lawrence lee sugiyama nett editors advances neural information cessing systems pages curran ciates inc tsutomu hirao yasuhisa yoshida masaaki nishino norihito yasuda masaaki nagata document summarization tree knapsack proceedings conference lem empirical methods natural language processing seattle washington usa yangfeng jacob eisenstein tion learning text level discourse parsing ceedings annual meeting tion computational linguistics volume long papers pages baltimore maryland ation computational linguistics chris kedzie kathleen mckeown hal daume iii content selection deep learning models proceedings summarization ference empirical methods natural language processing pages thomas kipf max welling supervised classication graph convolutional networks proceedings iclr kenton lee luheng mike lewis luke moyer end end neural coreference proceedings conference lution empirical methods natural language processing pages copenhagen denmark association computational linguistics junyi jessy kapil thadani amanda stent role discourse units near extractive proceedings annual rization ing special interest group discourse chin yew lin rouge package matic evaluation summaries text tion branches yang liu mirella lapata text proceedings tion pretrained encoders conference empirical methods ural language processing international joint conference natural language processing emnlp ijcnlp pages hong kong china association computational linguistics annie louis aravind joshi ani nenkova discourse indicators content selection rization proceedings sigdial ference pages tokyo japan association computational linguistics william mann sandra thompson rhetorical structure theory functional ory text organization text interdisciplinary nal study discourse christopher manning mihai surdeanu john bauer jenny finkel steven bethard david mcclosky stanford corenlp natural language processing toolkit proceedings annual meeting association computational guistics system demonstrations pages sociation computational linguistics eleni miltsakaki rashmi prasad aravind joshi bonnie webber penn discourse proceedings fourth international bank conference language resources evaluation ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural work based sequence model extractive rization documents aaai conference cial intelligence shashi narayan shay cohen mirella lapata ranking sentences extractive tion reinforcement learning proceedings conference north american ter association computational linguistics human language technologies volume long pers pages association tional linguistics ani nenkova kathleen mckeown matic summarization foundations trends information retrieval colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei peter liu exploring limits transfer learning unied text text arxiv preprint alexander rush sumit chopra jason weston neural attention model abstractive proceedings sentence summarization conference empirical methods natural language processing pages association computational linguistics evan sandhaus new york times annotated corpus linguistic data consortium philadelphia abigail peter liu christopher ning point summarization pointer generator networks proceedings annual meeting association tational linguistics volume long papers pages association computational tics eva sharma luyang huang zhe wang entity driven framework abstractive proceedings summarization ference empirical methods natural language processing international joint ence natural language processing ijcnlp pages jiwei tan xiaojun wan jianguo xiao abstractive document summarization proceedings based attentional neural model annual meeting association computational linguistics volume long papers pages vancouver canada association computational linguistics ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention advances neural information need cessing systems pages minjie wang lingfan zheng quan gan gai zihao mufei jinjing zhou huang chao ziyue huang qipeng guo hao zhang haibin lin junbo zhao jinyang alexander smola zheng zhang deep graph brary efcient scalable deep iclr workshop representation ing graphs learning graphs manifolds yizhong wang sujian jingfeng yang fast accurate neural discourse proceedings conference tation empirical methods natural language processing pages brussels belgium association computational linguistics florian wolf edward gibson representing discourse coherence corpus based study tational linguistics jiacheng danlu chen xipeng qiu xuanjing huang cached long short term memory ral networks document level sentiment proceedings conference cation empirical methods natural language ing pages austin texas association computational linguistics jiacheng greg durrett neural tive text summarization syntactic compression proceedings conference cal methods natural language processing hong kong china association computational guistics jin yao xiaojun wan jianguo xiao cent advances document summarization edge information systems michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan dragomir radev graph based neural multi document summarization proceedings ence computational natural language learning conll pages vancouver canada association computational linguistics yasuhisa yoshida jun suzuki tsutomu hirao masaaki nagata dependency based course parser single document summarization proceedings conference pirical methods natural language processing emnlp pages doha qatar ation computational linguistics xingxing zhang mirella lapata furu wei ming zhou neural latent extractive document proceedings summarization ference empirical methods natural language processing pages association tational linguistics xingxing zhang furu wei ming zhou bert document level pre training hierarchical bidirectional transformers document proceedings annual meeting tion association computational linguistics pages florence italy association computational linguistics ming zhong pengfei liu danqing wang xipeng qiu xuanjing huang searching tive neural extractive summarization works proceedings annual meeting association computational guistics pages florence italy tion computational linguistics qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural ument summarization jointly learning score select sentences proceedings nual meeting association computational linguistics volume long papers pages association computational linguistics appendix figure provides sets examples constructed graphs cnndm specically strictly symmetric self loop added nodes prevent graph growing sparse hand diagonal entries zero node rst graph points figure examples adjacent matrix ence graphs rst graphs
