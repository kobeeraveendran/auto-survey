discourse aware neural extractive text summarization jiacheng zhe yu jingjing university texas austin dynamics ai research utexas edu zhe gan yu cheng com r l c s c v v x r abstract recently bert adopted ument encoding state art text marization models sentence based extractive models result redundant uninformative phrases extracted summaries long range dependencies document tured bert pre trained tence pairs instead documents address issues present discourse aware neural summarization model discobert extracts sub sentential discourse units instead sentences candidates extractive selection ner granularity capture long range dependencies discourse units structural discourse graphs constructed based rst trees ence mentions encoded graph tional networks experiments proposed model outperforms state art methods signicant margin popular summarization benchmarks compared bert base models introduction neural networks achieved great success task text summarization nenkova et al yao et al main lines research abstractive extractive abstractive paradigm rush et al et al celikyilmaz et al sharma et al focuses generating summary word word encoding document extractive approach cheng lapata zhou et al narayan et al directly selects tences document assemble mary abstractive approach exible work rst author intern microsoft illustration datasets available com jiacheng xu discobert illustration discobert text figure marization sentence based bert model baseline selects sentences proposed discourse aware model discobert selects edus right gure illustrates discourse graphs use graph mentions pulitzer prizes highlighted examples rst graph induced rst discourse trees generally produces redundant summaries extractive approach enjoys better ity efciency cao et al recently hybrid methods posed advantage designing stage pipeline rst select rewrite compress candidate sentences chen bansal gehrmann et al zhang et al xu durrett compression rewriting aims discard uninformative phrases lected sentences hybrid systems suffer inevitable disconnection stages pipeline modeling long range context document summarization remains challenge xu prestigious bestowed journalists people arts today pulitzer prize journalism went post courier newspaper charleston south tiny staff daily circulation winner iconic photo new york times photographer daniel berehulak winning shows james dorbor suspected infected carried medical staff ebola treatment center monrovia liberia pulitzer awarded annually columbia recognize extraordinary work u s journalism literature drama categories winners coveted award included st louis post dispatch coref graphrst graphdocument pulitzer winner iconic photo new york times photographer daniel berehulak winning shows james dorbor suspected infected carried medical staff ebola treatment center monrovia liberia prestigious bestowed journalists people arts today pulitzer prize journalism went post courier newspaper charleston south tiny staff daily circulation selection et al pre trained language models vlin et al designed sentences short paragraph poor capturing range dependencies document pirical observations liu lapata adding standard encoders lstm transformer vaswani et al bert model inter sentential relations bring performance gain paper present discobert discourse aware neural extractive summarization model built bert perform compression extraction simultaneously reduce dancy sentences elementary course unit edu sub sentence phrase unit inating rst mann thompson carlson et al minimal selection unit instead sentence extractive summarization figure shows example discourse tion sentences broken edus tated brackets operating discourse unit level model discard redundant details sub sentences retaining additional pacity include concepts events leading concise informative summaries furthermore netune representations discourse units injection prior edge leverage intra sentence discourse relations specically discourse oriented graphs proposed rst graph gr coreference graph gc discourse graphs graph convolutional network gcn kipf welling imposed capture long range interactions edus rst graph constructed rst parse trees edus document hand coreference graph connects entities coreference clusters mentions document path coreference navigates model core event occurrences event parallel explores interactions concepts events main contribution threefold pose discourse aware extractive summarization model discobert operates sentential discourse unit level generate cise informative summary low propose structurally model dancy adopt rst discourse framework availability existing tools nature rst tree ture compression observations louis et al alternatives includes graph bank wolf gibson pdtb miltsakaki et al figure example discourse segmentation rst tree conversion original sentence segmented edus box parsed rst discourse tree box converted based rst discourse tree shown box c cleus nodes including satellite nodes including denoted solid lines dashed lines respectively relations italic edu head tree span edu head span inter sentential context types discourse graph iii discobert achieves new state art popular newswire text summarization datasets outperforming bert base models discourse graph construction section rst introduce cal structure theory rst mann son linguistic theory discourse ysis explain construct discourse graphs discobert types course graph considered rst graph erence graph edges initialized nected connections later added subset nodes based rst discourse parse tree coreference mentions discourse analysis discourse analysis focuses inter sentential tions document conversation rst framework discourse structure text represented tree format document segmented contiguous adjacent non overlapping text spans called elementary course units edus edu tagged nucleus satellite characterizes arity saliency nucleus nodes generally central satellite nodes peripheral important terms content grammatical reliance dependencies edus represent rhetorical relations work treat edu minimal unit content selection text summarization rst discourse tree converted rst discourse winner iconic photo new york times photographer daniel berehulak winning series shows james dorbor suspected infected ebola carried medical staff ebola treatment center monrovia liberia conversion sec ure shows example discourse segmentation parse tree sentence edus rhetorical relations represent functions different discourse units observed louis et al rst tree structure serves strong indicator content selection hand agreement rhetorical tions tends lower ambiguous encode rhetorical relations explicitly model content selection text summarization expect model select concise pivotal concept document low dancy traditional extractive rization methods model required select sentence parts sentence necessary proposed approach select ne grained edus der generated summaries redundant serves foundation discobert model rst graph selecting sentences candidates tive summarization assume sentence grammatically self contained edus restrictions need considered ensure maticality example figure illustrates rst discourse parse tree sentence iconic series grammatical sentence shows need stand dependencies edus ensure grammaticality selected combinations detail derivation dependencies found sec construction rst graph aims vide local paragraph level range document level connections edus use converted dependency version tree build rst graph gr initializing graph treating discourse dency th edu j th edu directed edge e coreference graph text summarization especially news tion usually suffers known position bias issue kedzie et al key information described beginning example figure details suspected child exact location photo carrying child unlikely reected nal summary algorithm construction coreference graph gc require coreference clusters c cn mentions cluster ci eim initialize graph gc edge gc n collect location occurences eim l lm j m k m gc end end return constructed graph gc document decent information spread middle end document ignored summarization models observe oracle sentences appear rst sentences cnndm dataset long news articles multiple core acters events document existing neural models poor ing long range context especially multiple ambiguous coreferences resolve encourage guide model capture long range context document pose coreference graph built discourse units algorithm describes construct coreference graph rst use stanford corenlp manning et al detect coreference clusters article coreference cluster discourse units containing mention cluster connected process iterated coreference mention clusters create nal coreference graph figure provides example pulitzer prizes important entity occurred multiple times multiple discourse units constructed coreference graph shown right graph gc constructed edges connected mentions pulitzer prizes discobert model overview figure provides overview proposed model consisting document encoder graph encoder document encoder trained bert model rst encode intentionally ignore entities mentions example simplicity figure left model architecture discobert stacked discourse graph encoders contain k stacked dge blocks right architecture discourse graph encoder dge block document token level attentive span extractor obtain edu representations corresponding text spans graph encoder takes output ment encoder input updates edu resentations graph convolutional network based constructed discourse graphs predict oracle labels assume document d segmented n edus total e d dn denotes th edu following liu lapata formulate extractive summarization sequential labeling task edu di scored neural networks decisions based scores edus oracle labels sequence binary labels stands selected denote labels y y y n training aim predict sequence labels y given document d inference need consider discourse dependency ensure coherence grammaticality output summary y document encoder bert pre trained deep bidirectional encoder vaswani et al devlin et al following liu lapata code document bert netune bert model summarization bert originally trained encode single sentence sentence pair news article typically contains words need adaptation apply bert document encoding specically insert tokens beginning end sentence respectively order encode long documents news articles tend maximum sequence length bert experiments input document tokenization denoted d dn di number bpe tokens th edu rst edu sentence token prepended di dj edu sentence token appended figure schema insertion approach liu lapata simplicity tokens shown equations bert model encode document hb hb hb document length input bert output hb bert encoder representation token sentence tation approach work setting need extract tation edus instead adopt tried inserting beginning end edu treating corresponding representation representation edu performance drops drastically prestigious honors cls sep bestowed arts today pulitzer prize south carolina tiny cls discourse graph convolutional network self attentive span extractor spanext proposed lee et al learn edu representation th edu words output bert encoder hb obtain edu representation follows hb hb ij ij aij hs aij hb ij ij score j th word edu aij normalized attention j th word w t words span hs weighted sum bert output hidden states paper w matrices b vectors eters learn abstract self attentive span extractor hs hb span extraction step ment represented sequence edu n rdhn hs sentations hs hs sent graph encoder graph encoder given constructed graph g v e nodes v correspond edus document edges e correspond rst discourse relations coreference mentions use graph volutional network update representations edus capture long range cies missed bert better summarization modularize architecture design present single discourse graph encoder dge layer multiple dge layers stacked experiments assume input k th dge layer denoted n rdhn corresponding output denoted rdhn k th dge layer designed follows n relu j jni ln represents layer normalization ni denotes neighorhood th edu node output th edu k th dge layer hs output document encoder k layers dataset document sum e graph sent edu tok tok gr cnndm nyt gc table statistics datasets rst block shows average number sentences edus tokens documents second block shows average number tokens reference summaries block shows average number edges constructed rst graphs gr coreference graphs gc respectively graph propagation obtain hg rdhn nal representation edus stacked dge layers different graphs parameter dges shared use graphs output concatenated hg r c hg training inference training hg predicting acle labels specically yi represents logistic function yi prediction probability ranging training loss model binary cross entropy loss given predictions oracles l y yi discobert graphs output document encoder hs prediction stead creation oracle operated edu level greedily pick edus sary dependencies drops inference given input document ter obtaining prediction probabilities edus e y yn sort y scending order select edus accordingly note dependencies edus forced prediction ensure grammacality erated summaries experiments section present experimental results popular news summarization datasets compare proposed model state art baselines conduct detailed analysis validate effectiveness discobert datasets evaluate models datasets new york times nyt sandhaus cnn mail cnndm hermann et al use script et al extract summaries raw data stanford corenlp sentence boundary detection tokenization parsing ning et al limitation bert encode bert bpes table provides statistics datasets edges gc undirected gr directional cnndm samples training validation test respectively use un anonymized version previous summarization work nyt licensed following previous work zhang et al xu durrett use samples training validation test respectively state art baselines compare model following state art neural text summarization models extractive models banditsum treats tive summarization contextual bandit lem trained policy gradient methods dong et al neusum extractive model architecture attention nism scores document emits index selection zhou et al compressive models jecs neural compression based summarization model blstm encoder xu durrett rst stage selecting sentences ond stage sentence compression pruning stituency parsing tree bert based models bert based models achieved signicant improvement cnndm nyt compared lstm counterparts bertsum rst bert based extractive marization model liu lapata baseline model bert implementation bertsum pnbert proposed bert based model training strategies including ment learning pointer networks zhong et al hibert hierarchical bert based model document encoding pretrained unlabeled data zhang et al implementation details use allennlp gardner et al code framework implementation graph model oracle sentence oracle discourse r l neusum zhou et al banditsum dong et al jecs xu durrett pnbert zhong et al pnbert w rl bert zhang et al hiberts hibert s hibert m bertsum liu lapata base raffel et al bert discobert discobert w gc discobert w gr discobert w gr gc table results test set cnndm dataset reported models asterisk symbol extra data pre training shorthands unigram bigram lap r l longest common subsequence encoding based dgl wang et al periments conducted single nvidia card mini batch size set gpu memory capacity length document truncated bpes use pre trained bert base uncased model ne tune periments train models steps rouge lin evaluation metrics validation criteria realization discourse units structure critical edu pre processing quires steps discourse segmentation rst parsing segmentation phase use neural discourse segmenter based bilstm crf framework wang et al segmenter achieved score rst dt test set human performance ing phase use shift reduce discourse parser extract relations identify neuclrity ji eisenstein dependencies edus crucial grammaticality selected edus steps learn derivation cies head inheritance tree conversion head inheritance denes head node valid non terminal tree node leaf node ldc upenn neuraleduseg com pku com jiyfeng dplp head determine head non terminal nodes based nuclearity example figure heads text spans need grounded gle edu propose simple effective schema convert rst discourse tree based discourse tree consider dependency restriction reliance lite nucleus create oracle processing model makes tion example figure model selects carried liberia date span enforce model select shows series number chosen edus depends average length reference summaries dencies edus mentioned length existing content optimal average number edus selected tuned ment set experimental results results cnndm table shows results cnndm rst section includes baseline sentence based oracle discourse based oracle second section lists performance line models including non bert based based variants performance proposed model listed section bert implementation sentence based bert model discobert discourse based bert model discourse graph encoder discobert w gc discobert w gr based bert model coreference graph rst graph respectively discobert w gr gc fusion model encoding graphs proposed discobert beats based counterpart competitor els help discourse graph coder graph based discobert beats art bert model signicant margin ablation study individual graphs shows rst graph slightly helpful coreference children head current node inherits head left child child n s head current node inherits head n child child node n s head s node depends head n node children n right child contain subject discourse head right n node depends head left n node model oracle sentence oracle discourse r l jecs xu durrett bert zhang et al hiberts hibertm hibert s hibert m bert discobert discobert w gc discobert w gr discobert w gr gc table results test set nyt dataset models asterisk symbol extra data pre training graph combination achieves ter performance overall results nyt results summarized ble proposed model surpasses previous state art bert based model signicant s hibert margin hibert m extra data pre training model notice nyt dataset improvement comes use edus minimal selection units cobert provides gain bert baseline use course graphs help case grammaticality segmentation partial selection tence output model grammatical original sentence ally examined automatically evaluated model output observed overall generated summaries grammatical given rst dependency tree constraining rhetorical tions edus set simple effective post processing rules helps complete edus cases automatic grammar checking followed xu durrett perform automatic mar checking grammarly table shows grammar checking results average number errors characters ndm nyt datasets reported compare discobert sentence based bert model shows summation number rors categories shown table source m cr pv pt cnndm nyt sent disco sent disco o table number errors characters based automatic grammaticality checking marly cnndm nyt lower values ter detailed error categories including correctness cr passive voice pv misuse punctuation pt compound complex sentences o listed left right model sent disco ref coherence grammaticality table human evaluation results ask turkers grade overall preference coherence icality mean values standard deviations reported clare hines lives brisbane diagnosed brain tumour suffering epileptic seizures number tests doctors discovered benign tumour wrapped acoustic facial balance nerve told surgically removed risked tumour turning malignant week brain surgery found pregnant jordan henderson action aston villa wembley sunday agreed new liverpool deal club s vice captain puts pen paper deal liverpool rodgers sider henderson role club captain steven gerrard moves la galaxy end campaign england international delighted agreed terms contract peak years career table example outputs cnndm cobert strikethrough indicates discarded edus originates missing improper missing anaphora resolution example johnny believed actually police selecting ond edu yields sentence actually ne clear mentioned summaries generated model retained quality original text related work human evaluation sampled documents test set cnndm sample asked turkers grade summaries results shown table bert model original bertsum model lects sentences document providing best overall readability coherence maticality cases reference summaries long phrases scores slightly lower sentence model discobert model slightly worse sent bert model fully comparable variants examples analysis examples model output table notice decent irrelevant details removed extracted summary despite success conducted ror analysis found errors inated rst dependency resolution upstream parsing error discourse parser misclassication rst dependencies hand crafted rules dependency resolution hurted grammaticality coherence generated outputs common punctuation issues include extra missing commas ing quotation marks coherence issue neural extractive summarization neural works widely extractive rization decoding approaches including ranking narayan et al index prediction zhou et al sequential labelling lapati et al zhang et al dong et al applied content selection model uses similar conguration encode document bert liu lapata use discourse graph structure graph encoder handle long range dependency issue neural compressive summarization text summarization compression deletion explored recent work xu durrett presented stage neural model selection compression based constituency tree pruning dong et al presented neural sentence compression model discrete operations including deletion addition different studies use edus minimal selection basis sentence compression achieved automatically model discourse summarization use course theory text summarization plored louis et al examined benet graph structure provided discourse lations text summarization hirao et al yoshida et al formulated tion problem trimming document course tree durrett et al presented system sentence extraction compression ilp methods discourse structure li et al demonstrated edus units content selection leads stronger summarization mance compared proposed method rst neural end end summarization model edus selection basis graph based summarization graph approach explored text summarization decades lexrank introduced stochastic based method computing relative importance textual units erkan radev sunaga et al employed gcn lation graphs sentence embeddings obtained rnn tan et al proposed based attention abstractive summarization model fernandes et al developed framework reason long distance relationships text rization conclusion paper present discobert uses discourse unit minimal selection basis reduce summarization redundancy leverages types discourse graphs inductive bias capture long range dependencies discourse units validate proposed approach popular summarization datasets observe sistent improvement baseline models future work explore better graph encoding methods apply discourse graphs tasks require long document encoding acknowledgement thanks junyi jessy li greg durrett yen chun chen members microsoft dynamics ai research team reading feedback suggestions references ziqiang cao furu wei wenjie li sujian li faithful original fact aware neural tive summarization aaai conference cial intelligence lynn carlson daniel marcu mary ellen okurovsky building discourse tagged pus framework rhetorical structure theory proceedings second sigdial workshop discourse dialogue asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents proceedings abstractive summarization conference north american chapter association computational linguistics man language technologies volume long pers pages new orleans louisiana association computational linguistics yen chun chen mohit bansal fast tive summarization reinforce selected tence rewriting proceedings annual meeting association computational guistics volume long papers pages association computational linguistics jianpeng cheng mirella lapata neural marization extracting sentences words proceedings annual meeting sociation computational linguistics volume long papers pages berlin germany sociation computational linguistics jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language proceedings conference standing north american chapter association computational linguistics human language technologies volume long short papers pages minneapolis minnesota ation computational linguistics yue dong zichao li mehdi rezagholizadeh jackie chi kit cheung editnts neural programmer interpreter model sentence proceedings cation explicit editing annual meeting association putational linguistics pages florence italy association computational linguistics yue dong yikang shen eric crawford herke van hoof jackie chi kit cheung banditsum extractive summarization contextual bandit proceedings conference cal methods natural language processing pages association computational tics greg durrett taylor berg kirkpatrick dan klein learning based single document rization compression anaphoricity proceedings annual straints ing association computational linguistics volume long papers pages ation computational linguistics gunes erkan dragomir r radev lexrank graph based lexical centrality salience text summarization journal articial intelligence search dialogue pages association tional linguistics patrick fernandes miltiadis allamanis marc brockschmidt structured neural tion arxiv preprint matt gardner joel grus mark neumann oyvind tafjord pradeep dasigi nelson f liu matthew ters michael schmitz luke zettlemoyer deep semantic natural language proceedings workshop cessing platform nlp open source software nlp oss pages melbourne australia association tional linguistics sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference cal methods natural language processing pages association computational tics karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa man phil blunsom teaching machines read comprehend c cortes n d lawrence d d lee m sugiyama r nett editors advances neural information cessing systems pages curran ciates inc tsutomu hirao yasuhisa yoshida masaaki nishino norihito yasuda masaaki nagata document summarization tree knapsack proceedings conference lem empirical methods natural language processing seattle washington usa yangfeng ji jacob eisenstein tion learning text level discourse parsing ceedings annual meeting tion computational linguistics volume long papers pages baltimore maryland ation computational linguistics chris kedzie kathleen mckeown hal daume iii content selection deep learning models proceedings summarization ference empirical methods natural language processing pages thomas n kipf max welling supervised classication graph convolutional networks proceedings iclr kenton lee luheng mike lewis luke moyer end end neural coreference proceedings conference lution empirical methods natural language processing pages copenhagen denmark association computational linguistics junyi jessy li kapil thadani amanda stent role discourse units near extractive proceedings annual rization ing special interest group discourse chin yew lin rouge package matic evaluation summaries text tion branches yang liu mirella lapata text proceedings tion pretrained encoders conference empirical methods ural language processing international joint conference natural language processing emnlp ijcnlp pages hong kong china association computational linguistics annie louis aravind joshi ani nenkova discourse indicators content selection rization proceedings sigdial ference pages tokyo japan association computational linguistics william c mann sandra thompson rhetorical structure theory functional ory text organization text interdisciplinary nal study discourse christopher manning mihai surdeanu john bauer jenny finkel steven bethard david mcclosky stanford corenlp natural language processing toolkit proceedings annual meeting association computational guistics system demonstrations pages sociation computational linguistics eleni miltsakaki rashmi prasad aravind joshi bonnie webber penn discourse proceedings fourth international bank conference language resources evaluation ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural work based sequence model extractive rization documents aaai conference cial intelligence shashi narayan shay b cohen mirella lapata ranking sentences extractive tion reinforcement learning proceedings conference north american ter association computational linguistics human language technologies volume long pers pages association tional linguistics ani nenkova kathleen mckeown al matic summarization foundations trends information retrieval colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li peter j liu exploring limits transfer learning unied text text arxiv preprint alexander m rush sumit chopra jason weston neural attention model abstractive proceedings sentence summarization conference empirical methods natural language processing pages association computational linguistics evan sandhaus new york times annotated corpus linguistic data consortium philadelphia abigail peter j liu christopher d ning point summarization pointer generator networks proceedings annual meeting association tational linguistics volume long papers pages association computational tics eva sharma luyang huang zhe hu lu wang entity driven framework abstractive proceedings summarization ference empirical methods natural language processing international joint ence natural language processing ijcnlp pages jiwei tan xiaojun wan jianguo xiao abstractive document summarization proceedings based attentional neural model annual meeting association computational linguistics volume long papers pages vancouver canada association computational linguistics ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention advances neural information need cessing systems pages minjie wang lingfan yu da zheng quan gan yu gai zihao ye mufei li jinjing zhou qi huang chao ma ziyue huang qipeng guo hao zhang haibin lin junbo zhao jinyang li alexander j smola zheng zhang deep graph brary efcient scalable deep iclr workshop representation ing graphs learning graphs manifolds yizhong wang sujian li jingfeng yang fast accurate neural discourse proceedings conference tation empirical methods natural language processing pages brussels belgium association computational linguistics florian wolf edward gibson representing discourse coherence corpus based study tational linguistics jiacheng xu danlu chen xipeng qiu xuanjing huang cached long short term memory ral networks document level sentiment proceedings conference cation empirical methods natural language ing pages austin texas association computational linguistics jiacheng xu greg durrett neural tive text summarization syntactic compression proceedings conference cal methods natural language processing hong kong china association computational guistics jin ge yao xiaojun wan jianguo xiao cent advances document summarization edge information systems michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan dragomir radev graph based neural multi document summarization proceedings ence computational natural language learning conll pages vancouver canada association computational linguistics yasuhisa yoshida jun suzuki tsutomu hirao masaaki nagata dependency based course parser single document summarization proceedings conference pirical methods natural language processing emnlp pages doha qatar ation computational linguistics xingxing zhang mirella lapata furu wei ming zhou neural latent extractive document proceedings summarization ference empirical methods natural language processing pages association tational linguistics xingxing zhang furu wei ming zhou bert document level pre training hierarchical bidirectional transformers document proceedings annual meeting tion association computational linguistics pages florence italy association computational linguistics ming zhong pengfei liu danqing wang xipeng qiu xuanjing huang searching tive neural extractive summarization works s proceedings annual meeting association computational guistics pages florence italy tion computational linguistics qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural ument summarization jointly learning score select sentences proceedings nual meeting association computational linguistics volume long papers pages association computational linguistics appendix figure provides sets examples constructed graphs cnndm specically gc strictly symmetric self loop added nodes prevent graph growing sparse hand diagonal entries gr zero node rst graph points figure examples adjacent matrix ence graphs gc rst graphs gr
