e f v c s c v v x r prism unified framework parameterized submodular information measures targeted data subset selection summarization preprint vishal kaushal department computer science indian institute technology bombay iitb ac suraj kothawade department computer engineering university texas dallas suraj edu ganesh ramakrishnan department computer science indian institute technology bombay iitb ac jeff bilmes department electrical computer engineering university washington seattle edu rishabh iyer department computer science university texas dallas rishabh edu march abstract increasing data techniques nding smaller effective subsets specic characteristics important motivated present prism rich class parameterized submodular information measures applications targeted subsets desired demonstrate utility prism applications apply prism improve supervised model s performance given additional labeling cost targeted subset selection prism tss subset unlabeled points matching target set added training set prism tss generalizes connected existing approaches targeted data subset selection second apply prism nuanced targeted summarization prism tsum data e image collections text videos summarized quicker human consumption additional user intent prism tsum handles multiple avors targeted summarization focused topic irrelevant privacy preserving update summarization unied way prism tsum generalizes unies existing past work targeted summarization extensive experiments image classication image collection summarization empirically verify superiority prism tss prism tsum state art introduction recent times seen unprecedented growth data modalities text images videos naturally given rise techniques nding effective smaller subsets data variety end tasks example data subset selection efcient cost effective training machine learning models need select samples informative training model training smaller subsets data entails signicant speedups reduction labeling time cost sacricing accuracy example summarization image collection video text document summarized quicker human consumption equal contribution authors ordered alphabetically preprint march eliminating redundancy preserving main content end tasks want able select subsets align certain target set present motivating applications fig motivating applications targeted data subset selection real world settings distribution shift training data test data cases model s performance improved given additional labeling cost augmenting training data informative samples matching target distribution called targeted subset large pool unlabeled data way achieving assuming access clean validation set matching target set distribution target example target set critical slice data e indoor images people dark background example images specic classes user care want improve model s performance target sacricing overall accuracy minimum additional labeling costs fig figure motivating applications example targeted data subset selection night images target represented train example targeted summarization query set private set target targeted summarization number applications require generic summarization e simply picking representative diverse subset massive dataset important capture certain user intent summarization fig user intent somewhat nuanced modeled query focused summarization small representative set points relevant specic query selected irrelevant privacy preserving summarization small representative set points desired irrelevant given topic completely different given private set data points update summarization subset selected conditioned summary seen user rest paper collectively refer different avors targeted summarization target assuming different semantics case contributions motivated applications present prism rich class parameterized submodular information measures specically summarize key contributions follows prism build submodular mutual information functions mi conditional gain functions cg recently introduced iyer et al unique novel extension described section comes ability consider query set mi conditioning set cg different auxiliary set compared ground set requirement targeted data subset selection targeted summarization target different ground set subset desired extension restricted submodularity enables potentially richer class mi cg functions c novel parameterization functions help model aspects trade offs query relevance diversity hardness privacy constraints study rich modeling capabilities different functions prism state specic results lemmas prism tss targeted data subset selection apply prism targeted data subset selection prism tss section demonstrate empirically section signicantly outperforms techniques improving accuracy image classication mnist classes interest gain model s performance training added targeted subset methods given tional labeling cost increase overall accuracy prism tss generalizes interesting connections number existing approaches targeted data subset selection c lemma prism tsum targeted summarization apply prism targeted summarization prism tsum tion prism tsum offers unied approach different semantics target query focused targeteddata subsetselectionaugment ttargeted subset tlabelled training datatargetsunlabelled datasetretrain modelaugmented training datageneric summarizationquery focusedprivacy collection preprint march summarization topic irrelevant privacy preserving summarization update summarization prism tsum generalizes number past approaches query focused update summarization demonstrate section outperforms methods real world image collections dataset owing richer learning parameters related work submodularity submodular information measures submodularity fujishige rich tractable non linear combinatorial optimization ensures tractable algorithms krause golovin nice connections convexity concavity bach lovsz iyer bilmes work builds provides unique novel extension recently introduced class submodular information measures gupta levin iyer et al data subset selection number papers studied data subset selection different applications set past works explores supervised data subset selection reducing training time examples approaches include submodular functions selection kaushal et al wei et al liu et al coresets nd effective weighted subsets training data craig mirzasoleiman et al discrete bi level optimization optimize held validation set performance glister killamsetty et al explore unsupervised subset selection optimal subset unlabeled set selected labeled minimize labelling costs kaushal et al iteratively called active learning number techniques uncertainty sampling query committee settles studied recently batch active learning prominent recent techniques combine diversity uncertainty wei et al sener savarese ash et al state art approach badge ash et al samples points diverse hypothesized gradients apply prism tss nd targeted subset unlabeled data improving model s performance target interest given additional labeling cost demonstrate superior performance compared summarization number instances summarization studied past including image tion summarization celis keswani ozkose et al singh et al tschiatschek et al text document summarization lin bilmes chali et al yao et al bairi et al video summarization kaushal et al gygli et al ji et al works focused generic summarization works studied query focused video summarization sharghi et al vasudevan et al xiao et al jiang han query focused document summarization lin bilmes li et al update summarization documents dang owczarzak delort alfonseca li et al best knowledge prism tsum rst attempt offer unied treatment different avors summarization preliminaries submodular functions let v denote ground set n data points v n set function function submodular fujishige satises diminishing marginal returns x y v j y facility location set cover log determinants examples iyer close connections submodularity entropy submodular functions viewed information functions zhang yeung submodularity ensures greedy algorithm achieves bounded approximation factor maximized nemhauser et al conditional gain cg given set items b v conditional gain gain function value adding b b b intuitively measures different b refer b conditioning set private set submodular conditional mutual information mi cmi given set items b v submodular mutual information mi gupta levin iyer et al dened b b b intuitively measures similarity b refer b query set conditional submodular mutual information cmi dened cg mi c intuitively cmi jointly models mutual similarity b dissimilarity c properties cg mi cmi cg mi cmi non negative monotone argument xed gupta levin iyer et al cmi mi necessarily submodular argument xed krause et al iyer et al instantiations dene turn submodular preprint march prism introduce prism extend cg mi cmi handle case target come auxiliary set v different ground set v targeted data subset selection v source set data instances target subset data points validation set specic set examples interest case targeted summarization v set data points user wants summarize images video frames video shots sentences target query set query focused summarization private set topic irrelevant privacy preserving summarization conditioning set update summarization let v v dene set function f dened discrete optimization problem dened subsets v nd optimal subset given query set q v dene q v maximize private set p v conditioning set set topics want subset irrelevant dene hp v function maximized shall offer rich class models motivating applications extend dening generalized submodular mutual information functions restricted submodularity generalized submodular mutual information gmi submodular tions expressive natural choices submodular need submodular sets optimizing subsets v instead requiring submodular inequality hold pairs sets x y particular dene subset c restricted submodularity c satises x y x y x y y c instances restricted submodularity form intersecting crossing submodular functions considered past fujishige consider following form restricted submodularity dene gmi given sets v v dene v sets x y v satisfy following conditions x v x v y set x set y v y v use notion gmi dene concave modular com query saturation q sat functions interesting connections past work section state properties gmi following lemma defer proof appendix b lemma given restricted submodular function v b v b v b monotone v xed b v equivalently b monotone b v xed v instantiations prism f sc psc gc wi iu ia jv sij sij ja q wi iu ia jq sij iu wi sij ia jp wi iu ia jq sij logdet log fl iv max ja sij fl max ja sij log log ja iv q sij max jq q st sij log p p sij p st ja sij iv max jp useful log sp q s sap q s p ap q q st p q st ap q ja sij max jp iv max jq useful sij sij com equation useful useful q sat ic v v ic ic table instantiations parameterizations prism section particularly useful c lemma present expressions different instantiations prism table discuss introduce new instantiations log determinant com q sat borrow basic instantiations set cover sc probabilistic set cover psc graph cut gc facility location fl iyer et al adapt setting distinct summary space v auxiliary space v derive alternative expression flmi interesting characteristics different submodular functions model different characteristics instantiations differ treatment interplay characteristics alignment target important note instantiations considered table parameterized models internal parameters represented jointly learned model parameters section functions let address broad spectrum semantics max ja sij iq ia max jq sij ia jq ic jq sij ia sij preprint march log determinant logdet refer mi cg cmi applied logdet function logdetmi logdetcg logdetcmi respectively present expressions fourth row table denote sa b similarity matrix items sets b denote sab sab construct similarity matrix way cross similarity q multiplied control trade query relevance diversity cross similarity p control hardness privacy constraints higher values ensure stricter privacy constraints transitioning topic irrelevant privacy preserving summarization simplicity notation provide cmi expression defer general expression proof lemma appendix b lemma similarity matrix dened f log q log log q st p similarly p q ap q q log p p sp ap sap log q st q st p st facility location fl present versions mi functions fl rst similar derived iyer et al presented fth row table instantiate variant considers cross similarities data points target note mi expression interesting characteristics different particular gets saturated models pairwise similarities target data points vice versa state lemma defer proof appendix b lemma given similarity kernel s sij j j v j v facility location function f maxja sij obtain expression mi q ia maxjq sij cg cmi expressions particularly useful case iq maxja sij finally note similar log determinant parameters mi cg functions appropriately multiplying cross similarities q appendix b details concave modular com notion generalized submodular mutual information functions presented earlier allows characterize rich class concave modular functions gmi functions dene set function sij n sij iv iv jav jav sij n sij jav jav ia jq sij restricted submodular state expression gmi function following lemma proof appendix b lemma function restricted submodular function v furthermore gmi f exactly q jq ia sij given kernel matrix satises sij j j v j v cg cmi expressions particularly useful case query saturation q sat dene set function ic v v rst restricted submodular function provide expression gmi result defer expressions cg conditional gmi variants proofs appendix b lemma function dened restricted submodular q furthermore ic expression interesting fact generalizes rouge lin common evaluation metric summarization details section graph cut gc gc dened fgc sij sij measures similarity elements sij ia jv ja j parameter captures trade diversity representativeness reproduce expressions gcmi gccg iyer et al row table note cmi expression gc involve private set exactly mi version proof appendix b like logdet introduce additional parameter gccg control sensitivity privacy modeled easily gc objective multiplying cross similarity data points private instances set cover sc probabilistic set cover psc let denote concepts covered set sc psc functions dened f w iu iu wi w weights concepts u set concepts probability nt cover concept reproduce expressions sc psc functions iyer et al rst rows table w p preprint march figure behavior different functions prism effect parameters plots share legend representational power prism empirically verify intuitive understanding expressions synthetically created dataset maximize different functions prism different parameters study characteristics subsets qualitatively quantitatively dene query coverage fraction queries covered subset query relevance fraction subset pertaining queries diversity measure diverse points selected subset privacy irrelevance fraction subset matching private instances present representative results fig provide detailed results appendix c mi functions verify increasing tends increase query relevance reducing query coverage diversity left fig gcmi lies end spectrum favoring query relevance lies end favoring diversity query coverage logdetmi com lie right fig expected cg functions increasing increases privacy irrelevance logdetcg outperforms flcg gccg terms diversity privacy irrelevance left fig cmi functions flcmi tends favor query coverage diversity contrast query relevance privacy irrelevance logdetcmi favors query relevance privacy irrelevance query coverage diversity right fig prism tss setting rst apply prism simple setting targeted data subset selection improving model s accuracy target classes instances given additional labeling cost k instances compromising overall accuracy follows let e initial training set labeled instances t set examples user cares desires better performance let u large unlabeled dataset maximize mi function t compute optimal subset u size k given t query target set augment e labeled train model achieve better accuracy compromising accuracy classes instances instantiating rich class mi functions including gcmi com logdetmi prism tss offers rich treatment targeted subset selection framework allows adding explicit diversity term helpful cases gcmi model diversity algorithm summarized algorithm algorithm prism tss require initial labeled set examples e large unlabeled dataset u target subset slice want improve accuracy t loss function l learning train model loss l labeled set e obtain parameters e compute gradients e yi u hypothesized labels e yi t compute similarity kernels s includes kernel elements u t u t dene submodular function diversity function g maxau t obtain labels elements l train model combined labeled set e l preprint march algorithm generalizes interesting connections number recently proposed subset selection approaches special case prism tss viewed approximating target set gradients connections glister closest setting glister killamsetty et al selects subset optimizing validation set target setting authors study active learning variant called active glister framework authors solve discrete bi level problem online meta learning based approach essentially gradient step instead completely solving inner optimization problem authors approach results submodular optimization problem number loss functions including hinge loss logistic loss square loss perceptron loss lemma shows glister applied targeted data selection appendix d details fact special case algorithm defer proof appendix d glister tss lemma glister tss hinge loss logistic loss perceptron loss special case algorithm com connections badge craig recently proposed data selection active learning algorithms craig mirzasoleiman et al badge ash et al craig applied supervised data selection proceeds selecting subset maximizes facility location objective iv maxja sij similarity sij computed gradients ith jth data point badge studies active learning based setting gradient based instead considers hypothesized labels computing gradients unlabeled set similar algorithm badge uses k select diverse subset data points instead maximizing facility location function easy consider extension badge fl note easily extend craig badge targeted scenario optimize function maxja sij sij gradient similarity points hypothesized labels unlabeled set function exactly prism tss approximating target set gradients natural formulation targeted data subset selection select subset average gradient difference target set t minimized particular dene e ix li loss ith data point denote lu loss unlabeled set lt loss target set following lemma proof appendix d shows minimizing gradient difference special case algorithm lemma minimizing gradient difference eq rewritten special case algorithm t j j gcmi ia jt e lu e lt diversity function prism tsum setting given set v data points images sentences document frames shots setting video goal nd summary v desired characteristics target assumes different semantics different avors summarization query set q query focused summarization private set topics p topic irrelevant privacy preserving summarization context update summarization target summary user seen goal nd summary different unied framework prism tsum given sets b t restricted submodular function consider following master optimization problem discuss different avors summarization seen special cases master optimization problem setting b v t yields generic summarization similarly setting b q t yields query focused summarization query set q setting b v t p gives privacy preserving summarization update summarization set t b v framework allows address avor joint query focused privacy preserving marization set b q t p possible avor query focused update summarization want summary similar q different achieved setting s q t preprint march parameter learning prism tsum multiple instantiations submodular information functions imparting certain characteristics summaries propose learning mixture model supervised human summaries build prior work learns mixtures submodular functions applications document summarization lin bilmes video summarization gygli et al kaushal et al image collection summarization tschiatschek et al extend joint learning internal parameters weights w individual components mixture denote parameter vector w prism tsum mixture model f fis instantiations prism diversity representation terms given n training examples v n y learn parameters optimizing following generalized hinge loss training example margin formulation min n f y f y n y human summary n max y v nth ground set video image collection text document v n features parameters learnt gradient descent specic objective functions gradient computations case query focused privacy preserving joint query focused privacy preserving summarizations presented appendix e generic summarization add standard submodular functions modeling representation diversity coverage mixture query focused summarization privacy preserving summarization use mi cmi functions respectively dened similar tschiatschek et al parameters learnt instantiate model learnt parameters maximize desired automatic summaries prism tsum generalizes existing approaches proposed prism tsum framework generalizes unies past work area inadvertently submodular information measures models mention past works defer details appendix f query dpp considered sharghi et al special case logdetmi similarly graph cut based query relevance term vasudevan et al lin li et al actually gcmi submodular function li et al update summarization gccg furthermore joint diversity query relevance term lin bilmes instance com square root concave function finally query specic rouge lin common evaluation metric document image summarization lin bilmes tschiatschek et al example query saturation q sat function connections demonstrate prism tsum rich effective model instances summarization experiments results effectiveness prism tss dataset baselines implementation details demonstrate effectiveness prism tss obtaining targeted subset improving image classication accuracy target classes mnist datasets simulate real world setting split available train set train validate data lake train set labeled instances poorly represents randomly picked classes target data lake large set labels use resembling large pool unlabeled data real world poorly represented classes perform validation set hold clue picking target interest performance measured test set respective datasets apply prism tss algorithm comparing mi functions existing approaches specically mi functions use logdetmi gcmi gcmi diversity equivalent intuitive approach minimizing average gradient difference target eq lemma existing approaches compare active learning baselines uncertainty sampling badge glister active glister running setting e select unlabeled subset active learning baselines explicitly information target set strengthen compare variants target aware rst targeted uncertainty sampling tus product uncertainty similarity target identify subset second glister tss lemma target set bi level optimization finally compare pure diversity representation functions fl gc logdet disparity sum dsum random sampling train model et al lenet lecun et al mnist cross entropy loss sgd optimizer training accuracy exceeds base model augmenting train set labeled version selected subset training model report average gain accuracy target classes overall gain accuracy classes averaged runs randomly picking classes target query focused case privacy preserving case cmi degenerates mi cg respectively preprint march figure comparison different methods targeted subset selection different budgets mnist x axis budgets y axis gain model accuracy target classes mi based approaches lines red signicantly outperform subset sizes section c figure targeted summarization results image collection summarization joint learning parameters proposed model prism tsum outperforms settings target section run prism tss different budgets study effect budget performance applicable internal parameters default values results table report results budget mnist setting realistic possible set target set smaller budget budget mnist report effect budget gain accuracy target classes fig datasets mi functions yield best improvement accuracy target classes gain model s performance training added targeted subset methods simultaneously increasing overall accuracy consistently outperform badge glister tss tus budgets recall discussion behavior different functions section expected logdetmi modeling query relevance diversity perform better functions tend prefer relevance gcmi tus functions tend prefer diversity representation badge fl gc dsum logdet observe budget increased mi functions outperform methods greater margins target class accuracy fig expected methods effective considering target details experimental setup additional discussion results appendix g effectiveness prism tsum dataset implementation details use image collections dataset tschiatschek et al dataset image collections images provides human summaries collection extend acquiring dense noun concept annotations image query focused privacy preserving joint query focused privacy preserving human summaries image collection suitable targeted summarization extract concepts images pre trained shelf networks represent preprint march mnist overall target method base random badge ash et al glister killamsetty et al glister tss settles tus logdet fl gc dsum logdetmi gcmi target overall table comparison prism tss mi functions methods budget mnist numbers gain accuracy target classes target classes overall base model training model text best existing approaches indicated highest blue highest red green respectively concept queries vector c universe concepts defer dataset implementation details appendix h prism tsum mixture model components appropriate instantiations mi cg cmi functions gc logdet fl com sc psc mixture weights internal parameters learnt following tschiatschek et al perform leave cross validation report average v rouge runs normalize v rouge s t human average random average results present targeted summarization results fig discussed section individual components mixture model models document video summarization compare approaches explicit past work targeted summarization image collection contrast performance individual components verify effect joint learning parameters compare prism tsum mixture model mixture exactly components prism tsum model weights learnt internal parameters set xed default values prism tsum outperforms techniques including mixture conrming effectiveness proposed framework especially joint learning parameters conclusion presented prism novel rich framework parameterized submodular information measures instantiations prism allow model broad spectrum semantics demonstrated effectiveness targeted data subset selection improving model s accuracy prism tss targeted summarization prism tsum showed prism tsum prism tss unify generalize past works areas experiments mnist image collections dataset empirically verify superiority prism existing methods references jordan t ash chicheng zhang akshay krishnamurthy john langford alekh agarwal deep batch active learning diverse uncertain gradient lower bounds iclr francis bach learning submodular functions convex optimization perspective arxiv preprint ramakrishna bairi rishabh iyer ganesh ramakrishnan jeff bilmes summarization multi document topic hierarchies submodular mixtures proceedings annual meeting association tational linguistics international joint conference natural language processing volume long papers pages preprint march l elisa celis vijay keswani implicit diversity image summarization proceedings acm computer interaction yllias chali moin tanvee mir tafseer nayeem abstractive multi document summarization submodular function based framework sentence compression merging proceedings eighth international joint conference natural language processing volume short papers pages hoa trang dang karolina owczarzak overview tac update summarization task tac jean yves delort enrique alfonseca dualsum topic model based approach update summarization proceedings conference european chapter association computational linguistics pages satoru fujishige submodular functions optimization elsevier anupam gupta roie levin online submodular cover problem acm siam symposium discrete algorithms michael gygli h grabner l gool video summarization learning submodular mixtures objectives ieee conference computer vision pattern recognition cvpr pages kaiming xiangyu zhang shaoqing ren jian sun deep residual learning image recognition proceedings ieee conference computer vision pattern recognition pages rishabh iyer jeff bilmes polyhedral aspects submodularity convexity concavity arxiv preprint rishabh iyer ninad khargoankar jeff bilmes himanshu asnani submodular combinatorial information measures applications machine learning arxiv preprint rishabh krishnan iyer submodular optimization machine learning theoretical results unifying scalable algorithms applications phd thesis zhong ji kailin xiong yanwei pang xuelong li video summarization attention based encoder decoder networks ieee transactions circuits systems video technology pin jiang yahong han hierarchical variational network user diversied query focused video summarization proceedings international conference multimedia retrieval pages vishal kaushal r iyer s kothawade sandeep subramanian ganesh ramakrishnan framework domain specic video summarization ieee winter conference applications computer vision wacv pages vishal kaushal rishabh iyer suraj kothawade rohan mahadev khoshrav doctor ganesh ramakrishnan learning data unied data subset selection active learning framework computer vision ieee winter conference applications computer vision wacv pages ieee vishal kaushal rishabh k iyer khoshrav doctor anurag sahoo p dubal s kothawade rohan mahadev kunal dargan ganesh ramakrishnan demystifying multi faceted video summarization tradeoff diversity representation coverage importance ieee winter conference applications computer vision wacv pages krishnateja killamsetty durga sivasubramanian ganesh ramakrishnan rishabh iyer glister generalization based data subset selection efcient robust learning arxiv preprint andreas krause daniel golovin submodular function maximization andreas krause ajit singh carlos guestrin near optimal sensor placements gaussian processes theory efcient algorithms empirical studies journal machine learning research alina kuznetsova hassan rom neil alldrin jasper uijlings ivan krasin jordi pont tuset shahab kamali stefan popov matteo malloci tom duerig al open images dataset unied image classication object detection visual relationship detection scale arxiv preprint yann lecun bernhard boser john s denker donnie henderson richard e howard wayne hubbard lawrence d jackel backpropagation applied handwritten zip code recognition neural computation chen li yang liu lin zhao improving update summarization supervised ilp sentence reranking proceedings conference north american chapter association computational linguistics human language technologies pages jingxuan li lei li tao li multi document summarization submodularity applied intelligence chin yew lin rouge package automatic evaluation summaries text summarization branches pages preprint march hui lin submodularity natural language processing algorithms applications phd thesis hui lin jeff bilmes class submodular functions document summarization proceedings annual meeting association computational linguistics human language technologies pages hui lin jeff bilmes learning mixtures submodular shells application document summarization arxiv preprint yuzong liu rishabh iyer katrin kirchhoff jeff bilmes svitchboard ii sver high quality complexity corpora conversational english speech sixteenth annual conference international speech communication association lszl lovsz submodular functions convexity mathematical programming state art pages springer baharan mirzasoleiman jeff bilmes jure leskovec coresets data efcient training machine learning models international conference machine learning pages pmlr george l nemhauser laurence wolsey marshall l fisher analysis approximations maximizing submodular set functions mathematical programming yunus emre ozkose bora celikkale erkut erdem aykut erdem diverse neural photo album summarization ninth international conference image processing theory tools applications ipta pages ieee joseph redmon ali farhadi incremental improvement arxiv preprint ozan sener silvio savarese active learning convolutional neural networks core set approach international conference learning representations burr settles active learning literature survey technical report university wisconsin madison department computer sciences aidean sharghi boqing gong mubarak shah query focused extractive video summarization european conference computer vision pages springer aidean sharghi jacob s laurel boqing gong query focused video summarization dataset evaluation memory network based approach proceedings ieee conference computer vision pattern recognition pages anurag singh lakshay virmani av subramanyam image representative summarization ieee fifth international conference multimedia big data bigmm pages ieee sebastian tschiatschek rishabh k iyer haochen wei jeff bilmes learning mixtures submodular functions image collection summarization advances neural information processing systems pages arun balajee vasudevan michael gygli anna volokitin luc van gool query adaptive video summarization quality aware relevance estimation proceedings acm international conference multimedia pages kai wei rishabh iyer jeff bilmes submodularity data subset selection active learning international conference machine learning pages pmlr shuwen xiao zhou zhao zijian zhang xiaohui yan min yang convolutional hierarchical attention network query focused video summarization aaai pages jin ge yao xiaojun wan jianguo xiao recent advances document summarization knowledge information systems zhen zhang raymond w yeung characterization entropy function information inequalities information theory ieee transactions bolei zhou agata lapedriza jianxiong xiao antonio torralba aude oliva learning deep features scene recognition places database advances neural information processing systems pages bolei zhou agata lapedriza aditya khosla aude oliva antonio torralba places million image database scene recognition ieee transactions pattern analysis machine intelligence preprint march appendix summary notations topic prism notation v v sa b sab e u explanation ground set n instances auxiliary set containing private set query set v v subset v cross similarity matrix items sets b similarity matrix items b parameter governing trade representation diversity gc parameter governing trade query relevance diversity mi cmi functions parameter governing hardness privacy constraints cg cmi functions initial set labeled instances set instances unlabeled data set t set instances target query set diversity function added mi function rithm weight private set conditioning set targeted summarization p t q t query set targeted summarization mixture model prism tsum parameters generalized hinge loss training example n ter w table summary notations paper targeted data subset selection prism tss targeted tion prism tsum appendix b proofs results section b properties generalized submodular mutual information functions restating lemma given restricted submodular function v b v b v b monotone v xed b v equivalently b monotone b v xed v proof non negativity generalized submodular mutual information follows denition particular v b v holds b b b f restricted submodular v prove monotonicity j b b b j given f restricted submodular v holds b j b b j follows submodularity inequality holds long sets subset v v e subsets non intersection v v monotone j b b preprint march b log determinant based information measures logdetmi logdetcg logdetcmi restating lemma setting f log q p sp ap sap log p q st q st p q ap q q st log q log log p st similarly p proof given positive semi denite matrix s log determinant function f log sa sub matrix comprising rows columns indexed following expressions follow directly denitions mi q log cmi log cg log note schur s complement sb st sab sab matrix includes cross similarities items sets b similarly sa b st ab result mutual information q q st log log q st q st aq aq cg p log log log log aq q st q st aq similarly proof conditional submodular mutual information follows simple observation p q q p p q log q p log plugging expressions mutual information log determinant function ap q st q sqp p st qp p sp q st p sp ap sap p q log q st q st log p q ap q ap sap ap q apq q st log proof cmi implicitly assumes simple way solve follows denote sap similarity matrix obtained multiplying cross similarity entries similarly denote sap q cross similarity obtained multiplying cross similarity p cross similarity q cmi function choice similarity matrix log p sp apsap q st q st p q ap q b facility location based information measures flcg theorem given similarity kernel s set u facility location fl written function iu maxja sij mutual information fl written q iu sij maxjq sij cg facility location written f iu sij maxjp sij expression conditional submodular mutual information iu sij maxjq sij maxjp sij similarly proof facility location set function u iu maxja sij s similarity kernel preprint march q q q max ja sij max jq sij max jaq sij iu iu iu max ja sij max jq sij ja sij max jq sij ja sij max jq sij ja sij max jp sij max jp sij max ja sij max jp sij iu iu conditional gain finally expression iinu iu ja sij max jq sij max jp sij jina sij max jinp sij jq sij max jp sij max max jaq sij max jp sij step follows observation c c c term rst term second term cancelling depending b obtain expression flcg flcmi special cases corollary setting u v expression mi cg cmi theorem obtain expression q iv sij maxjp sij flcmi iv sij maxjq sij flcg f iv sij maxjq sij maxjp sij corollary follows directly theorem similarly obtain expression restating lemma given similarity kernel s sij j j v j v facility location fl function maxja sij obtain expression mi q ia maxjq sij cg cmi expressions particularly useful case iq maxja sij proof assuming sii maximum similarity score kernel alternative formulation assumption u break sum elements ground set follows maxja sij minimum sets q term corresponding q similar argument follows terms q b ja sij max jq sij max jq sij max ja sij ja sij max jq sij ja sij max jq sij max jq sij max ja sij iv iaq ia iq follows q finally note ja sij max jq sij ja sij max jq sij iv v j sij similarly v q j q sij leaves q iq maxja sij ia maxjq sij expressions cg cmi nt sense fl require computing terms v access preprint march b concave modular gmi restating lemma function restricted submodular function v furthermore gmi f exactly q ia sij given kernel matrix satises sij j j v j v cg cmi expressions particularly useful case jq sij jq ia proof assume kernel matrix sij j given sij j j v j v notice holds s kernel identity kernel v v terms cross sets similarly iv ja sij n ia q iv jq sij n iq finally obtain f q q sij n sij n iv ja jq ia iq combining terms obtain f q q jq ia sij q ia jq sij finally restricted submodular notice submodular restricted v v similarly given sets v b v holds b b b implies restricted submodularity like fl expressions cg cmi nt sense com require computing terms v access b expressions query saturation function restating lemma function dened f submodular furthermore q ic ic v v restricted proof rst expand expression q note f ic f q ic ic v v q q q ic ic ic ic finally note set b v v holds b b b similarly sets v b v b b dened restricted submodular v similar manner obtain expressions conditional gain cg conditional gmi cgsmi query saturation function skip proof interest brevity b expression gccmi useful lemma graph cut function q words cmi function depend private set p preprint march proof deriving expression conditional submodular mutual information proceed follows let sij ia jp q q sij iaq jp q disjoint second term rst term nt effect p conditional submodular mutual information graph cut nt useful appendix c representational power prism section c experimental setup create synthetic dataset understand behaviour different functions prism corresponding control parameters generate different collections points space emulates space images queries private instances collection points representing images points representing queries points representing private instances points set distributed clusters different number points cluster standard deviation varied set query points private instances set randomly sampled replacement randomly selected clusters different functions prism different settings internal parameters maximize function produce summary compute relevant measures averaged different budgets intervals different collections scoring functions characterize query focused privacy preserving summaries dene following saturation phenomenon function nt gains picking query relevant items having picked query coverage calculated fraction query points covered summary measures summary nt starve query picking elements matching queries query point said covered summary exists selection summary belongs cluster query point quantify diversity summary calculating fraction unique clusters covered summary dene query relevance fraction points selected match query point dene privacy irrelevance fraction points selected match private instance figure comparison different functions prism effect parameters plots share legend c additional quantitative results fig left main paper presented behavior rst variant flmi change internal parameter fig present similar observations functions like logdetmi preprint march logdetmi figure effect effect different cg functions c fig right compare query coverage diversity query relevance gcmi flmi loddetmi com xing value applicable case compare version adds small diversity term functions measure effect saturation mi functions fig following observations gcmi logdetmi com favor query relevance diversity query coverage favors diversity query coverage query relevance furthermore observe com change addition diversity suggests saturated logdetmi signicantly changes behaviour addition diversity cases adding small diversity term reduces query relevance favor query coverage diversity expect report results privacy preserving summarization fig shows effect irrelevance term gccg flcg logdetcg expect increasing increases privacy irrelevance score ensuring stricter privacy irrelevance constraint fig compares diversity privacy irrelevance score different choices functions gc fl logdet xed value compare variants add small diversity unlike mi case cg functions saturate adding small diversity change selection finally trend log det outperforms fl gc terms diversity privacy irrelevance fig report results joint summarization comparison different functions flcmi flcmi div logdetcmi similar private query versions observe flcmi tends favor query coverage diversity contrast query relevance privacy irrelevance logdetcmi favors query relevance privacy irrelevance query coverage diversity c qualitative analysis fig visualization image points black query points green collection number synthetic dataset selected summary points blue selected labeled order selection f r d stand query coverage query relevance diversity privacy irrelevance respectively discussed soon increased summary produced query relevant diverse c preprint march figure behavior different functions prism effect parameters different fig add version adds small diversity term functions measure effect saturation mi functions text details figure visualization behavior varying collection number synthetic dataset appendix d details proofs related prism tss section d applying glister targeted subset selection subsection rst study application glister targeted data selection particular formulate glister killamsetty et al t min min recall given set loss examples use hypothesized labeles similar glister active killamsetty et al ia yi furthermore set consists unlabeled manner similar glister active apply targeted setting follows given current model parameters obtained training model labeled set apply step gradient approximation c preprint march obtain min t ia directly adapt theorem killamsetty et al obtain following lemma loss function l hinge loss logistic loss square loss perceptron loss eq written constrained submodular maximization problem means obtain solution simple greedy algorithm d glister tss special case prism tss certain cases restating lemma glister tss hinge loss logistic loss perceptron loss special case algorithm com proof proof follows directly appendix d killamsetty et al particular eq form jt l hinge loss perceptron loss similarly eq form jt c l logistic loss functions concave modular functions glister tss special case algorithm com gmi function d minimizing gradient difference target special case prism tss restating lemma minimizing gradient difference eq rewritten special case algorithm t j j gcmi ia jt e lu e lt diversity function proof prove result expand gradient difference expression e note minimizing gradient difference h dene immediately rst term independent constant similarly term instance gcmi expand second term e ia ja expanding minimizing gradient difference rewritten maximizing sum gcmi diversity term appendix e learning parameterized submodular information measures section present specic forms mixture model objective function computation gradients different cases generic query focused privacy preserving joint summarization e generic summarization denote dataset n training examples y n v n n n y human summary nth ground set image collection v n features denote mixture model case generic summarization f y w m preprint march fm instantiations different submodular functions wi weights m internal parameters respectively example case graph cut function dened parameters vector case generic summarization wm m max y v m m purpose learning parameters wi compute gradients wi fi yn n wi fi yn wi yn argmax f y w y v f y jy ij gradients respect respective internal parameters individual function components fi generalized graphcut y ij example compute gradient iv jy ij jy consider e query focused summarization denote dataset n training examples y n v n n n y human query summary query nth ground set image collection v n features denote mixture model case query summarization f y w m fm instantiations different submodular mutual information functions wi weights m internal parameters respectively m query relevance diversity tradeoff parameters parameters vector case query focused summarization wm m m max y v m wiifi y n m purpose learning parameters wi compute gradients wi ifi yn ifi y n wi ifi yn wi n wi ifi yn wi n yn argmax f y w y v preprint march gradients individual function components trade parameters computation functions follows y ij ij iv ifi respect respective query relevance diversity y iv max ij ij maxjy ij y maxjy ij iy ij y iy max ij logdetmi y log y sy st y log x y sy st y y sy y y sy y x y e privacy preserving summarization denote dataset n training examples y n v n p n n n y human privacy summary privacy set p n nth ground set image collection v n features denote mixture model case privacy preserving summarization f y p n w p n m fm instantiations different conditional gain functions wi weights m internal parameters respectively privacy sensitivity parameters parameters vector case privacy preserving summarization wm m m max y v m m p n p n purpose learning parameters wi compute gradients wi fi yn p n n p n wi fi yn p n wi p n wi fi yn p n wi p n yn argmax f y p n w y v preprint march gradients individual function components fi computation functions follows flcondgain y p n iv ij maxjp n ij respect respective privacy sensitivity parameters f y p n iv max jp ij ij max ij logdetcondgain y p n log p p y p n log x x y sy p p y p f y p n p p y p p p n st y p n e joint summarization denote dataset n training examples y n v n p n n n y human query privacy summary query set privacy set p n nth ground set image collection v n features denote mixture model case joint query focused privacy preserving summarization f y p n w p n fm instantiations different conditional submodular mutual information functions wi weights m internal parameters respectively query relevance vs diversity trade parameters privacy sensitivity parameters parameters vector case joint summarization wm m m m max y v m m p n p n purpose learning parameters compute gradients wi fi yn p n n p n wi fi yn p n wi p n wi fi yn p n wi p n wi fi yn p n wi p n yn argmax f y p n w y v m preprint march appendix f prism tsum generalizes past work section section discuss past works unknowingly fact instances mi functions gcmi query focused summarization works document summarization lin li et al video summarization vasudevan et al use gcmi papers study simple graph cut based query relevance term special case submodular mutual information framework single query point ia gcmi seamlessly extends consider query set gccg gccmi graph cut conditional gain function update summarization li et al table paper furthermore authors consider query focused update summarization case use gccmi expression existing summary goal select summary relevant query q different authors study graph cut query based summarization cases observe utility class functions logdetmi query focused summarization model sharghi et al similar logdetmi consider sequential dpp model structure particular assume sa sq e elements v v independent q log saqst aq similar query term sharghi et al e equation paper w shows logdetmi model makes sense query focused summarization com lin bilmes propose combination query relevance diversity term document summarization expression propose similar com ignore diversity term achieved state art results query focused document summarization rouge rouge common evaluation metric document summarization lin lin bilmes shown lin bilmes rouge metric actually submodular actually observe rouge fact exactly query saturation q sat function subsumed framework gmi framework signicantly extends provides rich class functions query focused preserving irrelevance update summarization appendix g additional details experimental setup discussion results prism tss experiments section classes scarce classes mnist train class valid class target total lake class train class valid class target total lake class table number datapoints partition dataset provide details experimental setup targeted subset selection datasets simulate real world scenario creating class imbalance classes training unlabeled dataset creating ratio classes scarce classes e classes times datapoints classes particular training set classes examples class examples class mnist scarce classes examples class examples class mnist mnist lake unlabeled set contains examples class classes examples class scarce evident goal able nd good representation scarce slices case slices obtain good results slices case scarcity classes slice data need correlated class mi functions baselines use class information use validation set examples class pick small targeted set consisting performing slices case observe slices highest error corresponds data scarce classes pick examples total target set examples total target set mnist pick target set mis classied examples average results multiple settings scarce classes e datapoints randomly selected validation set small equal number datapoints class discuss exact number datapoints table hyperparamters training dataset preprint march optimization algorithm sgd momentum learning rate cosine annealing momentum weight decay number epochs mnist got numbers taking stopping condition training accuracy training loss figure comparison different methods targeted subset selection different budgets mnist x axis budgets y axis gain model accuracy target classes mi based approaches lines red signicantly outperform subset sizes section report better resolution image presenting effect budget size performance methods fig following additional observations results pure retrieval function gcmi works better pure diversity function dsum expected task hand e targeted subset selection relevance target plays important role accordingly tends model diversity query relevance performs worse gcmi appears counter intuitive targeted version tss performs better glister cifar expected worse glister mnist think case glister tss depends heavily target set optimizing performance tends overt target instances case mnist vs case contrast mi functions work target set small note glister tss setting special case algorithm cross entropy loss appendix h additional details prism tsum experiments use image collection dataset tschiatschek et al dataset image collections images provides human summaries collection extend creating dense noun concept annotations image suitable task start designing universe concepts based object classes kuznetsova et al scenes zhou et al eliminate concepts common example closet unied list concepts ease annotation process adopt pseudo labelling followed human correction specically image concept labels model pre trained object concepts model pre trained scene concepts ask human annotators separately individually correct automatically generated labels pseudo labels followed nding consensus set concepts image preprint march arrive nal annotation concept vectors image developed python gui tool ease pseudo label correction process plan release addition available generic human summaries augment dataset query focused preserving joint query focused privacy preserving summaries image collection specically design uni concept bi concept queries private sets image collection cover different cases like concepts belonging image b concepts belonging different images concept image collection similar spirit sharghi et al ask group human annotators different annotated concepts create human summary images image collection query private pair ensure gold standard summaries followed verication round specically asked annotators accept reject summaries produced discarded human summaries rejected human annotators instantiate prism tsum represent images probabilistic feature vector taken output layer model redmon farhadi pre trained open images dataset kuznetsova et al concatenate probability vector scenes output layer zhou et al trained dataset zhou et al queries sets concepts mapped similar feature space k hot vectors number concepts query facilitate image query similarity images queries elements private set represented vector c universe concepts complex queries methods learning joint embedding text images employed chose simpler alternatives stick main focus area work initialize parameters randomly train mixture model epochs tschiatschek et al use v rouge max margin learning discussed section update parameters nesterov s accelerated gradient descent
