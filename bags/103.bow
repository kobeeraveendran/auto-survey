l u j v c s c v v i x r a query focused video summarization dataset evaluation and a memory network based approach aidean sharghi jacob laurel and boqing gong center for research in computer vision university of central florida orlando fl department of computer science university of alabama at birmingham al abstract recent years have witnessed a resurgence of interest in video summarization however one of the main obstacles to the research on video summarization is the user subjectivity users have various preferences over the summaries the subjectiveness causes at least two problems first no single video summarizer ts all users unless it interacts with and adapts to the individual users second it is very challenging to evaluate the performance of a video summarizer to tackle the rst problem we explore the recently posed query focused video summarization which introduces user preferences in the form of text queries about the video into the summarization process we propose a memory work parameterized sequential determinantal point process in order to attend the user query onto different video frames and shots to address the second challenge we contend that a good evaluation metric for video summarization should focus on the semantic information that humans can perceive rather than the visual features or temporal overlaps to this end we collect dense per video shot concept tions compile a new dataset and suggest an efcient uation method dened upon the concept annotations we conduct extensive experiments contrasting our video marizer to existing ones and present detailed analyses about the dataset and the new evaluation method introduction recent years have witnessed a resurgence of interest in video summarization probably due to the overwhelming video volumes showing up in our daily life indeed both consumers and professionals have the access to ubiquitous video acquisition devices nowadays while the video data is a great asset for information extraction and knowledge discovery due to its size and variability it is extremely hard for users to monitor or nd the occurrences in it jacob laurel contributed to this work while he was an nsf reu student at ucf thanks to the support of nsf cns intelligent video summarization algorithms allow us to quickly browse a lengthy video by capturing the essence and removing redundant information early video rization methods were built mainly upon basic visual ities low level appearance and motion features while recently more abstract and higher level cues are leveraged in the summarization works however one of the main obstacles to the research on video summarization is the user subjectivity users have various preferences over the summaries they would like to watch the subjectiveness causes at least two problems first no single video summarizer ts all users unless it teracts with and adapts to the users second it is very lenging to evaluate the performance of a video summarizer in an attempt to solve the rst problem we have ied a new video summarization mechanism query focused video summarization that introduces user preferences in the form of text queries about the video into the rization process while this may be a promising direction to personalize video summarizers the experimental study in was conducted on the datasets originally collected for the conventional generic video summarization it remains unclear whether the real users would generate tinct summaries for different queries and if yes how much the query focused summaries differ from each other in this paper we explore more thoroughly the focused video summarization and build a new dataset ticularly designed for it while we collect the user tions we meet the challenge how to dene a good tion metric to contrast system generated summaries to user labeled ones the second problem above mentioned due to the user subjectivity about the video summaries we contend that the pursuit of new algorithms for video summarization has actually left one of the basic problems underexplored how to benchmark different video marizers user study is too time consuming to compare different approaches and their variations at large in the prior arts of automating the evaluation scale figure comparing the semantic information captured by captions in and by the concept tags we collected cedure on one end a system generated summary has to consist of exactly the same key units frame or shot as in the user summaries in order to be counted as a good one on the other end pixels and low level features are used to compare the system and user maries whereas it is unclear what features and distance metrics match users criteria some works strive to nd a balance between the two extremes using the temporal overlap between two summaries to dene the evaluation metrics however all such metrics are derived from either the temporal or visual representations of the videos without explicitly encoding how humans perceive the information after all the tem generated summaries are meant to deliver similar mation to the users as those directly labeled by the users in terms of dening a better measure that closely tracks what humans can perceive from the video summaries we share the same opinion as yeung et al it is key to evaluate how well a system summary is able to retain the semantic information as opposed to the visual quantities of the user supplied video summaries arguably the semantic information is best expressed by the concepts that represent the fundamental characteristics of what we see in the video at multiple grains with the focus on different areas and from a variety of perspectives objects places people actions and their ner grained entities therefore as our rst contribution we collect dense video shot concept annotations for our dataset in other words we represent the semantic information in each video shot by a binary semantic vector in which the indicate the presence of corresponding concepts in the shot we suggest a new evaluation metric for the query focused and generic video summarization based on these semantic tor representations of the video in addition we propose a memory network terized sequential determinantal point process for ling the query focused video summarization unlike the hierarchical model in our approach does not rely on the costly user supervision about which queried concept pears in which video shot or any pre trained concept tectors instead we use the memory network to implicitly attend the user query about the video onto different frames within each shot extensive experiments verify the tiveness of our approach the rest of the paper is organized as follows we discuss some related works in section section elaborates the process of compiling the dataset acquiring annotations as well as a new evaluation metric for video summarization in section we describe our novel query focused rization model followed by detailed experimental setup and quantitative results in sections related work we discuss some related works in this section this work extends our previous efforts on alizing video summarizers both works explore the the dataset and the code of the new evaluation metric are publicly available at caption i walked around my bedroomdense tags chaircomputerroomdeskofficecaption i drove the car in trafficdense tags skystreetbuildinghandscartreewindowcaption i waited in line with my frienddense tags ladyfoodmendrinkhandshatcomputermarketbuildingdeskcaption i looked at my phonedense tags facecomputermenphonehandschairroomdeskhall figure the frequencies of concepts showing up in the video shots counted for each video separately focused video summarization but we study this problem more thoroughly in this paper through a new dataset with dense per video shot tagging of concepts our memory work based video summarizer requires less supervision for training than the hierarchical model in unlike our user annotated semantic vectors for the video shots yeung et al asked annotators to caption each video shot using a sentence a single sentence targets only limited information in a video shot and misses many details figure contrasts the concept annotations in our dataset with the captions for a few video shots the concept notations clearly provide a more comprehensive coverage about the semantic information in the shots memory networks are versatile in modeling the attention scheme in neural networks they are widely used to address question answering and visual question answering the query focusing in our marization task is analogous to attending questions to the facts in the previous works but the facts in our context are temporal video sequences moreover we lay a tial determinantal point process on top of the memory network in order to promote diversity in the summaries a determinantal point process dpp denes a tribution over the power sets of a ground set that ages diversity among items of the subsets there have been growing interest in dpp in machine learning and computer vision our model in this paper extends dpps modeling capabilities through the memory neural network dataset in this section we provide the details on compiling a comprehensive dataset for video summarization we opt to build upon the currently existing ut egocentric ute dataset mainly for two reasons the videos are sumer grade captured in uncontrolled everyday scenarios and each video is hours long and contains a diverse set of events making video summarization a naturally sirable yet challenging task in what follows we rst plain how we dene a dictionary of concepts and determine the best queries over all possibilities for the query focused video summarization then we describe the procedure of gathering user summaries for the queries we also show informative statistics about the collected dataset concept dictionary and queries we plan to have annotators to transform the semantic formation in each video shot to a binary semantic vector cf figures and with s indicating the presence of the responding concepts and s the absence such annotations serve as the foundation for an efcient and automatic uation method for video summarization described in tion the key is thus to have a dictionary that covers a wide range and multiple levels of concepts in order to have the right basis to encode the semantic information in we have constructed a lexicon of concepts by overlapping nouns in the video shot captions with those in the sentibank those nouns serve as a great starting point for us since they are mostly entry level words we prune out the concepts that are weakly related to visual content area which could be interpreted in ous ways and applicable to most situations additionally we merge the redundant concepts such as children and kids we also add some new concepts in order to struct an expressive and comprehensive dictionary two strategies are employed to nd the new concept candidates first after watching the videos we manually add the cepts that appear for a signicant frequency puter second we use the publicly available statistics about youtube and vine search terms to add the terms that are frequently searched by users pet animal the nal lexicon is a concise and diverse set of concepts cf figure that are deemed to be comprehensive for the ute videos of daily lives figure all annotators agree with each other on the prominent concepts in the video shot while they miss different subtle concepts figure two summaries generated by the same user for the queries hat phone and food drink respectively the shots in the two summaries beside the green bars exactly match each others while the orange bars show the query specic shots we also construct queries to acquire query focused user summaries using two or three concepts as opposed to gletons imagine a use case of video search engines the queries entered by users are often more than one word for each video we formalize queries they cover the ing four distinct scenarios i all the concepts in the query appear in the same video shots together such queries all concepts appear in the video but never jointly in a single shot queries only one of the concepts stituting the query appears in some shots of the video queries and none of the concepts in the query are present in the video such query we describe in the suppl materials how we obtain the queries to cover the four scenarios such queries and their user annotated maries challenge an intelligent video summarizer from ferent aspects and extents collecting user annotations we plan to build a video summarization dataset that fers efcient and automatic evaluation metrics and user summaries in response to different queries about the videos for the former we collect user annotations about the presence absence of concepts in each video shot this is a quite daunting task conditioning on the lengths of the videos and the size of our concept dictionary we use zon mechanical turk mturk for economy and efciency considerations for the latter we hire three student volunteers to have better quality control over the labeled video summaries we formly partition the videos to second long shots shot tagging visual content to semantic vector we ask mturkers to tag each video shot with all the cepts that are present in it to save the workers time from watching the shots we uniformly extract ve frames from each shot a concept is assumed relevant to the shot as long as it is found in any of the ve frames figure trates the tagging results for the same shot by three ent workers while all the workers captured the prominent concepts like sky lady street tree and car they missed different subtle ones the union of all their tations however provides a more comprehensive tic description about the video shot than that of any vidual annotator hence we ask three workers to annotate each shot and take their union to obtain the nal semantic vector for the shot on average we have acquired and concepts per shot for the four ute in sharp contrast the automatically videos respectively derived concepts from the shot captions are far from enough on average there are only and concepts respectively associated with each shot of the four videos evaluating video summaries thanks to the dense cept annotations per video shot we can conveniently trast a system generated video summary to user summaries according to the semantic information they entail we rst dene a similarity function between any two video shots by intersection over union iou of their ing concepts for instance if one shot is tagged by car street and another by street tree sign then the iou similarity between them is to nd the match between two summaries it is nient to execute it by the maximum weight matching of a bipartite graph where the summaries are on opposite sides of the graph the number of matched pairs thus enables us to compute precision recall and score although this procedure has been used in the previous work there the edge weights are calculated by low level visual features which by no means match the semantic information humans obtain from the videos in sharp contrast we use the iou user sky lady street market building hands tree car windowuser sky lady street hands tree car hat auser sky lady street tree car hat windowfooddrinkphonehat table inter user agreement evaluated by score and the three student volunteers o the oracle summary table the average lengths and standard deviations of the maries for different queries o o o user user user oracle similarities dened directly over the user annotated tic vectors as the edge weights acquiring user summaries in addition to the dense per video shot concept tagging we also ask annotators to label query focused video summaries for the queries described in section to ensure consistency in the summaries and better ity control over the summarization process we switch from mturk to three student volunteers in our university we meet and train the volunteers in person they each marize all four videos by taking queries into account an annotator receives videos queries summarization tasks in total we thus obtain three user summaries for each query video pair however we acknowledge that it is infeasible to have the annotators to summarize all the query video pairs from scratch the ute videos are each hours long to overcome this issue we expand each temporal video to a set of static key frames first we uniformly extract ve key frames to represent each shot in the same way as in tion second we pool all the shots corresponding to the three textual summaries as the initial candidate set third for each query we further include all the shots that are relevant to it into the set a shot is relevant to the query if the intersection of the concepts associated with it and the query is nonempty as a result we have a set of candidate shots for each query that covers the main story in the video as well as those of relevance to the query the annotators summarize the video by removing redundant shots from the set there are to shots in the candidate sets and the summaries labeled by the participants contain only shots on average oracle summaries supervised video summarization methods often learn from one summary per video or per query video pair in query focused rization while we have three user generated summaries per query we aggregate them into one called the oracle mary per query video pair by a greedy algorithm the gorithm starts from the common shots in the three user maries it then greedily chooses one shot every time such that this shot gives rise to the largest marginal gain over the evaluated score we leave the details to the suppl rials the oracle summaries achieve better agreements with the users than the inter user consensus cf table summaries of the same video differ due to queries figure shows two summaries labeled by the same user for two distinct queries hat phone and food drink note that the summaries both track the main events pening in the video while they differ in the query specic parts besides table reports the means and standard tions of the lengths of the summaries per video per user we can see that the queries highly inuence the resulting maries the large standard deviations attribute to the queries budgeted summary for all the summaries thus far we do not impose any constraints over the total number of shots to be included into the summaries after we receive the notations however we let the same participants further duce the lengths of their summaries to respectively shots and shots we call them budgeted summaries and leave them for future research approach we elaborate our approach to the query focused video summarization in this section denote by v a video that is partitioned to t segments and by q the query about the video in our experiments every segment vt sists of video shots each of which is second long and is used in section to collect the concept annotations query conditioned sequential dpp the sequential determinantal point process dpp is among the state of the art models for generic video rization we condition it on the query q as our overarching video summarization model p yt yt q t p q p yt q where the t th dpp variable yt selects subsets from the segment vt vt and the distribution p yt q is specied by a conditional dpp p yt q det it the nominator on the right hand side is the principle minor of the l ensemble kernel matrix indexed by the sets yt the denominator calculates the determinant figure our query focused video summarizer memory network right parameterized sequential determinantal point process left of the sum of the kernel matrix and a corrupted identity trix whose elements indexed by are s readers are referred to the great tutorial on dpp for more details note that the dpp kernel is parameterized by the query we have to carefully devise the way of terizing it in order to take account of the following ties in query focused video summarization a user selects a shot to the summary for two possible reasons one is that the shot is quite related to the query and thus becomes pealing to the user the other may attribute to the tual importance of the shot the user would probably choose a shot to represent a prominent event in the video even if the event is not quite relevant to the query to this end we use a memory network to model the two types of importance query related and contextual of a video shot simultaneously memory network to parameterize dpp kernels the memory network offers a neural network tecture to naturally attend a question to facts cf the most panel of figure in our work we shall measure the relevance between the query q and a video shot and rate such information into the dpp kernel therefore it is straightforward to substitute the question in memory network by our query but the facts are less obvious as discussed in section there could be various narios for a query and a shot all the query concepts may appear in the shot but possibly in different frames one or two concepts of the query may not be present in the shot it is also possible that none of the concepts are relevant to any frame in the shot in other words the memory network is supposed to screen all the video frames in order to mine the shot s relevance to the query hence we uniformly sample frames from each shot as the facts the video frames are represented using the same feature as cf fk on the rightmost panel of figure the memory network takes as input the video frames fk of a shot and a query the frames are transformed to memory vectors mk through an embedding matrix similarly the query q represented by a binary indication vector is mapped to the internal state u using an embedding matrix the attention scheme is implemented simply by a dot product followed by a softmax function pk mk where pk carries how much attention the query q incurred over the frame fk equipped with the attention scores pk we assemble another embedding ck of the frames obtained by the mapping matrix b in gure into the video shot sentation o o pick which is conditioned on the query q and entails the vance strength of the shot to the query as a result we pect the dpp kernel parameterized by the following ot i dt doj is also exible in modeling the importance of the shots to be selected into the video summary here i and j index two shots and d is another embedding matrix note that the contextual importance of a shot can be inferred from a shot s similarities to the others by the kernel matrix while the query related importance is mainly by the attention scheme in the memory network learning and inference we learn the overall video summarizer including the quential dpp and the memory network by maximizing the log likelihood of the user summaries in the training set we use stochastic gradient descent with mini batching to mize the embedding matrices a b c d the learning rates and numbers of epochs are chosen using the tion set at the test stage we sequentially visit the video segments vt and select shots from them using the learned summarization model it is notable that our approach requires less user tions than the sh dpp it learns directly from the user summaries and implicitly attend the queries to the video shots however sh dpp requires very costly annotations about the relevances between video shots and queries our new dataset does supply such supervisions so we shall clude sh dpp as the baseline method in our experiments experimental results we report experimental setup and results in this section features we extract the same type of features as used in the existing sh dpp method in order to have fair features of shot networkfeatures of shot nmemory networkfeatures of shot imemory networkfeatures of shot q q conditional bembedding aqueryembedding cinner table comparison results for query focused video summarization seqdpp sh dpp ours precision recall precision recall precision recall avg comparisons first we employ concept detectors from sentibank and use the detection scores for the features of each key frame key frames per second long shot however it is worth mentioning that our approach is not limited to using concept detection scores and more tantly unlike sh dpp does not rely on the per shot tions about the relevance to the query the shot user labeled semantic vectors serve for evaluation purpose only additionally we extract a six dimensional contextual ture vector per shot as the mean correlations of low level features including color histogram gist lbp bag of words as well as an attribute feature in a poral window whose size varies from to shots the six dimensional contextual features are appended to the key frame features in our experiments data split we run four rounds of experiments each ing one video out for testing and one for validation while keeping the remaining two for training since our video summarizer and the baselines are sequential models the small number two of training videos is not an issue as the videos are extremely long providing many variations and supervisions at the training stage comparison results query focused video summarization we contrast our video summarizer the memory network based sequential determinantal point process to several closely related ods we rst include sh dpp the most recent proach to the query focused video summarization our model improves upon seqdpp by taking the query into account and parameterizing the dpp kernel by the memory network seqdpp is thus directly comparable to ours we concatenate the query features binary indication vectors with the shot features and input them to seqdpp and dpp we set the same dimensionality for all the embedding spaces in our and the two baseline methods it turns out the embeddings are chosen due to their performances on the validation videos table compares the performances of the three video summarizers each video is taken in turn as the test video and the corresponding results are shown in each row the average results are included as the last row precision figure the effectiveness of various individual components in our proposed video summarizer call and score are reported for all the video rizers our approach outperforms the other two by a large margin more than score on average it seems like video is especially challenging for all the methods for video our summarizer generates a little longer summaries than the others do in the future work we will explore how to control the summary length in the sequential dpp model component wise analyses to investigate how each component in our framework contributes to the nal results we conduct more experiments by either removing or fying them figure shows the corresponding results the main benet from the memory network is the tion mechanism cf equation if we instead use a form distribution for the attention scores pi and append the query information u directly after the memory network output o the results become worse on all the four videos cf noattention in figure the noembd results are tained after we remove the last embedding matrix d when we compute the dpp kernels finally embsize are the results when we change the embeddings in our approach to the performance drops from our plete model verify that all the corresponding components are complementary jointly contributing to the nal results generic video summarization recall that our queries incur four different scenarios cf section when there are no video shots relevant to the query it reduces to the generic video summarization in some sort we single out such queries and contrast our summarizer to some ing and recent methods for generic video summarization scoresnoattentionnoembdembsize complete model table comparison results for generic video summarization when no video shots are relevant to the query submod quasi ours precision recall precision recall precision recall avg figure a nice behavior of our evaluation metric when we randomly remove video shots from the user summaries the recall between the original user summaries and the corrupted ones decreases almost linearly the evaluation by rouge is included for reference submod which employs submodular functions to courage diversity and quasi which is an unsupervised method based on group sparse coding unlike the dpp type of summarizers the baseline methods here are not able to automatically determine the lengths of the summaries we tune the threshold parameter in quasi such that the output lengths are no more or less than the oracle summary by shots for submod we set the budget parameter such that it generates summaries that are exactly as long as the oracle summaries as shown in table our approach still gives the best overall performance even though we reveal the acle sumamries lengths to the baseline methods probably due to its higher neural network based modeling capacity a nice behavior of our evaluation metric our evaluation method for video summarization is mainly motivated by yeung et al particularly we share the same opinion that the evaluation should focus on the semantic information which humans can perceive rather than the low level visual features or temporal laps however the captions used in are diverse making the rouge evaluation unstable and poorly correlated with human judgments and often missing subtle details cf figure for some examples we rectify those caveats by instead collecting dense cept annotations figure exhibits a few video shots where the concepts we collected provide a better coverage than the captions about the semantics in the shots moreover we conveniently dene an evaluation metric based on the iou similarity function between any two shots section thanks to the concept annotations our evaluation metric has some nice behaviors if we randomly remove some video shots from the user maries and compare the corrupted summaries with the nal ones an accuracy like metric should give rise to linearly decreasing values this is indeed what happens to our recall as shown in figure in contrast the rouge recall taking as input the shot captions exhibits some ality more results on randomly replacing some shots in the user summaries are included in the suppl materials conclusion in this work our central theme is to study the tiveness in video summarization we have analyzed the key challenges caused the subjectiveness and proposed some lutions in particular we compiled a dataset that is densely annotated with a comprehensive set of concepts and signed a novel evaluation metric that benets from the lected annotations we also devised a new approach to erating personalized summaries by taking user queries into account we employed memory networks and tal point processes in our summarizer so that our model leverages their attention schemes and diversity modeling pabilities respectively extensive experiments verify the fectiveness of our approach and reveals some nice behaviors of our evaluation metric acknowledgements this work is supported by nsf iis a gift from adobe systems and a gpu from nvidia we thank fei sha the anonymous reviewers and area chairs especially for their insightful suggestions over unionrouge over unionrouge scoreintersection over unionrouge references affandi fox adams and taskar ing the parameters of determinantal point process kernels in icml pages agarwal choromanska and choromanski notes on using determinantal point processes for arxiv preprint ing with applications to text clustering antol agrawal lu mitchell batra lawrence zitnick and parikh vqa visual question answering in proceedings of the ieee international ference on computer vision pages bahdanau cho and bengio neural machine translation by jointly learning to align and translate arxiv preprint batmanghelich quon kulesza kellis golland and bornn diversifying sparsity using variational determinantal point processes arxiv preprint borth chen ji and chang sentibank large scale ontology and classiers for detecting sentiment and emotions in visual content in proceedings of the acm international conference on multimedia pages acm chao gong grauman and sha margin determinantal point processes uai chen fang lin vedantam gupta dollar and zitnick microsoft coco captions arxiv preprint data collection and evaluation server chu song and video jaimes video summarization by visual summarization occurrence in proceedings of the ieee conference on puter vision and pattern recognition pages de avila lopes da luz and de querque araujo vsumm a mechanism designed to produce static video summaries and a novel evaluation method tern recognition letters gartrell paquet and koenigstein low rank torization of determinantal point processes for tion arxiv preprint gillenwater kulesza fox and taskar expectation maximization for learning determinantal point in advances in neural information processing processes systems pages goldman curless salesin and seitz schematic storyboarding for video visualization and editing in acm transactions on graphics tog volume pages acm gong chao grauman and sha diverse sequential subset selection for supervised video tion in advances in neural information processing systems pages gygli grabner riemenschneider and van gool creating summaries from user videos in european ence on computer vision pages springer gygli grabner and van gool video tion by learning submodular mixtures of objectives in ceedings of the ieee conference on computer vision and pattern recognition pages khosla hamid lin and sundaresan scale video summarization using web image priors in ceedings of the ieee conference on computer vision and pattern recognition pages kim sigal and xing joint summarization of large scale collections of web images and videos for line reconstruction in proceedings of the ieee conference on computer vision and pattern recognition pages kulesza and taskar k dpps fixed size in proceedings of the nantal point processes national conference on machine learning icml pages kulesza and taskar learning determinantal point cesses kulesza and taskar determinantal point processes for machine learning arxiv preprint kwok and adams priors for diversity in generative latent variable models in advances in neural information processing systems pages kwon and lee a unied framework for event marization and rare event detection in cvpr pages bacco hocevar lambert pas and ionescu video summarization from spatio temporal in proceedings of the acm trecvid video features summarization workshop pages acm lee ghosh and grauman discovering important people and objects for egocentric video summarization in cvpr volume page lee and grauman predicting important objects for international journal of egocentric video summarization computer vision li jegelka and sra fast dpp sampling for arxiv preprint om with application to kernel methods liu and kender optimization algorithms for the selection of key frame sequences of variable length in european conference on computer vision pages springer lu and grauman story driven summarization for in proceedings of the ieee conference egocentric video on computer vision and pattern recognition pages mariet and sra fixed point algorithms for learning terminantal point processes advances in neural information systems nips mariet and sra kronecker determinantal point cesses arxiv preprint ojala pietikainen and maenpaa multiresolution gray scale and rotation invariant texture classication with local binary patterns ieee transactions on pattern analysis and machine intelligence recognition in proceedings of the ieee conference on puter vision and pattern recognition pages zhang chao sha and grauman summary transfer exemplar based subset selection for video rizatio arxiv preprint zhang chao sha and grauman video summarization with long short term memory arxiv preprint zhao and xing quasi real time summarization for in proceedings of the ieee conference consumer videos on computer vision and pattern recognition pages oliva and torralba modeling the shape of the scene a holistic representation of the spatial envelope international journal of computer vision ordonez deng choi berg and berg from large scale image categorization to entry level gories in proceedings of the ieee international conference on computer vision pages potapov douze harchaoui and schmid category specic video summarization in eccv ropean conference on computer vision rav acha pritch and peleg making a long video in ieee computer short dynamic video synopsis society conference on computer vision and pattern nition volume pages ieee sharghi gong and shah query focused extractive video summarization in european conference on computer vision pages springer snoek zemel and adams a determinantal point process latent variable model for inhibition in neural spiking data in advances in neural information processing systems pages song vallmitjana stent and jaimes tvsum summarizing web videos using titles in proceedings of the ieee conference on computer vision and pattern tion pages sukhbaatar weston fergus al end to end ory networks in advances in neural information processing systems pages weston bordes chopra rush van merrienboer joulin and mikolov towards complete question answering a set of prerequisite toy tasks arxiv preprint weston chopra and bordes memory networks arxiv preprint wolf key frame selection by motion analysis in tics speech and signal processing ference proceedings ieee international conference on volume pages ieee xiong and grauman detecting snap points in tric video with a web photo prior in european conference on computer vision pages springer xiong merity and socher dynamic memory networks for visual and textual question answering arxiv preprint xu mukherjee li warner rehg and singh gaze enabled egocentric video summarization via constrained submodular maximization in proceedings of the ieee conference on computer vision and pattern tion pages yao mei and rui highlight detection with pairwise deep ranking for rst person video summarization yeung fathi and fei fei videoset video summary evaluation through text arxiv preprint yu cao feris smith and chang designing category level attributes for discriminative visual
