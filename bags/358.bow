metrics also disagree in the low scoring range revisiting summarization evaluation metrics manik bhandari pranav gour atabak ashfaq pengfei liu carnegie mellon university mbhandar pgour aashfaq cmu edu v o n l c s c v v i x r a abstract in text summarization evaluating the efcacy of automatic metrics without human judgments has become recently popular one exemplar work peyrard concludes that automatic metrics strongly disagree when ranking high scoring summaries in this paper we revisit their experiments and nd that their observations stem from the fact that metrics disagree in ranking summaries from any narrow scoring range we hypothesize that this may be because maries are similar to each other in a narrow scoring range and are thus difcult to rank apart from the width of the scoring range of summaries we analyze three other properties that impact inter metric agreement ease of summarization abstractiveness and coverage to encourage reproducible research we make all our analysis code and data publicly available introduction automatic metrics play a signicant role in summarization evaluation profoundly affecting the direction of system optimization due to its importance evaluating the quality of evaluation metrics also known as meta evaluation has been a crucial step generally there are two meta evaluation strategies i assessing how well each metric correlates with human judgments lin ng and abrecht louis and nenkova peyrard et al bhandari et al which requires procuring manual annotations that are expensive and time consuming and measuring the correlation between different metrics peyrard which is a human judgment free method in this work we focus on the latter and ask two research questions how do automated metrics correlate when ranking summaries in different scoring ranges low average and high we revisit the experiments of peyrard which concludes that automated rics strongly disagree for ranking high scoring summaries we nd that the scoring range has little effect on the correlation of metrics it is rather the width of the scoring range which affects inter metric correlation specically we observe that metrics agree in ranking summaries from the full scoring range but disagree in ranking summaries from low average and high scoring ranges when taken separately which other factors affect the correlations of metrics in addition to the width of the scoring range we analyze three properties of a reference summary on inter metric correlation ease of rization abstractiveness and coverage overall we nd that for highly extractive document reference summary pairs inter metric correlation is high whereas metrics disagree when ranking summaries of abstractive document reference summary pairs we summarize our contributions as follows we extend the analysis of peyrard and nd that not only do metrics disagree in the high scoring range they also disagree in the low and medium scoring we perform our analysis on the popular cnn dailymail dataset using traditional lexical range com manikbhandari revisitsummevalmetrics uses three experiments to reach their conclusion due to limitations of space we focus on the rst one here please see the appendix for a detailed analysis of the other two experiments this work is licensed under a creative commons attribution international license license details creativecommons org licenses matching metrics like rouge as well as recently popular semantic matching metrics like bertscore and moverscore apart from the width of the scoring range we analyze three linguistic properties of reference summaries which affect inter metric correlations preliminaries datasets dang and owczarzak dang and owczarzak are multi document multi reference summarization datasets used during the shared tasks ing peyrard we combine the two and refer to the joined dataset as tac cnn dailymail cnndm hermann et al is a commonly used summarization dataset modied by nallapati et al which contains news articles and associated highlights as summaries we use the non anonymized version evaluation metrics we examine six metrics that measure the semantic equivalence between two texts in our case between the system generated summary and the reference summary bertscore bscore measures soft overlap between contextual bert embeddings of tokens between the two zhang et al score ms applies a distance measure to contextualized bert and elmo word zhao et al js divergence measures jensen shannon divergence between the two text s bigram lin et al and measure the overlap of unigrams and bigrams lin rouge l measures the overlap of the longest common quence between two texts lin we use the recall variant of all metrics except moverscore which has no specic recall variant correlation measure kendall s is a measure of the rank correlation between any two measured quantities in our case scores given by evaluation metrics and is popular in meta evaluating metrics at the summary level peyrard we use the implementation given by virtanen et al summary generation to simulate the full scoring range of summaries that are possible for a document we follow peyrard and use a genetic algorithm peyrard and eckle kohler to generate extractive summaries we optimize for metrics rouge l bertscore and moverscore generating summaries per metric for each of the nearly k documents in the cnndm test set resulting in summaries per document after de duplication we are left with nearly summaries per document on average for the tac dataset we randomly sample summaries for each document from the nearly output summaries provided by peyrard experiment and analysis width of scoring range in this experiment we aim to re examine the results in peyrard and answer our rst research question how do different automated metrics correlate in ranking summaries in different scoring ranges we approach this as follows for each summary sij of document we rst calculate its mean score across all metrics after normalizing the metrics to be between and we use this to partition the scoring range of each document into three parts low scoring l medium scoring m and top scoring t which are the bottom third the middle third and the top third of the scoring range respectively we then analyze the summaries falling into these bins in two different ways github com tiiiger bert score github com aiphes moverscore the function dened in github com ukplab genetic swarm mds and l the python wrapper github com sebastiangehrmann rouge baselines ms metric bin bscore ms t t t t t ms rl rl bin l m t l m t l m t l m t l m t rl table kendall for the cumulative and non cumulative settings on tac higher values indicate greater inter metric correlation l m and t correspond to the low medium and top scoring range spectively please see sec for more details cumulative in this setting we aim to replicate peyrard s results which compared inter metric agreement on the whole set of summaries to that on the top scoring subset to do this we compute the average inter metric correlation for summaries belonging to i l m t m t and t as shown in the left side in tab note that here the width of the scoring range is different for each row non cumulative in this setting we analyze the average inter metric correlation on summaries longing to each scoring bin separately as shown in the right side of tab we advocate for the use of this setting as it controls for the width of the scoring range and it allows for a more ne grained analysis of the scoring range note that for each bin the correlation is calculated for summaries generated for each document and then averaged over all documents we only consider statistically signicant p kendall s values observations discussion our observations on the tac and cnndm datasets are shown in tab and respectively in the cumulative setting we observe the same trend reported by peyrard metric agreement decreases when the average score increases and is the lowest in the top scoring range t however in the non cumulative setting where metrics rank summaries from a narrow scoring range we observe that i metrics have low correlations in all three scoring ranges low medium and top and ii there is no clear trend in correlations across the bins comparing the cumulative and non cumulative settings one can see that decreasing the width of the scoring range reduces the inter metric correlations this suggests that rather than the scoring range the width of the scoring range has a strong impact on the correlation between metrics this may be because summaries from a narrow scoring range are similar to each other and thus difcult for different metrics to rank consistently factors affecting inter metric correlation in this experiment we aim to answer the second research question apart from the width of the scoring range which factors affect inter metric correlations specically we identify three factors which affect the correlation of metrics ease of summarization abstractiveness and coverage ease of summarization eos for each generated summary sij of document with reference summary ri we dene eos as maxj ri here mk is a metric function n normalized to be between and thus eos is the average over all metrics of the maximum score that any summary received a higher eos score for a document implies that for that document we can generate higher scoring extractive summaries according to many metrics abstractiveness we dene abstracriveness of a document with reference ri as where is the set of unique tokens of any text abstractiveness measures the overlap in laries of the document and its reference summary coverage we use the denition of coverage as provided by grusky et al i e the percentage ms metric bin bscore ms t t t t t ms rl rl bin l m t l m t l m t l m t l m t rl table kendall for the cumulative and non cumulative settings on cnndm higher values cate greater inter metric correlation l m and t correspond to the low medium and top scoring range respectively please see sec for more details tac eos tac abs tac cov cnndm eos e cnndm abs cnndm cov figure effect of different properties of reference summaries we only show correlation between bertscore and due to limited pages the trend is similar for all other pairs as shown in the appendix the plots for cnndm are more dense because of more documents in the cnndm test set as compared to tac cov and abs stand for coverage and abstractiveness respectively the trend lines in red are the point and point moving average for tac and cnndm respectively of words in the summary that are part of an extractive fragment with the article we refer the reader to grusky et al for a detailed description of coverage observations our observations are summarized in fig each point in the graph represents a document reference summary pair with its corresponding property on the axis and inter metric relation of its summaries on the y axis we nd that metrics agree with each other as documents become easier to summarize as documents become more abstractive the correlation between metrics decreases as the coverage of documents increases the correlation between metrics increases these observations suggest that automatic evaluation metrics have higher correlations for easier to marize and more extractive lower abstractiveness higher coverage document reference summary pairs implications and future directions in this work we revisit the conclusion of peyrard s work and show that instead of solely ing in high scoring range metrics disagree when ranking summaries from all three scoring ranges low medium and top this highlights the need to collect human judgments to identify trustworthy metrics moreover future meta evaluations should use uniform width bins when comparing correlations to ensure a more robust analysis additionally we analyze three linguistic properties of reference summaries and their effect on inter metric correlations our observation that metrics de correlate as references become more abstractive suggests that we need to exercise caution when using automatic metrics to compare summarization systems on abstractive datasets like xsum narayan et al moreover future work proposing new evaluation metrics can analyze them using these properties to get more insights about their behavior acknowledgments we would like to thank maxime peyrard for sharing the code and data used in peyrard and for his useful feedback about our experiments we would also like to thank graham neubig for his feedback and for providing the computational resources needed for this work references manik bhandari pranav narayan gour atabak ashfaq pengfei liu and graham neubig re evaluating in proceedings of the conference on empirical methods in natural evaluation in text summarization language processing emnlp hoa dang and karolina owczarzak overview of the tac update summarization task in proceedings of the first text analysis conference tac pages hoa dang and karolina owczarzak overview of the tac summarization track in proceedings of the first text analysis conference tac pages max grusky mor naaman and yoav artzi newsroom a dataset of million summaries with diverse extractive strategies in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long papers volume pages karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend in advances in neural information ing systems pages chin yew lin guihong cao jianfeng gao and jian yun nie an information theoretic approach to matic evaluation of summaries in proceedings of the human language technology conference of the naacl main conference pages new york city usa june association for computational linguistics chin yew lin rouge a package for automatic evaluation of summaries text summarization branches out annie louis and ani nenkova automatically assessing machine summary content without a gold standard computational linguistics ramesh nallapati bowen zhou cicero dos santos c a glar and bing xiang abstractive text summarization using sequence to sequence rnns and beyond conll page shashi narayan shay b cohen and mirella lapata do nt give me the details just the summary aware convolutional neural networks for extreme summarization in proceedings of the conference on empirical methods in natural language processing pages jun ping ng and viktoria abrecht better summarization evaluation with word embeddings for rouge in proceedings of the conference on empirical methods in natural language processing pages lisbon portugal september association for computational linguistics maxime peyrard and judith eckle kohler a general optimization framework for multi document rization using genetic algorithms and swarm intelligence in proceedings of the international conference on computational linguistics coling pages dec maxime peyrard teresa botschen and iryna gurevych learning to score system summaries for better content selection evaluation in proceedings of the emnlp workshop new frontiers in summarization page to appear september maxime peyrard studying summarization evaluation metrics in the appropriate scoring range in ings of the annual meeting of the association for computational linguistics pages florence italy july association for computational linguistics pauli virtanen ralf gommers travis e oliphant matt haberland tyler reddy david cournapeau evgeni burovski pearu peterson warren weckesser jonathan bright stefan j van der walt matthew brett joshua wilson k jarrod millman nikolay mayorov andrew r j nelson eric jones robert kern eric larson cj carey ilhan polat yu feng eric w moore jake vand erplas denis laxalde josef perktold robert cimrman ian henriksen e a quintero charles r harris anne m archibald antonio h ribeiro fabian pedregosa paul van mulbregt and scipy contributors scipy fundamental algorithms for scientic computing in python nature methods tianyi zhang varsha kishore felix wu kilian q weinberger and yoav artzi bertscore evaluating text generation with bert in international conference on learning representations wei zhao maxime peyrard fei liu yang gao christian m meyer and steffen eger moverscore text generation evaluating with contextualized embeddings and earth mover distance in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china november tion for computational linguistics cumulative bins on tac non cumulative bins on tac cumulative bins on cnndm non cumulative bins on cnndm figure disagreement between metrics on tac and cnndm a disagreement in addition to kendall s between metrics peyrard analyzes the disagreement between metrics and shows higher inter metric disagreement in the higher scoring range to analyze disagreement they randomly sample pairs of summaries say sa and sb with corresponding references ra rb for each pair of metrics say and and bin them into cumulative bins according to the average score for any one metric i e according to ra rb the disagreement for each bin is then calculated as the percentage of summary pairs for which ra rb but ra rb or vice versa i e ra rb but ra rb the use of cumulative bins suffers from the same phenomena as described in section i e the width of the bin may play a role in the agreement of metrics in fig we replicate the cumulative disagreement plot for the tac and cnndm datasets and show the corresponding non cumulative versions we observe that when we control for the width of scoring range inter metric disagreement is higher even in the low scoring range b f n ratio peyrard s nal experiment measures if improvements according to one metric are consistent across other metrics to this end they dene f n as follows let be the set of summaries for a document and m be the set of all metrics let s sample a summary s randomly then f n m m i e out of all the summaries ranked better than a summary by one metric how many are ranked better tac cnndm cnndm with random metrics figure f n ratio between metrics on tac and cnndm tac cnndm cnndm with random metrics figure f ration between metrics on tac and cnndm by all the metrics as shown in fig on the tac dataset as the average score of s averaged across all metrics increases f n decreases this may suggest that as summary quality improves different metrics do not agree on which summaries are of better quality however this quantity is misleading as the average score of s increases the numerator f will naturally decrease because for a higher scoring s the number of summaries that are better than s are fewer while the denominator n may remain large even if one metric is misaligned with others to prove this hypothesis we rst replicate the measure for tac and cnndm datasets in fig next instead of the real metric scores we assign each summary a random number sampled from in fig we see the same trend for random scores as for real metric scores this shows that this decreasing trend is indeed a property of the ratio f n rather than being a property specic to real evaluation metrics moreover one can come up with a modied ratio f as follows f n m m which measures out of all the summaries that are ranked worse than a summary by one metric how many are ranked worse by all metrics if metrics truly de correlated in only the higher scoring range one would expect the same decreasing trend for f however as is clear from fig the trend is reversed for real as well as random metric scores f increases as average score of s increases this is because similar to f n this measure is also misleading and sensitive to the numerator f which always increases as average score of s increases c factors affecting inter metric correlation c ease of summarization please see fig for ease of summarization vs kendall s for all metric pairs please see fig for abstractiveness vs kendall s for all metric pairs c abstractiveness c coverage please see fig for coverage vs kendall s for all metric pairs figure ease of summarization axis vs kendall s y axis for all metric pairs on the tac dataset figure ease of summarization axis vs kendall s y axis for all metric pairs on the cnndm dataset figure abstractiveness axis vs kendall s y axis for all metric pairs on the tac dataset figure abstractiveness axis vs kendall s y axis for all metric pairs on the cnndm dataset figure coverage axis vs kendall s y axis for all metric pairs on the tac dataset figure coverage axis vs kendall s y axis for all metric pairs on the cnndm dataset
