national research university higher school economics moscow russia dimas munoz montesinos aug abstract contents introduction models bert experiments synthetic text generation challenging limited success recently new architecture called transformers allow machine learning models understand better sequential data translation summarization bert transformers cores shown great performance tasks text classication translation nli tasks article analyse algorithms compare output quality text generation tasks infer masked token question answering conditional text generation models comparison architecture pre training performance related models conclusions references appendix appendix experiments results infer masked token question answering conditional text generation appendix simple transformer model introduction introduction natural language processing nlp large eld different tasks text classication named entities recognition language texts written translation tasks common challenge human languages usually unstructured texts task concerns article text generation conditional language model novel transformers architecture order understand text generation necesary dene language model wikipedia statistical language model probability distribution sequences words given sequence length assigns probability sequence consequence use conditional probability word sequence article assume fundamental knowledge deep learning word vectors embedding space describe models techniques relevant understand transformer based models models long time conditional text generation based models idea consists recurrent neural networks rrn try predict state sequence previous rnns receive names encoder decoder respectively extended rnns lstm introduced sepp hochreiter jurgen schmidhuber gru introduced junyoung chung texts generated rnns far perfect tend nonsense include spelling mistakes basically wrong prediction potential entire sentence meaningless furthermore possible apply parallelization rnns need process data sequence figure architecture model rst rnn called encoder second decoder case model receives input sentence abc produces xyz output sentence input output different lengths contextualized word embeddings traditionally model received sequence tokens usually words transformed static vectors short simply vectors numbers represent meaning word widely extended model introduced mikolov computes static vector token vectors called embeddings furthermore introduction vectors provided state art sota performance syntactic semantic word similarities power word vectors lend mathematical operators example add subtract vectors king man woman queen recent techniques consists incorporating context word embeddings replacing static vectors contextualized word representations led signicant improvements virtually nlp task elmo introduced kind word embeddings vectors learned functions internal states deep bidirectional language model breakthroughs lead models understanding words example difference homonyms rock stone music genre instead having static vector transformers google researches released new model called transformer paper attention need briey architecture consists self attention point wise fully connected layers figure similarly describe transformers include encoder decoder nal linear layer figure transformer model architecture described attention need models designed handle sequences data especially useful nlp note contrast contain recurrence convolution require process sequences order fact allows parallelize rnns reduces training time models models bert description bidirectional encoder representations transformers commonly known abbreviated form bert suggests designed pretrain deep bidirectional representations unlabeled text jointly conditioning left right context layers model architecure technical innovation implements multiple layers transformer encoders language modelling authors bert describe steps framework pre training tuning figure pre training model trained unlabeled data different pre training tasks masked sentence prediction authors pretrained bert bookscorpus words wikipedia words tuning model initialized pre trained parameters known transfer learning downstream task tuned separately process simple described article figure overall pre training tuning procedures described bert paper input representation order bert handle variety downstream tasks authors dened different inputs allow unambiguously represent single sentence pair sentences inputs tokenized text technique called wordpiece sub words units apart sub words units tokens authors introduced new tokens appended input sentences beginning input sep sentence second inputs sequences called token type ids segment ids indicates token belongs sentence series sentence series called mask ids input texts padded length indicates text padded certain position models figure example bert input single sentence different sentences input segments sequence starting position token sep figure tokens words easier handle unknown words word pieces powerful sense tokens cover word words occur dictionary lose information subword units input pre training tasks masked language model mlm task authors masked wordpiece tokens sequence dataset random predict masked tokens mask tokens token mask tuning create mismatch pre training tuning sentence prediction nsp words authors tasks question answering natural language inference nli based understanding relationship sentences directly captured language modeling order train model understands sentence relationships given pair sentences model predict second sentence subsequent sentence original document authors built dataset actual sentence random sentence monolingual corpus description generative pretrained transformer known large unsupervised transformer based language model successor gpt introduced june researchers openai paper language models unsupervised multitask learners consists solely stacked decoder blocks transformer vanilla transformer architecture decoder fed architecture word embedding concatenated context vector generated context vector zero initialized rst word encoder embedding furthermore vanilla transformer architecture self attention applied entire surrounding context words sentence masked self attention instead decoder allowed obfuscation masking remaining word positions glean information previous words sentence plus word close copy vanilla transformer architecture similar predecessor gpt authors trained simple objective given text predict word purpose crawled data internet similar way bert tuned downstream tasks analyse model performance different situations experiments figure gpt architecture described improving language understanding generative pre training transformer training objectives left input transformations tuning right authors moved layer normalization input sub block added layer normalization nal self attention block input representation uses byte pair encoding bpe input representation technique allows combine empirical benets word level lms generality byte level approaches bpe simple compression method common pair consecutive bytes data replaced byte occur data instance given sequence aaabdaaabac obtain zabdzabac important highlight fact authors apply kind pre processing data lowercasing tokenization vocabulary tokens believe general language model able compute probability generate string experiments infer masked token explained pretraining tasks bert mlm experiment need pretrained bert model huggingface provides wide variety pretrained tuned models pytorch tensorflow particular chose bert multilingual base model pretrained model languages largest wikipedia mlm objective apply softmax function normalize output pick words experiment simple require training proceeded test despite infered words correct cases cases word suggestions far good instance rst word today written english demolished gramatically models experiments figure given sentence car spanish returns words popular simple uncommon case words sense figure given sentence think nastya mask person russian returns words young big great context word new sense correct uncommon use english pretrained model bert base model rst output closed non english languages unable return word punctuation symbols believe use monolingual models obtain good results words train bert model language wide list arguments described conclusions section infered masked tokens experiments appendix wide variety tests languages english spanish russian question answering question answering tasks model receives text called context question context mark inital ending answer context model generate text similarly previous experiment import bert multilingual base model huggingface add fully connected layers obtain initial ending token positions context figure based positions pick portion context return answer question test previous training tuning bert adjust weights fully connected layers words technique called transfer learning model pre trained dataset use pre trained model carry knowledge solving dataset experiments figure sample question answering napoleon biography context extracted wikipedia answer good stricly accurate ask period order train model use xquad dataset consists subset paragraphs question answer pairs development set squad professional translations languages spanish russian experiments answers tend accurate ask simple questions use similar words context figure observe particularly difcult model understand synonyms homonyms example ask spanish napoleon crowned king italy monarch rst gives good answer march strictly second returns totally wrong answer november questions answers experiments appendix wide variety questions languages english spanish russian overall model answers correctly write questions english conditional text generation task experimented bert designed text generation trained predict token sentence surprisingly learned basic competence tasks like translating languages answering questions figure ability generate conditional text samples unprecedented quality appendix observe model capable generating synthetic texts close human quality provided different kind prompts analyse versatility despite shows coherence good quality particular write topics highly represented training data found failures sudden topic switching repetitive text failures example model negates wrote previous sentences figure text quality good observe wrote extravagant sentence ship water explain later bert architectures different rst transformer encoder architecture second transformer decoder com deepmind xquad models comparison figure architecture bert model rms titanic visited divers rst time years british vessel damaged bow ship found completely submerged titanic ship sunk international waters according experts damage titanic worse ship water fact according reports damage caused catastrophic failure figure text sample generated providing initial text bold letters fact lead use bert text generation efforts create new bert based algorithms purpose main idea consists adding decoder transformer end example recent paper bert looks promising task include article lack time resources train model main idea bert use half layers bert encoders half layers decoders addition bert tried implement simple transformer model capable generating conversation result poor training data small resources train model read details appendix models comparison architecture pre training difference encounter bert architecture despite transformers based architectures uses encoders case bert case decoders particularity related models makes bert understand text information different way consecuence performance variate depending dataset difference pre training method aims predict word sentence bert trained nsp mlm mind pre training methods use transfer learning tune model performance order evaluate bert authors prepared different versions models different number parameters tuned different downstream tasks version bert base number parameters gpt predecesor comparison purposes unfortunately common benchmarks bert datasets compare bert gpt glue swag tasks glue consists wide list nlp tasks sentiment analysis similarity nli question answering dataset bert tuned dataset bert base outperformed gpt prior approaches bert large got better result achieved status sota collection tasks swag consists multiple choice questions grounded situations given question situation pick correct answer choices identically glue bert outperformed gpt bert large improved gpt score surprising bert large got better score human expert small subset questions related models models described article bert demonstrate benets large scale language modeling papers leverage advances compute available text corpora signicantly surpass state art performance natural language understanding nlu modeling generation currently new models based bert architectures better results roberta robustly optimized version bert larger training data albert lite version bert distilbert reduced version bert structbert incorporate language structures bert pre training appart mentioned bert based models nvidia published megatron words simple efcient model parallel approach modicating existing pytorch transformer implementations released billion parameters version bert megatron models got better scores vanilla bert models tasks glue leaderboard com leaderboard squad albert sota status github squad glue structbert achieves second best score leaderboard swag roberta second position leaderboard allenai org swag submissions public conclusions figure evolution number parameters described authors distilbert additionally openai published recently new model called uses architecture largest version novel model billion parameters times previous non sparse language mode despite strong quantitative qualitative improvements particularly compared direct predecessor notable weaknesses text synthesis nlp tasks extensively described paper unfortunately include text samples generated article openai released model conclusions transformers disrupted sequence based deep learning signicantly variants transformer models showed bert outperform previous approaches rnns cases models advantage attention layers selectively weight different elements input data achieve state art performance language modelling benchmarks situations output transformers based models close human quality output quality depends familiar model topic instance able generate high quality text provide uncommon context recent pandemic new popular person case bert observe model tend answer incorrectly question answering tasks use homonyms synonyms instead words context experiments computers distant fully understanding unstructured texts written human languages observe trend build larger models nlp eld capture information unstructured texts furthermore unlikely humans need trained tons data terms zero shot conclusions shot tasks need improve efciency nlp algorithms text human sees lifetime general observe despite models achieve better performance specic tasks common sense humans produces better results deep learning model ignore fact learn daily life difcult measure type data provide model pre training similar knowledge likely reason deep learning models fail produce coherent texts addition previous points observe growing need pre train model language bert model apart grammar rules language example evident difference rtl ltr languages knowledge intrinsic language polite expressions informal style want obtain human quality text generation need large dataset english languages use translated data summary transformer based models future demonstrated superiority parallel computation modelling long range dependencies compared rnns lstm know best approach pre train reproduce human common sense mandatory text generation likely going new pre training methods larger models references references ilya sutskever oriol vinyals quoc sequence sequence learning neural networks sepp hochreiter jrgen schmidhuber long short term memory neural computation junyoung chung caglar gulcehre kyunghyun cho yoshua bengio empirical evaluation gated recurrent neural networks sequence modeling tomas mikolov kai chen greg corrado jeffrey dean efcient estimation word representations vector space matthew peters mark neumann mohit iyyer matt gardner deep christopher clark kenton lee luke zettlemoyer contextualized word representations ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez lukasz kaiser illia polosukhin attention need jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language understanding yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin johnson xiaobing liu ukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes jeffrey dean google neural machine translation system bridging gap human machine translation alec radford karthik narasimhan tim salimans ilya sutskever improving language understanding generative pre training alec radford jeffrey rewon child david luan dario amodei ilya sutskever language models unsupervised multitask learners philip gage new algorithm data compression users pranav rajpurkar jian zhang konstantin lopyrev percy liang squad questions machine comprehension text congying xia chenwei zhang hoang nguyen jiawei zhang text generation bert philip generalized shot intent detection bert conditional alex wang amanpreet singh julian michael felix hill omer levy samuel bowman glue multi task benchmark analysis platform natural language understanding references rowan zellers yonatan bisk roy schwartz yejin choi swag large scale adversarial dataset grounded commonsense inference yinhan liu myle ott naman goyal jingfei mandar joshi danqi chen omer levy mike lewis luke zettlemoyer veselin stoyanov roberta robustly optimized bert pretraining approach zhenzhong lan mingda chen sebastian goodman kevin gimpel piyush sharma radu soricut self supervised learning language representations albert lite bert victor sanh lysandre debut julien chaumond thomas wolf distilbert distilled version bert smaller faster cheaper lighter wei wang bin ming yan chen zuyi bao jiangnan xia liwei peng luo structbert incorporating language structures pre training deep language understanding mohammad shoeybi mostofa patwary raul puri patrick legresley jared casper bryan catanzaro training multi billion parameter language models model parallelism megatron tom brown benjamin mann nick ryder melanie subbiah jared kaplan prafulla dhariwal arvind neelakantan pranav shyam girish sastry amanda askell sandhini agarwal ariel herbert voss gretchen krueger tom henighan rewon child aditya ramesh daniel ziegler jeffrey clemens winter christopher hesse mark chen eric sigler mateusz litwin scott gray benjamin chess jack clark christopher berner sam mccandlish alec radford ilya sutskever dario amodei language models shot learners appendix experiments results appendix experiments results infer masked token despite options philosophical rare situations believe uncommon options replace masked token occurs options grammatically correct context sentence cases note incorrect english sentence options notes hello model mask today model real business mathematical new correct demolished unknown closed abandoned active option closed correct mask good idea correct incident accident explosion scene crash correct correct doctor ran emergency room mask patient doctor ran emergency room appendix experiments results sentence options notes russian mask mask mask spanish correct option incorrect rst second options correct appendix experiments results sentence options notes migo conmigo ella ellos dios correct option rst conmigo dinero tiempo aos problemas tiempo dinero pruebas informacin recursos colegio pueblo mar mundo hotel rst options correct rst tiempo options informacin recursos correct correct forth option mundo ayer estuve paseando encontr con mask tuviera mask podra vacaciones necesito aprobar examen para voy con mis amigos mask question answering english context napoleon biography extracted wikipedia napoleon bonaparte august french statesman military leader notorious artillery commander french revolution led successful campaigns french revolutionary wars emperor french napoleon briey days napoleon dominated european global affairs decade leading france series coalitions napoleonic wars won wars vast majority battles building large empire ruled continental europe nal collapse regarded greatest military commanders history wars campaigns studied military schools worldwide napoleon political cultural legacy celebrated controversial leaders human history wikipedia org wiki napoleon appendix experiments results question answer notes correct answer incorrect emperor napoleon bonaparte napoleon napoleon bonaparte august born napoleone buonaparte studied wars campaigns military worldwide schools correct studied military worldwide schools incorrect long napoleon dominated europe decade correct russian context extracted wikipedia higher school economics question answer notes hse founded year campuses located moscow correct answer gramatically incorrect partial answer higher school economics research university national correct accurate acronym higher school economics research national university higher school economics incorrect wikipedia org appendix experiments results spanish description sagrada familia extracted wikipedia tempo expiatorio sagrada familia temple expiatori sagrada famlia conocido simplemente como sagrada familia una baslica catlica barcelona espaa diseada por antoni gaud iniciada todava est construccin gaud mximo exponente arquitectura modernista catalana uno monumentos visitados espaa junto museo del prado alhambra granada iglesia visitada europa baslica san pedro del vaticano cuando est nalizada ser iglesia cristiana alta del mundo question answer notes escribe cmo cataln written catalan quin dise iglesia designed church comenz cundo construccin sagrada familia sagrada construction familia began lugar cristiano recibe visitas que sagrada christian familia place receives visits sagrada familia otros monumentos son muy visitados por espaa monuments highly visited tourists spain temple expiatori sagrada famlia correct antoni gaud correct correct outputs context incorrect museo del prado alhambra granada correct conditional text generation following synthetic texts generated fed model initial text let nish text samples written english end nonsense outputs languages note model output everytime run obtain different output wikipedia org wiki appendix experiments results week people reported saw unicorn park turned real unicorn fake unicorn foam plastic foam polystyrene plastic commonly construction materials fake unicorn foam known effective material creating fake animals foam polystyrene months new virus spread worldwide changed daily lifes new virus called ebola appeared countries killed people countries guinea liberia sierra leone virus spread direct contact uids infected persons include body uids blood sweat tears spread contaminated food water ebola arrives country lot people describe maria like strong resilient thank thanks offer talk think death penalty advocate death penalty believes life taken taken course case case murder belief fair penalty evidence sufcient prove person committed actual crime life sentence teacher good morning students good morning teacher teacher pleasure welcome today students yeah teacher want talk things students yeah teacher guys great enthusiasm students yeah teacher start rst topic favorite type book read appendix simple transformer model appendix simple transformer model implemented simple model figure gure able generating text based training data purpose took english dialogue written shakespeare figure visual description transformer model model consists transformer encoders transformer decoders capture text information fully connected layer obtain prediction token text kept model simple reasons resources train large model like bert parameters model larger need bigger dataset train unfortunately model predictions bad sense outputs nonsense tokens continue text sample text model generated initial text romeo likely need increase model size better capture text information consecuence need use data train generate coherent text took dialogue tensorflow tutorial tensorflow org text appendix simple transformer model romeo took perform sets big katharinad heart belie harpetsd sure darkness underd statutes deceive orderly likely body roodfor masteredond conclude aged astherefore suffer hiswittedwould royalties ouroan goddess wasyour things curst hag admirings twere spirit linger glory follow ised greatestar
