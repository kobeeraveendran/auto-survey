abstractive summarization reddit posts multi level memory networks byeongchang kim hyunwoo kim gunhee kim department computer science engineering center superintelligence seoul national university seoul korea byeongchang kim hyunwoo snu snu projects reddit tifu abstract address problem abstractive rization directions proposing novel dataset new model collect reddit tifu dataset consisting posts online discussion forum reddit use informal crowd generated posts text source contrast existing datasets use formal documents source news articles dataset suffer biases key sentences usually locate beginning text favorable summary candidates text similar forms second pose novel abstractive summarization model named multi level memory networks mmn equipped multi level memory store information text different levels abstraction quantitative evaluation user studies amazon mechanical turk reddit tifu dataset highly stractive mmn outperforms art summarization models introduction abstractive summarization methods der intensive study suffer ferior performance compared extractive ods allahyari nallapati admittedly task tion abstractive summarization ing extractive summarization argue inferior performance partly biases existing summarization datasets source text datasets hermann cohan grusky narayan originates formal documents news articles structural patterns extractive methods better advantage formal documents strong tendency key sentences locate ning text favorable summary dates inside text similar forms summarization methods generate good summaries simply memorizing keywords phrases particular locations text abstractive methods trained datasets tion implicitly forced learn structural patterns kedzie grusky narayan recently report similar extractive bias existing datasets alleviate bias lecting articles diverse news publications intro sentences gold summary different previous approaches pose alleviate bias issue changing source summarization dataset exploit generated posts online discussion forum reddit especially tifu subreddit casual conversational news articles observe source text reddit follow strict formatting disallows models simply rely locational biases tion passages rarely contain tences nearly identical gold mary new large scale dataset tive summarization named reddit tifu tains pairs online post source text corresponding long short summary tence posts written different users pair post summary ated user key contribution work pose novel memory network model named level memory networks mmn model equipped multi level memory networks ing information source text different levels abstraction word level level paragraph level document level design motivated abstractive tion highly challenging requires understand document salient words phrases sentences model sequentially read multiple levels mation generate good summary sentence abstractive summarization methods zhou liu cohan paulus employ sequence sequence models sutskever rnn encoder embeds input document rnn decodes summary sentence mmn major advantages based els rnns accumulate information xed length memories step regardless length input sequence fail utilize far distant information ishing gradient critical tion tasks input text usually long words hand tional memory explicitly captures long term mation second rnns build tions different ranges hidden states sequentially connected sequence holds hierarchical rnns learn multiple levels representation contrast model exploits set convolution operations different receptive elds build representations multiple levels multiple ranges sentences paragraphs document perimental results proposed mmn model improves abstractive summarization formance new reddit tifu isting newsroom abs grusky xsum narayan datasets performs state art abstractive els architecture zhou uate quantitative language metrics plexity rouge lin user studies amazon mechanical turk amt contributions work follows newly collect large scale abstractive summarization dataset named reddit tifu far know work rst use non formal text abstractive rization propose novel model named multi level memory networks mmn best knowledge model rst attempt leverage memory networks tive summarization discuss unique updates mmn existing memory networks section quantitative evaluation user studies amt model forms state art abstractive tion methods reddit tifu room abstractive subset xsum dataset related work work uniquely positioned text following topics neural abstractive summarization deep neural network models proposed abstractive summarization dominant architectures employ rnn based models attention mechanism rush chopra ati cohan hsu gehrmann addition cent advances deep network research promptly adopted improving abstractive marization notable examples include use variational autoencoders vaes miao blunsom graph based tention tan pointer generator els self attention networks liu reinforcement learning paulus pasunuru bansal contextual agent attention celikyilmaz tegration extractive models hsu gehrmann compared existing neural methods stractive summarization approach novel replace rnn based encoder explicit multi level convolutional memory based encoders consider quence represent hidden state level memory network exploits convolutions control extent representation multiple els sentences paragraphs text summarization datasets existing marization datasets use formal documents source text news articles exploited including duc word napoles cnn dailymail lapati hermann room grusky xsum narayan datasets cohan troduce datasets academic papers arxiv dataset posts words post words summ rottentomatoes idebate tifu short tifu long table statistics reddit tifu dataset pared existing opinion summarization corpora tentomatoes idebate wang ling average median parentheses values tioning closest works singh use memory network text summarization deal extractive summarization storing dings individual sentences memory compared previous memory networks building mmn novel features multi level memory network better abstracts multi level representation long document employing dilated convolutional memory write mechanism correlate adjacent memory cells iii proposing normalized gated tanh units avoid covariate shift network generating output sequence rnns reddit tifu dataset introduce reddit tifu dataset key statistics outlined table collect data reddit discussion forum platform large number subreddits diverse topics interests specically crawl posts jan mar tifu dit post strictly follow posting rules removed thanks following posts subreddit excellent corpus abstractive rization rule posts titles context removed title attempt encapsulate nature rule posts end summary descriptive consequences regard body text source tle short summary summary long summary result sets datasets tifu short tifu long figure shows example post tifu subreddit preprocessing build vocabulary dictionary choosing frequent words dataset com tifu wiki rules figure example post tifu subreddit pubmed propose sts dataset collection chinese microblog short text paired summary selects formal text posted veried ganizations news agencies government institutions compared previous summarization datasets dataset novel consists posts online forum reddit rotten tomatoes idebate dataset wang ling use online text source relatively small scale posts tomatoes compared posts tifu short shown table rotten tomatoes use multiple movie reviews written different users single source text sentence consensus professional editor summary pair dataset ent tifu written user idebate dataset collected short arguments debates controversial ics text formal hand dataset contains posts interesting stories happened daily life text unstructured informal neural memory networks ory network models proposed prove memorization capability neural networks kaiser yoo weston propose early memory networks language question ing memory networks proposed tasks sukhbaatar kumar miller park propose convolutional read memory network personalized image iliveprettyfarfromwindsor afivehourdrivelater ifinallygotbackhome homeworketc init andmychemistrytextbookbackinwindsor ialsohaveamathandchemtestnextweekwhichiamnowsocompletelyscrewedfor long summary text whichisfivehourdriveawayandiamnowscrewedfortherestofthesemester short summary words figure relative locations bigrams gold summary source text different datasets dataset cnn nallapati times sandhaus newsroom grusky newsroom abs grusky xsum narayan tifu short tifu long lead ext oracle lead oracle ratio ratio table comparison rouge scores different datasets row methods column art abstractive summarization method lead ext oracle extractive ones lead oracle rouge ratios lead ext oracle respectively report numbers dataset row corresponding cited papers exclude urls unicodes special acters lowercase words normalize digits subreddit names user ids replaced token respectively use package strip markdown format tokenize words common prexes summary sentences tifu trimmed oov words consideration vocabulary size covers word frequencies dataset set maximum length ument exclude gold summaries lengths short tifu long respectively posts datasets use maximum lengths based previous datasets words average summary gigaword duc cnn dailymail datasets respectively domly split dataset training test abstractive properties reddit tifu discuss abstractive characteristics found reddit tifu dataset compared existing marization datasets based news articles weak lead bias formal documents including news articles tend structured emphasize key information beginning text markdown github hand key information informal online text data spread text figure plots density histogram relative tions bigrams gold summary source text cnn dailymail newsroom bigrams highly concentrated parts documents contrarily reddit tifu dataset shows uniform distribution text characteristic seen rouge score comparison table lead baseline simply creates summary selecting rst sentences words document high score lead baseline implicates strong lead bias lead scores lowest tifu dataset difcult models simply advantage locational bias summary strong abstractness locational bias news articles tend contain wrap tences cover article resemblance gold summary existence measured score ext oracle baseline creates summary selecting sentences highest average score viewed upper bound extractive models narayan nallapati table rouge scores ext oracle lowest tifu dataset means sentences similar gold summary scarcely exist inside source text dailymailnewsroom absxsumreddit tifurelative figure illustration proposed multi level memory network mmn model word time extracting relevant information memory cells response previously erated words input model source text output quence summary words symbol dictionary text embedding online posts include lots morphologically ilar words closely embedded use fasttext bojanowski trained common crawl corpus initialize word embedding matrix wemb use embedding matrix wemb source text output sentences represent source text distributional space wembxi hot vector word source text likewise output words embedded construction multi level memory shown figure multi level memory network takes source text embedding input generates number memory tensors output superscript denote input output memory sentation respectively multi level memory network motivated human stand document remember single document ties levels abstraction word level level paragraph level document level figure comparison gated linear unit gehring proposed normalized gated tanh unit dataset property forces model trained focus comprehending entire text instead simply nding wrap sentences finally lead oracle table rouge ratios lead ext oracle respectively metrics quantify dataset according degree culty extractive methods suitability abstractive methods respectively high scores tifu dataset metrics potentially excellent benchmark evaluation abstractive summarization systems multi level memory networks mmn figure shows proposed multi level memory network mmn model mmn memorizes source text proper representation memory generates summary sentence multi level sequence convweight normsigmoidconvweight normtanhlayer normconvweight generate sets memory tensors associates cell different ber neighboring word embeddings based level abstraction build memory slots multi level memory exploit multi layer cnn write network layer chosen based size receptive eld issue convolution large receptive elds require layers large lter sizes example stacking layers lter size results receptive eld size output depends input words order grow receptive eld increasing computational cost exploit dilated lution koltun oord write network memory writing dilated convolution dilated convolution lter applied area larger length skipping input values certain gap formally length input lter dilated convolution operation ements sequence dened dilation rate lter size accounts direction dilation parameters lter dilated convolution reduces regular convolution ing larger dilation enables single output level represent wider range input effectively expanding receptive eld embedding source text recursively apply series dilated convolutions denote output convolution layer normalized gated tanh units tion followed new activation ized gated tanh unit ngtu illustrated figure layer normalization mixed normalization improves earlier work gehring weight malization applied glu figure tries preserve variance activations network scaling residual blocks serve heuristic preserve variance empirically work dataset contrarily proposed ngtu guarantees preservation activation ances signicantly improves mance multi level memory instead layer output cnns exploit outputs multiple layers cnns construct sets memories example memory constructed layer receptive eld sentence level embeddings ory layer receptive eld document level embeddings obtain level memory bling key value memory networks miller input recall output memory matrix respectively cates index convolutional layer level memory example set level ries uses output convolution layer respectively output memory representation add document embedding skip connection state based sequence generation discuss predict word time step based memory state ously generated words figure visualizes overall procedure decoding rst apply max pooling output layer encoder network build document embedding dwhole sigmoid element wise plication denote lter gate layer dilated convolution respectively ngtu extension existing gated tanh units gtu oord ing weight normalization salimans kingma dwhole decoder designed based wavenet oord uses series causal lated convolutions denoted globally condition dwhole obtain dings previously generated words lter gate hidden state spectively learnable parameters wembyt set level decoder network tifu short tifu long initialize generate number query vectors time memory network qol query vectors fed attention function level memory vaswani attention function softmax demb set demb embedding mension obtain output word probability finally lect word highest probability argmaxsv eos token repeat generating word feeding output convolution layer training use softmax cross entropy loss mated target ygt forces model predict extremes zero tinguish ground truth alternatives label smoothing alleviates issue acting regularizer makes model dent prediction smooth target bution uniform prior distribution pereyra edunov vaswani loss training set experiments experimental setting evaluation metrics evaluate rization performance language metrics perplexity standard rouge scores lin remind lower perplexity higher rouge scores indicate better performance datasets addition reddit tifu evaluate existing datasets abstractive set newsroom grusky xsum narayan suitable marks evaluation model aspects specialized abstractive rization meets goal work second larger vocabulary size reddit tifu evaluate learning capability model baselines compare abstractive summarization methods basic model heuristic extractive methods variants model choose seass zhou drgd state art methods abstractive marization test attention based model denoted att chopra heuristic extractive methods uses rst sentence text summary ext oracle takes sentence highest average score gold summary text ext oracle viewed upper bound extractive methods test variants method validate contribution component exclude key components model follows tional convolutions instead multi level memory ing gated linear units gehring quanties improvement dilated convolution assesses effect multi level memory dates normalized gated tanh unit refer appendix implementation details method log quantitative results implement label smoothing modifying ground truth distribution word ygt ygt smoothing parameter set ther details found appendix table compares summarization performance different methods tifu short long dataset model outperforms state art abstractive methods rouge plexity scores utilizes pointer network tifu short methods ext oracle att chopra seass zhou drgd mmn mmn nodilated mmn nomulti mmn nongtu ppl tifu long ext oracle att chopra seass zhou drgd mmn mmn nodilated mmn nomulti mmn nongtu table summarization results measured perplexity tifu short long dataset methods att mmn xsum newsroom abs table summarization results terms newsroom abs grusky xsum narayan mmn scores referred original papers topic aware convolutional model copy words source text good strategy dataset abstractive discussed table seass shows strong performance duc gigaword dataset source text single long sentence gold summary shorter sion sufcient summarize longer articles dataset second level representation drgd based variational autoencoder latent variables capture structural patterns gold summaries idea useful similarly structured formal documents verse online text tifu dataset state art abstractive methods good model perform better extractive methods ext oracle heuristic upper bound tractive methods successful highly tifu short tifu long baselines win lose att seass drgd gold tie win lose tie table amt results tifu short long mmn baselines gold summary percentages responses turkers vote approach baselines abstractive dataset effective simply retrieve existing sentences source text performance gaps tive extractive methods larger dataset datasets paulus cohan means dataset highly abstractive table compares performance mmn newsroom abs xsum dataset report numbers original papers model outperforms rnn based abstractive methods convolutional based methods rouge scores especially trained single end end training procedure model outperforms necessitates training stages lda sults assure formal documents large vocabulary sizes multi level memory effective abstractive datasets qualitative results perform types qualitative evaluation complement limitation automatic language metrics summarization evaluation user preferences perform amazon chanical turk amt tests observe general users preferences summarization different algorithms randomly sample test examples test source text summaries generated method baseline random order ask turkers choose relevant source text obtain answers different turkers test example compare tive baselines att seass drgd gold summary gold table summarizes results amt tests validate human annotators signicantly prefer results baselines pected gold summary voted summary examples figure shows selected explore data online forums quora stackoverow subreddits acknowledgments thank chris dongjoo kim yunseok jang anonymous reviewers helpful ments work supported kakao kakao brain corporations iitp grant funded korea government msit development systems video story understanding pass video turing test gunhee kim corresponding author references mehdi allahyari seyedamin pouriyeh mehdi asse saeid safaei elizabeth trippe juan gutierrez krys kochut text summarization niques brief survey jimmy lei jamie ryan kiros geoffrey ton layer normalization stat piotr bojanowski edouard grave armand joulin tomas mikolov enriching word vectors subword information tacl asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents abstractive summarization naacl hlt sumit chopra michael auli alexander rush abstractive sentence summarization attentive recurrent neural networks hlt arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang zli goharian discourse aware attention model abstractive summarization long uments naacl hlt sergey edunov myle ott michael auli david ier marcaurelio ranzato classical structured prediction losses sequence quence learning naacl hlt jonas gehring michael auli david grangier denis yarats yann dauphin convolutional sequence sequence learning icml sebastian gehrmann yuntian deng alexander rush abstractive summarization emnlp xavier glorot yoshua bengio ing difculty training deep feedforward neural networks aistats max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies naacl hlt figure examples abstractive summary generated model baselines set source text gold summary examples abstractive summarization baselines generate summary focusing keywords text model produces summary considering keywords context thanks multi level ory present examples appendix conclusions introduced new dataset reddit tifu stractive summarization informal online text proposed novel summarization model named multi level memory networks mmn periments showed reddit tifu dataset uniquely abstractive mmn model highly effective promising ture directions rouge metrics ited correctly capture paraphrased summaries new automatic metric abstractive summarization required second iknewmyparentswouldsaynosoisnuckoutofthehouse ihadbeentalkingtomymomabouthowsadevenhearingthethemesongmademe alsoshehadseenmewatchingabunchofsadanimethemesongsandtearingupalittlesoshemusthavethoughtiwasdepressed whenigothometodaymymomwaspracticallyintears source sneaking friends house sneaking friends att sneaking watching accidentally spoiling mom watching nochip soigetreadytotakeherhomeanddefleaher source hadtogivedogbacktopossibleabusers beingaccusedofstealingthefuckingdog nogooddeedgoesunpunished tried help dog got bit got accused called dog unk charged iwas unk dog playing attention got arrested unk unk karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend nips wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun unied model extractive abstractive tion inconsistency loss acl baotian qingcai chen fangze zhu sts large scale chinese short text summarization dataset emnlp ukasz kaiser nachum aurko roy samy bengio learning remember rare events iclr chris kedzie kathleen mckeown hal daume iii content selection deep learning models summarization emnlp diederik kingma jimmy adam method stochastic optimization iclr ankit kumar ozan irsoy jonathan james bury robert english brian pierce peter ondruska ishaan gulrajani richard socher ask dynamic memory networks ural language processing icml piji wai lam lidong bing zihao wang deep recurrent generative decoder abstractive text summarization emnlp chin yew lin rouge package matic evaluation summaries tsbo peter liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser noam shazeer generating wikipedia ing long sequences iclr yishu miao phil blunsom language latent variable discrete generative models sentence compression emnlp alexander miller adam fisch jesse dodge hossein karimi antoine bordes jason weston key value memory networks directly reading documents emnlp seil sangho lee jisung kim gunhee kim read write memory network movie story understanding iccv ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural work based sequence model extractive rization documents aaai ramesh nallapati bowen zhou cicero dos santos caglar gulcehre bing xiang stractive text summarization sequence sequence rnns conll courtney napoles matthew gormley benjamin annotated gigaword van durme naacl hlt akbc wekex shashi narayan shay cohen mirella lapata details mary topic aware convolutional neural networks extreme summarization emnlp shashi narayan shay cohen mirella lapata ranking sentences extractive rization reinforcement learning hlt aaron van den oord sander dieleman heiga zen karen simonyan oriol vinyals alex graves nal kalchbrenner andrew senior koray kavukcuoglu wavenet generative model raw audio ssw aaron van den oord nal kalchbrenner lasse holt oriol vinyals alex graves ditional image generation pixelcnn decoders nips paul hoa dang donna harman duc context ipm cesc chunseong park byeongchang kim hee kim attend personalized image captioning context sequence memory works cvpr ramakanth pasunuru mohit bansal reward reinforced summarization saliency entailment naacl hlt romain paulus caiming xiong richard socher deep reinforced model abstractive summarization iclr gabriel pereyra george tucker jan chorowski ukasz kaiser geoffrey hinton larizing neural networks penalizing condent output distributions iclr alexander rush sumit chopra jason weston neural attention model abstractive sentence summarization emnlp tim salimans diederik kingma weight normalization simple reparameterization celerate training deep neural networks nips evan sandhaus new york times annotated corpus ldc abigail peter liu christopher manning point summarization generator networks acl abhishek kumar singh manish gupta vasudeva varma hybrid memnet extractive marization cikm sainbayar sukhbaatar jason weston rob fergus end end memory networks nips ilya sutskever oriol vinyals quoc sequence sequence learning neural works nips jiwei tan xiaojun wan jianguo xiao stractive document summarization based attentional neural model acl ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need nips wang wang ling neural based abstract generation opinions ments naacl hlt jason weston sumit chopra antoine bordes memory networks iclr seungjoo yoo hyojin bahng sunghyo chung junsoo lee jaehyuk chang jaegul choo oring limited data shot colorization memory augmented networks cvpr fisher vladlen koltun multi scale text aggregation dilated convolutions iclr qingyu zhou nan yang furu wei ming zhou selective encoding abstractive sentence summarization acl description initial learning rate embedding dimension demb kernel size dilation rate description grad clip encoder layers decoder layers layers memory smoothing parameter description grad clip encoder layers decoder layers layers memory smoothing parameter common congurations tifu short tifu long newsroom abs xsum table model hyperparameters experiments tifu short long newsroom abstractive subset xsum novel gram ratio gram gram gram gram dataset cnn dailymail times newsroom newsroom ext newsroom mix newsroom abs xsum tifu short tifu long table comparison novel gram ratios reddit tifu summarization datasets measure extractive bias tion dataset examples figure illustrates selected examples summary generation set source text reference summary generated summaries method baselines examples baselines generate summary focusing keywords model produces summary considering keywords context thanks multi level memory implementation details parameters initialized xavier method glorot bengio apply adam optimizer kingma ply weight normalization salimans kingma layers set learning rate clip gradient epochs divide learning rate reaches train models epochs short epochs tifu long table summarizes setting eters model experiments short long dataset newsroom abstractive subset xsum novel gram ratios table compares ratios novel grams reference summary datasets ing narayan compute ratio follows rst count number grams reference summary appear source text divide total number grams higher tio identical grams source text cnn dailymail new york times newsroom datasets example hibit low novel gram ratios respectively means words reference summary exist inside source text maries formal documents news demic papers tend expressions source documents datasets suitable extractive tion abstractive hand dataset abstractive favorable mixed methods compare novel gram ratio xsum subsets newsroom newsroom ext subset favorable tractive methods newsroom mix set newsroom abs subset favorable tive methods summarize interesting servations follows expected favorable abstractive methods higher novel gram ratio second novel gram ratios newsroom abs xsum higher dataset data sources news publications argue novel gram ratios pretty good sufcient figure examples abstractive summary generated model baselines set source text reference summary weuseaninternalmessagingapplicationsoftwareatworkwhichhasbeengreatforcommunicatingwithotherteammates alotofushavestartedusingittocomplainaboutthingswearenothappyaboutatwork thisleadsmetotodaywherejustasiamabouttogohomemymanagercallsmeintoaprivatemeetinglookingreallyupset thentheymentionedtheprogramnameandthattheyhadreceivedanemail andsuddenlyirealizedihadfuckeduponeofthequirksofthisprogramisthatwhensomeoneisofflineitemailsthemthemessage arecentlyexco theycameonlinesowestartedhavingaconversation thenanotherco workerwalkeduptomeforachatwhohasbeenhavingaroughweekandcomplainedaboutourboss whentheyfinishedtheirrant ithenmessagedmyexco theyhadgoneoffline soanemailwassenttotheiroldworkemail pastemployeesemailsgetsenttotheboss incaseimportantemailsaresenttothem soafterthemeetingistillhavemyjob igearedupandwentonmywayformyfirstride iwentallthewaybacktomyhouseandalas nophone theseareallbackroadstomygirlfriendshousesoihadtoreallygetintheundergrowthtolookformyphone itgotdarkandiheadedhomefeelingdejected andasitturnsoutitfelloffmybikeandtumbledontothesideofhisdriveway ichalkedthisupasawinandconsideredmyselflucky untilyesterday iwokeupwithpoisonivyallovermybody andwhenisayallovermybody imeanallovermybody ihavesomeonmyarms legs face andmostimportantlyallovermydick andtheworstspotofthemallisonmydick ihaveneverbeensouncomfortableinmylife imusthavehadsomeoftheoilonmyhandsandscratchedanitchdownthere source taking ride new buying new att trying good wearing cargo phone therestofthefamilyproceededtoplaycardsandbecomequiteintoxicated beingthelittleshitthatiwas andprobablystillam tookthisopportunitytoraidtheliquorcooler andmadeoffwithabottleofwine alongwithafewotherofmyunderagecousins ranofftoconsumeourloot inmostsituations thathewillsmoothtalkitoverwiththem andthatsheshouldbringthemabottleoftheirfavoritewine welljohnandjanearehavingdinneroutsidetomeettheparents shediscoversthewineisgone janethenbeginstopanic andstartstearingupthesurroundingarealookingforit orwhyjohnwouldbringcrazytothefamilyreunion janenowslipsintocompletehysteria andrunsinsidetolockherselfinthebathroom thedayafter johnconvinceshisparentstotryagain andallgoesverywell source stealing bottle getting drunk stealing wine att getting drunk making family future dinner stealing alcohol cousin parent message program work emailed private message past worker boss saying people happy sent message boss meeting att ilied boss job job new program sacked work igot sent mettingby mycurfewforthenighthadbeenmidnight thispromptedmymothertocallmeangrilyandgroggilyclaimingthatshewasmadatme commentsquestions ididso andproceededtoturnairplanemodeoff shereluctantlyagreedandihurriedlyrushedtomyroom source funky stuff accidentally sent inappropriate text girl accidentally sent mother phone answering airplane accidentally adding mothers phone phone find sleeping accidentally sent text mom sending text unk bomb
