abstractive summarization of reddit posts with multi level memory networks byeongchang kim hyunwoo kim gunhee kim department of computer science and engineering center for superintelligence seoul national university seoul korea abstract we address the problem of abstractive rization in two directions proposing a novel dataset and a new model first we collect reddit tifu dataset consisting of k posts from the online discussion forum reddit we use such informal crowd generated posts as text source in contrast with existing datasets that mostly use formal documents as source such as news articles thus our dataset could less suffer from some biases that key sentences usually locate at the beginning of the text and favorable summary candidates are already side the text in similar forms second we pose a novel abstractive summarization model named multi level memory networks mmn equipped with multi level memory to store the information of text from different levels of abstraction with quantitative evaluation and user studies via amazon mechanical turk we show the reddit tifu dataset is highly stractive and the mmn outperforms the of the art summarization models introduction abstractive summarization methods have been der intensive study yet they often suffer from ferior performance compared to extractive ods allahyari et al nallapati et al see et al admittedly by task tion abstractive summarization is more ing than extractive summarization however we argue that such inferior performance is partly due to some biases of existing summarization datasets the source text of most datasets over al hermann et al cohan et al grusky et al narayan et al originates from formal documents such as news articles which have some structural patterns of which extractive methods better take advantage in formal documents there could be a strong tendency that key sentences locate at the ning of the text and favorable summary dates are already inside the text in similar forms hence summarization methods could generate good summaries by simply memorizing keywords or phrases from particular locations of the text moreover if abstractive methods are trained on these datasets they may not show much tion see et al because they are implicitly forced to learn structural patterns kedzie et al grusky et al and narayan et al recently report similar extractive bias in existing datasets they alleviate this bias by lecting articles from diverse news publications or regarding intro sentences as gold summary different from previous approaches we pose to alleviate such bias issue by changing the source of summarization dataset we exploit generated posts from the online discussion forum reddit especially tifu subreddit which are more casual and conversational than news articles we observe that the source text in reddit does not follow strict formatting and disallows models to simply rely on locational biases for tion moreover the passages rarely contain tences that are nearly identical to the gold mary our new large scale dataset for tive summarization named as reddit tifu tains pairs of an online post as source text and its corresponding long or short summary tence these posts are written by many different users but each pair of post and summary is ated by the same user another key contribution of this work is to pose a novel memory network model named level memory networks mmn our model is equipped with multi level memory networks ing the information of source text from different levels of abstraction word level level paragraph level and document level this design is motivated by that abstractive r a l c s c v v i x r a tion is highly challenging and requires not only to understand the whole document but also to nd salient words phrases and sentences our model can sequentially read such multiple levels of mation to generate a good summary sentence most abstractive summarization methods see et al li et al zhou et al liu al cohan et al paulus et al employ sequence to sequence models sutskever et al where an rnn encoder embeds an input document and another rnn decodes a summary sentence our mmn has two major advantages over based els first rnns accumulate information in a few xed length memories at every step regardless of the length of an input sequence and thus may fail to utilize far distant information due to ishing gradient it is more critical in tion tasks since input text is usually very long words on the other hand our tional memory explicitly captures long term mation second rnns can not build tions of different ranges since hidden states are sequentially connected over the whole sequence this still holds even with hierarchical rnns that can learn multiple levels of representation in contrast our model exploits a set of convolution operations with different receptive elds hence it can build representations of not only multiple levels but also multiple ranges sentences paragraphs and the whole document our perimental results show that the proposed mmn model improves abstractive summarization formance on both our new reddit tifu and isting newsroom abs grusky et al and xsum narayan et al datasets it performs several state of the art abstractive els with architecture such as see et al zhou et al li et al we uate with quantitative language metrics plexity and rouge lin and user studies via amazon mechanical turk amt the contributions of this work are as follows we newly collect a large scale abstractive summarization dataset named reddit tifu as far as we know our work is the rst to use non formal text for abstractive rization we propose a novel model named multi level memory networks mmn to the best of our knowledge our model is the rst attempt to leverage memory networks for the tive summarization we discuss the unique updates of the mmn over existing memory networks in section with quantitative evaluation and user studies via amt we show that our model forms state of the art abstractive tion methods on both reddit tifu room abstractive subset and xsum dataset related work our work can be uniquely positioned in the text of the following three topics neural abstractive summarization many deep neural network models have been proposed for abstractive summarization one of the most dominant architectures is to employ rnn based models with attention mechanism such as rush al chopra et al ati al cohan et al hsu et al gehrmann et al in addition cent advances in deep network research have been promptly adopted for improving abstractive marization some notable examples include the use of variational autoencoders vaes miao and blunsom li et al graph based tention tan et al pointer generator els see et al self attention networks liu al reinforcement learning paulus et al pasunuru and bansal contextual agent attention celikyilmaz al and tegration with extractive models hsu et al gehrmann et al compared to existing neural methods of stractive summarization our approach is novel to replace an rnn based encoder with explicit multi level convolutional memory while based encoders always consider the whole quence to represent each hidden state our level memory network exploits convolutions to control the extent of representation in multiple els of sentences paragraphs and the whole text summarization datasets most existing marization datasets use formal documents as source text news articles are exploited the most including in duc over al word napoles et al cnn dailymail lapati et al hermann et al room grusky et al and xsum narayan al datasets cohan et al troduce datasets of academic papers from arxiv dataset posts words post words summ rottentomatoes idebate tifu short tifu long table statistics of the reddit tifu dataset pared to existing opinion summarization corpora tentomatoes and idebate wang and ling we show average and median in parentheses values tioning one of the closest works to ours may be singh et al which use a memory network for text summarization however they only deal with extractive summarization by storing dings of individual sentences into memory compared to previous memory networks our i building a mmn has four novel features multi level memory network that better abstracts multi level representation of a long document employing a dilated convolutional memory write mechanism to correlate adjacent memory cells iii proposing normalized gated tanh units to avoid covariate shift within the network and generating an output sequence without rnns reddit tifu dataset we introduce the reddit tifu dataset whose key statistics are outlined in table we collect data from reddit which is a discussion forum platform with a large number of subreddits on diverse topics and interests specically we crawl all the posts from jan to mar in the tifu dit where every post should strictly follow the posting rules otherwise they are removed thanks to the following the posts in this subreddit can be an excellent corpus for abstractive rization rule posts and titles without context will be removed your title must make an attempt to encapsulate the nature of your rule all posts must end with a summary that is descriptive of your and its consequences thus we regard the body text as source the tle as short summary and the summary as long summary as a result we make two sets of datasets tifu short and tifu long figure shows an example post of the tifu subreddit preprocessing we build a vocabulary dictionary v by choosing the most frequent v k words in the dataset figure an example post of the tifu subreddit and pubmed hu et al propose the sts dataset as a collection of chinese microblog s short text each paired with a summary however it selects only formal text posted by veried ganizations such as news agencies or government institutions compared to previous summarization datasets our dataset is novel in that it consists of posts from the online forum reddit rotten tomatoes and idebate dataset wang and ling use online text as source but they are relatively small in scale k posts of tomatoes compared to k posts of tifu short as shown in table moreover rotten tomatoes use multiple movie reviews written by different users as single source text and one sentence consensus made by another professional editor as summary thus each pair of this dataset could be less ent than that of our tifu which is written by the same user the idebate dataset is collected from short arguments of debates on controversial ics and thus the text is rather formal on the other hand our dataset contains the posts of interesting stories happened in daily life and thus the text is more unstructured and informal neural memory networks many ory network models have been proposed to prove memorization capability of neural networks kaiser et al na et al yoo et al weston et al propose one of early memory networks for language question ing qa since then many memory networks have been proposed for qa tasks sukhbaatar al kumar et al miller et al park et al propose a convolutional read memory network for personalized image iliveprettyfarfromwindsor ifinallygotbackhome andmychemistrytextbookbackinwindsor summary text summary words figure relative locations of bigrams of gold summary in the source text across different datasets dataset cnn dm nallapati al ny times sandhaus newsroom grusky et al newsroom abs grusky et al xsum narayan et al tifu short tifu long pg lead ext oracle pg lead pg oracle r l r l r l ratio r l ratio r l table comparison of rouge scores between different datasets row and methods column pg is a of the art abstractive summarization method and lead and ext oracle are extractive ones pg lead and pg oracle are the rouge l ratios of pg with lead and ext oracle respectively we report the numbers for each dataset row from the corresponding cited papers we exclude any urls unicodes and special acters we lowercase words and normalize digits to subreddit names and user ids are replaced with and token respectively we use package to strip markdown format and to tokenize words common prexes of summary sentences tifu by are trimmed we do not take oov words into consideration since our vocabulary with size k covers about of word frequencies in our dataset we set the maximum length of a ument as we exclude the gold summaries whose lengths are more than and for short and tifu long respectively they amount to about k posts in both datasets less than and we use these maximum lengths based on previous datasets words on average per summary in gigaword duc and cnn dailymail datasets respectively we domly split the dataset into for training for test abstractive properties of reddit tifu we discuss some abstractive characteristics found in reddit tifu dataset compared to existing marization datasets based on news articles weak lead bias formal documents including news articles tend to be structured to emphasize key information at the beginning of the text on the other hand key information in informal online text data are more spread across the text figure plots the density histogram of the relative tions of bigrams of gold summary in the source text in the cnn dailymail and newsroom the bigrams are highly concentrated on the front parts of documents contrarily our reddit tifu dataset shows rather uniform distribution across the text this characteristic can be also seen from the rouge score comparison in table the lead baseline simply creates a summary by selecting the rst few sentences or words in the document thus a high score of the lead baseline implicates a strong lead bias the lead scores are the lowest in our tifu dataset in which it is more difcult for models to simply take advantage of locational bias for the summary strong abstractness besides the locational bias news articles tend to contain wrap up tences that cover the whole article and they its ten have resemblance to its gold summary existence can be measured by the score of the ext oracle baseline which creates a summary by selecting the sentences with the highest average score of thus it can be viewed as an upper bound for extractive models narayan al nallapati et al in table the rouge scores of the ext oracle are the lowest in our tifu dataset it means that the sentences that are similar to gold summary scarcely exist inside the source text in dailymailnewsroom absxsumreddit tifurelative figure illustration of the proposed multi level memory network mmn model word at a time by extracting relevant information from memory cells in response to previously erated words the input of the model is a source text xi xn and the output is a quence of summary words yt yt each of which is a symbol from the dictionary text embedding online posts include lots of morphologically ilar words which should be closely embedded thus we use the fasttext bojanowski al trained on the common crawl corpus to initialize the word embedding matrix wemb we use the same embedding matrix wemb for both source text and output sentences that is we represent a source text in a distributional space as i n i wembxi where xi is a one hot vector for i th word in the source text likewise output words is embedded as and by i and t t construction of multi level memory s s as shown in figure the multi level memory i n network takes the source text embedding as an input and generates s number of memory tensors ma c as output where superscript a and c denote input and output memory sentation respectively the multi level memory network is motivated by that when human stand a document she does not remember it as a single whole document but ties together several levels of abstraction word level level paragraph level and document level that figure comparison between the gated linear unit gehring et al and the proposed normalized gated tanh unit our dataset this property forces the model to be trained to focus on comprehending the entire text instead of simply nding wrap up sentences finally pg lead and pg oracle in table are the rouge l ratios of pg with lead and ext oracle respectively these metrics can quantify the dataset according to the degree of culty for extractive methods and the suitability for abstractive methods respectively high scores of the tifu dataset in both metrics show that it is potentially an excellent benchmark for evaluation of abstractive summarization systems multi level memory networks mmn figure shows the proposed multi level memory network mmn model the mmn memorizes the source text with a proper representation in the memory and generates a summary sentence one multi level sequence convweight normsigmoidconvweight normtanhlayer is we generate s sets of memory tensors each of which associates each cell with different ber of neighboring word embeddings based on the level of abstraction to build memory slots of such multi level memory we exploit a multi layer cnn as the write network where each layer is chosen based on the size of its receptive eld however one issue of convolution is that large receptive elds require many layers or large lter sizes for example stacking layers with a lter size of results in a receptive eld size of each output depends on input words in order to grow the receptive eld without increasing the computational cost we exploit the dilated lution yu and koltun oord et al for the write network memory writing with dilated convolution in dilated convolution the lter is applied over an area larger than its length by skipping input values with a certain gap formally for a d n length input and a lter w k the dilated convolution operation f on s ements of a sequence is dened as k s where is the dilation rate k is the lter size s i accounts for the direction of dilation and w and are the parameters of the lter with the dilated convolution reduces to a regular convolution ing a larger dilation enables a single output at the top level to represent a wider range of input thus effectively expanding the receptive eld to the embedding of a source text we recursively apply a series of dilated convolutions f rn we denote the output of the l th convolution layer as n normalized gated tanh units each tion is followed by our new activation of ized gated tanh unit ngtu which is illustrated in figure and layer normalization ba al this mixed normalization improves earlier work of gehring et al where only weight malization is applied to the glu as in figure it tries to preserve the variance of activations throughout the whole network by scaling the however we put of residual blocks by serve that this heuristic does not always preserve the variance and does not empirically work well in our dataset contrarily the proposed ngtu not only guarantees preservation of activation ances but also signicantly improves the mance multi level memory instead of using only the last layer output of cnns we exploit the outputs of multiple layers of cnns to construct s sets of memories for example memory constructed from the th layer whose receptive eld is may have sentence level embeddings while ory from the th layer whose receptive eld is may have document level embeddings we obtain each s th level memory ma c by bling key value memory networks miller et al s ma s mc s s and mc s rn are input and recall that ma output memory matrix respectively cates an index of convolutional layer used for the s th level memory for example if we set s and m we make three level ries each of which uses the output of the rd and th convolution layer respectively to output memory representation mc s we add the document embedding as a skip connection state based sequence generation we discuss how to predict the next word at time step t based on the memory state and ously generated words figure visualizes the overall procedure of decoding l dl l we rst apply max pooling to the output of the last layer of the encoder network to build a whole document embedding dwhole where is a sigmoid is the element wise plication and f l g denote the lter and gate for l th layer dilated convolution respectively and f l the ngtu is an extension of the existing gated tanh units gtu oord et al by ing weight normalization salimans and kingma dwhole dl n the decoder is designed based on wavenet oord et al that uses a series of causal lated convolutions denoted by t t as we globally condition dwhole to obtain dings of previously generated words ol f l hl t wl a hl g t hl t where hl are the lter and gate hidden state spectively and learnable parameters are wl and wl wembyt we set the level of the decoder network to l for tifu short and l for tifu long we initialize a next we generate s number of query vectors s at time to our memory network as qol t qs q and bs each of these query vectors qs t bs q q s where ws is fed into the attention function of each level of memory as in vaswani et al the attention function is ms ot softmax mc s t ma qs demb where we set demb for the embedding mension and ms ot next we obtain the output word probability st ot ms ot ol where wo finally we lect the word with the highest probability argmaxsv st unless is an eos token we repeat generating the next word by feeding into the output convolution layer of training we use the softmax cross entropy loss from mated yt to its target ygt however it forces the model to predict extremes zero or one to tinguish among the ground truth and alternatives the label smoothing alleviates this issue by acting as a regularizer that makes the model less dent in its prediction we smooth the target bution with a uniform prior distribution u pereyra et al edunov et al vaswani et al thus the loss over the training set d is experiments experimental setting evaluation metrics we evaluate the rization performance with two language metrics perplexity and standard rouge scores lin we remind that lower perplexity and higher rouge scores indicate better performance datasets in addition to reddit tifu we also evaluate on two existing datasets abstractive set of newsroom grusky et al and xsum narayan al these are suitable marks for evaluation of our model in two aspects first they are specialized for abstractive rization which meets well the goal of this work second they have larger vocabulary size k k than reddit tifu k and thus we can evaluate the learning capability of our model baselines we compare with three abstractive summarization methods one basic model two heuristic extractive methods and variants of our model we choose pg see et al seass zhou et al drgd li et al as the state of the art methods of abstractive marization we test the attention based model denoted as att chopra et al as heuristic extractive methods the uses the rst sentence in the text as summary and the ext oracle takes the sentence with the highest average score of l with the gold summary in the text thus ext oracle can be viewed as an upper bound for extractive methods we also test variants of our method to validate the contribution of each component we exclude one of key components from our model as follows with tional convolutions instead with no multi level memory with ing gated linear units gehring al that is quanties the improvement by the dilated convolution assesses the effect of multi level memory and dates the normalized gated tanh unit please refer to the appendix for implementation details of our method l log quantitative results we implement label smoothing by modifying the ground truth distribution for word ygt t to be t and for ygt t where is a smoothing parameter set to ther details can be found in the appendix table compares the summarization performance of different methods on the tifu short long dataset our model outperforms the state of art abstractive methods in both rouge and plexity scores pg utilizes a pointer network tifu short methods ext oracle att chopra et al pg see et al seass zhou et al drgd li et al mmn mmn nodilated mmn nomulti mmn nongtu ppl r l n a n a tifu long n a n a ext oracle att chopra et al pg see et al seass zhou et al drgd li et al mmn mmn nodilated mmn nomulti mmn nongtu table summarization results measured by perplexity and l on the tifu short long dataset methods att pg t mmn ours xsum newsroom abs r l r l table summarization results in terms of l on newsroom abs grusky et al and xsum narayan et al except mmn all scores are referred to the original papers t is the topic aware convolutional model to copy words from the source text but it may not be a good strategy in our dataset which is more abstractive as discussed in table seass shows strong performance in duc and gigaword dataset in which the source text is a single long sentence and the gold summary is its shorter sion yet it may not be sufcient to summarize much longer articles of our dataset even with its second level representation drgd is based on the variational autoencoder with latent variables to capture the structural patterns of gold summaries this idea can be useful for the similarly structured formal documents but may not go well with verse online text in the tifu dataset these state of the art abstractive methods are not as good as our model but still perform better than extractive methods although the ext oracle heuristic is an upper bound for tractive methods it is not successful in our highly tifu short tifu long baselines win lose att pg seass drgd gold tie win lose tie table amt results on the tifu short long between our mmn and four baselines and gold summary we show percentages of responses that turkers vote for our approach over baselines abstractive dataset it is not effective to simply retrieve existing sentences from the source text moreover the performance gaps between tive and extractive methods are much larger in our dataset than in other datasets see et al paulus et al cohan et al which means too that our dataset is highly abstractive table compares the performance of our mmn on newsroom abs and xsum dataset we report the numbers from the original papers our model outperforms not only the rnn based abstractive methods but also the convolutional based methods in all rouge scores especially even trained on single end to end training procedure our model outperforms t which necessitates two training stages of lda and these sults assure that even on formal documents with large vocabulary sizes our multi level memory is effective for abstractive datasets qualitative results we perform two types of qualitative evaluation to complement the limitation of automatic language metrics as summarization evaluation user preferences we perform amazon chanical turk amt tests to observe general users preferences between the summarization of different algorithms we randomly sample test examples at test we show a source text and two summaries generated by our method and one baseline in a random order we ask turkers to choose the more relevant one for the source text we obtain answers from three different turkers for each test example we compare with four tive baselines att pg seass and drgd and the gold summary gold table summarizes the results of amt tests which validate that human annotators signicantly prefer our results to those of baselines as pected the gold summary is voted the most summary examples figure shows selected explore the data in other online forums such as quora stackoverow and other subreddits acknowledgments we thank chris dongjoo kim yunseok jang and the anonymous reviewers for their helpful ments this work was supported by kakao and kakao brain corporations and iitp grant funded by the korea government msit no development of qa systems for video story understanding to pass the video turing test gunhee kim is the corresponding author references mehdi allahyari seyedamin pouriyeh mehdi asse saeid safaei elizabeth d trippe juan b gutierrez and krys kochut text summarization niques a brief survey in jimmy lei ba jamie ryan kiros and geoffrey e ton layer normalization in stat piotr bojanowski edouard grave armand joulin and tomas mikolov enriching word vectors with subword information in tacl asli celikyilmaz antoine bosselut xiaodong he and yejin choi deep communicating agents for abstractive summarization in naacl hlt sumit chopra michael auli and alexander m rush abstractive sentence summarization with in attentive recurrent neural networks hlt arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and zli goharian a discourse aware attention model for abstractive summarization of long uments in naacl hlt sergey edunov myle ott michael auli david ier and marcaurelio ranzato classical structured prediction losses for sequence to quence learning in naacl hlt jonas gehring michael auli david grangier denis yarats and yann n dauphin convolutional sequence to sequence learning in icml sebastian gehrmann yuntian deng and alexander rush bottom up abstractive summarization in emnlp xavier glorot and yoshua bengio ing the difculty of training deep feedforward neural networks in aistats max grusky mor naaman and yoav artzi newsroom a dataset of million summaries with diverse extractive strategies in naacl hlt figure examples of abstractive summary generated by our model and baselines in each set we too show the source text and gold summary examples of abstractive summarization baselines often generate the summary by mostly focusing on some keywords in the text while our model produces the summary considering both keywords and the whole context thanks to multi level ory we present more examples in the appendix conclusions we introduced a new dataset reddit tifu for stractive summarization on informal online text we also proposed a novel summarization model named multi level memory networks mmn periments showed that the reddit tifu dataset is uniquely abstractive and the mmn model is highly effective there are several promising ture directions first rouge metrics are ited to correctly capture paraphrased summaries for which a new automatic metric of abstractive summarization may be required second we can alsoshehadseenmewatchingabunchofsadanimethemesongsandtearingupalittlesoshemusthavethoughtiwasdepressed sneaking out of my friends house last sneaking out of my friends att sneaking out of not watching my accidentally spoiling my mom watching a nochip hadtogivedogbacktopossibleabusers beingaccusedofstealingthefuckingdog tried to help a dog got a bit and got accused of called a dog a unk might get charged with iwas a unk dog and i was nt playing attention and got arrested for being a unk unk karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom teaching chines to read and comprehend in nips wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun a unied model for extractive and abstractive tion using inconsistency loss in acl baotian hu qingcai chen and fangze zhu sts a large scale chinese short text summarization dataset in emnlp ukasz kaiser or nachum aurko roy and samy bengio learning to remember rare events in iclr chris kedzie kathleen mckeown and hal daume iii content selection in deep learning models of summarization in emnlp diederik kingma and jimmy ba adam a method for stochastic optimization in iclr ankit kumar ozan irsoy jonathan su james bury robert english brian pierce peter ondruska ishaan gulrajani and richard socher ask me anything dynamic memory networks for ural language processing in icml piji li wai lam lidong bing and zihao wang deep recurrent generative decoder for abstractive text summarization in emnlp chin yew lin rouge a package for matic evaluation of summaries in tsbo peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer generating wikipedia by ing long sequences in iclr yishu miao and phil blunsom language as a latent variable discrete generative models for sentence compression in emnlp alexander miller adam fisch jesse dodge hossein karimi antoine bordes and jason weston key value memory networks for directly reading documents in emnlp seil na sangho lee jisung kim and gunhee kim a read write memory network for movie story understanding in iccv ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural work based sequence model for extractive rization of documents in aaai ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang stractive text summarization using sequence sequence rnns and beyond in conll courtney napoles matthew gormley and benjamin in annotated gigaword van durme naacl hlt akbc wekex shashi narayan shay cohen and mirella lapata do nt give me the details just the mary topic aware convolutional neural networks for extreme summarization in emnlp shashi narayan shay b cohen and mirella lapata ranking sentences for extractive rization with reinforcement learning in hlt aaron van den oord sander dieleman heiga zen karen simonyan oriol vinyals alex graves nal kalchbrenner andrew senior and koray kavukcuoglu wavenet a generative model for raw audio in ssw aaron van den oord nal kalchbrenner lasse holt oriol vinyals alex graves al ditional image generation with pixelcnn decoders in nips paul over hoa dang and donna harman duc in context in ipm cesc chunseong park byeongchang kim and hee kim attend to you personalized image captioning with context sequence memory works in cvpr ramakanth pasunuru and mohit bansal reward reinforced summarization with saliency and entailment in naacl hlt romain paulus caiming xiong and richard socher a deep reinforced model for abstractive summarization in iclr gabriel pereyra george tucker jan chorowski ukasz kaiser and geoffrey hinton larizing neural networks by penalizing condent output distributions in iclr alexander m rush sumit chopra and jason weston a neural attention model for abstractive sentence summarization in emnlp tim salimans and diederik p kingma weight normalization a simple reparameterization to in celerate training of deep neural networks nips evan sandhaus new york times annotated corpus in ldc abigail see peter j liu and christopher d manning get to the point summarization with generator networks in acl abhishek kumar singh manish gupta and vasudeva varma hybrid memnet for extractive marization in cikm sainbayar sukhbaatar jason weston rob fergus al end to end memory networks in nips ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural works in nips jiwei tan xiaojun wan and jianguo xiao stractive document summarization with a based attentional neural model in acl ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in nips lu wang and wang ling neural based abstract generation for opinions and ments in naacl hlt jason weston sumit chopra and antoine bordes memory networks in iclr seungjoo yoo hyojin bahng sunghyo chung junsoo lee jaehyuk chang and jaegul choo oring with limited data few shot colorization via memory augmented networks in cvpr fisher yu and vladlen koltun multi scale text aggregation by dilated convolutions in iclr qingyu zhou nan yang furu wei and ming zhou selective encoding for abstractive sentence summarization in acl description initial learning rate embedding dimension demb kernel size k dilation rate d description grad clip of encoder layers of decoder layers layers used for memory m smoothing parameter description grad clip of encoder layers of decoder layers layers used for memory m smoothing parameter common congurations tifu short tifu long newsroom abs xsum table model hyperparameters in experiments on tifu short long newsroom abstractive subset and xsum novel n gram ratio gram gram gram gram dataset cnn dailymail ny times newsroom newsroom ext newsroom mix newsroom abs xsum tifu short tifu long table comparison of novel n gram ratios between reddit tifu and other summarization datasets measure to nd extractive bias in the tion dataset c more examples figure illustrates selected examples of summary generation in each set we show a source text a reference summary and generated summaries by our method and baselines in the examples while baselines generate summary by mostly focusing on some keywords our model produces summary considering both keywords and the whole context thanks to the multi level memory a implementation details all the parameters are initialized with the xavier method glorot and bengio we apply the adam optimizer kingma and ba with and we ply weight normalization salimans and kingma to all layers we set learning rate to and clip gradient at at every epochs we divide learning rate by until it reaches we train our models up to epochs for short and epochs for tifu long table summarizes the setting of eters for our model in all experiments on short long dataset newsroom abstractive subset and xsum b novel n gram ratios table compares the ratios of novel n grams in the reference summary between datasets ing see et al narayan et al we compute this ratio as follows we rst count the number of n grams in the reference summary that do not appear in the source text and divide it with the total number of n grams the higher the tio is the less the identical n grams are in the source text the cnn dailymail new york times newsroom datasets all for example hibit low novel gram ratios as respectively this means that about of the words in reference summary already exist inside the source text it is due to that the maries from formal documents news and demic papers tend to have same expressions with the source documents therefore these datasets may be more suitable for extractive tion than abstractive one on the other hand our dataset is more abstractive favorable for mixed methods we also compare the novel n gram ratio for xsum and three subsets of newsroom i newsroom ext a subset favorable for tractive methods newsroom mix a set and newsroom abs a subset favorable for tive methods we summarize two interesting servations as follows first as expected the more favorable for abstractive methods is the higher novel n gram ratio is second novel n gram ratios of newsroom abs and xsum are higher than those of our dataset even though their data sources are news publications thus we argue that novel n gram ratios are pretty good but not a sufcient figure examples of abstractive summary generated by our model and baselines in each set we too show the source text and reference summary weuseaninternalmessagingapplicationsoftwareatworkwhichhasbeengreatforcommunicatingwithotherteammates thentheymentionedtheprogramnameandthattheyhadreceivedanemail andsuddenlyirealizedihadfuckeduponeofthequirksofthisprogramisthatwhensomeoneisofflineitemailsthemthemessage arecentlyexco theycameonlinesowestartedhavingaconversation thenanotherco workerwalkeduptomeforachatwhohasbeenhavingaroughweekandcomplainedaboutourboss whentheyfinishedtheirrant ithenmessagedmyexco theyhadgoneoffline soanemailwassenttotheiroldworkemail pastemployeesemailsgetsenttotheboss incaseimportantemailsaresenttothem soafterthemeetingistillhavemyjob nophone theseareallbackroadstomygirlfriendshousesoihadtoreallygetintheundergrowthtolookformyphone itgotdarkandiheadedhomefeelingdejected andasitturnsoutitfelloffmybikeandtumbledontothesideofhisdriveway ichalkedthisupasawinandconsideredmyselflucky untilyesterday iwokeupwithpoisonivyallovermybody andwhenisayallovermybody imeanallovermybody ihavesomeonmyarms legs face andmostimportantlyallovermydick andtheworstspotofthemallisonmydick ihaveneverbeensouncomfortableinmylife imusthavehadsomeoftheoilonmyhandsandscratchedanitchdownthere taking a ride on my new buying my new att trying to be a good wearing my cargo for a my phone therestofthefamilyproceededtoplaycardsandbecomequiteintoxicated me beingthelittleshitthatiwas andprobablystillam tookthisopportunitytoraidtheliquorcooler andmadeoffwithabottleofwine i alongwithafewotherofmyunderagecousins ranofftoconsumeourloot now inmostsituations however thathewillsmoothtalkitoverwiththem andthatsheshouldbringthemabottleoftheirfavoritewine welljohnandjanearehavingdinneroutsidetomeettheparents shediscoversthewineisgone janethenbeginstopanic andstartstearingupthesurroundingarealookingforit johnconvinceshisparentstotryagain andallgoesverywell stealing a bottle of getting drunk and stealing a wine att getting drunk and making a family my future dinner stealing alcohol from my cousin s parent s a message program at work emailed a private message between a past co worker and myself to my boss saying how people where not happy with sent a message to my boss and now i m in a meeting with my att ilied about my boss to get my job and now i m in a job with a new a program that sacked from work and igot sent to a mettingby my thispromptedmymothertocallmeangrilyandgroggilyclaimingthatshewasmadatme commentsquestions ididso andproceededtoturnairplanemodeoff did some funky stuff with my accidentally sent an inappropriate text to a girl and then accidentally sent it to my mother s phone and now answering an airplane call and accidentally adding my mothers phone and her phone to find out she was sleeping on accidentally sent a text to my mom that i was sending her a text from the unk bomb
