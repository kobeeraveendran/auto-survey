towards information rich logical text generation with knowledge enhanced neural models hao bin wei and zhiwen polytechnical university xian china research beijing china nwpu edu cn guob edu cn com r a m i a s c v v i x r a abstract text generation system has made massive ing progress contributed by deep learning niques and has been widely applied in our life however existing end to end neural models suffer from the problem of tending to generate tive and generic text because they can not ground input context with background knowledge in der to solve this problem many researchers begin to consider combining external knowledge in text generation systems namely knowledge enhanced text generation the challenges of enhanced text generation including how to select the appropriate knowledge from large scale edge bases how to read and understand extracted knowledge and how to integrate knowledge into generation process this survey gives a hensive review of knowledge enhanced text eration systems summarizes research progress to solving these challenges and proposes some open issues and research directions introduction text generation also known as natural language generation nlg aims to make machines express like humans which has the capability to produce smooth meaningful and mative textual contents from the original template based and statistical methods to the deep learning based methods text generation has attracted the attention of massive searchers and made many remarkable advances depending on the data sources text generation can be divided into to text data to text and image to text generation we focus on the text to text generation in this survey because it still has massive research challenges and a wider range of cations text to text generation takes natural language text as input understands the input text to obtain semantic tations and generates corresponding output text according to task requirements most advances in text generation in recent years are ted from deep neural networks such as recurrent neural work rnn elman and transformer vaswani et al contact author with the help of these technologies text generation systems have been able to generate smooth topic consistent and even personalized text however existing text tion systems lack interactions with the real world and have little access to the external knowledge making them easy to generate the short and meaningless text we humans are constantly acquiring understanding and storing knowledge and will automatically combine our knowledge to understand the current situation in communicating writing or reading which is a huge challenge faced by text generation systems to generate more informative diverse and logical text text generation systems must have the ability to combine external knowledge which is a promising research direction fig is an example of a dialogue system with or without sense knowledge where we can see that combining with monsense knowledge including structured knowledge graph kg composed of triples and unstructured knowledge base kb composed of natural language text the dialogue agent can generate more informative diverse and logical response there are many researchers from both academia and dustry have begun to explore knowledge enhanced text eration system by incorporating different types of knowledge due to the complexity of the real world the scale of edge base is usually extremely large how to extract the most relevant knowledge from massive knowledge facts based on the simple text input is a huge challenge because of the sity of natural languages meanwhile how to effectively derstand the extracted knowledge and integrate it into neural network models to facilitate text generation is also a difcult problem to our best knowledge this is the rst survey to summarize knowledge enhanced text generation systems in detail to sum up we summarize contributions of our work as follows we briey introduce the development process of text generation formalize the denition of enhanced text generation and analyze a number of key challenges in this research area we summarize the current in knowledge enhanced text generation systems by tematically categorizing the state of the art works cording to research challenges research progress we propose some open issues and research directions for the reference of the community figure two dialogue examples with the rst line or combining external knowledge background and formalized denition in this section we briey introduce the development of text generation formalize the denition of general and knowledge enhanced text generation and summarize some research challenges nnlm and rnnlm the neural network language model nnlm bengio et al is rstly proposed for text generation tasks which leverages neural networks to model language representation nnlm maps the input into a low dimensional space thereby reducing the parameters of the model given the text quence sn and a parametrized language model p nnlm can be proximated as eq where t represents the t th word in sn p p nnlm has achieved promising results in experiments but as a typical feedforward neural network the length of the text it receives must be xed in advance so it can not capture context information of variable length to solve this problem the rnn language model rnnlm mikolov et al is proposed which is an auto regressive language model that utilizes rnn to encode variable length inputs into vector resentations this process can be formulated as eq p p n theoretically rnnlm can model text sequences of any length however due to the xed dimension hidden vector the information in excessively long context may not be stored efciently therefore variants of rnn including long term memory lstm and gated recurrent unit gru have been utilized to improve the performance of rnnlm which perform well in capturing long term dependencies encoder decoder framework the length of input and output text of above language models are equal but there are many cases where the length of them are different e the question and answer in the qa system to deal with this problem the encoder decoder framework sutskever et al is proposed composed of an encoder and a decoder the encoder uses rnn to encode the input sequence x xm into the intermediate tics representation c where m is the length of input sequence the decoder utilizes another rnn to generate the t th output word yt according to c and this process can be dened as eq where y xn and n is the length of the output text sequence y t t encoder decoder is a very general computing framework whose specic model in the encoder and decoder can be justed based on tasks it has been widely used in various text generation tasks since proposed such as machine translation text summarization and dialogue system knowledge enhanced text generation in addition to the encoder decoder framework various ral network models and algorithms have been applied to text generation tasks to improve the quality of generated text the development of text generation systems requires the ated text to be more informative diverse and logical rather than simple smooth or surface correct combined with ternal knowledge text generation systems can deeply stand the input and generate more informative more sistent with the logic of human expression and with less common sense mistakes given a set of knowledge facts f k where each fi may be a text sequence or a knowledge triple and k is the number of facts the knowledge types challenges structured kg representing knowledge graphs with vectors incorporating knowledge vectors into models unstructured kb extracting knowledge reading and understanding knowledge existing solutions word embedding based knowledge representation wang et al distance based knowledge representation gunel et al graph attention based knowledge representation guan et al concatenated with input vector liu et al attention based knowledge graph decoder qiu al gcn based knowledge incorporation de cao et al keyword matching ghazvininejad et al semantical level knowledge extraction lian et al memory network based knowledge understanding madotto et al transformer based knowledge understanding kim et al rl based knowledge understanding xu et al table a summary of challenges in knowledge enhanced text generation system knowledge enhanced text generation model can be lated as eq f f y t t there are two forms of external knowledge that is tured kg and unstructured kb the kg is essentially a mantic network containing multiple types of entities and lations with the form of head relation tile where head and tile are different entities entities refer to things in the real world and relations express connections between entities the kb has no xed form and usually store edge related to specic concepts in textual sequence form there are many challenges in combining external knowledge into text generation systems as shown in table when combining structured knowledge the rst challenge is how to obtain vector representations of knowledge triples as ceptable inputs to neural network models neural network models need input data with vector form while the tion stored in structured kb is symbolized it is a difcult problem to map these symbols into low dimensional dense vector spaces and how to incorporate knowledge vectors as additional input into neural network models to guide the eration process is also a challenge because knowledge facts in the unstructured kb are stored with the form of natural language text mapping knowledge facts to vector tions does not pose a research challenge the rst challenge in combining unstructured knowledge is how to extract the most appropriate knowledge from massive knowledge facts due to the possible semantic duplication of different edge the understanding of sentence level natural language text is a long term research challenge in nlp so how to ciently read and understand textual knowledge facts to tegrate them into generation systems is another challenge in combining unstructured knowledge researchers have made great efforts to address these challenges which will be tailed summarized in following sections text generation with structured kg structured kgs can store a wider range of knowledge types but less information due to its simple representation of triples however its unique symbolic storage form is quite different from vectors required by neural network models therefore how to map knowledge triples into low dimensional vector representations and efciently incorporate knowledge vectors into neural network models are key directions of the research which will be summarized in this section word embedding based knowledge representation the simplest way to obtain vector representations of tured kbs is to directly treat entities and relations in edge triples as common words and then use word embedding methods to obtain vector representations which has been widely used at the initial research stage in order to comprehensive understand the content of ument in reading comprehension tasks mihaylov et al haylov and frank utilize a bigru to encode edge triples as text sequences to get the key value memory and then the key value retrieval algorithm selects a single sum of weighted fact representations for each token to hance the understanding of the document context wang et al wang et al propose a kb based single relation qa system the entity linking module determines the optimal subject in the question to select knowledge facts which will be encoded into vectors using a bilstm the relation tion module calculates similarity scores of each question and its relation candidates to select the triple with highest score to answer the question distance based knowledge representation there is a gap between kg of symbolic form and vector resentation so directly encoding entities as common words may lead to certain information loss the concept of edge representation learning is proposed to represente entities and relations in low dimensional dense vector spaces for culation and reasoning bordes et al bordes et al propose the transe algorithm which uses the translation variant phenomenon of word vector and distance based ing function to obtain vector representations of entities and relations which can be better integrated with text generation system to provide more powerful knowledge support moussallem et al moussallem et al incorporate external knowledge into machine translation system to prove the quality of results knowledge facts are linked based on the translated document and encoded by the modied trane and then concatenated vectors into the internal tors of nmt embeddings as the input of the decoder gune et al gunel et al incorporate entity level knowledge from knowledge graph into transformer encoder decoder chitecture to produce coherent summaries extracted entities are initialized with the pretrained algorithm to get the vector representations and then are fed into separate head attention channel for generating summaries graph attention based knowledge representation to obtain more accurate vector representations the graph tention algorithm zhou et al is proposed which uses relation information to aggregate entities to generate new tity representations the attention mechanism makes better use of the interconnections between graph entities and guishes the hierarchy of connections which can enhance the effective information needed in text generation tasks to generate more informative responses zhou et al zhou et al propose the static graph attention mechanism to generate a static representation for a retrieved graph to augment the semantics of input words the dynamic graph attention mechanism is designed for attentively reading all knowledge triples for text generation guan et al guan et al propose an incremental encoding scheme for story ending generation to mine context clues hidden in the story context and adopt graph attention and contextual attention respectively to obtain the graph vectors the multi source attention mechanism combines commonsense knowledge for facilitating story comprehension to generate coherent and sonable story endings for comprehensive understanding paragraph level ment qiu et al qiu al construct sub graphs for entities to capture the structural information in the kb resentations of nodes in a sub graph are updated using the graph attention network with the document context which are combined with document representations to generate nal answer concatenating knowledge with input vector after vector representations of knowledge triples are tained the next challenge facing by knowledge enhanced text generation systems is how to integrate knowledge vectors into neural network models the simplest method is to directly concatenate knowledge vectors with input vectors to enhance vector representations of the input and then send them into the decoding stage for text generation for instance young et al young et al extract triples according to entity keys in the input to form a text sequence which is encoded by lstm to obtain vector sentations then knowledge vectors are added with the input vector to calculate the degree of correlation with the tive responses liu et al liu et al propose the idea of entity diffusion which means that conversation usually drifts from one entity to another the similarity between extracted entities and other entities is calculated to retrieve relevant tities word vectors of entities and relations are averaged to obtain vector representations of each triple which is nated with the input vector to guide the response generation attention based knowledge graph decoder the attention mechanism can focus on the important contents among the numerous input information and select the key formation while ignoring other unimportant through the tention mechanism knowledge enhanced text generation tems can focus on most critical parts of knowledge instead of feeding all the selected knowledge directly into neural works to produce more informative text for example moon et al moon et al construct kg embeddings to represent entities with algorithm and aggregate input contexts with relevant entities to ate candidate kg entities efciently an attention based graph decoder is proposed to walk an optimal path in a large kg to select candidate entities for paragraph level essay generation yang et al yang et al present a memory augmented neural model to bine commonsense knowledge knowledge concepts are tracted using input topics as query and stored into a memory matrix the model will attend on the memory and ically update it to incorporate information of the generated text for diverse and topic consistent essay generation cel et al koncel kedziorski et al introduce a graph transforming encoder to leverage the relational structure of knowledge graphs to encode knowledge graphs into vectors and then the decoder will attend on the input title and edge graphs to generate informative and topic coherent text gcn based knowledge incorporating gcn kipf and welling is a natural extension of cnn in the graph domain which learns node feature and structure information in the end to end manner it is a very powerful neural network framework on graphs so it has begun to attract researchers attention in text generation systems combining structured knowledge graphs de et al de cao et al consider question answering as an inference problem on a graph of the document tion nodes in the graph are entities appeared in the ment and edges in the graph represent the relations between entities the gcn is used to capture reasoning chains by propagating local contextual information along edges to form multi step reasoning for generating answers lv et al et al extract evidence from knowledge graph and make predictions based on the evidence the graph based contextual word representation learning module is used to dene the distance between words for learning better textual word representations using graph structural tion the graph based inference module is applied to encode neighbor information into the representations of nodes using gcn and aggregate evidence to generate answers text generation with unstructured kb unstructured kbs are composed of natural language text lated to concepts which express rich semantic information because of its textual form the unstructured kb can be ily combined with text generation systems whose input is text sequences however the scale of knowledge base is usually extremely huge which contains too redundant information therefore how to extract the knowledge required by text generation systems and efciently understand the knowledge to integrate it into the generation process are main research challenges there have been many researches of grounded text generation with unstructured kb which will be discussed in detail in this section key word matching based knowledge extraction the simplest way to extract knowledge from unstructured kb is the key matching method using words in the input as keywords this method is simple and direct but can only extract knowledge according to the surface information of words and can not combine deeper semantic information into the knowledge extraction for instance ghazvininejad et al ghazvininejad et al rstly introduce external knowledge into the fully driven neural conversation model given the dialogue history relevant knowledge facts are identied by keyword matching method using entities in the context as keys then retrieved know facts are fed into the memory network to retrieve and weight facts based on the input and dialogue context to hance the semantic representation of the input semantical level knowledge extraction the simple key word matching method may make it hard to accurately select the required knowledge due to the less information contained in single word therefore many searchers focus on the knowledge selection in the semantic level and put forward many novel ideas the same query in human conversation may be related to different responses so different knowledge may be utilized lian et al to solve this problem lian et al pose the idea of the posterior distribution over knowledge which is calculated from both the input query and response to provide more accurate guidance on knowledge selection by minimizing the distance between the prior and the posterior distribution over knowledge the prior distribution can be lized to select appropriate knowledge so as to generate mative responses even the actual response is unknown ren et al ren et al propose a global to local edge selection mechanism using the global perspective to select appropriate background knowledge a topic tion vector is learned from the dialogue context and external knowledge by a distantly supervised learning schema to lect the most likely text fragments the vector is then used to guide the local knowledge selection module at decoding stage to generate uency and appropriate responses zhao et al zhao et al represent a disentangled response coder to separate parameters relying on knowledge grounded dialogues from the whole model to solve the problem of ing knowledge grounded training data the decoder is posed of three components including language model to generate common words context processor to generate text words and knowledge processor to generate words from knowledge document by a hierarchical attention mechanism memory network based knowledge understanding after extracting relevant knowledge facts the most import is to read and understand the textual knowledge to enhance the input representation and guide text generation memory work sukhbaatar et al is proposed to improve the poor memory ability of rnn using external memory ponent to realize the storage of long term memory because of its powerful memory storage capacity memory network is widely used in knowledge enhanced text generation systems to retrieve read and condition on external knowledge madott et al madotto et al augment the memory network with a sequential generative architecture edge facts are fed into memory network to update the input query vector a gru is used as a dynamic query generator to generate the output words which will produce two tion to decide whether to generate common words or memory contents in order to carefully read and understand the trieved knowledge dinan et al dinan et al combine the memory network and transformer to encode the selected knowledge and the dialogue context to get the higher level semantic representation then the dot product attention tween the knowledge and context is performed to retrieved most relevant knowledge for generating the next response transformer based knowledge understanding transformer is an emerging sequential model which has caused great repercussions in nlp it exceeds rnn in tic information abstraction long term feature extraction and task comprehensive feature representation many researches have begun to use transformer to read and attend on external knowledge in text generation systems for example zhao et al zhao et al make use of multi head attention mechanism in transformer to encode the dialogue context response candidate and the relevant ment through the hierarchical interaction in the context and document the importance of different parts of the document and context is determined to select the most appropriate sponse li et al li et al employ the multi head tention to get the vector representation of external knowledge and input the incremental transformer incorporates the tor representation of knowledge and context into the encoding process to encode knowledge utterances span in the turn dialogue the decoder contains two processes where the rst pass focuses on contextual coherence and the pass renes the results of the rst pass by attending on the knowledge to increase the knowledge relevance and ness kim et al kim et al propose a sequential latent model which sequentially conditions on previously selected knowledge to produce informative responses the input terance and knowledge sentences are encoded into vectors and then model the knowledge selection as latent variables to joint inference knowledge selection of multi turn dialogue rl based knowledge understanding reinforcement learning is an subeld of machine learning that emphasizes how to act based on the state to maximize the expected rewards through continuously interacting with the environment rl can use the rewards and punishments given by the environment to continuously improve the strategy that is what kind of actions to take in what kind of state for imum cumulative rewards rl is actually very close to the way of human thinking which is why it is likely to become the future general articial intelligence paradigm based on deep q network dqn a typical rl algorithm xu et al xu et al propose the knowledge routed dqn to age topic transitions during the dialogue the relational nement branch encodes relations among different symptoms and the knowledge routed graph branch decides policy in rl under different medical knowledge the two branches ensure that the dialogue manager the agent interacting with the vironment can make more reasonable decisions from edge guiding and relation encoding conclusion and future directions this survey makes a systematic literature review of the search trends of knowledge enhanced text generation with the help of external knowledge text generation system can understand input text more deeply and comprehensively and generate more informative text which is a very promising search direction as an emerging research direction there are many open issues in the research of knowledge enhanced text generation system which will be briey discussed here combining structured and unstructured knowledge at present researches mainly focus on incorporating one form of external knowledge if the two forms of knowledge are combined more appropriately and informatively text may be generated structured kg can narrow down knowledge candidates using the prior information such as entities and graph paths unstructured kb can provide abundant mation to enhance text generation but we need strong pability of natural language understanding to select useful information both forms of knowledge have their own vantages and disadvantages due to their structural ences it is a challenging research direction to combine the structured and unstructured knowledge into generation tems which will certainly bring promising progress to the knowledge enhanced text generation system lifelong learning we humans continuously learn new knowledge update our knowledge base to adapt to the fast changing pace of society however existing text generation systems mostly utilize xed knowledge bases whose knowledge do not keep updating in real time to make text generation models more morphic they should have the ability of continuous lifelong learning a meaningful exploration of this is discussed by mazumder et al they propose mazumder et al the lifelong interactive learning and inference model which will actively ask users questions when encountering unknown concepts and update its knowledge base after corresponding answers are reached how to continuously obtain information from numerous external inputs and achieve lifelong learning is a important research direction in text generation acknowledgments this work was supported by the national key program of china and the national natural ence foundation of china no references bengio et al yoshua bengio rejean ducharme pascal vincent and christian jauvin a neural tic language model journal of machine learning research bordes al antoine bordes nicolas usunier and oksana alberto garcia duran yakhnenko translating embeddings for modeling multi relational data in advances in neural information processing systems pages jason weston de cao et al nicola de cao wilker aziz and ivan titov question answering by reasoning across documents in proceedings of with graph convolutional networks naacl hlt pages dinan et al emily dinan stephen roller kurt shuster angela fan michael auli and jason weston wizard of wikipedia knowledge powered conversational agents in international conference on learning sentations elman jeffrey l elman finding structure in time cognitive science ghazvininejad et al marjan ghazvininejad chris brockett ming wei chang bill dolan jianfeng gao wen tau yih and michel galley a knowledge grounded neural conversation model in thirty second aaai ference on articial intelligence guan et al jian guan yansen wang and minlie huang story ending generation with incremental ing and commonsense knowledge in proceedings of the aaai conference on articial intelligence volume pages gunel et al beliz gunel chenguang zhu michael zeng and xuedong huang mind the facts boosted coherent abstractive text summarization in neurips kim et al byeongchang kim jaewoo ahn and gunhee kim sequential latent knowledge selection for in international knowledge grounded dialogue ence on learning representations kipf and welling thomas n kipf and max welling semi supervised classication with graph convolutional networks arxiv preprint koncel kedziorski et al rik koncel kedziorski dhanush bekal yi luan mirella lapata and hannaneh hajishirzi text generation from knowledge graphs with in proceedings of naacl hlt graph transformers pages et al zekang li cheng niu fandong meng yang feng qian li and jie zhou incremental former with deliberation decoder for document grounded conversations in proceedings of the annual meeting of the association for computational linguistics pages lian et al rongzhong lian min xie fan wang jinhua peng and hua wu learning to select knowledge for response generation in dialog systems in proceedings of the international joint conference on articial telligence pages aaai press liu et al shuman liu hongshen chen zhaochun ren yang feng qun liu and dawei yin knowledge diffusion for neural dialogue generation in proceedings of the annual meeting of the association for tational linguistics volume long papers pages et al shangwen lv daya guo jingjing xu duyu tang nan duan ming gong linjun shou daxin jiang guihong cao and songlin hu based reasoning over heterogeneous external knowledge arxiv preprint for commonsense question answering madotto et al andrea madotto chien sheng wu and pascale fung effectively incorporating knowledge bases into end to end task oriented dialog tems in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages mazumder et al sahisnu mazumder nianzu ma and bing liu towards a continuous knowledge ing engine for chatbots arxiv preprint knowledgeable reader mihaylov and frank todor mihaylov and anette enhancing frank style reading comprehension with external commonsense knowledge in proceedings of the annual meeting of the association for computational linguistics volume long papers pages mikolov et al tomas mikolov martin karaat lukas burget jan and sanjeev khudanpur current neural network based language model in eleventh annual conference of the international speech cation association moon et al seungwhan moon pararth shah anuj kumar and rajen subba opendialkg explainable conversational reasoning with attention based walks over in proceedings of the annual knowledge graphs meeting of the association for computational linguistics pages et al diego moussallem mihael arcan axel cyrille ngonga ngomo and paul buitelaar augmenting neural machine translation with knowledge graphs arxiv preprint qiu al delai qiu yuanzhe zhang xinwei feng xiangwen liao wenbin jiang yajuan lyu kang liu and jun zhao machine reading comprehension using in proceedings tural knowledge graph aware network of the conference on empirical methods in ral language processing and the international joint conference on natural language processing ijcnlp pages ren et al pengjie ren zhumin chen christof monz jun ma and maarten de rijke thinking globally acting locally distantly supervised global to local edge selection for background based conversation arxiv preprint sukhbaatar et al sainbayar sukhbaatar jason ston rob fergus al end to end memory networks in advances in neural information processing systems pages sutskever et al ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural networks in advances in neural information processing systems pages vaswani et al ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in advances in neural information processing tems pages wang et al run ze wang zhen hua ling and yu hu knowledge base question answering with ieee access tive pooling for question representation xu et al lin xu qixian zhou ke gong dan liang jianheng tang and liang lin end to end knowledge routed relational dialogue system for matic diagnosis in proceedings of the aaai conference on articial intelligence volume pages yang et al an yang quan wang jing liu kai liu yajuan lyu hua wu qiaoqiao she and sujian li enhancing pre trained language representations with rich knowledge for machine reading comprehension in ceedings of the annual meeting of the association for computational linguistics pages young et al tom young iti chaturvedi hao zhou subham biswas and minlie huang augmenting end to end dialogue systems with in thirty second aaai commonsense knowledge conference on articial intelligence erik cambria zhao et al xueliang zhao chongyang tao wei wu can xu dongyan zhao and rui yan a document grounded matching network for response arxiv preprint lection in retrieval based chatbots zhao et al xueliang zhao wei wu chongyang tao can xu dongyan zhao and rui yan low resource in knowledge grounded dialogue generation tional conference on learning representations et al hao zhou tom young minlie huang haizhou zhao jingfang xu and xiaoyan zhu monsense knowledge aware conversation generation with graph attention in ijcai pages
