information rich logical text generation knowledge enhanced neural models hao bin wei zhiwen polytechnical university xian china research beijing china nwpu edu guob edu com abstract text generation system massive ing progress contributed deep learning niques widely applied life existing end end neural models suffer problem tending generate tive generic text ground input context background knowledge der solve problem researchers begin consider combining external knowledge text generation systems knowledge enhanced text generation challenges enhanced text generation including select appropriate knowledge large scale edge bases read understand extracted knowledge integrate knowledge generation process survey gives hensive review knowledge enhanced text eration systems summarizes research progress solving challenges proposes open issues research directions introduction text generation known natural language generation nlg aims machines express like humans capability produce smooth meaningful mative textual contents original template based statistical methods deep learning based methods text generation attracted attention massive searchers remarkable advances depending data sources text generation divided text data text image text generation focus text text generation survey massive research challenges wider range cations text text generation takes natural language text input understands input text obtain semantic tations generates corresponding output text according task requirements advances text generation recent years ted deep neural networks recurrent neural work rnn elman transformer vaswani contact author help technologies text generation systems able generate smooth topic consistent personalized text existing text tion systems lack interactions real world little access external knowledge making easy generate short meaningless text humans constantly acquiring understanding storing knowledge automatically combine knowledge understand current situation communicating writing reading huge challenge faced text generation systems generate informative diverse logical text text generation systems ability combine external knowledge promising research direction fig example dialogue system sense knowledge combining monsense knowledge including structured knowledge graph composed triples unstructured knowledge base composed natural language text dialogue agent generate informative diverse logical response researchers academia dustry begun explore knowledge enhanced text eration system incorporating different types knowledge complexity real world scale edge base usually extremely large extract relevant knowledge massive knowledge facts based simple text input huge challenge sity natural languages effectively derstand extracted knowledge integrate neural network models facilitate text generation difcult problem best knowledge rst survey summarize knowledge enhanced text generation systems detail sum summarize contributions work follows briey introduce development process text generation formalize denition enhanced text generation analyze number key challenges research area summarize current knowledge enhanced text generation systems tematically categorizing state art works cording research challenges research progress propose open issues research directions reference community figure dialogue examples rst line combining external knowledge background formalized denition section briey introduce development text generation formalize denition general knowledge enhanced text generation summarize research challenges nnlm rnnlm neural network language model nnlm bengio rstly proposed text generation tasks leverages neural networks model language representation nnlm maps input low dimensional space reducing parameters model given text quence parametrized language model nnlm proximated represents word nnlm achieved promising results experiments typical feedforward neural network length text receives xed advance capture context information variable length solve problem rnn language model rnnlm mikolov proposed auto regressive language model utilizes rnn encode variable length inputs vector resentations process formulated theoretically rnnlm model text sequences length xed dimension hidden vector information excessively long context stored efciently variants rnn including long term memory lstm gated recurrent unit gru utilized improve performance rnnlm perform capturing long term dependencies encoder decoder framework length input output text language models equal cases length different question answer system deal problem encoder decoder framework sutskever proposed composed encoder decoder encoder uses rnn encode input sequence intermediate tics representation length input sequence decoder utilizes rnn generate output word according process dened length output text sequence encoder decoder general computing framework specic model encoder decoder justed based tasks widely text generation tasks proposed machine translation text summarization dialogue system knowledge enhanced text generation addition encoder decoder framework ral network models algorithms applied text generation tasks improve quality generated text development text generation systems requires ated text informative diverse logical simple smooth surface correct combined ternal knowledge text generation systems deeply stand input generate informative sistent logic human expression common sense mistakes given set knowledge facts text sequence knowledge triple number facts knowledge types challenges structured representing knowledge graphs vectors incorporating knowledge vectors models unstructured extracting knowledge reading understanding knowledge existing solutions word embedding based knowledge representation wang distance based knowledge representation gunel graph attention based knowledge representation guan concatenated input vector liu attention based knowledge graph decoder qiu gcn based knowledge incorporation cao keyword matching ghazvininejad semantical level knowledge extraction lian memory network based knowledge understanding madotto transformer based knowledge understanding kim based knowledge understanding table summary challenges knowledge enhanced text generation system knowledge enhanced text generation model lated forms external knowledge tured unstructured essentially mantic network containing multiple types entities lations form head relation tile head tile different entities entities refer things real world relations express connections entities xed form usually store edge related specic concepts textual sequence form challenges combining external knowledge text generation systems shown table combining structured knowledge rst challenge obtain vector representations knowledge triples ceptable inputs neural network models neural network models need input data vector form tion stored structured symbolized difcult problem map symbols low dimensional dense vector spaces incorporate knowledge vectors additional input neural network models guide eration process challenge knowledge facts unstructured stored form natural language text mapping knowledge facts vector tions pose research challenge rst challenge combining unstructured knowledge extract appropriate knowledge massive knowledge facts possible semantic duplication different edge understanding sentence level natural language text long term research challenge nlp ciently read understand textual knowledge facts tegrate generation systems challenge combining unstructured knowledge researchers great efforts address challenges tailed summarized following sections text generation structured structured kgs store wider range knowledge types information simple representation triples unique symbolic storage form different vectors required neural network models map knowledge triples low dimensional vector representations efciently incorporate knowledge vectors neural network models key directions research summarized section word embedding based knowledge representation simplest way obtain vector representations tured kbs directly treat entities relations edge triples common words use word embedding methods obtain vector representations widely initial research stage order comprehensive understand content ument reading comprehension tasks mihaylov haylov frank utilize bigru encode edge triples text sequences key value memory key value retrieval algorithm selects single sum weighted fact representations token hance understanding document context wang wang propose based single relation system entity linking module determines optimal subject question select knowledge facts encoded vectors bilstm relation tion module calculates similarity scores question relation candidates select triple highest score answer question distance based knowledge representation gap symbolic form vector resentation directly encoding entities common words lead certain information loss concept edge representation learning proposed represente entities relations low dimensional dense vector spaces culation reasoning bordes bordes propose transe algorithm uses translation variant phenomenon word vector distance based ing function obtain vector representations entities relations better integrated text generation system provide powerful knowledge support moussallem moussallem incorporate external knowledge machine translation system prove quality results knowledge facts linked based translated document encoded modied trane concatenated vectors internal tors nmt embeddings input decoder gune gunel incorporate entity level knowledge knowledge graph transformer encoder decoder chitecture produce coherent summaries extracted entities initialized pretrained algorithm vector representations fed separate head attention channel generating summaries graph attention based knowledge representation obtain accurate vector representations graph tention algorithm zhou proposed uses relation information aggregate entities generate new tity representations attention mechanism makes better use interconnections graph entities guishes hierarchy connections enhance effective information needed text generation tasks generate informative responses zhou zhou propose static graph attention mechanism generate static representation retrieved graph augment semantics input words dynamic graph attention mechanism designed attentively reading knowledge triples text generation guan guan propose incremental encoding scheme story ending generation context clues hidden story context adopt graph attention contextual attention respectively obtain graph vectors multi source attention mechanism combines commonsense knowledge facilitating story comprehension generate coherent sonable story endings comprehensive understanding paragraph level ment qiu qiu construct sub graphs entities capture structural information resentations nodes sub graph updated graph attention network document context combined document representations generate nal answer concatenating knowledge input vector vector representations knowledge triples tained challenge facing knowledge enhanced text generation systems integrate knowledge vectors neural network models simplest method directly concatenate knowledge vectors input vectors enhance vector representations input send decoding stage text generation instance young young extract triples according entity keys input form text sequence encoded lstm obtain vector sentations knowledge vectors added input vector calculate degree correlation tive responses liu liu propose idea entity diffusion means conversation usually drifts entity similarity extracted entities entities calculated retrieve relevant tities word vectors entities relations averaged obtain vector representations triple nated input vector guide response generation attention based knowledge graph decoder attention mechanism focus important contents numerous input information select key formation ignoring unimportant tention mechanism knowledge enhanced text generation tems focus critical parts knowledge instead feeding selected knowledge directly neural works produce informative text example moon moon construct embeddings represent entities algorithm aggregate input contexts relevant entities ate candidate entities efciently attention based graph decoder proposed walk optimal path large select candidate entities paragraph level essay generation yang yang present memory augmented neural model bine commonsense knowledge knowledge concepts tracted input topics query stored memory matrix model attend memory ically update incorporate information generated text diverse topic consistent essay generation cel koncel kedziorski introduce graph transforming encoder leverage relational structure knowledge graphs encode knowledge graphs vectors decoder attend input title edge graphs generate informative topic coherent text gcn based knowledge incorporating gcn kipf welling natural extension cnn graph domain learns node feature structure information end end manner powerful neural network framework graphs begun attract researchers attention text generation systems combining structured knowledge graphs cao consider question answering inference problem graph document tion nodes graph entities appeared ment edges graph represent relations entities gcn capture reasoning chains propagating local contextual information edges form multi step reasoning generating answers extract evidence knowledge graph predictions based evidence graph based contextual word representation learning module dene distance words learning better textual word representations graph structural tion graph based inference module applied encode neighbor information representations nodes gcn aggregate evidence generate answers text generation unstructured unstructured kbs composed natural language text lated concepts express rich semantic information textual form unstructured ily combined text generation systems input text sequences scale knowledge base usually extremely huge contains redundant information extract knowledge required text generation systems efciently understand knowledge integrate generation process main research challenges researches grounded text generation unstructured discussed detail section key word matching based knowledge extraction simplest way extract knowledge unstructured key matching method words input keywords method simple direct extract knowledge according surface information words combine deeper semantic information knowledge extraction instance ghazvininejad ghazvininejad rstly introduce external knowledge fully driven neural conversation model given dialogue history relevant knowledge facts identied keyword matching method entities context keys retrieved know facts fed memory network retrieve weight facts based input dialogue context hance semantic representation input semantical level knowledge extraction simple key word matching method hard accurately select required knowledge information contained single word searchers focus knowledge selection semantic level forward novel ideas query human conversation related different responses different knowledge utilized lian solve problem lian pose idea posterior distribution knowledge calculated input query response provide accurate guidance knowledge selection minimizing distance prior posterior distribution knowledge prior distribution lized select appropriate knowledge generate mative responses actual response unknown ren ren propose global local edge selection mechanism global perspective select appropriate background knowledge topic tion vector learned dialogue context external knowledge distantly supervised learning schema lect likely text fragments vector guide local knowledge selection module decoding stage generate uency appropriate responses zhao zhao represent disentangled response coder separate parameters relying knowledge grounded dialogues model solve problem ing knowledge grounded training data decoder posed components including language model generate common words context processor generate text words knowledge processor generate words knowledge document hierarchical attention mechanism memory network based knowledge understanding extracting relevant knowledge facts import read understand textual knowledge enhance input representation guide text generation memory work sukhbaatar proposed improve poor memory ability rnn external memory ponent realize storage long term memory powerful memory storage capacity memory network widely knowledge enhanced text generation systems retrieve read condition external knowledge madott madotto augment memory network sequential generative architecture edge facts fed memory network update input query vector gru dynamic query generator generate output words produce tion decide generate common words memory contents order carefully read understand trieved knowledge dinan dinan combine memory network transformer encode selected knowledge dialogue context higher level semantic representation dot product attention tween knowledge context performed retrieved relevant knowledge generating response transformer based knowledge understanding transformer emerging sequential model caused great repercussions nlp exceeds rnn tic information abstraction long term feature extraction task comprehensive feature representation researches begun use transformer read attend external knowledge text generation systems example zhao zhao use multi head attention mechanism transformer encode dialogue context response candidate relevant ment hierarchical interaction context document importance different parts document context determined select appropriate sponse employ multi head tention vector representation external knowledge input incremental transformer incorporates tor representation knowledge context encoding process encode knowledge utterances span turn dialogue decoder contains processes rst pass focuses contextual coherence pass renes results rst pass attending knowledge increase knowledge relevance ness kim kim propose sequential latent model sequentially conditions previously selected knowledge produce informative responses input terance knowledge sentences encoded vectors model knowledge selection latent variables joint inference knowledge selection multi turn dialogue based knowledge understanding reinforcement learning subeld machine learning emphasizes act based state maximize expected rewards continuously interacting environment use rewards punishments given environment continuously improve strategy kind actions kind state imum cumulative rewards actually close way human thinking likely future general articial intelligence paradigm based deep network dqn typical algorithm propose knowledge routed dqn age topic transitions dialogue relational nement branch encodes relations different symptoms knowledge routed graph branch decides policy different medical knowledge branches ensure dialogue manager agent interacting vironment reasonable decisions edge guiding relation encoding conclusion future directions survey makes systematic literature review search trends knowledge enhanced text generation help external knowledge text generation system understand input text deeply comprehensively generate informative text promising search direction emerging research direction open issues research knowledge enhanced text generation system briey discussed combining structured unstructured knowledge present researches mainly focus incorporating form external knowledge forms knowledge combined appropriately informatively text generated structured narrow knowledge candidates prior information entities graph paths unstructured provide abundant mation enhance text generation need strong pability natural language understanding select useful information forms knowledge vantages disadvantages structural ences challenging research direction combine structured unstructured knowledge generation tems certainly bring promising progress knowledge enhanced text generation system lifelong learning humans continuously learn new knowledge update knowledge base adapt fast changing pace society existing text generation systems utilize xed knowledge bases knowledge updating real time text generation models morphic ability continuous lifelong learning meaningful exploration discussed mazumder propose mazumder lifelong interactive learning inference model actively ask users questions encountering unknown concepts update knowledge base corresponding answers reached continuously obtain information numerous external inputs achieve lifelong learning important research direction text generation acknowledgments work supported national key program china national natural ence foundation china references bengio yoshua bengio rejean ducharme pascal vincent christian jauvin neural tic language model journal machine learning research bordes antoine bordes nicolas usunier oksana alberto garcia duran yakhnenko translating embeddings modeling multi relational data advances neural information processing systems pages jason weston cao nicola cao wilker aziz ivan titov question answering reasoning documents proceedings graph convolutional networks naacl hlt pages dinan emily dinan stephen roller kurt shuster angela fan michael auli jason weston wizard wikipedia knowledge powered conversational agents international conference learning sentations elman jeffrey elman finding structure time cognitive science ghazvininejad marjan ghazvininejad chris brockett ming wei chang bill dolan jianfeng gao wen tau yih michel galley knowledge grounded neural conversation model thirty second aaai ference articial intelligence guan jian guan yansen wang minlie huang story ending generation incremental ing commonsense knowledge proceedings aaai conference articial intelligence volume pages gunel beliz gunel chenguang zhu michael zeng xuedong huang mind facts boosted coherent abstractive text summarization neurips kim byeongchang kim jaewoo ahn gunhee kim sequential latent knowledge selection international knowledge grounded dialogue ence learning representations kipf welling thomas kipf max welling semi supervised classication graph convolutional networks arxiv preprint koncel kedziorski rik koncel kedziorski dhanush bekal luan mirella lapata hannaneh hajishirzi text generation knowledge graphs proceedings naacl hlt graph transformers pages zekang cheng niu fandong meng yang feng qian jie zhou incremental deliberation decoder document grounded conversations proceedings annual meeting association computational linguistics pages lian rongzhong lian min xie fan wang jinhua peng hua learning select knowledge response generation dialog systems proceedings international joint conference articial telligence pages aaai press liu shuman liu hongshen chen zhaochun ren yang feng qun liu dawei yin knowledge diffusion neural dialogue generation proceedings annual meeting association tational linguistics volume long papers pages shangwen daya guo jingjing duyu tang nan duan ming gong linjun shou daxin jiang guihong cao songlin based reasoning heterogeneous external knowledge arxiv preprint commonsense question answering madotto andrea madotto chien sheng pascale fung effectively incorporating knowledge bases end end task oriented dialog tems proceedings annual meeting sociation computational linguistics volume long papers pages mazumder sahisnu mazumder nianzu bing liu continuous knowledge ing engine chatbots arxiv preprint knowledgeable reader mihaylov frank todor mihaylov anette enhancing frank style reading comprehension external commonsense knowledge proceedings annual meeting association computational linguistics volume long papers pages mikolov tomas mikolov martin karaat lukas burget jan sanjeev khudanpur current neural network based language model eleventh annual conference international speech cation association moon seungwhan moon pararth shah anuj kumar rajen subba opendialkg explainable conversational reasoning attention based walks proceedings annual knowledge graphs meeting association computational linguistics pages diego moussallem mihael arcan axel cyrille ngonga ngomo paul buitelaar augmenting neural machine translation knowledge graphs arxiv preprint qiu delai qiu yuanzhe zhang xinwei feng xiangwen liao wenbin jiang yajuan lyu kang liu jun zhao machine reading comprehension proceedings tural knowledge graph aware network conference empirical methods ral language processing international joint conference natural language processing ijcnlp pages ren pengjie ren zhumin chen christof monz jun maarten rijke thinking globally acting locally distantly supervised global local edge selection background based conversation arxiv preprint sukhbaatar sainbayar sukhbaatar jason ston rob fergus end end memory networks advances neural information processing systems pages sutskever ilya sutskever oriol vinyals quoc sequence sequence learning neural networks advances neural information processing systems pages vaswani ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need advances neural information processing tems pages wang run wang zhen hua ling knowledge base question answering ieee access tive pooling question representation lin qixian zhou gong dan liang jianheng tang liang lin end end knowledge routed relational dialogue system matic diagnosis proceedings aaai conference articial intelligence volume pages yang yang quan wang jing liu kai liu yajuan lyu hua qiaoqiao sujian enhancing pre trained language representations rich knowledge machine reading comprehension ceedings annual meeting association computational linguistics pages young tom young iti chaturvedi hao zhou subham biswas minlie huang augmenting end end dialogue systems thirty second aaai commonsense knowledge conference articial intelligence erik cambria zhao xueliang zhao chongyang tao wei dongyan zhao rui yan document grounded matching network response arxiv preprint lection retrieval based chatbots zhao xueliang zhao wei chongyang tao dongyan zhao rui yan low resource knowledge grounded dialogue generation tional conference learning representations hao zhou tom young minlie huang haizhou zhao jingfang xiaoyan zhu monsense knowledge aware conversation generation graph attention ijcai pages
