nlp driven ensemble based automatic subtitle generation and semantic video summarization technique aswin vb mohammed javed parag parihar aswanth k druval cr anpam dagar aravinda cv indian institute of information technology allahabad prayagraj uttar pradesh abstract this paper proposes an automatic subtitle generation and semantic video summarization technique the importance of automatic video tion is vast in the present era of big data video summarization helps in efficient storage and also quick surfing of large collection of videos without losing the important ones the summarization of the videos is done with the help of tles which is obtained using several text summarization algorithms the posed technique generates the subtitle for videos with without subtitles using speech recognition and then applies nlp based text summarization algorithms on the subtitles the performance of subtitle generation and video tion is boosted through ensemble method with two approaches such as tersection method and weight based learning method experimental results ported show the satisfactory performance of the proposed method introduction a concept like video summarization has a huge scope in the modern era video itory websites like google dailymotion vimeo are gaining popularity day by day the popularity of these websites is enormous in the present scenario a large amount of videos are being uploaded as well as downloaded from these online video repository websites for example the total number of people who use youtube are hours of video are uploaded to youtube every minute almost billion videos are watched on youtube every single day youtube gets over million visitors per in this scenario for a concept like video summarization has a huge scope the video summarization technique can be applied on the video thumbnail to attract more viewers it can be developed to show only interesting and important parts of the video it is not necessary for all the videos to come with a tle it is very difficult to summarize the videos like security footage s as they do nt have subtitles even after applying speech recognition this reduces the domain of video summarization but still summarization of video using subtitles is most efficient and fastest way of doing it if machine learning algorithms or histogram based were used to summarize videos it would have taken a long time to train them which will increase the time of development but dealing with subtitle which is ously text is much more easy to deal with and faster which makes the video zation easier and faster but the main problem here is that most of the videos comes without subtitles in such cases to rectify this problem the technique of speech recognition which can be applied on the audio of the video and generate subtitles by formatting the text obtained after speech recognition to extract different sentences from the video there is a need to detect silence in the audio as it recognizes the end of the sentence from that once the subtitle is obtained the video can be summarized with the generated sub titles for summarization of subtitles natural language processing nlp can be used which can be of various accuracy s therefore overall this paper proposes nlp based subtitle generation and video marization technique rest of the paper is organized as follows section explains the proposed model section explains experimental results and section presents the summary of the report proposed model for summarizing a video using subtitles the proposed method uses text rization algorithms which are nlp based methods further there is an ensemble technique using the text summarization algorithms the text summarization algorithms were used for filtering out key contents from subtitle file srt which will be taken as an input implement the algorithm on the input put the sentences in array and rank them according to their importance using different domains and will pick out the best sentences out of it so as to form a concise subtitle keeping in mind where those subtitles were originally placed according to subtitle i d next is ble technique which combines the different algorithms together for a more perfect intersection of all algorithm abstract of the input also we are training each rithm in ensemble technique to give precise models the flowchart in fig describes the flow of video summarization using subtitles that has been implemented the input video is fed in along with the subtitles if the subtitle is not present subtitle is ated using the subtitle generation algorithm and this subtitle along with the video is used to summarize depending upon whether single algorithm or the combined rithm is to be used and the summarized video is the output figure flow control of video summarization nlp based subtitle generation since all videos may not have subtitles along with them and this method can be applied on videos which have subtitles in case the user does not have a subtitle file then the subtitles is generated first and then process the video using methods listed below the subtitles if not provided are generated using speech recognition api of wit ai which is used by facebook for speech recognition basic idea on how the subtitle is generated is that chunks of audio is extracted from the video and apply speech recognition on them to elaborate this first the audio is extracted from the video file then max time interval is fixed for each subtitle for the video let it be sec which is in our case then the audio is scanned to detect silence in it if the silence occurs before seconds the audio will be cut at that point also a threshold is defined such that above this level the part of the audio is not treated as silence and vice versa also to be noted that second of extra silence is added at the starting and at the ending of the audio so that words to be recognized by the speech recognizer is not missed out each time the words are recognized in a particular chunk of the whole sentence is formatted in the form of a subtitle file such that each of the tence will be mentioned with the starting time stamp and the ending time stamp once the subtitle is available it can be summarized to obtain the summarized video there are text summarization algorithms which have been used to summarize a video these are luhn came around as an emerging year in the field of summarization when hans peter luhn suggested an for thought summarization this was a carrying a lot of weight achievement and a big head start in this sector and followed his consider was a action in summarization area luhn received a rule of thumb to recognize salient sentences from the text by features well known as definition and definition frequency this rithm checks for words that have high occurrence frequency and the words are sorted based on decreasing frequency the weight of a sentence is culated by summing weights of each relevant word and these sentences are sorted in decreasing order based on the summed weight and finally p most relevant sentences are taken from the subtitles as the output latent semantic analysis also known as lsa is based on words and concepts if each word represent only a single concept lsa would have been lot easier but un fortunately each word in english language represent several concepts like synonyms because of this varying concepts within each word lsa becomes a little tricky as each word map to more than one concept the basic concept of lsa is based on title words and index words title words are those words which appear in all the sentences in lsa and index words are those words which appear in more that tences which has the title word a document is considered as a bag of lsa words the order in which the words are arranged is not important whereas the count of a particular word plays an important role also each word is suppose to have only one meaning lsa is based on concepts which are represented as a pattern of words to understand the concept behind the word clustering is used in lsa the basic idea is to plot a xy graph cluding all the index words and title words based on their occurrences in each sentences then all the clusters in the graph are identified each of these clusters represent each concept and title in that sentence represent that particular concept hence concept for each title word can be extracted so in summarization the technique was used in such a way that the tence which is included in the most crowded cluster of the graph was taken using this concept of title words and index words any text document can be summarized text rank in the text rank algorithm it first takes the text which has to be split up and converted into sentences and further into vectors a similarity trix is constructed from the vectors and a graph is created from this matrix using this graph the sentence ranking is done based on the ranked tences summarized text is obtained the probability of going from tence a to sentence b is the similarity of sentences the text units which best defines the sentence are identified these text units are then added to the graph as vertices the relations which connect texts are identified which are used to draw edges of the graph between vertices a graph based ranking algorithm is used until it converges during the ranking rithm each vertex is assigned a value this value is used for lection sions finally the vertices are sorted on the basis of their final score value and then the sentences are sorted based on the sorted vertices in the graph lex rank in the lex rank algorithm first all the nouns and adjectives is rated out to form a document cluster now the idf inverse document frequency scores is found for each words in this cluster for each word let tf be the frequency of that word in the cluster the collection of all words which have score greater than a threshold value forms the centroid of the cluster so importance of a sentence will be higher if it tains more number of words which are present in the centroid so using this concept p most relevant sentences are selected the previous works of luhn showed that the key words can be extracted out as the most frequently occurring content words except the stop words and summarize the text according containing the most key words but since this much is not alone capable to generate proper summarized text edmundson edmundson proposed another algorithm and added three more ods to extract out key words which are namely pragmatic words cue and heading words and structural indicators sentence tion the results of edmundson cleared out the fact that the other three factors played a dominant role in generating the best summarized text as expected since it uses extra words call stigma and bonus word the marization will be biased according to those files so this method will not be used in the proposed method video summarization from the algorithms explained in the previous section edmundson summarization will not be used because it is biased according to the bonus and stigma words given by the user so it can not be used for a comparison figure output of summarization using algorithms fig is the summarization of the movie named mrs doubtfire contains a frame of output using algorithms lex rank luhn lsa and text rank a video of shah rukh khan s ted talk which was of minutes was taken and summarized the subtitle file of the input video had lines the video was fed into the above tioned algorithms separately the graph of subtitle text with it being relevant in the summarized video for the algorithms are described in fig fig fig fig figure luhn algorithm figure lex rank algorithm figure lsa algorithm weight based learning algorithms figure text rank algorithm taking intersection and combining a very basic and simple approach is to combine them directly meaning the idea is to run all the algorithms keep their outputs side by side and take a simple mathematical intersection as it is very obvious if a tence is in all the combining algorithms then its must be of great portance hence it should be included in the output file hence the user can choose from luhn lsa lexrank textrank which all he wants to bine multiple correct so that he can get the best results when all of them combined also edmundson can not be concatenated in this process because it requires two extra files of bonus words and stigma words hence taking that while combining them was not ideally possible this tersection gave very good results and implemented the ideology of ble properly the problem with the previous method was that it gave equal powers to all the algorithms but from the above explanations it can be seen that not all algorithms behave properly so here a method was devised where each could get some weightage the idea was simple the one that performs the best gets more as the name suggest an initial weights were taken for all algorithm and initialized all the algorithms with the same weights wl wlsa we wlr wtr now the check function will compare output of each of the algorithms and rank them accordingly on basis of their formance so the ones that performed may get an increment in their weights so during future summarizations they will get their scores cording to their weights and the sentences with higher score will be part of the output file in this way a clear view can be obtained of which rithm can give best output for the given input and the suggested algorithm can be used to obtain better results the figure is the weight allocation for different algorithms before and after summarization of shah rukh khan s video from the figure it can be understood that the weight of lsa was creased and that of lex was decreased so for this video lsa performed better and lex performedthe least so the weight of lex rank is increased by a unit and that of luhn is decreased figure weights before and after summarization of first video experimental results since there is no dataset for video summarization using subtitles and the zation technique a of videos was generated which were having different time length these videos were given to these algorithms and also the combined algorithm and the number of lines obtained in the summarized video was noted the efficiency of each algorithm was decided based on the output of combined video which was the intersection of all these algorithms so the efficiency can be defined for an algorithm as the ratio of the number of subtitles in the combined video to that of the output of the particular algorithm efficiency ncombined nalgorithm efficiency of video summarization by efficiency of an algorithm in summarization it is meant how much part of the summarized algorithm is present in the video generated by the ensemble technique as mentioned above efficiency of each algorithm can be calculated using formula and on applying this on the intersection method following results were obtained as shown in table efficiency of intersection method lex rank lsa luhn text on applying the dataset on the weighted ensemble technique with initial weights set to for all algorithms the following results were obtained as shown in table and the updated weights are shown in table table efficiency of weighted ensemble method lex rank lsa luhn text table weights ensemble method initial finial lsa luhn text lex rank from the tables and it is observed that lsa performed better and lex rank formed the least since for all videos lsa performed best and lex performed least their weights got affected whereas the weight of luhn and text rank remained changed table there is a huge difference in the efficiency value in weight based and intersection method because in weight based ensemble technique at each iteration the weight of the better algorithm is increased and that of the worst algorithm is creased from these two methods it is clear that lsa has a major contribution to the summarized video and lex has the least contribution complexity time complexity single summarization algorithm where n is the number of iterations until the summarization length is obtained and k is the number of sentences in the summarized subtitles combined summarization algorithm where is the number of methods to be combined n is the number of tions until the summarization length is obtained k is the number of sentences in the summarized subtitles space complexity single summarization algorithm where r is the total number of regions in the subtitle array l is the average length of the sentences in the summarized subtitle combined summarization algorithm where r is the total number of regions in the subtitle array l is the average length of the sentences in the summarized subtitle conclusion large number of videos are being generated and are increasing day by day hence video summarization technique will be very helpful video summarization provided a faster way browsing of large video collections and more efficient content indexing and access the use of nlp algorithms proved to be a very efficient way to form abstracts of videos the case of no subtitles was by using subtitle generation method to convert speech to text which turned out to be of great use in normal day to day usage many of the videos which is taken from phones do not contain subtitles hence there is future scope to work on this problem references mind blowing youtube facts figures and statistics available com youtube rachida hannane abdessamad elboushaki karim afdel p naghabhushan mohammed javed an efficient method for video shot boundary detection and keyframe extraction ing sift point distribution histogram s m s k l g g j liadh kelly johannes leveling report on summarization n r pratibha devihosur automatic text summarization using natural language niques cessing s liu long text summarization using neural networks and rule based approach t g dietterich ensemble methods in machine learning m dahale text summarization for compressed inverted indexes and snippets definition of subtitle available webster com dictionary subtitle j constine speech recognition using wit ai available facebook wit n nazari and m a mahdavi a survey on automatic text summarization journal of ai t k landauer p w foltz and d laham an introduction to latent semantic and data mining vol pp processes vol r mihalcea and p tarau textrank bringing order into text g erkan and d r radev lexrank graph based lexical centrality as salience in text artif int res vol h p edmundson new methods in automatic acm vol
