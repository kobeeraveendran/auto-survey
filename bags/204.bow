leafnats an open source toolkit and live demo system for neural abstractive text summarization tian shi virginia tech edu ping wang virginia tech edu chandan k reddy virginia tech vt edu a m l c s c v v i x r a abstract neural abstractive text summarization nats has received a lot of attention in the past few years from both industry and academia in this paper we introduce an open source toolkit namely leafnats for training and tion of different sequence to sequence based models for the nats task and for ing the pre trained models to real world cations the toolkit is modularized and tensible in addition to maintaining tive performance in the nats task a live news blogging system has also been mented to demonstrate how these models can aid blog news editors by providing them gestions of headlines and summaries of their articles introduction being one of the prominent natural language generation tasks neural abstractive text rization nats has gained a lot of ity rush et al see et al paulus et al different from extractive text marization gambhir and gupta nallapati et al verma and lee nats lies on modern deep learning models particularly sequence to sequence models to erate words from a vocabulary based on the resentations features of source documents rush et al nallapati et al so that it has the ability to generate high quality summaries that are verbally innovative and can also easily corporate external knowledge see et al many nats models have achieved better mance in terms of the commonly used tion measures such as rouge lin score compared to extractive text summarization proaches paulus et al celikyilmaz et al gehrmann et al we recently provided a comprehensive survey of the models shi et al ing their network structures parameter inference methods and decoding generation approaches for the task of abstractive text summarization a riety of nats models share many common erties and some of the key techniques are widely used to produce well formed and human readable summaries that are inferred from source articles such as encoder decoder framework sutskever et al word embeddings mikolov et al attention mechanism bahdanau et al pointing mechanism vinyals et al and beam search algorithm rush et al many of these features have also found tions in other language generation tasks such as machine translation bahdanau et al and dialog systems serban et al in addition other techniques that can also be shared across different tasks include training strategies fellow et al keneshloo et al zato et al data pre processing results processing and model evaluation therefore ing an open source toolbox that modularizes ferent network components and unies the ing framework for each training strategy can et researchers in language generation from ious aspects including efciently implementing new models and generalizing existing models to different tasks in the past few years different toolkits have been developed to achieve this goal some of them were designed specically for a single task such as parlai miller et al for dialog search and some have been further extended to other tasks for example opennmt klein et al and xnmt neubig et al are marily for neural machine translation nmt but have been applied to other areas the bottom up attention model gehrmann et al which has achieved state of the art performance for stractive text summarization is implemented in there are also several other general purpose language generation packages such as texar hu et al compared with these toolkits leafnats is specically designed for nats research but can also be adapted to other tasks in this toolkit we implement an end end training framework that can minimize the fort in writing codes for training evaluation dures so that users can focus on building models and pipelines this framework also makes it easier for the users to transfer pre trained parameters of user specied modules to newly built models in addition to the learning framework we have also developed a web application which is driven by databases web services and nats models to show a demo of deploying a new nats idea to a real life application using leafnats such an plication can help front end users e blog news authors and editors by providing suggestions of headlines and summaries for their articles the rest of this paper is organized as follows section introduces the structure and design of leafnats learning framework in section we describe the architecture of the live system demo based on the request of the system we propose and implement a new model using leafnats for headline and summary generation we conclude this paper in section leafnats in this section we introduce the structure and sign of leafnats toolkit which is built upon the lower level deep learning platform torch paszke et al as shown in fig it consists of four main components i e engines modules data and tools and playground engines in leafnats an engine represents a training algorithm for example end to end training see et al and adversarial ing goodfellow et al are two different training frameworks therefore we need to velop two different engines for them specically for leafnats we implement a task independent end to end training engine for nats but it can also be adapted to other nlp tasks such as nmt question answering ment classication the engine uses stract data models pipelines and loss functions figure the framework of leafnats toolkit to build procedures of training validation ing evaluation and application respectively so that they can be completely reused when menting a new model for example these dures include saving loading check point les ing training selecting n best models during idation and using the best model for generation during testing another feature of this engine is that it allows users to specify part of a neural network to train and reuse parameters from other models which is convenient for transfer learning modules modules are the basic building in leafnats we blocks of different models provide ready to use modules for constructing current neural network to sequence models for nats e pointer generator network see et al these modules include embedder rnn encoder attention luong et al temporal tion nallapati et al attention on decoder paulus et al and others we also use these basic modules to assemble a pointer generator coder module and the corresponding beam search the embedder can also be used algorithms to realize the embedding weights sharing nism paulus et al data and tools different models in nats are tested on three datasets see table namely cnn daily mail cnn dm hermann et al newsroom grusky et al and the pre processed cnn dm data is available here we provide tools to process the last two datasets data modules are used to prepare the input data for mini batch mization playground with the engine and modules we can develop different models by just assembling com com com leafnats process data of cnn dailymail dataset cnn dm newsroom bytecup table basic statistics of the datasets used validation train test these modules and building pipelines in ground we re implement different models in the nats toolkit shi et al to this framework the performance rouge scores lin of the pointer generator model on different datasets has been reported in table where we nd that most of the results are better than our previous plementations shi et al due to some minor changes to the neural network model dataset pointer generator newsroom s newsroom h pointer generator pointer generator coverage pointer generator cnn dm bytecup r l table performance of our implemented generator network on different datasets s and represent newsroom summary and headline datasets respectively a live system in this section we present a real world web cation of the abstractive text summarization els which can help front end users to write lines and summaries for their articles posts we will rst discuss the architecture of the system and then provide more details of the front end design and a new model built by leafnats that makes automatic summarization and headline generation possible architecture this is a news blog website which allows people to read duplicate edit post delete and comment articles it is driven by web services databases and our nats models this web application is developed with php html css and jquery lowing the concept of model view controller see fig in this framework when people interact with the front end views they send html requests to controllers that can manipulate models then the views will be changed with the updated tion for example in nats we rst write an ticle in a text area then this article along with cs vt edu leafnats figure the architecture of the live system the summarization request will be sent to the troller via jquery ajax call the controller municates with our nats models asynchronously via json format data finally generated lines and summaries are shown in the view design of frontend fig presents the front end design of our web application for creating a new post where labels represent the sequence of actions in this website an author can rst click on new post step to bring a new post view then he she can write content of an article in the corresponding text area step without specifying it s headline and lights i e summary by clicking nats ton step and waiting for a few seconds he she will see the generated headlines and highlights for the article in a new tab on the right hand side of the screen here each of the buttons in gray color denotes the resource of the training data for example bytecup means the model is trained with bytecup headline generation dataset the tokenized article content is shown in the bottom apart from plain text headlines and highlights our system also enables users to get a visual standing of how each word is generated via tention weights luong et al when ing the mouse tracker step on any token in the headlines or highlights related content in the ticle will be labeled with red color if the author would like to use one of the suggestions he she can click on the gray button step to add it to the text area on the left hand side and edit it nally he she can click post step to post the article the proposed model as shown in the fig our system can suggest to the users two headlines based on newsroom headline and bytecup datasets and summaries based on newsroom summary and cnn dm datasets they are treated as four tasks in this section to achieve this goal we use the figure front end design of the live demonstration of our system dataset newsroom s newsroom h cnn dm bytecup model multi task multi task transfer coverage transfer r l table performance of our model corresponding testing sets are shown in table from the table we observe that our model forms better in headline generation tasks ever the rouge scores in summarization tasks are lower than the models without sharing ding encoder and output layers it should be noted that by sharing the parameters this model requires less than million parameters to achieve such performance conclusion in this paper we have introduced a leafnats toolkit for building training testing evaluating and deploying nats models as well as a live news blogging system to demonstrate how the nats models can make the work of writing lines and summaries for news articles more cient an extensive set of experiments on ent benchmark datasets has demonstrated the fectiveness of our implementations the newly proposed model for this system has achieved petitive results with fewer number of parameters figure overview of the model used to generate lines and summaries ules provided in leafnats toolkit to assemble a new model see fig which has a shared ding layer a shared encoder layer a task specic encoder decoder bi lstm encoder and generator decoder layer and a shared output layer to train this model we rst build a multi task learning pipeline for newsroom dataset to learn parameters for the modules that are colored in ange in fig because articles in this dataset have both headlines and highlights the size of the dataset is large and the articles come from a variety of news agents then we build a fer learning pipeline for cnn daily and cup dataset and learn the parameters for modules labeled with blue and green color respectively with leafnats we can accomplish this work ciently the performance of the proposed model on the acknowledgments this work was supported in part by the us tional science foundation grants and references dzmitry bahdanau kyunghyun cho and yoshua gio neural machine translation by jointly arxiv preprint learning to align and translate asli celikyilmaz antoine bosselut xiaodong he and yejin choi deep communicating agents for in proceedings of the abstractive summarization conference of the north american chapter of the association for computational linguistics man language technologies volume long pers volume pages mahak gambhir and vishal gupta recent matic text summarization techniques a survey ticial intelligence review sebastian gehrmann yuntian deng and alexander rush bottom up abstractive summarization in proceedings of the conference on cal methods in natural language processing pages ian goodfellow jean pouget abadie mehdi mirza bing xu david warde farley sherjil ozair aaron courville and yoshua bengio generative in advances in neural information versarial nets processing systems pages max grusky mor naaman and yoav artzi newsroom a dataset of million summaries with diverse extractive strategies in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies volume long pers volume pages karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom teaching chines to read and comprehend in advances in ral information processing systems pages zhiting hu haoran shi zichao yang bowen tan tiancheng zhao junxian he wentao wang xingjiang yu lianhui qin di wang al texar a modularized versatile and arxiv preprint ble toolkit for text generation yaser keneshloo tian shi chandan k reddy and naren ramakrishnan deep reinforcement learning for sequence to sequence models arxiv preprint guillaume klein yoon kim yuntian deng jean senellart and alexander rush opennmt open source toolkit for neural machine translation proceedings of acl system demonstrations pages chin yew lin rouge a package for matic evaluation of summaries text summarization branches out thang luong hieu pham and christopher d ning effective approaches to attention based in proceedings of the neural machine translation conference on empirical methods in natural language processing pages tomas mikolov ilya sutskever kai chen greg s rado and jeff dean distributed tions of words and phrases and their in advances in neural information processing ity systems pages alexander miller will feng dhruv batra antoine bordes adam fisch jiasen lu devi parikh and jason weston parlai a dialog research in proceedings of the ware platform ference on empirical methods in natural language processing system demonstrations pages ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based quence model for extractive summarization of ments in thirty first aaai conference on articial intelligence ramesh nallapati bowen zhou cicero dos santos c a glar and bing xiang tive text summarization using sequence to sequence rnns and beyond conll page graham neubig matthias sperber xinyi wang matthieu felix austin matthews sarguna manabhan ye qi devendra singh sachan philip arthur pierre godard al xnmt the tensible neural machine translation toolkit vol mt researchers track page adam paszke sam gross soumith chintala gory chanan edward yang zachary devito ing lin alban desmaison luca antiga and adam lerer automatic differentiation in pytorch romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization arxiv preprint marcaurelio ranzato sumit chopra michael auli and wojciech zaremba sequence level ing with recurrent neural networks arxiv preprint alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of the tence summarization conference on empirical methods in natural guage processing pages abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proceedings of the nual meeting of the association for computational linguistics volume long papers pages association for computational linguistics iulian vlad serban alessandro sordoni yoshua gio aaron c courville and joelle pineau building end to end dialogue systems using ative hierarchical neural network models tian shi yaser keneshloo naren ramakrishnan and chandan k reddy neural abstractive text summarization with sequence to sequence models arxiv preprint ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural works in advances in neural information ing systems pages rakesh m verma and daniel lee extractive summarization limits compression generalized model and heuristics computacion y sistemas oriol vinyals meire fortunato and navdeep jaitly pointer networks in advances in neural formation processing systems pages
