semantic relevance based neural network text summarization text simplication shuming peking university sun peking university text summarization text simplication major ways simplify text poor ers including children non native speakers functionally illiterate text summarization produce brief summary main ideas text text simplication aims reduce linguistic complexity text retain original meaning recently approaches text summarization text simplication based sequence sequence model achieves success text generation tasks generated simplied texts similar source texts literally low semantic relevance work goal improve semantic relevance source texts simplied texts text summarization text simplication introduce semantic relevance based neural model encourage high semantic similarity texts summaries model source text represented gated attention encoder summary representation produced decoder similarity score representations maximized training experiments proposed model outperforms state art systems benchmark introduction text summarization text simplication text easier read understand especially poor readers including children non native speakers functionally erate text summarization simplify texts document level source texts consist sentences paragraphs simplied texts brief sentences main ideas source texts text simplication simplify texts sentence level aims simplify sentences reduce lexical structure complexity unlike text summarization require simplied sentences shorter requires words simple understand previous work extractive summarization achieves satisfying performance lecting sentences source texts radev cheng lapata cao extracting sentences generated texts grammatical retain meaning source texts simplify texts shorten texts previous related work regards text simplication combination operations splitting deletion paraphrasing requires rule based models heavy tic features zhu bernhard gurevych woodsend lapata filippova moe key laboratory computational linguistics peking university beijing china school electronics engineering computer science peking university beijing china mail edu moe key laboratory computational linguistics peking university beijing china school electronics engineering computer science peking university beijing china mail edu code available com shumingma srb association computational linguistics computational linguistics volume number table example simplied text generated summarization summary high similarity text literally low semantic relevance text night people caught smoke ight china united airlines chendu beijing later ight temporarily landed taiyuan airport passengers asked security check denied captain led collision crew passengers china united airlines exploded airport leaving people dead gold people smoked ight led collision crew passengers recent approaches use sequence sequence model tion rush chopra weston chen zhu tion nisioi cao zhang lapata sequence sequence model widely end end framework text generation machine translation compresses source text information dense vectors neural encoder neural decoder generates target text compressed vectors text text text summarization text simplication simplied texts high semantic relevance source texts current sequence sequence models tend produce grammatical coherent simplied texts regardless semantic relevance source texts table shows summary generated lstm sequence sequence model similar source text literally low semantic relevance work goal improve semantic relevance source texts generated simplied texts text summarization text simplication achieve goal propose semantic relevance based neural network model srb model compress source texts dense vectors encoder decode dense vector simplied texts decoder encoder produces representation source texts decoder produces representation generated texts similarity evaluation component introduced measure relevance source texts generated texts training maximizes similarity score encourage high semantic relevance source texts simplied texts order better represent long source text introduce self gated attention encoder memory input text conduct experiments corpus lcsts pwkp sew experiments proposed model better performance state art systems benchmark corpus contributions work follow propose semantic relevance based neural network model srb improve semantic relevance source texts generated simplied texts text summarization text simplication similarity evaluation component shuming semantic relevance based neural network summarization simplication introduced measure relevance source texts generated texts similarity score maximized encourage high semantic relevance source texts simplied texts introduce self gated encoder better represent long redundant text perform experiments corpus lcsts pwkp sew experiments proposed model outperforms state art systems benchmark corpus background sequence sequence model recent models text summarization text simplication based sequence model sequence sequence model able compress source texts continuous vector representation encoder generates simplied text decoder previous work nisioi chen zhu encoder layer long short term memory network lstm hochreiter schmidhuber maps source texts hidden vector decoder uni directional lstm producing hidden output dense representation words tth time step finally word generator computes distribution output words hidden state parameter matrix attention mechanism introduced better capture context information source texts bahdanau cho bengio attention vector calculated weighted sum encoder hidden states attentive score decoder hidden state encoder hidden state predicting output word decoder takes account attention vector contains alignment information source texts simplied texts attention mechanism word generator computes distribution output words sof tihi sof computational linguistics volume number admission hard cos attention admission extremely competitive figure semantic relevance based neural model consists decoder encoder cosine similarity function proposed model goal improve semantic relevance source texts simplied texts proposed model encourages high similarity representations figure shows proposed model model consists components encoder decoder similarity function encoder compresses source texts semantic vectors decoder generates summaries produces semantic vectors generated summaries finally similarity function evaluates relevance sematic vectors source texts generated summaries training objective maximize similarity score generated summaries high semantic relevance source texts self gated encoder goal complex text encoder provide series dense representation source texts decoder semantic relevance component previous work nisioi complex text encoder layer uni directional long short term memory work lstm produces dense representation source text text summarization text simplication source texts usually long noisy encoding information beginning texts vanish end texts leads bad representations texts directional lstm alternative deal problem needs double time encoder source texts represents middle texts texts long solve problem propose self gated encoder better represent long text text summarization text simplication words information source texts unimportant need simplied discarded introduce self gated shuming semantic relevance based neural network summarization simplication self gated layer admission extremely competitive figure self gated encoder measure importance word decide information reserved representation texts encoder reduce unnecessary information enhance important information represent long text self gated encoder try measure importance word decide information reserved representation texts time step upcoming word fed lstm cell outputs dense vector lstm function output vector lstm cell feed forward neural network measure importance decide information reversed feed forward neural network function measures proportion reserved information finally reversed information computed multiplying tht representation tth time step input embedding time step simplied text decoder goal simplied text decoder generate series simplied words dense representation source texts model dense representations source texts fed computational linguistics volume number attention layer generate context vector tihi sof kvskkvtk dense representation generated simplied computed layer lstm way respectively represent context information source texts target texts tth time step predict tth word decoder uses generate probability distribution candidate words parameter matrix output layer finally word highest probability predicted semantic relevance goal compute semantic relevance source texts generated texts given source semantic vector generated sementic vector use cosine similarity measure semantic relevance represented dot product magnitude source texts generated texts share language reasonable assume semantic vectors distributed space cosine similarity good way measure distance vectors space semantic relevance metric problem semantic vector methods represent text sentence mean pooling lstm output reserving state lstm model select state encoder representation source texts natural idea semantic vector summary feed encoder method wastes time encode sentence twice actually output decoder contains information source text generated shuming semantic relevance based neural network summarization simplication summaries simply compute semantic vector summary subtracting previous work proved effective represent span words encoding wang chang training given model parameter input text model produces corresponding summary semantic vector objective minimize loss function conditional probability summaries given source texts computed encoder decoder model cosine similarity semantic vectors term tries maximize semantic relevance source input target output use adam optimization method train model default hyper parameters learning rate section present evaluation model performance popular corpus perform case study explain semantic relevance generated summary source text experiments datasets introduce chinese text summarization dataset popular text simplication datasets simplication datasets alignments english wikipedia simple english wikipedia simple english wikipedia built children adults learning english language articles composed easy words short sentences simple english wikipedia natural public simplied text corpus text simplication benchmark datasets constructed simple english wikipedia large scale chinese short text summarization dataset lcsts lcsts constructed chen zhu dataset consists million text summary pairs constructed famous chinese social media website called sina split parts pairs pairs pairs iii text summary pairs iii manually annotated relevant scores ranged reserve pairs scores following previous work use training set development set iii test set parallel pwkp pwkp zhu bernhard gurevych widely benchmark evaluating text simplication systems consists aligned complex text english wikipedia simplication wikipedia corpus wikipedia org wikipedia org weibo sina com computational linguistics volume number aug simple text simple wikipedia aug dataset contains sentence pairs words average complex sentence words simple sentence following previous work zhang lapata remove duplicate sentence pairs split corpus pairs training pairs development pairs test english wikipedia simple english wikipedia sew sew publicly available dataset provided hwang build corpus rst align complex simple sentence pairs score semantic similarity complex sentence simple sentence classify sentence pair good good partial partial bad match following previous work nisioi discard unclassied matches use good matches partial matches scaled threshold greater corpus contains good matches good partial matches use corpus training set dataset provided development set test set development set consists sentence pairs test set contains sentence pairs complex sentence paired reference simplied sentences provided amazon mechanical turk workers settings describe experimental details text summarization text simplication respectively text summarization alleviate risk word segmentation mistakes sun sun wang use chinese character sequences source inputs target outputs limit model vocabulary size covers common characters character represented random initialized word embedding tune parameter development set model embedding size hidden state size encoder decoder size gated attention network use adam optimizer learn model parameters batch size set parameter encoder decoder based lstm unit following ous work chen zhu evaluation metric score rouge rouge lin hovy text simplication text simplication datasets contain lot named entities makes vocabulary large reduce vocabulary size follow setting zhang lapata recognize named entities stanford corenlp ger manning replace named entities anonymous symbols loc org misc represents entity sentence limit vocabulary size prune vocabulary frequent words replace rest words unk symbols test time replace unk symbols highest probability score attention alignment matrix following jean jean lter sentence pairs lengths exceed words training set encoder implemented lstm decoder based lstm luong style attention luong pham manning tune hyper parameter development set model lstm layers hidden size lstm embedding size use adam optimizer kingma learn parameters batch size set set dropout rate srivastava gradients clipped norm exceeds evaluation metric bleu score shuming semantic relevance based neural network summarization simplication table results model baseline systems models achieve substantial improvement rouge scores baseline systems results reported test sets word level character level rouge model chen zhu chen zhu attention chen zhu attention chen zhu attention implementation srb proposal table comparison model recent neural models text simplication models achieve substantial improvement bleu score baseline systems results reported test sets pwkp attention nisioi attention nisioi attention implementation srb proposal sew attention nisioi attention nisioi attention implementation srb proposal bleu bleu baseline systems rst compare sequence sequence model model sutskever vinyals widely model generate texts important baseline attention attention bahdanau cho bengio sequence sequence framework neural attention attention mechanism helps capture context information source texts model stronger baseline system basic results compare model baseline systems including attention refer proposed semantic relevance based neural model srb table shows results models baseline systems lcsts shown table models character level achieve better performance models word level implement model character level fair comparison implement attention model following details previous work chen zhu implementation attention better score mainly tune parameters development set srb outperforms attention score rouge computational linguistics volume number table results model state art systems copynet incorporates copying mechanism solve vocabulary problem higher rouge scores model incorporate mechanism currently future work implement technique improve performance results reported test sets word word level char character level score score score rouge model chen zhu chen zhu attention chen zhu attention chen zhu copynet srb proposal rouge table results model state art systems srb achieves best bleu scores compared related systems pwkp sew results reported test sets pwkp nts nisioi nts nisioi dress zhang lapata dress zhang lapata srb proposal sew pbmt wubben sbmt sari nts nisioi nts nisioi dress zhang lapata dress zhang lapata srb proposal bleu bleu table shows results text simplication corpus compare model attention attention attention attention pretrain word embeddings implement attention model carefully tune development set implementation bleu score pwkp bleu score sew srb outperforms baseline systems bleu score pwkp bleu score sew table summarizes results model state art systems copynet highest scores incorporates copying mechanism deals vocabulary word problem paper implement mechanism model model improved additional techniques focus paper compare srb models text simplication limit neural models table summarizes results srb related systems pwkp dataset compare srb nts nts dress dress run public release code shuming semantic relevance based neural network summarization simplication table example srb generated summary lcsts dataset compared system output attention reference text bat ppspptv careful calculation successful internet companies shanghai giant company like bat reason internet companies listed companies paying tax merged ebay tudou pps pptv yihaodian satised segment market years reference shanghai comes giant company shanghai giant company srb shanghai giant companies nts nts provided nisioi bleu score respectively dress dress use scores reported zhang lapata goal dress generate outputs closer references bleu dress dress relatively lower nts nts srb achieves bleu score outperforming previous systems sew dataset compare wean dot pbmt sbmt sari neural models described public release code pbmt sbmt sari fortunately provides predictions pbmt sbmt sari sew test set compare model systems shows neural models better performance bleu wean dot achieves best bleu score case study table example semantic relevance source text summary shows main idea source text reason shanghai giant company rnn context produces shanghai giant companies literally similar source text srb generates shanghai giant companies closer main idea semantics concludes srb produces summaries higher semantic similarity texts table shows examples different text simplication system outputs sew nts omits words lacks lot information pbmt generates evant words like siemens martin hurts uency adequacy generated sentence sbmt sari able generate uent sentence meaning different source text difcult understand compared statistic model srb generates uent sentence srb improves semantic revelance computational linguistics volume number table examples different text simplication system outputs sew dataset differences source texts shown bold source reference nts nts pbmt sbmt sari srb depending context closely related meaning constituent citizen residing area governed represented served politician restricted citizens elected politician word constituent refer citizen lives area governed represented served politician word restricted citizens elected politician depending context closely related meaning constituent citizen living area governed represented served politician restricted citizens elected politician restricted citizens elected politician depending context meaning closely related siemens martin citizen living area governed shurba restricted people elected terms context closely related sense component citizen living area covered served policy limited people elected policy depending context closely related meaning constituent citizen living area governed represented served politician word restricted citizens elected politician source texts generated texts generated sentence semantically correct close original meaning related work sutskever vinyals summarization achieved successful performance thanks abstractive text attention sequence sequence model nism bahdanau cho bengio rush chopra weston rst attention based encoder compress texts neural network language decoder generate summaries following work recurrent encoder introduced text summarization gained better performance lopyrev chopra auli rush chinese texts chen zhu built large corpus chinese short text summarization deal unknown word problem nallapati proposed generator pointer model decoder able generate words source texts solved issue incorporating copying mechanism ayana proposes minimum risk training method optimizes parameters target rouge scores zhu bernhard gurevych constructs wikipedia dataset proposes based simplication model rst statistical simplication model covering splitting dropping reordering substitution integrally woodsend lapata introduces driven model based quasi synchronous grammar captures structural mismatches complex rewrite operations wubben van den bosch krahmer presents method text simplication phrase based machine translation ranking outputs shuming semantic relevance based neural network summarization simplication kauchak proposes text simplication corpus evaluates language modeling text simplication proposed corpus narayan gardent propose hybrid approach sentence simplication combines deep semantics monolingual machine translation hwang introduces parallel simplication corpus evaluating similarity source text simplied text based wordnet glava tajner propose unsupervised approach lexical simplication makes use word vectors require regular corpora design automatic metrics text simplication introduce statistic machine translation tune proposed automatic metrics recently works focus neural sequence sequence model nisioi present sequence sequence model ranks predictions bleu sari zhang lapata propose deep reinforcement learning model improve simplicity uency adequacy simplied texts cao introduce novel sequence sequence model join copying restricted generation text simplication like sequence sequence model achieved success work related encoder decoder framework cho attention mechanism bahdanau cho bengio encoder decoder work machine tion sutskever vinyals jean luong pham manning text summarization rush chopra weston chopra auli rush nallapati cao natural language processing tasks neural attention model rst proposed bahdanau cho bengio methods improve neural attention model jean luong pham manning conclusion work goal improve semantic relevance source texts generated simplied texts text summarization text simplication achieve goal propose semantic relevance based neural network model srb similarity evaluation component introduced measure relevance source texts generated texts training maximizes similarity score encourage high semantic relevance source texts simplied texts order better represent long source text introduce self gated attention encoder memory input text conduct experiments corpus lcsts pwkp sew experiments proposed model better performance state art systems benchmark corpus acknowledgements work supported national natural science foundation china okawa research grant sun corresponding author paper work substantial extension conference version presented acl references ayana shiqi shen zhiyuan liu maosong sun neural headline generation minimum risk training corr bahdanau dzmitry kyunghyun cho yoshua bengio neural machine translation jointly learning align translate corr cao ziqiang wenjie sujian furu wei yanran attsum joint learning focusing summarization neural attention coling international conference computational linguistics proceedings conference technical papers december osaka japan pages computational linguistics volume number cao ziqiang chuwei luo wenjie sujian baotian qingcai chen fangze zhu joint copying restricted generation paraphrase proceedings thirty aaai conference articial intelligence pages cao ziqiang furu wei sujian wenjie ming zhou houfeng wang learning summary prior representation extractive summarization proceedings annual meeting association computational linguistics international joint conference natural language processing asian federation natural language processing acl july beijing china volume short papers pages cheng jianpeng mirella lapata neural summarization extracting sentences words proceedings annual meeting association computational linguistics acl august berlin germany volume long papers cho kyunghyun bart van merrienboer aglar glehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase representations rnn encoder decoder statistical machine translation proceedings conference empirical methods natural language processing emnlp pages chopra sumit michael auli alexander rush abstractive sentence summarization attentive recurrent neural networks naacl hlt conference north american chapter association computational linguistics human language technologies pages filippova katja enrique alfonseca carlos colmenares lukasz kaiser oriol vinyals sentence compression deletion lstms proceedings conference empirical methods natural language processing emnlp pages glava goran sanja tajner simplifying lexical simplication need simplied corpora proceedings annual meeting association computational linguistics acl pages jiatao zhengdong hang victor incorporating copying mechanism sequence sequence learning proceedings annual meeting association computational linguistics acl hochreiter sepp jrgen schmidhuber long short term memory neural computation lcsts large scale chinese short text summarization dataset proceedings conference empirical methods natural language processing emnlp lisbon portugal september pages hwang william hannaneh hajishirzi mari ostendorf wei aligning sentences standard wikipedia simple wikipedia naacl hlt pages jean sbastien kyunghyun cho roland memisevic yoshua bengio large target vocabulary neural machine translation proceedings annual meeting association computational linguistics acl pages kauchak david improving text simplication language modeling unsimplied text data proceedings annual meeting association computational linguistics acl pages kingma diederik jimmy adam method stochastic optimization corr lin chin yew eduard hovy automatic evaluation summaries gram occurrence statistics human language technology conference north american chapter association computational linguistics hlt naacl lopyrev konstantin generating news headlines recurrent neural networks corr luong thang hieu pham christopher manning effective approaches attention based neural machine translation proceedings conference empirical methods natural language processing emnlp pages shuming sun jingjing houfeng wang wenjie improving semantic relevance sequence sequence learning chinese social media text summarization proceedings annual meeting association computational linguistics acl vancouver canada july august volume short papers pages manning christopher mihai surdeanu john bauer jenny rose finkel steven bethard david mcclosky stanford corenlp natural language processing toolkit proceedings annual meeting association computational linguistics acl pages shuming semantic relevance based neural network summarization simplication nisioi sergiu sanja stajner simone paolo jingjing sun nallapati ramesh bowen zhou ccero nogueira dos santos aglar glehre bing xiang abstractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning conll berlin germany august pages narayan shashi claire gardent hybrid simplication deep semantics machine translation proceedings annual meeting association computational linguistics acl pages ponzetto liviu dinu exploring neural text simplication models proceedings annual meeting association computational linguistics acl pages radev dragomir timothy allison sasha blair goldensohn john blitzer arda elebi stanko dimitrov elliott drbek ali hakim wai lam danyu liu jahna otterbacher hong horacio saggion simone teufel michael topper adam winkel zhu zhang mead platform multidocument multilingual text summarization proceedings fourth international conference language resources evaluation lrec rush alexander sumit chopra jason weston neural attention model abstractive sentence summarization proceedings conference empirical methods natural language processing emnlp lisbon portugal september pages srivastava nitish geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov dropout simple way prevent neural networks overtting journal machine learning research sun houfeng wang wenjie fast online training frequency adaptive learning rates chinese word segmentation new word detection proceedings pages sutskever ilya oriol vinyals quoc sequence sequence learning neural networks advances neural information processing systems annual conference neural information processing systems pages wang wenhui baobao chang graph based dependency parsing bidirectional lstm proceedings annual meeting association computational linguistics acl woodsend kristian mirella lapata learning simplify sentences quasi synchronous grammar integer programming proceedings conference empirical methods natural language processing emnlp pages wubben sander antal van den bosch emiel krahmer sentence simplication monolingual machine translation annual meeting association computational linguistics proceedings conference pages dependency based gated recursive neural network chinese word segmentation meeting association computational linguistics pages wei courtney napoles ellie pavlick quanze chen chris callison burch optimizing statistical machine translation text simplication tacl zhang xingxing mirella lapata sentence simplication deep reinforcement learning corr zhu zhemin delphine bernhard iryna gurevych monolingual tree based translation model sentence simplication coling pages
