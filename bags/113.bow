t c o l c s c v v x r semantic relevance based neural network text summarization text simplication shuming ma peking university xu sun peking university text summarization text simplication major ways simplify text poor ers including children non native speakers functionally illiterate text summarization produce brief summary main ideas text text simplication aims reduce linguistic complexity text retain original meaning recently approaches text summarization text simplication based sequence sequence model achieves success text generation tasks generated simplied texts similar source texts literally low semantic relevance work goal improve semantic relevance source texts simplied texts text summarization text simplication introduce semantic relevance based neural model encourage high semantic similarity texts summaries model source text represented gated attention encoder summary representation produced decoder similarity score representations maximized training experiments proposed model outperforms state art systems benchmark introduction text summarization text simplication text easier read understand especially poor readers including children non native speakers functionally erate text summarization simplify texts document level source texts consist sentences paragraphs simplied texts brief sentences main ideas source texts text simplication simplify texts sentence level aims simplify sentences reduce lexical structure complexity unlike text summarization require simplied sentences shorter requires words simple understand previous work extractive summarization achieves satisfying performance lecting sentences source texts radev et al cheng lapata cao et al extracting sentences generated texts grammatical retain meaning source texts simplify texts shorten texts previous related work regards text simplication combination operations splitting deletion paraphrasing requires rule based models heavy tic features zhu bernhard gurevych woodsend lapata filippova et al moe key laboratory computational linguistics peking university beijing china school electronics engineering computer science peking university beijing china e mail edu cn moe key laboratory computational linguistics peking university beijing china school electronics engineering computer science peking university beijing china e mail edu cn code available com shumingma srb association computational linguistics computational linguistics volume number table example simplied text generated summarization summary high similarity text literally low semantic relevance text night people caught smoke ight china united airlines chendu beijing later ight temporarily landed taiyuan airport passengers asked security check denied captain led collision crew passengers china united airlines exploded airport leaving people dead gold people smoked ight led collision crew passengers recent approaches use sequence sequence model tion rush chopra weston hu chen zhu tion nisioi et al cao et al zhang lapata sequence sequence model widely end end framework text generation machine translation compresses source text information dense vectors neural encoder neural decoder generates target text compressed vectors text text text summarization text simplication simplied texts high semantic relevance source texts current sequence sequence models tend produce grammatical coherent simplied texts regardless semantic relevance source texts table shows summary generated lstm sequence sequence model similar source text literally low semantic relevance work goal improve semantic relevance source texts generated simplied texts text summarization text simplication achieve goal propose semantic relevance based neural network model srb model compress source texts dense vectors encoder decode dense vector simplied texts decoder encoder produces representation source texts decoder produces representation generated texts similarity evaluation component introduced measure relevance source texts generated texts training maximizes similarity score encourage high semantic relevance source texts simplied texts order better represent long source text introduce self gated attention encoder memory input text conduct experiments corpus lcsts pwkp ew sew experiments proposed model better performance state art systems benchmark corpus contributions work follow propose semantic relevance based neural network model srb improve semantic relevance source texts generated simplied texts text summarization text simplication similarity evaluation component shuming ma et al semantic relevance based neural network summarization simplication introduced measure relevance source texts generated texts similarity score maximized encourage high semantic relevance source texts simplied texts introduce self gated encoder better represent long redundant text perform experiments corpus lcsts pwkp ew sew experiments proposed model outperforms state art systems benchmark corpus background sequence sequence model recent models text summarization text simplication based sequence model sequence sequence model able compress source texts xn continuous vector representation encoder generates simplied text y ym decoder previous work nisioi et al hu chen zhu encoder layer long short term memory network lstm hochreiter schmidhuber maps source texts hidden vector hn decoder uni directional lstm producing hidden output st dense representation words tth time step finally word generator computes distribution output words yt hidden state st parameter matrix w attention mechanism introduced better capture context information source texts bahdanau cho bengio attention vector ct calculated weighted sum encoder hidden states hi attentive score decoder hidden state st encoder hidden state hi predicting output word decoder takes account attention vector contains alignment information source texts simplied texts attention mechanism word generator computes distribution output words yt sof st ct tihi n x ti hi n hj p sof computational linguistics volume number admission hard cos attention admission extremely competitive figure semantic relevance based neural model consists decoder encoder cosine similarity function proposed model goal improve semantic relevance source texts simplied texts proposed model encourages high similarity representations figure shows proposed model model consists components encoder decoder similarity function encoder compresses source texts semantic vectors decoder generates summaries produces semantic vectors generated summaries finally similarity function evaluates relevance sematic vectors source texts generated summaries training objective maximize similarity score generated summaries high semantic relevance source texts self gated encoder goal complex text encoder provide series dense representation source texts decoder semantic relevance component previous work nisioi et al complex text encoder layer uni directional long short term memory work lstm produces dense representation hn source text xn text summarization text simplication source texts usually long noisy encoding information beginning texts vanish end texts leads bad representations texts bi directional lstm alternative deal problem needs double time encoder source texts represents middle texts texts long solve problem propose self gated encoder better represent long text text summarization text simplication words information source texts unimportant need simplied discarded introduce self gated shuming ma et al semantic relevance based neural network summarization simplication self gated layer admission extremely competitive figure self gated encoder measure importance word decide information reserved representation texts encoder reduce unnecessary information enhance important information represent long text self gated encoder try measure importance word decide information reserved representation texts time step upcoming word xt fed lstm cell outputs dense vector ht f lstm function ht output vector lstm cell feed forward neural network measure importance decide information reversed g feed forward neural network function t measures proportion reserved information finally reversed information computed multiplying t ht xt t ht tht ht representation tth time step input embedding t time step simplied text decoder goal simplied text decoder generate series simplied words dense representation source texts model dense representations source texts fed computational linguistics volume number attention layer generate context vector ct ct tihi n x ti n p sof st st yt vt vs vt kvskkvtk st dense representation generated simplied computed layer lstm way ct st respectively represent context information source texts target texts tth time step predict tth word decoder uses ct st generate probability distribution candidate words w wc parameter matrix output layer finally word highest probability predicted semantic relevance goal compute semantic relevance source texts generated texts given source semantic vector vt generated sementic vector vs use cosine similarity measure semantic relevance represented dot product magnitude source texts generated texts share language reasonable assume semantic vectors distributed space cosine similarity good way measure distance vectors space semantic relevance metric problem semantic vector vs vt methods represent text sentence mean pooling lstm output reserving state lstm model select state encoder representation source texts vs natural idea semantic vector summary feed encoder method wastes time encode sentence twice actually output decoder sm contains information source text generated shuming ma et al semantic relevance based neural network summarization simplication summaries simply compute semantic vector summary subtracting hn sm vs sm hn previous work proved effective represent span words encoding wang chang training given model parameter input text model produces corresponding summary y semantic vector vs vt objective minimize loss function l vt conditional probability summaries given source texts computed encoder decoder model vt cosine similarity semantic vectors vs vt term tries maximize semantic relevance source input target output use adam optimization method train model default hyper parameters learning rate section present evaluation model performance popular corpus perform case study explain semantic relevance generated summary source text experiments datasets introduce chinese text summarization dataset popular text simplication datasets simplication datasets alignments english wikipedia simple english wikipedia simple english wikipedia built children adults learning english language articles composed easy words short sentences simple english wikipedia natural public simplied text corpus text simplication benchmark datasets constructed simple english wikipedia large scale chinese short text summarization dataset lcsts lcsts constructed hu chen zhu dataset consists million text summary pairs constructed famous chinese social media website called sina split parts pairs pairs ii pairs iii text summary pairs ii iii manually annotated relevant scores ranged reserve pairs scores following previous work use training set ii development set iii test set parallel pwkp pwkp zhu bernhard gurevych widely benchmark evaluating text simplication systems consists aligned complex text english wikipedia simplication wikipedia corpus wikipedia org wikipedia org weibo sina com computational linguistics volume number aug simple text simple wikipedia aug dataset contains sentence pairs words average complex sentence words simple sentence following previous work zhang lapata remove duplicate sentence pairs split corpus pairs training pairs development pairs test english wikipedia simple english wikipedia ew sew ew sew publicly available dataset provided hwang et al build corpus rst align complex simple sentence pairs score semantic similarity complex sentence simple sentence classify sentence pair good good partial partial bad match following previous work nisioi et al discard unclassied matches use good matches partial matches scaled threshold greater corpus contains k good matches k good partial matches use corpus training set dataset provided xu et al xu et al development set test set development set consists sentence pairs test set contains sentence pairs complex sentence paired reference simplied sentences provided amazon mechanical turk workers settings describe experimental details text summarization text simplication respectively text summarization alleviate risk word segmentation mistakes xu sun sun wang li use chinese character sequences source inputs target outputs limit model vocabulary size covers common characters character represented random initialized word embedding tune parameter development set model embedding size hidden state size encoder decoder size gated attention network use adam optimizer learn model parameters batch size set parameter encoder decoder based lstm unit following ous work hu chen zhu evaluation metric f score rouge rouge l lin hovy text simplication text simplication datasets contain lot named entities makes vocabulary large reduce vocabulary size follow setting zhang lapata recognize named entities stanford corenlp ger manning et al replace named entities anonymous symbols loc org misc n represents n th entity sentence limit vocabulary size prune vocabulary frequent words replace rest words unk symbols test time replace unk symbols highest probability score attention alignment matrix following jean et al jean et al lter sentence pairs lengths exceed words training set encoder implemented lstm decoder based lstm luong style attention luong pham manning tune hyper parameter development set model lstm layers hidden size lstm embedding size use adam optimizer kingma ba learn parameters batch size set set dropout rate srivastava et al gradients clipped norm exceeds evaluation metric bleu score shuming ma et al semantic relevance based neural network summarization simplication table results model baseline systems models achieve substantial improvement rouge scores baseline systems results reported test sets w word level c character level rouge l model w hu chen zhu c hu chen zhu attention w hu chen zhu attention c hu chen zhu attention c implementation srb c proposal table comparison model recent neural models text simplication models achieve substantial improvement bleu score baseline systems results reported test sets pwkp attention nisioi et al attention nisioi et al attention implementation srb proposal ew sew attention nisioi et al attention nisioi et al attention implementation srb proposal bleu bleu baseline systems rst compare sequence sequence model model sutskever vinyals le widely model generate texts important baseline attention attention bahdanau cho bengio sequence sequence framework neural attention attention mechanism helps capture context information source texts model stronger baseline system basic results compare model baseline systems including attention refer proposed semantic relevance based neural model srb table shows results models baseline systems lcsts shown table models character level achieve better performance models word level implement model character level fair comparison implement attention model following details previous work hu chen zhu implementation attention better score mainly tune parameters development set srb outperforms attention f score rouge l computational linguistics volume number table results model state art systems copynet incorporates copying mechanism solve vocabulary problem higher rouge scores model incorporate mechanism currently future work implement technique improve performance results reported test sets word word level char character level f score f score r l f score rouge l model w hu chen zhu c hu chen zhu attention w hu chen zhu attention c hu chen zhu copynet c gu et al srb c proposal rouge l table results model state art systems srb achieves best bleu scores compared related systems pwkp ew sew results reported test sets pwkp nts nisioi et al nts nisioi et al dress zhang lapata dress ls zhang lapata srb proposal ew sew pbmt r wubben et al sbmt sari xu et al nts nisioi et al nts nisioi et al dress zhang lapata dress ls zhang lapata srb proposal bleu bleu table shows results text simplication corpus compare model attention attention attention attention pretrain word embeddings implement attention model carefully tune development set implementation bleu score pwkp bleu score ew sew srb outperforms baseline systems bleu score pwkp bleu score ew sew table summarizes results model state art systems copynet highest scores incorporates copying mechanism deals vocabulary word problem paper implement mechanism model model improved additional techniques focus paper compare srb models text simplication limit neural models table summarizes results srb related systems pwkp dataset compare srb nts nts dress dress ls run public release code shuming ma et al semantic relevance based neural network summarization simplication table example srb generated summary lcsts dataset compared system output attention reference text bat ppspptv careful calculation successful internet companies shanghai giant company like bat reason internet companies listed companies paying tax merged ebay tudou pps pptv yihaodian satised segment market years reference shanghai comes giant company shanghai s giant company srb shanghai giant companies nts nts provided nisioi et al bleu score respectively dress dress ls use scores reported zhang lapata goal dress generate outputs closer references bleu dress dress ls relatively lower nts nts srb achieves bleu score outperforming previous systems ew sew dataset compare wean dot pbmt r sbmt sari neural models described nd public release code pbmt r sbmt sari fortunately xu et al provides predictions pbmt r sbmt sari ew sew test set compare model systems shows neural models better performance bleu wean dot achieves best bleu score case study table example semantic relevance source text summary shows main idea source text reason shanghai giant company rnn context produces shanghai s giant companies literally similar source text srb generates shanghai giant companies closer main idea semantics concludes srb produces summaries higher semantic similarity texts table shows examples different text simplication system outputs ew sew nts omits words lacks lot information pbmt r generates evant words like siemens martin hurts uency adequacy generated sentence sbmt sari able generate uent sentence meaning different source text difcult understand compared statistic model srb generates uent sentence srb improves semantic revelance computational linguistics volume number table examples different text simplication system outputs ew sew dataset differences source texts shown bold source reference nts nts pbmt r sbmt sari srb depending context closely related meaning constituent citizen residing area governed represented served politician restricted citizens elected politician word constituent refer citizen lives area governed represented served politician word restricted citizens elected politician depending context closely related meaning constituent citizen living area governed represented served politician restricted citizens elected politician restricted citizens elected politician depending context meaning closely related siemens martin citizen living area governed shurba restricted people elected terms context closely related sense component citizen living area covered served policy limited people elected policy depending context closely related meaning constituent citizen living area governed represented served politician word restricted citizens elected politician source texts generated texts generated sentence semantically correct close original meaning related work sutskever vinyals le summarization achieved successful performance thanks abstractive text attention sequence sequence model nism bahdanau cho bengio rush chopra weston rst attention based encoder compress texts neural network language decoder generate summaries following work recurrent encoder introduced text summarization gained better performance lopyrev chopra auli rush chinese texts hu chen zhu built large corpus chinese short text summarization deal unknown word problem nallapati et al proposed generator pointer model decoder able generate words source texts gu et al solved issue incorporating copying mechanism ayana et al proposes minimum risk training method optimizes parameters target rouge scores zhu bernhard gurevych constructs wikipedia dataset proposes based simplication model rst statistical simplication model covering splitting dropping reordering substitution integrally woodsend lapata introduces driven model based quasi synchronous grammar captures structural mismatches complex rewrite operations wubben van den bosch krahmer presents method text simplication phrase based machine translation ranking outputs shuming ma et al semantic relevance based neural network summarization simplication kauchak proposes text simplication corpus evaluates language modeling text simplication proposed corpus narayan gardent propose hybrid approach sentence simplication combines deep semantics monolingual machine translation hwang et al introduces parallel simplication corpus evaluating similarity source text simplied text based wordnet glava tajner propose unsupervised approach lexical simplication makes use word vectors require regular corpora xu et al design automatic metrics text simplication introduce statistic machine translation tune proposed automatic metrics recently works focus neural sequence sequence model nisioi et al present sequence sequence model ranks predictions bleu sari zhang lapata propose deep reinforcement learning model improve simplicity uency adequacy simplied texts cao et al introduce novel sequence sequence model join copying restricted generation text simplication like sequence sequence model achieved success work related encoder decoder framework cho et al attention mechanism bahdanau cho bengio encoder decoder work machine tion sutskever vinyals le jean et al luong pham manning text summarization rush chopra weston chopra auli rush nallapati et al cao et al natural language processing tasks neural attention model rst proposed bahdanau cho bengio methods improve neural attention model jean et al luong pham manning conclusion work goal improve semantic relevance source texts generated simplied texts text summarization text simplication achieve goal propose semantic relevance based neural network model srb similarity evaluation component introduced measure relevance source texts generated texts training maximizes similarity score encourage high semantic relevance source texts simplied texts order better represent long source text introduce self gated attention encoder memory input text conduct experiments corpus lcsts pwkp ew sew experiments proposed model better performance state art systems benchmark corpus acknowledgements work supported national natural science foundation china okawa research grant xu sun corresponding author paper work substantial extension conference version presented acl ma et al references ayana shiqi shen zhiyuan liu maosong sun neural headline generation minimum risk training corr bahdanau dzmitry kyunghyun cho yoshua bengio neural machine translation jointly learning align translate corr cao ziqiang wenjie li sujian li furu wei yanran li attsum joint learning focusing summarization neural attention coling international conference computational linguistics proceedings conference technical papers december osaka japan pages computational linguistics volume number cao ziqiang chuwei luo wenjie li sujian hu baotian qingcai chen fangze zhu li joint copying restricted generation paraphrase proceedings thirty aaai conference articial intelligence pages cao ziqiang furu wei sujian li wenjie li ming zhou houfeng wang learning summary prior representation extractive summarization proceedings annual meeting association computational linguistics international joint conference natural language processing asian federation natural language processing acl july beijing china volume short papers pages cheng jianpeng mirella lapata neural summarization extracting sentences words proceedings annual meeting association computational linguistics acl august berlin germany volume long papers cho kyunghyun bart van merrienboer aglar glehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase representations rnn encoder decoder statistical machine translation proceedings conference empirical methods natural language processing emnlp pages chopra sumit michael auli alexander m rush abstractive sentence summarization attentive recurrent neural networks naacl hlt conference north american chapter association computational linguistics human language technologies pages filippova katja enrique alfonseca carlos colmenares lukasz kaiser oriol vinyals sentence compression deletion lstms proceedings conference empirical methods natural language processing emnlp pages glava goran sanja tajner simplifying lexical simplication need simplied corpora proceedings annual meeting association computational linguistics acl pages gu jiatao zhengdong lu hang li victor o k li incorporating copying mechanism sequence sequence learning proceedings annual meeting association computational linguistics acl hochreiter sepp jrgen schmidhuber long short term memory neural computation lcsts large scale chinese short text summarization dataset proceedings conference empirical methods natural language processing emnlp lisbon portugal september pages hwang william hannaneh hajishirzi mari ostendorf wei wu aligning sentences standard wikipedia simple wikipedia naacl hlt pages jean sbastien kyunghyun cho roland memisevic yoshua bengio large target vocabulary neural machine translation proceedings annual meeting association computational linguistics acl pages kauchak david improving text simplication language modeling unsimplied text data proceedings annual meeting association computational linguistics acl pages kingma diederik p jimmy ba adam method stochastic optimization corr lin chin yew eduard h hovy automatic evaluation summaries n gram co occurrence statistics human language technology conference north american chapter association computational linguistics hlt naacl lopyrev konstantin generating news headlines recurrent neural networks corr luong thang hieu pham christopher d manning effective approaches attention based neural machine translation proceedings conference empirical methods natural language processing emnlp pages ma shuming xu sun jingjing xu houfeng wang wenjie li qi su improving semantic relevance sequence sequence learning chinese social media text summarization proceedings annual meeting association computational linguistics acl vancouver canada july august volume short papers pages manning christopher d mihai surdeanu john bauer jenny rose finkel steven bethard david mcclosky stanford corenlp natural language processing toolkit proceedings annual meeting association computational linguistics acl pages shuming ma et al semantic relevance based neural network summarization simplication nisioi sergiu sanja stajner simone paolo xu jingjing xu sun nallapati ramesh bowen zhou ccero nogueira dos santos aglar glehre bing xiang abstractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning conll berlin germany august pages narayan shashi claire gardent hybrid simplication deep semantics machine translation proceedings annual meeting association computational linguistics acl pages ponzetto liviu p dinu exploring neural text simplication models proceedings annual meeting association computational linguistics acl pages radev dragomir r timothy allison sasha blair goldensohn john blitzer arda elebi stanko dimitrov elliott drbek ali hakim wai lam danyu liu jahna otterbacher hong qi horacio saggion simone teufel michael topper adam winkel zhu zhang mead platform multidocument multilingual text summarization proceedings fourth international conference language resources evaluation lrec rush alexander m sumit chopra jason weston neural attention model abstractive sentence summarization proceedings conference empirical methods natural language processing emnlp lisbon portugal september pages srivastava nitish geoffrey e hinton alex krizhevsky ilya sutskever ruslan salakhutdinov dropout simple way prevent neural networks overtting journal machine learning research sun xu houfeng wang wenjie li fast online training frequency adaptive learning rates chinese word segmentation new word detection proceedings pages sutskever ilya oriol vinyals quoc v le sequence sequence learning neural networks advances neural information processing systems annual conference neural information processing systems pages wang wenhui baobao chang graph based dependency parsing bidirectional lstm proceedings annual meeting association computational linguistics acl woodsend kristian mirella lapata learning simplify sentences quasi synchronous grammar integer programming proceedings conference empirical methods natural language processing emnlp pages wubben sander antal van den bosch emiel krahmer sentence simplication monolingual machine translation annual meeting association computational linguistics proceedings conference pages dependency based gated recursive neural network chinese word segmentation meeting association computational linguistics pages xu wei courtney napoles ellie pavlick quanze chen chris callison burch optimizing statistical machine translation text simplication tacl zhang xingxing mirella lapata sentence simplication deep reinforcement learning corr zhu zhemin delphine bernhard iryna gurevych monolingual tree based translation model sentence simplication coling pages
