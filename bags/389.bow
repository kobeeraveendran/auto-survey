shot learning interleaved text summarization model pretraining synthetic data sanjeev kumar francine chen yan ying ulli hinrich sch information language processing cis lmu munich intelligence siemens healthineers princeton research institute los altos california intelligence siemens munich lmu org yan ying global ulli com org abstract interleaved texts posts belonging ferent threads occur sequence commonly occur online chat posts time consuming quickly obtain overview discussions existing systems entangle posts threads extract summaries threads major issue systems error propagation disentanglement component end trainable summarization system obviate explicit disentanglement systems require large labeled data address propose pretrain end trainable hierarchical encoder decoder system synthetic interleaved texts tuning real world ing dataset ami system performs traditional step system compare transformer models observed pretraining synthetic data encoder decoder outperforms transformer model trains encoder large dataset introduction interleaved texts increasingly common ring social media conversations slack stack exchange posts belonging ferent threads intermixed post quence meeting transcript ami corpus mccowan table mixing getting quick sense different conversational threads difcult conversation disentanglement interleaved posts grouped thread reader read posts cluster threads gist shang proposed step system takes interleaved text input rst disentangles posts thread wise clustering compresses thread wise posts single sentence summaries entanglement wang oard gates error downstream summarization task end end supervised summarization system implicitly identies conversations eliminate error propagation labeling interleaved texts difcult expensive task barker aker verberne ami utteracnes gon powerpoint presentation think huh sender send telly red signal tell switch switch got look desi designing ergonomics look little tiny weeny batteries like special lasting batteries summary project manager team members introduce industrial designer discussed interior workings remote team discussed options batteries infra red signals marketing expert presented research consumer preferences remotes general voice recognition team discussed option ergonomically designed remote table section shows ami asr scripts section shows human written summaries utterances refer ath sentence multi sentence mary propose pretraining approach tackle issues synthesized corpus leaved text summary pairs corpus ular document summary pairs train end end trainable encoder decoder system generate summary model learns infer gle major topics threads synthetic real world data decoder system obviates ment component enhances performance summarization task acts auxiliary task disentanglement additionally tuning encoder decode system learned disentanglement representations real world ami dataset achieves tial increment evaluation metrics despite small number labels propose hierarchical attention encoder decoder system levels formation interleaved text posts phrases words traditional levels post word nallapati tan cheng lapata remaining paper structured follows section discuss related work section provide detailed description section provide cal model detailed description synthetic data creation algorithm section describe discuss experiments section present conclusions related work aker shang designed system summarizes posts multi party conversations order vide readers overview discussed ters broadly follow step proach cluster posts extract mary cluster kinds summarization tive extractive abstractive summarization model utilizes level vocabulary generates novel sentences summary extractive models extract rearrange source words summary abstractive models based neural sequence sequence rush proved generate summaries higher rouge scores feature based stractive models proposed encoder decoder auto encoder model utilizes hierarchy networks word word followed sentence sentence model better capturing derlying structure vanilla sequential decoder model krause jing showed multi sentence ing image hierarchical recurrent neural network rnn topic topic followed word word better works suggest hierarchical decoder thread thread followed word word intrinsically disentangle posts generate appropriate summaries integration attention model bahdanau led advancement abstractive summarization nallapati chopra nallapati devised hierarchical attention mechanism model levels attention distributions source sentence word puted step word decoding based sentence attentions word attentions rescaled hierarchical attention itive computes level attentions new summary sentence trained end end semi supervised learning recently gained popularity helps training parameters large models training data researchers pre trained masked language models encoder reconstruct text bert devlin liu lapata bert encoder showed proved performance abstractive marization tasks similarly researchers lished pre trained models ent semi supervised learning technique model learned reconstruct original text bart lewis mass song work rely transfer learning demonstrate pretraining appropriate interleaved text data model readily transfers new domain examples model hierarchical encoder figure left hand section based nallapati word word post post encoders directional lstms word word bilstm encoder runs word embeddings post generates set hidden tions dimensions average pooled value word word resentations post post post bilstm encoder generates set representations corresponding posts figure hierarchical encoder decoder architecture left interleaved posts encoded hierarchically word word followed post post right summaries generated hierarchically thread thread followed word word overall given channel output tions word word post post dimensions respectively hierarchical decoder uni directional lstm decoders thread thread word word right hand figure step thread decoder pute elements post level attention attn aligns current thread decoder state vector vectors matrix phrase short quences words sentence post phrases interleaved texts equivalent visual patterns images attending phrases relevant thread recognition attending posts phrase level attentions focusing words channel responsibility entangling threads step thread decoder compute sequence attention weights corresponding set encoded word representations add aligns post representation word resentations element wise addition attn maps current thread decoder state vector scalar value use post level attention rescale sequence attention weights obtain phrase level tions weighted representation words crossed blue circle jwij input compute state thread thread decoder additionally use hidden state word word decoder lstm previously ated summary sentence second input motivation provide information previous sentence current state passed gle layer feedforward network distribution computed pst feedforward network figure process depicted yellow circle thread thread decoder keeps decoding pst greater additionally new state inputs step passed layer feedforward network followed dropout layer compute thread representation given thread representation word word decoder unidirectional attentional lstm generates summary thread right hand figure word word decoder based bahdanau step word word decoding summary thread compute elements word level attention refer bahdanau details use phrase level word attentions rescaling word level attention norm norm softmax renormalizes values contrary popular level hierarchical attention nallapati cheng lapata tan levels cal attention responsibility coordinated rescaling operation train minimize model following end end notations sequence pairs single thread texts sentence summaries minimum number threads maximum number threads minimum number posts maximum number posts window returns iterator object traverse given sequence elements windowed manner window size step size iterator object returns window size sequence pairs single thread texts single sentence maries window size sequence pairs single thread texts single sentence summaries sequence sentences sequence single sentence summaries uniform sampling function single thread text sequence sentences single sentence summary sequence words multi thread interleaved text multi sentence summary interleaved text set variable repeated times sequence positive integers removes element array sentence post sequence words array indexing operation set elements belong sequence pairs multi thread interleaved texts multi sentence summaries reverse reverses array pop table use lowercase italics variables percase italics sets sequences math symbols mathematical operations uppercase words methods tive log yst words ground truth summary generation respectively synthetic dataset obtaining labeled training data interleaved versation summarization challenging able ones extractive verberne small barker anguera train neural model thoroughly ify architecture issue synthesized dataset utilizing corpus ventional texts summaries available created corpus interleaved texts abstracts titles articles pubmed corpus dernoncourt lee chose pubmed abstracts contrast pora news articles stackoverow posts single sentence summary prehended abstract number sentences closely resembles conversationalist conversation selection array array algorithm interleaving algorithm procedure array pop array array return interleaving random interleaving sentences small number pubmed abstracts roughly bles interleaved texts correspondingly leaving titles resembles multi sentence mary devised algorithm creating thetic interleaved texts based idea interleave algorithm interleave rithm generates interleaved texts containing randomly interleaved sentences small ber abstracts number random value specied range number tences abstract random value specied range abstracts included interleaved text rst selected selected abstracts interleaved nally interleaved texts concatenation titles selected abstracts returned rst refer table terms tations algorithm interleave takes corpus abstract title pairs returns quence pairs multi thread interleaved texts multi sentence summaries leaved text generated sequence contain number threads ranging number randomly selected thread turn contain number posts sentences ranging number randomly selected window function given desired window size step size returns iterator object size window helps enlarge interleaved corpus redundancy abstracts randomly pled iterator element new abstracts included element sliding similarly sets sentences domly sampled selected abstracts interleaved text summary pairs corpus different parts interleave rithm selection interleaving described selection step determines number threads thread candidates leaved text chosen iterator element window size sequence pairs single thread texts single sentence summaries post candidates selected thread sen step determines number posts thread indices repeated times posts stored set interleaving step loop size equivalent length indices randomly selects thread index reverse pop step help selecting post selected thread fifo manner single sentence summary thread added multi sentence mary sequence exist previously interleaved text summary pair pus thread size post size thread larger ference corpus harder disentangling summarization task vary parameters create ferent synthetic corpora varying difculty experiments table shows example data instance interleaved pubmed corpus compiled experiments parameters word word encoder steps limited steps word decoder limited steps post post encoder thread thread decoder depend corpus type hard corpus piled steps post post encoder maximum sible size posts item corpus steps thread thread decoder mum possible threads item corpus initialized weights including word embeddings random normal distribution mean standard deviation embedding vectors interleaved text conducted evaluate inuence excessive ing long distance running urinary concentration caffeine assess effect program supervised tness walking patient education functional status patients documented diagnosis primary osteoarthritis knees participated examined effects intensity training ratings perceived exertion summary caffeine sport inuence endurance exercise urinary caffeine concentration supervised tness walking patients osteoarthritis knee randomized controlled trial effect training intensity ratings perceived tion table example synthetic interleaved text summary pair compiled pubmed corpus gorithm includes threads abstracts able superscribed symbols model input text dis upper shang ent rouge table synthetic interleaved text summarization performance rouge recall scores comparing models threads disentangled section upper bound threads entangled tion real world interleaved pubmed corpus dis disentangled ground truth ent entangled hidden states encoder decoder models set dimension texts cased vocabulary size limited pad short sequences special token use adam kingma tial learning rate batch size training training evaluation test sets hard interleaved pubmed corpus sizes tively report quantitative evaluation models upper bound upper bound experiments check impact disentanglement tive summarization models order rst evaluate performance model provided ground truth disentanglement thread indices information evaluate mance models end end step summarization ground truth disentangled ground truth disentanglement information posts threads disentangled concatenated posts thread wise sequentially arranged interleaved rst row table shows formance summarization model clearly model easily detect thread ary concatenated threads perform sets upper bound task disentanglement real world ios disentanglement shang unsupervised step system entangles clusters posts thread wise compresses clusters single sentence summaries trained end end generates multi sentence summaries given terleaved text table shows shang performs worse compare rows indicating model trained sufciently large dataset better tion unsupervised sentence compression method especially uency indicated approximately point increase tionally model trained entangled texts achieves slightly lower performance trained disentangled texts compare rows indicating disentanglement ponent avoided summaries available section table example model generations shown color indexes phrase level attention directly visualized table color coding matching generation shows phrase level attention actually supports learning gle interleaved texts transfer learning utilize interleaving algorithm pubmed data compile leaved corpus similar thread distribution corpus real meetings ami meeting corpus ami small size corpus train eval test split respectively analysis ami meetings summary sentences meetings summary sentences min max number threads respectively algorithm ate synthetic corpus pretrain model iterations synthetic corpus transfer tune model ami corpus parameters xed word word decoder hierarchical tion parameters pubmed ami interleaved text conducted evaluate inuence excessive ing long distance running urinary tion caffeine assess effect program supervised tness walking patient education functional status patients documented diagnosis primary teoarthritis knees participated examined effects intensity training ratings perceived exertion generation effect excessive running urinary concentration caffeine effect physical tness walking functional status pain pain effects intensity training perceived athletes table example generated summary sentences thread interleaved text summaries coloured differently colors attended phrases text identical generations table best viewed color figure rouge gram precision green recall blue ami tuned models different numbers pretraining iterations mum words summary reference solid horizontal lines scores model trained ami different domains use byte pair encoding bpe sennrich based subword tionary shown table readily transfers disentangling knowledge fore obtains boost recall maintaining precision system best scores model directly comparable unlike shang text based model uses audio video addition text additionally performed transfer ing experiments models pre trained ferent number iterations seen figure readily transfers disentangling edge obtains boost recall maintaining precision longer ing drives model generate shorter summaries similar pubmed abstracts results increasing precision decreasing recall experimented state art transformer based models sumextabs liu lapata bart lewis requires tuning encoder novo training decoder encoder decoder bart tuned use ami data novo training tuning purpose rows table results models learn requires tuning decoder hierarchical attention highly sophisticated supervised training encoder decoder bart larger model size yields better performance applications limited memory mobile devices model desirable furthermore spite pre trained encoder novo training large size decoder tiny ami data lead lower scores human evaluation performed itative evaluation system human ments following chen bansal performed comparative evaluation vided human judges graduate students uent english meetings words maries sources human reference step baseline learn referred model asked rate scale questions summary concise uent grammatical uency summary retain key information meeting relevancy sampled meetings maries corresponding sources duplicated randomly sampled dissimilar meetings assigned judge tate reference annotation sample asr transcript human written summaries model step shang learn rouge scores summary size table words ami corpus learn transfer leaning bart encoder decoder ers parameters uses addition text transformer models rows lots extra data pre training learn summaries project manager opened meeting went minutes previous industrial designer discussed interior workings remote team group discussed shape device decided device easier step shang summaries marketing report observed remote control users usability lab majority fty year olds reason want voice act speech headed like big yellow black remote far maybe meeting discuss table sections archical shang system summaries respectively asr transcripts table refer ath sentence multi sentence summary table model shang summaries table judges shown source summaries ratings received converted nary comparisons summarized table model summaries judged ter shang system summaries uency relevancy gwet brennan prediger kappa inter rater ment statistics strong agreement uency compared human summaries model summaries similar terms uency lower terms relevancy rater statistics indicating fair strength agreement small ami data size batch size initial learning rate bertsumext set tively batch size bertsumextabs initial learning rates bert transformer decoder bertsumextabs respectively brennan prediger kappa impact empirical distributions chance agreement better suited cases proportion agreements class differs metric win tie lose gwet model shang model human reference fluency relevancy fluency relevancy table comparative ratings human judges maries uency relevancy metrics gwet refer gwet brennan prediger kappa coefcients respectively compared statistics reference maries shang model generated summaries maximum words observe model generates approximately words outputs close ground truth man written summaries size approximately words shang system generates summaries average words ther median number threads number maries model human written summaries shang respectively indicates model learning generate human like summaries shang aims distill words permissible limit high recall low precision table additionally model twice shang values cates high readability supported human judges difference number threads summaries model reference cases respectively clearly indicates strength hierarchical model disentangling threads conclusion investigated use end end cal encoder decoder model levels hierarchical attention jointly rizing implicitly disentangling interleaved text train model examined use training synthesized data tuning adaptation new domain limited labeled real world data real world ami data tuned end end system outperforms step system experiments conducted transformer based sumextabs bart systems indicate transformer models rize interleaved texts specically model outperformed transformer based bart suggests use pretraining decoder encoder important indicates utility synthetic data references ahmet aker monica paramita emina kurtic adam funk emma barker mark hepple rob gaizauskas automatic label generation news comment clusters proceedings international natural language generation ence pages xavier anguera simon bozonnet nicholas evans corinne fredouille gerald friedland oriol vinyals speaker diarization review cent research ieee transactions audio speech language processing dzmitry bahdanau kyunghyun cho yoshua neural machine translation corr bengio jointly learning align translate emma barker monica lestari paramita ahmet aker emina kurtic mark hepple robert gaizauskas sensei annotated corpus human maries reader comment conversations line news proceedings annual meeting special interest group discourse dialogue pages yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proceedings annual ing association computational tics volume long papers pages bourne australia association computational linguistics jianpeng cheng mirella lapata neural marization extracting sentences words proceedings annual meeting sociation computational linguistics volume long papers pages sumit chopra michael auli alexander rush abstractive sentence summarization naacl hlt tentive recurrent neural networks conference north american chapter association computational guistics human language technologies san diego california usa june pages association computational linguistics franck dernoncourt young lee pubmed rct dataset sequential sentence proceedings cation medical abstracts eighth international joint conference ral language processing volume short papers pages asian federation natural guage processing jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language ing arxiv preprint baoyu jing pengtao xie eric xing automatic generation medical imaging reports proceedings annual meeting association computational linguistics volume long papers pages association computational linguistics diederik kingma jimmy adam corr method stochastic optimization jonathan krause justin johnson ranjay krishna fei fei hierarchical approach erating descriptive image paragraphs computer vision patterm recognition cvpr mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov luke zettlemoyer bart denoising sequence sequence training natural arxiv preprint lation language generation comprehension jiwei thang luong dan jurafsky hierarchical neural autoencoder paragraphs documents proceedings annual ing association computational tics international joint conference natural language processing volume long pers pages association tional linguistics manling lingyu zhang heng richard radke meeting summaries topic abstractive multi modal meeting summarization proceedings association computational linguistics pages florence italy association tational linguistics annual meeting yang liu mirella lapata text proceedings tion pretrained encoders conference empirical methods ural language processing international joint conference natural language processing emnlp ijcnlp pages hong kong china association computational linguistics zongyang aixin sun quan yuan gao cong topic driven reader comments tion proceedings acm international conference information knowledge ment pages acm mccowan carletta kraaij ashby ban flynn guillemot hain kadlec karaiskos kronenthal lathoud coln lisowska post dennis reidsma wellner ami meeting corpus ceedings measuring behavior tional conference methods techniques havioral research pages noldus tion technology ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization aaai conference articial uments gence ramesh nallapati bowen zhou ccero nogueira dos santos aglar gulcehre bing xiang abstractive text summarization sequence proceedings sequence rnns signll conference computational natural language learning conll berlin germany august pages acl alexander rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing emnlp lisbon portugal september pages ciation computational linguistics rico sennrich barry haddow alexandra birch neural machine translation rare words subword units proceedings nual meeting association computational linguistics volume long papers pages berlin germany association tional linguistics guokan shang wensi ding zekun zhang toine tixier polykarpos meladianos michalis giannis jean pierre vised abstractive meeting summarization sentence compression budgeted submodular proceedings annual maximization meeting association computational guistics volume long papers pages association computational linguistics kaitao song tan tao qin jianfeng yan liu mass masked sequence sequence pre training language generation arxiv preprint jiwei tan xiaojun wan jianguo xiao neural sentence summarization headline ijcai generation coarse approach pages suzan verberne emiel krahmer iris hendrickx sander wubben antal van den bosch creating reference data set summarization discussion forum threads language resources evaluation pages lidan wang douglas oard based message expansion disentanglement terleaved text conversations proceedings man language technologies annual ference north american chapter ation computational linguistics pages association computational linguistics
