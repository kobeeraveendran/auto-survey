gret global representation enhanced transformer rongxiang haoran shujian heng lidong weihua jiajun key laboratory novel software technology nanjing university nanjing china intelligence technology lab alibaba group hangzhou china wengrx funan whr inc com edu cn yuheng yh l bing weihua luowh inc com edu cn e f l c s c v v x r abstract transformer based encoder decoder framework achieved state art performance natural guage generation tasks encoder maps words sentence sequence hidden states fed decoder generate output sentence hidden states usually correspond input words focus capturing local information global sentence level information seldom explored leaving room improvement generation quality paper propose novel global representation enhanced transformer gret explicitly model global representation transformer network specically proposed model external state generated global representation encoder global representation fused decoder ing decoding process improve generation quality conduct experiments text generation tasks machine translation text summarization experimental results wmt machine translation tasks lcsts text marization task demonstrate effectiveness proposed approach natural language generation introduction transformer vaswani et al outperformed methods neural language generation nlg tasks like machine translation deng et al text tion chang huang hsu generally based encoder decoder framework consists modules encoder network decoder network encoder encodes input sentence quence hidden states corresponds cic word sentence decoder generates output sentence word word decoding time step coder performs attentive read luong pham manning vaswani et al fetch input hidden states decides word generate mentioned decoding process relies representations contained hidden states evidence showing den states encoder transformer contain corresponding author copyright association advancement articial intelligence www aaai org rights reserved local representations focus word level tion example previous work vaswani et al devlin et al song et al showed den states pay attention word word mapping weights attention mechanism determining target word generated similar word alignment frazier pointed global information sentence contrast individual words involved process generating sentence representation global information plays import role neural text generation tasks recurrent neural network rnn based models bahdanau cho bengio chen showed text summarization task introducing representations global information improve quality reduce repetition lin et al showed machine translation structure lated sentence correct introducing global information previous work shows global tion useful current neural network based model different rnn sutskever vinyals le cho et al bahdanau cho bengio cnn gehring et al self attention mechanism achieve long distance dependence explicit mechanism transformer model global representation sentence appealing challenge provide transformer kind global representation paper divide challenge issues need addressed model global contextual information use global information generation process propose novel global tion enhanced transformer gret solve rst issue propose generate global representation based local word level representations mentary methods encoding stage hand adopt modied capsule network sabour frosst ton generate global representation based tures extracted local word level representations cal representations generally related word word mapping redundant noisy generate global representation directly tering inadvisable capsule network strong ability feature extraction zhao et al help extract suitable features local states comparing networks like cnn krizhevsky sutskever hinton local states time tract feature vectors times deliberation hand propose layer wise recurrent structure strengthen global representation vious work shows representations layer different aspects meaning peters et al dou et al e lower layer contains syntactic information higher layer contains semantic information complete global context different aspects formation global representation generated capsule network obtain intra layer information proposed layer wise recurrent structure helpful ment combine inter layer information aggregating resentations layers methods model global representation fully utilizing different grained formation local representations second issue propose use context ing mechanism dynamically control tion global representation fused decoder step generation process coder states obtain global contextual information fore outputting words demand global information varies word word output sentence proposed gating mechanism utilize global resentation effectively improve generation quality viding customized representation state experimental results wmt translation tasks lcsts text summarization task gret model brings signicant improvements strong baseline previous researches approach gret model includes steps modeling global representation encoding stage incorporating decoding process describe approach section based transformer vaswani et al modeling global representation encoding stage propose methods eling global representation different granularity rstly use capsule network extract features local word level representations generate global tion based features layer wise recurrent structure adopted subsequently strengthen global representation aggregating representations layers encoder rst method focuses utilizing word level information generate sentence level sentation second method focuses combining different aspects sentence level information obtain complete global representation intra layer representation generation propose use capsules dynamic routing extract specic suitable features local representations stronger global representation modeling effective k output layer algorithm dynamic routing algorithm procedure r input layer output layer bki end r iterations end return u end input layer output layer ck end k output layer ckihi bki hi bki end h u hi strong feature extraction method sabour frosst hinton zhang liu song features hidden states encoder summarized capsules weights routes hidden states capsules updated dynamic routing algorithm iteratively formally given encoder transformer m layers input sentence xi words sequence hidden states hm hm hm mth layer encoder computed hm xi hm e e e e e qm query key value e vectors hidden states layer ln m layer normalization san function ba kiros hinton self attention work vaswani et al respectively omit ual network capsules um size k generated hm specically kth capsule um k computed um q h m wkhm m ckih ck non linear squash function sabour frosst q hinton ck computed t t t t ck bk b details capsule network shown sabour frosst hinton figure overview generating global tion capsule network matrix b initialized zero row column k respectively matrix dated capsules produced b b hm algorithm shown algorithm sequence capsules um generate global tation different original capsules network use concatenation method generate nal representation use attentive pooling method generate global formally mth layer global sentation computed k sm ffn akum k ak m um m um sm ffn k k um ffn puted feed forward network s m attentive method consider different roles capsules better model global representation overview process generating global tion shown figure inter layer representation aggregation traditionally transformer model fed layer s hidden states concatenation pooling methods e mean pooling easily decrease bleu machine translation experiment figure overview layer wise recurrent structure hm representations input sentence decoder generate output sentence following feed layer s global representation sm decoder directly current global representation tain intra layer information layers tations ignored shown different aspects meaning previous work wang et al dou et al based intuition propose layer wise recurrent structure aggregate tions generated employing capsule network ers encoder model complete global representation layer wise recurrent structure aggregates layer s intra global state gated recurrent unit cho et al gru achieve different aspects information previous layer s global representation formally adjust computing method sm atp attentive pooling function puted eq gru unit control tion ow forgetting useless information capturing suitable information aggregate previous layer s representations usefully layer wise recurrent structure achieve exquisite complete tion proposed structure need step encoding stage time consuming overview aggregation structure shown figure incorporating decoding process generating output word decoder state consider global contextual information bine global representation decoding process additive operation layer decoder guiding states output true words demand global information target word different propose context gating mechanism provide cic information according decoder hidden state specically given decoder n layers target sentence j words training stage hidden states rn rn rn j n layer decoder computed d kn rn d km vn e vm e rn j layer local statescapsulesglobal statedynamic layermth layer public widely previous work researchers replicate work easily machine translation zh en task use training set consists m tence pairs use validation set test set en sentence pairs respectively en tasks use training set consists m sentence pairs use validation set test set sentence pairs respectively ro en task use training set consists m sentence pairs use validation set test set sentence pairs respectively de de text summarization following hu chen zhu use training set consists m sentence pairs use subsets ii iii scored validation test sets sists sentence pairs respectively machine translation apply byte pair settings ing bpe sennrich haddow birch guage pairs limit vocabulary size k text summarization limit vocabulary size based character level vocabulary words chars replaced special token unk transformer set dimension input output layers feed forward layer employ parallel attention heads number layers encoder decoder sentence pairs batched approximate sentence length batch sentence maximum length sentence limited set value dropout rate use adam kingma ba update ters learning rate varied warm egy steps vaswani et al details shown vaswani et al number capsules set default time iteration set training time transformer days de en task training time gret model hours parameters baseline initialization training stage use beam search heuristic decoding beam size set measure tion quality nist bleu papineni et al summarization quality rouge lin main results machine translation employ proposed gret model machine translation tasks results marized table fair comparison reported eral transformer baselines settings reported previous work vaswani et al hassan et al gu et al researches enhancing local word level representations dou et al yang et al shaw uszkoreit vaswani yang et al results zh en task shown second column table improvement gret figure context gating mechanism fusing global representation decoding stage d kn qn layer km n residual network hidden state rn j vn e vm calculated hidden states rn e hm omit rn context gate gj j sm new state contains needed global tion computed rn j j sm g j output probability calculated output layer s hidden state p y j j method enables state achieve s customized global information overview shown figure training training process gret model dard transformer networks optimized maximizing likelihood output sentence y given input sentence denoted ltrans ltrans j j log p y j p y j dened equation experiment implementation detail data sets conduct experiments machine translation text summarization tasks machine translation employ approach language pairs chinese de german glish zh en english de text summarization use lcsts hu chen zhu evaluate proposed method data sets en romanian english ro en english german en statmt org translation task html hitsz edu cn article html step model transformer vaswani et al transformer hassan et al transformer gu et al deeprepre dou et al localness yang et al relpos shaw uszkoreit vaswani context aware yang et al gdr zheng et al transformer gret zhen ende deen roen table comparison gret transformer baseline related work chinese english zh english german en indicates results came paper en romania english ro indicate signicantly better baseline p de german english de en en tasks model rnnsearch hu chen zhu copynet gu et al mrt ayana liu sun ac abs li bing lam cgu lin et al transformer chang huang hsu transformer gret rouge l table comparison gret transformer baseline related work lcsts text summarization task indicates results came paper model based strong baseline system outperforms previous work reported best knowledge approach attains state art relevant researches results en en tasks widely data set recently shown fourth columns gret model attain bleu en de bleu de en competitive sults compared previous studies de de verify generality approach periment low resource language pair ro en task results shown column provement gret bleu material improvement low resource language pair shows proposed methods improve translation quality low resource scenario experimental results machine translation tasks modeling global representation current transformer network general approach limited language size training data ing translation quality text summarization machine translation employ proposed methods text summarization monolingual generation task important ical task natural language generation results shown table reports eral popular methods data set comparison figure comparison gtr different number de task capsules different iteration times en approach achieves considerable improvements l outperforms work settings improvement text summarization machine translation compared chine translation text summarization focuses tracting suitable information input sentence advantage gret model experiments tasks approach work different types language generation task improve performance text generation tasks ablation study effectiveness consumption module gret model ablation study model transformer capsule aggregate gate param inference bleu m m m m m m m m m approach table ablation study english german en de machine translation task model transformer base gtr base transformer big gret big param inference m m m m bleu table comparison gret transformer big setting vaswani et al en de task model average gret precision table precision bag words predictor based gret encoder state averaging local states average en de task section specically investigate capsule work aggregate structure gating mechanism affect performance global representation results shown table specically capsule network performance decreases bleu means extracting features local tions iteratively reduce redundant information noisy step determines quality global tation directly aggregating multi layers tions attains bleu improvement different aspects information layer excellent complement generating global representation gating mechanism performance decreases bleu score shows context gating mechanism important control proportion global representation decoding step gret model time think worthwhile improve generation quality reducing bit efciency scenario effectiveness different model settings experiment gret model big setting de task big model far larger base en model state art performance previous work vaswani et al results shown table transformer big figure comparison gtr different number de task capsules different iteration times en performs transformer base gret big improves bleu score comparing transformer big worth mention model base setting achieve similar performance big reduces parameters m vs m inference time vs analysis capsule number capsules iteration time namic routing algorithm affect performance proposed model evaluate gret model ent number capsules different iteration times en de task results shown figure empirical conclusions experiment rst iterations signicantly improve performance results iterations tend stabilize second increase capsule number nt gain think reason sentences shorter suitable capsules extract features probing experiment global representation learn interesting question following weng et al probing experiment train bag words predictor sm ybow unordered set imizing p taining words output sentence structure predictor simple feed forward network maps global state target word embedding matrix compare precision target words k words chosen predicted probability figure translation cases transformer gret model zh en task results shown table global state gret higher precision conditions shows proposed method obtain formation output sentence partial answers gret model improve generation quality analysis sentence length effectiveness global representation group en de test set length input tences evaluate models set divided sets figure shows results nd model outperforms baseline categories especially longer sentences shows fusing global resentation help generation longer sentences providing complete information case study real cases zh en task ference baseline model cases shown figure source indicates source sentence reference indicates human translation bold font indicates improvements model italic font indicates translation errors output gret decided previous state global representation avoid common translation errors like translation caused strong language model decoder ignores translation information example translation cities hefei case corrected gret model furthermore providing global information avoid current state focuses word word mapping case vanilla transformer translates moscow travel police according source input mosike lvyou jingcha omits words renyuan zhaolu leads fails translate target word recruiting related work work try generate global representation machine translation lin et al propose volutional method obtain global information guide translation process rnn based model itation cnn model global information methods employ transformer details shown weng et al text summarization chen propose rate global information rnn based model reduce tition use additional rnn model global resentation time consuming long dependence relationship hinders ness global representation zhang liu song propose sentence state lstm text representation method shows tive way obtaining representation tion transformer previous researches notice importance representations generated encoder focus ing use wang et al propose use sule network generate hidden states directly inspire use capsules dynamic routing algorithm extract specic suitable features hidden states wang et al dou et al propose utilize hidden states multiple layers contain different aspects information model complete representations inspires use states multiple layers enhance global representation conclusion paper address problem transformer nt model global contextual information crease generation quality propose novel gret model generate external state encoder taining global information fuse decoder namically approach solves issues model use global contextual information compare proposed gret state art model experimental results translation tasks text summarization task demonstrate ness approach future analysis combine methods enhancing local resentations improve generation performance acknowledgements like thank reviewers insightful comments shujian huang corresponding author work supported national key program china national science foundation china jiangsu provincial research foundation basic research s referenceinadditiontosuzhou othersecond tiercitiesincludinghefeiandnanjingwillalsointroducepropertymarketregulationsandcontrolpolicies transformerthesecond tiercities includinghefeiandnanjing arenotonlysuzhou butalsothecitiesofhefei gretthesecond tiercities notonlysuzhou butalsohefeiandnanjingwillalsointroducepropertymarketregulationsandcontrolpolicies m referencetherecruiting policemodeandequipmentofmoscowtourismpoliceofcershaveinspiredusalot transformerwehavealotofinspirationfromthemoscowtravelpolice thepolicemodel andtheequipment gretwehavealotofinspirationbythemoscowtravelpolicesrecruiting policemode andequipment experimenthere wetrainabag wordspredictorbymax tainingallwordsintheoutputsentence thestructureofthepredictorisasimplefeed forwardnetworkwhichmapstheglobalstatetothetargetwordembeddingmatrix wecomparetheprecisionofthetargetwordsinthetop kwordswhicharechosenthroughthepredictedprob ditions whichshowsthattheproposedmethodcanobtainmoreinformationabouttheoutputsentenceandpartialan swerswhythegretmodelcouldimprovethegenerationquality tencestore evaluatethemodels wendthatourmodeloutperformsthebaselineinallcategories especiallyinthelongersentences whichshowsthatfusingtheglobalrep resentationmayhelpthegenerationoflongersentencesbyprovidingmorecompleteinformation ferencebetweenthebaselineandourmodel thesourceindicatesthesourcesentenceandthereferenceindicateshumantranslation eachoutputfromgretisdecidedbypreviousstateandtheglobalrepresentation itcanavoidsomecommontranslationerrorslikeover undertranslation causedbythestronglanguagemodelofthedecoderignoressometransla tioninformation forexample furthermore providingglobalinformationcanavoidcurrentstateonlyfocusesonword wordmapping inmachinetranslation linetal volutionalmethodtoobtainglobalinformationtoguidethetranslationprocessinrnn basedmodel thelim itationofcnncannotmodeltheglobalinformationwellandtheremethodscannotemployonthetransformer intextsummarization rateglobalinformationinrnn basedmodeltoreducerepe tition theyuseanadditionalrnntomodeltheglobalrep resentation whichistime consumingandcannotgetthelong dependencerelationship whichhinderstheeffective nessoftheglobalrepresentation zhang liu statelstmfortextrepresentation ourmethodshowsanalterna tivewayofobtainingtherepresentation ontheimplementa tionofthetransformer manypreviousresearchesnoticetheimportanceoftherepresentationsgeneratedbytheencoderandfocusonmak ingfulluseofthem wangetal sulenetworktogeneratehiddenstatesdirectly whichinspireustousecapsuleswithdynamicroutingalgorithmtoextractspecicandsuitablefeaturesfromthesehiddenstates wangetal douetal whichinspiresustousethestatesinmultiplelayerstoenhancetheglobalrepresentation weaddresstheproblemthattransformerdoesntmodelglobalcontextualinformationwhichwillde creasegenerationquality weproposeanovelgretmodeltogenerateanexternalstatebytheencodercon tainingglobalinformationandfuseitintothedecoderdy namically ourapproachsolvesthebothissuesofhowtomodelandhowtousetheglobalcontextualinformation wecomparetheproposedgretwiththestate arttrans formermodel experimentalresultsonfourtranslationtasksandonetextsummarizationtaskdemonstratetheeffective nessoftheapproach inthefuture wewilldomoreanalysisandcombineitwiththemethodsaboutenhancinglocalrep resentationstofurtherimprovegenerationperformance references ayana s s liu z sun m neural line generation minimum risk training arxiv preprint ba j l kiros j r hinton g e layer malization arxiv preprint bahdanau d cho k bengio y neural chine translation jointly learning align translate corr chang c huang c hsu j y hybrid word character model abstractive summarization corr chen g chinese short text summary generation model combining global local information ncce cho k van merrienboer b gulcehre c bahdanau d bougares f schwenk h bengio y learning phrase representations rnn encoder decoder tical machine translation emnlp deng y cheng s lu j song k wang j wu s yao l zhang g zhang h zhang p et al alibaba s neural machine translation systems ence machine translation shared task papers devlin j chang m lee k toutanova k bert pre training deep bidirectional transformers guage understanding arxiv dou z tu z wang x shi s zhang t exploiting deep representations neural machine tion emnlp frazier l sentence processing tutorial review gehring j auli m grangier d dauphin y n convolutional encoder model neural machine translation arxiv preprint gehring j auli m grangier d yarats d dauphin y n convolutional sequence sequence learning arxiv preprint gu j lu z li h li v o copying mechanism sequence sequence learning acl gu j bradbury j xiong c li v o socher r non autoregressive neural machine translation iclr hassan h aue chen c chowdhary v clark j federmann c huang x junczys dowmunt m lewis w li m et al achieving human parity tomatic chinese english news translation arxiv preprint hu b chen q zhu f lcsts large scale chinese short text summarization dataset emnlp kingma d p ba j adam method stochastic optimization corr krizhevsky sutskever hinton g e imagenet classication deep convolutional neural works nips li p bing l lam w actor critic based ing framework abstractive summarization arxiv preprint incorporating lin j sun x ma s su q global encoding abstractive summarization acl lin j sun x ren x ma s su j su q deconvolution based global decoding neural machine translation acl lin c rouge package automatic tion summaries acl luong m pham h manning c d effective approaches attention based neural machine translation emnlp papineni k roukos s ward t zhu w bleu method automatic evaluation machine lation acl peters m e neumann m iyyer m gardner m clark c lee k zettlemoyer l deep contextualized word representations arxiv preprint sabour s frosst n hinton g e dynamic routing capsules corr sennrich r haddow b birch neural chine translation rare words subword units acl shaw p uszkoreit j vaswani self attention relative position representations naacl song k wang k yu h zhang y huang z luo w duan x zhang m alignment enhanced constraining nmt pre specied translations aaai sutskever vinyals o le q v sequence sequence learning neural networks nips vaswani shazeer n parmar n uszkoreit j jones l gomez n kaiser polosukhin tention need nips wang m xie j tan z su j et al ear time neural machine translation capsule networks arxiv wang q li f xiao t li y li y zhu j multi layer representation fusion neural machine lation coling weng r huang s zheng z dai x chen j neural machine translation word predictions emnlp yang b tu z wong d f meng f chao l s zhang t modeling localness self attention works emnlp yang b li j wong d f chao l s wang x tu z context aware self attention networks aaai zhang y liu q song l sentence state lstm text representation acl zhao w ye j yang m lei z zhang s zhao z investigating capsule networks dynamic routing text classication arxiv preprint zheng z huang s tu z dai x chen j dynamic past future neural machine lation emnlp ijcnlp
