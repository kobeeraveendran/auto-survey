gret global representation enhanced transformer rongxiang haoran shujian heng lidong weihua jiajun key laboratory novel software technology nanjing university nanjing china intelligence technology lab alibaba group hangzhou china wengrx funan whr inc com edu yuheng bing weihua luowh inc com edu abstract transformer based encoder decoder framework achieved state art performance natural guage generation tasks encoder maps words sentence sequence hidden states fed decoder generate output sentence hidden states usually correspond input words focus capturing local information global sentence level information seldom explored leaving room improvement generation quality paper propose novel global representation enhanced transformer gret explicitly model global representation transformer network specically proposed model external state generated global representation encoder global representation fused decoder ing decoding process improve generation quality conduct experiments text generation tasks machine translation text summarization experimental results wmt machine translation tasks lcsts text marization task demonstrate effectiveness proposed approach natural language generation introduction transformer vaswani outperformed methods neural language generation nlg tasks like machine translation deng text tion chang huang hsu generally based encoder decoder framework consists modules encoder network decoder network encoder encodes input sentence quence hidden states corresponds cic word sentence decoder generates output sentence word word decoding time step coder performs attentive read luong pham manning vaswani fetch input hidden states decides word generate mentioned decoding process relies representations contained hidden states evidence showing den states encoder transformer contain corresponding author copyright association advancement articial intelligence www aaai org rights reserved local representations focus word level tion example previous work vaswani devlin song showed den states pay attention word word mapping weights attention mechanism determining target word generated similar word alignment frazier pointed global information sentence contrast individual words involved process generating sentence representation global information plays import role neural text generation tasks recurrent neural network rnn based models bahdanau cho bengio chen showed text summarization task introducing representations global information improve quality reduce repetition lin showed machine translation structure lated sentence correct introducing global information previous work shows global tion useful current neural network based model different rnn sutskever vinyals cho bahdanau cho bengio cnn gehring self attention mechanism achieve long distance dependence explicit mechanism transformer model global representation sentence appealing challenge provide transformer kind global representation paper divide challenge issues need addressed model global contextual information use global information generation process propose novel global tion enhanced transformer gret solve rst issue propose generate global representation based local word level representations mentary methods encoding stage hand adopt modied capsule network sabour frosst ton generate global representation based tures extracted local word level representations cal representations generally related word word mapping redundant noisy generate global representation directly tering inadvisable capsule network strong ability feature extraction zhao help extract suitable features local states comparing networks like cnn krizhevsky sutskever hinton local states time tract feature vectors times deliberation hand propose layer wise recurrent structure strengthen global representation vious work shows representations layer different aspects meaning peters dou lower layer contains syntactic information higher layer contains semantic information complete global context different aspects formation global representation generated capsule network obtain intra layer information proposed layer wise recurrent structure helpful ment combine inter layer information aggregating resentations layers methods model global representation fully utilizing different grained formation local representations second issue propose use context ing mechanism dynamically control tion global representation fused decoder step generation process coder states obtain global contextual information fore outputting words demand global information varies word word output sentence proposed gating mechanism utilize global resentation effectively improve generation quality viding customized representation state experimental results wmt translation tasks lcsts text summarization task gret model brings signicant improvements strong baseline previous researches approach gret model includes steps modeling global representation encoding stage incorporating decoding process describe approach section based transformer vaswani modeling global representation encoding stage propose methods eling global representation different granularity rstly use capsule network extract features local word level representations generate global tion based features layer wise recurrent structure adopted subsequently strengthen global representation aggregating representations layers encoder rst method focuses utilizing word level information generate sentence level sentation second method focuses combining different aspects sentence level information obtain complete global representation intra layer representation generation propose use capsules dynamic routing extract specic suitable features local representations stronger global representation modeling effective output layer algorithm dynamic routing algorithm procedure input layer output layer bki end iterations end return end input layer output layer end output layer ckihi bki bki end strong feature extraction method sabour frosst hinton zhang liu song features hidden states encoder summarized capsules weights routes hidden states capsules updated dynamic routing algorithm iteratively formally given encoder transformer layers input sentence words sequence hidden states mth layer encoder computed query key value vectors hidden states layer layer normalization san function kiros hinton self attention work vaswani respectively omit ual network capsules size generated specically kth capsule computed wkhm ckih non linear squash function sabour frosst hinton computed details capsule network shown sabour frosst hinton figure overview generating global tion capsule network matrix initialized zero row column respectively matrix dated capsules produced algorithm shown algorithm sequence capsules generate global tation different original capsules network use concatenation method generate nal representation use attentive pooling method generate global formally mth layer global sentation computed ffn akum ffn ffn puted feed forward network attentive method consider different roles capsules better model global representation overview process generating global tion shown figure inter layer representation aggregation traditionally transformer model fed layer hidden states concatenation pooling methods mean pooling easily decrease bleu machine translation experiment figure overview layer wise recurrent structure representations input sentence decoder generate output sentence following feed layer global representation decoder directly current global representation tain intra layer information layers tations ignored shown different aspects meaning previous work wang dou based intuition propose layer wise recurrent structure aggregate tions generated employing capsule network ers encoder model complete global representation layer wise recurrent structure aggregates layer intra global state gated recurrent unit cho gru achieve different aspects information previous layer global representation formally adjust computing method atp attentive pooling function puted gru unit control tion forgetting useless information capturing suitable information aggregate previous layer representations usefully layer wise recurrent structure achieve exquisite complete tion proposed structure need step encoding stage time consuming overview aggregation structure shown figure incorporating decoding process generating output word decoder state consider global contextual information bine global representation decoding process additive operation layer decoder guiding states output true words demand global information target word different propose context gating mechanism provide cic information according decoder hidden state specically given decoder layers target sentence words training stage hidden states layer decoder computed layer local statescapsulesglobal statedynamic layermth layer public widely previous work researchers replicate work easily machine translation task use training set consists tence pairs use validation set test set sentence pairs respectively tasks use training set consists sentence pairs use validation set test set sentence pairs respectively task use training set consists sentence pairs use validation set test set sentence pairs respectively text summarization following chen zhu use training set consists sentence pairs use subsets iii scored validation test sets sists sentence pairs respectively machine translation apply byte pair settings ing bpe sennrich haddow birch guage pairs limit vocabulary size text summarization limit vocabulary size based character level vocabulary words chars replaced special token unk transformer set dimension input output layers feed forward layer employ parallel attention heads number layers encoder decoder sentence pairs batched approximate sentence length batch sentence maximum length sentence limited set value dropout rate use adam kingma update ters learning rate varied warm egy steps vaswani details shown vaswani number capsules set default time iteration set training time transformer days task training time gret model hours parameters baseline initialization training stage use beam search heuristic decoding beam size set measure tion quality nist bleu papineni summarization quality rouge lin main results machine translation employ proposed gret model machine translation tasks results marized table fair comparison reported eral transformer baselines settings reported previous work vaswani hassan researches enhancing local word level representations dou yang shaw uszkoreit vaswani yang results task shown second column table improvement gret figure context gating mechanism fusing global representation decoding stage layer residual network hidden state calculated hidden states omit context gate new state contains needed global tion computed output probability calculated output layer hidden state method enables state achieve customized global information overview shown figure training training process gret model dard transformer networks optimized maximizing likelihood output sentence given input sentence denoted ltrans ltrans log dened equation experiment implementation detail data sets conduct experiments machine translation text summarization tasks machine translation employ approach language pairs chinese german glish english text summarization use lcsts chen zhu evaluate proposed method data sets romanian english english german statmt org translation task html hitsz edu article html step model transformer vaswani transformer hassan transformer deeprepre dou localness yang relpos shaw uszkoreit vaswani context aware yang gdr zheng transformer gret zhen ende deen roen table comparison gret transformer baseline related work chinese english english german indicates results came paper romania english indicate signicantly better baseline german english tasks model rnnsearch chen zhu copynet mrt ayana liu sun abs bing lam cgu lin transformer chang huang hsu transformer gret rouge table comparison gret transformer baseline related work lcsts text summarization task indicates results came paper model based strong baseline system outperforms previous work reported best knowledge approach attains state art relevant researches results tasks widely data set recently shown fourth columns gret model attain bleu bleu competitive sults compared previous studies verify generality approach periment low resource language pair task results shown column provement gret bleu material improvement low resource language pair shows proposed methods improve translation quality low resource scenario experimental results machine translation tasks modeling global representation current transformer network general approach limited language size training data ing translation quality text summarization machine translation employ proposed methods text summarization monolingual generation task important ical task natural language generation results shown table reports eral popular methods data set comparison figure comparison gtr different number task capsules different iteration times approach achieves considerable improvements outperforms work settings improvement text summarization machine translation compared chine translation text summarization focuses tracting suitable information input sentence advantage gret model experiments tasks approach work different types language generation task improve performance text generation tasks ablation study effectiveness consumption module gret model ablation study model transformer capsule aggregate gate param inference bleu approach table ablation study english german machine translation task model transformer base gtr base transformer big gret big param inference bleu table comparison gret transformer big setting vaswani task model average gret precision table precision bag words predictor based gret encoder state averaging local states average task section specically investigate capsule work aggregate structure gating mechanism affect performance global representation results shown table specically capsule network performance decreases bleu means extracting features local tions iteratively reduce redundant information noisy step determines quality global tation directly aggregating multi layers tions attains bleu improvement different aspects information layer excellent complement generating global representation gating mechanism performance decreases bleu score shows context gating mechanism important control proportion global representation decoding step gret model time think worthwhile improve generation quality reducing bit efciency scenario effectiveness different model settings experiment gret model big setting task big model far larger base model state art performance previous work vaswani results shown table transformer big figure comparison gtr different number task capsules different iteration times performs transformer base gret big improves bleu score comparing transformer big worth mention model base setting achieve similar performance big reduces parameters inference time analysis capsule number capsules iteration time namic routing algorithm affect performance proposed model evaluate gret model ent number capsules different iteration times task results shown figure empirical conclusions experiment rst iterations signicantly improve performance results iterations tend stabilize second increase capsule number gain think reason sentences shorter suitable capsules extract features probing experiment global representation learn interesting question following weng probing experiment train bag words predictor ybow unordered set imizing taining words output sentence structure predictor simple feed forward network maps global state target word embedding matrix compare precision target words words chosen predicted probability figure translation cases transformer gret model task results shown table global state gret higher precision conditions shows proposed method obtain formation output sentence partial answers gret model improve generation quality analysis sentence length effectiveness global representation group test set length input tences evaluate models set divided sets figure shows results model outperforms baseline categories especially longer sentences shows fusing global resentation help generation longer sentences providing complete information case study real cases task ference baseline model cases shown figure source indicates source sentence reference indicates human translation bold font indicates improvements model italic font indicates translation errors output gret decided previous state global representation avoid common translation errors like translation caused strong language model decoder ignores translation information example translation cities hefei case corrected gret model furthermore providing global information avoid current state focuses word word mapping case vanilla transformer translates moscow travel police according source input mosike lvyou jingcha omits words renyuan zhaolu leads fails translate target word recruiting related work work try generate global representation machine translation lin propose volutional method obtain global information guide translation process rnn based model itation cnn model global information methods employ transformer details shown weng text summarization chen propose rate global information rnn based model reduce tition use additional rnn model global resentation time consuming long dependence relationship hinders ness global representation zhang liu song propose sentence state lstm text representation method shows tive way obtaining representation tion transformer previous researches notice importance representations generated encoder focus ing use wang propose use sule network generate hidden states directly inspire use capsules dynamic routing algorithm extract specic suitable features hidden states wang dou propose utilize hidden states multiple layers contain different aspects information model complete representations inspires use states multiple layers enhance global representation conclusion paper address problem transformer model global contextual information crease generation quality propose novel gret model generate external state encoder taining global information fuse decoder namically approach solves issues model use global contextual information compare proposed gret state art model experimental results translation tasks text summarization task demonstrate ness approach future analysis combine methods enhancing local resentations improve generation performance acknowledgements like thank reviewers insightful comments shujian huang corresponding author work supported national key program china national science foundation china jiangsu provincial research foundation basic research referenceinadditiontosuzhou othersecond tiercitiesincludinghefeiandnanjingwillalsointroducepropertymarketregulationsandcontrolpolicies transformerthesecond tiercities includinghefeiandnanjing arenotonlysuzhou butalsothecitiesofhefei gretthesecond tiercities notonlysuzhou butalsohefeiandnanjingwillalsointroducepropertymarketregulationsandcontrolpolicies referencetherecruiting policemodeandequipmentofmoscowtourismpoliceofcershaveinspiredusalot transformerwehavealotofinspirationfromthemoscowtravelpolice thepolicemodel andtheequipment gretwehavealotofinspirationbythemoscowtravelpolicesrecruiting policemode andequipment experimenthere wetrainabag wordspredictorbymax tainingallwordsintheoutputsentence thestructureofthepredictorisasimplefeed forwardnetworkwhichmapstheglobalstatetothetargetwordembeddingmatrix wecomparetheprecisionofthetargetwordsinthetop kwordswhicharechosenthroughthepredictedprob ditions whichshowsthattheproposedmethodcanobtainmoreinformationabouttheoutputsentenceandpartialan swerswhythegretmodelcouldimprovethegenerationquality tencestore evaluatethemodels wendthatourmodeloutperformsthebaselineinallcategories especiallyinthelongersentences whichshowsthatfusingtheglobalrep resentationmayhelpthegenerationoflongersentencesbyprovidingmorecompleteinformation ferencebetweenthebaselineandourmodel thesourceindicatesthesourcesentenceandthereferenceindicateshumantranslation eachoutputfromgretisdecidedbypreviousstateandtheglobalrepresentation itcanavoidsomecommontranslationerrorslikeover undertranslation causedbythestronglanguagemodelofthedecoderignoressometransla tioninformation forexample furthermore providingglobalinformationcanavoidcurrentstateonlyfocusesonword wordmapping inmachinetranslation linetal volutionalmethodtoobtainglobalinformationtoguidethetranslationprocessinrnn basedmodel thelim itationofcnncannotmodeltheglobalinformationwellandtheremethodscannotemployonthetransformer intextsummarization rateglobalinformationinrnn basedmodeltoreducerepe tition theyuseanadditionalrnntomodeltheglobalrep resentation whichistime consumingandcannotgetthelong dependencerelationship whichhinderstheeffective nessoftheglobalrepresentation zhang liu statelstmfortextrepresentation ourmethodshowsanalterna tivewayofobtainingtherepresentation ontheimplementa tionofthetransformer manypreviousresearchesnoticetheimportanceoftherepresentationsgeneratedbytheencoderandfocusonmak ingfulluseofthem wangetal sulenetworktogeneratehiddenstatesdirectly whichinspireustousecapsuleswithdynamicroutingalgorithmtoextractspecicandsuitablefeaturesfromthesehiddenstates wangetal douetal whichinspiresustousethestatesinmultiplelayerstoenhancetheglobalrepresentation weaddresstheproblemthattransformerdoesntmodelglobalcontextualinformationwhichwillde creasegenerationquality weproposeanovelgretmodeltogenerateanexternalstatebytheencodercon tainingglobalinformationandfuseitintothedecoderdy namically ourapproachsolvesthebothissuesofhowtomodelandhowtousetheglobalcontextualinformation wecomparetheproposedgretwiththestate arttrans formermodel experimentalresultsonfourtranslationtasksandonetextsummarizationtaskdemonstratetheeffective nessoftheapproach inthefuture wewilldomoreanalysisandcombineitwiththemethodsaboutenhancinglocalrep resentationstofurtherimprovegenerationperformance references ayana liu sun neural line generation minimum risk training arxiv preprint kiros hinton layer malization arxiv preprint bahdanau cho bengio neural chine translation jointly learning align translate corr chang huang hsu hybrid word character model abstractive summarization corr chen chinese short text summary generation model combining global local information ncce cho van merrienboer gulcehre bahdanau bougares schwenk bengio learning phrase representations rnn encoder decoder tical machine translation emnlp deng cheng song wang yao zhang zhang zhang alibaba neural machine translation systems ence machine translation shared task papers devlin chang lee toutanova bert pre training deep bidirectional transformers guage understanding arxiv dou wang shi zhang exploiting deep representations neural machine tion emnlp frazier sentence processing tutorial review gehring auli grangier dauphin convolutional encoder model neural machine translation arxiv preprint gehring auli grangier yarats dauphin convolutional sequence sequence learning arxiv preprint copying mechanism sequence sequence learning acl bradbury xiong socher non autoregressive neural machine translation iclr hassan aue chen chowdhary clark federmann huang junczys dowmunt lewis achieving human parity tomatic chinese english news translation arxiv preprint chen zhu lcsts large scale chinese short text summarization dataset emnlp kingma adam method stochastic optimization corr krizhevsky sutskever hinton imagenet classication deep convolutional neural works nips bing lam actor critic based ing framework abstractive summarization arxiv preprint incorporating lin sun global encoding abstractive summarization acl lin sun ren deconvolution based global decoding neural machine translation acl lin rouge package automatic tion summaries acl luong pham manning effective approaches attention based neural machine translation emnlp papineni roukos ward zhu bleu method automatic evaluation machine lation acl peters neumann iyyer gardner clark lee zettlemoyer deep contextualized word representations arxiv preprint sabour frosst hinton dynamic routing capsules corr sennrich haddow birch neural chine translation rare words subword units acl shaw uszkoreit vaswani self attention relative position representations naacl song wang zhang huang luo duan zhang alignment enhanced constraining nmt pre specied translations aaai sutskever vinyals sequence sequence learning neural networks nips vaswani shazeer parmar uszkoreit jones gomez kaiser polosukhin tention need nips wang xie tan ear time neural machine translation capsule networks arxiv wang xiao zhu multi layer representation fusion neural machine lation coling weng huang zheng dai chen neural machine translation word predictions emnlp yang wong meng chao zhang modeling localness self attention works emnlp yang wong chao wang context aware self attention networks aaai zhang liu song sentence state lstm text representation acl zhao yang lei zhang zhao investigating capsule networks dynamic routing text classication arxiv preprint zheng huang dai chen dynamic past future neural machine lation emnlp ijcnlp
