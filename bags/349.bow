topic aware abstractive text summarization chujie zheng edu university delaware usa harry jiannan wang edu university delaware usa kunpeng zhang edu university maryland usa ling fan edu cn tongji university china t c o l c s c v v x r abstract automatic text summarization aims condensing document shorter version preserving key information ent extractive summarization simply selects text ments document abstractive summarization generates summary word word manner current state art sota abstractive summarization methods based transformer based encoder decoder architecture focus novel self supervised objectives pre training models capture contextual information words documents little attention paid incorporating global semantics better fine tune downstream abstractive summarization task study propose topic aware abstractive tion taas framework leveraging underlying semantic structure documents represented latent topics cally taas seamlessly incorporates neural topic modeling encoder decoder based sequence generation procedure tention summarization design able learn preserve global semantics documents makes summarization fective proved experiments real world datasets compared cutting edge baseline methods taas outperforms bart recognized sota model f measure rouge l respectively taas achieves rable performance pegasus prophetnet difficult accomplish given training pegasus prophetnet quires enormous computing capacity study keywords abstractive text summarization topic aware attention semantics introduction today s digital economy facing tremendous information day leads information overload poses great challenges efficient information consumption shown figure summarization enables quick condensed overview content applications help users navigate ocean content abundance marization widely studied topic natural language processing nlp area short coherent snippet tomatically generated longer text accurate concise summarization critical downstream tasks figure example text information retrieval recommender systems illustrated automatic summarization algorithms reduce reading time users selection process easier improve effectiveness indexing biased human summaries increase number texts consumer able process researchers developing summarization niques primarily fall categories extractive rization abstractive summarization extractive summarization involves selection phrases sentences source document generate new summary involves ranking relevance phrases order choose relevant meaning source modify words contrary abstractive summarization generates entirely new phrases sentences word word manner capture meaning source text challenging direction consistent humans summarization holds hope general solutions task present study focuses abstractive summarization recently deep learning models shown promising results domains inspired successful application deep ing methods machine translation abstractive text tion specifically framed sequence sequence learning task sequence models especially game changing transformer based encoder decoder framework come help transformer wide range downstream cations reading comprehension question answering espn com natural language inference achieved astonishing mance example recent transformer based ered largest language model far whopping billion parameters produce amazing results tasks zero shots learning similarly current state art sota abstractive summarization methods focus novel self supervised objectives pre training contribution attention mechanism transformer based models capture syntactical contextual information words documents little attention paid incorporating semantic information global level better fine tune specific abstractive summarization task ticular latent topics documents play role text summarization generated summary expected capture key information source text topic modeling ods proposed discover latent semantic structures collection documents topic distributed words documents corresponding ities belonging specific topic intuition study paper leveraging topic association information word document model able assign weights words likely represent key topics documents generate better summarization like prior studies adopt sequence sequence model generate summaries key component model represent encode source text current approaches include sum approach self attention approach sum proach summarizes latent representations input sequence latent representation decoding recent years witnessed prosperity approach sequence modeling approach shortcomings need addressed easy generate fake dependencies overly strong assumption adjacent interactions quence dependent true real world exist noisy information sequence likely capture point wise dependencies tive group dependencies greatly ignored importance individual input sequence likely equal inspires research efforts attention based sequence ing self attention transformer style architecture common developed short term long term dependencies inputs captured refer details focus capturing contextual information syntactical level semantics looked significantly reduce sequence modeling performance especially summarization task motivated intend semantic structure input document consideration particular latent topics document overcome limitation focus local contextual information high level semantics neglected paper bring idea rization task propose new framework named topic aware abstractive summarization taas believe design help find informative words comprehensive tation input sequence leads better summary empirical experiments demonstrate effectiveness compared existing methods overall main contributions paper fold propose new framework abstractive summarization topic information incorporated helps ture semantic information provides guidance generation preserve key information generic framework opens new perspective nlp tended language tasks implement topic aware attention topic level tures neural topic modeling transformer based encoder decoder efficiently extracts salient topics understands long term dependency informativeness words input sequence conduct experiments real world dataset pare performance model state art approaches demonstrate effectiveness model summarization task discuss impact important hyperparameters model different types data performance related work lines research closely related paper attention mechanism text summarization topic modeling attention mechanism brings nlp new stage inception present models based attention mechanism achieved breakthrough performance tasks nlp idea attention mechanism inspired human visual attention allows focus certain region high resolution attention interpreted vector importance weight predict infer element pixel image word sentence usually use attention vector estimate strongly correlated elements element highly relevant target assigned higher weight irrelevant ones associated lower weights language generation task example attention mechanism provides guidance words contribute sequence generation commonly seen nlp applications text summarization widely studied topic nlp aims provide high level view input document short cise summary types summarization approaches extractive abstractive summarization early extractive methods formulate problem selecting subset sentences capture main idea input document handcrafted features graph based structural information advancement models encoder decoder network shown promising ability generate abstractive summary framework encoder obtains comprehensive sentation input sequence decoder generates output summary based latent representation recurrent ral network rnn transformer commonly adopted encoder decoder network particular transformer attention mechanism state art standard academia industry variants achieved ing results summarization task example bart model effective text generation tasks symbol representation input sequence h output hidden state x y output sequence s latent representation r topic word distribution p r topic embedding topic attention topic attention token topic table notations implements bidirectional encoder left right gressive decoder pegasus prophetnet introduce different pre training objectives text summarization pegasus masks important sentences generates gap sentences rest document additional pre training objective prophetnet predicts tokens simultaneously time step encourages model plan future tokens common drawback transformer based summarization models higher level global semantic structure text usually ignored latent topics motivates study designs topic aware attention summarization topic model important component taas covers semantically relevant terms form coherent topics probabilistic generative models unsupervised manner basic assumption topic models ment mixture topics topic distributed words vocabulary corpus learn distributions latent dirichlet allocation lda introduced imposing latent ables dirichlet prior recently development deep generative networks stochastic variational inference enables neural network based topic modeling proven effective auto encoding variational bayes provides generic framework deep generative topic modeling especially variational auto encoder vae consists generative network inference network framework severs important foundation studies field ample neural variational document model nvdm applies vae unsupervised document modeling bag words ment representation gaussian softmax model gsm extends nvdm constructing topic distribution softmax tion applied projection gaussian random vectors recent years topic modeling extended nlp tasks including text summarization different develop topic attention neural topic modeling text summarization preliminaries section formally define summarization task key notations concepts explained briefly review sequence sequence transformer based architecture taas built notations paper listed table problem definition automatic text summarization aims condensing document shorter version preserving key information let input document tokens word embedding token given taas model learns function maps sequence tokens y summary tokens automatic generation process achieved ing probability beam search algorithm usually implemented neural networks transformer parameterized transformer architecture key idea sequence sequence model represent input sequence low dimensional vector preserving contextual information sequence possible new task specific sequence arbitrary length automatically generated practice usually consists encoder decoder encoder encodes key information input sequence generates contextualized tation s input decoder taking single encoder layer example given input sequence obtain h hidden state encoder layer input given learned representation input token typical approach obtain sequence level representation s sum word level representations shown eq note strategy ignores complicated relationships token level latent representations h s different approaches obtain sequence level representation proposed example adds special token cls located beginning sequence final hidden state responding token encoder aggregate sequence representation adopt approach paper transform h leverage sequence sequence transformer network given input sequence transformer calculates multi head self attention mapping variable length sequence symbol representation sequence equal length h r hidden size single head attention ample transformer multiplies w w w input sequence find query matrix q key matrix k value matrix v like eq q w k w v w attention calculated according eq k v softmax qk v latent representation input sequence s written follows output hidden state token cls written eq s topic aware abstractive summarization taas section describe details proposed model taas considers syntactical semantic structures text abstractive summarization illustrated fig taas consists major components neural topic modeling deep learning based topic model variational autoencoder vae implemented learn latent topic vectors t document topic distribution topic word distribution networks e encoder decoder generative manner b topic aware attention incorporate latent structure documents semantic level subsequent based summarization model introduce topic aware attention derstand impact words summarization believe design help model capture global information leveraging topic features learned neural topic eling c encoder decoder based sequence modeling based encoder decoder framework employed understand plicated syntactical features text hidden state generated encoder topic attention calculate latent representation contextual global information captured representation input decoder output summary generation neural topic model topic weighted attention built extracting latent topic information neural topic model ntm ntm based variational auto encoder vae involved tinuous latent variable z latent topics given input sequence document d contains tokens latent topic variable z r corresponds topic proportion document d denotes number topics topic assignment observed word ntm implements vae learn latent topic vectors networks generative network inference network generative network encoder compressor transforms input text data latent resentation e latent topic vector inference network decoder reverter reconstructs latent representation original input design neural networks parameterize multinomial topic distribution eliminate need predefine distributions guide generative process requires specifying simple prior e diagonal gaussian overall generative process document d n z pass diagonal gaussian distribution mean variance parameterize multinomial document topic distribution build unbiased gradient estimator variable distribution w b trainable parameters parameters involved generative process denoted inference network approximate true tion diagonal gaussian d d parametrized use fully connected networks represent d d d d d d overall use variational inference approximate posterior distribution z loss function defined negative variational lower bound elbo eq z probabilities encoding decoding processes standard normal prior z e z topic aware attention incorporate document level semantics embedded latent topics encoder decoder model taas introduces topic aware attention mechanism components connected enrich representation input sequence better summarization mentioned ferent prior studies develop sum self attention approach representing input document past research emphasize contextual information syntactical level semantics neglected deteriorate sequence modeling performance especially summarization motivates design topic aware attention based approach figure specifically designed follows ntm obtain topic word distribution r denote number topics vocabulary size respectively given topic embedding hidden state input sequence calculate attention weight attention weight token topic output hidden layer encoder network h r h r simple attention design drawbacks trained model able generalize unseen documents words appear training vocabulary dimensionality h mismatches usually larger hidden size overcome limitations add transformation component e mapping realized fully connected feed forward network residual connection followed layer normalization p figure overview proposed topic aware abstractive summarization model taas decoder decoder outputs summary sequential manner specifically input sequence sent layer encoder hidden representation generated rest layers encoder output previous layer serves input current layer final state encoder topic attention generate latent h representation serves initial hidden state decoder previous works recurrent neural network rnn attention transformer widely architectures encoder decoder network major weakness based approach lies contextualized representation s short term impact generated sequence beginning generation process address challenge attention mechanism introduced entire encoder context account illustrated eq s available decoding conditioning current decoder state stand self attention calculation word embedding output sampled softmax previous step h h h z h y s motivated great success transformer based model like bert recent years employ sequence sequence transformer architecture abstractive summarization task given topic attention hidden state h e h superscript omitted latent representation s calculated s h figure architecture topic attention p r saves confined pre defined lary hidden size encoder p considered topic embedding carries latent features information topic attention weight rewritten h token average attention weight topics topic aware attention weight normalized softmax obtain final attention denote final topic attention encoder decoder sequence modeling final sequence sequence model uses standard decoder framework encoder generates contextualized latent representation s input sequence input parameter estimation taas consists objectives ntm encoder decoder sequence modeling need jointly optimized objective function taas defined eq negative elbo defined eq hidden states encoder following extractive summarization methods cross entropy loss predicted output decoder true summary hyper parameter balances importance ntm encoder decoder overall process taas sketched algorithm algorithm taas topic aware abstractive summarization input input sequence d output summary y training phase topic word distribution batch p h hp calculate eq eq obtain latent representation s h update parameters taas learning rate objective function eq sequence generation summary topic attention end end test phase y repeat steps learned parameters experiments section describe dataset evaluation metrics parameter settings conduct experiments compare taas state art text summarization models parameter sensitivity model ablation analysis discussed codes publicly available experimental settings dataset data use study cnn daily mail cnn dm dataset contains news articles cnn articles daily mail text dataset tokens average paired multi sentence summaries sentences tokens average serve ground truth summarization task training validation test sets include data pairs respectively evaluation metrics following existing works use rouge summarization performance evaluation rouge measures overlapping generated summary ground truth summary rouge n rouge l commonly rouge metrics practice stand rouge n gram longest common subsequence lcs recall context rouge means generated summary covers ground truth summary precision measures generated com taas taas summary fact included ground truth summary study report score rouge l experiment performance comparison parameter setting experiments conducted machine geforce rtx ti gpus memory limitation batch size training testing set following suggestion freeze parameters encoder token embedding fine tuning decoder benchmark methods evaluate topic aware based design effective text summarization compare taas model following methods incorporate semantic structure documents grouped rule based extractive abstractive categories simple rule based method chooses sentences document summary summarunner layer bi directional rnn based sequence model extractive summarization formulates summarization problem sequence sification problem sentence binary classifier learned decide included refersh formulates extractive summarization problem ranking task sentences input document uses lstm select sentences input documents following abstractive summarization methods drm introduces neural network model novel intra attention attends input continuously generates output separately model reads input quence bi directional lstm encoder single lstm decoder generate summary pointer generator network pgn constructs generator network summarization copies words source text aid accurate reproduction tion retains ability produce novel words generator novel framework viewed balance extractive abstractive approach unified framework converts language problem text text format pretrained large language corpus framework adjusted different language tasks including summarization bart employs bidirectional encoder enhance sequence understanding left right decoder generate summary prophetnet predicts tokens simultaneously based previous context tokens time step sign encourages model plan future generation process pegasus introduces new pre train objective mask tokens mask important sentences enables model capture global information implementation com huggingface transformers master examples rule based extractive abstractive model refresh summarunner drm pgn bart method taas industry sota prophetnet pegasus rouge l table performance comparison cnn dm dataset score rouge sentences able generate candidate sentences surrounding sentence context main goal study leverage topic level semantics help summarization task training built sshleifer distilbart checkpoint released huggingface bart model taas uses similar ture bart layer encoder bi directional transformer layer layer decoder uni directional transformer layer add topic aware attention layer encoder decoder incorporate semantic structure experiment set eq focus loss sequence modeling parameters ntm fixed note tuned balance loss ntm sequence modeling leave future research limitation computing capacity calculate topic word distribution samples batch use adam optimizer learning rate note hyperparameters adam optimizer related topic word distribution use dropout rate layers experiment results performance comparison table shows performance parison taas model aforementioned benchmark models cnn dm dataset separate prophetnet gasus industry sota category given positions cnn dm leaderboard note reproduce performance hyperparameter settings original papers given limited computing capability report scores hyperparameter settings taas model taas performance denoted bold text improvement percentage second best performing models denoted underlined text industry sota els following observations taas outperforms naive rule based method expected looking sentences article able capture key information different reporters different ing styles prefer summarize important topics co sshleifer distilbart figure qualitative evaluation taas bart tention summary beginning like use inductive approach leave important information end case summary inaccurate taas superior performance recent state art academic models tractive abstractive model uses similar architecture bart particularly comparison bart find improves performance regrading measure rouge l respectively confirms incorporating global semantic structures latent topics text generates better summarization improvements models simple rule based model terms f scores amazingly large fact writing style news articles pattern sentences kind summarize article experiments conducted rization datasets study effects model types documents truth bob barker returned host price right wednesday barker retired host generated summary bart prophetnet price right returned hosting time years despite away past years television legend nt miss beat bob barker hosted game years stepping bob barker hosted tv game years stepping barker handled price guessing game turning hosting duties drew carey despite away past years barker nt miss beat tv legend returned best contestants told come april edition price right encountered host drew xarey pegasus barker hosted price right years stepped taas bob barker returns hosting price right time years year old tv legend stepped years hosted years table example summaries generated models qualitative evaluation conduct qualitative evaluation intuitively demonstrate word attention weights deed changed topic aware attention mechanism taas model generate novel sentences lated document topics shown figure highlight yellow color phases highest attention values topic aware attention taas self attention bart illustrate word level attention differences provide generated summary comparison important phases self attention focus leading articles fails capture important information text generated result meaningful compared generated taas summaries approaches successfully covered keywords news taas generates clear coherent results especially summarizes key ideas correctly models target abstractive summarization task assess extent models able perform rewriting generating abstractive summary result bart informative uses original sentences news e sentence highlighted purple figure taas captures key information article demonstrates novel sentence structure observe duplicated information summary generated taas example years conveys tion hosted years indicates additional room improvements future research additional comparison present summaries article baseline models shown table discussion effect different number topics taas achieves relatively superior performance lines demonstrating effectiveness incorporating aware attention summarization like deep learning els taas involves hyperparameters mance sensitive number topics critical needs exploration vary value ranging obtain f scores rouge l follow prior literature report f score rouge l figure impact number topics model formance note reported score calculated test set result shown figure ing observations taas achieves best performance reasonable given fact batch training articles number latent topics rouge l f scores different s nt vary reasons balancing ntm encoder decoder sequence modeling set focus summarization generated summary includes attended words likely similar varying given nature dataset unlikely exhibit diverse topics articles batch effect different document lengths test taas performs equally different lengths articles separate entire cnn dm dataset subsets based number sentences denoted cnn short sentences cnn dm medium sentences cnn dm long sentences articles figure performance comparison score rouge taas bart different lengths articles respectively shown figure taas achieves better mance longer articles improvement rouge l f score bart compared short medium articles longer articles likely diverse topics topic attention summarization model salient indicates adding topic level mation improve model summarization task conclusion work study abstractive text summarization problem proposing topic aware attention model incorporate global semantic structures text particular combine neural topic modeling encoder decoder like sequence sequence model topic attention summarization conduct extensive iments real world dataset compare proposed approach cutting edge methods results demonstrate perior performance recognized models academia comparable performance industry sota shed light model performance affected important parameters characteristics textual data current study paper incorporates topic word distribution framework important output ntm e document topic distribution currently neglected worth exploring future work furthermore training model powerful computing resources improve summarization performance test robustness model interesting pursue references jimmy lei ba jamie ryan kiros geoffrey e hinton layer tion arxiv preprint david m blei alp kucukelbir jon d mcauliffe variational inference review statisticians journal american statistical association chaitanya chemudugunta padhraic smyth mark steyvers combining concept hierarchies statistical topic models proceedings acm conference information knowledge management jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language understanding arxiv preprint thomas l griffiths michael jordan joshua b tenenbaum david m blei hierarchical topic models nested chinese restaurant process advances neural information processing systems xiaotao gu yuning mao jiawei han jialu liu wu cong yu daniel finnie hongkun yu jiaqi zhai nicholas zukoski generating representative headlines news stories proceedings web conference kaiming xiangyu zhang shaoqing ren jian sun deep residual learning image recognition proceedings ieee conference computer vision pattern recognition dan jurafsky james h martin speech language processing vol diederik p kingma max welling auto encoding variational bayes arxiv preprint mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy ves stoyanov luke zettlemoyer bart denoising sequence sequence pre training natural language generation translation comprehension arxiv preprint wei li andrew mccallum pachinko allocation dag structured mixture models topic correlations proceedings international conference machine learning chin yew lin rouge package automatic evaluation summaries text summarization branches yang liu mirella lapata text summarization pretrained encoders proceedings conference empirical methods natural language processing international joint conference natural language cessing emnlp ijcnlp yuning mao liyuan liu qi zhu xiang ren jiawei han facet aware evaluation extractive text summarization arxiv yu meng jiaxin huang guangyuan wang zihan wang chao zhang yu zhang jiawei han discriminative topic mining category guided text embedding proceedings web conference yishu miao edward grefenstette phil blunsom discovering discrete latent topics neural variational inference arxiv preprint yishu miao lei yu phil blunsom neural variational inference text processing international conference machine learning ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents arxiv preprint shashi narayan shay b cohen mirella lapata ranking sentences extractive summarization reinforcement learning proceedings conference north american chapter association computational linguistics human language technologies volume long papers romain paulus caiming xiong richard socher deep reinforced model abstractive summarization arxiv preprint alec radford jeffrey wu rewon child david luan dario amodei ilya sutskever language models unsupervised multitask learners openai blog colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li peter j liu exploring limits transfer learning unified text text transformer journal machine learning research abigail peter j liu christopher d manning point summarization pointer generator networks arxiv preprint akash srivastava charles sutton autoencoding variational inference topic models arxiv preprint ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention need advances neural information processing systems yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang ming zhou prophetnet predicting future n gram sequence sequence pre training arxiv preprint liang yang fan wu junhua gu chuan wang xiaochun cao di jin fang guo graph attention topic modeling network proceedings web conference yi yang kunpeng zhang sdtm supervised bayesian deep topic model text analytics available ssrn jingqing zhang yao zhao mohammad saleh peter j liu pegasus pre training extracted gap sentences abstractive summarization arxiv preprint qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural document summarization jointly learning score select sentences proceedings annual meeting association computational linguistics volume long papers
