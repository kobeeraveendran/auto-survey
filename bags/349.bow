topic aware abstractive text summarization chujie zheng edu university of delaware usa harry jiannan wang edu university of delaware usa kunpeng zhang edu university of maryland usa ling fan edu cn tongji university china t c o l c s c v v i x r a abstract automatic text summarization aims at condensing a document to a shorter version while preserving the key information ent from extractive summarization which simply selects text ments from the document abstractive summarization generates the summary in a word by word manner most current state the art sota abstractive summarization methods are based on the transformer based encoder decoder architecture and focus on novel self supervised objectives in pre training while these models well capture the contextual information among words in documents little attention has been paid to incorporating global semantics to better fine tune for the downstream abstractive summarization task in this study we propose a topic aware abstractive tion taas framework by leveraging the underlying semantic structure of documents represented by their latent topics cally taas seamlessly incorporates a neural topic modeling into an encoder decoder based sequence generation procedure via tention for summarization this design is able to learn and preserve global semantics of documents and thus makes summarization fective which has been proved by our experiments on real world datasets as compared to several cutting edge baseline methods we show that taas outperforms bart a well recognized sota model by and regarding the f measure of and rouge l respectively taas also achieves rable performance to pegasus and prophetnet which is difficult to accomplish given that training pegasus and prophetnet quires enormous computing capacity beyond what we used in this study keywords abstractive text summarization topic aware attention semantics introduction in today s digital economy we are facing a tremendous amount of information every day which often leads to information overload and poses great challenges to efficient information consumption as shown in figure summarization enables a quick and condensed overview of the content and has been used in various applications to help users navigate in the ocean of content abundance marization has been a widely studied topic in natural language processing nlp area where a short and coherent snippet is tomatically generated from a longer text an accurate and concise summarization is very critical to many downstream tasks such as figure an example of text information retrieval and recommender systems as illustrated by automatic summarization by algorithms can reduce reading time make users selection process easier improve the effectiveness of indexing be less biased than human summaries and increase the number of texts consumer are able to process researchers have been developing various summarization niques that primarily fall into two categories extractive rization and abstractive summarization extractive summarization involves the selection of phrases and sentences from the source document to generate the new summary it involves ranking the relevance of phrases in order to choose only those most relevant to the meaning of the source it does not modify any words on the contrary abstractive summarization generates entirely new phrases and sentences in a word by word manner to capture the meaning of the source text this is a more challenging direction but consistent with what humans do in summarization which holds the hope of more general solutions to this task thus the present study focuses on abstractive summarization recently deep learning models have shown promising results in many domains inspired by the successful application of deep ing methods for machine translation abstractive text tion is specifically framed as a sequence to sequence learning task therefore various sequence models especially the game changing transformer based encoder decoder framework can come to help transformer has been used in a wide range of downstream cations such as reading comprehension question answering and espn com natural language inference and has achieved astonishing mance for example the recent transformer based is ered the largest language model so far with a whopping billion parameters and can produce amazing results in various tasks with zero or few shots learning similarly most current state of the art sota abstractive summarization methods also focus on novel self supervised objectives in pre training within the contribution from attention mechanism transformer based models can well capture the syntactical and contextual information among words in documents however little attention has been paid to incorporating semantic information at a global level to better fine tune for the specific abstractive summarization task in ticular the latent topics in documents should play a role in text summarization since the generated summary is expected to capture the key information of the source text many topic modeling ods have been proposed to discover the latent semantic structures of a collection of documents each topic is distributed over words from the documents with their corresponding ities of belonging to a specific topic the intuition behind our study in this paper is that by leveraging the topic association information of each word in the document our model is able to assign more weights to words that are more likely to represent the key topics of the documents and thus generate better summarization like prior studies we also adopt a sequence to sequence model to generate summaries a key component in a model is how to represent and encode a source text current approaches include a sum up approach and a self attention approach the sum up proach summarizes all latent representations of an input sequence into one latent representation for decoding recent years have witnessed the prosperity of this approach in sequence modeling however this approach has three shortcomings that need to be addressed it is easy to generate fake dependencies due to the overly strong assumption that any adjacent interactions in a quence must be dependent which may not be true in the real world because there might exist some noisy information in a sequence it is likely to capture point wise dependencies only while the tive group dependencies are greatly ignored the importance of each individual input in a sequence is likely not to be equal which inspires many research efforts on attention based sequence ing among which self attention in transformer style architecture is the most common and well developed one the short term and long term dependencies among inputs are well captured please refer to for details they seem to focus on capturing contextual information at a syntactical level while the semantics are looked which might significantly reduce the sequence modeling performance especially for the summarization task motivated by this we intend to take the semantic structure of an input document into consideration in particular the latent topics of a document this can overcome the limitation that we only focus on local contextual information while all high level semantics are neglected therefore in our paper we bring this idea to the rization task and propose a new framework named topic aware abstractive summarization taas we believe that this design can help us find informative words for a more comprehensive tation of the input sequence which leads to a better summary our empirical experiments also demonstrate its effectiveness as compared to existing methods overall the main contributions of this paper are three fold we propose a new framework for abstractive summarization with topic information incorporated which helps to ture the semantic information and provides guidance during generation to preserve the key information this generic framework opens a new perspective in nlp and can be tended to other language tasks we implement topic aware attention using topic level tures through neural topic modeling and transformer based encoder decoder which efficiently extracts salient topics and understands the long term dependency and informativeness of words in the input sequence we conduct experiments on a real world dataset and pare the performance of our model with several state of art approaches to demonstrate the effectiveness of our model on the summarization task we also discuss the impact of important hyperparameters in the model and different types of data on the performance related work three lines of research are closely related to our paper attention mechanism text summarization and topic modeling attention mechanism brings nlp to a new stage since its inception in at present various models based on the attention mechanism have achieved breakthrough performance in many tasks of nlp the idea of attention mechanism is inspired by the human visual attention which allows us to focus on a certain region with high resolution the attention can be interpreted as a vector of importance weight to predict or infer one element such as a pixel in an image or a word in a sentence we usually use an attention vector to estimate how strongly it is correlated with other elements the element highly relevant to the target should be assigned with a higher weight while the irrelevant ones should be associated with lower weights using the language generation task as an example the attention mechanism provides guidance on which how words contribute to the sequence generation which has been commonly seen in many nlp applications text summarization is a widely studied topic in nlp it aims to provide a high level view of the input document to a short and cise summary there are two types of summarization approaches extractive and abstractive summarization early extractive methods formulate the problem as selecting a subset of sentences to capture the main idea of the input document using handcrafted features and graph based structural information with the advancement of models the encoder decoder network has been shown promising ability to generate the abstractive summary in this framework the encoder obtains a comprehensive sentation for the input sequence and the decoder generates the output summary based on the latent representation recurrent ral network rnn and transformer are commonly adopted in the encoder decoder network in particular the transformer along with the attention mechanism has become a state of the art standard in both academia and industry several variants have achieved ing results in the summarization task for example bart is such a model and very effective for text generation tasks where it symbol representation input sequence h output from hidden state for x y output sequence s latent representation for r topic word distribution p r topic embedding topic attention for topic attention of token under topic table notations implements the bidirectional encoder and the left to right gressive decoder pegasus and prophetnet introduce different pre training objectives for text summarization pegasus masks important sentences and generates those gap sentences from the rest of the document as an additional pre training objective prophetnet predicts the next tokens simultaneously at each time step which encourages the model to plan for the future tokens one common drawback in these transformer based summarization models is that higher level global semantic structure in the text is usually ignored such as latent topics which motivates our study that designs topic aware attention for summarization topic model is an important component in the taas it covers semantically relevant terms that form coherent topics via probabilistic generative models in an unsupervised manner one basic assumption among various topic models is that a ment is a mixture of topics and each topic is distributed over words in the vocabulary of the corpus to learn these distributions latent dirichlet allocation lda is introduced by imposing latent ables with dirichlet prior recently the development of deep generative networks and stochastic variational inference enables neural network based topic modeling that is proven to be effective among which auto encoding variational bayes provides a generic framework for deep generative topic modeling especially the variational auto encoder vae that consists of a generative network and an inference network this framework severs as an important foundation for many studies in this field for ample the neural variational document model nvdm applies vae for unsupervised document modeling with bag of words ment representation gaussian softmax model gsm extends nvdm by constructing the topic distribution with a softmax tion which is applied to the projection of the gaussian random vectors in recent years topic modeling has also been extended to other nlp tasks including text summarization they are different from ours in that we develop a topic attention via neural topic modeling and for text summarization preliminaries in this section we first formally define the summarization task with key notations and concepts explained we then briefly review sequence to sequence transformer based architecture which our taas is built upon notations throughout the paper are listed in table problem definition automatic text summarization aims at condensing a document to a shorter version while preserving the key information let be an input document with tokens and is the word embedding for the token given our taas model learns a function that maps to another sequence of tokens where y is the summary with tokens this automatic generation process is achieved by ing the probability via the beam search algorithm is usually implemented by neural networks or transformer parameterized by transformer architecture the key idea behind the sequence to sequence model is to represent an input sequence as a low dimensional vector while preserving the contextual information in the sequence as much as possible upon which a new task specific sequence with an arbitrary length can be automatically generated in practice it usually consists of an encoder and a decoder where the encoder encodes key information from the input sequence and generates a contextualized tation s which is the input to the decoder taking a single encoder layer as an example given an input sequence we can obtain h where each is the hidden state from that encoder layer for the input given is a learned representation for input token one of the typical approach to obtain a sequence level representation s is to sum up all word level representations as shown in eq note that this strategy ignores the complicated relationships among all token level latent representations h s different approaches to obtain a sequence level representation have been proposed for example adds a special token cls located at the beginning of a sequence the final hidden state responding to this token by the encoder as is used as the aggregate sequence representation we adopt this approach in this paper to transform to h we leverage the sequence to sequence transformer network given an input sequence transformer calculates multi head self attention for mapping one variable length sequence of symbol representation to another sequence of equal length h with r where is the hidden size using single head attention as an ample transformer first multiplies w w and w to the input sequence to find the query matrix q the key matrix k and the value matrix v like eq q w k w v w then the attention is calculated according to eq k v softmax qk v thus the latent representation of an input sequence s can be written as follows which is also the output from the hidden state for the first token cls be written as eq s topic aware abstractive summarization taas in this section we describe the details of our proposed model taas which considers both syntactical and semantic structures of text for abstractive summarization as illustrated in fig taas consists of three major components a neural topic modeling it is a deep learning based topic model where the variational autoencoder vae is implemented to learn latent topic vectors t document topic distribution and topic word distribution via two networks i e encoder and decoder in a generative manner b topic aware attention to incorporate the latent structure of documents at a semantic level into a subsequent based summarization model we introduce topic aware attention to derstand the impact of words on the summarization we believe that such a design can help our model capture global information by leveraging the topic features learned from the neural topic eling c encoder decoder based sequence modeling the based encoder decoder framework is employed to understand plicated syntactical features in the text the hidden state generated by the encoder along with the topic attention is used to calculate a latent representation where the contextual and global information is captured this representation is the input to the decoder for the output summary generation neural topic model our topic weighted attention is built on extracting the latent topic information through neural topic model ntm ntm is based on variational auto encoder vae involved with a tinuous latent variable z as latent topics given an input sequence document d contains tokens the latent topic variable z r corresponds to the topic proportion of document d here denotes the number of topics is the topic assignment for the observed word ntm implements a vae to learn latent topic vectors via two networks a generative network and an inference network the generative network encoder is a compressor that transforms the input text data into a latent resentation i e a latent topic vector while the inference network decoder is a reverter that reconstructs the latent representation back to the original input such a design of using neural networks to parameterize the multinomial topic distribution can eliminate the need to predefine distributions to guide the generative process it only requires specifying a simple prior e a diagonal gaussian therefore the overall generative process for document d can n z here we pass a diagonal gaussian distribution with mean and variance to parameterize the multinomial document topic distribution and build an unbiased gradient estimator for the variable distribution w and b are trainable parameters all parameters involved in this generative process are denoted by the inference network is to approximate the true tion using a diagonal gaussian d d that is parametrized by we use three fully connected networks and to represent d and d as d d and d d overall we use variational inference to approximate the posterior distribution over z the loss function is defined as the negative of variational lower bound elbo in eq z and are probabilities for encoding and decoding processes is a standard normal prior i z e z topic aware attention to incorporate the document level semantics embedded in latent topics into the encoder decoder model taas introduces a topic aware attention mechanism from which two components are connected to enrich the representation of the input sequence for better summarization as we mentioned above it is very ferent from prior studies where they either develop a sum up or a self attention approach to representing the input document these past research emphasize contextual information at a syntactical level while the semantics are neglected which can deteriorate the sequence modeling performance especially for the summarization this motivates us to design a topic aware attention based approach see figure specifically it is designed as follows from ntm we obtain a topic word distribution r where and denote the number of topics and the vocabulary size respectively given a topic embedding and the hidden state of an input sequence we calculate the attention weight a as a where is the attention weight for the token under the topic is the output of the last hidden layer of the encoder network h r and h r however this simple attention design has two drawbacks the trained model is not able to generalize to unseen documents that have some words that never appear in the training vocabulary the dimensionality of and h mismatches because the is usually much larger than the hidden size to overcome these limitations we add a transformation component i e mapping to realized by a fully connected feed forward network with a residual connection followed by a layer normalization p figure overview of our proposed topic aware abstractive summarization model taas to the decoder then the decoder outputs a summary in a sequential manner specifically the input sequence is sent to the first layer of the encoder where a hidden representation is generated for the rest of the layers in the encoder the output from the previous layer serves as the input of the current layer the final state of the encoder along with the topic attention is used to generate the latent h representation which serves as the initial hidden state for the decoder in previous works recurrent neural network rnn and attention in transformer are two most widely used architectures for the encoder decoder network one major weakness of this based approach lies in that the contextualized representation s has a short term impact on the generated sequence because it is only used at the beginning of the generation process to address this challenge the attention mechanism is introduced to take the entire encoder context into account as illustrated in eq s is available during decoding by conditioning the current decoder state on it is a stand in for self attention calculation and is the word embedding for the output sampled from the softmax at the previous step h h h z h y s motivated by the great success of the transformer based model like bert in recent years we employ this sequence to sequence transformer architecture for our abstractive summarization task given the topic attention and the hidden state h i e h with the superscript omitted the latent representation s is calculated as s h figure architecture of topic attention p r saves us from being confined to pre defined lary where is the hidden size of the encoder p is considered as the topic embedding which carries latent features and information for the topic now the attention weight a can be rewritten as a h further for every token we average the attention weight over topics to get a topic aware attention as this weight is then normalized via softmax to obtain our final attention as we denote this final topic attention as encoder decoder sequence modeling our final sequence to sequence model uses a standard decoder framework where the encoder generates a contextualized latent representation s of the input sequence which is the input parameter estimation taas consists of two objectives from ntm and encoder decoder sequence modeling that need to be jointly optimized the objective function of taas is defined as eq is the negative of elbo defined in eq and is the hidden states from encoder the following are extractive summarization methods cross entropy loss between the predicted output of decoder and the true summary is a hyper parameter that balances the importance between ntm and the encoder decoder the overall process of taas is sketched in algorithm algorithm taas topic aware abstractive summarization input input sequence d output summary y training phase for all do for all do topic word distribution each batch p h a hp calculate using eq and eq obtain latent representation s h update parameters of taas learning rate objective function see eq sequence generation summary topic attention end for end for test phase y repeat steps for and the learned parameters experiments in this section we first describe the dataset evaluation metrics and parameter settings then we conduct several experiments to compare taas against the state of the art text summarization models parameter sensitivity and model ablation analysis are also discussed codes are publicly available experimental settings dataset the data we use in this study is the cnn daily mail cnn dm dataset which contains news articles from cnn and articles from daily mail the text in the dataset has tokens on average paired with multi sentence summaries sentences or tokens on average which serve as the ground truth for our summarization task the training validation and test sets include and data pairs respectively evaluation metrics following existing works we use rouge for summarization performance evaluation rouge measures the overlapping between the generated summary and the ground truth summary rouge n and rouge l are the most commonly used rouge metrics in practice which stand for rouge n gram and longest common subsequence lcs recall in the context of rouge means how much the generated summary covers the ground truth summary whereas precision measures how much of the generated com taas taas summary was in fact included in the ground truth summary in this study we report the score of and rouge l for every experiment for performance comparison parameter setting our experiments are conducted on a machine with two geforce rtx ti gpus due to the memory limitation the batch size for training and testing is set to following the suggestion by we freeze parameters in the encoder and the token embedding while only fine tuning the decoder benchmark methods to evaluate whether the topic aware based design is effective in text summarization we compare our taas model with the following methods which do not incorporate the semantic structure of the documents and can be grouped into rule based extractive and abstractive categories is a simple rule based method that chooses the first three sentences from a document as its summary summarunner is a two layer bi directional rnn based sequence model for extractive summarization it formulates the summarization problem as a sequence sification problem for each sentence a binary classifier is learned to decide if it is included refersh formulates the extractive summarization problem as a ranking task among all sentences of an input document it uses lstm to select sentences from the input documents the following are abstractive summarization methods drm introduces a neural network model with a novel intra attention that attends over the input and continuously generates output separately the model reads the input quence with a bi directional lstm encoder and a single lstm decoder to generate the summary pointer generator network pgn constructs a generator network for summarization which copies words from the source text to aid accurate reproduction of tion and retains the ability to produce novel words through the generator this novel framework can be viewed as a balance between extractive and abstractive approach is a unified framework that converts every language problem into a text to text format it is pretrained with a large language corpus and the framework can be adjusted for different language tasks including summarization bart employs the bidirectional encoder to enhance the sequence understanding and the left to right decoder to generate the summary prophetnet predicts the next tokens simultaneously based on previous context tokens at each time step this sign encourages the model to plan for the future generation process pegasus introduces a new pre train objective to not only mask tokens but also mask some important sentences which enables the model to capture global information among their implementation at com huggingface transformers master examples rule based extractive abstractive model refresh summarunner drm pgn bart our method taas industry sota prophetnet pegasus rouge l table performance comparison on cnn dm dataset regarding score of rouge sentences and thus be able to generate candidate sentences using the surrounding sentence context the main goal of this study is to leverage topic level semantics to help with the summarization task thus our training is built on top of the sshleifer distilbart checkpoint released by huggingface for the bart model taas uses a similar ture to bart a layer encoder with a bi directional transformer layer and a layer decoder with a uni directional transformer layer we add one topic aware attention layer between encoder and decoder to incorporate the semantic structure during our experiment we set in eq to to only focus on the loss from the sequence modeling part while the parameters in ntm are fixed note that could be tuned to balance the loss from both ntm and sequence modeling which we leave as future research due to the limitation of our computing capacity we calculate the topic word distribution using samples within each batch we use adam optimizer with learning rate of note that the here are the hyperparameters of adam optimizer which are not related to the topic word distribution we use a dropout rate of across all layers experiment results performance comparison table shows the performance parison between our taas model and aforementioned benchmark models on the cnn dm dataset we separate prophetnet and gasus into the industry sota category given their top positions on the cnn dm leaderboard note that we can not reproduce their performance using the hyperparameter settings from the original papers given our limited computing capability and thus report their scores using the same hyperparameter settings of our taas model we show taas performance denoted using bold text and the improvement percentage over the second best performing models denoted as underlined text except for the industry sota els we have the following observations taas outperforms the naive rule based method this is expected where only looking at the first sentences of an article is not able to capture all key information because different reporters have different ing styles some prefer to summarize all important topics at the co sshleifer distilbart figure a qualitative evaluation of taas and bart on tention and summary beginning while others like to use an inductive approach to leave the important information at the end using for the latter case will make the summary inaccurate taas has a superior performance over recent state of the art academic models both tractive and abstractive since our model uses a similar architecture to bart we particularly make a comparison against bart and find that it improves the performance by and regrading the measure of and rouge l respectively this confirms that incorporating global semantic structures such as latent topics of text indeed generates better summarization the improvements of all models over the simple rule based model in terms of the f scores are not amazingly large this may due to the fact that the writing style of news articles could have a pattern that the first few sentences kind of summarize the whole article more experiments should be conducted on other rization datasets to study the effects of our model on other types of documents truth bob barker returned to host the price is right on wednesday barker had retired as host in generated summary bart prophetnet the price is right returned to hosting for the first time in eight years despite being away from the show for most of the past years a television legend did nt seem to miss a beat bob barker hosted the game show for years before stepping down in bob barker hosted the tv game show for years before stepping down in barker handled the first price guessing game of the show before turning hosting duties over to drew carey despite being away from the show for most of the past eight years barker did nt seem to miss a beat a tv legend returned to doing what he does best contestants told to come on down on the april edition of the price is right encountered not host drew xarey pegasus barker hosted the price is right for years he stepped down in taas bob barker returns to hosting the price is right for the first time in eight years the year old tv legend stepped down from the show in after years on the show which he hosted for years table example summaries generated by various models qualitative evaluation we conduct a qualitative evaluation to intuitively further demonstrate word attention weights are deed changed by our topic aware attention mechanism and our taas model can generate more novel sentences that are more lated to the document topics as shown in figure we highlight yellow color the top five phases with the highest attention values using topic aware attention in taas and self attention in bart to illustrate the word level attention differences we also provide the generated summary for comparison important phases by the self attention focus on the leading part of articles which fails to capture the important information in the text the generated result is also less meaningful as compared to that generated by taas although summaries by both approaches have successfully covered the keywords in the news taas generates more clear and coherent results and especially it summarizes the key ideas correctly since both models target an abstractive summarization task we further assess the extent to which models are able to perform rewriting by generating an abstractive summary the result from bart is less informative which uses several original sentences from the news e a whole sentence highlighted in purple in figure taas not only captures the key information of the article well but also demonstrates a novel sentence structure we also observe some duplicated information in the summary generated by taas for example after years on the show conveys the same tion as which he hosted for years which indicates additional room for improvements in our future research as an additional comparison we also present the summaries of the same article by other baseline models shown in table discussion effect of different number of topics taas achieves relatively superior performance over several lines demonstrating the effectiveness of incorporating the aware attention into summarization like many deep learning els taas involves many hyperparameters to which the mance might be sensitive among which the number of topics is a critical one that needs more exploration to do so we vary the value of ranging from to to obtain f scores of rouge l here we follow prior literature to only report f score of rouge l figure the impact of the number of topics on model formance note that every reported score here is calculated on the test set the result is shown in figure from which we have the ing observations taas achieves the best performance when this is reasonable given the fact that each batch during training has articles where the number of latent topics should not be too many or too few the rouge l f scores for different s do nt vary too much which might be due to several reasons i the balancing ntm and encoder decoder sequence modeling part is set to as we focus on summarization each generated summary includes top attended words that are likely to be similar even varying given the nature of the dataset where it is unlikely to exhibit diverse topics for articles in a batch effect of different document lengths to test whether taas performs equally well on different lengths of articles we separate the entire cnn dm dataset into three subsets based on the number of sentences which are denoted as cnn short sentences cnn dm medium sentences and cnn dm long sentences with and articles figure performance comparison score of rouge of taas and bart on different lengths of articles respectively as shown in figure taas achieves better mance for longer articles up to improvement of rouge l f score over bart as compared to and for short and medium articles longer articles are likely to have more diverse topics which make the topic attention in the summarization model more salient this further indicates that adding topic level mation can improve the model in the summarization task conclusion in this work we study the abstractive text summarization problem by proposing a topic aware attention model to incorporate global semantic structures of text in particular we combine neural topic modeling and encoder decoder like sequence to sequence model via topic attention for summarization we conduct extensive iments on a real world dataset to compare our proposed approach with several cutting edge methods the results demonstrate the perior performance over some well recognized models in academia and comparable performance to industry sota we also shed light on how the model performance is affected by important parameters and the characteristics of textual data although our current study in this paper incorporates topic word distribution into the framework the other important output from ntm i e document topic distribution is currently neglected which might be worth exploring in our future work furthermore training our model with more powerful computing resources to improve the summarization performance and test the robustness of the model is always interesting to pursue references jimmy lei ba jamie ryan kiros and geoffrey e hinton layer tion arxiv preprint david m blei alp kucukelbir and jon d mcauliffe variational inference a review for statisticians journal of the american statistical association chaitanya chemudugunta padhraic smyth and mark steyvers combining concept hierarchies and statistical topic models in proceedings of the acm conference on information and knowledge management jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language understanding arxiv preprint thomas l griffiths michael i jordan joshua b tenenbaum and david m blei hierarchical topic models and the nested chinese restaurant process in advances in neural information processing systems xiaotao gu yuning mao jiawei han jialu liu you wu cong yu daniel finnie hongkun yu jiaqi zhai and nicholas zukoski generating representative headlines for news stories in proceedings of the web conference kaiming he xiangyu zhang shaoqing ren and jian sun deep residual learning for image recognition in proceedings of the ieee conference on computer vision and pattern recognition dan jurafsky and james h martin speech and language processing vol diederik p kingma and max welling auto encoding variational bayes arxiv preprint mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer bart denoising sequence to sequence pre training for natural language generation translation and comprehension arxiv preprint wei li and andrew mccallum pachinko allocation dag structured mixture models of topic correlations in proceedings of the international conference on machine learning chin yew lin rouge a package for automatic evaluation of summaries in text summarization branches out yang liu and mirella lapata text summarization with pretrained encoders in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language cessing emnlp ijcnlp yuning mao liyuan liu qi zhu xiang ren and jiawei han facet aware evaluation for extractive text summarization arxiv yu meng jiaxin huang guangyuan wang zihan wang chao zhang yu zhang and jiawei han discriminative topic mining via category name guided text embedding in proceedings of the web conference yishu miao edward grefenstette and phil blunsom discovering discrete latent topics with neural variational inference arxiv preprint yishu miao lei yu and phil blunsom neural variational inference for text processing in international conference on machine learning ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based sequence model for extractive summarization of documents arxiv preprint shashi narayan shay b cohen and mirella lapata ranking sentences for extractive summarization with reinforcement learning in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long papers romain paulus caiming xiong and richard socher a deep reinforced model for abstractive summarization arxiv preprint alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever language models are unsupervised multitask learners openai blog colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu exploring the limits of transfer learning with a unified text to text transformer journal of machine learning research abigail see peter j liu and christopher d manning get to the point summarization with pointer generator networks arxiv preprint akash srivastava and charles sutton autoencoding variational inference for topic models arxiv preprint ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in advances in neural information processing systems yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang and ming zhou prophetnet predicting future n gram for sequence to sequence pre training arxiv preprint liang yang fan wu junhua gu chuan wang xiaochun cao di jin and fang guo graph attention topic modeling network in proceedings of the web conference yi yang and kunpeng zhang sdtm a supervised bayesian deep topic model for text analytics available at ssrn jingqing zhang yao zhao mohammad saleh and peter j liu pegasus pre training with extracted gap sentences for abstractive summarization arxiv preprint qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao neural document summarization by jointly learning to score and select sentences in proceedings of the annual meeting of the association for computational linguistics volume long papers
