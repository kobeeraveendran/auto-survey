v o n g l s c v v x r submitted conference paper iclr read highlight summarize cal neural semantic encoder based approach rajeev bhatt ambati department electrical engineering pennsylvania state university state college pa usa com saptarashmi bandyopadhyay department computer science engineering pennsylvania state university state college pa usa com prasenjit mitra college information science technology pennsylvania state university state college pa usa edu abstract traditional sequence sequence models variations attention mechanism hierarchical attention applied text summarization problem hierarchy way humans use guage forming paragraphs sentences sentences words cal models usually worked better traditional counterparts effect mainly hierarchical attention anisms sparse hard attention noisy soft attention paper propose method based extracting highlights document key concept conveyed sentences typical text summarization dataset consisting documents tokens length average capturing long term dependencies important e sentence grouped rst sentence document form summary lstms long term memory proved useful machine translation fail capture long term dependencies modeling long sequences address issues adapted neural semantic encoders nse text summarization class memory augmented neural networks improving functionalities proposed novel hierarchical nse outperforms similar previous models signicantly quality summarization improved augmenting guistic factors lemma speech pos tags word dataset improved vocabulary coverage generalization cal nse model factored dataset outperformed state art nearly rouge points designed rst gpu based self critical reinforcement learning model introduction large number documents need read limited time resort reading summaries instead document automatically generating abstractive summaries problem applications e automatic authoring banerjee mitra developed automatic text summarization systems condense large documents short readable summaries single e rush et al et al nallapati et al multi document summarization e celikyilmaz et al nallapati et al hen et al text summarization broadly classied categories extractive e nallapati et al narayan et al abstractive summarization e nallapati et al chopra et al chen bansal extractive approaches select sentences given submitted conference paper iclr document groups form concise summaries contrast abstractive approaches generate human readable summaries primarily capture semantics input documents contain rephrased key content task falls classication paradigm belongs generative modeling paradigm harder problem solve backbone state art summarization models typical encoder decoder sutskever et al architecture proved effective sequential modeling tasks machine translation sentiment analysis natural language generation contains encoder maps raw input word vector representations latent vector decoder usually equipped variant attention mechanism bahdanau et al uses latent vectors generate output sequence summary case models trained supervised learning setting minimize cross entropy loss predicted target summary encoder decoder models proved effective short sequence tasks machine translation length sequence tokens text summarization length sequences vary tokens modeling long term dependencies increasingly difcult despite metric s known drawbacks text summarization models evaluated rouge lin discrete similarity score predicted target summaries based gram gram gram overlap cross entropy loss convenient objective train model rouge differentiable create mismatch metrics training evaluation particular summary scores rouge evaluation comparable target summary assigned lower probability supervised model tackle problem self critic policy gradient method rennie et al train models directly rouge score reward paper propose architecture addresses issues discussed problem formulation let d dn set document sentences sentence n set words s sm set summary sentences general sentences d continuation sentence related example terms factual details pronouns dividing document multiple paragraphs celikyilmaz et al leaves possibility sentence level dependency start end document similarly abstracting single document sentence chen bansal include related information multiple document sentences good human written summary summary sentence compressed version document sentences mathematically s s dk d dk c compressor intend learn figure represents fundamental idea sequence sequence architecture sentence s summary representations related document sentences dk expected form cluster represents highlight document adapt neural semantic encoder nse text summarization improving tion mechanism compose function standard sequence sequence model decoder access input sequence hidden states lstm hochreiter schmidhuber suffers difculties discussed nse equipped additional memory maintains rich representation words evolving time propose novel hierarchical nse separate word memories sentence enrich word representations document memory enrich sentence representations performed better previous counterparts nallapati et al nallapati et al ling rush finally use maximum entropy self critic model achieve better performance rouge evaluation submitted conference paper iclr figure document sentences rst projected semantic space typically encoder sequence sequence model highlights document representing closely related sentence semantics respectively lights decoder form concise summaries related work rst encoder decoder text summarziation rush et al coupled tention mechanism encoder decoder models gave state art performance neural machine translation nmt maximum sequence length nmt tokens cal document lengths text summarization vary tokens lstm effective loss memory time long sequences nallapati et al chical et al mitigate effect word lstm encode decode words sentence lstm encode decode sentences use lstms separately words sentences improves ability model retain memory longer sequences additionally nallapati et al explored hierarchical model consisting feature rich encoder incorporating position named entity recognition ner tag term frequency tf inverse document frequency idf scores rnn sequential model puting time step needs previous time steps computed slow computation time steps performed parallel chopra et al convolutional layers coupled attention mechanism bahdanau et al increase speed encoder input rnn fed sequentially expected capture positional information works nallapati et al chopra et al found positional embeddings useful reasons unknown nallapati et al proposed extractive summarization model classies sentences based content saliency novelty position deal vocabulary oov words facilitate copying salient information input sequence output et al proposed pointer generator network bines pointing vinyals et al generation vocabulary soft switch attention models longer sequences tend repetitive decoder repeatedly attending position encoder mitigate issue et al coverage mechanism penalize decoder attending locations encoder pointer generator coverage model et al highly extractive copying article tences time paulus et al introduced intra attention model attention depends predictions previous time steps main issues sequence sequence models optimization entropy objective provide excellent results models suffer match training objective evaluation metrics rouge lin meteor banerjee lavie popular algorithm train decoder teacher forcing algorithm minimizes negative log likelihood cross entropy loss decoding time step given previous ground truth outputs testing stage prediction submitted conference paper iclr ous time step fed input decoder instead ground truth exposure bias results error accumulation time step model exposed predictions training instead recent works summarization models trained forcement learning rl rouge lin score reward paulus et al chen bansal celikyilmaz et al hen et al earlier attempt q learning single multi document summarization later ling rush proposed coarse hierarchical attention model select salient sentence sentence attention reinforce williams feed decoder narayan et al reinforce rank sentences extractive tion celikyilmaz et al proposed deep communicating agents operate small chunks document learned self critical rennie et al training approach sisting intermediate rewards chen bansal advantage actor critic method extract sentences followed decoder form abstractive summaries model suffer limiting assumption summary sentence abstracted version single source sentence paulus et al trained intra attention model self critical policy gradient algorithm rennie et al rl objective gives high rouge score output summaries readable humans mitigate problem paulus et al weighted sum supervised learning loss rl loss humans rst form abstractive representation want try words communicating intuitive hierarchy sentence representation words observed nallapati et al ling rush hierarchical attention models failed outperform simple attention model rush et al unlike feedforward networks rnns expected capture input sequence order strangely positional embeddings found effective nallapati et al chopra et al ling rush nallapati et al explored approaches solve issues improve performance neural models abstractive summarization proposed models section rst describe baseline neural semantic encoder nse class discuss provements compose function attention mechanism propose hierarchical nse finally discuss self critic model boost performance rouge evaluation neural semantic encoder neural semantic encoder munkhdalai yu memory augmented neural network augmented encoding memory supports read compose write operations unlike traditional sequence sequence models additional memory relieves lstm burden remember input sequence compared attention model bahdanau et al uses additional context vector nse anytime access input sequence larger memory encoding memory evolved basic operations described follows ot lst m read zt sof t mr zt t ct f m lp c ot mt t lst m w ct submitted conference paper iclr mt zt ht r w f m lp c multi layer read compose write operations respectively el rl ek rk xt rd raw embedding vector current time step f lst m perceptron lst m vectors ones matrix ones outer product instead raw input read function lst m equation uses lstm project word embeddings internal space memory obtain hidden states ot alignment scores zt past memory calculated ot key simple dot product attention mechanism shown equation weighted sum gives retrieved input memory equation multi layer perceptron composing new information equation uses lstm projects composed states internal space memory obtain write states ht finally equation memory updated erasing retrieved memory zt writing write vector ht process performed time step input sequence encoded memories m t similarly decoder obtain write vectors eventually fed projection softmax layers vocabulary distribution r improved nse vanilla nse described performed machine translation dot product attention mechanism simplistic text summarization machine translation sufcient compute correlation word vectors semantic spaces different languages contrast text summarization needs word sentence sentence sentence correlation word word correlation search attention mechanism better capacity model complex semantic relationships inherent text summarization found additive attention mechanism bahdanau et al given equation performs zt sof tanh w u ot battn w u battn learnable parameters important difference compose function multi layer perceptron mlp machine translation sequences short length text summarization consists longer sequences sentence sentence dependencies history previously composed words necessary overcoming repetition rush et al maintaining novelty powerful function disposal lstm replaced mlp lstm shown ht lst m w ct standard text summarization task limited size word vocabulary vocabulary oov words replaced tokens pointer networks vinyals et al facilitate ability copy words input sequence output pointing later et al proposed hybrid pointer generator mechanism improve pointing retaining ability generate new words points words input sequence generates new words vocabulary generation probability pgen calculated retrieved memories attention distribution current input hidden state ot write state ht follows pgen t mmr t w t h ht w t o ot bptr wm wh wo bptr learnable parameters sigmoid activation function pgen soft switch choose generating word vocabulary sampling pvocab copying word input sequence sampling attention distribution document maintain auxiliary vocabulary oov words input sequence obtain following nal probability distribution total extended vocabulary pgenpvocab pgen zt w wi submitted conference paper iclr note w oov word zero similarly w appear source document zt zero ability produce oov words primary advantages pointer generator mechanism use smaller vocabulary size speed computation output projection softmax layers w wi hierarchical nse figure hierarchical nse given article m sentences consisting n words processed nse read r compose c write w operations sentence memory updated n times word sentence m k encoder step updated sentence memories m n sm concatenated form cumulative sentence memory ms decoder uses cumulative sentence memory ms document memory md similar fashion produce write vectors ht passed softmax layer obtain vocabulary distribution m n m n n humans read document organize terms word semantics followed sentence semantics document semantics text summarization task reading document sentences similar meanings continual information grouped pressed words hierarchical model rst introduced yang et al document classication later explored unsuccessfully text summarization nallapati et al work propose use hierarchical model improved nse advantage augmented memory hierarchical document representation use separate memory sentence represent words sentence document memory represent sentences word memory composes novel words document memory composes novel sentences encoding process later extract highlights decode summaries shown figure let d input document sequence sin number sentences document tin number words sentence let mi rtind sentence memories encode words sentence m d m d rsind document memory encodes sentences present document time step input token read retrieve aligned content corresponding sentence memory m s t note retrieved document memory weighted combination sentence representations forms highlight composition sentence document memories written simultaneously way words encoded contextual meaning new simpler sentences formed functionality model follows document memory m d t xt ot ot lst m r t s zs t zd t m s r t zs t m d r t zd ms ot md ms ct lst m c r t r t submitted conference paper iclr lst m w t u s m s m d t u d si sin sin ct zs zd t ht t ht encoder stage decoder stage m s fattn attention mechanism given u pdate remains vanilla nse given concat vector concatenation note nse munkhdalai yu concept shared memory use multiple memories resenting words document memory representing sentences fundamentally different shared memory concept hierarchy self critical sequence training discussed earlier training supervised learning setting creates mismatch training testing objectives feeding ground truth labels training time step creates exposure bias testing feed predictions previous time step policy gradient methods overcome directly optimizing non differentiable metrics rouge lin meteor banerjee lavie posed markov decision process set actions vocabulary reward r rouge score nd policy set sampled words y yt achieves highest rouge score possible summaries self critical model rennie et al proposed image captioning self critical sequence training reinforce algorithm williams modifying baseline greedy output current model time step t model predicts words yt sampled baseline output greedily generated considering probable word vocabulary yt sampled model trained following loss function lrl training objective probability increasing com looping data creating guresropy regularization generate samples high additionally learns t model ht v v v l lrl ht t sampling probability v size ulary similar exploration exploitation trade regularization coefcient explicitly controls trade higher corresponds exploration lower sponds exploitation found tensorflow based open source implementations self critic models use function tf py func runs cpu slow best knowledge rst gpu based implementation experiments results dataset cnn daily mail dataset nallapati et al standard benchmark compare text summarization models corpus training pairs submitted conference paper iclr validation pairs test pairs dened scripts source document ing set words spanning sentences average summaries consist words sentences nallapati et al unique characteristics dataset long documents ordered multi sentence summaries present exciting challenges mainly proven sequence sequence lstm based models nd hard learn long term dependencies long documents train validation test split examples fair parison existing models factoring lemma speech pos tag surface words observed hyay increase performance nmt models terms bleu score drastically improvement vocabulary coverage better generalization added pre processing step incorporating lemma pos tag word dataset ing supervised model factored data process extracting lemma pos tags described bandyopadhyay refer appendix example factoring training settings plain nse models truncated article maximum tokens summary tokens hierarchical nse models articles truncated maximum sentences words sentence shorter sequences padded pad tokens factored models lemma pos tag separator word sequence lengths close times non factored counterparts practical reasons memory time tokens article tokens summary models including pointer generator model use vocabulary size words source target previous works nallapati et al large vocabulary sizes models copy mechanism smaller vocabulary obtain good performance large vocabularies increase computation time memory plays prominent role retrieval update vital start good initialization dimensional pre trained glove pennington et al word vectors represent input sequence model sentence memories initialized glove word vectors words sentence document memories initialized vector representations sentences sentence represented average glove word vectors words models trained adam optimizer default learning rate applied regularization usage dropout penalty resulted similar performance drastically increased training time hierarchical models process sentence time attention distributions need memory larger batch size turn speeds training process non factored model trained nvidia tesla gpus batch size examples gpu takes approximately minutes epoch factored sequences long batch size examples gpu nvidia tesla gpus hier model reaches optimal cross entropy loss epochs unlike epochs nallapati et al et al self critical model training started best supervised model learning rate manually changed needed reported results obtained training days evaluation models evaluated standard metric rouge report scores rouge l quantitively represent word overlap bigram overlap longest common subsequence reference summary summary ated results obtained pyrouge performance models improvements summarized table direct implementation nse performed poorly simple dot product attention mechanism nmt transformation word vectors language english french mere matrix multiplication cause correspondence words underlying linear structure imposed org project submitted conference paper iclr table rouge scores test set hierarchical hier nse model outperform previous hierarchical pointer generator models hier nse factor factored model hier nse sc self critic model paradigm models rouge f score hierattn nallapati et al abstractive model nallapati et al pointer generator et al pointer generator coverage et al hier nse hier nse factor intra attention paulus et al dca celikyilmaz et al l hier nse sc supervised learning reinforcement learning learning word vectors pennington et al text summarization word tence condensation group words sentences complex neural network based attention mechanism proposed improved performance dot product ditive bahdanau et al mechanisms perform similarly nmt task difference pronounced text summarization task simply nature problem described earlier replacing multi layered perceptron mlp nse lstm improved performance remembers previously composed facilitates composition novel words eliminates need additional mechanisms penalize repetitions coverage et al intra attention paulus et al finally ing memories sentence enriches corresponding word representation document memory enriches sentence representation help decoder refer appendix example outputs table shows results comparison previous methods hierarchical model outperforms nallapati et al hier rouge points factored model achieves new state art sota result outperforming celikyilmaz et al rouge points table performance nse models cnn daily mail corpus note data factored model plain nse nse improved attention nse improved compose hierarchical nse rouge f score l conclusion work presented memory augmented neural network text summarization task addresses shortcomings lstm based models applied critical pre processing step factoring dataset inherent linguistic information outperforms state art large margin future explore new sparse functions martins astudillo enforce strict sparsity selecting highlights sentences general framework processing extracting highlights powerful pre trained models like bert devlin et al xlnet yang et al submitted conference paper iclr references dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate url org cite accepted iclr oral presentation saptarashmi bandyopadhyay factored neural machine translation loresmt ceedings workshop technologies mt low resource languages pp dublin ireland august european association machine translation url aclweb org anthology satanjeev banerjee alon lavie meteor automatic metric mt evaluation proceedings acl workshop improved correlation human judgments trinsic extrinsic evaluation measures machine translation summarization pp ann arbor michigan june association computational linguistics url aclweb org anthology siddhartha banerjee prasenjit mitra wikikreator improving wikipedia stubs automatically proceedings annual meeting association computational linguistics international joint conference natural language processing volume long papers pp beijing china july association computational linguistics doi url aclweb org anthology asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents abstractive summarization proceedings conference north american ter association computational linguistics human language technologies volume long papers pp new orleans louisiana june association putational linguistics url aclweb anthology yen chun chen mohit bansal fast abstractive summarization reinforce selected sentence rewriting proceedings annual meeting association computational guistics volume long papers pp melbourne australia july association computational linguistics url aclweb org anthology sumit chopra michael auli alexander m rush abstractive sentence summarization attentive recurrent neural networks proceedings conference north american chapter association computational linguistics human language technologies pp san diego california june association computational linguistics url aclweb org anthology jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language understanding corr url org stefan hen margot mieskes iryna gurevych reinforcement learning approach bernhard fisseni bernhard schroder adaptive multi document summarization torsten zesch eds proceedings international conference german ciety computational linguistics language technology gscl university duisburg essen germany september october pp gscl e v url inf uni wp content pdf sepp hochreiter jurgen schmidhuber long short term memory neural comput november issn neco url neco chin yew lin rouge package automatic evaluation summaries text summarization branches pp barcelona spain july association computational tics url aclweb org anthology submitted conference paper iclr jeffrey ling alexander rush coarse attention models document summarization proceedings workshop new frontiers summarization pp copenhagen mark september association computational linguistics url aclweb org anthology andre f t martins ramon f astudillo softmax sparsemax sparse model tention multi label classication proceedings international conference ternational conference machine learning volume pp jmlr org url acm org citation tsendsuren munkhdalai hong yu neural semantic encoders proceedings ference european chapter association computational linguistics volume long papers pp valencia spain april association computational tics url aclweb org anthology ramesh nallapati bowen zhou cicero dos santos c aglar gulcehre bing xiang tive text summarization sequence sequence rnns proceedings signll conference computational natural language learning pp berlin germany august association computational linguistics url aclweb org anthology ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents proceedings thirty aaai conference articial intelligence pp aaai press url acm org citation shashi narayan shay b cohen mirella lapata ranking sentences extractive tion reinforcement learning proceedings conference north american chapter association computational linguistics human language technologies ume long papers pp new orleans louisiana june association putational linguistics url aclweb anthology romain paulus caiming xiong richard socher deep reinforced model abstractive summarization international conference learning representations url https net jeffrey pennington richard socher christopher manning glove global vectors word resentation proceedings conference empirical methods natural language processing emnlp pp doha qatar october association computational linguistics url aclweb org steven j rennie etienne marcheret youssef mroueh jerret ross vaibhava goel self critical sequence training image captioning ieee conference computer vision pattern recognition cvpr pp alexander m rush sumit chopra jason weston neural attention model abstractive tence summarization proceedings conference empirical methods natural language processing pp lisbon portugal september association putational linguistics url aclweb anthology abigail peter j liu christopher d manning point summarization pointer generator networks proceedings annual meeting association computational linguistics volume long papers pp vancouver canada july association computational linguistics url https aclweb org anthology ilya sutskever oriol vinyals quoc v le sequence sequence learning neural networks z ghahramani m welling c cortes n d lawrence k q weinberger eds advances neural information processing systems pp submitted conference paper iclr curran associates inc url nips cc sequence sequence learning neural networks pdf oriol vinyals meire fortunato navdeep jaitly pointer networks proceedings international conference neural information processing systems volume pp cambridge ma usa mit press url acm citation ronald j williams simple statistical gradient following algorithms connectionist issn ment learning mach learn url zhilin yang zihang dai yiming yang jaime g carbonell ruslan salakhutdinov quoc v corr le xlnet generalized autoregressive pretraining language understanding url org zichao yang diyi yang chris dyer xiaodong alex smola eduard hovy hierarchical proceedings conference attention networks document classication north american chapter association computational linguistics human language technologies pp san diego california june association computational linguistics url aclweb org appendix figure shows self critical model examples shown tables chosen shortest article lengths available space constraints figure self critic training reduces exposure bias learning policy samples score better greedy samples test time supervised learning setting submitted conference paper iclr table sample outputs non factored factored input articles factoring surface word augmented lemma pos tag separated original article build blockbuster ght floyd mayweather manny pacquiao las vegas steps gear tuesday night american holds open workout media session streamed live world watch uk factored article dt build build nn dt blockbuster blockbust nn ght ght nn floyd oyd nnp mayweather mayweath nnp cc manny manni nnp pacquiao pacquiao nnp las la nnp vegas vega nnp nnp cd steps step nns rb dt gear gear nn tuesday tuesday nnp night night nn wrb dt american american nnp holds hold vbz open open jj workout workout nn dt media media nns dt session session nn md vb streamed stream vbn live live jj dt world world nn cc prp md watch watch vb prp rb gt summary oyd mayweather holds open media workout uk pm edt american takes manny pacquiao las vegas mayweather s training streamed live world hier nse output build blockbuster ght oyd mayweather manny pacquiao las vegas steps gear tuesday night session streamed live world watch uk session media s open workout media hier nse sc oyd mayweather manny pacquiao las vegas american holds open workout media streamed live world gt summary factored oyd oyd nnp mayweather mayweath nnp holds hold vbz open open jj media media nns workout workout nn cd uk uk nnp vbd pm pm cd edt edt nnp nn american american jj takes vbz manny manni nnp pacquiao pacquiao nnp las la nnp vegas vega nnp nnp cd mayweather mayweath nnp s s pos training train nn vbz vbg streamed stream vbn live live jj dt world world nn hier nse output factored dt session session md vb streamed stream vbn live live jj dt world world nn cc prp md watch watch vb prp rb nnp nnp nnp nnp american american nnp holds hold vbz open open jj workout workout nn dt media media nns submitted conference paper iclr table sample outputs non factored factored input articles factoring surface word augmented lemma pos tag separated original article cnn justin timberlake jessica biel welcome parenthood celebrity couple announced arrival son silas randall timberlake statements people silas middle timberlake s maternal grandfather bill bomar died randall musician s middle father s rst people reports couple announced pregnancy january instagram post rst baby factored article jj cnn cnn nnp nnp justin justin nnp timberlake timberlak nnp cc jessica jessica nnp biel biel nnp welcome welcom nn parenthood parenthood nn dt celebrity celebr nn couple coupl nn announced announc vbd dt arrival arriv nn prp son son nn silas sila nnp randall randal nnp timberlake timberlak nnp statements statement nns people peopl nns silas sila nnp wa vbd dt middle middl jj nn timberlake timberlak nnp s s pos maternal matern jj grandfather grandfath nn bill bill nnp bomar bomar nnp wp died die vbd cd randall randal nnp vbz dt musician musician nn s s pos jj middle middl nn nn rb rb hi prp father father nn s s pos rst rst jj people peopl nnp reports report nns dt couple coupl nn announced announc vbd dt pregnancy pregnanc nn january januari nnp dt instagram instagram nnp post post nn prp vbz dt rst rst jj baby babi nn dt gt summary timberlake biel welcome son silas randall timberlake couple announced pregnancy january hier nse output silas middle timberlake s maternal grandfather bill bomar couple announced pregnancy january instagram post rst baby hier nse sc justin timberlake jessica biel couple son silas randall timberlake rst baby gt summary factored timberlake timberlak nnp cc biel biel nnp welcome welcom vbp son son nn silas sila nnp randall randal nnp timberlake timberlak nnp dt couple coupl nn announced announc vbd dt pregnancy pregnanc nn january januari nnp hier nse output factored justin justin nnp timberlake nnp nnp cc jessica jessica nnp nnp nnp nnp statements statement nns people peopl nns nnp vbz dt rst rst jj baby vbz nn dt timberlake jj bill bill nn couple nn s s pos son son nn submitted conference paper iclr table sample outputs hierarchical nse self critical model original article cnn hillary clinton s ofcial announcement went online social media sponded big way terms like hillary clinton yes whyimnotvotingforhillary trending certainly nt far twitter clinton tweeted announcement opinion thought new paign million views announcment tweets hour facebook video views far sunday evening tweeted immediate support word groundtruth summary response social media led multiple trending topics hillary clinton s presidential nouncement responded video new campaign logo hier nse output hillary clinton tweeted announcement opinion thought new campaign tweeted immediate support word hillary clinton yes hier nse sc hillary clinton s ofcial announcement clinton hillary clinton table factored input outputs example table article factored jj cnn cnn nnp nnp onc nnp hillary hillari nnp clinton clinton nnp s s pos ofcial ofci jj announcement announc nn went went vbd online onlin nn social social jj media media nns responded respond vbd dt big big jj way way nn terms term nns like like hillary hillari nnp clinton clinton nnp nnp cc yes ye uh rb whyimnotvotingforhillary whyimnotvotingforhillari nnp trending trend nn certainly certainli rb prp md nt nt rb vb far far rb twitter twitter nnp nnp rb befor clinton clinton nnp tweeted tweet vbd prp announcement announc nn nn dt opinion opinion nn cc thought thought nn prp new new jj campaign campaign nn nn ex vbd cd million million cd views view nns prp announcment announc jj tweets tweet nns cd hour hour nn cc cd facebook facebook nnp video video nn views view nns rb far far rb sunday sunday nnp evening vbg nn dt tweeted tweet vbd prp immediate immedi jj support support nn cd word word nn gt summary factored response respons nnp social social jj media media nns led led vbd multiple multipl vb trending trend vbg topics topic nns hillary hillari nnp clinton clinton nnp s s pos presidential presidenti jj announcement announc nn dt responded respond vbd prp video video nn cc prp new new jj campaign campaign nn logo logo nn hier nse output factored hillary nnp nnp clinton clinton nnp s s pos ofcial jj announcement announc nn went went vbd online onlin nn clinton clinton nnp tweeted tweet vbd prp new new jj campaign campaign nn dt opinion opinion cc thought thought vbd twitter twitter nn terms term nns like like hillary nnp nnp clinton clinton nnp nnp nnp jj submitted conference paper iclr table sample outputs hierarchical nse self critical model original article blackpool talks sign austria defender thomas piermayr year old training championship club week keen board expected conrmed campaign league season piermayr free agent playing colorado rapids austria international spell inverness caledonian thistle thomas piermayr left action colorado rapids tries tackle obafemi martins year groundtruth summary thomas piermayr training blackpool week austrian defender free agent leaving mls colorado rapids blackpool championship look set relegated hier nse output thomas training championship club week austria international spell inverness caledonian thistle blackpool talks sign austria defender thomas hier nse sc blackpool talks sign austria defender thomas training championship club week free agent table factored input outputs example table factored article blackpool blackpool nnp vbp talks talk nns sign sign vb austria austria nnp defender defend nn thomas thoma nnp piermayr piermayr nnp dt year old year old jj ha vbz vbn training train vbg dt championship championship nnp club club nn thi dt week week nn cc prp vbp keen keen jj vb prp board board nn wp vbz expected expect vbn vb conrmed conrm vbn dt campaign campaign nn league leagu nnp nnp jj season season nn piermayr piermayr nnp vbz free free jj agent agent nn cc vbd vbn playing play vbg colorado colorado nnp rapids rapid nnp dt jj austria austria nnp nnp international intern jj vbd dt spell spell nn inverness inver nnp caledonian caledonian nnp thistle thistl nnp cd thomas thoma nnp piermayr piermayr nnp nnp left left vbd action action nn dt colorado colorado nnp rapids rapid nnp nnp tries tri vbz tackle tackl vb obafemi obafemi nnp martins martin nnp jj year year nn gt summary factored thomas thoma nnp piermayr piermayr nnp ha vbz vbn training train vbg blackpool blackpool nnp thi dt week week nn austrian austrian jj defender defend nn vbz free free jj agent agent nn leaving leav vbg mls ml nnp nn colorado colorado nnp rapids rapid nnp blackpool blackpool nnp vbp nn dt championship championship nnp cc look look vb set set vbn vb relegated releg vbn hier nse output factored dt year old year old jj ha vbz vbn training train nnp dt championship championship nnp club club nnp thi dt week week nn dt jj austria austria nnp nnp international intern jj vbd dt spell spell nn nnp nnp nnp nnp nnp nnp
