submitted conference paper iclr read highlight summarize cal neural semantic encoder based approach rajeev bhatt ambati department electrical engineering pennsylvania state university state college usa com saptarashmi bandyopadhyay department computer science engineering pennsylvania state university state college usa com prasenjit mitra college information science technology pennsylvania state university state college usa edu abstract traditional sequence sequence models variations attention mechanism hierarchical attention applied text summarization problem hierarchy way humans use guage forming paragraphs sentences sentences words cal models usually worked better traditional counterparts effect mainly hierarchical attention anisms sparse hard attention noisy soft attention paper propose method based extracting highlights document key concept conveyed sentences typical text summarization dataset consisting documents tokens length average capturing long term dependencies important sentence grouped rst sentence document form summary lstms long term memory proved useful machine translation fail capture long term dependencies modeling long sequences address issues adapted neural semantic encoders nse text summarization class memory augmented neural networks improving functionalities proposed novel hierarchical nse outperforms similar previous models signicantly quality summarization improved augmenting guistic factors lemma speech pos tags word dataset improved vocabulary coverage generalization cal nse model factored dataset outperformed state art nearly rouge points designed rst gpu based self critical reinforcement learning model introduction large number documents need read limited time resort reading summaries instead document automatically generating abstractive summaries problem applications automatic authoring banerjee mitra developed automatic text summarization systems condense large documents short readable summaries single rush nallapati multi document summarization celikyilmaz nallapati hen text summarization broadly classied categories extractive nallapati narayan abstractive summarization nallapati chopra chen bansal extractive approaches select sentences given submitted conference paper iclr document groups form concise summaries contrast abstractive approaches generate human readable summaries primarily capture semantics input documents contain rephrased key content task falls classication paradigm belongs generative modeling paradigm harder problem solve backbone state art summarization models typical encoder decoder sutskever architecture proved effective sequential modeling tasks machine translation sentiment analysis natural language generation contains encoder maps raw input word vector representations latent vector decoder usually equipped variant attention mechanism bahdanau uses latent vectors generate output sequence summary case models trained supervised learning setting minimize cross entropy loss predicted target summary encoder decoder models proved effective short sequence tasks machine translation length sequence tokens text summarization length sequences vary tokens modeling long term dependencies increasingly difcult despite metric known drawbacks text summarization models evaluated rouge lin discrete similarity score predicted target summaries based gram gram gram overlap cross entropy loss convenient objective train model rouge differentiable create mismatch metrics training evaluation particular summary scores rouge evaluation comparable target summary assigned lower probability supervised model tackle problem self critic policy gradient method rennie train models directly rouge score reward paper propose architecture addresses issues discussed problem formulation let set document sentences sentence set words set summary sentences general sentences continuation sentence related example terms factual details pronouns dividing document multiple paragraphs celikyilmaz leaves possibility sentence level dependency start end document similarly abstracting single document sentence chen bansal include related information multiple document sentences good human written summary summary sentence compressed version document sentences mathematically compressor intend learn figure represents fundamental idea sequence sequence architecture sentence summary representations related document sentences expected form cluster represents highlight document adapt neural semantic encoder nse text summarization improving tion mechanism compose function standard sequence sequence model decoder access input sequence hidden states lstm hochreiter schmidhuber suffers difculties discussed nse equipped additional memory maintains rich representation words evolving time propose novel hierarchical nse separate word memories sentence enrich word representations document memory enrich sentence representations performed better previous counterparts nallapati nallapati ling rush finally use maximum entropy self critic model achieve better performance rouge evaluation submitted conference paper iclr figure document sentences rst projected semantic space typically encoder sequence sequence model highlights document representing closely related sentence semantics respectively lights decoder form concise summaries related work rst encoder decoder text summarziation rush coupled tention mechanism encoder decoder models gave state art performance neural machine translation nmt maximum sequence length nmt tokens cal document lengths text summarization vary tokens lstm effective loss memory time long sequences nallapati chical mitigate effect word lstm encode decode words sentence lstm encode decode sentences use lstms separately words sentences improves ability model retain memory longer sequences additionally nallapati explored hierarchical model consisting feature rich encoder incorporating position named entity recognition ner tag term frequency inverse document frequency idf scores rnn sequential model puting time step needs previous time steps computed slow computation time steps performed parallel chopra convolutional layers coupled attention mechanism bahdanau increase speed encoder input rnn fed sequentially expected capture positional information works nallapati chopra found positional embeddings useful reasons unknown nallapati proposed extractive summarization model classies sentences based content saliency novelty position deal vocabulary oov words facilitate copying salient information input sequence output proposed pointer generator network bines pointing vinyals generation vocabulary soft switch attention models longer sequences tend repetitive decoder repeatedly attending position encoder mitigate issue coverage mechanism penalize decoder attending locations encoder pointer generator coverage model highly extractive copying article tences time paulus introduced intra attention model attention depends predictions previous time steps main issues sequence sequence models optimization entropy objective provide excellent results models suffer match training objective evaluation metrics rouge lin meteor banerjee lavie popular algorithm train decoder teacher forcing algorithm minimizes negative log likelihood cross entropy loss decoding time step given previous ground truth outputs testing stage prediction submitted conference paper iclr ous time step fed input decoder instead ground truth exposure bias results error accumulation time step model exposed predictions training instead recent works summarization models trained forcement learning rouge lin score reward paulus chen bansal celikyilmaz hen earlier attempt learning single multi document summarization later ling rush proposed coarse hierarchical attention model select salient sentence sentence attention reinforce williams feed decoder narayan reinforce rank sentences extractive tion celikyilmaz proposed deep communicating agents operate small chunks document learned self critical rennie training approach sisting intermediate rewards chen bansal advantage actor critic method extract sentences followed decoder form abstractive summaries model suffer limiting assumption summary sentence abstracted version single source sentence paulus trained intra attention model self critical policy gradient algorithm rennie objective gives high rouge score output summaries readable humans mitigate problem paulus weighted sum supervised learning loss loss humans rst form abstractive representation want try words communicating intuitive hierarchy sentence representation words observed nallapati ling rush hierarchical attention models failed outperform simple attention model rush unlike feedforward networks rnns expected capture input sequence order strangely positional embeddings found effective nallapati chopra ling rush nallapati explored approaches solve issues improve performance neural models abstractive summarization proposed models section rst describe baseline neural semantic encoder nse class discuss provements compose function attention mechanism propose hierarchical nse finally discuss self critic model boost performance rouge evaluation neural semantic encoder neural semantic encoder munkhdalai memory augmented neural network augmented encoding memory supports read compose write operations unlike traditional sequence sequence models additional memory relieves lstm burden remember input sequence compared attention model bahdanau uses additional context vector nse anytime access input sequence larger memory encoding memory evolved basic operations described follows lst read sof lst submitted conference paper iclr multi layer read compose write operations respectively raw embedding vector current time step lst perceptron lst vectors ones matrix ones outer product instead raw input read function lst equation uses lstm project word embeddings internal space memory obtain hidden states alignment scores past memory calculated key simple dot product attention mechanism shown equation weighted sum gives retrieved input memory equation multi layer perceptron composing new information equation uses lstm projects composed states internal space memory obtain write states finally equation memory updated erasing retrieved memory writing write vector process performed time step input sequence encoded memories similarly decoder obtain write vectors eventually fed projection softmax layers vocabulary distribution improved nse vanilla nse described performed machine translation dot product attention mechanism simplistic text summarization machine translation sufcient compute correlation word vectors semantic spaces different languages contrast text summarization needs word sentence sentence sentence correlation word word correlation search attention mechanism better capacity model complex semantic relationships inherent text summarization found additive attention mechanism bahdanau given equation performs sof tanh battn battn learnable parameters important difference compose function multi layer perceptron mlp machine translation sequences short length text summarization consists longer sequences sentence sentence dependencies history previously composed words necessary overcoming repetition rush maintaining novelty powerful function disposal lstm replaced mlp lstm shown lst standard text summarization task limited size word vocabulary vocabulary oov words replaced tokens pointer networks vinyals facilitate ability copy words input sequence output pointing later proposed hybrid pointer generator mechanism improve pointing retaining ability generate new words points words input sequence generates new words vocabulary generation probability pgen calculated retrieved memories attention distribution current input hidden state write state follows pgen mmr bptr bptr learnable parameters sigmoid activation function pgen soft switch choose generating word vocabulary sampling pvocab copying word input sequence sampling attention distribution document maintain auxiliary vocabulary oov words input sequence obtain following nal probability distribution total extended vocabulary pgenpvocab pgen submitted conference paper iclr note oov word zero similarly appear source document zero ability produce oov words primary advantages pointer generator mechanism use smaller vocabulary size speed computation output projection softmax layers hierarchical nse figure hierarchical nse given article sentences consisting words processed nse read compose write operations sentence memory updated times word sentence encoder step updated sentence memories concatenated form cumulative sentence memory decoder uses cumulative sentence memory document memory similar fashion produce write vectors passed softmax layer obtain vocabulary distribution humans read document organize terms word semantics followed sentence semantics document semantics text summarization task reading document sentences similar meanings continual information grouped pressed words hierarchical model rst introduced yang document classication later explored unsuccessfully text summarization nallapati work propose use hierarchical model improved nse advantage augmented memory hierarchical document representation use separate memory sentence represent words sentence document memory represent sentences word memory composes novel words document memory composes novel sentences encoding process later extract highlights decode summaries shown figure let input document sequence sin number sentences document tin number words sentence let rtind sentence memories encode words sentence rsind document memory encodes sentences present document time step input token read retrieve aligned content corresponding sentence memory note retrieved document memory weighted combination sentence representations forms highlight composition sentence document memories written simultaneously way words encoded contextual meaning new simpler sentences formed functionality model follows document memory lst lst submitted conference paper iclr lst sin sin encoder stage decoder stage fattn attention mechanism given pdate remains vanilla nse given concat vector concatenation note nse munkhdalai concept shared memory use multiple memories resenting words document memory representing sentences fundamentally different shared memory concept hierarchy self critical sequence training discussed earlier training supervised learning setting creates mismatch training testing objectives feeding ground truth labels training time step creates exposure bias testing feed predictions previous time step policy gradient methods overcome directly optimizing non differentiable metrics rouge lin meteor banerjee lavie posed markov decision process set actions vocabulary reward rouge score policy set sampled words achieves highest rouge score possible summaries self critical model rennie proposed image captioning self critical sequence training reinforce algorithm williams modifying baseline greedy output current model time step model predicts words sampled baseline output greedily generated considering probable word vocabulary sampled model trained following loss function lrl training objective probability increasing com looping data creating guresropy regularization generate samples high additionally learns model lrl sampling probability size ulary similar exploration exploitation trade regularization coefcient explicitly controls trade higher corresponds exploration lower sponds exploitation found tensorflow based open source implementations self critic models use function func runs cpu slow best knowledge rst gpu based implementation experiments results dataset cnn daily mail dataset nallapati standard benchmark compare text summarization models corpus training pairs submitted conference paper iclr validation pairs test pairs dened scripts source document ing set words spanning sentences average summaries consist words sentences nallapati unique characteristics dataset long documents ordered multi sentence summaries present exciting challenges mainly proven sequence sequence lstm based models hard learn long term dependencies long documents train validation test split examples fair parison existing models factoring lemma speech pos tag surface words observed hyay increase performance nmt models terms bleu score drastically improvement vocabulary coverage better generalization added pre processing step incorporating lemma pos tag word dataset ing supervised model factored data process extracting lemma pos tags described bandyopadhyay refer appendix example factoring training settings plain nse models truncated article maximum tokens summary tokens hierarchical nse models articles truncated maximum sentences words sentence shorter sequences padded pad tokens factored models lemma pos tag separator word sequence lengths close times non factored counterparts practical reasons memory time tokens article tokens summary models including pointer generator model use vocabulary size words source target previous works nallapati large vocabulary sizes models copy mechanism smaller vocabulary obtain good performance large vocabularies increase computation time memory plays prominent role retrieval update vital start good initialization dimensional pre trained glove pennington word vectors represent input sequence model sentence memories initialized glove word vectors words sentence document memories initialized vector representations sentences sentence represented average glove word vectors words models trained adam optimizer default learning rate applied regularization usage dropout penalty resulted similar performance drastically increased training time hierarchical models process sentence time attention distributions need memory larger batch size turn speeds training process non factored model trained nvidia tesla gpus batch size examples gpu takes approximately minutes epoch factored sequences long batch size examples gpu nvidia tesla gpus hier model reaches optimal cross entropy loss epochs unlike epochs nallapati self critical model training started best supervised model learning rate manually changed needed reported results obtained training days evaluation models evaluated standard metric rouge report scores rouge quantitively represent word overlap bigram overlap longest common subsequence reference summary summary ated results obtained pyrouge performance models improvements summarized table direct implementation nse performed poorly simple dot product attention mechanism nmt transformation word vectors language english french mere matrix multiplication cause correspondence words underlying linear structure imposed org project submitted conference paper iclr table rouge scores test set hierarchical hier nse model outperform previous hierarchical pointer generator models hier nse factor factored model hier nse self critic model paradigm models rouge score hierattn nallapati abstractive model nallapati pointer generator pointer generator coverage hier nse hier nse factor intra attention paulus dca celikyilmaz hier nse supervised learning reinforcement learning learning word vectors pennington text summarization word tence condensation group words sentences complex neural network based attention mechanism proposed improved performance dot product ditive bahdanau mechanisms perform similarly nmt task difference pronounced text summarization task simply nature problem described earlier replacing multi layered perceptron mlp nse lstm improved performance remembers previously composed facilitates composition novel words eliminates need additional mechanisms penalize repetitions coverage intra attention paulus finally ing memories sentence enriches corresponding word representation document memory enriches sentence representation help decoder refer appendix example outputs table shows results comparison previous methods hierarchical model outperforms nallapati hier rouge points factored model achieves new state art sota result outperforming celikyilmaz rouge points table performance nse models cnn daily mail corpus note data factored model plain nse nse improved attention nse improved compose hierarchical nse rouge score conclusion work presented memory augmented neural network text summarization task addresses shortcomings lstm based models applied critical pre processing step factoring dataset inherent linguistic information outperforms state art large margin future explore new sparse functions martins astudillo enforce strict sparsity selecting highlights sentences general framework processing extracting highlights powerful pre trained models like bert devlin xlnet yang submitted conference paper iclr references dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate url org cite accepted iclr oral presentation saptarashmi bandyopadhyay factored neural machine translation loresmt ceedings workshop technologies low resource languages dublin ireland august european association machine translation url aclweb org anthology satanjeev banerjee alon lavie meteor automatic metric evaluation proceedings acl workshop improved correlation human judgments trinsic extrinsic evaluation measures machine translation summarization ann arbor michigan june association computational linguistics url aclweb org anthology siddhartha banerjee prasenjit mitra wikikreator improving wikipedia stubs automatically proceedings annual meeting association computational linguistics international joint conference natural language processing volume long papers beijing china july association computational linguistics doi url aclweb org anthology asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents abstractive summarization proceedings conference north american ter association computational linguistics human language technologies volume long papers new orleans louisiana june association putational linguistics url aclweb anthology yen chun chen mohit bansal fast abstractive summarization reinforce selected sentence rewriting proceedings annual meeting association computational guistics volume long papers melbourne australia july association computational linguistics url aclweb org anthology sumit chopra michael auli alexander rush abstractive sentence summarization attentive recurrent neural networks proceedings conference north american chapter association computational linguistics human language technologies san diego california june association computational linguistics url aclweb org anthology jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language understanding corr url org stefan hen margot mieskes iryna gurevych reinforcement learning approach bernhard fisseni bernhard schroder adaptive multi document summarization torsten zesch eds proceedings international conference german ciety computational linguistics language technology gscl university duisburg essen germany september october gscl url inf uni content pdf sepp hochreiter jurgen schmidhuber long short term memory neural comput november issn neco url neco chin yew lin rouge package automatic evaluation summaries text summarization branches barcelona spain july association computational tics url aclweb org anthology submitted conference paper iclr jeffrey ling alexander rush coarse attention models document summarization proceedings workshop new frontiers summarization copenhagen mark september association computational linguistics url aclweb org anthology andre martins ramon astudillo softmax sparsemax sparse model tention multi label classication proceedings international conference ternational conference machine learning volume jmlr org url acm org citation tsendsuren munkhdalai hong neural semantic encoders proceedings ference european chapter association computational linguistics volume long papers valencia spain april association computational tics url aclweb org anthology ramesh nallapati bowen zhou cicero dos santos aglar gulcehre bing xiang tive text summarization sequence sequence rnns proceedings signll conference computational natural language learning berlin germany august association computational linguistics url aclweb org anthology ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents proceedings thirty aaai conference articial intelligence aaai press url acm org citation shashi narayan shay cohen mirella lapata ranking sentences extractive tion reinforcement learning proceedings conference north american chapter association computational linguistics human language technologies ume long papers new orleans louisiana june association putational linguistics url aclweb anthology romain paulus caiming xiong richard socher deep reinforced model abstractive summarization international conference learning representations url https net jeffrey pennington richard socher christopher manning glove global vectors word resentation proceedings conference empirical methods natural language processing emnlp doha qatar october association computational linguistics url aclweb org steven rennie etienne marcheret youssef mroueh jerret ross vaibhava goel self critical sequence training image captioning ieee conference computer vision pattern recognition cvpr alexander rush sumit chopra jason weston neural attention model abstractive tence summarization proceedings conference empirical methods natural language processing lisbon portugal september association putational linguistics url aclweb anthology abigail peter liu christopher manning point summarization pointer generator networks proceedings annual meeting association computational linguistics volume long papers vancouver canada july association computational linguistics url https aclweb org anthology ilya sutskever oriol vinyals quoc sequence sequence learning neural networks ghahramani welling cortes lawrence weinberger eds advances neural information processing systems submitted conference paper iclr curran associates inc url nips sequence sequence learning neural networks pdf oriol vinyals meire fortunato navdeep jaitly pointer networks proceedings international conference neural information processing systems volume cambridge usa mit press url acm citation ronald williams simple statistical gradient following algorithms connectionist issn ment learning mach learn url zhilin yang zihang dai yiming yang jaime carbonell ruslan salakhutdinov quoc corr xlnet generalized autoregressive pretraining language understanding url org zichao yang diyi yang chris dyer xiaodong alex smola eduard hovy hierarchical proceedings conference attention networks document classication north american chapter association computational linguistics human language technologies san diego california june association computational linguistics url aclweb org appendix figure shows self critical model examples shown tables chosen shortest article lengths available space constraints figure self critic training reduces exposure bias learning policy samples score better greedy samples test time supervised learning setting submitted conference paper iclr table sample outputs non factored factored input articles factoring surface word augmented lemma pos tag separated original article build blockbuster ght floyd mayweather manny pacquiao las vegas steps gear tuesday night american holds open workout media session streamed live world watch factored article build build blockbuster blockbust ght ght floyd oyd nnp mayweather mayweath nnp manny manni nnp pacquiao pacquiao nnp las nnp vegas vega nnp nnp steps step nns gear gear tuesday tuesday nnp night night wrb american american nnp holds hold vbz open open workout workout media media nns session session streamed stream vbn live live world world prp watch watch prp summary oyd mayweather holds open media workout edt american takes manny pacquiao las vegas mayweather training streamed live world hier nse output build blockbuster ght oyd mayweather manny pacquiao las vegas steps gear tuesday night session streamed live world watch session media open workout media hier nse oyd mayweather manny pacquiao las vegas american holds open workout media streamed live world summary factored oyd oyd nnp mayweather mayweath nnp holds hold vbz open open media media nns workout workout nnp vbd edt edt nnp american american takes vbz manny manni nnp pacquiao pacquiao nnp las nnp vegas vega nnp nnp mayweather mayweath nnp pos training train vbz vbg streamed stream vbn live live world world hier nse output factored session session streamed stream vbn live live world world prp watch watch prp nnp nnp nnp nnp american american nnp holds hold vbz open open workout workout media media nns submitted conference paper iclr table sample outputs non factored factored input articles factoring surface word augmented lemma pos tag separated original article cnn justin timberlake jessica biel welcome parenthood celebrity couple announced arrival son silas randall timberlake statements people silas middle timberlake maternal grandfather bill bomar died randall musician middle father rst people reports couple announced pregnancy january instagram post rst baby factored article cnn cnn nnp nnp justin justin nnp timberlake timberlak nnp jessica jessica nnp biel biel nnp welcome welcom parenthood parenthood celebrity celebr couple coupl announced announc vbd arrival arriv prp son son silas sila nnp randall randal nnp timberlake timberlak nnp statements statement nns people peopl nns silas sila nnp vbd middle middl timberlake timberlak nnp pos maternal matern grandfather grandfath bill bill nnp bomar bomar nnp died die vbd randall randal nnp vbz musician musician pos middle middl prp father father pos rst rst people peopl nnp reports report nns couple coupl announced announc vbd pregnancy pregnanc january januari nnp instagram instagram nnp post post prp vbz rst rst baby babi summary timberlake biel welcome son silas randall timberlake couple announced pregnancy january hier nse output silas middle timberlake maternal grandfather bill bomar couple announced pregnancy january instagram post rst baby hier nse justin timberlake jessica biel couple son silas randall timberlake rst baby summary factored timberlake timberlak nnp biel biel nnp welcome welcom vbp son son silas sila nnp randall randal nnp timberlake timberlak nnp couple coupl announced announc vbd pregnancy pregnanc january januari nnp hier nse output factored justin justin nnp timberlake nnp nnp jessica jessica nnp nnp nnp nnp statements statement nns people peopl nns nnp vbz rst rst baby vbz timberlake bill bill couple pos son son submitted conference paper iclr table sample outputs hierarchical nse self critical model original article cnn hillary clinton ofcial announcement went online social media sponded big way terms like hillary clinton yes whyimnotvotingforhillary trending certainly far twitter clinton tweeted announcement opinion thought new paign million views announcment tweets hour facebook video views far sunday evening tweeted immediate support word groundtruth summary response social media led multiple trending topics hillary clinton presidential nouncement responded video new campaign logo hier nse output hillary clinton tweeted announcement opinion thought new campaign tweeted immediate support word hillary clinton yes hier nse hillary clinton ofcial announcement clinton hillary clinton table factored input outputs example table article factored cnn cnn nnp nnp onc nnp hillary hillari nnp clinton clinton nnp pos ofcial ofci announcement announc went went vbd online onlin social social media media nns responded respond vbd big big way way terms term nns like like hillary hillari nnp clinton clinton nnp nnp yes whyimnotvotingforhillary whyimnotvotingforhillari nnp trending trend certainly certainli prp far far twitter twitter nnp nnp befor clinton clinton nnp tweeted tweet vbd prp announcement announc opinion opinion thought thought prp new new campaign campaign vbd million million views view nns prp announcment announc tweets tweet nns hour hour facebook facebook nnp video video views view nns far far sunday sunday nnp evening vbg tweeted tweet vbd prp immediate immedi support support word word summary factored response respons nnp social social media media nns led led vbd multiple multipl trending trend vbg topics topic nns hillary hillari nnp clinton clinton nnp pos presidential presidenti announcement announc responded respond vbd prp video video prp new new campaign campaign logo logo hier nse output factored hillary nnp nnp clinton clinton nnp pos ofcial announcement announc went went vbd online onlin clinton clinton nnp tweeted tweet vbd prp new new campaign campaign opinion opinion thought thought vbd twitter twitter terms term nns like like hillary nnp nnp clinton clinton nnp nnp nnp submitted conference paper iclr table sample outputs hierarchical nse self critical model original article blackpool talks sign austria defender thomas piermayr year old training championship club week keen board expected conrmed campaign league season piermayr free agent playing colorado rapids austria international spell inverness caledonian thistle thomas piermayr left action colorado rapids tries tackle obafemi martins year groundtruth summary thomas piermayr training blackpool week austrian defender free agent leaving mls colorado rapids blackpool championship look set relegated hier nse output thomas training championship club week austria international spell inverness caledonian thistle blackpool talks sign austria defender thomas hier nse blackpool talks sign austria defender thomas training championship club week free agent table factored input outputs example table factored article blackpool blackpool nnp vbp talks talk nns sign sign austria austria nnp defender defend thomas thoma nnp piermayr piermayr nnp year old year old vbz vbn training train vbg championship championship nnp club club thi week week prp vbp keen keen prp board board vbz expected expect vbn conrmed conrm vbn campaign campaign league leagu nnp nnp season season piermayr piermayr nnp vbz free free agent agent vbd vbn playing play vbg colorado colorado nnp rapids rapid nnp austria austria nnp nnp international intern vbd spell spell inverness inver nnp caledonian caledonian nnp thistle thistl nnp thomas thoma nnp piermayr piermayr nnp nnp left left vbd action action colorado colorado nnp rapids rapid nnp nnp tries tri vbz tackle tackl obafemi obafemi nnp martins martin nnp year year summary factored thomas thoma nnp piermayr piermayr nnp vbz vbn training train vbg blackpool blackpool nnp thi week week austrian austrian defender defend vbz free free agent agent leaving leav vbg mls nnp colorado colorado nnp rapids rapid nnp blackpool blackpool nnp vbp championship championship nnp look look set set vbn relegated releg vbn hier nse output factored year old year old vbz vbn training train nnp championship championship nnp club club nnp thi week week austria austria nnp nnp international intern vbd spell spell nnp nnp nnp nnp nnp nnp
