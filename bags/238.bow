using local knowledge graph construction to scale models to multi document inputs angela fan fair loria claire gardent cnrs loria chloe braud cnrs loria antoine bordes fair angelafan com claire gardent chloe fr t c o l c s c v v i x r a abstract query based open domain nlp tasks require information synthesis from long and diverse web results current approaches tively select portions of web text as input to sequence to sequence models using ods such as tf idf ranking we propose constructing a local graph structured edge base for each query which compresses the web search information and reduces dundancy we show that by linearizing the graph into a structured input sequence models can encode the graph representations within a standard sequence to sequence setting for two generative tasks with very long text put long form question answering and document summarization feeding graph resentations as input can achieve better mance than using retrieved text portions introduction effective information synthesis is at the core of many natural language processing tions such as open domain question answering and multi document summarization in such tasks a fundamental challenge is the ability to distill evant knowledge from hundreds of thousands of tokens of noisy and redundant input such as pages current approaches predominantly conduct tf idf based information extraction to identify key portions of the information and then provide this as sequential input to a sequence to sequence model the sub selected portions are limited to a few thousand words as models often struggle to encode much longer sequences in this work we propose a mechanism to structure free text into local knowledge graphs that are then linearized into sequences creating a canonical form in which information is presented to models by constructing a graph ary redundant information can be merged or carded producing substantially compressed input figure multi document input to linearized graph multi document input resulting from web search queries are converted to a graph structured knowledge base using erence resolution and information extraction then linearized into a sequence for models color indicates erence resolution node weight is indicated by circle radius and edge weight by line thickness small enough to be fully encoded by models such a method can be seen as merging previous work on symbolic knowledge bases for information extraction with newer approaches ing deep neural networks to encode knowledge our approach shown in figure takes a query and its corresponding multi document web search results and builds for each query a specic local knowledge graph we present several modeling contributions to effectively encode the entire graph as a sequence and attend to the most relevant tions within this linearization we demonstrate the effectiveness of this approach on two scale generative tasks with both long and noisy multi document web input and paragraph length output long form question answering on the dataset fan et al and wikipedia lead graph generation as a multi document tion problem liu et al related work interest in generative sequence modeling has tensied due to recent improvements peters et al devlin et al radford et al making the challenge of information synthesis more relevant in contrast to extractive tasks which only require models to identify spans and can do so effectively on long documents by ing at the paragraphs independently generative quence models must combine multiple pieces of evidence from long and noisy multi document put to generate correct and convincing responses multi document input previous work in multi document summarization barzilay et al applies various techniques to handle long input including clustering to nd similar information honarpisheh et al extractive methods to select relevant sentences daume iii and marcu gillick and favre berg kirkpatrick et al di fabbrizio et al bing et al cao et al cluding maximal marginal relevance fabbri et al and incorporating queries baumel et al and graphs ganesan et al yasunaga et al however there are few large scale multi document summarization datasets and many approaches have focused on extractive selection or hybrid extractive abstractive models in this work we use graph construction to re structure document input for abstractive generation advancements in question answering have amined performance on datasets with document input such as triviaqa joshi et al various approaches have been proposed including leveraging tf idf and bigram ing with an rnn to nd relevant information chen et al models that score individual paragraphs for sub selection clark and gardner and nearest neighbor search with paragraph re ranking das et al however these approaches have been applied to extractive tion answering tasks that require span tion rather than abstractive text generation in an information synthesis setting using knowledge bases previous work has explored various ways of resenting information in knowledge bases al and improving these tions chen et al knowledge bases have been leveraged to improve performance on various tasks from coreference resolution ng and cardie and question answering zheng bao et al cui et al sun et al to signal processing et al various works convert text into abstract meaning sentations liu et al for domains such as news vossen et al rospocher et al and link nodes to large knowledge bases such as dbpedia auer et al wities et al combine open information extraction with erence and lexical inference to build knowledge representations they apply this to tweets and analyze the accuracy on various aspects of graph construction das et al construct graphs from procedural text to track entity position to swer when and if entities are created destroyed or moved in contrast we build graphs from tially longer multi document input and use them for multi sentence text generation recently many have explored neural tectures that can encode graph structured input bruna et al kipf and welling beck et al zhou et al xu et al lai et al these models often represent graphs as adjacency matrices to generalize architectures such as convolutional networks to graph inputs rather than encoding a static knowledge graph or leveraging external knowledge graphs we build a local graph for each query and model these using standard models we leave the ration of graph networks for future work graph construction we describe how symbolic graph representations of knowledge can be constructed from text our approach assumes a multi document input such as web pages resulting from the execution of a query the graph construction process presses the web search input to a signicantly smaller size allowing models to encode the tirety of the compression and reduces dancy through merge operations allowing relevant information to be more easily identied tions edges are merged similarly with existing edges between the same two nodes such merge operations allow strings such as the nobel prize and nobel prize to be represented as one node rather than separately similarly coreference olution aids in merging by identifying that bert einstein and he refer to the same entity and thus merging them the construction of the graph reduces redundancy the size of the graph can be modied by controlling which triples are added using tf idf overlap see figure step idf overlap of the triple with the question can be used to determine if the triple contains relevant formation this improves robustness to noisy web search input and helps lter entirely irrelevant tions such as scraped html tags modeling graphs as sequences current models for text generation often use architectures such as the transformer vaswani et al these models are signed to encode sequences rather than graphs we describe now how to convert a graph into a structured input sequence our complete model will take as input a linearized graph by coding graph attributes such as node and edge weight as embeddings we add hierarchical and memory compressed attention mechanisms to scale models to encode the full graph and attend to the most relevant information within it figure and nally we integrate the benets of language modeling using multi task training graph to sequence for example linearization to represent the graph as a quence for it is linearized into a tured standard form of subject node object node and predicate edge separated by indicator tokens as shown in figure the earization sub albert einstein obj the bel prize pred won would be created the earization is accomplished through graph sal in a manner following the directed edges formed by predicates and starting from the node with the largest weight as the root for two nodes that are connected by multiple cates the predicates are concatenated shown in figure so a linearization such as pred won s received would indicate that albert einstein both won and received the nobel prize figure steps of graph construction color relates the document sentence used to produce the graph output text to triples to graph graph construction proceeds in several steps outlined in figure we apply coreference resolution clark and ning and open information extraction stanovsky et al to convert sentences into a triple of the form subject predicate object the sentence albert einstein a german cal physicist won the nobel prize would become albert einstein won the nobel prize a graph is constructed using the triples by resenting subjects and objects as nodes connected by predicates as directed edges for example the the triple would become albert einstein won bel prize nodes and edges have a name property that is the text they represent they also have a weight property that denotes the number of times that node or edge has appeared for example in figure the node with name albert einstein has weight and edge with name won has weight merging nodes and edges when subsequent triples are added to the graph they are merged with the existing graph if they already exist to duce information replication to merge nodes the tf idf overlap of the new node s name is lated with the existing graph node names and the new node is merged into an existing node if the tf idf is higher than some threshold see ure steps and for example merge use the implementation available here github com huggingface neuralcoref use the implementation available here com supervised oie figure graph attribute embeddings in addition to word and position embeddings models receive a graph weight embedding to encode node and edge weight and a query relevance embedding that encodes search result rank mechanism for models to scale the graph dings we denote the embedding for position t as et such that eword is the word embedding t for the gw embedding models learn a gating function g based on the word and gw dings such a mechanism provides capacity for the model to decide when the additional embeddings are useful based on the words in the input the gate is calculated by applying an mlp w to the concatenation of the word and gw embeddings the learned gate is then applied to gw dings to create the output h ggw t w egw t ggw hgw eword t t egw t t models learn a gating mechanism for the qr embedding in a similar manner the full ding the model receives is as follows t epos eword t hgw t hqr t hierarchical attention one challenge in modeling long sequences is that attention mechanisms struggle to make sharp lections when softmax ing over long sequences fan et al when attention is blurry there lacks a strong distinction between noise and vant information we assume that graphs are constructed from query based web search input and thus leverage this query to learn a subselection operation using hierarchical top k attention depicted in figure the query is encoded with a transformer encoder and the linearized graph with another transformer encoder we model the interaction between the query and the input sequence e web search results or linearized graph by computing an tention distribution between the question and the input then selecting the top k positions with the most attention weight such a mechanism can be thought of as building a query dependent sentation of the most relevant knowledge which is commonly done in question answering tures like bidaf seo et al the top figure model architecture gray indicates standard transformer elements green indicates modication encoding graph information transformer models have two embeddings a word embedding and a position embedding simply linearizing the graph however loses attribute information such as node and edge weight instead we encode these attributes as embeddings in addition to the word and position embeddings to represent graph weight gw node and edge weight is provided as an embedding for each token the node weight and edge weight are equivalent to the number of merge operations for example if albert einstein occurred times in the text the gw embedding for the tokens albert and einstein would be as shown in figure we encode a query relevance qr embedding to represent the relevance of the web search to the query as ranked by the information retrieval tem e search engine information from the top web search results is likely more relevant than formation from the last web search results so viding this embedding allows the model to guish between these different information sources in figure tokens representing sentences from the rst document have qr embedding and kens from the second document have value models now have access to several different types of embeddings but all embedding mation contributes equally as there is no nism to distinguish between them we introduce a operation limits the number of tokens making the attention mechanism sharper scaling to encode the graph recent progress has improved the ability of language models to process longer sequences sukhbaatar et al dai et al but models remain limited in their capacity to code long documents the multi document sults of query based web search have hundreds of thousands of tokens beyond the limit of current models to handle for example the dataset provides an average of k tokens of web search input however by compressing the web search results into a knowledge graph we icantly reduce the number of tokens by an order of magnitude and make it possible for a model to access the entirety of the search information to represent the full graph models must scale to encode around k input tokens the attention mechanism in transformer architectures becomes computationally expensive for sequences of this length instead we experiment with the compressed attention mca mechanism posed for language models in liu et al and apply it to the encoder side of els at each self attention layer mca alternates between local attention computed between smaller chunks of tokens and strided tions to reduce the number of keys and values used in attention computation by adding the mca mechanism to the encoder e mca we are able to encode the complete linearized graph multi tasking with kb completion fan et al used multi task training on guage modeling and various tasks to corporate the benets of language modeling in models we extend this by training ditionally on knowledge graph completion els receive at training time sequences of a earized graph with nodes edges or both tively masked and must predict the missing tent words for example models might receive as input sub albert einstein obj mask pred won and need to predict the nobel prize this can be seen as both a multi word extension of the masked language model training proposed in devlin et al and as learning the task of liu et al the mechanism is termed dmca as it is applied on the decoder side knowledge base completion lacroix et al bordes et al at training time the tasks are distinguished using an indicator token in the input experimental setup we evaluate our approach on two datasets with multi document web input and multi sentence stractive output we use models that leverage a query concatenated with web search sults that have been processed into a supporting document e tf idf subselection linearized graph to generate long output datasets and evaluation first we experiment with the explain like i m five fan et al question ing dataset of k complex questions paired with multi sentence explanatory answers words on average to facilitate question answering the dataset provides the top web search hits from querying the question which results in k words on average see appendix for examples previous work fan et al used tf idf to nd sentences in the web search that have the largest overlap with the question and created a idf extraction of about words as input for instead we construct a local models knowledge graph for each question from the web search hits following the average length of the tf idf support document constructed in fan et al we experiment with modeling the rst n tokens of the linearized graph then scale to encode the entire graph using e mca wikisum second we experiment on the isum commoncrawl liu et al rization with million examples this task formulates wikipedia lead paragraph tion as a multi document summarization problem where the paragraph must be generated using the cited article references and other queried content from web search the query used is the title of the wikipedia article see appendix for examples previous work liu et al applied idf ranking to order the paragraphs of web search given a query models receive the ordered paragraphs ranked by tf idf as input liu et al model the rst n words of this re ranking and then n using com tree master wikisum mca we construct the knowledge graph for each wikipedia article from the rst k words of the ranked web search and experiment with encoding and tokens evaluation both tasks evaluate the sentence generation output against the gold output using rouge on wikisum we report only rouge l following liu et al we conduct a comparative human evaluation on the dataset we use crowdworkers to ine the responses of two models on different questions from the test set for each question evaluators are shown two answers and asked to choose the one they prefer to reduce variance answers are standardized at words each training and generation to reduce the vocabulary size of varied web ument content we apply byte pair encoding nrich et al to generate k codes for each dataset we implement our models in py ott et al using the transformer big architecture and training schedule described in vaswani et al detailed parameters are listed in the appendix for generation we use beam search with beam size and tune a minimum and maximum length on the validation set baselines we compare our results to the transformer quence models presented in fan et al for and liu et al for wikisum we evaluate three additional baseline models sentence selection with maximal marginal relevance fan et al used tf idf to identify relevant sentences in the web ments to form a support document of around words however recent work fabbri et al has shown that using maximal marginal relevance is an effective strategy for selecting relevant information while reducing redundancy we explore using mmr to lect sentences from the web text to nate to form a support document multi task triples to examine the impact of solely restructuring the input into open ie triples but not leveraging graph length of provided web input is around k words and maximum length is around k words model q d to a tf idf q d to a mmr multi task multi task triples multi task trip q d to a graph multi task graph attention e mca input length avg avg avg avg k rouge l table answer generation on using els receiving the question and a support document e tf idf selection triples linearized graph to produce the answer denotes results from fan et al model t d to p lm d mca t d to p multi task multi task graph attention e mca inputlen rouge l k k lm d mca k table lead paragraph generation on wikisum moncrawl using models receiving the title and a support document e tf idf ranking linearized graph to produce the lead paragraph denotes results from liu et al that use data scraped from unrestricted web search not the static commoncrawl version construction to reduce redundancy we periment with a triples only baseline the triples are concatenated to form the input multi task top triples as an ternative to using graph construction to press the full set of open ie triples we plore using tf idf overlap with the query to nd the most relevant information we select the top triples to concatenate as input results we examine the performance of our proposed proach and the choices made in graph construction and modeling we analyze the quality of the pression created by graph construction and the bustness and interpretability of this process linearized graph improves performance in table we compare our methods to various baselines on the dataset using mmr to lect the most relevant non redundant input is ilar to the tf idf baseline from fan et al the multi task triples baseline izes the input by forming triples but does not move redundant triples it produces marginally better results compared to the baseline multi task model sub selecting to the top triples is harmful as similar text has high tf idf lap with the query so redundant information is lected creating the graph structure brings an provement of around similar trends are seen for the wikisum dataset in table where graph construction improves the multi task model by these provements are statistically signicant at the condence level for both datasets a further improvement is seen by using the hierarchical attention mechanism to attend to only the most relevant information in the linearized graph input for it brings an ditional improvement and on isum a improvement by using mca to scale models to code the entire graph further gains can be seen particularly in information synthesis tasks prior work has shown the importance of reading more information liu et al achieved a point rouge improvement by reading k tokens in our setting e mca improves stead of our results around rouge on and rouge on wikisum we display random ations from both datasets in the appendix we use human evaluation to compare the task baseline to the multi task graph top tention model of evaluations prefer the multi task graph top k attention model we conduct a two tailed binomial test and nd this sult is statistically signicant with p analysis of modeling choices ablation on model components table quentially removes the graph embeddings the knowledge base completion multi tasking and the multi tasking from fan et al and reveals that each of these is important for performance graph attribute embeddings table plays the effect of removing the graph attribute embeddings and gating mechanism removing each is slightly harmful and the combination of all three together provide the best performance more web search documents figure right shows that graph construction with more web model iterative removal of model components multi task graph graph embeddings kb completion multi tasking lm multi tasking from fan et al removing graph embedding components graph gated graph weight query relevance no graph weight embedding no query relevance embedding no gating varying number of hits in graph multi task graph top k attention e mca with graph on search hits with graph on search hits with graph on search hits with graph on search hits varying k in hierarchical top k atttention multi task graph e mca top k top top top k top k table ablations on the validation set model q d to a q d to a shufe q d to a graph web shufe input tf idf web web shufe web shufe table importance of web search relevance on tion for modeling input words search information is important for answer token coverage the graph on the top search hit alone is missing of the answer tokens but this creases as more search hits are used table indicates that this lack of coverage of the answer tokens correlates with generation quality models receiving a graph built on the rst search hits alone are substantially worse than all hits top k attention table shows the effect of the top k hierarchical attention mechanism for various values of k attending to too many tokens lowers rouge for the task of writing proximately word answers attending to input tokens likely means the model is focusing on irrelevant information and tokens is too few figure interpretable attention of models on a subgraph when answering a question in figure left distribution of number of nodes middle number of edges right weight of the largest node in graph construction on the training set search results is only missing of tokens but has around k words when analyzing just the rst tokens to match the average length of the tf idf extraction the graph is better only ing of tokens further the merging and carding operations done during graph construction do not have a large effect on answer token age the full set of triples marginally reduces the percentage of answer tokens missing to instead of this indicates that much of the information in the full set triples is redundant and unnecessary for good answer token coverage graph representation is more robust to poor search relevance ordering we analyze the robustness of our approach to the ordering of web search results in table instead of constructing the graph from the rst web search result to the last we shufe the web search results and construct the graph on this shufed input we compare this to modeling the web search results directly no tf idf retrieval and a model that ceives this shufed web search input the graph is more robust to shufing as more information can be encoded in the graph due to its compression effect the search hit ordering is less critical figure left graph construction drastically reduces input size by an order of magnitude right graph construction encodes more tokens present in the answer compared to idf extraction and building the graph from more search hits increases answer token coverage analysis on for both plots graph improves answer token coverage despite compression figure displays the distribution of the number of nodes edges and the largest node weight for each local graph built on the dataset the web search results are compressed to a few hundred nodes by merging redundancy and ming irrelevant triples from the graph the input is reduced by an order of magnitude figure left despite compression the graph retains more answer tokens than tf idf subselection ure right displays the percentage of answer kens not present in the input the tf idf traction from fan et al is missing of tokens the graph constructed on all web interpretable attention on subgraphs figure shows an example of the nodes and edges the model focuses upon most when answering a question on to construct this tion we calculate the top nodes the model attends to and then their top edges the model tion on a sub portion of the linearized input can be visualized as an interpretable graph that sponds well to the model s generated answer for example the relationship general relativity is einstein s theory becomes the generated sentence general relativity is a theory of albert einstein conclusion many open domain nlp tasks rely upon document input from the web to facilitate tasks such as answering questions or writing summaries but current approaches struggle to encode the tirely of this information we propose ing one knowledge graph per query and show that this method compresses information and reduces redundancy we show on two abstractive tion tasks that using the linearized graph achieves better performance than tf idf retrieval references soren auer christian bizer georgi kobilarov jens lehmann richard cyganiak and zachary ives dbpedia a nucleus for a web of open data in the semantic web pages springer junwei bao nan duan ming zhou and tiejun zhao knowledge based question answering as in proceedings of the chine translation nual meeting of the association for computational linguistics volume long papers volume pages regina barzilay kathleen r mckeown and michael elhadad information fusion in the context of multi document summarization in proceedings of the annual meeting of the association for putational linguistics tal baumel matan eyal and michael elhadad query focused abstractive summarization rating query relevance multi document coverage and summary length constraints into els arxiv preprint daniel beck gholamreza haffari and trevor cohn graph to sequence learning using arxiv preprint gated graph neural networks taylor berg kirkpatrick dan gillick and dan klein jointly learning to extract and compress in proceedings of the annual meeting of the ciation for computational linguistics human guage technologies volume pages sociation for computational linguistics lidong bing piji li yi liao wai lam weiwei guo and rebecca j passonneau abstractive document summarization via phrase selection and merging arxiv preprint antoine bordes jason weston ronan collobert and yoshua bengio learning structured in twenty fifth aaai dings of knowledge bases conference on articial intelligence joan bruna wojciech zaremba arthur szlam and yann lecun spectral networks and cally connected networks on graphs arxiv preprint jurgen martin pahl o stahlhut and c e liedtke a knowledge based system for text dependent evaluation of remote sensing data in joint pattern recognition symposium pages springer ziqiang cao wenjie li sujian li and furu wei improving multi document summarization via text in thirty first aaai conference on classication articial intelligence danqi chen adam fisch jason weston and antoine bordes reading wikipedia to answer domain questions in acl danqi chen richard socher christopher d ning and andrew y ng learning new facts from knowledge bases with neural tensor works and semantic word vectors arxiv preprint christopher clark and matt gardner simple and effective multi paragraph reading sion arxiv preprint kevin clark and christopher d manning deep reinforcement learning for mention ranking ence models arxiv preprint kevin clark and christopher d manning proving coreference resolution by learning arxiv preprint level distributed representations wanyun cui yanghua xiao haixun wang yangqiu song seung won hwang and wei wang kbqa learning question answering over corpora and knowledge bases proceedings of the dowment zihang dai zhilin yang yiming yang william w cohen jaime carbonell quoc v le and ruslan salakhutdinov transformer xl attentive guage models beyond a xed length context arxiv preprint rajarshi das shehzaad dhuliawala manzil heer and andrew mccallum step retriever reader interaction for scalable domain question answering rajarshi das tsendsuren munkhdalai xingdi yuan adam trischler and andrew mccallum building dynamic knowledge graphs from text ing machine reading comprehension arxiv preprint hal daume iii and daniel marcu a channel model for document compression in ceedings of the annual meeting on association for computational linguistics pages sociation for computational linguistics jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language ing corr giuseppe di fabbrizio amanda stent and robert gaizauskas a hybrid approach to document summarization of opinions in reviews in proceedings of the international natural guage generation conference inlg pages alexander r fabbri irene li tianwei she suyi li a and dragomir r radev multi news large scale multi document summarization dataset and abstractive hierarchical model arxiv preprint angela fan yacine jernite ethan perez david ier jason weston and michael auli in proceedings of long form question answering acl angela fan mike lewis and yann dauphin erarchical neural story generation in acl kavita ganesan chengxiang zhai and jiawei han opinosis a graph based approach to tive summarization of highly redundant opinions in proceedings of the international conference on computational linguistics coling pages dan gillick and benoit favre a scalable global in proceedings of the model for summarization workshop on integer linear programming for ral langauge processing pages association for computational linguistics mohamad ali honarpisheh gholamreza sani and ghassem mirroshandel a document multi lingual automatic summarization in proceedings of the third international system joint conference on natural language processing volume ii mandar joshi eunsol choi daniel weld and luke zettlemoyer triviaqa a large scale distantly supervised challenge dataset for reading sion in acl thomas n kipf and max welling supervised classication with graph convolutional networks arxiv preprint timothee lacroix nicolas usunier and guillaume obozinski canonical tensor decomposition arxiv preprint for knowledge base completion yuxuan lai yansong feng xiaohan yu zheng wang kun xu and dongyan zhao lattice cnns for matching based chinese question answering arxiv preprint fei liu jeffrey flanigan sam thomson norman sadeh and noah a smith toward tive summarization using semantic representations arxiv preprint peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer generating wikipedia by rizing long sequences in iclr vincent ng and claire cardie improving chine learning approaches to coreference resolution in proceedings of the annual meeting on sociation for computational linguistics pages association for computational linguistics myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier and fairseq a fast extensible michael auli in proceedings of toolkit for sequence modeling naacl hlt demonstrations matthew e peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer deep contextualized word resentations in naacl alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever language models are unsupervised multitask learners marco rospocher marieke van erp piek vossen antske fokkens itziar aldabe german rigau aitor soroa thomas ploeger and tessel bogaard building event centric knowledge graphs from news journal of web semantics rico sennrich barry haddow and alexandra birch neural machine translation of rare words with subword units in acl minjoon seo aniruddha kembhavi ali farhadi and hannaneh hajishirzi bidirectional attention ow for machine comprehension in iclr gabriel stanovsky julian michael luke zettlemoyer and ido dagan supervised open information extraction in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long papers pages sainbayar sukhbaatar edouard grave piotr janowski and armand joulin adaptive tention span in transformers haitian sun bhuwan dhingra manzil zaheer kathryn mazaitis ruslan salakhutdinov and william w hen open domain question answering ing early fusion of knowledge bases and text arxiv preprint ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need nips piek vossen tommaso caselli and yiota zopoulou storylines for structuring massive streams of news in proceedings of the first shop on computing news storylines pages rachel wities vered shwartz gabriel stanovsky meni adler ori shapira shyam upadhyay dan roth eugenio martnez camara iryna gurevych and ido dagan a consolidated open edge representation for multiple texts in ings of the workshop on linking models of cal sentential and discourse level semantics pages kun xu lingfei wu zhiguo wang yansong feng michael witbrock and vadim sheinin graph to sequence learning with arxiv preprint attention based neural networks michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan and dragomir radev graph based neural multi document summarization arxiv preprint zhiping zheng question answering using web in proceedings of the news as knowledge base tenth conference on european chapter of the sociation for computational linguistics volume pages association for computational guistics jie zhou ganqu cui zhengyan zhang cheng yang zhiyuan liu and maosong sun graph ral networks a review of methods and applications arxiv preprint appendix dataset examples we display input and target examples for and wikisum in figure and figure respectively generation examples we display examples of model generations for and wikisum selected randomly in ure and figure similar to liu et al we observe that our models are also ble of generating full wikipedia pages we display randomly sampled examples of wikipedia article generation in figure implementation details training we train models using the former big architecture with the following eters dropout attention dropout an verse square root learning rate schedule with tial learning rate for warmup warmup updates and a minimum learning rate of we train with and small batchsize to scale to longer sequences to offset the batchsize crease we increase the update frequency of the gradient updates generation we generate using beam search of size we tune a minimum and maximum length at validation time and use minimum length and maximum length we use a n gram straint of n following fan et al question why consumers are still so terried of genetically modied organisms gmos yet there is little debate in the scientic community of whether they are safe or not scientists are for gmos beginning of web search the controversial safety of gmos and the skepticism of the use of gmos college paper writing service url diamond chemicals plc the merseyside appendix f research question for antisocial personality disorder affects family relations and interactions the controversial safety of gmos and the skepticism of the use of gmos gmo facts what is a gmo genetically modied organisms the safety of gmos is unknown poll skepticism of genetically modied foods abc news abc news network june web fernandez cornejo jorge and seth james wechsler the controversy over gmos particularly in food continues scientists are split pros and cons of gmo s september and environmentalists and consumer groups remain unconvinced about the safety of gmos the controversy around genetically modied foods on the surface food evolution is a resetting of the controversial conversation around genetically modied organisms gmos we just ask people by a show of hands to tell us are they concerned about gmos for their own safety or the when gmos are the movie star can documentaries on controversial science be entertaining and the message is that gmo food is safe to eat and that naysayers are genetically modied organisms gmos the top ve anti gmo tropes gmos are genetically modied organisms the evidence on gmo safety by ramez naam genetically modied organisms what are gmos with the use of gm technology pure and safe equivalents can be produced using gmos and industrial scale here s a bullet point summation of what nathanael johnson learned about gmos in gmo questions animal vegetable controversy by pretty darn safe the controversy surrounding genetically modied organisms what do we tolerate as far as detrimental this would be a profound service to scientic skepticism with regards to gmos current gmos are safe to however target answer there is little difference in essence between what is called gmo now and the techniques we have been using to domesticate and cultivate the food in the past its an arbitrary line that s been drawn in the sand and people fail to realize this often that being said i think it is more then wrong the patenting of crops and again even more then wrong to genetically modify crops to not have viable seeds so that seed washing ca nt be used to grow the next crop so the real god damned issues are being masked and lost by this retarded polemic between gmo and more conventional genetic modication of organisms figure example of dataset input output title dwight h perkins economist beginning of web search memorial minute adopted by the faculty of arts and sciences harvard university abram bergson john kenneth galbraith carl kaysen raymond vernon dwight h perkins chairman ed was a generous and understanding man who could see the good qualities in people and bring them out he was always determined and persistent which meant that he also saw to completion what he started the list of projects large and small that he led is long in he was one of the authors of the speech of secretary of state james byrnes in which the secretary announced the return of responsibility for the german economy to the germans in he together with ray vernon did a pioneering study of the new york metropolitan region a study that tried to identify the economic political and social forces that were shaping that vast urban area at the time the problems of urban areas were mainly the concern of architects and philosophers almost no economist and only a few political scientists knew much or cared greatly about such issues the studies that resulted provided rich fare for a generation of urban planners to follow mason edward s harvard square library home biographies mason edward s mason edward s edward sagendorph mason was a member of the rst parish unitarian church in harvard square who exemplied liberal religion in both thought and action in addition to his notable contributions to the science of government he served as chairman of the sloan commission on cable communication which issued recommendations for the future on the cable target lead paragraph dwight heald perkins born in chicago illinois in is an american academic economist sinologist and professor at harvard university he is the son of lawrence bradford perkins architect and margery blair perkins and the grandson of dwight heald perkins the architect for whom he was named he married julie rate perkins in and they have three adult children early life perkins earned an undergraduate degree at cornell university in after two years military service in the us navy perkins resumed his studies at harvard he earned a ma in economics in and a ph in economics in his doctoral thesis was price formation in communist china academic career perkins teaching career at harvard began when he was still a graduate student and continued uninterrupted through when he became a research and emeritus professor he was a member of both the department of economics of the faculty of arts and sciences at harvard and of the harvard kennedy school figure example of wikisum commoncrawl dataset input output question why is touching microber towels such an uncomfortable feeling true answer do you mean the kind of cloths used to clean glasses and lenses i ve never noticed any uncomfortable feeling myself but i do nd touching certain cleaning cloths can be quite uncomfortable there s a brand called e cloth which market themselves as not needing any cleaning supplies basically the threads have tiny little hooks and barbs in them so dragging them over skin makes you feel like it s catching against you generation the microber is made up of a bunch of tiny bers that are attached to them when you touch them the bers that make up the microber are attracted to each other when they touch they re attracted to the other end of the ber which is what makes them feel uncomfortable question what happened to needing www in front of an address true answer explaining like if you were you know when you send a package in the mail you write the address and maybe if the package is fragile you ll stamp fragile all over it why do nt you also stamp not fragile on the packages that are not fragile because it s implicit if you do nt say anything the mailman knows it s not fragile same thing goes for the www since you can have multiple services on your domain ex url which means you want to share les or url which means you want to access the mail server on that domain in the beginning you d also write url to state that you wanted to access the html of the server as the web evolved and of the average user wants to access the html version of the website website owners no longer require you to type url instead if you type url they assume you want to access the html version just like you do nt need to stamp non fragile on your package you do nt need to type www everyone assumes that s what you want if you say nothing otherwise generation the www is a subdomain name that is used to refer to a web page it s a name used to describe the web page that you re looking for for example if you want to go to url you can go to url and then go to url the www is the name of the site that you are looking for and it s the name that you were looking for in the rst place so if you wanted to go back to url you could go to url and go to url and the www would be the same as the www but it would be a different name for the site so if the www was the same you would be looking for a different site and you would nt be looking at the same site so you would have to go through the www if the www were the same and you were nt looking for the same page you d have to look for the different sites and so on but if you were to go from url you might have to use the www as a reference to the site but you d still have to have a www question why do we have nightmares when stressed out true answer i can tell you that your mind and in particular your dreams are subject to the input that it the journey of your life as you have interpreted it yours dreams tend to be your subconscious representing issues you have within whether it be past or present with things and people that are sort of preinterpreted symbology from your deep and ever power subconsciousness all your nightmares are just dreams and your inner mind knows exactly can mess with you the most and sometimes it brings that to light it usually being an issue you may be stressed about generation when you re stressed your brain releases a hormone called cortisol this hormone is released by your adrenal glands and is released when you are stressed this is why it s called ght mode when your body is stressed it releases hormones that make you feel more stressed the reason why you do nt feel stressed is because your brain is trying to get rid of the stress hormone which causes you to feel stressed and you re less likely to get stressed if you re really stressed you re probably going to be more stressed than if you were nt stressed figure random examples of question generated answer pairs in title wylie rogers true paragraph wylie rogers born march is an american ice hockey goaltender he played with the rio grande valley killer bees of the central hockey league prior to turning professional rogers attended the university of alaska fairbanks where he played four seasons of ncaa men s division i ice hockey with the alaska nanooks men s ice hockey team rogers played the season in the netherlands with the heerenveen yers generation wylie rogers is an american football coach and former player he is currently the head football coach at the university of north carolina at chapel hill rogers played college football at north carolina state university he was a member of the north carolina tar heels men s football team that won the ncaa division i football championship in he also played for the university s men s basketball team after his playing career ended rogers was hired as the head coach at unc chapel hill in he led the tar heels to their rst ncaa division ii championship in true paragraph mustabad is a village and mandal in siricilla rajanna district in telangana state of south east india musthabad is the main town for the mustabad mandal musthabad is located km distance from its district main city siricilla it is located km distance from its state main city hyderabad geography mustabad is located at n e generation mustabad is a village in the indian state of telangana it is located in the region of mustabad mandal mustabad was founded in the early by a group of local businessmen who wanted to create a village for the people of the region the village is named after the mustabad river which is a tributary of the larger river according to the census of india mustabad had a population of title mustabad title wolf valley true paragraph wolf valley is a graben rift valley in the elsinore trough in western riverside county california wolf valley is the southernmost of the graben valleys making up the elsinore trough created by the elsinore fault zone it lies between the wildomar fault on the east at the foot of the temecula basin and the willard fault on the west at the foot of the santa ana mountains wolf valley lies south of temecula creek the southern end of the graben valley is where the elsinore fault zone changes the direction of strike along the southern side of agua tibia mountain northeast of pala mountain the valley is drained by pechanga creek and its tributaries itself a tributary of the santa margarita river the pechanga indian reservation and part of the city of temecula is located in this valley head of wolf valley n w mouth of wolf valley n w generation wolf valley is a valley in the u s state of california it is located in the foothills of the sierra nevada the valley was named after the wolf river which ows through the valley wolf valley was the site of the battle of wolf valley which took place in the late and early centuries the battle was fought in the early century and the valley now holds the wolf valley war memorial figure random examples of lead paragraph generation in wikisum kokal kokal is a village in kollam district in the state of tamil nadu india demographics as of india census kokal had a population of transport kokal railway station is located at kokal junction the station is well connected to other parts of the state by road the nearest airport is at tiruchirappalli education the primary and secondary school education is imparted by government aided and private schools the medium of instruction followed by different schools are english telugu malayalam and other languages in popular culture in the lm the character of the protagonist of the lm is played by a character named kokal in the movie the protagonist is portrayed by a ramachandran who is also the protagonist in other media the village has a tv station called kokal kokal which is a part of the local tv station the village also has a radio station called kollam radio which is the local radio station of the same name the radio station is also called kollam fm and is known as the station s main radio station donald milne donald milne is a canadian politician he is a member of the legislative assembly of british columbia representing the electoral district of vancouver milne was born in vancouver british columbia he was elected to the british columbia house of commons in the provincial election defeating liberal candidate and former mayor of vancouver david campbell electoral record by election results future milne will be a candidate for vancouver city council in the provincial election personal life milne lives in vancouver with his wife and two children awards and honours milne was a recipient of the queen elizabeth ii diamond jubilee medal from the queen s birthday honours in he also received the order of the british empire in the birthday honours for services to british columbia and the community in british columbia in the birthday honours milne is also a fellow of the royal canadian academy of arts and sciences and the royal society of canada rbc and is a past president of the vancouver chapter of the rbc society of arts and sciences in milne was awarded an honorary doctor of laws degree by the university of vancouver in recognition of his service to the community and his contribution to the development of vancouver s arts and science as a leading gure in the vancouver art scene in he was awarded a lifetime achievement award by the vancouver arts council for his work on vancouver art and science as well as an honorary doctorate of laws causa from the royal college of art and sciences university of toronto in the academic year decimiana decimiana is a genus of owering plants in the legume family fabaceae species include decimiana acuminata muell benth ex benth ecuador peru bolivia brazil decimiana angustifolia benth species there are currently two species in this genus decimiana acutifolia and decimiana costata which are native to ecuador and peru but are now considered a separate species the following is a list of species of this genus in the world including decimiana borealis benth ecuador colombia decimiana cunninghamii benth ecuador bolivia some of these species are listed in the iucn red list of threatened species the others are listed as endangered species by the international union for conservation of nature iucn and are listed on the endangered species list by the united states department of agriculture usda a few of the species of a albiora are listed by the iucn as endangered or threatened species but they are now listed as critically endangered the others the other members of the family are listed under the iucn s red list but the other two are listed separately as endangered the other three are listed together as endangered and listed as vulnerable species figure random examples of full wikipedia generation in wikisum
