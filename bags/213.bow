strass a light and effective method for extractive summarization based on sentence embeddings leo bouscarrat antoine bonnefoy thomas peel cecile pereira eura nova marseille france leo bouscarrat antoine bonnefoy thomas peel cecile eu abstract this paper introduces strass tion by transformation selection and it is an extractive text summarization ing method which leverages the semantic mation in existing sentence embedding spaces our method creates an extractive summary by selecting the sentences with the closest dings to the document embedding the model learns a transformation of the document bedding to minimize the similarity between the extractive summary and the ground truth summary as the transformation is only posed of a dense layer the training can be done on cpu therefore inexpensive over inference time is short and linear cording to the number of sentences as a second contribution we introduce the french cass dataset composed of judgments from the french court of cassation and their sponding summaries on this dataset our sults show that our method performs similarly to the state of the art extractive methods with effective training and inferring time introduction summarization remains a eld of interest as numerous industries are faced with a growing amount of textual data that they need to process creating summary by hand is a costly and demanding task thus automatic methods to erate them are necessary there are two ways of summarizing a document abstractive and tive summarization in abstractive summarization the goal is to ate new textual elements to summarize the text summarization can be modeled as a to sequence problem for instance rush et al tried to generate a headline from an article however when the system generates longer maries redundancy can be a problem see et al introduce a pointer generator model pgn that generates summaries by copying words from the text or generating new words moreover they added a coverage loss as they noticed that other models made repetitions on long summaries even if it provides state of the art results the pgn is slow to learn and generate paulus et al added a layer of reinforcement learning on an encoder decoder architecture but their results can present uency issues in extractive summarization the goal is to tract part of the text to create a summary there are two standard ways to do that a sequence beling task where the goal is to select the tences labeled as being part of the summary and a ranking task where the most salient sentences are ranked rst it is hard to nd datasets for these tasks as most summaries written by humans are abstractive nallapati et al introduce a way to train an extractive summarization model without labels by applying a recurrent neural network rnn and using a greedy matching proach based on rouge recently narayan et al combined reinforcement learning to tract sentences and an encoder decoder ture to select the sentences some models combine extractive and tive summarization using an extractor to select sentences and then an abstractor to rewrite them chen and bansal cao et al hsu et al they are generally faster than models using only abstractors as they lter the input while maintaining or even improving the quality of the summaries this paper presents two main contributions first we propose an inexpensive scalable trainable and efcient method of extractive text summarization based on the use of sentence beddings our idea is that similar embeddings are semantically similar and so by looking at the l u j l c s c v v i x r a figure training of the model the blocks present steps of the analysis all the elements above the blocks are inputs document embedding sentences embeddings threshold real summary embedding trade off proximity of the embeddings it is possible to rank the sentences secondly we introduce the french cass dataset section composed of judgments with their corresponding summaries model related work in our model strass it is possible to use an embedding function trained with state of the art methods is a classical method used to form a word into a vector mikolov et al methods like keep information about semantics mikolov et al pagliardini et al create embedding of tences it has state of the art results on datasets for unsupervised sentence similarity evaluation embedrank bennani smires et al plies to extract keyphrases from a ment in an unsupervised fashion it hypothesizes that keyphrases that have an embedding close to the embedding of the entire document should resent this document well we adapt this idea to select sentences for maries section we suppose that sentences close to the document share some meaning with the document and are sentences that summarize well the text we go further by proposing a pervised method where we learn a transformation of the document embedding to an embedding of the same dimension but closer to sentences that summarize the text this paper embedding function embedding space and embedding will refer to the function that takes a textual element as input and outputs a vector the vector space and the vectors the aim is to construct an extractive summary our approach strass uses embeddings to lect a subset of sentences from a document we apply to the document to the tences of the document and to the summary we suppose that if we have a document with an d and a set s with all the embeddings of the sentences of the document and a reference summary with an embedding ref sum there is a subset of sentences es s forming the reference summary our target is to nd an afne function irn irn such that if s es t otherwise where t is a threshold and sim is a similarity function between two embeddings the training of the model is based on four main steps shown in figure transform the document embedding by applying an afne function learned by a ral network section extract a subset of sentences to form a summary section approximate the embedding of the tractive summary formed by the selected tences section are lowercased vectors embeddings are cased and in bold sets are uppercased and matrices are percased and in bold document embeddingsentences embeddingsneural network to learn a transformationreal summary of the sentences of the extracted summaryselectionscoringapproximationtransformationapproximation of the embedding of our extracted summaryscoring the quality of our summarytrade score the embedding of the resulting mary approximation with respect to the bedding of the real summary section to generate the summary only the rst two steps are used the selected sentences are the output approximation and scoring are only essary during the training phase when computing loss function transformation to learn an afne function in the embedding space the model uses a simple neural network a single fully connected feed forward layer irn irn w d with w the weight matrix of the hidden layer and b the bias vector optimization is only conducted on these two elements sentence extraction inspired by embedrank bennani smires et al our proposed approach is based on dings similarities instead of selecting the top n elements our approach uses a threshold all the sentences with a score above this threshold are lected as in pagliardini et al our larity score is the cosine similarity selection of sentences is the rst element d s t s t with sigmoid the sigmoid function and a normalized cosine similarity explained in section a sigmoid function is used instead of a hard threshold as all the functions need to be tiable to make the back propagation sel outputs a number between and indicates that a tence should be selected and that it should not with this function we select a subset of sentences from the text that forms our generated summary approximation as we want to compare the embedding of our erated extractive summary and the embedding of the reference summary the model approximates the embedding of the proposed summary as the system uses the approximation is the erage of the sentences weighted by the number of words in each sentence we have to apply this proximation to the sentences extracted with sel which compose our generated summary the proximation is s t s nb d s t ss where nb is the number of words in the tence corresponding to the embedding s scoring the quality of our generated summary is scored by comparing its embedding with the reference mary embedding here the compression ratio is added to the score in order to force the model to output shorter summaries the compression ratio is the number of words in the summary divided by the number of words in the document loss nb sum nb cos sum ref sum with a trade off between the similarity and the compression ratio cos y y irn the cosine similarity and gen sum s t the user should note that is also useful to change the trade off between the proximity of the maries and the length of the generated one a higher results in a shorter summary normalization to use a single selection threshold on all our uments a normalization is applied on the ities to have the same distribution for the ties on all the documents first we transform the cosine similarity from irn irn to irn irn y cos y then as in mori and sasaki the function is reduced and centered in y x y y xkx y where y is an embedding x is a set of dings x and are the mean and standard deviation a threshold is applied to select the closest tences on this normalized cosine similarity in der to always select at least one sentence we stricted our similarity measure in where for each document the closest sentence has a ilarity of y x y x y x max xkx experiments datasets to evaluate our approach two datasets were used with different intrinsic document and summary structures which are presented in this section more detailed information is available in the pendices table gure and gure we introduce a new dataset for text rization the cass this dataset is posed of judgments given by the french court of cassation between and and their summaries one summary by original ment those summaries are written by lawyers and explain in a short way the main points of the judgments as multiple lawyers have ten summaries there are different types of mary ranging from purely extractive to purely stractive this dataset is maintained up to date by the french government and new data are regularly added our version of the dataset is composed of judgements the cnn dailymail dataset hermann et al nallapati et al is composed of couples containing a news article and its highlights the highlights show the key points of an article we use the split created by nallapati et al and rened by see et al oracles we introduce two oracles even if these models do not output the best possible results for extractive summarization they show good results the rst model called oracle is the same as the baseline but instead of taking the document embedding the model takes the embedding of the summary and then extracts the closest sentences the second model called oraclesent extracts the closest sentence to each sentence of the mary this is an adaptation of the idea that ati et al and chen and bansal used to create their reference extractive summaries evaluation details rouge lin is a widely used set of metrics to evaluate summaries the three main metrics in this set are and which pare the grams and grams of the generated and reference summaries and rouge l which sures the longest sub sequence between the two summaries rouge is the standard measure for summarization especially because more cated ones like meteor denkowski and lavie require resources not available for many languages our results are compared with the unsupervised system textrank mihalcea and tarau rios et al and with the supervised systems pointer generator network see et al and rnn ext chen and bansal the generator network is an abstractive model and rnn ext is extractive for all datasets a embedding of mension was trained on the training split to choose the hyperparameters a grid search was computed on the validation set then the set of hyperparameters with the highest rouge l were used on the test set the selected hyperparameters are available in appendix a baseline results an unsupervised version of our approach is to use the document embedding as an approximation for the position in the embedding space used to select the sentences of the summary it is the application of embedrank bennani smires et al on the extractive summarization task this approach is used as a baseline for our model dataset is available here com euranova cass dataset tables and present the results for the cass and the cnn dailymail datasets as expected the pervised model performs better than the vised one on the three datasets the supervision has improved the score in terms of and rouge l in the same way our oracles are ways better than the learned models proving that there is still room for improvements information concerning the length of the generated summaries baseline textrank pgn rnn ext strass oracle oracle sent rl table results of different models on the french cass dataset using rouge with condence the models of the rst block are unsupervised the models of the second block are supervised and the models of the last block are the oracles is the f measure and rl stand for and rouge l baseline textrank pgn rnn ext strass oracle oracle sent pgn rl results of different models on the table cnn dailymail the pgn is the lead score as reported in see et al the scores with are taken from the corresponding publications is the f measure and rl stand for and rouge l figure processing time of the summarization tion y axis by the number of lines of the text as input axis results computed on an and the position of the sentences taken are able in the appendices a on the french cass dataset our method forms similarly to the rnn ext the pgn performs a bit better rouge l compared to the other models which could be linked to the fact that it can select elements smaller than sentences on the cnn dailymail dataset our supervised model performs poorly we observe a signicant difference and rouge l between the two oracles it could be explained by the fact that the summaries are multi topic and our models do not handle such case therefore as our loss does nt look at the diversity strass may miss some topics in the generated summary a second limitation of our approach is that our model does nt consider the position of the tences in the summary information which presents a high relevance in the cnn dailymail dataset strass has some advantages first it is able on cpu and thus light to train and run deed the neural network in our model is only posed of one dense layer the most recent vances in text summarization with neural networks are all based on deep neural networks requiring gpu to be learned efciently second the method is scalable the processing time is linear with the number of lines of the documents figure the model is fast at inference time as dings are fast to generate our model generated the summaries of the cass dataset in less than minutes on an cpu conclusion and perspectives to conclude we proposed here a simple effective and scalable extractive summarization method strass creates an extractive summary by selecting the sentences with the closest beddings to the projected document embedding the model learns a transformation of the ment embedding to maximize the similarity tween the extractive summary and the ground truth summary we showed that our approach obtains similar results than other extractive methods in an effective way there are several perspectives to our work first we would like to use the sentence dings as an input of our model as this should crease the accuracy additionally we want to vestigate the effect of using other ding spaces especially more generalist ones or other embedding functions like le and mikolov or bert devlin et al for now we have only worked on sentences but this model can use any embeddings so we could try to build summaries with smaller textual ments than sentences such as key phrases noun phrases likewise to apply our model on topic texts we could try to create clusters of tences where each cluster is a topic and then tract one sentence by cluster moreover currently the loss of the system is only composed of the proximity and the sion ratio other meaningful metrics for document summarization such as diversity and tivity could be added into the loss especially submodular functions could allow to obtain near optimal results and allow to include ments like diversity lin and bilmes other information we could add is the position of the sentences in the documents like narayan et al finally the approach could be extended to query based summarization v v muralikrishna et al one could use the embedding tion on the query and take the sentences that are the closest to the embedding of the query acknowledgement we thank cecile capponi carlos ramisch laume stempfel jakey blue and our anonymous reviewer for their helpful comments references federico barrios federico lopez luis argerich and rosa wachenchauzer variations of the larity function of textrank for automated tion corr kamil bennani smires claudiu musat andreea mann michael baeriswyl and martin jaggi simple unsupervised keyphrase extraction using in proceedings of the sentence embeddings conference on computational natural language learning pages association for tational linguistics ziqiang cao wenjie li sujian li and furu wei retrieve rerank and rewrite soft template based in proceedings of the neural summarization annual meeting of the association for tional linguistics volume long papers pages association for computational tics yen chun chen and mohit bansal fast tive summarization with reinforce selected sentence rewriting in proceedings of the annual ing of the association for computational linguistics volume long papers pages tion for computational linguistics michael denkowski and alon lavie meteor universal language specic translation evaluation for any target language in proceedings of the ninth workshop on statistical machine translation pages association for computational tics jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language in proceedings of the conference standing of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota ation for computational linguistics karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom teaching chines to read and comprehend in proceedings of the international conference on neural mation processing systems volume pages cambridge ma usa mit press wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun a unied model for extractive and abstractive summarization using inconsistency loss in proceedings of the annual meeting of the association for tional linguistics volume long papers pages association for computational tics quoc le and tomas mikolov distributed sentations of sentences and documents in ings of the international conference on chine learning volume of proceedings of chine learning research pages bejing china pmlr chin yew lin rouge a package for automatic in text summarization evaluation of summaries branches out proceedings of the shop pages barcelona spain association for computational linguistics hui lin and jeff bilmes a class of submodular functions for document summarization in ings of the annual meeting of the association for computational linguistics human language technologies volume hlt pages stroudsburg pa usa association for tional linguistics rada mihalcea and paul tarau textrank bringing order into text in proceedings of emnlp pages barcelona spain tion for computational linguistics linguistics volume long papers pages association for computational linguistics r v v muralikrishna s y pavan kumar and ch reddy a hybrid method for query based automatic summarization system tional journal of computer applications a appendices a datasets the composition of the datasets and the splits are available in table a preprocessing on the french cass dataset we have deleted all the accents of the texts and we have lower cased all the texts as some of them where entirely cased without any accent to create the maries all the ana parts of the xml les vided in the original dataset where taken and catenate to form a single summary for each ument these summaries explain different key points of the judgment on the cnn dailymail the preprocessing of see et al was used as an extra cleaning step we deleted the documents that had an empty story a hyperparameters to obtain the embeddings functions for both datasets we trained a model of dimension with unigrams on the train splits for the cass dataset the baseline model has a threshold at the oracle at and strass has a threshold at and a at textrank was used with a ratio of the pgn for the cnn dailymail dataset the baseline model has a threshold at the oracle at and strass has a threshold at and a at textrank was used with a ratio of a results a rouge score more detailed results are available in tables and high recall with low precision is generally onym of long summary tomas mikolov kai chen greg corrado and jeffrey dean efcient estimation of word tations in vector space corr tomas mikolov wen tau yih and geoffrey zweig linguistic regularities in continuous space in proceedings of the word representations conference of the north american chapter of the association for computational linguistics human language technologies pages atlanta georgia association for computational linguistics tatsunori mori and takuro sasaki tion gain ratio meets maximal marginal relevance a method of summarization for multiple documents in ntcir ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based sequence model for extractive summarization of documents corr ramesh nallapati bowen zhou cicero dos tos caglar gulcehre and bing xiang abstractive text summarization using sequence sequence rnns and beyond in proceedings of the signll conference on computational natural language learning pages berlin many association for computational linguistics shashi narayan shay b cohen and mirella lapata do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization conference on empirical methods in natural guage processing pages brussels gium association for computational linguistics shashi narayan shay b cohen and mirella lapata ranking sentences for extractive in rization with reinforcement learning ings of the conference of the north american chapter of the association for computational guistics human language technologies volume long papers pages association for computational linguistics matteo pagliardini prakhar gupta and martin jaggi unsupervised learning of sentence dings using compositional n gram features corr romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization corr alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of the tence summarization conference on empirical methods in natural guage processing pages association for computational linguistics abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proceedings of the nual meeting of the association for computational dataset cass cnn dailymail sd ss td ts train val test table size information for the datasets sd and ss are respectively the average number of sentences in the document and in the summary td and tt are respectively the number of tokens in the document and in the summary train val and test are respectively the number of documents in the train validation and test sets baseline textrank pgn rnn ext strass oracle oracle sent p r p r rl p rl r rl table full results of different models on the french cass dataset using rouge with condence the models in the rst part are unsupervised models then supervised models and the last part is the oracle p is precision r is recall and is the f measure and rl stand for and rouge l baseline textrank pgn rnn ext strass oracle oracle sent lead pgn p r p r rl p rl r rl table full results of different models on the cnn dailymail the pgn is the lead score as reported in see et al the scores with are taken from the corresponding publications p is precision r is recall and is the f measure and rl stand for and rouge l model reference strass oracle oracle sent s w w s size information for the generated summary on the test split of the cass dataset s w w s are respectively the average number of sentences the average number of words and the average number of words per sentences percentage of times that a sentence is taken in a erated summary in function of their position in the ment on the cass dataset density of the number of sentences in the generated summaries for several models and the reference on the cass dataset density of the number of words in the generated maries for several models and the reference on the cass dataset figure information about the length of the generated summaries for the cass dataset ten the fourth sentences than the rst three and still have better results than the which means that the fourth sentences could have some interest with strass the rst three sentences have a ferent tendency than the rest of the text showing that the rst three sentences may have a different structure than the rest then the farther a sentence is in the text the lower the probability to take it a words and sentences on the french cass dataset the summaries erated by the models are generally close in terms of length number of words number of sentences and number of words per sentences gure all the tested extractive methods tend to lect sentences at the beginning of the documents the rst sentence make an exception to that rule gure we observe that this sentence can have the list of the lawyers and judges that were present at the case strass tends to generate longer summaries with more sentences the discrepancy in the average number of sentences between the reference and oraclesent is due to sentences that are extracted multiple times on the cnn dailymail dataset strass tends to extract less sentences but longer ones ing to the oraclesent gure on the gure we can see that the three models tend to extract different sentences oraclesent which is the best performing model tends to tract the rst sentences oracle extracts more taken in summariesstrassoracleoracle in the resultsdensity of the number of sentencesstrassoracleoracle in the resultsdensity of the number of wordsstrassoracleoracle sentreference model reference strass oracle oracle sent s w w s size information for the generated summary on the test split of the cnn dm dataset s w w s are tively the average number of sentences the average ber of words and the average number of words per tences percentage of times that a sentence is taken in a erated summary in function of their position in the ment on the cnn dm dataset density of the number of sentences in the generated summaries for several models and the reference on the cnn dm dataset density of the number of words in the generated summaries for several models and the reference on the cnn dm dataset figure information about the length of the generated summaries for the cnn dm dataset taken in summariesstrassoracleoracle in the resultsdensity of the number of sentencesstrassoracleoracle in the resultsdensity of the number of wordsstrassoracleoracle sentreference
