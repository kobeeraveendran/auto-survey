strass light effective method extractive summarization based sentence embeddings leo bouscarrat antoine bonnefoy thomas peel cecile pereira eura nova marseille france leo bouscarrat antoine bonnefoy thomas peel cecile eu abstract paper introduces strass tion transformation selection extractive text summarization ing method leverages semantic mation existing sentence embedding spaces method creates extractive summary selecting sentences closest dings document embedding model learns transformation document bedding minimize similarity extractive summary ground truth summary transformation posed dense layer training cpu inexpensive inference time short linear cording number sentences second contribution introduce french cass dataset composed judgments french court cassation sponding summaries dataset sults method performs similarly state art extractive methods effective training inferring time introduction summarization remains eld interest numerous industries faced growing textual data need process creating summary hand costly demanding task automatic methods erate necessary ways summarizing document abstractive tive summarization abstractive summarization goal ate new textual elements summarize text summarization modeled sequence problem instance rush et al tried generate headline article system generates longer maries redundancy problem et al introduce pointer generator model pgn generates summaries copying words text generating new words added coverage loss noticed models repetitions long summaries provides state art results pgn slow learn generate paulus et al added layer reinforcement learning encoder decoder architecture results present uency issues extractive summarization goal tract text create summary standard ways sequence beling task goal select tences labeled summary ranking task salient sentences ranked rst hard nd datasets tasks summaries written humans abstractive nallapati et al introduce way train extractive summarization model labels applying recurrent neural network rnn greedy matching proach based rouge recently narayan et al combined reinforcement learning tract sentences encoder decoder ture select sentences models combine extractive tive summarization extractor select sentences abstractor rewrite chen bansal cao et al hsu et al generally faster models abstractors lter input maintaining improving quality summaries paper presents main contributions propose inexpensive scalable trainable efcient method extractive text summarization based use sentence beddings idea similar embeddings semantically similar looking l u j l c s c v v x r figure training model blocks present steps analysis elements blocks inputs document embedding sentences embeddings threshold real summary embedding trade proximity embeddings possible rank sentences secondly introduce french cass dataset section composed judgments corresponding summaries model related work model strass possible use embedding function trained state art methods classical method form word vector mikolov et al methods like information semantics mikolov et al pagliardini et al create embedding tences state art results datasets unsupervised sentence similarity evaluation embedrank bennani smires et al plies extract keyphrases ment unsupervised fashion hypothesizes keyphrases embedding close embedding entire document resent document adapt idea select sentences maries section suppose sentences close document share meaning document sentences summarize text proposing pervised method learn transformation document embedding embedding dimension closer sentences summarize text paper embedding function embedding space embedding refer function takes textual element input outputs vector vector space vectors aim construct extractive summary approach strass uses embeddings lect subset sentences document apply document tences document summary suppose document d set s embeddings sentences document reference summary embedding ref sum subset sentences es s forming reference summary target nd afne function irn irn s es t t threshold sim similarity function embeddings training model based main steps shown figure transform document embedding applying afne function learned ral network section extract subset sentences form summary section approximate embedding tractive summary formed selected tences section lowercased vectors embeddings cased bold sets uppercased matrices percased bold document embeddingsentences embeddingsneural network learn transformationreal summary sentences extracted summaryselectionscoringapproximationtransformationapproximation embedding extracted summaryscoring quality summarytrade score embedding resulting mary approximation respect bedding real summary section generate summary rst steps selected sentences output approximation scoring essary training phase computing loss function transformation learn afne function embedding space model uses simple neural network single fully connected feed forward layer irn irn w d w weight matrix hidden layer b bias vector optimization conducted elements sentence extraction inspired embedrank bennani smires et al proposed approach based dings similarities instead selecting n elements approach uses threshold sentences score threshold lected pagliardini et al larity score cosine similarity selection sentences rst element d s t s t sigmoid sigmoid function normalized cosine similarity explained section sigmoid function instead hard threshold functions need tiable propagation sel outputs number indicates tence selected function select subset sentences text forms generated summary approximation want compare embedding erated extractive summary embedding reference summary model approximates embedding proposed summary system uses approximation erage sentences weighted number words sentence apply proximation sentences extracted sel compose generated summary proximation s t s nb d s t ss nb number words tence corresponding embedding s scoring quality generated summary scored comparing embedding reference mary embedding compression ratio added score order force model output shorter summaries compression ratio number words summary divided number words document loss nb sum nb cos sum ref sum trade similarity compression ratio cos y y irn cosine similarity gen sum s t user note useful change trade proximity maries length generated higher results shorter summary normalization use single selection threshold uments normalization applied ities distribution ties documents transform cosine similarity irn irn irn irn y cos y mori sasaki function reduced centered y x y y xkx y y embedding x set dings x mean standard deviation threshold applied select closest tences normalized cosine similarity der select sentence stricted similarity measure document closest sentence ilarity y x y x y x max xkx experiments datasets evaluate approach datasets different intrinsic document summary structures presented section detailed information available pendices table gure gure introduce new dataset text rization cass dataset posed judgments given french court cassation summaries summary original ment summaries written lawyers explain short way main points judgments multiple lawyers summaries different types mary ranging purely extractive purely stractive dataset maintained date french government new data regularly added version dataset composed judgements cnn dailymail dataset hermann et al nallapati et al composed couples containing news article highlights highlights key points article use split created nallapati et al rened et al oracles introduce oracles models output best possible results extractive summarization good results rst model called oracle baseline instead taking document embedding model takes embedding summary extracts closest sentences second model called oraclesent extracts closest sentence sentence mary adaptation idea ati et al chen bansal create reference extractive summaries evaluation details rouge lin widely set metrics evaluate summaries main metrics set pare grams grams generated reference summaries rouge l sures longest sub sequence summaries rouge standard measure summarization especially cated ones like meteor denkowski lavie require resources available languages results compared unsupervised system textrank mihalcea tarau rios et al supervised systems pointer generator network et al rnn ext chen bansal generator network abstractive model rnn ext extractive datasets embedding mension trained training split choose hyperparameters grid search computed validation set set hyperparameters highest rouge l test set selected hyperparameters available appendix baseline results unsupervised version approach use document embedding approximation position embedding space select sentences summary application embedrank bennani smires et al extractive summarization task approach baseline model dataset available com euranova cass dataset tables present results cass cnn dailymail datasets expected pervised model performs better vised datasets supervision improved score terms rouge l way oracles ways better learned models proving room improvements information concerning length generated summaries baseline textrank pgn rnn ext strass oracle oracle sent rl table results different models french cass dataset rouge condence models rst block unsupervised models second block supervised models block oracles f measure rl stand rouge l baseline textrank pgn rnn ext strass oracle oracle sent pgn rl results different models table cnn dailymail pgn lead score reported et al scores taken corresponding publications f measure rl stand rouge l figure processing time summarization tion y axis number lines text input axis results computed position sentences taken able appendices french cass dataset method forms similarly rnn ext pgn performs bit better rouge l compared models linked fact select elements smaller sentences cnn dailymail dataset supervised model performs poorly observe signicant difference rouge l oracles explained fact summaries multi topic models handle case loss nt look diversity strass miss topics generated summary second limitation approach model nt consider position tences summary information presents high relevance cnn dailymail dataset strass advantages able cpu light train run deed neural network model posed dense layer recent vances text summarization neural networks based deep neural networks requiring gpu learned efciently second method scalable processing time linear number lines documents figure model fast inference time dings fast generate model generated summaries cass dataset minutes cpu conclusion perspectives conclude proposed simple effective scalable extractive summarization method strass creates extractive summary selecting sentences closest beddings projected document embedding model learns transformation ment embedding maximize similarity tween extractive summary ground truth summary showed approach obtains similar results extractive methods effective way perspectives work like use sentence dings input model crease accuracy additionally want vestigate effect ding spaces especially generalist ones embedding functions like le mikolov bert devlin et al worked sentences model use embeddings try build summaries smaller textual ments sentences key phrases noun phrases likewise apply model topic texts try create clusters tences cluster topic tract sentence cluster currently loss system composed proximity sion ratio meaningful metrics document summarization diversity tivity added loss especially submodular functions allow obtain near optimal results allow include ments like diversity lin bilmes information add position sentences documents like narayan et al finally approach extended query based summarization v v muralikrishna et al use embedding tion query sentences closest embedding query acknowledgement thank cecile capponi carlos ramisch laume stempfel jakey blue anonymous reviewer helpful comments references federico barrios federico lopez luis argerich rosa wachenchauzer variations larity function textrank automated tion corr kamil bennani smires claudiu musat andreea mann michael baeriswyl martin jaggi simple unsupervised keyphrase extraction proceedings sentence embeddings conference computational natural language learning pages association tational linguistics ziqiang cao wenjie li sujian li furu wei retrieve rerank rewrite soft template based proceedings neural summarization annual meeting association tional linguistics volume long papers pages association computational tics yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proceedings annual ing association computational linguistics volume long papers pages tion computational linguistics michael denkowski alon lavie meteor universal language specic translation evaluation target language proceedings ninth workshop statistical machine translation pages association computational tics jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language proceedings conference standing north american chapter association computational linguistics human language technologies volume long short papers pages minneapolis minnesota ation computational linguistics karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend proceedings international conference neural mation processing systems volume pages cambridge ma usa mit press wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun unied model extractive abstractive summarization inconsistency loss proceedings annual meeting association tional linguistics volume long papers pages association computational tics quoc le tomas mikolov distributed sentations sentences documents ings international conference chine learning volume proceedings chine learning research pages bejing china pmlr chin yew lin rouge package automatic text summarization evaluation summaries branches proceedings shop pages barcelona spain association computational linguistics hui lin jeff bilmes class submodular functions document summarization ings annual meeting association computational linguistics human language technologies volume hlt pages stroudsburg pa usa association tional linguistics rada mihalcea paul tarau textrank bringing order text proceedings emnlp pages barcelona spain tion computational linguistics linguistics volume long papers pages association computational linguistics r v v muralikrishna s y pavan kumar ch reddy hybrid method query based automatic summarization system tional journal computer applications appendices datasets composition datasets splits available table preprocessing french cass dataset deleted accents texts lower cased texts entirely cased accent create maries ana parts xml les vided original dataset taken catenate form single summary ument summaries explain different key points judgment cnn dailymail preprocessing et al extra cleaning step deleted documents story hyperparameters obtain embeddings functions datasets trained model dimension unigrams train splits cass dataset baseline model threshold oracle strass threshold textrank ratio pgn cnn dailymail dataset baseline model threshold oracle strass threshold textrank ratio results rouge score detailed results available tables high recall low precision generally onym long summary tomas mikolov kai chen greg corrado jeffrey dean efcient estimation word tations vector space corr tomas mikolov wen tau yih geoffrey zweig linguistic regularities continuous space proceedings word representations conference north american chapter association computational linguistics human language technologies pages atlanta georgia association computational linguistics tatsunori mori takuro sasaki tion gain ratio meets maximal marginal relevance method summarization multiple documents ntcir ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents corr ramesh nallapati bowen zhou cicero dos tos caglar gulcehre bing xiang abstractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning pages berlin association computational linguistics shashi narayan shay b cohen mirella lapata nt details summary topic aware convolutional neural networks proceedings treme summarization conference empirical methods natural guage processing pages brussels gium association computational linguistics shashi narayan shay b cohen mirella lapata ranking sentences extractive rization reinforcement learning ings conference north american chapter association computational guistics human language technologies volume long papers pages association computational linguistics matteo pagliardini prakhar gupta martin jaggi unsupervised learning sentence dings compositional n gram features corr romain paulus caiming xiong richard socher deep reinforced model abstractive marization corr alexander m rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages association computational linguistics abigail peter j liu christopher d manning point summarization generator networks proceedings nual meeting association computational dataset cass cnn dailymail sd ss td ts train val test table size information datasets sd ss respectively average number sentences document summary td tt respectively number tokens document summary train val test respectively number documents train validation test sets baseline textrank pgn rnn ext strass oracle oracle sent p r p r rl p rl r rl table results different models french cass dataset rouge condence models rst unsupervised models supervised models oracle p precision r recall f measure rl stand rouge l baseline textrank pgn rnn ext strass oracle oracle sent lead pgn p r p r rl p rl r rl table results different models cnn dailymail pgn lead score reported et al scores taken corresponding publications p precision r recall f measure rl stand rouge l model reference strass oracle oracle sent s w w s size information generated summary test split cass dataset s w w s respectively average number sentences average number words average number words sentences percentage times sentence taken erated summary function position ment cass dataset density number sentences generated summaries models reference cass dataset density number words generated maries models reference cass dataset figure information length generated summaries cass dataset fourth sentences rst better results means fourth sentences interest strass rst sentences ferent tendency rest text showing rst sentences different structure rest farther sentence text lower probability words sentences french cass dataset summaries erated models generally close terms length number words number sentences number words sentences gure tested extractive methods tend lect sentences beginning documents rst sentence exception rule gure observe sentence list lawyers judges present case strass tends generate longer summaries sentences discrepancy average number sentences reference oraclesent sentences extracted multiple times cnn dailymail dataset strass tends extract sentences longer ones ing oraclesent gure gure models tend extract different sentences oraclesent best performing model tends tract rst sentences oracle extracts taken summariesstrassoracleoracle resultsdensity number sentencesstrassoracleoracle resultsdensity number wordsstrassoracleoracle sentreference model reference strass oracle oracle sent s w w s size information generated summary test split cnn dm dataset s w w s tively average number sentences average ber words average number words tences percentage times sentence taken erated summary function position ment cnn dm dataset density number sentences generated summaries models reference cnn dm dataset density number words generated summaries models reference cnn dm dataset figure information length generated summaries cnn dm dataset taken summariesstrassoracleoracle resultsdensity number sentencesstrassoracleoracle resultsdensity number wordsstrassoracleoracle sentreference
