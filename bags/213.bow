strass light effective method extractive summarization based sentence embeddings leo bouscarrat antoine bonnefoy thomas peel cecile pereira eura nova marseille france leo bouscarrat antoine bonnefoy thomas peel cecile abstract paper introduces strass tion transformation selection extractive text summarization ing method leverages semantic mation existing sentence embedding spaces method creates extractive summary selecting sentences closest dings document embedding model learns transformation document bedding minimize similarity extractive summary ground truth summary transformation posed dense layer training cpu inexpensive inference time short linear cording number sentences second contribution introduce french cass dataset composed judgments french court cassation sponding summaries dataset sults method performs similarly state art extractive methods effective training inferring time introduction summarization remains eld interest numerous industries faced growing textual data need process creating summary hand costly demanding task automatic methods erate necessary ways summarizing document abstractive tive summarization abstractive summarization goal ate new textual elements summarize text summarization modeled sequence problem instance rush tried generate headline article system generates longer maries redundancy problem introduce pointer generator model pgn generates summaries copying words text generating new words added coverage loss noticed models repetitions long summaries provides state art results pgn slow learn generate paulus added layer reinforcement learning encoder decoder architecture results present uency issues extractive summarization goal tract text create summary standard ways sequence beling task goal select tences labeled summary ranking task salient sentences ranked rst hard datasets tasks summaries written humans abstractive nallapati introduce way train extractive summarization model labels applying recurrent neural network rnn greedy matching proach based rouge recently narayan combined reinforcement learning tract sentences encoder decoder ture select sentences models combine extractive tive summarization extractor select sentences abstractor rewrite chen bansal cao hsu generally faster models abstractors lter input maintaining improving quality summaries paper presents main contributions propose inexpensive scalable trainable efcient method extractive text summarization based use sentence beddings idea similar embeddings semantically similar looking figure training model blocks present steps analysis elements blocks inputs document embedding sentences embeddings threshold real summary embedding trade proximity embeddings possible rank sentences secondly introduce french cass dataset section composed judgments corresponding summaries model related work model strass possible use embedding function trained state art methods classical method form word vector mikolov methods like information semantics mikolov pagliardini create embedding tences state art results datasets unsupervised sentence similarity evaluation embedrank bennani smires plies extract keyphrases ment unsupervised fashion hypothesizes keyphrases embedding close embedding entire document resent document adapt idea select sentences maries section suppose sentences close document share meaning document sentences summarize text proposing pervised method learn transformation document embedding embedding dimension closer sentences summarize text paper embedding function embedding space embedding refer function takes textual element input outputs vector vector space vectors aim construct extractive summary approach strass uses embeddings lect subset sentences document apply document tences document summary suppose document set embeddings sentences document reference summary embedding ref sum subset sentences forming reference summary target afne function irn irn threshold sim similarity function embeddings training model based main steps shown figure transform document embedding applying afne function learned ral network section extract subset sentences form summary section approximate embedding tractive summary formed selected tences section lowercased vectors embeddings cased bold sets uppercased matrices percased bold document embeddingsentences embeddingsneural network learn transformationreal summary sentences extracted summaryselectionscoringapproximationtransformationapproximation embedding extracted summaryscoring quality summarytrade score embedding resulting mary approximation respect bedding real summary section generate summary rst steps selected sentences output approximation scoring essary training phase computing loss function transformation learn afne function embedding space model uses simple neural network single fully connected feed forward layer irn irn weight matrix hidden layer bias vector optimization conducted elements sentence extraction inspired embedrank bennani smires proposed approach based dings similarities instead selecting elements approach uses threshold sentences score threshold lected pagliardini larity score cosine similarity selection sentences rst element sigmoid sigmoid function normalized cosine similarity explained section sigmoid function instead hard threshold functions need tiable propagation sel outputs number indicates tence selected function select subset sentences text forms generated summary approximation want compare embedding erated extractive summary embedding reference summary model approximates embedding proposed summary system uses approximation erage sentences weighted number words sentence apply proximation sentences extracted sel compose generated summary proximation number words tence corresponding embedding scoring quality generated summary scored comparing embedding reference mary embedding compression ratio added score order force model output shorter summaries compression ratio number words summary divided number words document loss sum cos sum ref sum trade similarity compression ratio cos irn cosine similarity gen sum user note useful change trade proximity maries length generated higher results shorter summary normalization use single selection threshold uments normalization applied ities distribution ties documents transform cosine similarity irn irn irn irn cos mori sasaki function reduced centered xkx embedding set dings mean standard deviation threshold applied select closest tences normalized cosine similarity der select sentence stricted similarity measure document closest sentence ilarity max xkx experiments datasets evaluate approach datasets different intrinsic document summary structures presented section detailed information available pendices table gure gure introduce new dataset text rization cass dataset posed judgments given french court cassation summaries summary original ment summaries written lawyers explain short way main points judgments multiple lawyers summaries different types mary ranging purely extractive purely stractive dataset maintained date french government new data regularly added version dataset composed judgements cnn dailymail dataset hermann nallapati composed couples containing news article highlights highlights key points article use split created nallapati rened oracles introduce oracles models output best possible results extractive summarization good results rst model called oracle baseline instead taking document embedding model takes embedding summary extracts closest sentences second model called oraclesent extracts closest sentence sentence mary adaptation idea ati chen bansal create reference extractive summaries evaluation details rouge lin widely set metrics evaluate summaries main metrics set pare grams grams generated reference summaries rouge sures longest sub sequence summaries rouge standard measure summarization especially cated ones like meteor denkowski lavie require resources available languages results compared unsupervised system textrank mihalcea tarau rios supervised systems pointer generator network rnn ext chen bansal generator network abstractive model rnn ext extractive datasets embedding mension trained training split choose hyperparameters grid search computed validation set set hyperparameters highest rouge test set selected hyperparameters available appendix baseline results unsupervised version approach use document embedding approximation position embedding space select sentences summary application embedrank bennani smires extractive summarization task approach baseline model dataset available com euranova cass dataset tables present results cass cnn dailymail datasets expected pervised model performs better vised datasets supervision improved score terms rouge way oracles ways better learned models proving room improvements information concerning length generated summaries baseline textrank pgn rnn ext strass oracle oracle sent table results different models french cass dataset rouge condence models rst block unsupervised models second block supervised models block oracles measure stand rouge baseline textrank pgn rnn ext strass oracle oracle sent pgn results different models table cnn dailymail pgn lead score reported scores taken corresponding publications measure stand rouge figure processing time summarization tion axis number lines text input axis results computed position sentences taken able appendices french cass dataset method forms similarly rnn ext pgn performs bit better rouge compared models linked fact select elements smaller sentences cnn dailymail dataset supervised model performs poorly observe signicant difference rouge oracles explained fact summaries multi topic models handle case loss look diversity strass miss topics generated summary second limitation approach model consider position tences summary information presents high relevance cnn dailymail dataset strass advantages able cpu light train run deed neural network model posed dense layer recent vances text summarization neural networks based deep neural networks requiring gpu learned efciently second method scalable processing time linear number lines documents figure model fast inference time dings fast generate model generated summaries cass dataset minutes cpu conclusion perspectives conclude proposed simple effective scalable extractive summarization method strass creates extractive summary selecting sentences closest beddings projected document embedding model learns transformation ment embedding maximize similarity tween extractive summary ground truth summary showed approach obtains similar results extractive methods effective way perspectives work like use sentence dings input model crease accuracy additionally want vestigate effect ding spaces especially generalist ones embedding functions like mikolov bert devlin worked sentences model use embeddings try build summaries smaller textual ments sentences key phrases noun phrases likewise apply model topic texts try create clusters tences cluster topic tract sentence cluster currently loss system composed proximity sion ratio meaningful metrics document summarization diversity tivity added loss especially submodular functions allow obtain near optimal results allow include ments like diversity lin bilmes information add position sentences documents like narayan finally approach extended query based summarization muralikrishna use embedding tion query sentences closest embedding query acknowledgement thank cecile capponi carlos ramisch laume stempfel jakey blue anonymous reviewer helpful comments references federico barrios federico lopez luis argerich rosa wachenchauzer variations larity function textrank automated tion corr kamil bennani smires claudiu musat andreea mann michael baeriswyl martin jaggi simple unsupervised keyphrase extraction proceedings sentence embeddings conference computational natural language learning pages association tational linguistics ziqiang cao wenjie sujian furu wei retrieve rerank rewrite soft template based proceedings neural summarization annual meeting association tional linguistics volume long papers pages association computational tics yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proceedings annual ing association computational linguistics volume long papers pages tion computational linguistics michael denkowski alon lavie meteor universal language specic translation evaluation target language proceedings ninth workshop statistical machine translation pages association computational tics jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language proceedings conference standing north american chapter association computational linguistics human language technologies volume long short papers pages minneapolis minnesota ation computational linguistics karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend proceedings international conference neural mation processing systems volume pages cambridge usa mit press wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun unied model extractive abstractive summarization inconsistency loss proceedings annual meeting association tional linguistics volume long papers pages association computational tics quoc tomas mikolov distributed sentations sentences documents ings international conference chine learning volume proceedings chine learning research pages bejing china pmlr chin yew lin rouge package automatic text summarization evaluation summaries branches proceedings shop pages barcelona spain association computational linguistics hui lin jeff bilmes class submodular functions document summarization ings annual meeting association computational linguistics human language technologies volume hlt pages stroudsburg usa association tional linguistics rada mihalcea paul tarau textrank bringing order text proceedings emnlp pages barcelona spain tion computational linguistics linguistics volume long papers pages association computational linguistics muralikrishna pavan kumar reddy hybrid method query based automatic summarization system tional journal computer applications appendices datasets composition datasets splits available table preprocessing french cass dataset deleted accents texts lower cased texts entirely cased accent create maries ana parts xml les vided original dataset taken catenate form single summary ument summaries explain different key points judgment cnn dailymail preprocessing extra cleaning step deleted documents story hyperparameters obtain embeddings functions datasets trained model dimension unigrams train splits cass dataset baseline model threshold oracle strass threshold textrank ratio pgn cnn dailymail dataset baseline model threshold oracle strass threshold textrank ratio results rouge score detailed results available tables high recall low precision generally onym long summary tomas mikolov kai chen greg corrado jeffrey dean efcient estimation word tations vector space corr tomas mikolov wen tau yih geoffrey zweig linguistic regularities continuous space proceedings word representations conference north american chapter association computational linguistics human language technologies pages atlanta georgia association computational linguistics tatsunori mori takuro sasaki tion gain ratio meets maximal marginal relevance method summarization multiple documents ntcir ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents corr ramesh nallapati bowen zhou cicero dos tos caglar gulcehre bing xiang abstractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning pages berlin association computational linguistics shashi narayan shay cohen mirella lapata details summary topic aware convolutional neural networks proceedings treme summarization conference empirical methods natural guage processing pages brussels gium association computational linguistics shashi narayan shay cohen mirella lapata ranking sentences extractive rization reinforcement learning ings conference north american chapter association computational guistics human language technologies volume long papers pages association computational linguistics matteo pagliardini prakhar gupta martin jaggi unsupervised learning sentence dings compositional gram features corr romain paulus caiming xiong richard socher deep reinforced model abstractive marization corr alexander rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages association computational linguistics abigail peter liu christopher manning point summarization generator networks proceedings nual meeting association computational dataset cass cnn dailymail train val test table size information datasets respectively average number sentences document summary respectively number tokens document summary train val test respectively number documents train validation test sets baseline textrank pgn rnn ext strass oracle oracle sent table results different models french cass dataset rouge condence models rst unsupervised models supervised models oracle precision recall measure stand rouge baseline textrank pgn rnn ext strass oracle oracle sent lead pgn table results different models cnn dailymail pgn lead score reported scores taken corresponding publications precision recall measure stand rouge model reference strass oracle oracle sent size information generated summary test split cass dataset respectively average number sentences average number words average number words sentences percentage times sentence taken erated summary function position ment cass dataset density number sentences generated summaries models reference cass dataset density number words generated maries models reference cass dataset figure information length generated summaries cass dataset fourth sentences rst better results means fourth sentences interest strass rst sentences ferent tendency rest text showing rst sentences different structure rest farther sentence text lower probability words sentences french cass dataset summaries erated models generally close terms length number words number sentences number words sentences gure tested extractive methods tend lect sentences beginning documents rst sentence exception rule gure observe sentence list lawyers judges present case strass tends generate longer summaries sentences discrepancy average number sentences reference oraclesent sentences extracted multiple times cnn dailymail dataset strass tends extract sentences longer ones ing oraclesent gure gure models tend extract different sentences oraclesent best performing model tends tract rst sentences oracle extracts taken summariesstrassoracleoracle resultsdensity number sentencesstrassoracleoracle resultsdensity number wordsstrassoracleoracle sentreference model reference strass oracle oracle sent size information generated summary test split cnn dataset tively average number sentences average ber words average number words tences percentage times sentence taken erated summary function position ment cnn dataset density number sentences generated summaries models reference cnn dataset density number words generated summaries models reference cnn dataset figure information length generated summaries cnn dataset taken summariesstrassoracleoracle resultsdensity number sentencesstrassoracleoracle resultsdensity number wordsstrassoracleoracle sentreference
