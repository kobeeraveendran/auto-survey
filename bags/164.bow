semantic sentence embeddings paraphrasing text summarization chi zhang shagan sah thang nguyen dheeraj peri alexander loui carl salvaggio raymond ptucha rochester institute technology rochester usa kodak alaris imaging science rochester usa abstract paper introduces sentence vector encoding work suitable advanced natural language processing latent representation shown encode sentences mon semantic information similar vector tions vector representation extracted decoder model trained sentence paraphrase pairs demonstrate application sentence tations different tasks sentence paraphrasing paragraph summarization making attractive commonly recurrent frameworks process text experimental sults help gain insight vector representations suitable advanced language embedding index terms sentence embedding sentence encoding sentence paraphrasing text summarization deep learning introduction modeling temporal sequences patterns requires ding pattern vector space example passing frame video convolutional ral network cnn sequence vectors obtained vectors fed recurrent neural network rnn form powerful descriptor video annotation similar techniques glove form vector representations words embeddings sentences sequence word tors vector sequences fed rnn powerful descriptor sentence given vector representations sentence video mapping vector spaces solved ing connection visual textual spaces ables tasks captioning summarizing searching images video intuitive humans vectorizing paragraphs similar methods richer textual descriptions recent advances vectorizing sentences represent exact sentences faithfully pair current sentence prior sentence like glove map words similar meaning close desire method map sentences similar meaning close example toy sentences man jumped stream person hurdled creek similar meaning humans close traditional sentence vector representations like words ower rose tulip close good sentence vector representations toy sentences lie close introduced embedded tor space inspired meteor captioning benchmark allows substitution similar words choose map similar sentences close possible utilize phrase datasets ground truth captions multi human captioning datasets example coco dataset images captions different evaluators average captions image convey semantic ing present encoder decoder framework sentence paraphrases generate vector representation tences framework maps sentences similar semantic meaning nearby vector encoding space main contributions paper usage sentences widely available image video captioning datasets form sentence paraphrase pairs pairs train encoder decoder model application sentence embeddings graph summarization sentence paraphrasing evaluations performed metrics vector tions qualitative human evaluation extend vectorized sentence approach hierarchical architecture enabling encoding complex structures paragraphs applications text summarization rest paper organized follows section reviews related techniques section presents proposed encoder decoder framework sentence graph paraphrasing section discusses experimental sults concluding remarks presented section related work machine learning algorithms require inputs sented xed length feature vectors challenging task inputs text sentences paragraphs studies addressed problem supervised unsupervised approaches example presented tence vector representation created paragraph tor representation application representations shown individual sentence embeddings paragraph search relevant video segments alternate approach uses encoder decoder framework rst encodes inputs time rst layer layer long short term memory lstm variable length approach shown video captioning tasks encodes entire video decodes word time numerous recent works generating long tual paragraph summaries videos example present hierarchical recurrent network comprise paragraph generator built sentence sentence generator encodes sentences compact tations paragraph generator captures inter sentence performed similar narratives long dependencies videos combining sentences connective words propriate transitions learned unsupervised learning methodology vector representation sentence extracted encoder decoder model sentence paraphrasing tested text summarizer vector representation sentences consider sentence paraphrasing framework encoder decoder model shown fig given tence encoder maps sentence vector vector fed decoder produce phrase sentence represent paraphrase sentence pairs let denote word embedding sentence denote word embedding sentence length paraphrase sentences choices encoder explored including lstm gru rhn model use rnn encoder lstm cells easy plemented performs model specically words converted token ids embedded glove encode sentence embedded words iteratively processed lstm cell decoder neural language model conditions encoder output computation similar encoder vector htx encodes input sentence vector known vector representation input sentence paper note attention encoder decoder ensures information extracted input sentence encoder goes words attention adopted order avoid information leakage fig sentence paraphrasing model red blue cells represent encoder decoder respectively mediate vector black vector encoded sentence softmax training training example word level need compute logits classes expensive universe classes pending size vocabulary large given dicted sentence ground truth sentence use sampled softmax candidate sampling algorithm training sample pick small set sampled classes cording chosen sampling function set candidates created containing union target class sampled classes training task gures given set classes target class hierarchical encoder text summarization sentence paragraph represented tor method described section tors fed hierarchical encoder summarized text generated word word rnn decoder shown fig rst divide vectors chunks stride denotes number temporal units jacent chunks apart chunk feature vector extracted lstm layer fed second layer feature vector gives proper abstract responding chunk use lstm units second layer build hierarchical encoder rst lstm layer serves lter explore local temporal structure subsequences second lstm learns temporal dependencies subsequences result feature vector generated second layer called summarizes input vectors tracted entire paragraph finally rnn decoder converts word sequence forming summarized sentence integrate soft attention mechanism archical encoder attention mechanism allows lstm pay attention different temporal locations input sequence input sequence output sequence strictly aligned attention especially helpful start eos levels sequence training details sentence paraphrasing trained model described section visual caption datasets word bedding initialized glove number units layer encoder decoder empirically set generate sets vocabularies size stochastic gradient descent employed timization initial learning rate decay factor set respectively paragraph summarization task summarize tailed description single sentence tacos multi level corpus select detailed descriptions tences total samples training testing employed erarchical architecture described section stride feature vectors short paragraphs zero padded fed model vector sentence tation extracted paraphrasing model model robust soft attention layer training use learning rate adam mizer lstm cells set units sentence generation layer set units sentence paraphrasing given reference sentence objective produce mantically related sentence paraphrasing model trained visual caption datasets evaluated sick dataset tuning results shown table evaluation metrics experiment son spearman mean squared use setup calculation metrics table test set results sick semantic relatedness task denote number hidden units denote size vocabulary son spearman metric respectively mse order visualize performance method applied pca vector representation fig visualizes paraphrase sentence pairs sick dataset representations sensitive semantic information sentences pairwise sentences close example point close watching looking semantically related semantic relatedness grammar correctness veried human generated scores score erage different human annotators scores values fig paragraph summarizer red blue cells represent encoder decoder respectively encoder puts vector representation generated sentences paragraph decoder outputs words summarized text intermediate vector black vector encoded paragraph experimental results datasets visual caption datasets numerous datasets multiple captions images videos example vtt dataset comprised videos tences describing videos sentences phrases sentences describing visual input form pairs sentences create input target samples likewise msvd coco table lists statistics datasets captions datasets held test set total created training samples table sentence pairs statistics captioning datasets msvd msrvtt mscoco flickr sent sent samp sent pairs sick dataset use sentences involving sitional knowledge sick dataset test set tence paraphrasing task consists english tence pairs annotated relatedness means crowd sourcing techniques sentence relatedness score point rating scale sentence pair provided meant quantify degree semantic relatedness tween sentences tacos multi level corpus extract training pairs paragraph summarization task tacos multi level corpus dataset provides coherent multi sentence descriptions complex videos featuring cooking activities levels detail detailed short single tence description training test video sequences annotations description fig sne visualizations single sentence descriptions subset sequences tacos multi level corpus skip thoughts skip gram points colored based sequence ids different annotations sequence best viewed color young boys playing outdoors man smiling nearby group kids playing yard old man standing background brown dog attacking animal man pants brown dog helping animal man pants people kickboxing spectators watching people ghting spectators watching kids red shirts playing leaves kids jumping leaves little girl looking woman costume little girl looking man costume woman removing peel potato woman peeling potato children standing wooden hut kids standing close kid gun training captions styles ics sentences dataset limited approach forming sentence paraphrasing pairs senting sentences vectors valid table evaluation short single sentence summarization tacos multi level corpus vectors skip thoughts skip gram respectively skip gram skip thoughts fig paraphrase sentence pairs represented projected space pca point represents sentence sick dataset responding sentence shown right score indicates sentence pair related totally incorrect syntax score indicates highly related grammatically correct sentences human evaluation come visual caption sick test sets human evaluated scores sentence pairs inversely proportional euclidean distance vector representation corresponding sentences meteor rouge cider fig shows sne vector representation skip thoughts skip gram domly selected test sequences plots point represents single sentence points describing video sequence clustered points color nicely grouped visualization text summarization conclusion addition paraphrasing useful text summarization use tacos level corpus sentences detailed descriptions video sequence rst converted vectors model vectors fed summarizer described section performance summarized text evaluated based metric scores compared thoughts skip gram note skip gram frequency based average word sentence shown table scores generated model close comparable benchmark thoughts result reasonable dataset showed use deep lstm based model quence learning problem encode sentences common semantic information similar vector representations presented latent representation sentences shown useful sentence paraphrasing document tion believe reversing encoder sentences helped model learn long dependencies long sentences advantages simple straightforward sentation applicability variety tasks research area lead higher quality vector resentations challenging sequence learning tasks references subhashini venugopalan translating videos natural language deep recurrent neural networks yao describing videos exploiting temporal structure iccv andrew shin caption narrative video icip ieee captioning multiple sentences junyoung chung empirical evaluation gated recurrent neural networks sequence modeling arxiv preprint chi zhang batch normalized recurrent highway subhashini venugopalan sequence sequence networks icip ieee video text iccv tomas mikolov distributed representations words phrases compositionality nips jeffrey pennington richard socher christopher manning glove global vectors word tion emnlp kyunghyun cho properties neural chine translation encoder decoder approaches arxiv preprint quoc tomas mikolov distributed tations sentences documents icml vol nal kalchbrenner edward grefenstette phil som convolutional neural network modelling sentences arxiv preprint han zhao zhengdong pascal poupart adaptive hierarchical sentence model arxiv preprint ryan kiros skip thought vectors nips satanjeev banerjee alon lavie meteor tomatic metric evaluation improved tion human judgments proceedings acl workshop intrinsic extrinsic evaluation measures machine translation summarization tsung lin microsoft coco common objects context eccv jinsoo choi textually customized video maries arxiv preprint sutskever sequence sequence learning neural networks nips subhashini venugopalan sequence video text iccv haonan video paragraph captioning hierarchical recurrent neural networks cvpr sebastien jean large target cabulary neural machine translation arxiv preprint pingbo pan hierarchical recurrent neural coder video representation application tioning cvpr dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly learning align translate arxiv preprint jun msr vtt large video description dataset bridging video language cvpr david chen william dolan collecting highly parallel data paraphrase evaluation proceedings association computational linguistics human language technologies volume acl tsung lin microsoft coco common objects context european conference computer sion springer peter young image descriptions visual denotations new similarity metrics semantic ence event descriptions transactions ciation computational linguistics marco marelli sick cure evaluation compositional distributional semantic models lrec anna rohrbach coherent multi sentence video description variable level detail german conference pattern recognition christopher manning kai sheng tai richard socher improved structured long short term memory networks association computational linguistics representations semantic laurens van der maaten geoffrey hinton alizing data sne journal machine learning research vol nov
