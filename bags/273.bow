abstractive summarization combination pre trained sequence sequence saliency models itsumi saito kyosuke nishida kosuke nishida junji tomita ntt media intelligence laboratory ntt corporation itsumi saito ntt abstract pre trained sequence sequence seq seq models signicantly improved racy language generation tasks cluding abstractive summarization uency abstractive summarization greatly improved tuning models clear identify important parts source text included summary study investigated effectiveness ing saliency models identify tant parts source text trained seq seq models extensive experiments proposed new bination model consisting saliency model extracts token sequence source text seq seq model takes quence additional input text tal results showed tion models outperformed simple tuned seq seq model cnn xsum datasets seq seq model pre trained large scale corpora cnn dataset proposed bination model exceeded previous performed model points introduction pre trained language models bert vlin signicantly improved accuracy language processing tasks apply bert language eration tasks model structure suitable language generation pre trained seq seq models language eration lewis raffel based encoder decoder transformer model standard model language tion recently proposed work progress els achieved state art results ous language generation tasks including tive summarization generating summary sential correctly predict source text included summary previous studies pre training ined combining extractive summarization stractive summarization gehrmann hsu pre trained seq seq models achieved higher accuracy compared previous models clear ing source text important learned pre training purpose study clarify tiveness combining saliency models tify important source text trained seq seq model abstractive marization task main contributions follows investigated combinations trained seq seq token level saliency models saliency models share parameters encoder seq seq model extract important tokens dently encoder proposed new combination model conditional summarization model portant tokens cit token quence extracted saliency model plicitly given seq seq model ditional input text evaluated combination models cnn hermann xsum narayan datasets cit model outperformed simple tuned model terms rouge scores datasets task denition study focuses tasks abstractive marization saliency detection main task abstractive summarization sub task saliency detection prediction portant parts source text problem mulations task described task abstractive summarization given source text output abstractive mary task saliency detection given source text words output saliency score study investigate tions models tasks pre trained seq seq model pre trained seq seq models applied abstractive summarization song dong raffel models use simple transformer based decoder model vaswani encoder decoder model pre trained large labeled data transformer based encoder decoder work dene transformer based encoder decoder model follows encoder encoder consists layer coder blocks input encoder output layer encoder blocks dened rld encoder block consists self attention module layer feed forward network decoder decoder consists layer coder blocks inputs decoder output encoder output previous step decoder output layer transformer decoder blocks dened rtd step projected ulary space decoder outputs highest probability token token decoder block consists self attention module context attention module layer feed forward network multi head attention encoder coder blocks use multi head attention sists combination attention heads denoted head headi weight matrix attention head attention dened softmax rij rid rjd layer self attention given resentation context attention summary loss function tune seq seq model abstractive summarization use cross entropy loss lsum log number training samples saliency models studies proposed combination token level saliency model seq seq model pre trained reported effectiveness gehrmann zhou use simple token level saliency model basic model study basic saliency model basic saliency model consists transformer encoder blocks encodersal single layer feed forward network dene saliency score token source text encodersal represents output layer encodersal learnable parameters represents sigmoid function types saliency model combination study use types saliency model combination shared encoder tor model structure based basic saliency model describe shared encoder shared encoder shares parameters encodersal encoder seq seq model model jointly trained seq seq model saliency score bias representations seq seq model extractor extractor extracts important tokens sentences source text basis saliency score extractor arated seq seq model model trained independently pseudo reference label saliency model predicts saliency score token reference label train saliency model supervised manner reference label token typically given training data summarization consists source text reference summary reference saliency labels pseudo reference labels aligning source summary token sequences tracting common tokens gehrmann pseudo labels train saliency model supervised manner saliency loss function train saliency model supervised way pseudo reference labels use binary cross entropy loss lsal log combination types roughly categorize combinations types figure shows image combination rst type uses shared encoder models consist shared encoder decoder shared encoder module plays roles saliency detection ing seq seq model saliency scores bias representation seq seq model models type second type uses extractor models consist extractor encoder decoder follow steps rst tractor extracts important tokens sentences source text second encoder uses input seq seq models proposed model cit belongs type type uses shared encoder extractor models consist extractor shared encoder decoder follow steps rst extractor extracts important tokens source text second shared encoder uses input seq seq model loss function viewpoint loss major types model function use saliency loss denote loss function seq seq model labs loss function extractor lext lext trained lsal labs trained lsum lsum lsal shared encoder combine saliency model seq seq model multi task model trains shared encoder decoder minimizing summary saliency losses loss function model labs lsum lsal selective encoding model uses saliency score weight shared encoder specically nal output shared encoder weighted sample pseudo reference label token combined models section describes combinations pre trained seq seq model saliency models replace input decoder zhou bigru use transformer fair comparison loss function model labs lsum figure combinations seq seq saliency models purple encoder blue decoder red shared encoder shared model saliency detection encoding yellow extractor independent saliency model extract important sentences tokens source text colored blocks represents transformer blocks gray linear transformation green context attention pink output trained supervised manner saliency score summary combination model structure loss function model labs lsum lsal selective attention model weights attention scores decoder unlike model specically attention score step weighted row weight matrix attention head context attention ilsl ilsl gehrmann took similar approach model weights copy probability pointer generator model pre trained seq seq model copy mechanism weight context attention transformer decoder blocks loss function model labs lsum combination model structure loss function model labs lsum lsal extractor rene input text sentence extraction generation seg model rst extracts saliency sentences basis sentence level saliency score calculated token level saliency score extractor xlxj number tokens set tokens sentence sentences extracted according level saliency score concatenated text extracted sentences input seq seq model training extracted imizes rouge scores reference summary text test average number sentences training set loss function extractor lext lsal seq seq model labs lsum proposed extractor extract additional input text conditional summarization model portant tokens propose new combination extractor seq seq model cit consider important tokens explicitly models softly weight representations source text attention scores select salient tokens explicitly seg explicitly extracts salient sentences source text token level mation seq seq model drops important information extracting tences contrast cit uses tokens extracted according saliency scores additional input seq seq model adding token level formation cit effectively guide tive summary dropping important formation specically tokens tracted descending order saliency score obtained inputting extractor order retains order source text combined text given seq seq model input text loss function extractor lext lsal seq seq model labs lsum proposed combination extractor shared encoder combination cit model bines cit cit uses extractor extracting important tokens trained shared encoder seq seq model model trained unsupervised way shared encoder output weighted saliency score estimated output shared encoder loss tion extractor lext lsal seq seq model labs lsum combination cit model bines cit train saliency models model trained cit unsupervised way model attention score weighted loss function extractor lext lsal seq seq model labs lsum experiments dataset cnn dataset hermann xsum dataset narayan standard datasets news summarization details datasets listed table cnn highly tractive summarization dataset xsum highly abstractive summarization dataset model congurations bartlarge lewis state art models trained seq seq model robertabase liu initial model extractor extractor cit stop words duplicate tokens ignored xsum dataset implementation seq seq model tuning bartlarge combination models rameters ofcial code tuning set train dev eval cnn xsum avg length table details datasets paper models bart lewis bart tuning seg cit cit cit table results bart combined models cnn dataset row groups models scribed order robertabase set learning rate batch size evaluation metrics rouge scores including rouge evaluation metrics lin rouge scores calculated results saliency models improve summarization curacy highly extractive datasets rouge scores combined models cnn dataset shown table combined models outperformed simple tuned bart indicates saliency tection effective highly extractive datasets proposed models cit achieved highest accuracy cit model outperformed saliency models cates cit model effectively guides stractive summarization combining explicitly extracted tokens saliency models improve summarization curacy highly abstractive datasets rouge scores combined models xsum com transformers com pytorch fairseq com pltrdy models bart lewis bart tuning seg cit cit cit table results bart combined models xsum dataset underlined result represents best result models outperformed simple tuning result cnn models cit toks cit sents xsum table results saliency models cnn xsum datasets cit extracted tokens sentences source text lapata summaries xsum highly tive result bertsumext xsum reported dataset shown table cit model performed best improvement smaller cnn dataset accuracy seg els decreased xsum dataset results different cnn dataset reason difference traced quality pseudo saliency labels cnn highly extractive dataset relatively easy create token alignments erating pseudo saliency labels contrast summary xsum highly abstractive short makes difcult create pseudo labels high quality simple token ment improve accuracy summarization dataset improve quality pseudo saliency labels accuracy saliency model accurate outputs extractors analyzed quality tokens extracted extractor cit results rized table cnn dataset scores tor tokens higher data params models massbase large hugenews ernie large cit table results state art models posed model cnn dataset report size pre training data parameters utilized model ata models data params massbase large hugenews cit table results state art models posed model xsum dataset lapata els rouge score lower sentence based extraction method token level extractor nds important tokens seq seq model learns generate uent summary rating important tokens hand extractive result xsum dataset lower highly abstractive datasets little overlap tokens need consider high quality pseudo saliency labels evaluate similarity sequences cit model outperform tuned models study focuses binations saliency models pre trained seq seq model studies focus pre training egy compared cit model models rouge scores shown bles table model outperformed recent pre trained models cnn dataset pegasushugenews pre trained largest corpus comprised news like articles curacy abstractive summarization proved model improved accuracy additional pre training result dicates effective combine saliency models seq seq model generating highly extractive summary hand xsum dataset pegasushugenews improved rouge scores achieved best results xsum dataset summaries include expressions written source text ing pre training data learning terns effective improving quality pseudo saliency labels able improve accuracy cit model related work discussion pre trained language models abstractive summarization liu bert sentence level extractive summarization model zhang proposed new pre trained model considers document level tion sentence level extractive summarization researchers published pre trained encoder decoder models recently wang lewis raffel wang pre trained based pointer generator model lewis pre trained standard transformer based encoder decoder model large unlabeled data achieved state art results dong xiao extended bert structure handle seq seq tasks studies focused learn universal pre trained model consider combination pre trained saliency els abstractive summarization model abstractive summarization saliency models hsu gehrmann incorporated word level extractive model pointer generator model models weight copy probability source text extractive model guide pointer generator model copy important words proposed keyword guided abstractive chen bansal marization model proposed sentence extraction writing model trained end end manner learning reinforcement cao proposed search rewrite model mendes proposed combination sentence level extraction compression models based pre trained model contrast purpose clarify combined models effective rst investigate combination pre trained seq seq saliency models compared variety combinations claried combination effective conclusion rst study conducted extensive experiments investigate effectiveness corporating saliency models pre trained seq seq model results found saliency models effective nding portant parts source text seq model pre trained large scale pora especially generating highly tive summary proposed new nation model cit outperformed simple tuning combination models nation model improved summarization racy additional pre training data applied pre trained model cent studies conducted improve marization accuracy increasing pre training data developing new pre training strategies study sheds light importance saliency models abstractive summarization references hangbo bao dong furu wei wenhui wang nan yang xiaodong liu wang songhao piao jianfeng gao ming zhou hsiao wuen hon pseudo masked language els unied language model pre training corr ziqiang cao wenjie sujian furu wei retrieve rerank rewrite soft template based neural summarization acl pages yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting acl pages jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language standing naacl hlt pages transfer learning unied text text corr kaitao song tan tao qin jianfeng yan liu mass masked sequence quence pre training language generation icml pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need nips pages liang wang wei zhao ruoyu jia sujian jingming liu denoising based sequence pre training text generation emnlp ijcnlp pages dongling xiao han zhang kun sun hao tian hua haifeng wang gen enhanced pre training tuning framework natural language generation corr yan weizhen yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang ming zhou prophetnet predicting future gram sequence sequence pre training corr yongjian weijia jia tianyi liu wenmian yang improving abstractive document marization salient information modeling acl pages jingqing zhang yao zhao mohammad saleh ter liu pegasus pre training tracted gap sentences abstractive summarization corr xingxing zhang furu wei ming zhou hibert document level pre training cal bidirectional transformers document rization acl pages qingyu zhou nan yang furu wei ming zhou selective encoding abstractive sentence summarization acl pages dong nan yang wenhui wang furu wei aodong liu wang jianfeng gao ming zhou hsiao wuen hon unied language model pre training natural language neurips pages ing generation sebastian gehrmann yuntian deng alexander rush abstractive summarization emnlp pages karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching nips pages chines read comprehend wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun unied model extractive abstractive summarization inconsistency loss acl pages mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov luke zettlemoyer bart denoising sequence sequence training natural language generation translation comprehension corr chenliang weiran sheng gao guiding generation abstractive text tion based key information guide network acl pages chin yew lin rouge package matic evaluation summaries acl pages yang liu fine tune bert extractive marization corr yang liu mirella lapata text tion pretrained encoders emnlp ijcnlp pages yinhan liu myle ott naman goyal jingfei dar joshi danqi chen omer levy mike lewis luke zettlemoyer veselin stoyanov roberta robustly optimized bert pretraining approach corr afonso mendes shashi narayan sebastiao miranda zita marinho andre martins shay jointly extracting compressing cohen documents summary state representations naacl pages shashi narayan shay cohen mirella lapata details summary topic aware convolutional neural networks emnlp pages treme summarization colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei peter liu exploring limits
