abstractive summarization combination pre trained sequence sequence saliency models itsumi saito kyosuke nishida kosuke nishida junji tomita ntt media intelligence laboratory ntt corporation itsumi saito ntt co jp r m l c s c v v x r abstract pre trained sequence sequence seq seq models signicantly improved racy language generation tasks cluding abstractive summarization uency abstractive summarization greatly improved ne tuning models clear identify important parts source text included summary study investigated effectiveness ing saliency models identify tant parts source text trained seq seq models extensive experiments proposed new bination model consisting saliency model extracts token sequence source text seq seq model takes quence additional input text tal results showed tion models outperformed simple ne tuned seq seq model cnn dm xsum datasets seq seq model pre trained large scale corpora cnn dm dataset proposed bination model exceeded previous performed model points l introduction pre trained language models bert vlin et al signicantly improved accuracy language processing tasks apply bert language eration tasks model structure suitable language generation pre trained seq seq models language eration lewis et al raffel et al based encoder decoder transformer model standard model language tion recently proposed work progress els achieved state art results ous language generation tasks including tive summarization generating summary sential correctly predict source text included summary previous studies pre training ined combining extractive summarization stractive summarization gehrmann et al hsu et al pre trained seq seq models achieved higher accuracy compared previous models clear ing source text important learned pre training purpose study clarify tiveness combining saliency models tify important source text trained seq seq model abstractive marization task main contributions follows investigated combinations trained seq seq token level saliency models saliency models share parameters encoder seq seq model extract important tokens dently encoder proposed new combination model conditional summarization model portant tokens cit token quence extracted saliency model plicitly given seq seq model ditional input text evaluated combination models cnn dm hermann et al xsum narayan et al datasets cit model outperformed simple ne tuned model terms rouge scores datasets task denition study focuses tasks abstractive marization saliency detection main task abstractive summarization sub task saliency detection prediction portant parts source text problem mulations task described task abstractive summarization given source text x output abstractive mary y yt task saliency detection given source text x l words xl output saliency score s sl study investigate tions models tasks pre trained seq seq model pre trained seq seq models applied abstractive summarization song et al dong et al raffel et al models use simple transformer based decoder model vaswani et al encoder decoder model pre trained large labeled data transformer based encoder decoder work dene transformer based encoder decoder model follows encoder encoder consists m layer coder blocks input encoder x xi xl output m layer encoder blocks dened e hm el rld hm hm h m encoder block consists self attention module layer feed forward network decoder decoder consists m layer coder blocks inputs decoder output encoder h m output e previous step decoder output m layer transformer decoder blocks dened h m dt rtd hm hm step t hm dt projected ulary space decoder outputs highest probability token token decoder block consists self attention module context attention module layer feed forward network multi head attention encoder coder blocks use multi head attention sists combination k attention heads denoted k v o head headi q v w v kw k weight matrix attention head attention k v dened softmax rij dk d k rid k v rjd m th layer self attention given q k v k resentation h m context attention h m v q h m e summary loss function ne tune seq seq model abstractive summarization use cross entropy loss lsum log p t n t n t n number training samples saliency models studies proposed combination token level saliency model seq seq model pre trained reported effectiveness gehrmann et al zhou et al use simple token level saliency model basic model study basic saliency model basic saliency model consists m transformer encoder blocks encodersal single layer feed forward network dene saliency score l token l l source text sl encodersal represents output layer encodersal rd learnable parameters represents sigmoid function types saliency model combination study use types saliency model combination shared encoder tor model structure based basic saliency model describe shared encoder shared encoder shares parameters encodersal encoder seq seq model model jointly trained seq seq model saliency score bias representations seq seq model extractor extractor extracts important tokens sentences source text basis saliency score extractor arated seq seq model model trained independently pseudo reference label saliency model predicts saliency score sl token xl reference label rl xl train saliency model supervised manner reference label token typically given training data summarization consists source text reference summary reference saliency labels pseudo reference labels aligning source summary token sequences tracting common tokens gehrmann et al pseudo labels train saliency model supervised manner saliency loss function train saliency model supervised way pseudo reference labels use binary cross entropy loss lsal n l n l l log sn rn l l sn l combination types roughly categorize combinations types figure shows image combination rst type uses shared encoder models consist shared encoder decoder shared encoder module plays roles saliency detection ing seq seq model saliency scores bias representation seq seq model models type second type uses extractor models consist extractor encoder decoder follow steps rst tractor extracts important tokens sentences source text second encoder uses input seq seq models proposed model cit belongs type type uses shared encoder extractor models consist extractor shared encoder decoder follow steps rst extractor extracts important tokens source text second shared encoder uses input seq seq model loss function viewpoint loss major types model function use saliency loss denote loss function seq seq model labs loss function extractor lext lext trained lsal labs trained lsum lsum lsal shared encoder combine saliency model seq seq model multi task mt model trains shared encoder decoder minimizing summary saliency losses loss function model labs lsum lsal selective encoding se model uses saliency score weight shared encoder specically nal output hm el shared encoder weighted rn n th sample l pseudo reference label token xl hm el hm el sl combined models section describes combinations pre trained seq seq model saliency models replace input decoder hm el hm el zhou et al bigru use transformer fair comparison loss function model labs lsum figure combinations seq seq saliency models purple encoder blue decoder red shared encoder shared model saliency detection encoding yellow extractor independent saliency model extract important c sentences xs e tokens c source text x colored blocks represents m transformer blocks gray linear transformation green context attention pink output trained supervised manner s saliency score y summary combination se mt model structure se loss function model labs lsum lsal selective attention sa model weights attention scores decoder unlike se model specically attention score rl step t weighted sl t th row ai rt l weight matrix th attention head context attention eq il ilsl l ilsl gehrmann et al took similar approach model weights copy probability pointer generator model pre trained seq seq model copy mechanism weight context attention transformer decoder blocks loss function model labs lsum combination sa mt model structure sa loss function model labs lsum lsal extractor rene input text sentence extraction generation seg model rst extracts saliency sentences basis sentence level saliency score sj sj calculated token level saliency score extractor sl sj nj sl l xlxj nj xj number tokens set tokens j th sentence p sentences extracted according level saliency score concatenated text xs extracted sentences input seq seq model training extracted xs imizes rouge l scores reference summary text test average number sentences xs training set p loss function extractor lext lsal seq seq model labs lsum proposed extractor extract additional input text conditional summarization model portant tokens propose new combination extractor seq seq model cit consider important tokens explicitly se sa models softly weight representations source text attention scores select salient tokens explicitly seg explicitly extracts salient sentences source text token level mation seq seq model drops important information extracting tences contrast cit uses tokens extracted according saliency scores additional input seq seq model adding token level formation cit effectively guide tive summary dropping important formation specically k tokens c ck tracted descending order saliency score s s obtained inputting x extractor order c retains order source text x combined text x given seq seq model input text loss function extractor lext lsal seq seq model labs lsum proposed combination extractor shared encoder combination cit se model bines cit se cit uses extractor extracting important tokens se trained shared encoder seq seq model se model trained unsupervised way e shared encoder output h m weighted saliency score s eq s estimated output shared encoder eq loss tion extractor lext lsal seq seq model labs lsum combination cit sa model bines cit sa train saliency models sa model trained cit unsupervised way se model attention score weighted s eq loss function extractor lext lsal seq seq model labs lsum experiments dataset cnn dm dataset hermann et al xsum dataset narayan et al standard datasets news summarization details datasets listed table cnn dm highly tractive summarization dataset xsum highly abstractive summarization dataset model congurations bartlarge lewis et al state art models trained seq seq model robertabase liu et al initial model extractor extractor cit stop words duplicate tokens ignored xsum dataset implementation seq seq model ne tuning bartlarge combination models rameters ofcial code ne tuning set train dev eval cnn dm xsum avg length table details datasets paper models bart lewis et al bart ne tuning mt se se mt sa sa mt seg cit cit se cit sa rl table results bart combined models cnn dm dataset row groups models scribed order robertabase set learning rate batch size evaluation metrics rouge scores including rouge l r l evaluation metrics lin rouge scores calculated results saliency models improve summarization curacy highly extractive datasets rouge scores combined models cnn dm dataset shown table combined models outperformed simple tuned bart indicates saliency tection effective highly extractive datasets proposed models cit se achieved highest accuracy cit model outperformed saliency models cates cit model effectively guides stractive summarization combining explicitly extracted tokens saliency models improve summarization curacy highly abstractive datasets rouge scores combined models xsum com transformers com pytorch fairseq com pltrdy models bart lewis et al bart ne tuning mt se se mt sa sa mt seg cit cit se cit sa r l table results bart combined models xsum dataset underlined result represents best result models outperformed simple ne tuning result cnn dm models cit k toks cit sents xsum r l r l table results saliency models cnn dm xsum datasets cit extracted k tokens sentences source text lapata summaries xsum highly tive result bertsumext xsum reported dataset shown table cit model performed best improvement smaller cnn dm dataset accuracy mt se mt seg els decreased xsum dataset results different cnn dm dataset reason difference traced quality pseudo saliency labels cnn dm highly extractive dataset relatively easy create token alignments erating pseudo saliency labels contrast summary xsum highly abstractive short makes difcult create pseudo labels high quality simple token ment improve accuracy summarization dataset improve quality pseudo saliency labels accuracy saliency model accurate outputs extractors analyzed quality tokens extracted extractor cit results rized table cnn dm dataset scores tor k tokens higher data params r l models g m massbase g m g m g g m large g m hugenews t m g m ernie large g m g m g m cit table results state art models posed model cnn dm dataset report size pre training data parameters utilized model et al ata et al et al et al et al et al al et al models data params r l massbase g m g m g m large g m hugenews t m g m g m cit table results state art models posed model xsum dataset et al lapata et al et al et al els rouge l score lower sentence based extraction method token level extractor nds important tokens seq seq model learns generate uent summary rating important tokens hand extractive result xsum dataset lower highly abstractive datasets little overlap tokens need consider high quality pseudo saliency labels evaluate similarity sequences cit model outperform tuned models study focuses binations saliency models pre trained seq seq model studies focus pre training egy compared cit model models rouge scores shown bles table model outperformed recent pre trained models cnn dm dataset pegasushugenews pre trained largest corpus comprised news like articles curacy abstractive summarization proved model improved accuracy additional pre training result dicates effective combine saliency models seq seq model generating highly extractive summary hand xsum dataset pegasushugenews improved rouge scores achieved best results xsum dataset summaries include expressions written source text ing pre training data learning terns effective improving quality pseudo saliency labels able improve accuracy cit model related work discussion pre trained language models abstractive summarization liu bert sentence level extractive summarization model zhang et al proposed new pre trained model considers document level tion sentence level extractive summarization researchers published pre trained encoder decoder models recently wang et al lewis et al raffel et al wang et al pre trained based pointer generator model lewis et al pre trained standard transformer based encoder decoder model large unlabeled data achieved state art results dong et al xiao et al extended bert structure handle seq seq tasks studies focused learn universal pre trained model consider combination pre trained saliency els abstractive summarization model abstractive summarization saliency models hsu et al gehrmann et al et al incorporated word level extractive model pointer generator model models weight copy probability source text extractive model guide pointer generator model copy important words li et al proposed keyword guided abstractive chen bansal marization model proposed sentence extraction writing model trained end end manner learning reinforcement cao et al proposed search rewrite model mendes et al proposed combination sentence level extraction compression models based pre trained model contrast purpose clarify combined models effective rst investigate combination pre trained seq seq saliency models compared variety combinations claried combination effective conclusion rst study conducted extensive experiments investigate effectiveness corporating saliency models pre trained seq seq model results found saliency models effective nding portant parts source text seq model pre trained large scale pora especially generating highly tive summary proposed new nation model cit outperformed simple tuning combination models nation model improved summarization racy additional pre training data applied pre trained model cent studies conducted improve marization accuracy increasing pre training data developing new pre training strategies study sheds light importance saliency models abstractive summarization references hangbo bao li dong furu wei wenhui wang nan yang xiaodong liu yu wang songhao piao jianfeng gao ming zhou hsiao wuen hon pseudo masked language els unied language model pre training corr ziqiang cao wenjie li sujian li furu wei retrieve rerank rewrite soft template based neural summarization acl pages yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting acl pages jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language standing naacl hlt pages transfer learning unied text text corr kaitao song xu tan tao qin jianfeng lu yan liu mass masked sequence quence pre training language generation icml pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention need nips pages liang wang wei zhao ruoyu jia sujian li jingming liu denoising based sequence pre training text generation emnlp ijcnlp pages dongling xiao han zhang yu kun li yu sun hao tian hua wu haifeng wang gen enhanced pre training tuning framework natural language generation corr yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang ming zhou prophetnet predicting future gram sequence sequence pre training corr yongjian weijia jia tianyi liu wenmian yang improving abstractive document marization salient information modeling acl pages jingqing zhang yao zhao mohammad saleh ter j liu pegasus pre training tracted gap sentences abstractive summarization corr xingxing zhang furu wei ming zhou hibert document level pre training cal bidirectional transformers document rization acl pages qingyu zhou nan yang furu wei ming zhou selective encoding abstractive sentence summarization acl pages li dong nan yang wenhui wang furu wei aodong liu yu wang jianfeng gao ming zhou hsiao wuen hon unied language model pre training natural language neurips pages ing generation sebastian gehrmann yuntian deng alexander rush abstractive summarization emnlp pages karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching nips pages chines read comprehend wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun unied model extractive abstractive summarization inconsistency loss acl pages mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov luke zettlemoyer bart denoising sequence sequence training natural language generation translation comprehension corr chenliang li weiran xu si li sheng gao guiding generation abstractive text tion based key information guide network acl pages chin yew lin rouge package matic evaluation summaries acl pages yang liu fine tune bert extractive marization corr yang liu mirella lapata text tion pretrained encoders emnlp ijcnlp pages yinhan liu myle ott naman goyal jingfei du dar joshi danqi chen omer levy mike lewis luke zettlemoyer veselin stoyanov roberta robustly optimized bert pretraining approach corr afonso mendes shashi narayan sebastiao miranda zita marinho andre f t martins shay b jointly extracting compressing cohen documents summary state representations naacl pages shashi narayan shay b cohen mirella lapata nt details summary topic aware convolutional neural networks emnlp pages treme summarization colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li peter j liu exploring limits
