a semantic approach to summarization divyanshu bhartiya ashudeep singh department of computer science and engineering iit kanpur under the guidance of harish karnick abstract sentence extraction based summarization of documents has some limitations as it does nt go into the semantics of the document also it lacks the capability of sentence generation which is intuitive for humans in this report we hereby take the task of summarization to semantic levels with the use of wordnet and sentence compression for enabling sentence generation we involve semantic role labelling to get semantic representation of text and use of segmentation to form clusters of related pieces of text picking out the centroids and sentence generation completes the task we evaluate the results of our system against human composed summaries and also present an evaluation done by humans to measure the quality attributes of our summaries introduction text one has in today s era when the size of information and data is increasing exponentially there is an upcoming need to create a concise version of the information available most of the information is available in text form till now humans have tried to create summaries of the documents creating summaries is a tedious task and one has to read the whole information document to be able to write an abstract and a concise representation of to avoid the unnecessary and redundant content while creating a summary of a document one has to get a hold of all the information the document contains and he must be well versed to be able to convey the idea the document represents there is a need to be able to do the aforementioned task automatically we hereby propose an approach to do this task of summarization automatically we came up with a semantic approach to do this task our main motivation was to follow the same footprints that a human takes while creating a summary a human understand the document and links up the parts of documents that trying to convey a piece of information he then compresses the information to his need and create the best representation of the document we hereby introduce the same approach identifying the meaning of the document linking them up getting the best representation and creating a concise version of it ii previous work the earliest work on automatic summarization was around five decades ago in luhn et al proposed that word frequency plays a very important role in determining its saliency while extracting summaries from scientific text subsequently baxendale proposed the importance of a word s position and edmundson explained how key phrases can be used to extract summaries various works published at those times dealt with the news wire data or scientific documents but with the resurgence in the availability of wide tagged corpora and tools available to do processing of natural language at both syntactic and semantic levels and the ever growing size of the information available through internet has regrown the interest of nlp researchers in automatic summarization techniques during the last decade a crucial issue that the researchers felt the need to address was evaluation during the past decade many system like text retrieval conference evaluation competitions trec document understanding conference duc and message understanding conferences muc have created sets of text material and have discussed performance evaluation to evaluate issues however a universal strategy summarization is still absent summarization tasks can be widely separated into three single document summarization multi document summarization query based summarization or topic driven the summarization in single document summarization challenge is to look for flow of information or discourse in the text which should be conveyed by a good summary while in multi document summarization tasks the techniques which are of importance are similarity measures and clustering techniques machine learning techniques for summarization of a single document kupiec al described a naive bayes method to determine whether a sentence belongs to a summary or not for this they used a large set of technical documents with their abstracts to determine the underlying probabilities of features like sentence length the presence of uppercase words presence of more frequent words and then choosing the n highest probable sentences for the summary generation to this approach many features were proposed and tested like tf idf scores but these techniques were soon replaced by other sophisticated machine learning techniques which used bigger feature sets extracted from document and summary in later work lin used decision tree arguing that the features extracted from the text were independent of each other his system performed better than the naive bayes systems this justified the dependence between the various textual features used in the techniques then during the same time wordnet became the prime source of deep natural language analysis for summarization mckeown and radev emphasized on using semantic structure of text rather than statistics of words of the documents they describe a notion of cohesion in text meaning greater cohesion amongst parts of text where similar meaning words occur the cohesion phenomenon occurs at the word sequence levels and hence form lexical chains identifying lexical chains in the text they used the strongest few lexical chains to represent the text in the summary multi document summarization requires higher abstraction and information fusion the earliest attempt on document summarization was summons radev and mckeown it tackles news articles relating to single domain but collected from varied sources it has built in message understanding templates for text understanding and representation the two way step is first processing the input texts and filling in the templates and then using sophisticated linguistic generator to get a single short summary but as expected the system despite being promising was not widely used because of narrow domain application and hard coded templates this tool was then improved by barziley and mckeown defining a clustering problem that will deal with the text representation problem to construct clusters they define features like tf idf scores noun phrases proper nouns and synsets from the wordnet after themes as identified into clusters the next step is information fusion depending on the dependency parse structures of the extracted sentences phrases in the themes specific phrases are used for the final generation part once the summary content is decided a grammatical text is generated by translating those structures to arguments required by surge language generation system through the summons system and its further improvements radev and mckeowns brought in the the concept of centroid based clustering summarization paradigm which was then explored by discovering more features that prove important to make two portions of text similar into although the research on summarization was started about a years ago there is still a long way to go before systems are built which create beautiful short summaries as good as human generated ones the research in this area is hot because of the ever increasing information on the world wide web and accurate summarization techniques are certainly important for text understanding with the highly efficient and accurate systems to tag represent and analyze texts in english like penn tree bank trained pos taggers wordnet named entity taggers semantic role labelers there is a hope in the research community that this research in summarization and related fields in natural language can be taken through separate directions in this project we work on applying these state of the art techniques over text and extracting meaningful and concise summaries in the domain of single document summarization tasks iii technologies our approach to summarization begins with the task of pronominal resolution followed by part of speech tagging and semantic role labelling to create frames of a sentence wordnet is then used to get to the synsets and the semantics of the sentence clustering is then applied to the frames followed by getting the respective centroids of the frames pronominal resolution in machine pronominal or anaphora resolution is an important task before we go deep into the process of summarization it is widely used translation summarization or question answering system anaphora is the act of referral that is it denotes to the antecedent in the left anaphora resolution is the task of identifying the subject noun for a reference pronoun when humans perform pronominal resolution they keep in mind the central idea and topic of the current information being conveyed humans do nt need to go back in the document structure or jot down the points to do the resolution humans are capable of doing that intuitively and by refreshing the memory task pronominal resolution is needed to avoid reference structures they can attain that john helped mary she was happy for the help provided by him now sentence two is more informative than second one however if the summary only contains sentence that wo nt make any sense and will be incomprehensible however on performing the pronominal resolution we can obtain john helped mary mary was happy for the help provided by john part of speech tagging part of speech tagging or pos tagging is the task of grammar tagging of a language it is the task of identifying the nouns verbs adjectives pronouns adverbs this is accomplished by tagging each word or token of the text with its respective part of speech the tagging is mostly performed in accordance with the grammar rules of language we use the stanford s implementation of pos tagging to achieve our task this software uses log linear part of speech taggers following is an example to illustrate its use mary was happy for the help provided by john the output of stanford pos tagger is a parse tree which starts at s sentence being the top level node and branching arising from the grammar rules of english language the output is root s np nnp mary vp vbd was adjp jj happy pp in for np np the nn help vp vbn provided pp in by np nnp john pos tagger example this parse tree will help us retrieve the nouns and the verbs in a sentence which will be put to further use semantic role labelling semantic role labelling is a shallow semantic parsing technique in nlp it detects the predicates associated with a verb in a sentence it is the task of finding the arguments and the modifiers associated with a verb it is analogical to a function with certain parameters each function can be considered a verb corresponding to an action as each action is associated with an agent and a theme the parameters of the function can be considered as the agent and themes each verb is associated with modifiers like temporal locational or an adverb these modifiers can also be considered to be parameters of the respective function representing the verb so in short if a sentence is represented by the following pattern agent action theme modifiers the sentence can be translated as f argn where f is the action and are the agent theme and modifiers respectively then example john helped mary semantic role labelling captures the semantics of a sentence as this help retrieve the specific actions and their respective arguments semantic role labelling helps in finding the meaning of the sentences and the associated actions in the sentence it recognises the participants of the propositions governed by verbs senna it is a software which does a variety of nlp tasks like pos tagging ner recognition chunking semantic role labelling and syntactic parsing it s a c based system which we employed to do semantic role labelling given a sentence senna srl creates frames for each of the verb occurring in the sentence propbank annotation propbank is a in which the arguments of each predicate are annotated with their semantic roles in relation to the predicate propbank associates each verb with arguments and some modifiers it is similar to framenet each of this predicate is associated with a frame bush met him privately in the white house on thursday relation met bush him argm mnr privately argm loc in the white house argm tmp on thursday rel argm tmp mnr loc fig structure of a frame by srl we use these frames to our disposal to get the semantic representation of our document each frame represents a part of sentence describing its meaning wordnet wordnet is a freely available standard from princeton university put to the use of natural language tasks wordnet is a lexical database of english comprising and arranged in nouns verbs adjectives and adverbs they are grouped into sets of cognitive synonyms these sets are called synsets as they describe the semantic and lexical relations between words similar types of words form a synset meant to convey a similar sense of meaning synsets are organized in relations based on their hierarchy noun synsets have a relationship of hyponymy and hypernymy for example car is a kind of vehicle then the synset of car has a hypernym synset that contains vehicle similar holds for hyponymy verbs synsets have relationship of hypernymy and troponymy troponymy forms the bottom of the tree and expresses more specific manners we use wordnet to get the synsets of the nouns and verbs associated with a frame wordnet will enable to find the related pieces of text in a document and will ease up the problem of textual entailment the different levels of hierarchy lead to different levels of abstraction in our approach we just to one level up and one level down search not going deep into relationships of hyponymy or hypernymy iv our approach summarization task was initiated with the thought in mind of getting a summary of the document which will not be based on extraction of informative sentences from the document but the idea of generation of sentences to create summaries we should be able to represent the document in a model from which it s possible to generate sentences also we needed the semantics to come into play while creating the summaries so our idea of generation of sentences comes from compressing a given sentence this can be achieved in a lot of fashion for example getting rid of unnecessary verbs from a sentence or avoiding adverbs our system takes a document as an input and does the pronominal resolution on it pronominal resolution is used to form chains in the documents and resolve the pronouns with their respective subjects this will help us in the process of information chunks without any ambiguity extracting replacing pronouns with their respective and related nouns will help us in the later phases were the measurement of context or flow of information is difficult to manage getting rid of ambiguous or questionable information is a prior task in our approach of summarization once the document is resolved we do the part of speech tagging on it part of speech tagging is a basic move in natural language processing identifying nouns and verbs is the main motivation of doing pos tagging pos tagging forms the basis of semantic role labelling where frames are built on verbs as the root of the frames also the use of part of speech tagging enables us to identify the elements on which the synsets need to be constructed now we move to the crux of our system now we perform semantic role labelling on the pre processed data semantic role labelling enables us to reduce the granularity of the document the main motivation of using srl is to introduce the semantics an approach was needed granularity of document from sentences to a model where the semantics of the sentence can be represented in short we needed a representative model which will clearly depict the semantics srl functionality of senna helps us to create frames from a sentence each frame will be constructed using verb as the root node and its respective arguments as its branches hence a propbank annotation hence a sentence containing one or more verbs is now a collection of frames to decrease if document d has sentences sn after this phase we have a collection of frames where m now we do nt need sentences to work around with and here on we progress on using these frames as the basis d srl si i n now that the document is decomposed into frames we move forward to the next phase of forming synsets wordnet dictionary helps to find the synsets the hyponym synsets and hypernym synsets this is used for the fact that a meaning can be later be repeated in the document and hence to avoid redundant data wordnet can be used to capture this wordnet will help to collect all the pieces of text in the document where a similar idea or type of information is described this will enable us to capture all those sentences where the topic of description remains same to go about this idea we find the hyponym and hypernym synsets for the arguments nouns for the frame s root verb we find the hypernyms and troponyms of the verb now each frame is associated with two sets a set describing the arguments synsets and a set describing the verb s synsets synsets n n framei args synsets v v where synsets n hyponyms hypernyms synsets v hypernyms v troponyms v each frame now describes the possible meanings and also have a sematic representation to it now we do the task of textual entailment where we use the synsets created by wordnet to find frames entailing each other this is accomplished by finding the match between a frame s arguments synsets and another frame s arguments synsets this measures the entailment content of an agent or theme of a frame with another in short if there is high matching between the two frames arguments synsets it reveals that the subject of two frames or the parameters of two different frames verbs are same we also measure the matching between the verb s synsets to see if the similar type of action is performed somewhere else in the document let the score of arguments synsets matching be denoted by a fi fj that is it calculates the arguments synsets similarity similarly that matching score of verb s synsets is depicted by v fi fj the minimum scores is which is nothing matches at all these two matchings lead to various scenarios in order of priority where the frames meaning are measured a fi fj and v fi fj this is the highest priority case the scores reveal that there is some resemblance between two frames in terms of the arguments as well as the action both the frames are talking about similar agents doing some similar action a fi fj and v fi fj this is the second scenario where there is no matching between verb s synsets this case tells that the two frames have similar parameters but there is no relationship between the actions performed by the arguments both the frames talk about similar agents but completely different tasks are performed by them for example fi john helped mary fj john killed bob a fi fj and v fi fj this is the third priority case where the two frames talk about similar actions but their agents are different this scenario is of lower priority because subject matching is more important than action matching for example fi john helped mary fj bob helped a dog a fi fj and v fi fj this is the lowest priority matching the frames are irrelevant to each other and have no sort of connection between them now once we have the matching scores we can link these frames as based on their matching score construct a graph g with frames being the nodes of a graph and the edges being the matching between the two frames the graph will be a directed weighted graph the wrights being the sum of a fi fj and v fi fj this score measures the similarity between two frames constructing the graph will connect all the related frames we take only a limited number of matchings based on the score of matching we reduce the number of edges taking some of the top scores to create edges this ensures we connect only those frames that have good similarity measure score segmentation we introduce the process of creating segments in the graph this is accomplished by joining all the connected frames from a source node it basically is creating of all frames that are accessible from an originating node the algorithm for creating segments is of segments add to sj s of all segments for each frame in document if sj for some j else end return s create a new segment sk s s sk add to sk add to sk creating segments segmentation generates clusters in our graph it forms all the frames that have some sort of relationship among each other we can say that the clusters so obtained contain similar type of information and those have frames have a great deal of matching amongst each other now the problem just remains to get the frames that will hence be the best representation of the frames of the segments so thus formed we can say these frames to be a good and concise representation of the information depicted by the frames in the cluster centroids extraction just like the centroid of a cluster are the middle point of a cluster we call the best frames of a segment to be the centroid of that segment getting the centroids from a segment is based on a number of features like the number of incident on that frame the number of outgoing edges from that frame the frame position in the document the length of the frame the number of named entities in the frame we calculate a weighted sum of these features for each frame in a cluster wi fi fi the score obtained from incoming and outgoing edges is of great value as it tells us about the information network present in the document due to that particular frame each frame gets a score based on these features and the frame with highest scores are chosen as the representation of the segment we extract some number of frames from each segment and hereby we obtain the final frames that will represent our document to generate sentences from these obtained frames we observe the arguments in the frame and generate sentences by concatenating verb the sentences so obtained are expected to be a summary of the document input we take the assumption that is the subject of verb and represents the object of verb this assumption does not always lead to grammatically correct sentences observations and results semantic role labelling receives all transitive verbs and creates frames out of it since the transitive verbs represent some action that is performed and have a subject and an object intransitive verbs lack these qualities they lack the need of an object to which the verb is referenced hence srl does correct labelling for most of the sentences in the document it is also able to recognise most of the verbs that are associated with a proposition finding similarity on the basis of wordnet gives satisfying results apart from synset matching we employed the use of individual matching of words present in verbs and arguments to avoid repeated sentences that hold the same meaning and thus avoiding redundant information feature score on the basis of number of outgoing and incoming edges result the best score as it links up all the connected frames henceforth making it the most suitable frame to be picked for representing the frames it has joined the features taken for calculating the centroids helps in getting impressive frames from all over the document the process of segmentation and getting centroids produces good quality frames however the sentence generation part is a little faulty as it has not been able to produce grammatically correct sentences also sometimes the centroids that are selected are from the clauses of the sentences they represent this means that these frames may not be able to generate grammatically correct sentences since they do nt from the principal verb of the sentence some of the sentences are grammatically correct and do nt require modification the sentence generation needs improvements to be able to recognise the type of sentence it can form from the frame so obtained to measure the performance of our summary we use a document which already has a human written summary we measure the performance of our summary generated say s with to measure the summary quality we perform the frame formation process s this will result in a collection of frames now we have two collections of frames that is the frames centroids of our segmentation process on the document and the set of frames obtained from doing semantic role labelling we again perform similarity matching using wordnet between these two sets s i centroids argmax j sim i j the similarity score between the two summaries depends upon the number of centroids we choose to form our summary the average score comes out to be about match this is not a bad result as we ca nt expect the summaries to be of the same intellectual and abstract quality our approach focussed on semantics and sentence compression this result shows that the semantics are well conveyed in our summary to measure the qualities of a summary like expressiveness information content abstraction we performed a human evaluation in reference the grounds of human evaluation we executed our system on some chapters from into the wild animal form and ncert class x science we had summaries on those chapters beforehand we asked humans to rate our system generated summaries to human compiled respective information content summaries on grammatical correctness abstraction expressiveness and excess or unnecessary information these qualities are rated on a scale of to being the least and being the most the mean and standard deviation observed over these qualities is depicted in the table the results suggest that information content is good in our summaries generated but some people are of the opinion that there is a lot of unrelated information and the standard deviation varies a lot also we need to improve upon the abstractness quality as most of the frames are just a short hand representation of sentences in document and hence turn out to be factual summary quality attributes quality information content grammatical correctness abstractness expressiveness excess unnecessary detail mean standard deviation fig fig fig fig fig aaai spring symposium computational approaches to analyzing weblogs pp hu liu august mining and summarizing customer reviews in proceedings of the tenth acm sigkdd international conference on knowledge discovery and data mining pp acm brusilovsky schwarz weber january elm art an intelligent tutoring system on world wide web in intelligent tutoring springer berlin heidelberg adamson bhartiya gujral kedia singh ros january automatically generating discussion questions in artificial intelligence in education pp springer berlin heidelberg vi conclusion summarization is an important field of research these days and finds applications in the fields of information retrieval like in news mining and customer review mining and educational systems like intelligent tutors question generation systems many different approaches have seem to work well on different types of text like factual scientific papers or news our system seems to perform well on factual data further modifications could be improving the feature space for centroid selection better sentence generation capabilities or improved metric for semantic similarities also a lot number of heuristics can be applied to preprocess the document and improve the quality of information in the primary stages our system needs improvement in sentence generation and semantic role labelling this work has been done in the motivation to produce better quality and meaningful summaries which has been exemplified by our results references luhn the automatic creation of literature abstracts ibm journal of research development baxendale machine made index for technical literature an experiment ibm journal of research development edmundson new methods in automatic extracting journal of the acm kupiec pedersen and chen a trainable document summarizer in proceedings sigir pages new york ny usa lin training a selection function for extraction in proceedings of cikm pages new york ny usa miller wordnet a lexical database for english commun acm das martins a survey on automatic text summarization literature survey for the language and statistics ii course at cmu barzilay mckeown and elhadad information fusion in the context of multi document summarization in proceedings of acl mckeown and radev generating summaries of multiple news articles in proceedings of sigir pages seattle washington mckeown klavans hatzivassiloglou barzilay and eskin towards multidocument summarization by reformulation progress and prospects in aaai iaai pages marcus marcinkiewicz santorini building large annotated of english the penn treebank a computational linguistics kristina toutanova and christopher manning enriching the knowledge sources used in a maximum entropy part of speech tagger in proceedings of the joint sigdat conference on empirical methods in natural language processing and very large corpora emnlp pp kristina toutanova dan klein christopher manning and yoram singer feature rich part of speech tagging with a cyclic dependency network in proceedings of hlt naacl pp collobert weston june fast semantic extraction using a novel neural network architecture in annual association for computational linguistics vol no kingsbury palmer may from treebank to propbank in lrec ku liang chen march opinion extraction summarization and tracking in news and blog corpora in
