text generation with exemplar based adaptive decoding hao peng ankur p parikh manaal faruqui bhuwan dhingra dipanjan das paul g allen school of computer science engineering university of washington seattle wa google ai language new york ny school of computer science carnegie mellon university pittsburgh pa washington edu aparikh mfaruqui com cmu edu r a l c s c v v i x r a abstract we propose a novel conditioned text ation model it draws inspiration from ditional template based text generation niques where the source provides the tent i e what to say and the template ences how to say it building on the ful encoder decoder paradigm it rst encodes the content representation from the given put text to produce the output it retrieves exemplar text from the training data as soft templates which are then used to construct an exemplar specic decoder we evaluate the proposed model on abstractive text marization and data to text generation pirical results show that this model achieves strong performance and outperforms ble baselines introduction conditioned text generation is the essence of many natural language processing nlp tasks e text summarization mani machine translation koehn and data to text ation kukich mckeown reiter and dale in its common neural sequence sequence formulation sutskever et al cho et al an encoder decoder architecture is used the decoder generates the text sively token by token conditioning on the feature representations encoded from the source typically with attention bahdanau et al and copy mechanisms gu et al see et al this paradigm is capable of generating uent stractive text but in an uncontrolled and times unreliable way often producing degenerate outputs and favoring generic utterances vinyals and le li et al the encoder decoder approach differs template based siderably from earlier work done during internship at google ods becker foster and white iter et al gatt and reiter inter alia where the source content is lled into the slots of a handcrafted template these solutions fer higher generation precision compared to ral approaches wiseman et al but tend to lack the naturalness of neural systems and are less scalable to open domain settings where the number of required templates can be prohibitively large to sidestep the scalability problems with crafted templates it has been proposed to use lar training samples as exemplars to guide the coding process gu et al guu et al weston et al pandey et al cao et al inter alia in general existing methods accomplish this by a using traditional tion retrieval ir techniques for exemplar tion e tf idf and then concatenating the exemplar to the source as additional inputs ing the decoder to attend over and copy from both we propose a different strategy for using plars for motivation figure shows a target pair together with its exemplar from the gaword dataset graff et al the target is a summary of the source sentence and the exemplar is retrieved from the training set there is word overlap between the exemplar and the sired output which would be easily captured by an attention copy mechanism e norway and aid despite this ideally the model should also exploit the structural and stylistic aspects to produce an output with a similar sentence structure even if the words are different indeed in traditional templates the source term exemplar indicates a training instance used to help generation we aim to distinguish from templates since here no explicit procedure is involved use the training target as the exemplar whose source is most similar to the current input describes the details norway said friday it would source give zimbabwe million kroner million dollars million euros in aid to help the country deal with a lack of food and clean drinking water and a cholera outbreak exemplar norway boosts earthquake aid to pakistan target million euros to zimbabwe norway grants aid of figure a source target pair from gigaword training set along with its exemplar is supposed to determine what to say while the templates aim to address how to say it reminiscent of the classical content selection and surface realization pipeline reiter and dale for instance an ideal template for this example might look as follows grants aid of to formulation in the neural it aspect is primarily controlled by the decoder the how to say inspired by the above intuition we propose exemplar based adaptive decoding where a tomized decoder is constructed for each plar this is achieved by letting the exemplars to directly inuence decoder parameters through a reparameterization step the adaptive coder can be used as a drop in replacement in the encoder decoder architecture it offers the tial to better incorporate the exemplars structural and stylistic aspects into decoding without sive increase in the amount of parameters or putational overhead we empirically evaluate our approach on stractive text summarization and data to text eration on which most of the recent efforts on exemplar guided text generation have been ied on three benchmark datasets our approach outperforms comparable baselines and achieves performance competitive with the state of the art the proposed method can be applicable in many other conditioned text generation tasks our plementation is available at washington background this section lays out the necessary background and notations for further technical discussion we begin with conditioned text generation and the encoder decoder framework sutskever et al cho et al in the interest of the tation clarity will use an elman network man as a running example for the decoder which is briey reviewed in the proposed technique generalizes to other neural network chitectures conditioned text generation and the decoder architecture our discussion centers around conditioned text generation i e the model aims to output the target yt given the source input xs both of which are sequences of tokens each token xi yi takes one value from a vocabulary v and y could vary depending on the tasks they will tively be articles and summaries for text rization and for data to text generation would be structured data which can sometimes be earized lebret et al wiseman et al inter alia and y is the output text we aim to learn a parameterized conditional distribution of the target text y given the source y t t where t is the prex of y up to the t token inclusive the probability of each target token is usually estimated with a softmax function exp y exp p t wy denotes a learned vector for token y v depends on y t and x and is computed by a tion which we will describe soon a typical implementation choice for computing ht is the encoder decoder architecture sutskever et al more specically an encoder rst gathers the feature representations from the source then a decoder f is used to compute the ht feature vectors f yt and are respectively the collections of rameters for the encoder and the decoder both of which can be implemented as recurrent ral networks rnns such as lstms iter and schmidhuber or grus cho et al or the transformer vaswani et al in sutskever et al the dependence of f on g is made by using the last hidden state of the encoder as the initial state of the decoder such dependence can be further supplemented with tention bahdanau et al and copy nisms gu et al see et al as we will do in this work introduces how we use exemplars to inform decoding by dynamically constructing the coder s parameters for the notation clarity we will use the elman network as a running example reviewed below elman networks given input sequence an elman network elman computes the den state at time step t from the previous one and the current input token by ht qvt where p and q are learned d d parameter trices with being the hidden dimension and vt is the embedding vector for token we omit the bias term for clarity method this section introduces the proposed method in detail our aim is to use exemplars to inform the decoding procedure i e how to say it to complish this we reparameterize the decoder s rameters with weighted linear sums where the efcients are determined by an exemplar the coder is adaptive in the sense that its parameters vary according to the exemplars the adaptive coder can be used as a drop in replacement in the encoder decoder architecture before going into details let us rst overview the high level tion procedure of our model given source text the model generates an output as follows run a standard encoder to gather the content representations from the source retrieve its exemplar zx and compute exemplar specic coefcients construct the adaptive decoder parameters using the coefcients computed at step then the output is generated by applying the adaptive decoder followed by a softmax just as in any other decoder architecture aiming for a smoother transition we will rst scribe step in and then go back to discuss step in for clarity we shall assume that the decoder is implemented as an elman network man equation the proposed technique generalizes to other neural network architectures as we will discuss later in reparameterizing the rnn decoder at its core the exemplar specic adaptive decoder involves a reparameterization step which we now describe we focus on the parameters of the elman network decoder i e p and q in equation parameter construction with linear sums we aim to reparameterize the pair of matrices p q in a way that they are inuenced by the exemplars let us rst consider an extreme case where one assigns a different pair of parameter matrices to each exemplar without any sharing this leads to an unreasonably large amount of parameters which are difcult to estimate reliably we instead construct p and q from a set of dened parameters matrices take p for example it is computed as the weighted sum of pi matrices r p ipi where pi rdd with being the size of the hidden states r is a hyperparameter determining the number of pi matrices to use the tion is weighted by the coefcients i which are computed from the exemplar zx for clarity the dependence of both p and i on zx is suppressed when the context is clear equation constructs the decoder s parameter matrix p using a linear combination of the exemplar informs this procedure through the coefcients i s the detailed computation of which is deferred to the other matrix q can be similarly constructed by q i iqi constraints in the above formulation the number of parameters is still r times more than a standard elman network which can lead to tting with a limited amount of training data sides it would be more interesting to compare the adaptive decoder to a standard rnn under a parable parameter budget therefore we want to further limit the amount of parameters this can be achieved by forcing the ranks of pi and qi to be since it then takes parameters to form each of them instead of more formally we the amount of parameters grows linearly with the ber of possible exemplars which as we will soon discuss in can be as large as the training set of choosing r empirically we set it equal to in the experiments please see the end of for a related discussion bound their ranks by construction i pi i a denotes the outer product of two vectors are learned dimensional vectors each qi can be similarly constructed by a separate set of vectors qi and i i i i let up vp denote the stack of i i vectors i e up vp r r equations and can be compactly written as p up where is the diagonal matrix built from the dimensional coefcient vector r the construction of q is similar but with a ent set of parameters matrices uq and q uq q note that despite their similarities to svd at a rst glance equations and are not ing matrix factorization rather we are ing up vp uq vq directly p q pi and qi are never explicitly instantiated peng et al to summarize we reparameterize p and q as interpolations of matrices by the fact that b the ranks of p and q are upper bounded by r as pointed out by krueger and memisevic the parameter matrices of a trained rnn tend to have full rank therefore in the experiments we set r equal to the hidden size d aiming to allow the adaptive decoder to use full rank matrices in the recurrent computation yet if one holds a priori beliefs that the matrices should have lower ranks using r could be desirable when r d an adaptive rnn constructed by the above approach has parameters which is comparable to the parameters in a standard elman network bias term in the elman network can be constructed as b b with b being a learned d r matrix does not include the bias term which contributes additional parameters to the former and to the latter incorporating exemplars we now discuss the computation of coefcients through which the exemplars inform the decoder construction equations and before ing the neural network architecture we begin by describing the exemplar retrieval procedure retrieving exemplars zx intuitively similar source texts should hold similar targets fore given source input we use the training target as its exemplar zx whose source is most similar to to compute the similarities between source texts we use bag of words bow features and cosine similarity we extract the plar for each instance this step is part of the processing and we do not change the exemplars as the training proceeds there are of course many other strategies to get the exemplars e using handcrafted or heuristically created hard templates reiter et al becker foster and white ter alia randomly sampling multiple training stances guu et al or learning a neural reranker cao et al using more cally extracted exemplars is denitelly interesting to explore which we defer to future work computing coefcients next we describe the computation of the r dimensional coefcient vector which is used to construct the adaptive coder equations and intuitively the matrices pi and qi s in equation and thereafter can be seen as turing different aspects of the generated text and determines how much each of them contributes to the adaptive decoder construction a natural choice to calculate is to use the similarities tween the exemplar and each of the aspects to accomplish this we run a rnn encoder over zx and use the last hidden state as its vector resentation a we further associate each pi qi pair with a learned vector ci and then i is puted as the similarity between a and ci using an inner product i more compactly ca with c the source of an exemplar is only used in the retrieval and never fed into the encoder decoder model for a training instance we additionally disallow using its own target as the exemplar clarity the dependence of a on the exemplar zx is suppressed just as algorithm adaptive decoder construction procedure retrieve the exemplar zx compute zx s representation a compute coefcients construct the decoder f eq eqs end procedure nyt giga wikibio inst avg len train dev test src tgt k m k k k k k n a closing this section algorithm summarizes the procedure to construct an adaptive decoder discussion although we ve based our discussion on elman networks so far it is straightforward to apply this method to its gated variants hochreiter and schmidhuber cho et al inter alia and other recurrent neural tures bradbury et al vaswani et al peng et al inter alia throughout the periments we will be using an adaptive lstm decoder as a drop in replacement in the encoder decoder architecture it introduces a sonable amount of additional parameters and putational overhead especially when one uses a the sizes small encoder for the exemplar i e of the ci vectors in equation are small it can benet from the highly optimized gpu mentations e since it uses the same recurrent computation as a standard nonadaptive rnn in addition to the neural networks the tive decoder requires access to the full training set due to the retrieval step in this sense it is semi parametric the idea to dynamically struct the parameters is inspired by works ha et al and earlier works therein it proves successful in tasks such as tion jia et al liu et al and machine translation platanios et al many recent template based generation models include the emplars as content in addition to the source and allow the decoder to attend over and copy from both gu et al guu et al weston et al pandey et al cao et al inter alia we compare to this approach in the experiments and show that our model offers nothing prohibits adaptively constructing other nents of the model e the encoder g yet our motivation is to use exemplars to inform how to say it which is ily determined by the decoder in contrast the encoder relates more to selecting the content table number of instances and average text lengths for the datasets used in the experiments the lengths are averaged over training instances vorable performance and that they can potentially be combined to achieve further improvements experiments this section empirically evaluates the proposed model on two sets of text generation tasks stractive summarization and data to text generation before heading into the imental details we rst describe the architectures of the compared models in compared models in addition to previous works we compare to the following baselines aiming to control for founding factors due to detailed implementation choices the encoder decoder ture enhanced with attention and copy anisms the encoder is implemented with a bi directional lstm bilstm hochreiter and schmidhuber schuster and wal graves and the decoder a uni directional one we tie the input dings of both the encoder and the decoder as well as the softmax weights press and wolf we use beam search during tion with length penalty wu et al attexp it is based on it codes attends over and copies from the emplars in addition to the source inputs our model using the adaptive decoder adadec closely builds upon it uses a cally constructed lstm decoder and does not use attention or copy mechanisms over the encoded exemplars the extracted exemplars are the same as those used by attexp to ensure fair parisons we use comparable training procedures and regularization techniques for the above els the readers are referred to the appendix for further details such as hyperparameters text summarization datasets we empirically evaluate our model on two benchmark text summarization datasets annotated gigaword corpus gigaword graff et al napoles et al gaword contains news articles sourced from various news services over the last two decades to produce the dataset we follow the split and preprocessing by rush et al and pair the rst sentences and the headlines in the news articles it results in a train dev split the average lengths of the source and target texts are and respectively new york times annotated corpus it contains news nyt sandaus articles published between and by new york times we use the split and preprocessing by durrett et al following their effort we evaluate on a smaller portion of the test set where the gold summaries are longer than tokens we further randomly sample instances from the training data for validation ing in a train dev split compared to gigaword the inputs and targets in nyt are much longer averaging and respectively table summarizes some statistics of the datasets we note that some recent works use a ent split of the nyt corpus paulus et al gehrmann et al and thus are not ble to the models in table we decide to use the one by durrett et al because their cessing script is publicly available for both datasets we apply byte paired ing bpe sennrich et al which proves to improve the generation of proper nouns fan et al by using adaptive decoders empirical results table compares the models on gigaword test set in rouge lin our model adadec improves over by more than rouge scores cao et al and the full model by cao et al hold the best published results the former uses extensive handcrafted features and relies on external mation extraction and syntactic parsing systems com berkeley doc summarizer of the ofcial script model rg l open nmt cao et al basic cao et al full et al this work this work attexp this work adadec text summarization performance table in rouge scores dubbed as rg x on gigaword test set denotes the models using retrieved exemplars while uses handcrafted features bold font indicates best performance open nmt numbers are taken from cao et al while the latter uses additional encoding tion and copy mechanisms over the exemplars extracted using a novel neural reranker adadec achieves better or comparable performance to the state of the art models without using any handcrafted features or reranking techniques the basic model by cao et al ablates the reranking component from their full model and uses the top exemplar retrieved by the ir system therefore it is a more comparable baseline to ours adadec outperforms it by more than rouge scores surprisingly we do not observe interesting improvements by attexp over the sequence to sequence baseline we believe that our model can benet from better extracted exemplars by e applying a reranking system such exploration is deferred to future work the nyt experimental results are summarized in table we follow previous works and port limited length rouge recall values rett et al is an extractive model and paulus et al an abstractive approach based on forcement learning our adadec model forms both we observe similar trends when paring adadec to the and attexp baselines with the exception that attexp does improve over data to text generation data to text generation aims to generate textual descriptions of structured data which can be following durrett et al and paulus et al we truncate the predictions to the lengths of the gold maries and evaluate rouge recall instead of on length predictions model durrett et al paulus et al this work this work attexp this work adadec table nyt text summarization test performance in rouge recall values this is a smaller portion of the original test data after ltering out instances with summaries shorter than tokens durrett et al denotes the models using retrieved exemplars and bold font indicates best performance seen as a table consisting of a collection of records liang et al for a given entity each record is an attribute value tuple figure shows an example for entity jacques louis david the table species the entity s properties with ples born august nationality french and so forth the table is paired with a description which the model is supposed to generate using the table as input we refer the readers to lebret et al for more details about the task dataset and implementation details we use the wikibio dataset lebret et al it is tomatically constructed by pairing the tables and the opening sentences of biography articles from english wikipedia we follow the split and processing provided along with the dataset with around k train dev instances following lebret et al we linearize the tables such that we can conveniently train the sequence to sequence style models described in table summarizes some statistics of the dataset in contrast to the text summarization ment we do not apply bpe here ther the word embeddings are initialized with glove pennington et al xed during training and not tied with the softmax weights in addition to the models introduced in we additionally compare to ing to study whether the adaptive decoder can ther benet from attention and copy mechanisms over the exemplars empirical results following liu et al we report and bleu scores papineni jacques louis david august december was a french painter in the neoclassical style figure a training instance from the wikibio dataset it consists of a collections of records for jacques louis david top and a piece of textual description bottom et al table summarizes the to text generation results on the wikibio test set overall we observe similar trends to those in the summarization experiment by ing over and copying from the exemplars texp improves upon the baseline by around absolute scores also utilizing emplar information our adadec model performs by a larger margin for and for bleu we further study whether we can get further improvements by bining both achieves around absolute improvements over adadec less than those by attexp over this vides evidence that to some extend the ways texp and adadec incorporate exemplar mation might be complementary wiseman et al is a template motivated model based on a semi markov model liu et al hold the current state of the art results they encode the table structures by using a position and led beddings and structure aware attention and gating techniques these techniques are beyond the scope of this work which focuses mainly on the decoding end use the script by lin to calculate the rouge score and the mteval script for bleu github com moses smt mosesdecoder master scripts generic mteval model bleu wiseman et al liu et al this work this work attexp this work adadec this work table data to text generation performance in and bleu on the wikibio test set indicates the models using retrieved exemplars analysis we now qualitatively evaluate our model by studying how its outputs are affected by using different exemplars figure shows two domly sampled gigaword development instances it compares the outputs by adadec i e without attention copy over exemplars when ing different exemplars controlling for the same in each example exemplar source inputs is retrieved by the system i e a training target while the remaining ones are produced by the authors by modifying the rst one in styles and sometimes introducing distractions in the tent in the top example the model includes ple into the subject three vs three people der the inuence by exemplar exemplar changes the tense and adds some distraction by changing the place from britain to canada the model follows the tense switch but gets fused by the distraction and decides to let a train in southern europe collide into north america which it should not looking at the bottom ple the model in general follows the exemplar in using noun adjuncts or prepositional phrases e new home sales vs sales of new homes except the rst one perhaps confused by the distraction in exemplar the model makes a judgment on the specic amount of growth but gets it wrong related work exemplar based generation partly inspired by traditional template based generation kukich reiter and dale inter alia many cent efforts have been devoted to augmenting text generation models with retrieved exemplars dosh et al mason and charniak song et al lin et al inter alia a portuguese train derailed source in the northern region of oporto on wednesday killing three people exemplar train collision output train derailment two die in a britain three killed in portuguese two people were exemplar killed in britain train collision output portuguese train derailment three people killed in a train collision in exemplar canada killed two people output in northern mexico killing three portuguese train derails sales of new homes in the source u s increased by percent in may the biggest gain in years u s sales of new homes exemplar up strongly in march output percent in may us new home sales rise the sales of new homes exemplar in the u s grow strongly output sales of new homes in us rise in may u s economic new home sales grow by exemplar tics percent output us new home sales grow percent in may figure two randomly sampled gigaword opment instances used for qualitative evaluation exemplar s are retrieved by the system while the remaining ones are produced by the authors notable exemplars changes are highlighted in bold purple and output changes in italic yellow without committing to an explicit cess a typical method is to include exemplars as additional inputs to the sequence to sequence models gu et al pandey et al guu et al inter alia wiseman et al took a different approach and used a semi markov model to learn templates dynamic parameter construction the idea of using a smaller network to generate weights for a larger one dues back to stanley et al and koutnik et al mainly under the evolution computing context it is later revisited with resentation learning moczulski et al nando et al al shedivat et al ter alia and successfully applied to tion jia et al liu et al and machine translation platanios et al it also relates to the meta learning set up thrun and pratt schwenk and yoshua bengio learning phrase representations using rnn encoder decoder in proc of for statistical machine translation emnlp conclusion we presented a text generation model using exemplar informed adaptive decoding it rameterizes the decoder using the information gathered from retrieved exemplars we mented with text summarization and data to text generation and showed that the proposed model achieves strong performance and outperforms comparable baselines on both the proposed model can be applicable in other conditioned text generation tasks we release our tion at cs washington acknowledgments we thank antonios anastasopoulos ming wei chang michael collins jacob devlin yichen gong luheng he kenton lee dianqi li zhouhan lin slav petrov oscar tackstrom kristina toutanova and other members of the google ai language team for the helpful sion and the anonymous reviewers for their able feedback references maruan al shedivat avinava dubey and eric p contextual explanation networks xing dzmitry bahdanau kyunghyun cho and yoshua gio neural machine translation by jointly learning to align and translate in proc of iclr tilman becker practical template based ral language generation with tag in proceedings of the sixth international workshop on tree adjoining grammar and related frameworks james bradbury stephen merity caiming xiong and richard socher quasi recurrent neural work in proc of iclr greg durrett taylor berg kirkpatrick and dan klein learning based single document tion with compression and anaphoricity constraints in proc of acl jeffrey l elman finding structure in time cognitive science angela fan david grangier and michael auli in controllable abstractive summarization ceedings of the workshop on neural machine translation and generation chrisantha fernando dylan banarse malcolm reynolds frederic besse david pfau max berg marc lanctot and daan wierstra convolution by evolution differentiable pattern producing networks in proceedings of the genetic and evolutionary computation conference mary ellen foster and michael white niques for text planning with xslt in proceeedings of the workshop on nlp and xml rdf rdfs and owl in language technology albert gatt and ehud reiter simplenlg a in alisation engine for practical applications ceedings of the european workshop on natural language generation sebastian gehrmann yuntian deng and alexander m rush bottom up abstractive summarization in proc of emnlp david graff junbo kong ke chen and kazuaki maeda english gigaword second edition alex graves supervised sequence labelling with recurrent neural networks volume of studies in computational intelligence springer jiatao gu zhengdong lu hang li and victor ok incorporating copying mechanism in li sequence to sequence learning in proc of acl jiatao gu yong wang kyunghyun cho and search engine guided in proc of tor ok li parametric neural machine translation aaai ziqiang cao wenjie li sujian li and furu wei retrieve rerank and rewrite soft template based neural summarization in proc of acl kelvin guu tatsunori b hashimoto yonatan oren and percy liang generating sentences by editing prototypes tacl ziqiang cao furu wei wenjie li and sujian li faithful to the original fact aware neural abstractive summarization in proc of aaai kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger david ha andrew dai and quoc v le networks in proc of iclr kaiming he xiangyu zhang shaoqing ren and jian sun deep residual learning for image nition in proc of cvpr sepp hochreiter and jurgen schmidhuber neural computation long short term memory micah hodosh peter young and julia hockenmaier framing image description as a ranking task data models and evaluation metrics jair xu jia bert de brabandere tinne tuytelaars and in luc v gool dynamic lter networks proc of neurips diederik kingma and jimmy ba adam a in proc of method for stochastic optimization iclr durk p kingma tim salimans and max welling variational dropout and the local terization trick in proc of neurips philipp koehn statistical machine translation cambridge university press jan koutnik faustino gomez and jurgen ber evolving neural networks in compressed weight space in proceedings of the annual ence on genetic and evolutionary computation david krueger and roland memisevic larizing rnns by stabilizing activations in proc of iclr karen kukich design of a knowledge based port generator in proc of acl remi lebret david grangier and michael auli neural text generation from structured data with in proc of application to the biography domain emnlp jiwei li michel galley chris brockett jianfeng gao and bill dolan a diversity promoting jective function for neural conversation models in proc of naacl percy liang michael i jordan and dan klein learning semantic correspondences with less vision in proc of acl chin yew lin rouge a package for automatic in text summarization evaluation of summaries branches out proceedings of the shop kevin lin dianqi li xiaodong he zhengyou zhang and ming ting sun adversarial ranking for language generation in proc of neurips minh thang luong hieu pham and christopher d manning effective approaches to in proc of based neural machine translation emnlp inderjeet mani advances in automatic text marization mit press rebecca mason and eugene charniak specic image captioning in proc of conll kathleen mckeown text generation bridge university press marcin moczulski misha denil jeremy appleyard and nando de freitas acdc a structured efcient linear layer courtney napoles matthew gormley and benjamin in van durme annotated gigaword ceedings of the joint workshop on automatic edge base construction and web scale knowledge extraction gaurav pandey danish contractor vineet kumar and sachindra joshi exemplar encoder decoder for neural conversation generation in proc of acl kishore papineni salim roukos todd ward and wei jing zhu bleu a method for automatic uation of machine translation in proc of acl romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization in proc of iclr hao peng roy schwartz sam thomson and noah a smith rational recurrences in in proc of emnlp hao peng sam thomson and noah a smith deep multitask learning for semantic dependency parsing in proc of acl hao peng sam thomson and noah a smith backpropagating through structured argmax using a spigot in proc of acl hao peng sam thomson swabha swayamdipta and noah a smith learning joint semantic parsers from disjoint data in proc of naacl jeffrey pennington richard socher and pher d manning glove global vectors for word representation in proc of emnlp emmanouil antonios platanios mrinmaya sachan graham neubig and tom mitchell tual parameter generation for universal neural chine translation in proc of emnlp pengfei liu xipeng qiu and xuanjing huang dynamic compositional neural networks over tree structure in proc of ijcai or press and lior wolf using the output in proc of bedding to improve language models eacl tianyu liu kexiang wang lei sha baobao chang and zhifang sui table to text generation by structure aware learning in proc of aaai ehud reiter and robert dale building applied natural language generation systems natural guage engineering yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin johnson xiaobing liu ukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith son riesa alex rudnick oriol vinyals greg rado macduff hughes and jeffrey dean google s neural machine translation system ing the gap between human and machine translation ehud reiter somayajulu sripada jim hunter jin yu and ian davy choosing words in generated weather forecasts articial intelligence alexander m rush sumit chopra and jason weston a neural attention model for abstractive tence summarization in proc of emnlp evan sandaus the new york times annotated corpus ldc corpora linguistic data consortium m schuster and k k paliwal bidirectional recurrent neural networks transactions on signal proccesing abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proc of acl rico sennrich barry haddow and alexandra birch neural machine translation of rare words with subword units in proc of acl yiping song rui yan xiang li dongyan zhao and ming zhang two are better than one an ensemble of and generation based dialog systems nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever and ruslan salakhutdinov dropout a simple way to prevent neural networks from overtting jmlr k o stanley d b dambrosio and j gauci a hypercube based encoding for evolving scale neural networks articial life ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural works in proc of neurips sebastian thrun and lorien pratt editors learning to learn kluwer academic publishers ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in proc of neurips oriol vinyals and quoc le a neural tional model in proc of icml jason weston emily dinan and alexander miller retrieve and rene improved sequence eration models for dialogue in proceedings of the international workshop on search oriented versational ai sam wiseman stuart shieber and alexander rush challenges in data to document generation in proc of emnlp sam wiseman stuart m shieber and alexander m rush learning neural templates for text eration in proc of emnlp and further truncate the source to the rst units during evaluation we apply beam search of width with a length penalty wu et al a data to text generation we in general follow the implementation details in the summarization experiment with the following modications we do not apply byte paired encoding here and use a vocabulary of size k the word embeddings are initialized using version dimensional glove et al and xed during ing further the softmax weights are not tied to the embeddings three layer dimensional bilstm coders are used with residual connections the exemplar encoder uses an one layer dimensional bilstm early stopping is applied based on ment set performance a maximum decoding length of tokens is used appendices a implementation details our implementation is based on tensorflow for both experiments we use the similar plementation strategies for the baselines and our model aiming for a fair comparison a text summarization we train the models using adam kingma and ba with a batch size of we use the default values in tensorflow adam implementation for initial learning rate and the models are trained for up to epochs with the learning rate annealed at a rate of every epochs a weight decay of is applied to all ters with being the current learning rate the norms of gradients are clipped to early ping is applied based on rouge l performance on the development set for the weights of the output softmax function are tied with the word embeddings which are the encoders we randomly initialized use dimensional layer bilstms in the gigaword experiment and dimensional layer bilstms in the nyt experiment both with residual connections he et al the decoders are one layer adaptive lstms and have the same size as the encoders and so do the word embeddings we apply variational dropout kingma et al in the encoder rnns and dropout srivastava et al in the embeddings and the softmax layer the rates of which are empirically selected from the last hidden state at the top layer of the encoder is fed through an one layer tanh mlp and then used to as the decoder s tial state we use the attention function by luong et al and copy mechanism by see et al the exemplar encoder uses layer bilstm for gigaword and nyt periments respectively for numerical stability equation is scaled to have norms of d with being the hidden size of the adaptive coder peng et al in the gigaword experiment we use k bpe types and limit the maximum decoding length to be subword units while for nyt we use k types with a maximum decoding length of tensorflow
