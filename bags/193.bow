text generation exemplar based adaptive decoding hao peng ankur p parikh manaal faruqui bhuwan dhingra dipanjan das paul g allen school computer science engineering university washington seattle wa google ai language new york ny school computer science carnegie mellon university pittsburgh pa washington edu aparikh mfaruqui com cmu edu r l c s c v v x r abstract propose novel conditioned text ation model draws inspiration ditional template based text generation niques source provides tent e template ences building ful encoder decoder paradigm rst encodes content representation given text produce output retrieves exemplar text training data soft templates construct exemplar specic decoder evaluate proposed model abstractive text marization data text generation pirical results model achieves strong performance outperforms ble baselines introduction conditioned text generation essence natural language processing nlp tasks e text summarization mani machine translation koehn data text ation kukich mckeown reiter dale common neural sequence sequence formulation sutskever et al cho et al encoder decoder architecture decoder generates text sively token token conditioning feature representations encoded source typically attention bahdanau et al copy mechanisms gu et al et al paradigm capable generating uent stractive text uncontrolled times unreliable way producing degenerate outputs favoring generic utterances vinyals le li et al encoder decoder approach differs template based siderably earlier work internship google ods becker foster white iter et al gatt reiter inter alia source content lled slots handcrafted template solutions fer higher generation precision compared ral approaches wiseman et al tend lack naturalness neural systems scalable open domain settings number required templates prohibitively large sidestep scalability problems crafted templates proposed use lar training samples exemplars guide coding process gu et al guu et al weston et al pandey et al cao et al inter alia general existing methods accomplish traditional tion retrieval ir techniques exemplar tion e tf idf concatenating exemplar source additional inputs ing decoder attend copy propose different strategy plars motivation figure shows target pair exemplar gaword dataset graff et al target summary source sentence exemplar retrieved training set word overlap exemplar sired output easily captured attention copy mechanism e norway aid despite ideally model exploit structural stylistic aspects produce output similar sentence structure words different traditional templates source term exemplar indicates training instance help generation aim distinguish templates explicit procedure involved use training target exemplar source similar current input describes details norway said friday source zimbabwe million kroner million dollars million euros aid help country deal lack food clean drinking water cholera outbreak exemplar norway boosts earthquake aid pakistan target million euros zimbabwe norway grants aid figure source target pair gigaword training set exemplar supposed determine templates aim address reminiscent classical content selection surface realization pipeline reiter dale instance ideal template example look follows grants aid formulation neural aspect primarily controlled decoder inspired intuition propose exemplar based adaptive decoding tomized decoder constructed plar achieved letting exemplars directly inuence decoder parameters reparameterization step adaptive coder drop replacement encoder decoder architecture offers tial better incorporate exemplars structural stylistic aspects decoding sive increase parameters putational overhead empirically evaluate approach stractive text summarization data text eration recent efforts exemplar guided text generation ied benchmark datasets approach outperforms comparable baselines achieves performance competitive state art proposed method applicable conditioned text generation tasks plementation available washington background section lays necessary background notations technical discussion begin conditioned text generation encoder decoder framework sutskever et al cho et al interest tation clarity use elman network man running example decoder briey reviewed proposed technique generalizes neural network chitectures conditioned text generation decoder architecture discussion centers conditioned text generation e model aims output target yt given source input xs sequences tokens token xi yi takes value vocabulary v y vary depending tasks tively articles summaries text rization data text generation structured data earized lebret et al wiseman et al inter alia y output text aim learn parameterized conditional distribution target text y given source y t t t prex y t token inclusive probability target token usually estimated softmax function exp y exp p t wy denotes learned vector token y v depends y t x computed tion describe soon typical implementation choice computing ht encoder decoder architecture sutskever et al specically encoder rst gathers feature representations source decoder f compute ht feature vectors f yt respectively collections rameters encoder decoder implemented recurrent ral networks rnns lstms iter schmidhuber grus cho et al transformer vaswani et al sutskever et al dependence f g hidden state encoder initial state decoder dependence supplemented tention bahdanau et al copy nisms gu et al et al work introduces use exemplars inform decoding dynamically constructing coder s parameters notation clarity use elman network running example reviewed elman networks given input sequence elman network elman computes den state time step t previous current input token ht qvt p q learned d d parameter trices hidden dimension vt embedding vector token omit bias term clarity method section introduces proposed method detail aim use exemplars inform decoding procedure e complish reparameterize decoder s rameters weighted linear sums efcients determined exemplar coder adaptive sense parameters vary according exemplars adaptive coder drop replacement encoder decoder architecture going details let rst overview high level tion procedure model given source text model generates output follows run standard encoder gather content representations source retrieve exemplar zx compute exemplar specic coefcients construct adaptive decoder parameters coefcients computed step output generated applying adaptive decoder followed softmax decoder architecture aiming smoother transition rst scribe step discuss step clarity shall assume decoder implemented elman network man equation proposed technique generalizes neural network architectures discuss later reparameterizing rnn decoder core exemplar specic adaptive decoder involves reparameterization step describe focus parameters elman network decoder e p q equation parameter construction linear sums aim reparameterize pair matrices p q way inuenced exemplars let rst consider extreme case assigns different pair parameter matrices exemplar sharing leads unreasonably large parameters difcult estimate reliably instead construct p q set dened parameters matrices p example computed weighted sum pi matrices r p ipi pi rdd size hidden states r hyperparameter determining number pi matrices use tion weighted coefcients computed exemplar zx clarity dependence p zx suppressed context clear equation constructs decoder s parameter matrix p linear combination exemplar informs procedure coefcients s detailed computation deferred matrix q similarly constructed q iqi constraints formulation number parameters r times standard elman network lead tting limited training data sides interesting compare adaptive decoder standard rnn parable parameter budget want limit parameters achieved forcing ranks pi qi takes parameters form instead formally parameters grows linearly ber possible exemplars soon discuss large training set choosing r empirically set equal experiments end related discussion bound ranks construction pi denotes outer product vectors learned dimensional vectors qi similarly constructed separate set vectors qi let vp denote stack vectors e vp r r equations compactly written p diagonal matrix built dimensional coefcient vector r construction q similar ent set parameters matrices uq q uq q note despite similarities svd rst glance equations ing matrix factorization ing vp uq vq directly p q pi qi explicitly instantiated peng et al summarize reparameterize p q interpolations matrices fact b ranks p q upper bounded r pointed krueger memisevic parameter matrices trained rnn tend rank experiments set r equal hidden size d aiming allow adaptive decoder use rank matrices recurrent computation holds priori beliefs matrices lower ranks r desirable r d adaptive rnn constructed approach parameters comparable parameters standard elman network bias term elman network constructed b b b learned d r matrix include bias term contributes additional parameters incorporating exemplars discuss computation coefcients exemplars inform decoder construction equations ing neural network architecture begin describing exemplar retrieval procedure retrieving exemplars zx intuitively similar source texts hold similar targets fore given source input use training target exemplar zx source similar compute similarities source texts use bag words bow features cosine similarity extract plar instance step processing change exemplars training proceeds course strategies exemplars e handcrafted heuristically created hard templates reiter et al becker foster white ter alia randomly sampling multiple training stances guu et al learning neural reranker cao et al cally extracted exemplars denitelly interesting explore defer future work computing coefcients describe computation r dimensional coefcient vector construct adaptive coder equations intuitively matrices pi qi s equation seen turing different aspects generated text determines contributes adaptive decoder construction natural choice calculate use similarities tween exemplar aspects accomplish run rnn encoder zx use hidden state vector resentation associate pi qi pair learned vector ci puted similarity ci inner product compactly c source exemplar retrieval fed encoder decoder model training instance additionally disallow target exemplar clarity dependence exemplar zx suppressed algorithm adaptive decoder construction procedure retrieve exemplar zx compute zx s representation compute coefcients construct decoder f eq eqs end procedure nyt giga wikibio inst avg len train dev test src tgt k m k k k k k n closing section algorithm summarizes procedure construct adaptive decoder discussion ve based discussion elman networks far straightforward apply method gated variants hochreiter schmidhuber cho et al inter alia recurrent neural tures bradbury et al vaswani et al peng et al inter alia periments adaptive lstm decoder drop replacement encoder decoder architecture introduces sonable additional parameters putational overhead especially uses sizes small encoder exemplar e ci vectors equation small benet highly optimized gpu mentations e uses recurrent computation standard nonadaptive rnn addition neural networks tive decoder requires access training set retrieval step sense semi parametric idea dynamically struct parameters inspired works ha et al earlier works proves successful tasks tion jia et al liu et al machine translation platanios et al recent template based generation models include emplars content addition source allow decoder attend copy gu et al guu et al weston et al pandey et al cao et al inter alia compare approach experiments model offers prohibits adaptively constructing nents model e encoder g motivation use exemplars inform ily determined decoder contrast encoder relates selecting content table number instances average text lengths datasets experiments lengths averaged training instances vorable performance potentially combined achieve improvements experiments section empirically evaluates proposed model sets text generation tasks stractive summarization data text generation heading imental details rst describe architectures compared models compared models addition previous works compare following baselines aiming control founding factors detailed implementation choices encoder decoder ture enhanced attention copy anisms encoder implemented bi directional lstm bilstm hochreiter schmidhuber schuster wal graves decoder uni directional tie input dings encoder decoder softmax weights press wolf use beam search tion length penalty wu et al attexp based codes attends copies emplars addition source inputs model adaptive decoder adadec closely builds uses cally constructed lstm decoder use attention copy mechanisms encoded exemplars extracted exemplars attexp ensure fair parisons use comparable training procedures regularization techniques els readers referred appendix details hyperparameters text summarization datasets empirically evaluate model benchmark text summarization datasets annotated gigaword corpus gigaword graff et al napoles et al gaword contains news articles sourced news services decades produce dataset follow split preprocessing rush et al pair rst sentences headlines news articles results train dev split average lengths source target texts respectively new york times annotated corpus contains news nyt sandaus articles published new york times use split preprocessing durrett et al following effort evaluate smaller portion test set gold summaries longer tokens randomly sample instances training data validation ing train dev split compared gigaword inputs targets nyt longer averaging respectively table summarizes statistics datasets note recent works use ent split nyt corpus paulus et al gehrmann et al ble models table decide use durrett et al cessing script publicly available datasets apply byte paired ing bpe sennrich et al proves improve generation proper nouns fan et al adaptive decoders empirical results table compares models gigaword test set rouge lin model adadec improves rouge scores cao et al model cao et al hold best published results uses extensive handcrafted features relies external mation extraction syntactic parsing systems com berkeley doc summarizer ofcial script model rg l open nmt cao et al basic cao et al et al work work attexp work adadec text summarization performance table rouge scores dubbed rg x gigaword test set denotes models retrieved exemplars uses handcrafted features bold font indicates best performance open nmt numbers taken cao et al uses additional encoding tion copy mechanisms exemplars extracted novel neural reranker adadec achieves better comparable performance state art models handcrafted features reranking techniques basic model cao et al ablates reranking component model uses exemplar retrieved ir system comparable baseline adadec outperforms rouge scores surprisingly observe interesting improvements attexp sequence sequence baseline believe model benet better extracted exemplars e applying reranking system exploration deferred future work nyt experimental results summarized table follow previous works port limited length rouge recall values rett et al extractive model paulus et al abstractive approach based forcement learning adadec model forms observe similar trends paring adadec attexp baselines exception attexp improve data text generation data text generation aims generate textual descriptions structured data following durrett et al paulus et al truncate predictions lengths gold maries evaluate rouge recall instead length predictions model durrett et al paulus et al work work attexp work adadec table nyt text summarization test performance rouge recall values smaller portion original test data ltering instances summaries shorter tokens durrett et al denotes models retrieved exemplars bold font indicates best performance seen table consisting collection records liang et al given entity record attribute value tuple figure shows example entity jacques louis david table species entity s properties ples born august nationality french forth table paired description model supposed generate table input refer readers lebret et al details task dataset implementation details use wikibio dataset lebret et al tomatically constructed pairing tables opening sentences biography articles english wikipedia follow split processing provided dataset k train dev instances following lebret et al linearize tables conveniently train sequence sequence style models described table summarizes statistics dataset contrast text summarization ment apply bpe ther word embeddings initialized glove pennington et al xed training tied softmax weights addition models introduced additionally compare ing study adaptive decoder ther benet attention copy mechanisms exemplars empirical results following liu et al report bleu scores papineni jacques louis david august december french painter neoclassical style figure training instance wikibio dataset consists collections records jacques louis david piece textual description et al table summarizes text generation results wikibio test set overall observe similar trends summarization experiment ing copying exemplars texp improves baseline absolute scores utilizing emplar information adadec model performs larger margin bleu study improvements bining achieves absolute improvements adadec attexp vides evidence extend ways texp adadec incorporate exemplar mation complementary wiseman et al template motivated model based semi markov model liu et al hold current state art results encode table structures position led beddings structure aware attention gating techniques techniques scope work focuses mainly decoding end use script lin calculate rouge score mteval script bleu github com moses smt mosesdecoder master scripts generic mteval model bleu wiseman et al liu et al work work attexp work adadec work table data text generation performance bleu wikibio test set indicates models retrieved exemplars analysis qualitatively evaluate model studying outputs affected different exemplars figure shows domly sampled gigaword development instances compares outputs adadec e attention copy exemplars ing different exemplars controlling example exemplar source inputs retrieved system e training target remaining ones produced authors modifying rst styles introducing distractions tent example model includes ple subject vs people der inuence exemplar exemplar changes tense adds distraction changing place britain canada model follows tense switch gets fused distraction decides let train southern europe collide north america looking ple model general follows exemplar noun adjuncts prepositional phrases e new home sales vs sales new homes rst confused distraction exemplar model makes judgment specic growth gets wrong related work exemplar based generation partly inspired traditional template based generation kukich reiter dale inter alia cent efforts devoted augmenting text generation models retrieved exemplars dosh et al mason charniak song et al lin et al inter alia portuguese train derailed source northern region oporto wednesday killing people exemplar train collision output train derailment die britain killed portuguese people exemplar killed britain train collision output portuguese train derailment people killed train collision exemplar canada killed people output northern mexico killing portuguese train derails sales new homes source u s increased percent biggest gain years u s sales new homes exemplar strongly march output percent new home sales rise sales new homes exemplar u s grow strongly output sales new homes rise u s economic new home sales grow exemplar tics percent output new home sales grow percent figure randomly sampled gigaword opment instances qualitative evaluation exemplar s retrieved system remaining ones produced authors notable exemplars changes highlighted bold purple output changes italic yellow committing explicit cess typical method include exemplars additional inputs sequence sequence models gu et al pandey et al guu et al inter alia wiseman et al took different approach semi markov model learn templates dynamic parameter construction idea smaller network generate weights larger dues stanley et al koutnik et al mainly evolution computing context later revisited resentation learning moczulski et al nando et al al shedivat et al ter alia successfully applied tion jia et al liu et al machine translation platanios et al relates meta learning set thrun pratt schwenk yoshua bengio learning phrase representations rnn encoder decoder proc statistical machine translation emnlp conclusion presented text generation model exemplar informed adaptive decoding rameterizes decoder information gathered retrieved exemplars mented text summarization data text generation showed proposed model achieves strong performance outperforms comparable baselines proposed model applicable conditioned text generation tasks release tion cs washington acknowledgments thank antonios anastasopoulos ming wei chang michael collins jacob devlin yichen gong luheng kenton lee dianqi li zhouhan lin slav petrov oscar tackstrom kristina toutanova members google ai language team helpful sion anonymous reviewers able feedback references maruan al shedivat avinava dubey eric p contextual explanation networks xing dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly learning align translate proc iclr tilman becker practical template based ral language generation tag proceedings sixth international workshop tree adjoining grammar related frameworks james bradbury stephen merity caiming xiong richard socher quasi recurrent neural work proc iclr greg durrett taylor berg kirkpatrick dan klein learning based single document tion compression anaphoricity constraints proc acl jeffrey l elman finding structure time cognitive science angela fan david grangier michael auli controllable abstractive summarization ceedings workshop neural machine translation generation chrisantha fernando dylan banarse malcolm reynolds frederic besse david pfau max berg marc lanctot daan wierstra convolution evolution differentiable pattern producing networks proceedings genetic evolutionary computation conference mary ellen foster michael white niques text planning xslt proceeedings workshop nlp xml rdf rdfs owl language technology albert gatt ehud reiter simplenlg alisation engine practical applications ceedings european workshop natural language generation sebastian gehrmann yuntian deng alexander m rush abstractive summarization proc emnlp david graff junbo kong ke chen kazuaki maeda english gigaword second edition alex graves supervised sequence labelling recurrent neural networks volume studies computational intelligence springer jiatao gu zhengdong lu hang li victor ok incorporating copying mechanism li sequence sequence learning proc acl jiatao gu yong wang kyunghyun cho search engine guided proc tor ok li parametric neural machine translation aaai ziqiang cao wenjie li sujian li furu wei retrieve rerank rewrite soft template based neural summarization proc acl kelvin guu tatsunori b hashimoto yonatan oren percy liang generating sentences editing prototypes tacl ziqiang cao furu wei wenjie li sujian li faithful original fact aware neural abstractive summarization proc aaai kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger david ha andrew dai quoc v le networks proc iclr kaiming xiangyu zhang shaoqing ren jian sun deep residual learning image nition proc cvpr sepp hochreiter jurgen schmidhuber neural computation long short term memory micah hodosh peter young julia hockenmaier framing image description ranking task data models evaluation metrics jair xu jia bert de brabandere tinne tuytelaars luc v gool dynamic lter networks proc neurips diederik kingma jimmy ba adam proc method stochastic optimization iclr durk p kingma tim salimans max welling variational dropout local terization trick proc neurips philipp koehn statistical machine translation cambridge university press jan koutnik faustino gomez jurgen ber evolving neural networks compressed weight space proceedings annual ence genetic evolutionary computation david krueger roland memisevic larizing rnns stabilizing activations proc iclr karen kukich design knowledge based port generator proc acl remi lebret david grangier michael auli neural text generation structured data proc application biography domain emnlp jiwei li michel galley chris brockett jianfeng gao bill dolan diversity promoting jective function neural conversation models proc naacl percy liang michael jordan dan klein learning semantic correspondences vision proc acl chin yew lin rouge package automatic text summarization evaluation summaries branches proceedings shop kevin lin dianqi li xiaodong zhengyou zhang ming ting sun adversarial ranking language generation proc neurips minh thang luong hieu pham christopher d manning effective approaches proc based neural machine translation emnlp inderjeet mani advances automatic text marization mit press rebecca mason eugene charniak specic image captioning proc conll kathleen mckeown text generation bridge university press marcin moczulski misha denil jeremy appleyard nando de freitas acdc structured efcient linear layer courtney napoles matthew gormley benjamin van durme annotated gigaword ceedings joint workshop automatic edge base construction web scale knowledge extraction gaurav pandey danish contractor vineet kumar sachindra joshi exemplar encoder decoder neural conversation generation proc acl kishore papineni salim roukos todd ward wei jing zhu bleu method automatic uation machine translation proc acl romain paulus caiming xiong richard socher deep reinforced model abstractive marization proc iclr hao peng roy schwartz sam thomson noah smith rational recurrences proc emnlp hao peng sam thomson noah smith deep multitask learning semantic dependency parsing proc acl hao peng sam thomson noah smith backpropagating structured argmax spigot proc acl hao peng sam thomson swabha swayamdipta noah smith learning joint semantic parsers disjoint data proc naacl jeffrey pennington richard socher pher d manning glove global vectors word representation proc emnlp emmanouil antonios platanios mrinmaya sachan graham neubig tom mitchell tual parameter generation universal neural chine translation proc emnlp pengfei liu xipeng qiu xuanjing huang dynamic compositional neural networks tree structure proc ijcai press lior wolf output proc bedding improve language models eacl tianyu liu kexiang wang lei sha baobao chang zhifang sui table text generation structure aware learning proc aaai ehud reiter robert dale building applied natural language generation systems natural guage engineering yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin johnson xiaobing liu ukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith son riesa alex rudnick oriol vinyals greg rado macduff hughes jeffrey dean google s neural machine translation system ing gap human machine translation ehud reiter somayajulu sripada jim hunter jin yu ian davy choosing words generated weather forecasts articial intelligence alexander m rush sumit chopra jason weston neural attention model abstractive tence summarization proc emnlp evan sandaus new york times annotated corpus ldc corpora linguistic data consortium m schuster k k paliwal bidirectional recurrent neural networks transactions signal proccesing abigail peter j liu christopher d manning point summarization generator networks proc acl rico sennrich barry haddow alexandra birch neural machine translation rare words subword units proc acl yiping song rui yan xiang li dongyan zhao ming zhang better ensemble generation based dialog systems nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov dropout simple way prevent neural networks overtting jmlr k o stanley d b dambrosio j gauci hypercube based encoding evolving scale neural networks articial life ilya sutskever oriol vinyals quoc v le sequence sequence learning neural works proc neurips sebastian thrun lorien pratt editors learning learn kluwer academic publishers ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention need proc neurips oriol vinyals quoc le neural tional model proc icml jason weston emily dinan alexander miller retrieve rene improved sequence eration models dialogue proceedings international workshop search oriented versational ai sam wiseman stuart shieber alexander rush challenges data document generation proc emnlp sam wiseman stuart m shieber alexander m rush learning neural templates text eration proc emnlp truncate source rst units evaluation apply beam search width length penalty wu et al data text generation general follow implementation details summarization experiment following modications apply byte paired encoding use vocabulary size k word embeddings initialized version dimensional glove et al xed ing softmax weights tied embeddings layer dimensional bilstm coders residual connections exemplar encoder uses layer dimensional bilstm early stopping applied based ment set performance maximum decoding length tokens appendices implementation details implementation based tensorflow experiments use similar plementation strategies baselines model aiming fair comparison text summarization train models adam kingma ba batch size use default values tensorflow adam implementation initial learning rate models trained epochs learning rate annealed rate epochs weight decay applied ters current learning rate norms gradients clipped early ping applied based rouge l performance development set weights output softmax function tied word embeddings encoders randomly initialized use dimensional layer bilstms gigaword experiment dimensional layer bilstms nyt experiment residual connections et al decoders layer adaptive lstms size encoders word embeddings apply variational dropout kingma et al encoder rnns dropout srivastava et al embeddings softmax layer rates empirically selected hidden state layer encoder fed layer tanh mlp decoder s tial state use attention function luong et al copy mechanism et al exemplar encoder uses layer bilstm gigaword nyt periments respectively numerical stability equation scaled norms d hidden size adaptive coder peng et al gigaword experiment use k bpe types limit maximum decoding length subword units nyt use k types maximum decoding length tensorflow
