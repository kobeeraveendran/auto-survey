text generation exemplar based adaptive decoding hao peng ankur parikh manaal faruqui bhuwan dhingra dipanjan das paul allen school computer science engineering university washington seattle google language new york school computer science carnegie mellon university pittsburgh washington edu aparikh mfaruqui com cmu edu abstract propose novel conditioned text ation model draws inspiration ditional template based text generation niques source provides tent template ences building ful encoder decoder paradigm rst encodes content representation given text produce output retrieves exemplar text training data soft templates construct exemplar specic decoder evaluate proposed model abstractive text marization data text generation pirical results model achieves strong performance outperforms ble baselines introduction conditioned text generation essence natural language processing nlp tasks text summarization mani machine translation koehn data text ation kukich mckeown reiter dale common neural sequence sequence formulation sutskever cho encoder decoder architecture decoder generates text sively token token conditioning feature representations encoded source typically attention bahdanau copy mechanisms paradigm capable generating uent stractive text uncontrolled times unreliable way producing degenerate outputs favoring generic utterances vinyals encoder decoder approach differs template based siderably earlier work internship google ods becker foster white iter gatt reiter inter alia source content lled slots handcrafted template solutions fer higher generation precision compared ral approaches wiseman tend lack naturalness neural systems scalable open domain settings number required templates prohibitively large sidestep scalability problems crafted templates proposed use lar training samples exemplars guide coding process guu weston pandey cao inter alia general existing methods accomplish traditional tion retrieval techniques exemplar tion idf concatenating exemplar source additional inputs ing decoder attend copy propose different strategy plars motivation figure shows target pair exemplar gaword dataset graff target summary source sentence exemplar retrieved training set word overlap exemplar sired output easily captured attention copy mechanism norway aid despite ideally model exploit structural stylistic aspects produce output similar sentence structure words different traditional templates source term exemplar indicates training instance help generation aim distinguish templates explicit procedure involved use training target exemplar source similar current input describes details norway said friday source zimbabwe million kroner million dollars million euros aid help country deal lack food clean drinking water cholera outbreak exemplar norway boosts earthquake aid pakistan target million euros zimbabwe norway grants aid figure source target pair gigaword training set exemplar supposed determine templates aim address reminiscent classical content selection surface realization pipeline reiter dale instance ideal template example look follows grants aid formulation neural aspect primarily controlled decoder inspired intuition propose exemplar based adaptive decoding tomized decoder constructed plar achieved letting exemplars directly inuence decoder parameters reparameterization step adaptive coder drop replacement encoder decoder architecture offers tial better incorporate exemplars structural stylistic aspects decoding sive increase parameters putational overhead empirically evaluate approach stractive text summarization data text eration recent efforts exemplar guided text generation ied benchmark datasets approach outperforms comparable baselines achieves performance competitive state art proposed method applicable conditioned text generation tasks plementation available washington background section lays necessary background notations technical discussion begin conditioned text generation encoder decoder framework sutskever cho interest tation clarity use elman network man running example decoder briey reviewed proposed technique generalizes neural network chitectures conditioned text generation decoder architecture discussion centers conditioned text generation model aims output target given source input sequences tokens token takes value vocabulary vary depending tasks tively articles summaries text rization data text generation structured data earized lebret wiseman inter alia output text aim learn parameterized conditional distribution target text given source prex token inclusive probability target token usually estimated softmax function exp exp denotes learned vector token depends computed tion describe soon typical implementation choice computing encoder decoder architecture sutskever specically encoder rst gathers feature representations source decoder compute feature vectors respectively collections rameters encoder decoder implemented recurrent ral networks rnns lstms iter schmidhuber grus cho transformer vaswani sutskever dependence hidden state encoder initial state decoder dependence supplemented tention bahdanau copy nisms work introduces use exemplars inform decoding dynamically constructing coder parameters notation clarity use elman network running example reviewed elman networks given input sequence elman network elman computes den state time step previous current input token qvt learned parameter trices hidden dimension embedding vector token omit bias term clarity method section introduces proposed method detail aim use exemplars inform decoding procedure complish reparameterize decoder rameters weighted linear sums efcients determined exemplar coder adaptive sense parameters vary according exemplars adaptive coder drop replacement encoder decoder architecture going details let rst overview high level tion procedure model given source text model generates output follows run standard encoder gather content representations source retrieve exemplar compute exemplar specic coefcients construct adaptive decoder parameters coefcients computed step output generated applying adaptive decoder followed softmax decoder architecture aiming smoother transition rst scribe step discuss step clarity shall assume decoder implemented elman network man equation proposed technique generalizes neural network architectures discuss later reparameterizing rnn decoder core exemplar specic adaptive decoder involves reparameterization step describe focus parameters elman network decoder equation parameter construction linear sums aim reparameterize pair matrices way inuenced exemplars let rst consider extreme case assigns different pair parameter matrices exemplar sharing leads unreasonably large parameters difcult estimate reliably instead construct set dened parameters matrices example computed weighted sum matrices ipi rdd size hidden states hyperparameter determining number matrices use tion weighted coefcients computed exemplar clarity dependence suppressed context clear equation constructs decoder parameter matrix linear combination exemplar informs procedure coefcients detailed computation deferred matrix similarly constructed iqi constraints formulation number parameters times standard elman network lead tting limited training data sides interesting compare adaptive decoder standard rnn parable parameter budget want limit parameters achieved forcing ranks takes parameters form instead formally parameters grows linearly ber possible exemplars soon discuss large training set choosing empirically set equal experiments end related discussion bound ranks construction denotes outer product vectors learned dimensional vectors similarly constructed separate set vectors let denote stack vectors equations compactly written diagonal matrix built dimensional coefcient vector construction similar ent set parameters matrices note despite similarities svd rst glance equations ing matrix factorization ing directly explicitly instantiated peng summarize reparameterize interpolations matrices fact ranks upper bounded pointed krueger memisevic parameter matrices trained rnn tend rank experiments set equal hidden size aiming allow adaptive decoder use rank matrices recurrent computation holds priori beliefs matrices lower ranks desirable adaptive rnn constructed approach parameters comparable parameters standard elman network bias term elman network constructed learned matrix include bias term contributes additional parameters incorporating exemplars discuss computation coefcients exemplars inform decoder construction equations ing neural network architecture begin describing exemplar retrieval procedure retrieving exemplars intuitively similar source texts hold similar targets fore given source input use training target exemplar source similar compute similarities source texts use bag words bow features cosine similarity extract plar instance step processing change exemplars training proceeds course strategies exemplars handcrafted heuristically created hard templates reiter becker foster white ter alia randomly sampling multiple training stances guu learning neural reranker cao cally extracted exemplars denitelly interesting explore defer future work computing coefcients describe computation dimensional coefcient vector construct adaptive coder equations intuitively matrices equation seen turing different aspects generated text determines contributes adaptive decoder construction natural choice calculate use similarities tween exemplar aspects accomplish run rnn encoder use hidden state vector resentation associate pair learned vector puted similarity inner product compactly source exemplar retrieval fed encoder decoder model training instance additionally disallow target exemplar clarity dependence exemplar suppressed algorithm adaptive decoder construction procedure retrieve exemplar compute representation compute coefcients construct decoder eqs end procedure nyt giga wikibio inst avg len train dev test src tgt closing section algorithm summarizes procedure construct adaptive decoder discussion based discussion elman networks far straightforward apply method gated variants hochreiter schmidhuber cho inter alia recurrent neural tures bradbury vaswani peng inter alia periments adaptive lstm decoder drop replacement encoder decoder architecture introduces sonable additional parameters putational overhead especially uses sizes small encoder exemplar vectors equation small benet highly optimized gpu mentations uses recurrent computation standard nonadaptive rnn addition neural networks tive decoder requires access training set retrieval step sense semi parametric idea dynamically struct parameters inspired works earlier works proves successful tasks tion jia liu machine translation platanios recent template based generation models include emplars content addition source allow decoder attend copy guu weston pandey cao inter alia compare approach experiments model offers prohibits adaptively constructing nents model encoder motivation use exemplars inform ily determined decoder contrast encoder relates selecting content table number instances average text lengths datasets experiments lengths averaged training instances vorable performance potentially combined achieve improvements experiments section empirically evaluates proposed model sets text generation tasks stractive summarization data text generation heading imental details rst describe architectures compared models compared models addition previous works compare following baselines aiming control founding factors detailed implementation choices encoder decoder ture enhanced attention copy anisms encoder implemented directional lstm bilstm hochreiter schmidhuber schuster wal graves decoder uni directional tie input dings encoder decoder softmax weights press wolf use beam search tion length penalty attexp based codes attends copies emplars addition source inputs model adaptive decoder adadec closely builds uses cally constructed lstm decoder use attention copy mechanisms encoded exemplars extracted exemplars attexp ensure fair parisons use comparable training procedures regularization techniques els readers referred appendix details hyperparameters text summarization datasets empirically evaluate model benchmark text summarization datasets annotated gigaword corpus gigaword graff napoles gaword contains news articles sourced news services decades produce dataset follow split preprocessing rush pair rst sentences headlines news articles results train dev split average lengths source target texts respectively new york times annotated corpus contains news nyt sandaus articles published new york times use split preprocessing durrett following effort evaluate smaller portion test set gold summaries longer tokens randomly sample instances training data validation ing train dev split compared gigaword inputs targets nyt longer averaging respectively table summarizes statistics datasets note recent works use ent split nyt corpus paulus gehrmann ble models table decide use durrett cessing script publicly available datasets apply byte paired ing bpe sennrich proves improve generation proper nouns fan adaptive decoders empirical results table compares models gigaword test set rouge lin model adadec improves rouge scores cao model cao hold best published results uses extensive handcrafted features relies external mation extraction syntactic parsing systems com berkeley doc summarizer ofcial script model open nmt cao basic cao work work attexp work adadec text summarization performance table rouge scores dubbed gigaword test set denotes models retrieved exemplars uses handcrafted features bold font indicates best performance open nmt numbers taken cao uses additional encoding tion copy mechanisms exemplars extracted novel neural reranker adadec achieves better comparable performance state art models handcrafted features reranking techniques basic model cao ablates reranking component model uses exemplar retrieved system comparable baseline adadec outperforms rouge scores surprisingly observe interesting improvements attexp sequence sequence baseline believe model benet better extracted exemplars applying reranking system exploration deferred future work nyt experimental results summarized table follow previous works port limited length rouge recall values rett extractive model paulus abstractive approach based forcement learning adadec model forms observe similar trends paring adadec attexp baselines exception attexp improve data text generation data text generation aims generate textual descriptions structured data following durrett paulus truncate predictions lengths gold maries evaluate rouge recall instead length predictions model durrett paulus work work attexp work adadec table nyt text summarization test performance rouge recall values smaller portion original test data ltering instances summaries shorter tokens durrett denotes models retrieved exemplars bold font indicates best performance seen table consisting collection records liang given entity record attribute value tuple figure shows example entity jacques louis david table species entity properties ples born august nationality french forth table paired description model supposed generate table input refer readers lebret details task dataset implementation details use wikibio dataset lebret tomatically constructed pairing tables opening sentences biography articles english wikipedia follow split processing provided dataset train dev instances following lebret linearize tables conveniently train sequence sequence style models described table summarizes statistics dataset contrast text summarization ment apply bpe ther word embeddings initialized glove pennington xed training tied softmax weights addition models introduced additionally compare ing study adaptive decoder ther benet attention copy mechanisms exemplars empirical results following liu report bleu scores papineni jacques louis david august december french painter neoclassical style figure training instance wikibio dataset consists collections records jacques louis david piece textual description table summarizes text generation results wikibio test set overall observe similar trends summarization experiment ing copying exemplars texp improves baseline absolute scores utilizing emplar information adadec model performs larger margin bleu study improvements bining achieves absolute improvements adadec attexp vides evidence extend ways texp adadec incorporate exemplar mation complementary wiseman template motivated model based semi markov model liu hold current state art results encode table structures position led beddings structure aware attention gating techniques techniques scope work focuses mainly decoding end use script lin calculate rouge score mteval script bleu github com moses smt mosesdecoder master scripts generic mteval model bleu wiseman liu work work attexp work adadec work table data text generation performance bleu wikibio test set indicates models retrieved exemplars analysis qualitatively evaluate model studying outputs affected different exemplars figure shows domly sampled gigaword development instances compares outputs adadec attention copy exemplars ing different exemplars controlling example exemplar source inputs retrieved system training target remaining ones produced authors modifying rst styles introducing distractions tent example model includes ple subject people der inuence exemplar exemplar changes tense adds distraction changing place britain canada model follows tense switch gets fused distraction decides let train southern europe collide north america looking ple model general follows exemplar noun adjuncts prepositional phrases new home sales sales new homes rst confused distraction exemplar model makes judgment specic growth gets wrong related work exemplar based generation partly inspired traditional template based generation kukich reiter dale inter alia cent efforts devoted augmenting text generation models retrieved exemplars dosh mason charniak song lin inter alia portuguese train derailed source northern region oporto wednesday killing people exemplar train collision output train derailment die britain killed portuguese people exemplar killed britain train collision output portuguese train derailment people killed train collision exemplar canada killed people output northern mexico killing portuguese train derails sales new homes source increased percent biggest gain years sales new homes exemplar strongly march output percent new home sales rise sales new homes exemplar grow strongly output sales new homes rise economic new home sales grow exemplar tics percent output new home sales grow percent figure randomly sampled gigaword opment instances qualitative evaluation exemplar retrieved system remaining ones produced authors notable exemplars changes highlighted bold purple output changes italic yellow committing explicit cess typical method include exemplars additional inputs sequence sequence models pandey guu inter alia wiseman took different approach semi markov model learn templates dynamic parameter construction idea smaller network generate weights larger dues stanley koutnik mainly evolution computing context later revisited resentation learning moczulski nando shedivat ter alia successfully applied tion jia liu machine translation platanios relates meta learning set thrun pratt schwenk yoshua bengio learning phrase representations rnn encoder decoder proc statistical machine translation emnlp conclusion presented text generation model exemplar informed adaptive decoding rameterizes decoder information gathered retrieved exemplars mented text summarization data text generation showed proposed model achieves strong performance outperforms comparable baselines proposed model applicable conditioned text generation tasks release tion washington acknowledgments thank antonios anastasopoulos ming wei chang michael collins jacob devlin yichen gong luheng kenton lee dianqi zhouhan lin slav petrov oscar tackstrom kristina toutanova members google language team helpful sion anonymous reviewers able feedback references maruan shedivat avinava dubey eric contextual explanation networks xing dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly learning align translate proc iclr tilman becker practical template based ral language generation tag proceedings sixth international workshop tree adjoining grammar related frameworks james bradbury stephen merity caiming xiong richard socher quasi recurrent neural work proc iclr greg durrett taylor berg kirkpatrick dan klein learning based single document tion compression anaphoricity constraints proc acl jeffrey elman finding structure time cognitive science angela fan david grangier michael auli controllable abstractive summarization ceedings workshop neural machine translation generation chrisantha fernando dylan banarse malcolm reynolds frederic besse david pfau max berg marc lanctot daan wierstra convolution evolution differentiable pattern producing networks proceedings genetic evolutionary computation conference mary ellen foster michael white niques text planning xslt proceeedings workshop nlp xml rdf rdfs owl language technology albert gatt ehud reiter simplenlg alisation engine practical applications ceedings european workshop natural language generation sebastian gehrmann yuntian deng alexander rush abstractive summarization proc emnlp david graff junbo kong chen kazuaki maeda english gigaword second edition alex graves supervised sequence labelling recurrent neural networks volume studies computational intelligence springer jiatao zhengdong hang victor incorporating copying mechanism sequence sequence learning proc acl jiatao yong wang kyunghyun cho search engine guided proc tor parametric neural machine translation aaai ziqiang cao wenjie sujian furu wei retrieve rerank rewrite soft template based neural summarization proc acl kelvin guu tatsunori hashimoto yonatan oren percy liang generating sentences editing prototypes tacl ziqiang cao furu wei wenjie sujian faithful original fact aware neural abstractive summarization proc aaai kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger david andrew dai quoc networks proc iclr kaiming xiangyu zhang shaoqing ren jian sun deep residual learning image nition proc cvpr sepp hochreiter jurgen schmidhuber neural computation long short term memory micah hodosh peter young julia hockenmaier framing image description ranking task data models evaluation metrics jair jia bert brabandere tinne tuytelaars luc gool dynamic lter networks proc neurips diederik kingma jimmy adam proc method stochastic optimization iclr durk kingma tim salimans max welling variational dropout local terization trick proc neurips philipp koehn statistical machine translation cambridge university press jan koutnik faustino gomez jurgen ber evolving neural networks compressed weight space proceedings annual ence genetic evolutionary computation david krueger roland memisevic larizing rnns stabilizing activations proc iclr karen kukich design knowledge based port generator proc acl remi lebret david grangier michael auli neural text generation structured data proc application biography domain emnlp jiwei michel galley chris brockett jianfeng gao bill dolan diversity promoting jective function neural conversation models proc naacl percy liang michael jordan dan klein learning semantic correspondences vision proc acl chin yew lin rouge package automatic text summarization evaluation summaries branches proceedings shop kevin lin dianqi xiaodong zhengyou zhang ming ting sun adversarial ranking language generation proc neurips minh thang luong hieu pham christopher manning effective approaches proc based neural machine translation emnlp inderjeet mani advances automatic text marization mit press rebecca mason eugene charniak specic image captioning proc conll kathleen mckeown text generation bridge university press marcin moczulski misha denil jeremy appleyard nando freitas acdc structured efcient linear layer courtney napoles matthew gormley benjamin van durme annotated gigaword ceedings joint workshop automatic edge base construction web scale knowledge extraction gaurav pandey danish contractor vineet kumar sachindra joshi exemplar encoder decoder neural conversation generation proc acl kishore papineni salim roukos todd ward wei jing zhu bleu method automatic uation machine translation proc acl romain paulus caiming xiong richard socher deep reinforced model abstractive marization proc iclr hao peng roy schwartz sam thomson noah smith rational recurrences proc emnlp hao peng sam thomson noah smith deep multitask learning semantic dependency parsing proc acl hao peng sam thomson noah smith backpropagating structured argmax spigot proc acl hao peng sam thomson swabha swayamdipta noah smith learning joint semantic parsers disjoint data proc naacl jeffrey pennington richard socher pher manning glove global vectors word representation proc emnlp emmanouil antonios platanios mrinmaya sachan graham neubig tom mitchell tual parameter generation universal neural chine translation proc emnlp pengfei liu xipeng qiu xuanjing huang dynamic compositional neural networks tree structure proc ijcai press lior wolf output proc bedding improve language models eacl tianyu liu kexiang wang lei sha baobao chang zhifang sui table text generation structure aware learning proc aaai ehud reiter robert dale building applied natural language generation systems natural guage engineering yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin johnson xiaobing liu ukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith son riesa alex rudnick oriol vinyals greg rado macduff hughes jeffrey dean google neural machine translation system ing gap human machine translation ehud reiter somayajulu sripada jim hunter jin ian davy choosing words generated weather forecasts articial intelligence alexander rush sumit chopra jason weston neural attention model abstractive tence summarization proc emnlp evan sandaus new york times annotated corpus ldc corpora linguistic data consortium schuster paliwal bidirectional recurrent neural networks transactions signal proccesing abigail peter liu christopher manning point summarization generator networks proc acl rico sennrich barry haddow alexandra birch neural machine translation rare words subword units proc acl yiping song rui yan xiang dongyan zhao ming zhang better ensemble generation based dialog systems nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov dropout simple way prevent neural networks overtting jmlr stanley dambrosio gauci hypercube based encoding evolving scale neural networks articial life ilya sutskever oriol vinyals quoc sequence sequence learning neural works proc neurips sebastian thrun lorien pratt editors learning learn kluwer academic publishers ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need proc neurips oriol vinyals quoc neural tional model proc icml jason weston emily dinan alexander miller retrieve rene improved sequence eration models dialogue proceedings international workshop search oriented versational sam wiseman stuart shieber alexander rush challenges data document generation proc emnlp sam wiseman stuart shieber alexander rush learning neural templates text eration proc emnlp truncate source rst units evaluation apply beam search width length penalty data text generation general follow implementation details summarization experiment following modications apply byte paired encoding use vocabulary size word embeddings initialized version dimensional glove xed ing softmax weights tied embeddings layer dimensional bilstm coders residual connections exemplar encoder uses layer dimensional bilstm early stopping applied based ment set performance maximum decoding length tokens appendices implementation details implementation based tensorflow experiments use similar plementation strategies baselines model aiming fair comparison text summarization train models adam kingma batch size use default values tensorflow adam implementation initial learning rate models trained epochs learning rate annealed rate epochs weight decay applied ters current learning rate norms gradients clipped early ping applied based rouge performance development set weights output softmax function tied word embeddings encoders randomly initialized use dimensional layer bilstms gigaword experiment dimensional layer bilstms nyt experiment residual connections decoders layer adaptive lstms size encoders word embeddings apply variational dropout kingma encoder rnns dropout srivastava embeddings softmax layer rates empirically selected hidden state layer encoder fed layer tanh mlp decoder tial state use attention function luong copy mechanism exemplar encoder uses layer bilstm gigaword nyt periments respectively numerical stability equation scaled norms hidden size adaptive coder peng gigaword experiment use bpe types limit maximum decoding length subword units nyt use types maximum decoding length tensorflow
