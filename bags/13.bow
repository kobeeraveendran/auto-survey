r a r i s c v v i x r a improving update summarization by revisiting the mmr criterion florian juan manuel torres and marc el diro universite de montreal cp succursale centre ville montral quebec canada laboratoire informatique davignon bp avignon cedex france ecole polytechnique de montreal cp succursale centre ville montral quebec canada october abstract this paper describes a method for multi document update tion that relies on a double maximization criterion a maximal marginal relevance like criterion modied and so called smmr is used to select sentences that are close to the topic and at the same time distant from sentences used in already read documents summaries are then generated by assembling the high ranked material and applying some ruled based linguistic post processing in order to obtain length reduction and tain coherency through a participation to the text analysis ence tac evaluation campaign we have shown that our method achieves promising results introduction text summarization is the process of automatically creating a compressed sion of a given text that provides useful information for the user oriented summaries focus on a user s need and extract the information related to the specied topic given explicitly in the form of a query on the other hand generic summaries try to cover as much as possible the information tent over the past few years extensive experiments on query oriented document summarization have been carried out extractive summarization duces summaries by choosing a subset of sentences in the original documents sentences are then ordered and assembled according to their relevance to erate the summary this contrasts with abstractive summarization that involves rephrasing information in the text although human beings typically produce summaries in an abstractive way most of the research is on extractive summarization this is due to the fact that tools needed to construct semantic representations or generate natural language have not reached a mature stage today moreover existing abstractive summarizers often depend on an tive component for example use a language generation component on top of a multi document extractive summarizer to produce the nal summary in this paper we focus on query oriented multi document text summarization where the goal is to produce a summary of multiple documents about a specied topic with the ever increasing popularity of news search engines displaying the information in a more practical and pleasant way is becoming a challenging and important issue one possible solution is to summarize multiple news so as to propose only one short text instead of raw aggregated headlines this is intuitively a reasonable solution though producing summaries from large collection of documents is a very complicated task however as the number of documents increases facts that are considered as important and have to appear in the summary also become more numerous in this case a choice must then be made to drop important facts in order to satisfy size constraints one way to tackle this problem is to remove facts that the user is already aware of this variant of text summarization is called update summarization more formally update summarization is the task of producing summaries while minimizing redundancy with previously read documents from now on history recently introduced at the document understanding conference duc update summarization is an emerging summarization task that brings new challenges to sentence ranking algorithms indeed segments have to be selected according to their salience but also to their ability to capture elty existing approaches are derived from state of the art query oriented document summarizers by the addition of some constraints about redundancy and novelty detection these include machine reading graph based marization maximal marginal relevance mmr and novelty boosting the fact that most of them are relying on linguistic resources or tools such as taggers and parsers is a limiting factor for the adaptation to other languages or domains in this paper we propose a sentence ranking algorithm inspired by the well known mmr re ordering algorithm sentences are scored thanks to a double maximization criterion that strives to maximize sentence s relevance while imizing non redundancy with the previously read documents our formulation combines word level similarity measures in an information retrieval approach ranking sentences by their similarity to the topic and the inverse ity to other sentences in history we show that our method although using minimal linguistic resources can achieve good results among state of the art summarizers preliminary results about the sentence re ranking process were published in the remainder of this paper is organized as follows an overview of related work is provided in section section presents our three understanding conferences are conducted since by the national institute of standards and technology nist nlpir nist gov steps summarization method pre processing sentence ranking and linguistic post processing experimental results are presented in section followed by discussions and conclusions related work introduced by luhn in the fties research on automatic summarization can be qualied as a long tradition in the strategy proposed by luhn source tences are scored for their component word values as determined by type weights scored sentences are then ranked and selected from the top until some summary length threshold is reached finally the summary is generated by sembling the selected sentences in original source order although fairly simple this extractive methodology is still used in current approaches later on tended this work by adding simple heuristic features of sentences such as their position in the text or some key phrases indicating the importance of the tences as the range of possible features for source characterization widened choosing appropriate features feature weights and feature combinations have became a central issue a natural way to tackle this problem is to consider sentence extraction as a classication task to this end several machine ing approaches that uses document summary pairs have been proposed summarization then started gaining more momentum with the uation followed by the duc evaluation conferences new tasks have been continuously added to the summarization issue as proaches became more robust and resources grew larger were amongst the rst to tackle the update summarization problem their approach originally developed as a tool to monitor changes in news coverage over time uses topic detection and tracking techniques to determine which sentences capture ness and novelty the most intuitive way to go about update summarization would be to be identify temporal references within documents dates elapsed times temporal expressions and to construct a timeline of the events it is a complex task as temporal references depend on surrounding elements in the discourse but also require an understanding of the ontological and cal foundations of temporal reference construction assuming the timeline is constructed update summaries could be produced by assembling sentences containing the most recent events however most recently written material is not necessarily latest facts this way focusing the summaries on information that the user is not aware of can be seen as identifying unseen facts ing approaches rely exclusively on content based redundancy removal without recourse to temporal detection propose a machine reading method to struct knowledge representations from clusters of documents sentences that are containing new facts i e that could not be inferred by any document from the history are selected to generate the summary a rule based method using fuzzy coreference cluster graphs was introduced by this approach can be text summarization evaluation conference summac conducted in may nlpir nist gov related projects tipster summac index html applied to various summarization tasks but requires to manually write the tence ranking scheme rst use a nave similarity ratio to select sentences that are relevant and dissimilar to sentences from history on top of this ranking approach a second method called novelty boosting is used the latter extends the topic by the unique terms in the cluster thus biasing the ranking towards maximizing relevance not only with respect to the topic but also to the novel aspects of the topic in the cluster method in this section we present the details of the proposed text summarization method as mentioned earlier our work models sentence ranking as a double tion criterion we dene h to represent the previously read set of documents history q to represent the query and s the candidate sentence the ing subsections formally dene document pre processing the sentence scoring method and the summary generation process pre processing the rst step is to prepare documents for the ranking process as we use tractive summarization documents have to be chunked into cohesive textual segments that will be assembled to produce the summary the importance of pre processing is predominant because the selection of segments is based on words they contains the choice was made to split documents into full sentences in this way obtaining textual segments that are likely to be matically correct afterwards sentences are going through several basic malization steps in order to reduce computational complexity an example of document pre processing is given in table the process is composed by the following steps sentence splitting a simple rule based method is used for sentence documents are chunked at the dot exclamation and tion mark signs prior to that ambiguous composed person names i e george w bush are detected to reduce segmentation errors sentence ltering words are converted to lowercase and cleared up from sloppy punctuation words that do not carry meaning such as tional or very common words are removed date normalization dates are rewritten and extended with time related words for example december is replaced by and standardized dates allow to minimize enriched with december software is available from nist gov the scoring function bias i e considering only one word for one concept instead of three in this example while enrichment is useful to link facts that were happening at the same period of time month or year word normalization remaining words are replaced by their simplied forms i e inected forms go goes went gone are replaced by go using a word root database entries in case of ambiguity the most frequent word is chosen washington a federal judge monday found president clinton in civil contempt of court for lying in a deposition about the nature of his sexual relationship with former white house intern monica s lewinsky clinton in a january deposition in the paula jones sexual harassment case swore that he did not have a sexual relationship with lewinsky clinton later explained that he did not believe he had lied in the case because the type of sex he had with lewinsky did not fall under the denition of sexual relations used in the case a federal judge monday found president clinton in civil contempt of court for lying in a deposition about the nature of his sexual relationship with former white house intern monica s lewinsky clinton in a january deposition in the paula jones sexual ment case swore that he did not have a sexual relationship with lewinsky clinton later explained that he did not believe he had lied in the case because the type of sex he had with lewinsky did not fall under the denition of sexual relations used in the case judge monday president clinton civil contempt court federal lie deposition nature sex relation former white house intern monica clinton january deposition paula jones sex harassment case swear sex relation clinton late explain believe lie case type sex lewinsky fall dene sex relation use l a n g i r o e t t i l s e s s e c o r p table example of pre processing applied to the document from cluster of duc news agency name is removed document is segmented into sentences words are normalized punctuation and case are removed dates are standardized end enriched ranking sentences are scored according to the fact that they contain material satisfying the need formulated in the user s query ranking sentences for query oriented summarization can be seen as a passage retrieval task in information retrieval in this paradigm sentences sharing most of their vocabulary with the query are likely to be informational for the reader each sentence is then scored by computing a combination of two similarity measures with the query the rst similarity measure is the well known cosine computed on the sentence and the query vectorial representations in the documents term space denoted spectively and the decision was made not to use the classical tf idf weighting scheme because of the diculty to nd similar data and generate pertinent weight lists the main weakness of cosine and more generally of all similarity measures using words for tokens is that they are relying too much on term normalization their performance dramatically decreases with wrongly or non normalized words that is why we propose a second similarity measure based on the jaro winkler distance that can bridge morphologically similar words in order to smooth normalization and misspelling errors this measure can be classied as an improved edit distance between two word sequences the jaro winkler distance denoted jw calculates the number of operations required to transform a string into another one it uses the number of ing characters and transpositions to compute a similarity score between two terms giving more favourable ratings to terms that match from the beginning originally introduced to tackle normalization issues in automatic tion of chemistry articles this distance was extended to compute a similarity measure between a sentence and the query q q q qq m max where is the term set of s in which the terms m that already have maximized m during the previous steps of the summation are removed the nal score is calculated using a linear combination of the two similarity measures equation shows how to compute the relevance score between a sentence s and a query q q q the maximal marginal relevance mmr algorithm has been successfully used in query oriented summarization it strives to reduce redundancy while maintaining query relevance in selected sentences the summary is constructed incrementally from a list of ranked sentences at each iteration the sentence which maximizes mmr is chosen mmr arg max q ss max sj e sj where s is the set of candidates sentences and e is the set of selected sentences represents an interpolation coecient between relevance and redundancy in the original formulation and were computed using the cosine ilarity measure although this measure has been proven to be ecient any other similarity measure between sentences remains appropriate we propose an interpretation of mmr to tackle the update summarization issue unlike previous work such as our approach does not require ative re ranking to remove sentences containing redundant material the set of selected sentences e is replaced by the set of sentences in history in terms of computational complexity this means that each candidate is compared to all sentences from h since and are ranged in they can be seen as probabilities even though they are not this way is considered as the probability to be relevant to the topic and as the probability to be redundant with history we propose to rewrite by adding the constant as nr stands for novelty relevance nr arg max q max shh sh arg max q sh max shh ss ss this makes more sense because it combines relevance and non redundance instead of focusing on redundancy penalization according to our intuition we presume that is more or less corresponding to an or combination but we are obviously looking for a criterion corresponding to and since the similarities are independent we can use the product combination sentences are scored thanks to a double maximization criterion in which the best ranked one will be the most relevant to the query and the most dierent to the sentences in h q h max shh sh decreasing parameter in with the length of the summary was suggested by and successfully used in the duc by thereby emphasizing the relevance at the outset but increasingly prioritizing redundancy removal as the process continues similarly we propose to follow this assumption in smmr that as the amount of data in history increases using a function denoted we have dened this parameter prioritizes non redundancy function as novelty factor h n n a special breed of redundancy is proliferating in news articles as ists increasingly rely on the fact that news articles have to be as universally understandable as possible this means that most of the news articles contain previous facts pointers to previous articles in order for a reader that does not know anything on the subject to catch on this is why we think that a normalized longest common substring lcs measure between two sentences n is well adapted to be used as the non redundancy measure for ple lcs can easily detect sentence rewritings specially when the sentence is structured around a redundant sub sentence post processing once sentences are selected to be assembled in the nal summary some tic treatments are applied indeed once out of their contexts discursive forms are considerably decreasing summary coherence for example two sentences one next to the other in the summary may be in opposition while not dealing with the same subject our rule based linguistic post processing targeted tence length reduction and coherency maximization an example of summary post processing is given in table the process is composed by the following steps acronym rewriting rst occurrence of an acronym is replaced by its complete form acronym and denition following ones only by their duced forms denitions are automatically mined in the corpus by pattern matching in case of acronym ambiguity the most frequent one is selected date and number rewriting numbers are reformatted and dates are normalized to the us standard forms mm dd yyyy mm yyyy and mm dd temporal references rewriting time tags are used to replace fuzzy temporal references for example the end of next year with poral tag is replaced by the end of discursive form rewriting ambiguous discursive forms are deleted for example but it is is replaced by it is finally say and parenthesized content are removed and ation cleaned sentences are ordered within the summary by original document order and temporal order of documents since the acronym rewriting process is dent to the sentence order and modies sentence s lengths multiple passes are required to generate the nal summary within summary redundancy is aged by using a simple similarity threshold that prevents duplicate and highly redundant sentences to enter the summary example ambiguous say clause he said is removed last u s scientists issued a report saying the rate of ice melting in the arctic is increasing and within a century could lead to summertime free ocean conditions not seen in the area in a million years the rate of ice melting in the arctic is increasing and a panel of researchers says it sees no natural process that is likely to change that trend for the white sea ice reects solar radiation back into space but as the ice melts the dark water will absorb some of the light warming and melting more ice words the rate of ice melting in the arctic is increasing and a panel of researchers says it sees no natural process that is likely to change that trend the white sea ice reects solar radiation back into space but as the ice melts the dark water will absorb some of the light warming and melting more ice in us scientists issued a report saying the rate of ice melting in the arctic is increasing and within a century could lead to summertime ice free ocean conditions not seen in the area in a million years words l a n i g i r o e s s e c o r p table example of post processing treatments applied to the summary duced from cluster b of tac dates are standardized tences are ordered with temporal constraints ambiguous discursive forms are deleted experiments the method described in the previous section has been implemented and uated by participating to the text analysis conference tac update summarization conducted by the national institute of standards and technology nist the following subsections present details of the dierent experiments the tac update track piloted in document understanding duc the update marization task consists in producing a short word summary of a set of newswire articles under the assumption that the user has already read a given set of earlier articles the purpose of each update summary is to inform the reader of new information about a particular topic the test data set in tac comprises topics each topic has a topic statement examples are given in table and relevant documents which have been divided into two document set a and document set b each document set has documents where all the documents in set a chronologically precede any of the documents in set b the documents are coming from the collection of news articles information about the tac update track is available at nist gov nist data was consisting of three temporal document sets a b and c arctic and antarctic ice melt describe the developments and impact of the continuing arctic and antarctic ice melts paris riots describe the violent riots occurring in the paris suburbs beginning ber include details of the causes and casualties of the riots and government and police responses table example of topic statements and given a duc topic and its two document sets a and b the task is to create two brief uent summaries that contribute to satisfying the information need expressed in the topic statement the rst one is a topic oriented summary of the document set a while the second one is an update summary of the document set b produced under the assumption that the reader has already read documents in set a evaluation all summaries produced by our approach were evaluated both automatically and manually by the nist the manual evaluation comprised three scores an overall responsiveness based on both the linguistic quality of the summary and the amount of information in the summary that helps to satisfy the information need expressed in the topic narrative a linguistic quality guided by consideration of the following factors grammaticality non redundancy referential clarity focus structure and coherence a pyramid recall score computed on summary content units scus annotations human annotators select overlapping content in multiple model summaries to construct a pyramid of scus most existing automated evaluation methods work by comparing the erated summaries to one or more reference summaries ideally produced by humans in the tac evaluation four human summaries were written for each document set to evaluate the quality of our generated summaries several automatic measures were computed is a n gram recall measure calculated between a candidate summary and a set of reference summaries it is computed as between very poor and very good is available at isi edu srref srref n gramss co grams n gramss grams where n stands for the length of the n gram and co grams is the maximum number of n grams co occurring in a candidate summary and a set of reference summaries in our experiments and rouge will be computed basic elements is similar to rouge but uses minimal length ments of sensible meaning as units such as kitchen knife or bank of america in the tac nist received runs from participants for the update summarization task each participant submitted up to three runs ranked by priority all runs were evaluated automatically runs but manual evaluations were provided only for runs with priority and runs in addition one baseline summarizer was included in the evaluation it consists in returning all the leading sentences up to words in the most recent document the duc update data was used to train our system and to estimate the interpolation coecient of the similarity measure and the novelty factor as the duc update task was consisting of three temporal documents sets we have adapted the data set to match the tac guideline by removing the third cluster parameters for the relevance function and the novelty factor were tuned using this modied data set the optimal values we have found are and h c with c for cluster a no history and for cluster b n ocial results table shows the results obtained by our submission at the update rization task of tac our system has achieved good results for overall responsiveness and linguistic quality respectively ranked and out of submissions but average ones for automatic evaluations ranked between the and place out of submissions giving more condence to manual evaluation we can say that our system performed quite well one surprising result is that our system has obtained high marks in linguistic quality despite elements is available at isi edu the simplicity of our rule based post processing evaluation overall responsiveness linguistic quality pyramid rouge basic elements score rank table results of manual and automatic evaluations at the tac update task for a comparative evaluation figures and show the results obtained by all the systems participating in the update summarization task at tac the baseline consisting of word summaries generated by taking the rst sentences in most recent articles is also shown in the two gures it is worth noting that teams were allowed to submit up to three runs generally consisting of dierent parameter congurations that way the number of submissions that have obtained better marks than our system may have in fact been produced by a number of systems three times lower being more balanced between content and linguistic evaluations our system always outperforms the widely used based baseline that have been proved to be very challenging figure scatter plot of linguistic quality and overall responsiveness for the tac update task our system red star and the baseline big blue mond are highlighted figure scatter plot of and rouge average recall scores for the tac update task our system red star and the baseline big blue diamond are highlighted results for separated document sets are presented in table one can say that evaluation scores are signicantly lower for summaries of document sets b but it is worth noting that manual evaluation ranks are signicantly better overall responsiveness going from to and linguistic quality from to this shows that from the linguistic quality point of view our system is less aected by the increasing diculty of update summarization than other approaches evaluation overall resp linguistic quality pyramid rouge docset a rank score score docset b rank table automatic and manual evaluation results for document set a and b additional results in these additional experiments rouge scores have been computed using the conguration described in the ocial guidelines of tac to observe the behavior of our method on presence of noisy data we have added in each cluster a number of random documents taken from dierent clusters since each cluster contains relevant documents this means a and noise on the data sets results on noisy data are given in guidelines are available at nist gov tac table there is no signicant performance loss on our method proving that information retrieval approaches are robust for query oriented summarization evaluation rouge table comparison of rouge average recall scores for our system on and noisy tac data we also wanted to examine the impact of the novelty factor used in equation on the summaries produced for document sets b on figure we observe an improvement of the rouge scores for all the values greater than zero obtaining the best results for values comprised between and the dierence with the optimal value found on the training data is minimal but handicap our performance the size of the adapted duc training data was obviously too small topics of documents to avoid problems n figure plot of rouge average recall scores for docset b summaries in relation for the tac update task to the novelty factor n discussion the summarizer based on the smmr sentence scoring algorithm succeeds in identifying most relevant but containing new facts sentences from clusters of news articles the results obtained during the tac evaluation prove that our method can achieve good results for both linguistic and content quality unlike other approaches our system does not use large linguistic or knowledge resources which makes it lightweight and easily adaptable to any other language tac or any domain computing the whole tac test data takes less than ve minutes on a dual core with of ram as applications that are ject to use update summarization algorithms are gathering tremendous amount of data such as news aggregators computational complexity is becoming an important feature to take into consideration we have observed another interesting result on our submission automatic and manual evaluations are not often correlated to illustrate this lack of relation the topics that within our submission have received the best manual and automatic scores are compared results are shown in table as we can see manual and automatic evaluation scores are in total contradiction indeed according to manual evaluations our best summaries have been generated for the topic while automatic scores for this topic are poor inversely according to automatic scores our best topic is while its manual scores are very poor by scrutinizing the generated summaries shown in the table we have identied the reasons of this issue redundancy is the main factor for these high rouge scores units of meaning such as the billed woodpecker are split in an incorrect way wrongly increasing the number of matching tokens used for computing recall scores this example proves that using only automatic evaluations is somehow risky evaluation overall responsiveness linguistic quality pyramid rouge basic elements table results of manual and automatic evaluations for topics et ranks obtained by the topic within our submission are shown in parenthesis the topic ranked in rst place contains the summaries that have obtained the best scores in comparison to the other topics of our submission conclusions in this paper we have explained how we had revisited the classical mmr rithm in order to propose a novel approach to update summarization so called the smmr an important aspect of our approach is that it does not requires ranking nor linguistic which makes it a simple and ecient method to tackle the issue of update summarization system only uses minimal linguistic resources for post processing that are easily adaptable to any other language a d b d a d b d martha stewart in prison describe martha stewart s experiences while in prison new york it s check in day for martha stewart larry stewart who is not related to martha stewart was acquitted of the charges q what will happen to the company martha stewart living omnimedia stewart spends up to three hours a night writing on a prison typewriter with ribbons purchased at a prison store bacanovic and stewart were both given the option of staying out of prison while they appealed martha stewart has been exercising reading and making friends in prison but the food at the minimum security prison camp in west virginia is terrible the domestic diva s daughter said martha stewart in a christmas message posted on her personal web site called for sentencing reform and took a swipe at the bad food in prison since entering federal prison in october martha stewart has tried her hand at ceramics learned to crochet and become an expert on vending machine snacks martha stewart who is about to get out of prison seems to have undergone a makeover on the cover of the latest newsweek one of the tasks ahead of stewart is to try and spin the goodwill she gained in prison into prots for her martha stewart living omnimedia inc ivory billed woodpecker describe developments in the rediscovery of the ivory billed woodpecker long thought to be extinct the ivory billed woodpecker a bird long thought extinct has been sighted in the swamp forests of eastern arkansas for the rst time in more than years nell university scientists said the ivory billed woodpecker long suspected to be extinct has been rediscovered in the big woods region of eastern arkansas searchers reported in the journal science to be published the ivory billed pecker is one of six north american bird species thought to have gone extinct since the ivory billed woodpecker once prized for its plumage and sought by american indians as magical was thought to be extinct for years recordings of the ivory billed woodpecker s distinctive double rap sounds have convinced doubting researchers that the large bird once thought extinct is still living in an east arkansas swamp the recordings seem to indicate that there is more than one ivory billed woodpecker in the area for half a century watchers have longed for a glimpse of the ivory billed woodpecker a bird long given up for extinct but recently rediscovered in arkansas the ivory billed woodpecker was thought to be extinct until it was spotted in the swamps of southeast arkansas in the ivory bill was or is the largest north american woodpecker table examples of our submission for the topics and of tac the novelty factor characterized in our sentence scoring method by a ear function h turns out to be a very important parameter requiring to be tuned in a more judicious manner using a linear function that relies on the number of previous clusters instead of the exact amount of text can be ardous high redundancy within news articles forces us to believe that the reader n can gain knowledge of only a reduced number of concepts this is the reason why we think computing the novelty factor by using the concept redundancy is worthy of further work recent work by gives some interesting ideas on how to remove redundancy by constructing novel graph based representations from documents it was pointed out that question answering and query oriented tion have been converging on a common task the value added by summarization lying in the linguistic quality we have seen that applying simple ruled based linguistic treatments to candidate sentences allows to signicantly increase the linguistic quality current research works are predominantly focused on the english language this is why we are currently developing a bilingual evaluation corpus english and french among the others this point sounds like a promise for further investigation acknowledgments this work was supported by the agence nationale de la recherche france project sinequa com references j allan r gupta and v khandelwal temporal summaries of new topics in proceedings of the annual international acm sigir conference on research and development in information retrieval pages acm new york ny usa f boudin exploration dapproches statistiques pour resume tique phd thesis universite davignon pays de vaucluse december florian boudin marc el and juan manuel torres moreno a able mmr approach to sentence scoring for multi document update marization in coling companion volume posters and tions pages manchester uk august coling organizing committee florian boudin and juan manuel torres moreno a cosine minimization approach for user oriented multi document update marization in recent advances in natural language processing ranlp pages borovets bulgaria september florian boudin juan manuel torres moreno and patricia morales an ecient statistical approach for automatic organic istry summarization in bengt nordstrom and aarne ranta editors international conference on natural language processing gotal volume of lecture notes in computer science pages burg sweden august springer r brandow k mitze and l f rau automatic condensation of electronic publications by sentence selection information processing and ment j carbonell and j goldstein the use of mmr diversity based in annual ing for reordering documents and producing summaries international acm sigir conference on research and development in formation retrieval pages acm press new york ny usa h iii practical structured learning for natural language ing phd thesis university of southern california august h p edmundson new methods in automatic extracting journal of the acm jacm g erkan and d r radev lexrank graph based lexical centrality as salience in text summarization journal of articial intelligence research b hachey g murray and d reitter the embra system at duc query oriented multi document summarization with a very large latent semantic space in document understanding conference duc ver canada october a hickl k roberts and f lacatusu lcc s gistexter at duc machine reading for update summarization in document understanding conference duc rochester usa april erhard hinrichs temporal anaphora in discourses of english linguistics and philosophy february e hovy c y lin l zhou and j fukumoto automated in fifth conference on language tion evaluation with basic elements resources and evaluation lrec may j kupiec j pedersen and f chen a trainable document summarizer in proceedings of the annual international acm sigir conference on research and development in information retrieval pages acm new york ny usa yulia ledeneva eect of preprocessing on extractive summarization with maximal frequent sequences in micai advances in articial telligence volume of lecture notes in computer science pages springer berlin heidelberg wenjie li furu wei qin lu and yanxiang he ranking tences with positive and negative reinforcement for query oriented update in proceedings of the international conference on summarization computational linguistics coling pages manchester uk august coling organizing committee chin yew lin rouge a package for automatic evaluation of maries in stan szpakowicz marie francine moens editor text rization branches out proceedings of the workshop pages barcelona spain july association for computational linguistics z lin t s chua m y kan w s lee l qiu and s ye nus at duc in document understanding using evolutionary models of text conference duc rochester usa april h p luhn the automatic creation of literature abstracts ibm journal of research and development i mani g klein d house l hirschman t firmin and b sundheim summac a text summarization evaluation natural language ing i mani and m t maybury advances in automatic text summarization mit press g murray s renals and j carletta extractive summarization of ing recordings in ninth european conference on speech communication and technology eurospeech lisboa portugal september ani nenkova and rebecca passonneau evaluating content selection in summarization the pyramid method in daniel marcu susan dumais and salim roukos editors hlt naacl main proceedings pages boston massachusetts usa may association for tational linguistics d r radev and k r mckeown generating natural language summaries from multiple on line sources computational linguistics g salton a wong and c s yang a vector space model for automatic indexing communications of the acm k sparck jones a statistical interpretation of term specicity and its application in retrieval journal of documentation s teufel and m moens sentence extraction as a classication task in acl eacl workshop on intelligent and scalable text summarization pages kapil thadani and kathleen mckeown a framework for decreasing textual redundancy in coling manchester uk august coling organizing committee w e winkler the state of record linkage and current research problems statistics of income division internal revenue service publication r rene witte ralf krestel and sabine bergler generating update in document understanding conference duc maries for duc rochester usa april s ye l qiu t s chua and m y kan nus at duc ing documents via concept links in document understanding conference duc vancouver canada october
