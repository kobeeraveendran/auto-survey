multi mode translation natural language python code transformers colin clement microsoft cloud com dawn drain microsoft cloud com jonathan stanford university edu alexey svyatkovskiy microsoft cloud com neel sundaresan microsoft cloud com abstract simultaneously modeling source code natural language ing applications automated software development understanding suant achieving technology introduce python method text text transfer transformer trained translate pairs python method feature combinations single model predict methods natural language tation strings docstrings summarize code docstrings common style present analysis modeling fort large scale parallel corpus million python methods lion method docstring pairs ing docstring method eration outperforms sized auto regressive language models english pre trained randomly initialized searchnet test set best model dicts syntactically correct method bodies achieved bleu score method generation docstring corresponding author work microsoft internship generation summarization achieved rouge score method generation docstring tion introduction software keystone modern society touching billions people services devices daily writing documenting source code software ing labor intensive tasks software opers need repeatedly refer online umentation resources order understand existing code bases progress oper productivity improved ence source code documentation velopment environment featuring intelligent machine learning based code completion analysis tools recent progress natural language ing nlp especially encoder decoder based transformer models vaswani pre training radford lewis led state art formance language modeling tion devlin translation raffel summarization liu ata grammar correction bryant entity recognition dialogue tion budzianowski vulic quantitative advances come deeper understanding learned hidden representations power ers kovaleva voita clark ethayarajh arguably natural programming languages increasingly ing playgrounds nlp modeling languages denition mar syntax known relationships entities offer enticing opportunities deeper probing nlp models tasks theoretical importance nlp tasks practical utility software development environments language ing generation code pletion raychev bruch svyatkovskiy tion summarization generate documentation natural language summaries moreno scalabrino wan alon summarize set code changes moreno tion grammar error correction patch detect bugs zhai joint bedding code natural language code search husain work focus jointly modeling source code python concomitant natural language documentation docstrings transformers study dual tasks generating method code bodies signatures docstrings generating strings signatures method code ies previous work allamanis yin neubig leveraged grammar code extract features like stract syntax tree modeling treating code natural language separate modalities follow examples like barone sennrich treat python docstrings fundamentally different ral languages representing source code natural language docstrings sequences tokens sharing vocabulary present multi mode translation method resulting python method text text transfer transformer inspired text text transfer transformer raffel single model learn code language generation understand relationships paper organized follows begin sec presenting examples performance novel multi mode python method text text transfer transformer model trained translate pairs combinations method signatures docstrings ies feature source target sec scribe training data pre processing steps source code natural language followed compared existing allel docstring method corpora like searchnet presented barone barone nrich sec explain like lewis pre training scheme demonstrating speed training time docstring generation sec analyze classify python docstrings abling style conditioned docstring generation sections discuss results method generation string generation respectively compare models randomly initialized pre trained english multi mode training figure shows examples inputs puts model example tasks blue predicting body method figure real examples performing method generation combinations signatures docstrings leading comment input sequence instructs model output particular target signature body instructs predict combination features signature body target docstring style oneline def count example lst count return count example target docstring style numpydoc def count example lst count return count example count number numbers list count number numbers list list count numbers parameters lst list returns int number numbers list figure performing docstring generation example method showing output target prex indicates line blue numpydoc docstring red styles signature middle red predicting method natural language docstring green predicting body signature docstring note ment target specification structs model choose particular form output note correctly learns interpret natural language prets related example greater number model duces syntactically correct code discuss later model tically incorrect code correctly infers types lst numbers iterables containing numbers prompted source code produce docstring summary styles figure shows model prompted methods generated fig blue line blue style numpydoc red style infers intent signature code infers type argument list return type int produces terse sentence summary function cases order teach maximally late separate method features signatures docstrings bodies trained translate pairs feature combinations feature appear source target scheme vantageous corpus unbalanced methods featuring docstrings model learn leverage features present additionally shown code predictable natural language hindle method argument names nating signal relatively rigid ture model learn ignore content docstrings multi mode method comes training model generate method bodies docstrings appendix detailed description multi mode training scheme dataset data consists github ries includes public repositories belled containing primarily python source code featuring stars commit past years successfully cloned repositories extracting million python les default head state repository removed literal duplicate les resulting million unique les remove grained clones removing license les literal contents pre training step comprising raw text order extract method level tion tuning standard library ast produce level abstract syntax tree ast python extracting individual class method failed parse come issue different styles white space tab conventions successfully parsing million unique python les python module astunparse ast method unparse source code tuned model trained syntactically incorrect code statistics docstring corpus summarized table parallel method docstring corpus twice large largest irrespective guage large largest python parallel corpus csn method ignored comments generally represent trivia normal language syntax cleaned docstrings removing non ascii characters normalizing unicode replacing commit hashes paths urls placeholder tokens studies randomly split les repository level prevent data leakage training tion test set pre training majority python methods million methods possess strings imbalance fact tunity light recent trend nlp unsupervised pre training language els vast amounts raw text devlin pre trained models ing points downstream tasks like cation translation summarization tion answering consistently yields state art results lewis raffel following trend use similar masking objective recent text text transfer transformer raffel shown figure ing inputs sample random subset token spans length replaced token teach dataset methods docstring languages csn husain ciurumelea barone sennrich python python python python table summary statistics python parallel corpus compared presented literature csn contains python methods docstrings languages parallel corpus large largest size largest python parallel corpus figure denoising auto encoder pre training sequence sequence tasks based masking objective raffel python les rst tokenized spaces replaced character ordinal space character similarly newlines tabs note indentation token multiple replace random sub sequences tokens numbered masks train model return mask followed tokens replaced sequence sequence model replace missing tokens training target prised numbered mask tokens followed tokens mask represents architecture decoder transformer vocabulary byte pair bpe encoder trained raw python les self attention encoder decoder layers encoder layers hidden mension totaling million ters experiments paper ing extended gpt tokenizer pre trained raw source code total weeks sixteen tesla gpus epochs total training docstring eration observed faster gence lower loss starting pre trained model compared random tialization appendix details experiments trained starting pre trained model docstring analysis examining docstring samples corpus salient features different styles documentation python community prescribed facto style docstrings python hancement protocol goodger van rossum describe line multi line docstrings mandates tion modern large scale projects utilize docstring styles parseable lowing automatic creation tion source code documentation sites sphinx number standard styles evolved nity currently dominant parseable docstring styles ones supported sphinx restructuredtext rest jones ofcial google style google numpy style technically rest maintainers javadoc style jav difference tween style mainly syntax exist denoting sections type description annotation method arguments returned yielded tities exist dened tion styles line containing line paragraph containing lines label docstring described far includes informal user docstring styles project specic styles like sage mathematics toolkit brary table shows breakdown fraction styles corpus rality docstrings line common style paragraph common styles machine parseable styles discussed comprising total number strings appendix contains detailed tributions method signature docstring method body character line lengths style line paragraph rest google numpy javadoc fraction methods table docstring style statistics million pythondocstrings visualize space styles fasttext vector embeddings strings obtaining dimension continuous vector representations pca reduce dimensionality plied distributed stochastic neighbor bedding sne obtain dimensional visualization figure shows docstrings embedded colored docstring style dened clear clustering styles indicating similar docstrings use style parseable styles natural chotomy parseable non parseable styles left dominated line paragraph styles parseable styles largely right observation generate mentation consistent style given project translate ods informal descriptions useful search indices figure visualization continuous dings docstring corpus strings colored docstring style embeddings obtained fasttext dimensional embedding obtained pca dimensionality reduction initialization sne model med random med english csn test med random ppl bleu syntax prec rec prec rec stat prec rec prec rec prec rec prec rec barone sennrich test barone table comparing models random weight initialization pre trained english task method generation signature natural language docstring rst rows use test set consisting methods fourth fth rows compare performance medium codesearchnet python test set nal rows compare performance parallel test set barone sennrich syntax fraction predicted methods correct syntax python grammar method generation turn attention method ation predicting method code body method signature natural guage docstring rst discuss benchmark task medium model million parameters pendix details training scratch starting publicly released openai glish pre trained checkpoint weights ments extended tokenizer including white space tab tabs tokens total vocabulary size beam decoding beam width row tab shows double bleu score overall better recall signicantly better rouge scores baselines methods generated syntactically correct python methods syntactically correct trained tesla gpus epochs weeks training time appendix hyper parameters baselines trained hardware week training time achieving better validation loss perplexity english pre trained initialization slightly beats random ization indicate learned biases english particularly benecial writing python code rics margin error note barone sennrich modeled methods docstrings obtaining similar bleu score python parallel corpus barone test set obtains nearly double scores large discrepancy explained data leaking test set model med random med english ppl bleu csn test med random barone test barone table comparing models dom weight initialization pre trained english task natural guage docstring generation signature method body rst rows evaluated test set methods fourth fth rows shows performance medium csn python test set rows compare model barone test set training set barone test set smaller resentative sample python code domain fourth rows tab performance publicly available csn python test set notably worse results test set csn curated set ing methods test methods fewer lines code calculated performance subset test set curated way csn observing scores test set lower nominal test set mance closer csn performance believe curating choice explains ence test set csn test set conclude tests short methods easier complete plausible bodes automatic code completion applications docstring generation examine results docstring generation task evaluation poses conditioned signatures method bodies method generation set benchmark random ization pre trained english initialization hyperparameters table shows rouge scores baselines margin error somewhat surprising result given english domain docstrings row shows superior medium terms bleu rouge metrics present results licly available csn test set similar method generation task performs worse csn data likely reasons discussed sec evaluated barone parallel test set shown second row tab performs notably worse barone test set test set contradicting hypothesis doubling method generation bleu score data leakage higher bleu score reported barone indicating real progress code summarization eld docstring generation similar code marization domains different docstrings contain structured annotations arguments return values raised exceptions line unit tests doctest wang wang reports best rouge test set code summarization specify statistic reporting strong conclusions mance compared state art conclusion work presented novel multi mode python method text text transfer model largest parallel corpus python source code docstrings reported literature date trained translate pairs combinations method tures docstrings method bodies feature source target introduced control token prexes docstring generation cilitate docstring generation styles focusing modeling tasks ing python methods docstrings summarizing python source code methods docstrings commonly ring styles compared new proach auto regressive baselines trained individual docstring method eration tasks codesearchnet test set achieves bleu score method generation docstring generation rouge score method generation docstring generation demonstrated fectiveness dynamic masked pre training reducing docstring generation training time looking forward plan age downstream mated software engineering tasks including code documentation method generation natural language statements velop model evaluation criteria age unique properties source codes acknowledgements like thank microsoft cloud smartml engineering team help preparing data shao kun deng development compelling user experiences leveraging christian bird ful discussions appendix docstring statistics figure shows distributions tures docstrings corpus row distribution total character level length method signatures left docstrings ter code bodies blue lines methods possessing docstring vast majority methods docstrings characters row shows distribution line lengths concomitant features row common line length docstrings comprising vast majority docstrings multiple lines pre training details figure complete training script facebook research quence fairseq modeling library pre trained data pre noised processed fairseq preprocess command placed directory indicated dir architecture training hyper parameters set script trained hyperparameters data described sec figure shows learning curves gle model architecture trained docstrings starting random initializations starting pre trained model gure shows pre trained initialization converged better figure histogram number characters row python signatures left docstrings middle method body right blue lines methods docstrings yellow lines methods docstrings vast majority docstrings characters row shows histograms number lines features described row validation loss faster randomly initialized model training details experiments fairseq library openai english checkpoint supplied huggingface library ure shows complete training script english pre trained initialization trained checkpoint provided models trained tesla gpus memory days multi mode training details order better teach stand relationships ent features code signatures docstrings bodies taught translate pairs combinations features figure learning curves training sequence transformer translating python method denitions docstrings blue curves represent training validation loss convergence validation loss stops decreasing occurs steps epochs optimization pre trained model cal hyperparameters reaches beats best idation loss steps epochs dir fairseq train tokens translation lang src lang tgt embeddings decoder input output embed transformer dropout dropout embed dim embed dim target positions source positions ffn embed dim ffn embed dim attention heads attention heads smoothing dropout decay adam norm scheduler updates freq invalid size inputs valid test dir dir models interval betas eps logdir dir tensorboard learned pos learned pos figure fairseq train script pre train setting relevant parameters fairseq train dir adam betas decay norm optimizer scheduler updates init decay sample break mode complete tokens freq target positions invalid size inputs valid test figure fairseq train script train gpt model baselines solid black line ing loss curves tion loss tasks indicated tab dashed lines indicate tasks strings present target showing generally predictable targets validation loss larger trained tesla gpus epochs weeks training time contain feature source target way model learn produce method bodies ing signatures docstrings table spells exactly combinations provided model source source target example comment string target feature style added structing model feature combination signature body docstring target style imperative added styles dened discussed main text figure shows training curves references java doc technical report miltiadis allamanis daniel tarlow andrew gordon wei bimodal modelling source code natural language ceedings international conference international conference machine ing volume page jmlr org uri alon shaked brody omer levy eran yahav generating sequences structured representations code arxiv preprint antonio valerio miceli barone rico sennrich parallel corpus python functions figure learning curve multi mode training black line training loss lines validation loss mode translation dashed lines indicate docstrings target solid lines code target documentation strings automated code umentation code generation arxiv preprint marcel bruch martin monperrus mira mezini learning examples prove code completion systems ings joint meeting european software engineering conference acm sigsoft symposium foundations ware engineering pages christopher bryant mariano felice edward briscoe automatic annotation uation error types grammatical error rection association computational guistics pawe budzianowski ivan vulic hello help use pretrained language models arxiv preprint oriented dialogue systems adelina ciurumelea sebastian proksch ald gall suggesting comment tions python neural language models edition ieee international ference software analysis evolution reengineering saner ieee kevin clark urvashi khandelwal omer levy christopher manning bert look arxiv preprint analysis bert attention jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language derstanding arxiv preprint kawin ethayarajh contextual textualized word representations comparing geometry bert elmo dings arxiv preprint david goodger guido van rossum docstring conventions pep google google python style guide cal report xiaodong hongyu zhang sunghun kim deep code search proceedings international conference software gineering icse page new york usa association computing ery abram hindle earl barr zhendong mark gabel premkumar devanbu naturalness software national conference software engineering icse pages ieee sources signature docstring body sig sig body doc body table table possible translation ities features function nature sig docstring body train model translate sources targets indicated chosen pairs feature combinations contain feature source target tem instructed target code bodies performing function completion hamel husain hsiang tiferet gazit miltiadis allamanis marc brockschmidt codesearchnet challenge evaluating state semantic code search arxiv preprint richard jones restructuredtext primer docutils sourceforge net march olga kovaleva alexey romanov anna rogers revealing arxiv preprint anna rumshisky dark secrets bert mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy ves stoyanov luke zettlemoyer bart denoising sequence sequence pre training natural language generation translation comprehension arxiv preprint yang liu mirella lapata text arxiv marization pretrained encoders preprint numpydoc maintainers numpydoc string guide technical report laura moreno jairo aponte giriprasad sridhara andrian marcus lori pollock shanker automatic generation ural language summaries java classes international conference gram comprehension icpc pages ieee laura moreno gabriele bavota massimiliano penta rocco oliveto andrian marcus gerardo canfora automatic proceedings tion release notes acm sigsoft international symposium foundations software engineering pages alec radford karthik narasimhan tim salimans ilya sutskever improving language understanding generative pre training url amazonaws com assets researchcovers languageunsupervised language understanding paper pdf colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei peter liu exploring limits transfer learning unied text text transformer arxiv preprint veselin raychev martin vechev eran hav code completion statistical proceedings language models acm sigplan conference programming language design implementation pages simone scalabrino gabriele bavota pher vendome mario linares vasquez denys poshyvanyk rocco oliveto tomatically assessing code understandability far ieee acm ternational conference automated software engineering ase pages ieee alexey svyatkovskiy shao kun deng shengyu intellicode neel sundaresan compose code generation transformer arxiv preprint alexey svyatkovskiy ying zhao shengyu neel sundaresan pythia assisted code completion system proceedings acm sigkdd international conference knowledge discovery data mining pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin tention need advances neural information processing systems pages elena voita rico sennrich ivan titov evolution representations transformer study machine tion language modeling objectives arxiv preprint yao wan zhou zhao min yang guandong haochao ying jian philip improving automatic source code tion deep reinforcement learning ceedings acm ieee international conference automated software ing pages wenhua wang yuqun zhang zhengran zeng guandong trans based framework unifying code arxiv preprint rization code search thomas wolf lysandre debut victor sanh julien chaumond clement delangue thony moi pierric cistac tim rault remi louf morgan funtowicz jamie brew huggingface transformers state arxiv art natural language processing pengcheng yin graham neubig tactic neural model general purpose code generation proceedings annual meeting association computational linguistics volume long papers pages vancouver canada association computational linguistics juan zhai xiangzhe shi minxue pan shiqing lei weifeng zhang lin tan xiangyu zhang cpc automatically classifying propagating natural language comments program analysis
