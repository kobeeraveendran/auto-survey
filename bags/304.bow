m l c s c v v x r deep learning models automatic summarization big thing nlp pirmin lemberger com onepoint sablons paris groupeonepoint com abstract text summarization nlp task aims convert textual document shorter keeping meaning possible pedagogical article reviews number recent deep learning tectures helped advance research eld discuss particular applications pointer networks hierarchical transformers reinforcement learning assume basic knowledge chitecture transformer networks nlp helpful nlp task quarter century able search web querying search engine couple relevant keywords tool internet useless garbage dump data google s pagerank algorithm redened expect far relevance search results concerned recently semantic processing added wizardry helps engine interpret query expressed plain language distant future pinpoint documents engaging short kind conversation search engine bookseller important dierence bookseller search engine hesitant book read try ask bookseller summarize sentences kind summarization task long totally reach classic rule based nlp approaches considered realistic foreseeable future slowly things changing recent progress deep learning models nlp moment imagine drop list input eld favorite search engine allow set length automatic summary given document sentence sentences page summary helpful actually possible quickly prove useful ubiquitous improving document search help multitude tasks instance help scientists dizzying ow publications elds like medicine ai prosaically help producing short product descriptions online stores logues large handled manually examples applications automatic summarization described instance larger documents hundreds pages like novels generic summarization tools belong realm science ction thanks surprising exibility deep learning models wait long tools summarize page documents sentences specic areas knowledge aim article describe recent data sets deep learning architectures brought little closer goal dicult task summarizing task dicult number reasons common nlp tasks like translation instance given document summary objectively best general rule judged equally good human hard dene precisely good summary score use evaluation good training data long scarce expensive collect human evaluation summary subjective involves judgments like style coherence completeness readability unfortunately score currently known easy compute faithful human judgment rouge score best obvious shortcomings shall rouge simply counts number words grams common summary produced machine reference summary written human precisely reports combination corresponding recall precision recall overlapping n grams words reference summary precision overlapping n grams words machine summary combination reported rouge n geometric mean known score rouge score faithfully reect human judgment advantage computational simplicity takes count exibility associated multiple summaries result rearranging words valid summary types summarization systems extractive summarization systems select number segments source document summary advantage proach resulting summary guaranteed grammatically correct general extractive systems achiever high rouge scores reliable option discuss abstractive summarization systems hand generate words sentences reformulate meaning source human writer viewed compression systems attempt preserve meaning kind systems obviously dicult develop involves ability paraphrase formation include external knowledge describe instances kinds better data recently main data set training summarization models cnn daily mail data set contains examples news ticle paired multiline summary detailed examination revealed limitations data set bias evaluation ability system perform text summarization turned instance useful information spread unevenly source beginning documents summaries contain large fragments source certainly best way teaching system produce good abstractive summaries things changed recently bigpatent dataset instance contains millions patent documents summaries leviate shortcomings novel approach produce growing data sets training rization models uses video transcripts talks given international scientic conferences basic assumption transcripts good starting point producing high quality summaries scientic papers transcript directly summary paper authors talksumm method propose create summary retrieving sequence relevant sentences paper presented talk sentence deemed relevant depending words speaker uses describe talk assuming given sentence paper mind given point time clever architectures improved cost functions section describe neural network models developed cently summarization task aim completeness course merely illustrate diversity ideas proposed tackle fundamental nlp problem basic neural network architectures possible learn kind task architectures lstm recurrent neural networks rnn bert transformer models tention mechanism figure basic encoder decoder architecture attention xi input token embeddings coeci attention weights step t hi context vectors ht sentence embedding step t obtained weighting context vectors attention weights decoder states embeddings generated token inference time ground truth tokens training time teacher forcing pt vocab probability distribution time t xed vocabulary readers unfamiliar topics recommend links provide excellent introductions figure sents architecture converts sequence tokens sequence possibly dierent length denes vectors refer talking figure sketches transformer network self attention cies embeddings hidden vectors roughly speaking transformer converts sequence token embeddings xi sequence context aware embeddings hi input vectors typically include positional figure bert encoder transformer architecture core idea transformer smart implementation attention mechanism allows computations parallelized eciently gpu possible classical rnn input vector xj sum token embedding position embedding outputs hi context aware token embeddings information needed contrast rnn networks mutation symmetry inputs transformer summarizing stuttering rst architecture present addresses abstractive summarization task early attempts apply vanilla architectures summarization revealed number issues straightforward approach factual details source document like dates locations phone numbers reproduced incorrectly summary nite vocabulary prevents words like proper names taken accounts unnecessary repetitions source fragments occur words model tends stutter figure shows examples unwanted behaviors authors pose improvements vanilla attention mechanism mitigate shortcomings overcome nite vocabulary limitation allow network copy word directly source use summary needed precise mechanism called pointer network remember figure section pointer gen coverage contains output system proposed fragments summary shown blue factual errors red unnecessary repetitions green vanilla network decoder computes probability distribution pt time step t words w xed nite vocabulary usual pt vocab computed softmax layer takes attention context vector ht decoder state st inputs pointer network additional copy probability pcopy computed represents probability word copied source generated decoder probability pcopy computed sigmoid layer having ht st xt vectors inputs gure word actually copied mined attention weights decoder puts time word wi source putting probability model produce word w given following mixture pcopy pt pcopy wi w second avoid repeating segments authors dene coverage vector ct time step t estimates attention word wi source received decoder time t ct coverage vector dierent places network inform attention mechanism charge computing attention weights addition usual dependence encoder context vector hi word wi decoder state st decoder aware words paying attention second correct loss function remember time step t weight attention word wi ct attention word received past word wi receives attention time t received past cost function penalize large values ct way penalize attention repeated words denes additional term loss function time step t sum input tokens ct added additional hyperparameter usual negative log likelihood lt t target word w ml log t train set lt coverage min ct lt lt ml lt coverage results additional tricks shown gure documents sequences contextualized sentences example illustrates recent ideas dened new sota extractive summary task builds directly key idea lead bert model transfer learning based clever training task transformer encoder let s little detail summarize hibert architecture document summarization basic observation extractive classication cast tence tagging problem simply train model identify sentence document kept summary purpose ert architecture uses nested encoder transformers illustrated gure rst transformer encoder classic sentence encoder kth transforms sequence words wk sentence document summarized sentence embedding hk vector conventionally identied context vector end tence token wk wk second transformer encoder sits document coder transforms sequence sentence embeddings hd sequence document aware sentence embeddings dd embeddings turn converted sequence probabilities pd figure hibert architecture involves hierarchy transformer encoders classify sentence document summary pj probability jth sentence summary training complex hierarchical network scratch impractical require unrealistic document summary pairs known best strategy train complex network limited data use transfer learning purpose hibert architecture rst pretrained auxiliary task consists predicting sentences randomly masked large corpus documents turing english mathematician highly inuential turing widely considered father articial intelligence mask turing english mathematician turing widely considered father articial intelligence figure shows architecture masked sentence prediction task adds transformer decoder hibert architecture order convert document aware sentence embedding dk sequence kth sentence masked generate words wk word step decoder uses context vector hi document aware sentence embedding dk document encoder figure architecture masked sentence prediction task tence transformer decoder added hibert architecture recover words masked sentence information encapsulated document aware embedding dk trained way network gathers large semantic knowledge requiring expansive labeling procedure second stage aging learned pretraining task network ne tuned actual target task summarization sentence binary tagging task gure describes masked sentence prediction task obviously reminiscent sentence level masked language model mlm pretraining original bert model remember mlm task consisted recovering randomly masked words sentences reinforcement learning comes rescue explained earlier central issue summarization task lack unique best summary rouge score takes account level ignores order words grams generated summary cost function actually like minimize like rouge score nal loss function include term strategy followed work present concerns abstractive tion problem score like rouge sequence words wj generated decoder constant respect eters network making backpropagation impossible situation hopeless expectation rouge score sentences wj sampled joint probability distribution wj ned generator dierentiable function parameters way clear minimize loss dened expectation wj p rouge wj actually view generator model reinforcement learning rl agent action time step t generates word wt depending inner state st encapsulates history previous actions need open book rl learn minimize basic result rl known policy gradient theorem states gradient lrl wj p rouge wj log p wj log p wj log p j index j token reinforce algorithm approximates expectation single sample wj distribution wj computed generator lrl log j practice scores like rouge large variance hinders vergence gradient descent fortunately enhance speed convergence comparing wj baseline b dependent wj change gradient lrl readily veried considerably reduce variance dramatically improve convergence lrl wj log j main point nd appropriate baseline idea work discussing baseline equal rouge score sequence words wj generator actually generates inference time remember sequence words successively maximize conditional probabilities computed softmax decoder step t wt arg max wt choice baseline b called self critical sequence training scst altogether reinforcement loss term reads lrl rouge wj wj log wt sampled successively t wt successively maximizes t j loss term prompts p generate word sequences wj rouge score larger sequence wj currently generated decoder benets including scst reinforcement learning term lrl loss function rst motivated construction lrl rst place makes possible use non dierentiable score like rouge stochastic gradient descent training procedure second benet cures called exposure bias exposure bias results classic teacher forcing procedure typically train model procedure trains decoder rnn ground truth words w j train set inference time decoder course use generated tokens wj result accumulation errors scst choice baseline b amounts train decoder distribution actually inference time w nal loss function weighted sum reinforcement ing loss lrl standard maximum likelihood objective lml takes account non uniqueness summaries point certainly incentive model produce readable messages hand favors readable sentences basically denes language model order avoid repetitions authors use enhanced attention mechanism involves pointer network similar described rst example s models described previous section use deep learning implement purely statistical approach summarization task recent research tries nd better loss functions researchers recital instance explore interesting idea good summary answer questions original text allows models work surprisingly short documents sonably expect build system summarize pages novel page techniques rely crunching huge amounts textual data far obvious abstract summarization principle able leverage real world knowledge sense document book summarized unlikely language models initialized clever pretraining tasks capture common sense likely collected sensory experience short term ity building useful summarization tools narrow scope specic areas expertise knowledge basis ontologies available radical step building system better real world understanding arise multimodal learners designed aggregate audio video text modalities movies instance promising results obtained path acknowledgments like thank thomas scialom researcher recital kindly share knowledge pointing attention summarizing summarization page helped kick start exploration deep learning summarization models references bigpatent large scale dataset abstractive coherent marization eva sharma chen li lu wang talksumm dataset scalable annotation method tic paper summarization based conference talks guy lev michal shmueli scheuer jonathan herzig achiya jerbi david konopnicki point summarization pointer generator networks abigail peter j liu christopher d manning document level pre training hierarchical bidirectional transformers document summarization xingxing zhang furu wei ming zhou deep reinforced model abstractive summarization romain paulus caiming xiong richard socher rouge package automatic evaluation summaries chin yew lin self critical sequence training image captioning steven j nie etienne marcheret youssef mroueh jarret ross vaibhava goel attention beam search guillaume genthial blog understanding lstm networks christopher olah colah s blog illustrated bert elmo co jay alammar blog illustrated transformer jay alammar blog neural machine translation jointly learning align translate dzmitry bahdanau kyunghyun cho yoshua bengio reinforcement learning introduction richard s sutton andrew g barto mit press cambridge ma answers unite unsupervised metrics reinforced summarization els thomas scialom sylvain lamprier benjamin piwowarski jacopo iano multimodal abstractive summarization videos shruti palaskar jindrich libovicky spandana gella florian metze summarizing summarization github thomas scialom recital
