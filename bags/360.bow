v o n l c s c v v i x r a topic centric unsupervised multi document summarization of scientic and news articles amanuel alambo computer science and engineering wright state university dayton oh alambo edu cori lohstroh oneil center wright state university dayton oh lohstroh edu erik madaus oneil center wright state university dayton oh madaus edu swati padhee computer science and engineering wright state university dayton oh padhee edu brandy foster oneil center wright state university dayton oh brandy edu tanvi banerjee computer science and engineering wright state university dayton oh tanvi edu krishnaprasad thirunarayan computer science and engineering wright state university dayton oh t edu michael raymer computer science and engineering wright state university dayton oh michael edu abstract recent advances in natural language processing have enabled automation of a wide range of tasks including machine translation named entity recognition and sentiment analysis automated summarization of documents or groups of ments however has remained elusive with many efforts limited to extraction of keywords key phrases or key sentences accurate abstractive summarization has yet to be achieved due to the inherent difculty of the problem and limited availability of training data in this paper we propose a topic centric pervised multi document summarization framework to generate extractive and abstractive summaries for groups of scientic articles across fields of study fos in microsoft academic graph mag and news articles from task the posed algorithm generates an abstractive summary by developing salient language unit selection and text generation techniques our approach matches the state of the art when evaluated on automated extractive evaluation metrics and performs better for abstractive summarization on ve human evaluation metrics entailment coherence conciseness readability and grammar we achieve a kappa score of between two co author linguists who evaluated our results we plan to publicly share a human validated gold standard dataset of topic clustered research articles and their summaries to promote research in abstractive summarization index terms abstraction language units multi document summarization text generation hierarchical clustering this effort was sponsored in whole or in part by the air force search laboratory usaf under memorandum of understanding partnership intermediary agreement no the u s government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright notation thereon i introduction with the large number of articles published in the research and media community there is an increasing demand to produce summaries that are coherent concise informative and matical summarization comes in two forms extractive and abstractive extractive summarization is focused on extracting signicant sentences from source documents and has been well studied abstractive summarization aims at ways of fusing or paraphrasing sentences in source documents to form abstractive sentences due to the challenges of capturing abstractive concepts shared among sentences across source documents and synthesizing an informative summary there has been limited progress in abstractive summarization while there are recent advances in unsupervised multi document abstractive summarization they are usually limited to forming summaries by copying words in source documents and re arranging the words to form new sentences these approaches identify salient phrases from sentences in source documents and fuse them to form abstractive summaries thus they do not perform abstraction of sentences our framework consists of two phases an extractive phase and an abstractive phase in the extractive phase we follow a three fold approach first we identify the core article and peripheral articles for each set of related articles second we instantiate clusters using the language units of a core article and perform centroid based clustering to place language units from peripheral articles into the clusters initialized by the language units in a core article third we fuse language units in a cluster using an enhanced multi sentence compression msc technique with a novel algorithm to maximize topical coverage and relevance of a path to the language units in the cluster in the abstractive summarization phase we employ text generation to generate abstractive language units alus and we use msc to fuse the generated alus into an abstractive summary unlike where articles are topically clustered scientic articles in do not come topically grouped therefore for we use topical hierarchical agglomerative clustering hac to cluster articles the key contributions of our study are an alu generation technique using a novel msc based algorithm for selection of informative paths and a gold standard dataset of topical clusters of articles from and their doc abstractive summaries we use abstracts of articles from mag for this study ii related work different techniques have been proposed for unsupervised including sequence to sequence abstractive summarization models neural models with and without attention abstract meaning representation amr and centroid based summarization our approach techniques in centroid based extends the state of the art summarization by employing language unit identication from articles and a novel text generation technique abstractive summarization has received signicant attention due to progress in deep representation learning propose meansum which consists of an autoencoder and a summarization module to produce abstractive summaries the abstractive summarization approach of called ilpsumm includes identication of informative content and clustering of similar sentences from the documents to form summaries extended the technique proposed in by introducing a paraphrastic fusion model called parafuse based on sensitive substitution of target words while lexical tion enables the generation of novel words it is limited when it comes to capturing the context in the source document propose a pointer generator network to summarize news articles from the cnn dailymail dataset further propose a framework that takes an article and a topic and generates a summary specic to the topic however their work is supervised and relies on the availability of human generated training corpus to train their model iii data collection we work with two datasets to better understand and evaluate our proposed approach the benchmark dataset and scientic articles from mag we queried mag for the most cited abstracts for each of the fos published in the fos we used are articial intelligence articial neural network big data case based reasoning cybernetics cyberwarfare data mining data science decision support system electronic warfare expert system human machine interaction ligent agent knowledge based systems machine learning multi agent system prediction algorithms predictive lytics predictive modeling and sensor fusion iv proposed method a extractive phase fig shows the sequence of steps we devised for and extractive summarization the difference in the extractive phase of these tasks is has topic modeling and hierarchical agglomerative clustering in its pipeline fig extractive summarization the pipeline following cluster of abstracts is for each cluster of topics topic modeling for we determine groups of topically related abstracts for each fos we rst build lda topic models for an fos using number of topics in the range of to we then determine the optimal number of topics from an ensemble of the lda models that maximizes the coherence score the topics and thus keywords generated using the lda model that gives the highest coherence score are used for topical hac topical hierarchical agglomerative clustering different topics generated using an lda model can have semantically redundant keywords we thus cluster topics having high similarity among their keywords using hac we use scibert embeddings to represent each keyword in a topic a topic is then represented as a concatenation of its keywords once each topic is the representations of represented we conduct topical hac to determine the number of clusters for a collection of topics we ran hac for several clusters ranging from to the total number of topics we use silhouette coefcient to determine the optimal number of clusters we introduce a topical similarity metric for suring the similarity between a pair of topics each keyword in a topic is compared with all the keywords in another topic and the sum of highest similarity scores is preserved i topic j topic j itopic i where topic j maximum of cosine similarities between term i and terms in topic j cluster topic ids table i topics and their cluster membership a abstract is assigned to a topic that is the most dominant among all possible topics the abstract addresses table i shows the three clusters and their constituent topic ids table ii shows topical distribution among abstracts for a selected eld of study it can be seen two abstracts have the same dominant topic these abstracts form a set of documents on which we perform multi document summarization abstract id dominant topic dominant topic topic keywords inspire state device accelerator small size high power ved advantage inspire state device accelerator small size high power ved advantage table ii abstracts and dominant topic core and peripheral articles identication we identify the core article from a cluster of articles based on how similar an article is to other articles computes the article similarity score of an article an article with the highest cumulative similarity with other articles in a cluster is chosen as the core article the rest of the articles in the cluster are peripheral articles casi i jc j n where i j n number of articles in the cluster the cluster of articles sim based cosine similarity centroid based clustering after core and peripheral ticles are identied we generate extractive language units elus from the core and the peripheral articles recent studies in centroid based summarization utilized sentences in documents as standalone elus to initiate clusters and to quantify semantic relatedness however this approach breaks the interdependence among sentences in a document and eventually leads to incoherent summaries we address this issue by identifying the sentences that are interdependent using neural coreference resolution and preserving them as one elu once the elus from the core article have instantiated clusters the elus from the peripheral articles are placed into a cluster based on the cosine similarity between the embedding of an elu from the peripheral article and the embeddings of the elus from the core article an elu embedding is constructed by concatenating the embeddings of the sentences using sent bert and performing dimensionality tion to units using t sne the purpose of dimensionality reduction is to have a uniform dimension among elus even when they contain different number of sentences so that cosine similarity can be computed multi sentence compression the number of clusters formed in the centroid based clustering stage is the same as the number of elus in the core article after clusters of elus are formed we build word graphs for each cluster fig shows a sample word graph constructed for a cluster consisting of the following elus radars are required to limit emissions in adjacent bands but traditional rectangular pulses have high out of band emissions millimeter wave radars are popularly used in last mile radar based defense systems fig word graph for two elus using tokens and pos tags of the tokens are used for a node we develop an algorithm for extracting paths based on topical coverage and relevance a path is selected using an additional criterion that a candidate path should at least span two elus in the cluster next we generate topically informative and relevant paths from the word graph while maintaining the word summary limit topical coverage measures how well a path covers the dominant topics discussed by the articles of the elus relevance measures how relevant a path is to the elus the cumulative score of a path is determined by a weighted sum of topical coverage and relevance we experimented with values of in the range of to a topical coverage formulation ctopics icpath cpath kcctopics kc where cpath candidate path ctopics cluster of topics topical coverage is measured with respect to the cluster of topics path relevance formulation celu where cpath candidate path celu cluster of elus vectorial representation of candidate path vectorial representation of cluster of elus path relevance is measured with respect to the elus c cumulative score ctopics celu a path is selected from the word graph if the path is longer than the average minimum length of a sentence in an fos or topic and smaller than the average maximum length of a sentence if the combined topical coverage and relevance for the path meets or exceeds a threshold of if a path picked from the word graph is semantically similar to an already selected path by an order of threshold of or more we compare the combined topical coverage and relevance of the two paths and keep the one with a higher score and remove the other the selection of is based on empirical observations b abstractive phase fig shows the steps we followed for abstractive tion the difference in the abstractive phase of and is has headline generation component elu the ability to repair ship and work together will be the key to a stable coalition alu it s a good time for a new political party that can bring ity and development table iii alu generated using m parameters and generates candidate alus while ne tuning we set the temperature to number of generated samples to top k random sampling to to generate more alus and minimize redundancy we train the for epochs with a batch size of and attain a loss of we select an alu that maximizes semantic similarity and minimizes syntactic similarity with the elu used for generation we use the normalized sum of and for syntactic similarity we introduce an abstractiveness score for an alu as shown in we use bart for headline generation for each article that is later used for alu generation along with an elu fig alus generation using elu elu elu elu alu alu where alu abstractive language unit elu extractive language unit cossimxbert cosine similarity on x dimension bert embeddings we select an alu that gives the highest abstractiveness score from candidate alus table iii shows a sample elu and highest scoring alu generated multi sentence compression after generating alus for a cluster we build a word graph and run our msc algorithm as used in the extractive phase i e the same ranking formulation and path selection algorithm is used for selecting informative paths from a word graph built this time from a cluster of alus fig shows a cluster of alus and the generated fused paths that form the nal abstractive summary v results and discussion a extractive evaluation we use rouge metrics for evaluating extractive summaries taking the source articles as the reference summary fig abstractive summarization pipeline abstractive language unit alu generation we start our abstractive phase with a pragmatic assumption that the title headline of an article is an abstraction of the individual extractive language units elus within the same article we propose a method to generate an alu for an elu using the elu and title headline as prompts for generating text combining bidirectional encodings of the title headline with an elu enables generating abstractive text for elus consisting of two or more sentences we encode each sentence using sentence bert and then we concatenate these representations next we perform dimensionality reduction using t sne to encode an elu for encoding a title headline we use sentence bert without dimensionality reduction we ne tune a model for an fos fig and use the tuned model to generate alus given a concatenation of the bidirectional encodings of the elu and the title headline of an article we ne tune a model such that it has in addition to the human evaluation metrics we also use copy rate for evaluating abstractive summaries copy rate assesses the rate of novel word generation as shown in table vi our framework achieves the lowest copy rate indicating that we are able to generate more novel words task model ilpsumm parafuse our approach ilpsumm parafuse our approach copy rate table vi copy rate evaluation fig comparison of abstractive summaries human evaluator evaluator i evaluator ii model ilpsumm parafuse our approach ilpsumm parafuse our approach entailment coherence conciseness readability grammar table vii abstractive summarization results human evaluator evaluator i evaluator ii model ilpsumm parafuse our approach ilpsumm parafuse our approach entailment coherence conciseness readability grammar table viii abstractive summarization results experimental results show that our proposed approach forms signicantly better in human abstractive evaluation rics and copy rate this is mainly due to the alu generation using a ne tuned model and minimizing the syntactic similarity of generated alus abstractive evaluation for our proposed approach consistently performs better than summ or parafuse on the human evaluation criteria summ and parafuse show better results in entailment in contrast our approach generally performs comparably across the criteria thus we can clearly infer generating summaries that are entailed by source articles is easier than generating summaries that are coherent concise readable and ical this is because if summaries have words copied from the source articles it is highly likely that they are entailed by the source articles since the baseline approaches ilpsumm and parafuse have higher copy rate they do well in entailment however with our approach having a low copy rate and fig candidate alus and compressed alu paths model ilpsumm parafuse our proposed method r l table iv extractive evaluation and extractive evaluation results are shown in tables iv and v respectively it can be seen that our proposed method performs comparably to the baseline approaches on and rouge l metrics b abstractive evaluation since metrics based on lexical overlap such as rouge favor extractive summaries we conduct abstractive summary evaluation using ve human evaluation metrics we propose for this study the metrics have been developed in consultation with two co author linguists the ve human evaluation metrics are entailment coherence conciseness readability and grammar our co author linguists evaluated the abstractive summaries on a scale of to on each of the human evaluation metrics our co author linguists independently reviewed the and results generated using our approach ilpsumm and parafuse when determining the rating for each criterion they used the source articles to validate the summary then they used their own compiled summaries to compare to the resulting abstractive summary the closer the tive summary was to the details in their notes the higher the entailment the human evaluators judged coherence by sentence structure and whether the sentences showed logical progression when examining conciseness they looked for areas of the abstractive summary that were repeated they also noted whether a sentence carried the logical progression of the paragraph for readability they did not take grammar or spacing into consideration they looked for sentence fragments word order and took note of instances of missing subjects or verbs when rating grammar they gave the abstractive summary a lower rating for comma splices or extra spacing than if there were fragments or inappropriate punctuation model ilpsumm parafuse our proposed method r l table v extractive evaluation generating summaries that are entailed by the sources articles is difcult yet our proposed approach still has the best entailment score for task abstractive evaluation for our proach performs better than the baseline approaches in ence conciseness readability and grammar across two of our human evaluators while marginally losing to the baselines cording to one of our evaluators as for entailment ilpsumm performs the best which is attributed to the high copy rate by ilpsumm even though our approach generates signicantly more novel words than ilpsumm or parafuse we lose to the best entailment score by only further ilpsumm parafuse and our proposed approach perform generally better on than on we surmise this is due to the headline generation task for while we use provided titles for vi conclusion and future work we proposed an unsupervised multi document abstractive summarization framework that when given a set of documents from mag automatically clusters the documents and then generates summaries for each cluster our framework consists of extractive and abstractive phases in the extractive phase we use coreference resolution to extract groups of dependent sentences from source articles and centroid based clustering followed by an enhanced multi sentence sion algorithm to generate topically informative and relevant summaries in the abstractive phase we use text generation technique to generate abstractive language units that are thesized into an abstractive summary the number of maries in our proposed method is adaptively determined based on the semantic analysis of the topics discussed in the ments we introduce a dataset of topically clustered groups of scientic articles across fields of study and their abstractive summaries results show that our proposed approach performs better than state of the art centroid based summarization techniques on human evaluation metrics and copy rate in the future we plan to use additional knowledge and metadata such as citation relationships among scientic articles for document summarization vii acknowledgment the authors are deeply grateful to daniel foose for helping with developing scripts for efcient data collection from mag references q zhou n yang f wei s huang m zhou and t zhao a joint sentence scoring and selection framework for neural extractive document summarization ieee acm transactions on audio speech and language processing vol pp e chu and p liu meansum a neural model for unsupervised document abstractive summarization in international conference on machine learning pp m t nayeem t a fuad and y chali abstractive unsupervised multi document summarization using paraphrastic sentence fusion in proceedings of the international conference on computational linguistics pp s banerjee p mitra and k sugiyama multi document abstractive summarization using ilp based multi sentence compression in fourth international joint conference on articial intelligence k filippova multi sentence compression finding shortest paths in word graphs in proceedings of the international conference on computational linguistics coling pp y zhao x shen w bi and a aizawa unsupervised rewriter for multi sentence compression in proceedings of the annual meeting of the association for computational linguistics pp a sinha z shen y song h ma d eide b hsu and k wang an overview of microsoft academic service mas and applications in proceedings of the international conference on world wide web pp t shi y keneshloo n ramakrishnan and c k reddy neural stractive text summarization with sequence to sequence models arxiv preprint c khatri g singh and n parikh abstractive and extractive text summarization using document context vector and recurrent neural networks arxiv preprint p j liu m saleh e pot b goodrich r sepassi l kaiser and n shazeer generating wikipedia by summarizing long sequences arxiv preprint a see p j liu and c d manning get to the point tion with pointer generator networks arxiv preprint k liao l lebanoff and f liu abstract meaning representation for multi document summarization arxiv preprint f liu j flanigan s thomson n sadeh and n a smith toward stractive summarization using semantic representations arxiv preprint a vaswani n shazeer n parmar j uszkoreit l jones a n gomez kaiser and i polosukhin attention is all you need in advances in neural information processing systems pp j devlin m chang k lee and k toutanova bert pre training of deep bidirectional transformers for language understanding arxiv preprint k krishna and b v srinivasan generating topic oriented summaries the conference of using neural attention in proceedings of the north american chapter of the association for computational linguistics human language technologies volume long papers pp d m blei a y ng and m i jordan latent dirichlet allocation journal of machine learning research vol no jan pp i beltagy k lo and a cohan scibert a pretrained language model for scientic text arxiv preprint k lee l he m lewis and l zettlemoyer end to end neural coreference resolution arxiv preprint n reimers and i gurevych sentence bert sentence embeddings using siamese bert networks arxiv preprint y liu fine tune bert for extractive summarization arxiv preprint f boudin and e morin keyphrase extraction for n best reranking in multi sentence compression s narayan s b cohen and m lapata ranking sentences for extractive summarization with reinforcement learning arxiv preprint a radford j wu r child d luan d amodei and i sutskever language models are unsupervised multitask learners openai blog vol no p k thirunarayan t immaneni and m v shaik selecting labels for news document clusters in international conference on application of natural language to information systems springer pp m lewis y liu n goyal m ghazvininejad a mohamed o levy v stoyanov and l zettlemoyer bart denoising sequence to sequence pre training for natural language generation translation and sion arxiv preprint
