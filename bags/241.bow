t c o v c s c v v i x r a a survey and taxonomy of adversarial neural networks for text to image synthesis jorge jonathan haicheng xingquan of computer electrical engineering and computer science florida atlantic university boca raton fl usa email edu edu edu provincial key laboratory of e business nanjing university of finance and economics nanjing china email ustc edu abstract text to image synthesis refers to computational methods which translate human written textual tions in the form of keywords or sentences into images with similar semantic meaning to the text in earlier research image synthesis relied mainly on word to image correlation analysis combined with supervised methods to nd best alignment of the visual content matching to the text recent progress in deep learning dl has brought a new set of unsupervised deep learning methods particularly deep erative models which are able to generate realistic visual images using suitably trained neural network models the change of direction from the computer vision based approaches to articial intelligence ai driven methods ignited the intense interest in industry such as virtual reality recreational professional esports gaming and computer aided design to automatically generate compelling images from text based natural language descriptions in this paper we review the most recent development in the text to image synthesis research domain our goal is to provide value by delivering a comparative review of the state of the art models in terms of their architecture and design our survey rst introduces image synthesis and its challenges and then reviews key concepts such as generative adversarial networks gans and deep convolutional decoder neural networks dcnn after that we propose a taxonomy to summarize gan based text image synthesis into four major categories semantic enhancement gans resolution enhancement gans diversity enhancement gans and motion enhancement gans we elaborate the main tive of each group and further review typical gan architectures in each group the taxonomy and the review outline the techniques and the evolution of different approaches and eventually provide a clear roadmap to summarize the list of contemporaneous solutions that utilize gans and dcnns to generate enthralling results in categories such as human faces birds owers room interiors object tion from edge maps games the survey will conclude with a comparison of the proposed solutions challenges that remain unresolved and future developments in the text to image synthesis domain text to image synthesis generative adversarial network gan deep learning machine keywords learning introduction gans and the variations that are now being proposed is the most interesting idea in the last years in ml in my opinion yann lecun a picture is worth a thousand words while written text provide efcient effective and concise ways for communication visual content such as images is a more comprehensive accurate and wiley interdisciplinary reviews data mining and knowledge discovery equally contributing authors figure early research on text to image synthesis zhu et al the system uses correlation between keywords or keyphrase and images and identies informative and picturable text units then searches for the most likely image parts conditioned on the text and eventually optimizes the picture layout conditioned on both the text and image parts ligible method of information sharing and understanding generation of images from text descriptions i e text to image synthesis is a complex computer vision and machine learning problem that has seen great progress over recent years automatic image generation from natural language may allow users to describe visual elements through visually rich text descriptions the ability to do so effectively is highly desirable as it could be used in articial intelligence applications such as computer aided design image editing chen et al yan et al game engines for the development of the next generation of video et al and pictorial art generation elgammal et al traditional learning based text to image synthesis in the early stages of research text to image synthesis was mainly carried out through a search and in order to connect supervised learning combined process zhu et al as shown in figure text descriptions to images one could use correlation between keywords or keyphrase images that identies informative and picturable text units then these units would search for the most likely image parts conditioned on the text eventually optimizing the picture layout conditioned on both the text and the image parts such methods often integrated multiple articial intelligence key components including natural language processing computer vision computer graphics and machine learning the major limitation of the traditional learning based text to image synthesis approaches is that they lack the ability to generate new image content they can only change the characteristics of the given training images alternatively research in generative models has advanced signicantly and delivers solutions to learn from training images and produce new visual content for example yan et al models each image as a composite of foreground and background in addition a layered generative model with disentangled latent variables is learned using a variational encoder to generate visual content because the learning is customized conditioned by given attributes the generative models of can generate images with respect to different attributes such as gender hair color age as shown in figure gan based text to image synthesis although generative model based text to image synthesis provides much more realistic image synthesis results the image generation is still conditioned by the limited attributes in recent years several papers have been published on the subject of text to image synthesis most of the contributions from these papers rely on multimodal learning approaches that include generative adversarial networks and deep figure supervised learning based text to image synthesis yan et al the supervised ing process aims to learn layered generative models to generate visual content because the learning is customized conditioned by the given attributes the generative models of can generative images with respect to different attributes such as hair color age figure generative adversarial neural network gan based text to image synthesis huang et al gan based text to image synthesis combines discriminative and generative learning to train neural networks resulting in the generated images semantically resemble to the training samples or lored to a subset of training images i e conditioned outputs is a feature embedding function which converts text as feature vector z is a latent vector following normal distributions with zero mean denotes a synthetic image generated from the generator using latent vector z and the text features as the input d denotes the prediction of the discriminator based on the input x the generated image and text information of the generated image the explanations about the generators and discriminators are detailed in section figure a visual summary of gan based text to image synthesis process and the summary of gan based frameworks methods reviewed in the survey convolutional decoder networks as their main drivers to generate entrancing images from text wu et al reed et al goodfellow et al xu et al odena et al first introduced by ian goodfellow et al goodfellow et al generative adversarial networks gans consist of two neural networks paired with a discriminator and a generator these two models compete with one another with the generator attempting to produce synthetic fake samples that will fool the discriminator and the discriminator attempting to differentiate between real genuine and synthetic samples because gans adversarial training aims to cause generators to produce images similar to the real training images gans can naturally be used to generate synthetic images image synthesis and this process can even be customized further by using text descriptions to specify the types of images to generate as shown in figure much like text to speech and speech to text conversion there exists a wide variety of problems that text to image synthesis could solve in the computer vision eld specically reed et al haynes et al nowadays researchers are attempting to solve a plethora of computer vision lems with the aid of deep convolutional networks generative adversarial networks and a combination of multiple methods often called multimodal learning methods reed et al for simplicity multiple learning methods will be referred to as multimodal learning hereafter ngiam et al searchers often describe multimodal learning as a method that incorporates characteristics from several methods algorithms and ideas this can include ideas from two or more learning approaches in order to create a robust implementation to solve an uncommon problem or improve a solution reed et al yang et al li et al dash et al baltrusaitis et al in this survey we focus primarily on reviewing recent works that aim to solve the challenge of text image synthesis using generative adversarial networks gans in order to provide a clear roadmap we propose a taxonomy to summarize reviewed gans into four major categories our review will elaborate the motivations of methods in each category analyze typical models their network architectures and possible drawbacks for further improvement the visual abstract of the survey and the list of reviewed gan frameworks is shown in figure the remainder of the survey is organized as follows section presents a brief summary of existing works on subjects similar to that of this paper and highlights the key distinctions making ours unique section gives a short introduction to gans and some preliminary concepts related to image generation as they are the engines that make text to image synthesis possible and are essential building blocks to achieve photo realistic images from text descriptions section proposes a taxonomy to summarize gan based text to image synthesis discusses models and architectures of novel works focused solely on text to image synthesis this section will also draw key contributions from these works in relation to their applications section reviews gan based text to image synthesis benchmarks performance metrics and comparisons including a simple review of gans for other applications in section we conclude with a brief summary and outline ideas for future interesting developments in the eld of to image synthesis related work with the growth and success of gans deep convolutional decoder networks and multimodal ing methods these techniques were some of the rst procedures which aimed to solve the challenge of image synthesis many engineers and scientists in computer vision and ai have contributed through tensive studies and experiments with numerous proposals and publications detailing their contributions because gans introduced by goodfellow et al are emerging research topics their practical applications to image synthesis are still in their infancy recently many new gan architectures and signs have been proposed to use gans for different applications e using gans to generate tal texts wang and wan or using gans to transform natural images into cartoons chen et al although gans are becoming increasingly popular very few survey papers currently exist to marize and outline contemporaneous technical innovations and contributions of different gan tures hong et al creswell et al survey papers specically attuned to analyzing different contributions to text to image synthesis using gans are even more scarce we have thus found two surveys huang et al wu et al on image synthesis using gans which are the two most closely related publications to our survey objective in the following paragraphs we briey summarize each of these surveys and point out how our objectives differ from theirs in huang et al the authors provide an overview of image synthesis using gans in this survey the authors discuss the motivations for research on image synthesis and introduce some ground information on the history of gans including a section dedicated to core concepts of gans namely generators discriminators and the min max game analogy and some enhancements to the inal gan model such as conditional gans addition of variational auto encoders in this survey we will carry out a similar review of the background knowledge because the understanding of these preliminary concepts is paramount for the rest of the paper three types of approaches for image ation are reviewed including direct methods single generator and discriminator hierarchical methods two or more generator discriminator pairs each with a different goal and iterative methods each generator discriminator pair generates a gradually higher resolution image following the introduction huang et al discusses methods for text to image and image to image synthesis respectively and also describes several evaluation metrics for synthetic images including inception scores and frechet inception distance fid and explains the signicance of the discriminators acting as learned loss tions as opposed to xed loss functions different from the above survey which has a relatively broad scope in gans our objective is heavily focused on text to image synthesis although this topic text to image synthesis has indeed been ered in huang et al they did so in a much less detailed fashion mostly listing the many different works in a time sequential order in comparison we will review several representative methods in the eld and outline their models and contributions in detail similarly to huang et al the second survey paper wu et al begins with a standard introduction addressing the motivation of image synthesis and the challenges it presents followed by a section dedicated to core concepts of gans and enhancements to the original gan model in addition the paper covers the review of two types of applications unconstrained applications of image thesis such as super resolution image inpainting and constrained image synthesis applications namely image to image text to image and sketch to image and also discusses image and video editing using gans again the scope of this paper is intrinsically comprehensive while we focus specically on text to image and go into more detail regarding the contributions of novel state of the art models other surveys have been published on related matters mainly related to the advancements and plications of gans zhang et al but we have not found any prior works which focus specically on text to image synthesis using gans to our knowledge this is the rst paper to do so preliminaries and frameworks in this section we rst introduce preliminary knowledge of gans and one of its commonly used variants conditional gan i e cgan which is the building block for many gan based text to image synthesis models after that we briey separate gan based text to image synthesis into two types simple gan frameworks vs advanced gan frameworks and discuss why advanced gan architecture for image synthesis notice that the simple vs advanced gan framework separation is rather too brief our taxonomy in the next section will propose a taxonomy to summarize advanced gan frameworks into four categories based on their objective and designs generative adversarial neural network before moving on to a discussion and analysis of works applying gans for text to image synthesis there are some preliminary concepts enhancements of gans datasets and evaluation metrics that are present in some of the works described in the next section and are thus worth introducing as stated previously gans were introduced by ian goodfellow et al goodfellow et al in and consist of two deep neural networks a generator and a discriminator which are trained pendently with conicting goals the generator aims to generate samples closely related to the original data distribution and fool the discriminator while the discriminator aims to distinguish between samples from the generator model and samples from the true data distribution by calculating the probability of the sample coming from either source a conceptual view of the generative adversarial network gan architecture is shown in figure the training of gans is an iterative process that with each iteration updates the generator and the discriminator with the goal of each defeating the other leading each model to become increasingly adept at its specic task until a threshold is reached this is analogous to a min max game between the two models according to the following equation min g max d v dd gg dd in eq denotes a multi dimensional sample e an image and z denotes a multi dimensional latent space vector e a multidimensional data point following a predened distribution function such as that of normal distributions dd denotes a discriminator function controlled by parameters d which aims to classify a sample into a binary space gg denotes a generator function controlled by parameters g which aims to generate a sample from some latent space vector for example means using a latent vector z to generate a synthetic fake image and dd means to classify an image as binary output i e true false or in the gan setting the discriminator dd is learned to distinguish a genuine true image labeled as from fake images labeled as therefore given a true image the ideal output from the discriminator dd would be given a fake image generated from the generator the ideal prediction from the discriminator dd gg z would be indicating the sample is a fake image following the above denition the min max objective function in eq aims to learn parameters for the discriminator and generator g to reach an optimization goal the discriminator intends to differentiate true vs fake images with maximum capability maxd whereas the generator intends to minimize the difference between a fake image vs a true image ming in other words the discriminator sets the characteristics and the generator produces elements often images iteratively until it meets the attributes set forth by the discriminator gans are often used with images and other visual elements and are notoriously efcient in generating compelling and convincing photorealistic images most recently gans were used to generate an original painting in an unsupervised fashion radford et al the following sections go into further detail regarding how the generator and discriminator are trained in gans generator in image synthesis the generator network can be thought of as a mapping from one representation space latent space to another actual data creswell et al when it comes to image synthesis all of the images in the data space fall into some distribution in a very complex and dimensional feature space sampling from such a complex space is very difcult so gans instead train a generator to create synthetic images from a much more simple feature space usually random noise called the latent space the generator network performs up sampling of the latent space and is usually real data sample from real data sigmoid function real fake random noise vector z fake sample from generator figure a conceptual view of the generative adversarial network gan architecture the generator is trained to generate synthetic fake resemble to real samples from a random noise distribution the fake samples are fed to the discriminator along with real samples the discriminator is trained to differentiate fake samples from real samples the iterative training of the generator and the discriminator helps gan deliver good generator generating samples very close to the underlying training samples a deep neural network consisting of several convolutional fully connected layers creswell et al the generator is trained using gradient descent to update the weights of the generator network with the aim of producing data in our case images that the discriminator classies as real discriminator the discriminator network can be thought of as a mapping from image data to the probability of the image coming from the real data space and is also generally a deep neural network consisting of several convolution fully connected layers however the discriminator performs down sampling as opposed to up sampling like the generator it is trained using gradient descent but its goal is to update the weights so that it is more likely to correctly classify images as real or fake in gans the ideal outcome is for both the generator s and discriminator s cost functions to converge so that the generator produces photo realistic images that are indistinguishable from real data and the discriminator at the same time becomes an expert at differentiating between real and synthetic data this however is not possible since a reduction in cost of one model generally leads to an increase in cost of the other this phenomenon makes training gans very difcult and training them simultaneously both models performing gradient descent in parallel often leads to a stable orbit where neither model is able to converge to combat this the generator and discriminator are often trained independently in this case the gan remains the same but there are different training stages in one stage the weights of the generator are kept constant and gradient descent updates the weights of the discriminator and in the other stage the weights of the discriminator are kept constant while gradient descent updates the weights of the generator this is repeated for some number of epochs until a desired low cost for each model is reached salimans al cgan conditional gan conditional generative adversarial networks cgan are an enhancement of gans proposed by mirza and osindero shortly after the introduction of gans by goodfellow et al the objective function of the cgan is dened in eq which is very similar to the gan objective function in eq except that the inputs to both discriminator and generator are conditioned by a class label min g max d v dd gg e e dd the main technical innovation of cgan is that it introduces an additional input or inputs to the original gan model allowing the model to be trained on information such as class labels or other conditioning variables as well as the samples themselves concurrently whereas the original gan was trained only with samples from the data distribution resulting in the generated sample reecting the general data distribution cgan enables directing the model to generate more tailored outputs sample from real data real still bird images sigmoid function real fake random noise vector z fake sample from generator condition vector y still bird figure a conceptual view of the conditional gan architecture the generator generates samples from a random noise distribution and some condition vector in this case text the fake samples are fed to the discriminator along with real samples and the same condition vector and the discriminator calculates the probability that the fake sample came from the real data distribution in figure the condition vector is the class label text string red bird which is fed to both the generator and discriminator it is important however that the condition vector is related to the real data if the model in figure was trained with the same set of real data red birds but the condition text was yellow sh the generator would learn to create images of red birds when conditioned with the text yellow sh note that the condition vector in cgan can come in many forms such as texts not just limited to the class label such a unique design provides a direct solution to generate images conditioned by predened specications as a result cgan has been used in text to image synthesis since the very rst day of its invention although modern approaches can deliver much better text to image synthesis results simple gan frameworks for text to image synthesis in order to generate images from text one simple solution is to employ the conditional gan cgan designs and add conditions to the training samples such that the gan is trained with respect to the underlying conditions several pioneer works have followed similar designs for text to image synthesis an essential disadvantage of using cgan for text to image synthesis is that that it can not handle complicated textual descriptions for image generation because cgan uses labels as conditions to strict the gan inputs if the text inputs have multiple keywords or long text descriptions they not be used simultaneously to restrict the input instead of using text as conditions another two proaches reed et al dash et al use text as input features and concatenate such features with other features to train discriminator and generator as shown in figure and c to ensure text being used as gan input a feature embedding or feature representation learning bengio et al zhang et al function is often introduced to convert input text as numeric features which are further concatenated with other features to train gans advanced gan frameworks for text to image synthesis motivated by the gan and conditional gan cgan design many gan based frameworks have been proposed to generate images with different designs and architectures such as using multiple tors using progressively trained discriminators or using hierarchical discriminators figure outlines several advanced gan frameworks in the literature in addition to these frameworks many news figure a simple architecture comparisons between ve gan networks for text to image synthesis this gure also explains how texts are fed as input to train gan to generate images conditional gan cgan mirza and osindero use labels to condition the input to the generator and the inator the nal output is discriminator similar to generic gan manifold interpolation aware discriminator gan gan int cls reed et al feeds text input to both generator and discriminator texts are preprocessed as embedding features using function and concatenated with other input before feeding to both generator and discriminator the nal output is discriminator similar to generic gan auxiliary classier gan ac gan odena et al uses an auxiliary sier layer to predict the class of the image to ensure that the output consists of images from different classes resulting in diversied synthesis images text conditioned auxiliary classier gan gan dash et al share similar design as gan int cls whereas the output include both a discriminator and a classier which can be used for classication and e text conditioned semantic classier gan text segan cha et al uses a regression layer to estimate the semantic vance between the image so the generated images are not limited to certain classes and are semantically matching to the text input figure a high level comparison of several advanced gans framework for text to image synthesis all frameworks take text red triangle as input and generate output images from left to right a uses multiple discriminators and one generator durugkar et al nguyen et al b uses multiple stage gans where the output from one gan is fed to the next gan as input zhang et al denton et al c progressively trains symmetric discriminators and generators huang et al and d uses a single stream generator with a hierarchically nested discriminator trained from end to end zhang et al signs are being proposed to advance the eld with rather sophisticated designs for example a recent work gao et al proposes to use a pyramid generator and three independent discriminators each focusing on a different aspect of the images to lead the generator towards creating images that are photo realistic on multiple levels another recent publication cha et al proposes to use criminator to measure semantic relevance between image and text instead of class prediction like most discriminator in gans does resulting a new gan structure outperforming text conditioned auxiliary classier tac gan dash et al and generating diverse realistic and relevant to the input text regardless of class in the following section we will rst propose a taxonomy that summarizes advanced gan works for text to image synthesis and review most recent proposed solutions to the challenge of ing photo realistic images conditioned on natural language text descriptions using gans the solutions we discuss are selected based on relevance and quality of contributions many publications exist on the subject of image generation using gans but in this paper we focus specically on models for text image synthesis with the review emphasizing on the model and contributions for text to image thesis at the end of this section we also briey review methods using gans for other image synthesis applications text to image synthesis taxonomy and categorization in this section we propose a taxonomy to summarize advanced gan based text to image synthesis frameworks as shown in figure the taxonomy organizes gan frameworks into four categories cluding semantic enhancement gans resolution enhancement gans diversity enhancement gans and motion enhancement gags following the proposed taxonomy each subsection will introduce several typical frameworks and address their techniques of using gans to solve certain aspects of the text to mage synthesis challenges gan based text to image synthesis taxonomy although the ultimate goal of text to image synthesis is to generate images closely related to the textual descriptions the relevance of the images to the texts are often validated from different perspectives due to the inherent diversity of human perceptions for example when generating images matching to the description rose owers some users many know the exact type of owers they like and intend to generate rose owers with similar colors other users may seek to generate high quality rose owers with a nice background e garden the third group of users may be more interested in generating owers similar to rose but with different colors and visual appearance e roses begonia and peony the fourth group of users may want to not only generate ower images but also use them to form a meaningful action e a video clip showing ower growth performing a magic show using those owers or telling a love story using the owers from the text to image synthesis point of view the rst group of users intend to precisely control the semantic of the generated images and their goal is to match the texts and images at the semantic level the second group of users are more focused on the resolutions and the qualify of the images in addition to the requirement that the images and texts are semantically related for the third group of users their goal is to diversify the output images such that their images carry diversied visual appearances and are also semantically related the fourth user group adds a new dimension in image synthesis and aims to generate sequences of images which are coherent in temporal order i e capture the motion information based on the above descriptions we categorize gan based text to image synthesis into a taxonomy with four major categories as shown in fig semantic enhancement gans semantic enhancement gans represent pioneer works of gan frameworks for text to image synthesis the main focus of the gan frameworks is to ensure that the generated images are semantically related to the input texts this objective is mainly achieved by using a neural network to encode texts as dense features which are further fed to a second network to generate images matching to the texts resolution enhancement gans resolution enhancement gans mainly focus on generating high qualify images which are semantically matched to the texts this is mainly achieved through a multi stage gan framework where the outputs from earlier stage gans are fed to the second or later stage gan to generate better qualify images diversity enhancement gans diversity enhancement gans intend to diversify the output ages such that the generated images are not only semantically related but also have different types and visual appearance this objective is mainly achieved through an additional component to timate semantic relevance between generated images and texts in order to maximize the output diversity motion enhancement gans motion enhancement gans intend to add a temporal dimension to the output images such that they can form meaningful actions with respect to the text descriptions this goal mainly achieved though a two step process which rst generates images matching to the actions of the texts followed by a mapping or alignment procedure to ensure that images are coherent in the temporal order in the following we will introduce how these gan frameworks evolve for text to image synthesis and will also review some typical methods of each category semantic enhancement gans semantic relevance is one the of most important criteria of the text to image synthesis for most gnas discussed in this survey they are required to generate images semantically related to the text tions however the semantic relevance is a rather subjective measure and images are inherently rich in terms of its semantics and interpretations therefore many gans are further proposed to enhance the text to image synthesis from different perspectives in this subsection we will review several classical approaches which are commonly served as text to image synthesis baseline dc gan deep convolution generative adversarial network dc gan reed et al represents the pioneer work for text to image synthesis using gans its main goal is to train a deep convolutional generative adversarial network dc gan on text features during this process these text features are encoded by another neural network this neural network is a hybrid convolutional recurrent network at the character level concurrently both neural networks have also feed forward inference in the way they condition text features generating realistic images automatically from natural language text is the motivation of several of the works proposed in this computer vision eld however actual articial intelligence ai figure a taxonomy and categorization of advanced gan frameworks for text to image synthesis we categorize advanced gan frameworks into four major categories semantic enhancement gans resolution enhancement gans diversity enhancement gans and motion enhancement gags the relationship between relevant frameworks and their publication date are also outlined as a reference systems are far from achieving this task reed et al liu et al yang et al li et al wang and gupta zhang et al mirza and osindero lately recurrent ral networks led the way to develop frameworks that learn discriminatively on text features at the same time generative adversarial networks gans began recently to show some promise on generating compelling images of a whole host of elements including but not limited to faces birds owers and common images such as room et al dc gan is a multimodal learning model that attempts to bridge together both of the above mentioned unsupervised machine learning algorithms the recurrent neural networks rnn and generative adversarial networks gans with the sole purpose of speeding the generation of text to image synthesis deep learning shed some light to some of the most sophisticated advances in natural language resentation image synthesis wu et al reed et al wang et al huang et al and classication of generic data han et al however a bulk of the latest breakthroughs in deep learning and computer vision were related to supervised learning reed et al even though natural language and image synthesis were part of several contributions on the supervised side of deep learning unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems text based natural language and image synthesis dong et al yang et al reed et al cha et al zhang et al these subproblems are typically subdivided as focused research areas dc gan s contributions are mainly driven by these two research areas in order to generate plausible images from natural language dc gan contributions revolve around developing a straightforward yet effective gan architecture and training strategy that allows natural text to image synthesis these contributions are primarily tested on the caltech ucsd birds and flowers datasets each image in these datasets carry ve text descriptions these text descriptions were created by the research team when setting up the evaluation environment the gans model is subsequently trained on several subcategories subcategories in this research represent the training and testing sub datasets the performance shown by these experiments display a promising yet effective way to generate images from textual natural language descriptions reed et al dc gan extensions following the pioneer dc gan framework reed et al many researches propose revised work structures e different discriminaotrs in order to improve images with better semantic relevance to the texts based on the deep convolutional adversarial network dc gan network architecture gan cls with image text matching discriminator gan int learned with text manifold interpolation and gan int cls which combines both are proposed to nd semantic match between text and age similar to the dc gan architecture an adaptive loss function i e perceptual loss johnson et al is proposed for semantic image synthesis which can synthesize a realistic image that not only matches the target text description but also keep the irrelavant background from source images dong et al regarding to the perceptual losses three loss functions i e pixel struction loss activation reconstruction loss and texture reconstruction loss are proposed in cha et al in which they construct the network architectures based on the dc gan i e gan int in dong et al pixel gan int cls vgg and gan int cls gram with respect to three losses a residual transformation unit is added in the network to retain similar structure of the source image following the dong et al and considering the features in early layers address background while foreground is obtained in latter layers in cnn a pair of discriminators with different architectures i e paired d gan is proposed to synthesize background and foreground from a source image ately vo and sugimoto meanwhile the skip connection in the generator is employed to more precisely retain background information in the source image mc gan when synthesising images most text to image synthesis methods consider each output image as one single unit to characterize its semantic relevance to the texts this is likely problematic because most images naturally consist of two crucial components foreground and background without properly separating these two components it s hard to characterize the semantics of an image if the whole image is treated as a single unit without proper separation in order to enhance the semantic relevance of the images a multi conditional gan gan park et al is proposed to synthesize a target image by combining the background of a source image and a text described foreground object which does not exist in the source image a unique feature of mc gan is that it proposes a synthesis block in which the background feature is extracted from the given image without non linear function i e only using convolution and batch normalization and the foreground feature is the feature map from the previous layer because mc gan is able to properly model the background and foreground of the generated images a unique strength of mc gan is that users are able to provide a base image and mc gan is able to preserve the background information of the base image to generate new images resolution enhancement gans due to the fact that training gans will be much difcult when generating high resolution images a two stage gan i e is proposed in which rough e low resolution images are generated in stage i and rened in stage ii to further improve the quality of generated images the second version of stackgan i e is proposed to use multi stage gans to generate multi scale images a color consistency regularization term is also added into the loss to keep the consistency of images in different scales while stackgan and are both built on the global sentence vector is posed to use attention mechanism i e deep attentional multimodal similarity model damsm to model the multi level information i e word level and sentence level into gans in the following stackgan and will be explained in detail recently dynamic memory generative adversarial network i e dm et al which uses a dynamic memory component is proposed to focus on reningthe initial generated image which is the key to the success of generating high quality images stackgan in zhang et al proposed a model for generating photo realistic images from text descriptions called stackgan stacked generative adversarial network zhang et al in their work they dene a two stage model that uses two cascaded gans each corresponding to one of the stages the stage i gan takes a text description as input converts the text description to a text embedding containing several conditioning variables and generates a low quality image with rough shapes and colors based on the computed conditioning variables the stage ii gan then takes this low quality stage i image as well as the same text embedding and uses the conditioning variables to correct and add more detail to the stage i result the output of stage ii is a photorealistic image that resembles the text description with compelling accuracy one major contribution of stackgan is the use of cascaded gans for text to image synthesis through a sketch renement process by conditioning the stage ii gan on the image produced by the stage i gan and text description the stage ii gan is able to correct defects in the stage i output resulting in high quality images prior works have utilized stacked gans to separate the age generation process into structure and style wang and gupta multiple stages each generating lower level representations from higher level representations of the previous stage huang et al and multiple stages combined with a laplacian pyramid approach denton et al which was troduced for image compression by p burt and e adelson in and uses the differences between consecutive down samples of an original image to reconstruct the original image from its down sampled version burt and adelson however these works did not use text descriptions to condition their generator models conditioning augmentation is the other major contribution of stackgan prior works transformed the natural language text description into a xed text embedding containing static conditioning variables which were fed to the generator reed et al stackgan does this and then creates a gaussian distribution from the text embedding and randomly selects variables from the gaussian distribution to add to the set of conditioning variables during training this encourages robustness by introducing small variations to the original text embedding for a particular training image while keeping the training image that the generated output is compared to the same the result is that the trained model produces more diverse images in the same distribution when using conditioning augmentation than the same model using a xed text embedding zhang et al proposed by the same users as stackgan is also a stacked gan model but organizes the generators and discriminators in a tree like structure zhang et al with multiple stages the rst stage combines a noise vector and conditioning variables with conditional augmentation introduced in zhang et al for input to the rst generator which generates a low resolution image by default this can be changed depending on the desired number of stages each lowing stage uses the result from the previous stage and the conditioning variables to produce gradually higher resolution images these stages do not use the noise vector again as the creators assume that the randomness it introduces is already preserved in the output of the rst stage the nal stage produces a high quality image introduces the joint conditional and unconditional approximation in their designs zhang et al the discriminators are trained to calculate the loss between the image produced by the generator and the conditioning variables measuring how accurately the image represents the scription as well as the loss between the image and real images probability of the image being real or fake the generators then aim to minimize the sum of these losses improving the nal result attentional generative adversarial network xu et al is very similar in terms of its structure to zhang et al discussed in the previous section but some novel ponents are added like previous works reed et al zhang et al a text encoder generates a text embedding with conditioning variables based on the overall sentence additionally the text encoder generates a separate text embedding with conditioning variables based on individual words this process is optimized to produce meaningful variables using a bidirectional recurrent neural work brnn more specically bidirectional long short term memory lstm schuster and paliwal which for each word in the description generates conditions based on the previous word as well as the next word bidirectional the rst stage of generates a low resolution image based on the sentence level text embedding and random noise vector the output is fed along with the level text embedding to an attention model which matches the word level conditioning variables to regions of the stage i image producing a word context matrix this is then fed to the next stage of the model along with the raw previous stage output each consecutive stage works in the same manner but produces gradually higher resolution images conditioned on the previous stage two major contributions were introduced in attngan the attentional generative network and the deep attentional multimodal similarity model damsm zhang et al the attentional ative network matches specic regions of each stage s output image to conditioning variables from the word level text embedding this is a very worthy contribution allowing each consecutive stage to focus on specic regions of the image independently adding attentional details region by region as opposed to the whole image the damsm is also a key feature introduced by which is used after the result of the nal stage to calculate the similarity between the generated image and the text embedding at both the sentence level and the more ne grained word level table shows scores from different metrics for stackgan attngan and hdgan on the cub oxford and coco datasets the table shows that outperforms the other models in terms of is on the cub dataset by a small amount and greatly outperforms them on the coco dataset hdgan hierarchically nested adversarial network hdgan is a method proposed by zhang et al and its main objective is to tackle the difcult problem of dealing with photographic images from semantic text descriptions these semantic text descriptions are applied on images from diverse datasets this method introduces adversarial objectives nested inside hierarchically oriented networks zhang et al hierarchical networks helps regularize mid level manifestations in addition to regularize level manifestations it assists the training of the generator in order to capture highly complex still media elements these elements are captured in statistical order to train the generator based on settings tracted directly from the image the latter is an ideal scenario however this paper aims to incorporate a single stream architecture this single stream architecture functions as the generator that will form an optimum adaptability towards the jointed discriminators once jointed discriminators are setup in an optimum manner the single stream architecture will then advance generated images to achieve a much higher resolution zhang et al the main contributions of the hdgans include the introduction of a visual semantic similarity sure zhang et al this feature will aid in the evaluation of the consistency of generated images in addition to checking the consistency of generated images one of the key objectives of this step is to test the logical consistency of the end product zhang et al the end product in this case would be images that are semantically mapped from text based natural language descriptions to each area on the picture e a wing on a bird or petal on a ower deep learning has created a multitude of opportunities and challenges for researchers in the computer vision ai eld coupled with gan and multimodal ing architectures this eld has seen tremendous growth reed et al liu et al yang et al li et al wang and gupta zhang et al mirza and osindero based on these advancements hdgans attempt to further extend some desirable and less common features when generating images from textual natural language zhang et al in other words it takes sentences and treats them as a hierarchical structure this has some positive and negative implications in most cases for starters it makes it more complex to generate compelling images however one of the key benets of this elaborate process is the realism obtained once all processes are completed in addition one common feature added to this process is the ability to identify parts of sentences with bounding boxes if a sentence includes common characteristics of a bird it will surround the attributes of such bird with bounding boxes in practice this should happen if the desired image have other ements such as human faces e eyes hair owers e petal size color or any other inanimate object e a table a mug finally hdgans evaluated some of its claims on common ideal text to image datasets such as cub coco and reed et al zhang et al liu et al yang et al li et al wang and gupta zhang et al mirza and osindero these datasets were rst utilized on earlier works reed et al and most of them sport modied features such image annotations labels or descriptions the qualitative and quantitative results reported by researchers in this study were far superior of earlier works in this same eld of computer vision ai diversity enhancement gans in this subsection we introduce text to image synthesis methods which try to maximize the diversity of the output images based on the text descriptions ac gan two issues arise in the traditional gans mirza and osindero for image synthesis bilirty problem traditional gans can not predict a large number of image categories and diversity problem images are often subject to one to many mapping so one image could be labeled as different tags or being described using different texts to address these problems gan conditioned on additional information e cgan is an alternative solution however although cgan and many previously troduced approaches are able to generate images with respect to the text descriptions they often output images with similar types and visual appearance slightly different from the cgan auxiliary classier gans ac gan odena et al poses to improve the diversity of output images by using an auxiliary classier to control output images the overall structure of ac gan is shown in fig in ac gan every generated image is ated with a class label in addition to the true fake label which are commonly used in gan or cgan the discriminator of ac gan not only outputs a probability distribution over sources i e whether the image is true or fake it also output a probability distribution over the class label i e predict which class the image belong to by using an auxiliary classier layer to predict the class of the image ac gan is able to use the predicted class labels of the images to ensure that the output consists of images from different classes resulting in diversied synthesis images the results show that ac gan can generate images with high diversity tac gan building on the ac gan tac gan dash et al is proposed to replace the class information with textual descriptions as the input to perform the task of text to image synthesis the architecture of tac gan is shown in fig which is similar to ac gan overall the major difference between tac gan and ac gan is that tac gan conditions the generated images on text descriptions instead of on a class label this design makes tac gan more generic for image synthesis for tac gan it imposes restrictions on generated images in both texts and class labels the input vector of tac gan s generative network is built based on a noise vector and embedded vector tation of textual descriptions the discriminator of tac gan is similar to that of the ac gan which not only predicts whether the image is fake or not but also predicts the label of the images a minor difference of tac gan s discriminator compared to that of the ac gan is that it also receives text information as input before performing its classication the experiments and validations on the owers dataset show that the results produced by tac gan are slightly better that other approaches including gan int cls and stackgan text segan in order to improve the diversity of the output images both ac gan and tac gan s discriminators predict class labels of the synthesised images this process likely enforces the semantic diversity of the images but class labels are inherently restrictive in describing image semantics and images described by text can be matched to multiple labels therefore instead of predicting images class labels an alternative solution is to directly quantify their semantic relevance the architecture of text segan is shown in fig in order to directly quantify semantic evance text segan cha et al adds a regression layer to estimate the semantic relevance between the image and text instead of a classier layer of predicting labels the estimated semantic reference is a fractional value ranging between and with a higher value reecting better semantic relevance between the image and text due to this unique design an inherent advantage of text segan is that the generated images are not limited to certain classes and are semantically matching to the text input experiments and validations on ower dataset show that text segan can generate diverse images that are semantically relevant to the input text in addition the results of text segan show improved inception score compared to other approaches including gan int cls stackgan tac gan and hdgan mirrorgan and scene graph gan due to the inherent complexity of the visual images and the diversity of text descriptions i e same words could imply different meanings it is difculty to precisely match the texts to the visual images at the semantic levels for most methods we have discussed so far they employ a direct text to image generation process but there is no validation about how generated images comply with the text in a reverse fashion to ensure the semantic consistency and diversity mirrorgan qiao et al employs a ror structure which reversely learns from generated images to output texts an image to text process to further validate whether generated are indeed consistent to the input texts mirrowgan includes three modules a semantic text embedding module stem a global local collaborative attentive ule for cascaded image generation glam and a semantic text regeneration and alignment module stream the back to back text to image and image to text t are combined to sively enhance the diversity and semantic consistency of the generated images in order to enhance the diversity of the output image scene graph gan johnson et al poses to use visual scene graphs to describe the layout of the objects allowing users to precisely specic the relationships between objects in the images in order to convert the visual scene graph as input for gan to generate images this method uses graph convolution to process input graphs it computes a scene layout by predicting bounding boxes and segmentation masks for objects after that it converts the computed layout to an image with a cascaded renement network motion enhancement gans instead of focusing on generating static images another line of text to image synthesis research focuses on generating videos i e sequences of images from texts in this context the synthesised videos are often useful resources for automated assistance or story telling obamanet and one early interesting work of motion enhancement gans is to generate spoofed speech and lip sync videos or talking face of barack obama i e obamanet based on text input kumar et al this framework is consisted of three parts i e text to speech using mouth shape representation synced to the audio using a time delayed lstm and video generation conditioned on the mouth shape using u net architecture although the results seem promising obamanet only models the mouth region and the videos are not generated from noise which can be regarded as video prediction other than video generation another meaningful trial of using synthesised videos for automated assistance is to translate spoken language e text into sign language video sequences i e stoll et al this is often achieved through a two step process converting texts as meaningful units to generate images followed by a learning component to arrange images into sequential order for best representation more ically using rnn based machine translation methods texts are translated into sign language gloss quences then glosses are mapped to skeletal pose sequences using a lookup table to generate videos a conditional dcgan with the input of concatenation of latent representation of the image for a base pose and skeletal pose information is built in li et al a text to video model is proposed based on the cgan in which the input is the isometric gaussian noise with the text gist vector served as the generator a key component of generating videos from text is to train a conditional generative model to extract both static and dynamic information from text followed by a hybrid framework combining a variational autoencoder vae and a generative adversarial network gan more specically relies on two types of features static features and dynamic features to erate videos static features called gist are used to sketch text conditioned background color and object layout structure dynamic features on the other hand are considered by transforming input text into an image lter which eventually forms the video generator which consists of three entangled neural networks the text gist vector is generated by a gist generator which maintains static information e background and a which captures the dynamic information i e actions in the text to generate videos as demonstrated in the paper li et al the generated videos are semantically related to the texts but have a rather low quality e only resolution storygan different from which generates videos from a single text storygan aims to produce dynamic scenes consistent of specied texts i e story written in a multi sentence paragraph using a sequential gan model li et al story encoder context encoder and discriminators are the main nents of this model by using stochastic sampling the story encoder intends to learn an low dimensional embedding vector for the whole story to keep the continuity of the story the context encoder is posed to capture contextual information during sequential image generation based on a deep rnn two discriminators of storygan are image discriminator which evaluates the generated images and story discriminator which ensures the global consistency the experiments and comparisons on clevr dataset and pororo cartoon dataset which are inally used for visual question answering show that storygan improves the generated video qualify in terms of structural similarity index ssim visual qualify consistence and relevance the last three measure are based on human evaluation table a summary of different gans and datasets used for validation a x symbol indicates that the model was evaluated using the corresponding dataset method names cgan mirza and osindero ac gan odena et al tac gan dash et al text segan cha et al gan int cls reed et al stackgan zhang et al zhang et al xu et al dc gan reed et al hdgan zhang et al mirrorgan qiao et al evaluation datasets mnist coco cub x x x x x x x x x x x x x x x x x x x x x x x x gan based text to image synthesis applications mark and evaluation and comparisons text to image synthesis applications computer vision applications have strong potential for industries including but not limited to the cal government military entertainment and online social media elds wu et al nie et al hong et al mansimov et al asmuth et al fang et al text to image sis is one such application in computer vision ai that has become the main focus in recent years due to its potential for providing benecial properties and opportunities for a wide range of applicable areas text to image synthesis is an application byproduct of deep convolutional decoder networks in bination with gans wu et al reed et al xu et al deep convolutional networks have contributed to several breakthroughs in image video speech and audio processing this learning method intends among other possibilities to help translate sequential text descriptions to images mented by one or many additional methods algorithms and methods developed in the computer vision eld have allowed researchers in recent years to create realistic images from plain sentences advances in the computer vision deep convolutional nets and semantic units have shined light and redirected cus to this research area of text to image synthesis having as its prime directive to aid in the generation of compelling images with as much delity to text descriptions as possible to date models for generating synthetic images from textual natural language in research ratories at universities and private companies have yielded compelling images of owers and birds reed et al though owers and birds are the most common objects studied thus far research has been applied to other classes as well for example there have been studies focused solely on human faces wu et al reed et al wang et al yin et al it s a fascinating time for computer vision ai and deep learning researchers and enthusiasts the consistent advancement in hardware software and contemporaneous development of computer vision ai research disrupts multiple industries these advances in technology allow for the extraction of several data types from a variety of sources for example image data captured from a variety of photo ready devices such as smart phones and online social media services opened the door to the analysis of large amounts of media datasets fang et al the availability of large media datasets allow new frameworks and algorithms to be proposed and tested on real world data text to image synthesis benchmark datasets a summary of some reviewed methods and benchmark datasets used for validation is reported in table in addition the performance of different gans with respect to the benchmark datasets and performance metrics is reported in table in order to synthesize images from text descriptions many frameworks have taken a minimalistic proach by creating small and background less images mao et al in most cases the experiments were conducted on simple datasets initially containing images of birds and owers reed et al contributed to these data sets by adding corresponding natural language text descriptions to subsets of the cub mscoco and datasets which facilitated the work on text to image synthesis for several papers released more recently while most deep learning algorithms use mnist lecun and cortes dataset as the mark there are three main datasets that are commonly used for evaluation of proposed gan models for text to image synthesis cub wang et al oxford nilsback and zisserman coco lin et al and krizhevsky cub wang et al contains birds with matching text descriptions and oxford nilsback and zisserman contains categories of ers with images each and matching text descriptions these datasets contain individual jects with the text description corresponding to that object making them relatively simple coco lin et al is much more complex containing images with different object types krizhevsky dataset consists of colour images in classes with ages per class in contrast to cub and oxford whose images each contain an individual object coco s images may contain multiple objects each with a label so there are many labels per image the total number of labels over the images is million lin et al text to image synthesis benchmark evaluation metrics several evaluation metrics are used for judging the images produced by text to image gans proposed by salimans et al inception scores is calculates the entropy randomness of the conditional distribution obtained by applying the inception model introduced in szegedy et al and marginal distribution of a large set of generated images which should be low and high respectively for ful images low entropy of conditional distribution means that the evaluator is condent that the images came from the data distribution and high entropy of the marginal distribution means that the set of generated images is diverse which are both desired features the is score is then computed as the divergence between the two entropies fcn scores isola et al are computed in a similar manner relying on the intuition that realistic images generated by a gan should be able to be classied correctly by a classier trained on real images of the same distribution therefore if the fcn classier classies a set of synthetic images accurately the image is probably realistic and the corresponding gan gets a high fcn score frechet inception distance fid heusel et al is the other commonly used uation metric and takes a different approach actually comparing the generated images to real images in the distribution a high fid means there is little relationship between statistics of the synthetic and real images and vice versa so lower fids are better the performance of different gans with respect to the benchmark datasets and performance metrics is reported in table in addition figure further lists the performance of gans with respect to their inception scores is table a summary of performance of different methods with respect to the three benchmark datasets and four performance metrics inception score is frechet inception distance fid human classier hc and ssim scores the generative adversarial networks inlcude dcgan gan int cls gan paired d gan stackgan attngan objgan hdgan dm gan tac gan text segan scene graph gan and mirrorgan the three benchmark datasets include cub oxford and coco datasets a dash indicates that no data was found methods is dcgan gan int cls dong gan paired d gan stackgan obj gan hdgan dm gan tac gan text segan scene graph gan mirrorgan cub datasets metrics coco oxford fid hc ssim is fid hc ssim is fid hc ssim gan based text to image synthesis results comparison while we gathered all the data we could nd on scores for each model on the cub oxford and coco datasets using is fid fcn and human classiers we unfortunately were unable to nd certain data for and hdgan missing in table the best evaluation we can give for those with missing data is our own opinions by looking at examples of generated images provided in their papers in this regard we observed that hdgan produced relatively better visual results on the cub and oxford datasets while attngan produced far more impressive results than the rest on the more complex coco dataset this is evidence that the attentional model and damsm introduced by attngan are very effective in producing high quality images examples of the best results of birds and plates of vegetables generated by each model are presented in figures and respectively in terms of inception score is which is the metric that was applied to majority models except dc gan the results in table show that only showed slight improvement over its decessor stackgan for text to image synthesis however did introduce a very worthy enhancement for unconditional image generation by organizing the generators and discriminators in a tree like structure this indicates that revising the structures of the discriminators generators can bring a moderate level of improvement in text to image synthesis in addition the results in table also show that dm gan zhu et al has the best mance followed by obj gan li et al notice that both dm gan and obj gan are most recently developed methods in the eld both published in indicating that research in text to image synthesis is continuously improving the results for better visual perception and interception nical wise dm gan zhu et al is a model using dynamic memory to rene fuzzy image contents initially generated from the gan networks a memory writing gate is used for dm gan to select portant text information and generate images based on he selected text accordingly on the other hand obj gan li et al focuses on object centered text to image synthesis the proposed framework of obj gan consists of a layout generation including a bounding box generator and a shape generator and an object driven attentive image generator the designs and advancement of dm gan and gan indicate that research in text to image synthesis is advancing to put more emphasis on the image details and text semantics for better understanding and perception notable mentions it is worth noting that although this survey mainly focuses on text to image synthesis there have been other applications of gans in broader image synthesis eld that we found fascinating and worth cub coco oxford s i g a n in t c l s d c g a n d ong g a n paired d g a n stack g a n stack g a n attn g a n o bj g a n h d g a n m a n t a c g a n d scene g raph g a n m irror g a n text se g a n figure performance comparison between gans with respect to their inception scores is figure examples of best images of birds generated by gan int cls stackgan and hdgan images reprinted from zhang et al b xu et al and zhang et al respectively ing a small section to for example yin et al used sem latent gans to generate images of faces based on facial attributes producing impressive results that at a glance could be mistaken for real faces xu et al fang et al and karpathy and fei fei demonstrated great success in erating text descriptions from images image captioning with great accuracy with xu et al using an attention based model that automatically learns to focus on salient objects and karpathy and fei fei using deep visual semantic alignments finally there is a contribution made by that was not mentioned in the dedicated section due to its relation to unconditional image generation as opposed to conditional namely a color regularization term zhang et al this additional term aims to keep the samples generated from the same input at different stages more consistent in color which resulted in signicantly better results for the unconditional model conclusion the recent advancement in text to image synthesis research opens the door to several compelling ods and architectures the main objective of text to image synthesis initially was to create images from figure examples of best images of a plate of vegetables generated by gan int cls stackgan and hdgan images reprinted from zhang et al b xu et al and zhang et al respectively simple labels and this objective later scaled to natural languages in this paper we reviewed novel methods that generate in our opinion the most visually rich and photo realistic images from text based natural language these generated images often rely on generative adversarial networks gans deep convolutional decoder networks and multimodal learning methods in the paper we rst proposed a taxonomy to organize gan based text to image synthesis works into four major groups semantic enhancement gans resolution enhancement gans diversity enhancement gans and motion enhancement gans the taxonomy provides a clear roadmap to show the motivations architectures and difference of different methods and also outlines their evolution timeline and relationships following the proposed taxonomy we reviewed important features of each method and their architectures we indicated the model denition and key contributions from some advanced gan framworks including stackgan attngan dc gan ac gan gan hdgan text segan storygan many of the solutions surveyed in this paper tackled the highly complex challenge of generating photo realistic images beyond swatch size samples in other words beyond the work of reed et al in which images were generated from text in tiny swatches lastly all methods were evaluated on datasets that included birds owers humans and other miscellaneous elements we were also able to allocate some important papers that were as impressive as the papers we nally surveyed though these notable papers have yet to contribute directly or indirectly to the expansion of the vast computer vision ai eld looking into the future an excellent extension from the works surveyed in this paper would be to give more independence to the several learning ods e less human intervention involved in the studies as well as increasing the size of the output images acknowledgements conflict of interest references the authors declare that there is no conict of interest regarding the publication of this article asmuth j dixon d hanna k hsu s c kumar r paragano v pope a samarasekera s and sawhney h multimedia applications of computer vision in proceedings fourth ieee workshop on applications of computer vision cat no princeton nj usa volume doi acv pages baltrusaitis t ahuja c and morency l multimodal machine learning a survey and omy corr arxiv bengio y courville a and vincent p representation learning a review and new tives ieee transactions on pattern analysis and machine intelligence burt p and adelson e the laplacian pyramid as a compact image code ieee transactions on cha m gown y l and kung h t adversarial learning of semantic relevance in text to communications image synthesis aaai cha m gwon y and kung h adversarial nets with perceptual losses for text to image synthesis in ieee international workshop on machine learning for signal processing mlsp pages ieee cha m gwon y and kung h t adversarial nets with perceptual losses for text to image synthesis in ieee international workshop on machine learning for signal processing mlsp tokyo japan pages cha m gwon y l and kung h t adversarial learning of semantic relevance in text to image synthesis in proceedings of the association for the advancement of articial intelligence aaai chen j shen y gao j liu j and liu x language based image editing with recurrent attentive models in proc of the eee cvf conference on computer vision and pattern recognition chen y lai y and liu y cartoongan generative adversarial networks for photo cartoonization in ieee cvf conference on computer vision and pattern recognition cvpr lake salt city usa creswell a white t dumoulin v arulkumaran k sengupta b and bharath a tive adversarial networks an overview ieee signal processing magazine pages dash a gamboa j c b ahmed s afzal m z and liwicki m tac gan text conditioned auxiliary classier generative adversarial network corr arxiv dash a gamboa j c b ahmed s liwicki m and afzal m z tac gan text conditioned auxiliary classier generative adversarial network arxiv preprint denton e chintala s szlam a and fergus r deep generative image models using a laplacian pyramid of adversarial networks corr arxiv denton e l chintala s szlam a and fergus r deep generative image models using a laplacian pyramid of adversarial networks in cortes c lawrence n d lee d d sugiyama m and garnett r editors advances in neural information processing systems pages curran associates inc dong h yu s wu c and guo y semantic image synthesis via adversarial learning in proceedings of the ieee international conference on computer vision pages dong h zhang j mcilwraith d and guo y learning text to image synthesis with textual data augmentation in ieee international conference on image processing icip beijing china pages durugkar i gemp i and mahadevan s generative multi adversarial networks elgammal a liu b elhoseiny m and mazzone m can creative adversarial networks generating art by learning about styles and deviating from style norms corr arxiv in ieee international conference on image processing icip athens greece volume doi icip pages image captioning with word level attention fang f wang h and tang p gao l chen d song j xu x zhang d and shen h t perceptual pyramid ial networks for text to image synthesis in proceedings of the association for the advancement of articial intelligence aaai goodfellow i j pouget abadie j mirza m xu b d warde farley ozair s courville a and bengio y generative adversarial networks in proceedings of nips han h li y and zhu x convolutional neural network learning for generic data classication information sciences haynes m norton a mcparland a and cooper r speech to text for broadcasters from research to implementation smpte motion imaging journal heusel m ramsauer h unterthiner t nessler b and hochreiter s gans trained by a two time scale update rule converge to a local nash equilibrium corr arxiv hong s yang d choi j and lee h inferring semantic layout for hierarchical text to image synthesis corr arxiv hong y hwang u yoo j and yoon s how generative adversarial networks and their variants work an overview acm computing surveys csur huang h yu p and wang c an introduction to image synthesis with generative adversarial nets corr arxiv huang x li y poursaeed o hopcroft j and belongie s stacked generative adversarial networks in ieee conference on computer vision and pattern recognition pages isola p zhu j zhou t and efros a image to image translation with conditional adversarial networks corr arxiv johnson j alahi a and fei fei l perceptual losses for real time style transfer and resolution in european conference on computer vision pages springer johnson j gupta a and fei fei l image generation from scene graphs in proceedings of the cvpr karpathy a and fei fei l deep visual semantic alignments for generating image descriptions ieee transactions on pattern analysis and machine intelligence krizhevsky a master s thesis dept of computer science univ of toronto kumar r sotelo j kumar k brebisson a and bengio y obamanet photo realistic lip sync from text arxiv preprint lecun y and cortes c mnist handwritten digit database li c su y and liu w text to text generative adversarial networks in international joint conference on neural networks ijcnn rio de janeiro pages li c wang z and qi h fast converging conditional generative adversarial networks for in ieee international conference on image processing icip athens greece image synthesis pages li w zhang p zhang l huang q he x lyu s and gao j object driven text image synthesis via adversarial training corr li y gan z shen y liu j cheng y wu y carin l carlson d and gao j gan a sequential conditional gan for story visualization in proceedings of the ieee conference on computer vision and pattern recognition pages li y min m r shen d carlson d and carin l video generation from text in second aaai conference on articial intelligence lin t maire m belongie s bourdev l girshick r hays j perona p ramanan d zitnick c and dollar p microsoft coco common objects in context corr arxiv liu x meng g xiang s and pan c semantic image synthesis via conditional in international conference on pattern recognition icpr generative adversarial networks beijing china pages mansimov e parisotto e ba j l and salakhutdinov r generating images from captions with attention corr arxiv mao x li q xie h lau r y k wang z and smolley s p least squares generative adversarial networks in ieee international conference on computer vision iccv venice pages mirza m and osindero s conditional generative adversarial nets corr arxiv arxiv preprint mirza m and osindero s conditional generative adversarial nets ngiam j khosla a kim m nam j lee h and ng a y multimodal deep learning in proceedings of the international conference on international conference on machine learning omnipress usa pages nguyen t d le t vu h and phung d dual discriminator generative adversarial nets in proc of nips nie d trullo r petitjean c ruan s and shen d medical image synthesis with aware generative adversarial networks corr arxiv nilsback m and zisserman a automated ower classication over a large number of classes in proceedings of the indian conference on computer vision graphics and image processing odena a olah c and shlens j conditional image synthesis with auxiliary classier gans corr arxiv odena a olah c and shlens j conditional image synthesis with auxiliary classier gans in proceedings of the international conference on machine learning volume pages jmlr org park h yoo y and kwak n mc gan multi conditional generative adversarial network for image synthesis arxiv preprint qiao t zhang j xu d and tao d mirrorgan learning text to image generation by redescription corr radford a metz l and chintala s unsupervised representation learning with deep lutional generative adversarial networks corr arxiv reed s akata z mohan s tenka s schiele b and lee h learning what and where to draw in proc of nips international conference reed s akata z yan x logeswaran l schiele b and lee h generative adversarial text to image synthesis proceedings of the international conference on machine learning icml salimans t goodfellow i zaremba w cheung v radford a and chen x improved techniques for training gans corr arxiv schuster m and paliwal k bidirectional recurrent neural networks ieee transactions on signal processing stoll s camgoz n c hadeld s and bowden r sign language production using neural machine translation and generative adversarial networks in bmvc page szegedy c vanhoucke v ioffe s and shlens j rethinking the inception architecture for computer vision in ieee conference on computer vision and pattern recognition pages vo d m and sugimoto a paired d gan for semantic image synthesis in asian conference on computer vision pages springer wang k gou c duan y lin y zheng x and wang f the caltech ucsd dataset computation and neural systems technical report cns wang k gou c duan y lin y zheng x and wang f generative adversarial networks introduction and outlook ieee caa journal of automatica sinica wang k and wan x sentigan generating sentimental texts via mixture adversarial networks in proceedings of the twenty seventh international joint conference on articial intelligence wang x and gupta a generative image modeling using style and structure adversarial works corr arxiv wang y chang l cheng y jin l and cheng z learning face sketch from facial attribute text in ieee international conference on image processing icip athens greece pages wu x xu k and hall p a survey of image synthesis and editing with generative adversarial networks tsinghua science and technology xu k ba j kiros r cho k courville a salakhutdinov r zemel r and bengio y show attend and tell neural image caption generation with visual attention corr arxiv xu t zhang p huang q zhang h gan z huang x and he x attngan fine grained text to image generation with attentional generative adversarial networks corr arxiv yan c yang j sohn k and lee h conditional image generation from visual attributes in in leibe b matas j sebe n welling m eds computer vision eccv eccv lecture notes in computer science springer cham volume yan z zhang h wang b paris s and yu y automatic photo adjustment using deep neural networks acm transactions on graphics tog yang j kannan a batra d and parikh d lr gan layered recursive generative adversarial networks for image generation corr arxiv yang m zhao w xu w feng y zhao z chen x and lei k multitask learning for cross domain image captioning ieee transactions on multimedia yin w fu y sigaly l and xue x semi latent gan learning to generate and modify facial images from attributes corr arxiv zhang d yin j zhu x and zhang c network representation learning a survey ieee transactions on big data doi tbdata zhang g tu e and cui d stable and improved generative adversarial nets gans a in proceedings of the international conference on image processing pages constructive survey zhang h xu t li h zhang s wang x huang x and metaxas d realistic image synthesis with stacked generative adversarial networks ieee transactions on pattern analysis and machine intelligence zhang h xu t li h zhang s wang z huang x and metaxas d stackgan text to photo realistic image synthesis with stacked generative adversarial networks in ieee international conference on computer vision iccv venice pages zhang s zhai j luo d zhan y and chen j recent advance on generative adversarial networks in proceedings of the international conference on machine learning and cybernetics pages zhang z xie y and yang l photographic text to image synthesis with a nested adversarial network corr arxiv zhu m pan p chen w and yang y dm gan dynamic memory generative adversarial networks for text to image synthesis in proceedings of the ieee conference on computer vision and pattern recognition pages zhu x goldberg a eldawy m dyer c and strock b a text to picture synthesis system for augmenting communication in prof of aaai international conference pages
