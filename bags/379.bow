neural abstractive text summarizer telugu language mohan bharath aravindh gowtham akhil tech iiit allahabad allahabad india tech shiv nadar university greater noida india tech university hyderabad hyderabad telangana india com edu com abstract abstractive text summarization process constructing semantically relevant shorter sentences captures essence overall meaning source text actually difficult time consuming humans summarize manually large documents text work abstractive text summarization english significant work reported telugu abstractive text summarization like propose abstractive text summarization approach telugu language deep learning paper proposing abstractive text summarization deep learning model telugu language proposed architecture based encoder decoder sequential models attention mechanism applied model manually created dataset generate sentence summary source text got good results measured qualitatively keywords deep learning lstm telugu neural networks nlp summarization introduction textual data increasing current internet age need process condense text simultaneously preserving meaning source text text summarization creating short accurate semantically relevant summary given text help easy fast retrieval information text summarization classified categories extractive text summarization methods form summaries copying parts source text taking measure importance words source text joining sentences form summary source text abstractive text summarization methods create new semantically relevant phrases form summaries rephrasing words source text abstractive methods actually harder accurate semantically relevant summaries model expected comprehend meaning text try express understanding relevant words phrases abstractive models capabilities like generalization paraphrasing significant work focused extractive text summarization methods especially english source language reported work telugu abstractive text summarization deep learning models available datasets telugu text summarization goal build model given telugu news article output semantically relevant sentence summary title sentence corresponding telugu article proposed deep learning model encoder decoder architecture achieved good results measured qualitatively manually created dataset fact available datasets training dataset created telugu news websites taking headline summary main content source text created dataset telugu news articles corresponding summaries taken headline respective article created dataset way articles belonging different domains politics entertainment sports business national equally distributed maintain balance dataset create word embeddings telugu words use embeddings fasttext created word embeddings nearly languages word embedding dimensions related work work based abstractive text summarization deep learning models telugu language reported work deep learning models mainly inspired papers rush encoder decoder neural attention model perform abstractive text summarization english data found performed beat previous non deep based approaches konstantin lopyrev proposed encoder decoder recurrent neural network attention mechanism generate headlines english attention mechanism inspired bahdanau seminal approach section provide brief overview model architecture individual components recurrent neural network encoder decoder rnns text sequential type data recurrent neural networks type neural networks handling sequential data rnns variable length input sequence output sequence uses internal hidden capture current input previous hidden state simple mathematical representation recurrent network seen whxt hidden state timestep output time step weight matrix connecting input layer hidden states weight matrix connecting hidden states current time step hidden states previous timestep biases hidden states output layer parameters time step activation functions rnns multi layered called deep rnns layered rnns layer extracts information previous layer lstms long short term memory lstm type rnn architecture complicated hidden unit computations introducing gates input forget output memory cells allows memorizing forgetting long distance training model effectively handle vanishing gradient problem lstm basic unit encoder decoder model perform summarization fig lstm unit motivation lstm captures long term dependency pretty information starting sequence able traverse line selective restricting information flow lstm unit gates lstm weight matrix input timestep dot product forget gate layer cell state time step hidden state previous timestep forget gate sigmoid activation input gate layer output gate layer encoder decoder model input gate cell state timestep weight matrix previous state current input encoder decoder model based neural networks aims handling mapping highly structured input output vanilla encoder decoder model encoder rnn reads source text word word encodes information hidden state passes information forward decoder starts final hidden state encoder timestep computes probability distribution total words vocabulary taking softmax function gives probability values words vocabulary probable word selected timestep continues end sentence token selected decoder timesteps reaches threshold words generated far form summary sentence length input sequence encoder network length output sequence decoder network generates encoded representation contains information input sequence focus tth input sequence encoder network output length fig encoder decoder architecture encoder decoder framework encoder tries summarize entire input fixed dimension vector decoder takes input output previous step hidden state vector previous time step timestep decoder word selected previous time step hidden state vector previous time step given input current timestep attention mechanism basic encoder decoder model performs short sentences fails generalize longer sentences paragraphs input decoder timestep fixed size vector fixed size vector able capture relevant information step decoder certain parts input relevant model learn focus step decoder encoder stage timestep longer source text generate appropriate word attention attention model calculates importance input encoding current step decoder kind similarity check decoder output timestep input encodings similarity check taking dot product current hidden state decoder input encodings input encodings normalizing importance vector convert probabilities passing softmax probability distribution form context vector multiplying encodings importanceit battn attention distribution softmax importanceit context vector ait context vector fed layers generate distribution vocabulary sample loss time step losst target summary word negative log probability target summary word loss time steps summed backpropagation required gradients minimize loss apply gradient descent learn parameters network training language python libraries tensorflow keras dataset created manually contains telugu news articles corresponding summaries taken headline respective articles dataset divided parts training testing model hidden units loss function cross entropy training epochs training encoder decoder architecture end end loss propagated encoder evaluation evaluation results generated summaries source text qualitatively sample generated summaries given original summary generated summary table results source text observed generated summaries semantically relevant source text cases test data despite limited training data model good job conclusions implemented abstractive text summarization telugu language encoder decoder architecture attention mechanism specifically datasets telugu language paired human generated summaries got semantically relevant good results test data measured qualitatively generated summaries relevant given fact dataset created scratch limitation dataset size got good results text summarization telugu language difficulties lack quality summaries large given dataset future like create large dataset corpus purpose comparable standard english text summarization datasets available work numeric metric specifically telugu text summarization tasks capture semantic relevance summary given source text future shall explore possibility transformer architecture telugu text summarization task taken permission competent authorities use images data given paper case dispute future shall wholly responsible references edouard grave piotr bojanowski prakhar gupta armand joulin tomas mikolov learning word vectors languages alexander rush sumit chopra jason weston neural attention model abstractive sentence summarization konstantin lopyrev generating news headlines recurrent neural networks dzmitry bahdanau kyughyun cho yoshua bengio neural machine translation jointly learning align translate
