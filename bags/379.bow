neural abstractive text summarizer for telugu language mohan bharath aravindh gowtham akhil m tech iiit allahabad allahabad u p india b tech shiv nadar university greater noida u p india m tech university of hyderabad hyderabad telangana india com edu in com abstract abstractive text summarization is the process of constructing semantically relevant shorter sentences which captures the essence of the overall meaning of the source text it is actually difficult and very time consuming for humans to summarize manually large documents of text much of work in abstractive text summarization is being done in english and almost no significant work has been reported in telugu abstractive text summarization so we would like to propose an abstractive text summarization approach for telugu language using deep learning in this paper we are proposing an abstractive text summarization deep learning model for telugu language the proposed architecture is based on encoder decoder sequential models with attention mechanism we have applied this model on manually created dataset to generate a one sentence summary of the source text and have got good results measured qualitatively keywords deep learning lstm telugu neural networks nlp summarization introduction textual data is ever increasing in the current internet age we need some process to condense the text and simultaneously preserving the meaning of the source text text summarization is creating a short accurate and semantically relevant summary of a given text it would help in easy and fast retrieval of information text summarization can be classified into two categories extractive text summarization methods form summaries by copying from the parts of the source text by taking some measure of importance on the words of the source text and then joining those sentences together to form a summary of the source text abstractive text summarization methods create new semantically relevant phrases it can also form summaries by rephrasing or by using the words that were not in the source text abstractive methods are actually harder for an accurate and semantically relevant summaries the model is expected to comprehend the meaning of the text and then try to express that understanding using the relevant words and phrases so abstractive models can have capabilities like generalization paraphrasing significant work is being focused on extractive text summarization methods and especially with english as the source language there is no reported work for telugu abstractive text summarization using deep learning models and also there are no available datasets for telugu text summarization our goal is to build a model such that when given the telugu news article it should output semantically relevant sentence as the summary title sentence for the corresponding telugu article we have proposed a deep learning model using encoder decoder architecture and we have achieved good results measured qualitatively we have manually created the dataset because of the fact there are no available datasets training dataset has been created from the telugu news websites by taking the headline as the summary and the main content as the source text and we have created a dataset with telugu news articles with their corresponding summaries which are taken as the headline of the respective article we have created the dataset in such a way that the articles belonging to the different domains i politics entertainment sports business national are more or less equally distributed to maintain a balance to the dataset to create word embeddings for the telugu words we have made use of embeddings by fasttext which has created word embeddings for nearly languages with each word embedding of dimensions related work as our work is based on abstractive text summarization using deep learning models on telugu language on which there is no reported work our deep learning models are mainly inspired by these three papers rush et al used an encoder decoder neural attention model to perform abstractive text summarization on english data and found that it performed very well and beat the previous non deep based approaches konstantin lopyrev proposed a encoder decoder recurrent neural network with attention mechanism to generate headlines for english attention mechanism itself is inspired from bahdanau seminal approach in this section we provide a brief overview of the model architecture used and its individual components recurrent neural network encoder decoder rnns text is a sequential type of data recurrent neural networks are a type of neural networks used for handling sequential data rnns can take a variable length input sequence x and can output sequence yt it uses an internal hidden to capture both the current input and the previous hidden state a simple mathematical representation of recurrent network can be seen below h whxt bh hidden state at timestep t by output at time step t wh is the weight matrix connecting input layer to hidden states and uh is the weight matrix connecting the hidden states of current time step to the hidden states of previous timestep bh and by are biases of hidden states and of the output layer w u and b are parameters and they are the same at each time step h and y are activation functions rnns can also be multi layered they are called deep rnns these are layered rnns where each layer extracts information from the previous layer lstms long short term memory lstm is a type of rnn architecture with complicated hidden unit computations so by introducing gates which are input forget output and memory cells it allows memorizing and forgetting over a long distance of training and the model can effectively handle the vanishing gradient problem lstm is a basic unit of our encoder decoder model to perform summarization fig lstm unit the motivation behind using an lstm is that it captures long term dependency pretty well and the information in the starting of the sequence is able to traverse down the line this is done by being selective and restricting the information flow in the lstm unit there are three gates in an lstm wf weight matrix xt input at timestep t dot product forget gate layer ft bf ct ct ft ct cell state at time step t hidden state of previous timestep forget gate sigmoid activation input gate layer it bi output gate layer ot ot encoder decoder model ct bc ct ct it it input gate ct cell state at timestep t wi weight matrix between previous state and current input encoder decoder model is based on neural networks which aims at handling the mapping between the highly structured input and output in the vanilla encoder decoder model the encoder rnn first reads the source text word by word and then encodes the information in a hidden state and passes the information forward decoder starts from the final hidden state of the encoder and at every timestep it computes a probability distribution on the total words in the vocabulary by taking a softmax function which gives probability values to all the words in the vocabulary and the most probable word is selected for that timestep and this continues until the end of sentence token is selected by the decoder or until the no of timesteps reaches the threshold all the words generated so far will form the summary sentence be a length t input sequence to the encoder network y be a length u output sequence the decoder network generates each encoded representation ht contains information about the input sequence with focus on the tth input of the sequence be the encoder network output of length t fig encoder decoder architecture in the encoder decoder framework the encoder tries to summarize the entire input in a fixed dimension vector ht decoder takes as input the output of the previous step and the hidden state vector from the previous time step so at every timestep of the decoder the word selected in the previous time step and the hidden state vector of the previous time step is given as input in the current timestep attention mechanism the basic encoder decoder model performs well on very short sentences but it fails to generalize well for longer sentences paragraphs the only input to the decoder at the first timestep is a fixed size vector from this fixed size vector may not be able to capture all the relevant information at each step of the decoder only certain parts of the input are relevant to how do we make the model learn what to focus on at each step of the decoder the encoder stage last timestep of the longer source text generate an appropriate word attention attention model calculates the importance of each input encoding for the current step of the decoder by doing a kind of similarity check between decoder output at this timestep and all the input encodings this similarity check is done by taking dot product between the current hidden state of the decoder and all the input encodings doing this for all of the input encodings and normalizing we get an importance vector we then convert it to probabilities by passing through softmax which would give probability distribution then we form a context vector by multiplying with the encodings importanceit v battn attention distribution at softmax importanceit context vector ht ei ait context vector is then fed into two layers to generate distribution over the vocabulary from which we sample for the loss at time step t losst where wt is the target summary word i e negative log probability of the target summary word loss at all the time steps is summed up now we can do backpropagation and get all the required gradients and to minimize the loss we can apply gradient descent and learn the parameters of the network training language python libraries tensorflow keras dataset which is created manually contains telugu news articles and their corresponding summaries which are taken from the headline of the respective articles dataset is divided into two parts for training and for testing model has hidden units loss function is cross entropy training is done for epochs training of encoder decoder architecture is done end to end i loss is propagated to the encoder as well evaluation evaluation of the results i e generated summaries of the source text have been done qualitatively here a sample of the generated summaries has been given below original summary generated summary table results source text we have observed that the generated summaries are more or less semantically relevant to the source text in many cases in the test data despite the limited training data our model did a good job conclusions we have implemented abstractive text summarization for telugu language using encoder decoder architecture with attention mechanism specifically there are no datasets for telugu language that have paired human generated summaries we have got semantically relevant good results on most of the test data measured qualitatively although there are very few generated summaries which were not at all relevant given the fact the dataset has been created from the scratch and the limitation of the dataset size we have got good results for text summarization in telugu language one of the difficulties is the lack of quality summaries of large given dataset in future we would like to create a very large dataset corpus for this purpose comparable to the standard english text summarization datasets available we would also work towards a numeric metric specifically to telugu text summarization tasks which would also capture the semantic relevance of the summary to the given source text in future we shall explore the possibility of using the transformer architecture for telugu text summarization task we have taken permission from competent authorities to use the images data as given in the paper in case of any dispute in the future we shall be wholly responsible references edouard grave piotr bojanowski prakhar gupta armand joulin tomas mikolov learning word vectors for languages cs cl alexander m rush sumit chopra jason weston a neural attention model for abstractive sentence summarization cs cl konstantin lopyrev generating news headlines with recurrent neural networks cl dzmitry bahdanau kyughyun cho yoshua bengio neural machine translation by jointly learning to align and translate cs cl
