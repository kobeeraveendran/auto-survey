deep communicating agents abstractive summarization asli antoine xiaodong yejin research allen school computer science engineering university washington research institute articial intelligence com xiaodong com antoineb washington edu abstract present deep communicating agents encoder decoder architecture dress challenges representing long document abstractive tion deep communicating agents task encoding long text divided multiple collaborating agents charge subsection input text encoders connected gle decoder trained end end forcement learning generate focused coherent summary empirical results demonstrate multiple communicating encoders lead higher quality summary compared strong baselines cluding based single encoder multiple non communicating encoders introduction focus task abstractive tion long document contrast extractive summarization summary composed subset sentences words lifted text abstractive summarization requires generative ability rephrase restructure sentences compose coherent concise mary recurrent neural networks rnns capable generating uent language variants encoder decoder rnns sutskever bahdanau shown promising sults abstractive summarization task rush nallapati fundamental challenge strong performance neural models encoding short text generalize long text motivation approach able dynamically attend different parts input work author microsoft research figure illustration deep communicating agents presented paper agent encodes paragraph multiple layers passing new sages multiple layers agents able ordinate focus important aspects input text capture salient facts recent work marization addresses issues improved attention models chopra pointer networks coverage mechanisms coherence focused training objectives paulus jaques tive mechanism representing long document remains challenge simultaneous work investigated use deep communicating agents sukhbaatar collaborative tasks logic puzzles foerster visual dialog das reference games lazaridou work builds approaches propose rst study communicating agents encode long text summarization key idea model divide hard task encoding long text multiple orating encoder agents charge ent subsection text figure agents encodes assigned text independently broadcasts encoding allowing agents share global context information different sections ment agents adapt encoding aroma howtouseit massageadabofaromatherapeuticbalmoroil messages aagent figure multi agent encoder decoder overview agent encodes paragraph local encoder followed multiple contextual layers agent communication concentrated messages layer condensed agent context communication illustrated figure word context vectors agent specic generation probabilities enable voting suitable vocabulary words yen nal distribution assigned text light global context peat process multiple layers ing new messages layer agent completes encoding deliver information decoder novel contextual agent tion figure contextual agent attention enables decoder integrate information multiple agents smoothly decoding step work trained end end self critical forcement learning rennie ate focused coherent summaries empirical results cnn dailymail new york times datasets demonstrate ple communicating encoders lead higher quality summaries compared strong baselines ing based single encoder multiple non communicating encoders human evaluations indicate model able produce cused summaries agents gather salient mation multiple areas document communicate information reducing common mistakes missing key facts repeating content including unnecessary details analysis reveals model attains better performance coder interacts multiple agents anced way conrming benet representing long document multiple encoding agents model extend commnet model sukhbaatar sequence generation notation document sequence paragraphs split multiple coding agents encodes rst paragraph second paragraph paragraph quence words construct ulary training documents frequently appearing words word embedded dimensional vector variables linear projection matrices multi agent encoder agent encodes word sequences following stacked encoders local encoder rst layer local encoder agent tokens sponding paragraph fed single layer directional lstm blstm producing cal encoder hidden states hidden state dimensionality local encoderb lstmsource tiredofcountingb lstmb lstmsource youdonthaveb lstmwordattentionb lstmsource melatoninsupplement lstmword context vector agentattention word context vector word context vector agent context vector fragrances start thatmakeyoufeeldecoderlstm vocabulary distributionazooazoocalmyen final distributionagent attention agent attention contextual encoderlayers lstmb lstmb lstm output local encoder layer fed contextual encoder contextual encoder framework enables agent communication cycles multiple coding layers output contextual coder adapted representation agent encoded information conditioned tion received agents layer agent jointly encodes mation received previous layer ure cell contextual layer blstm takes inputs hidden states adjacent lstm cells hidden state message vector ous layer agents outputs indicates index token sequence message received agent layer average outputs agents layer hidden state output kth contextual layer agent average messages ceived encoder agents parametric function feed forward model tion messages message projected agent previous encoding document learned parameters shared agent equation combines information sent agents context current token paragraph yields different features current context relation topics source document layer agent modies tion context relative information received agents updates mation sends agents accordingly figure multi agent encoder message passing agents transmit hidden state output current layer message passed average pool receiving agent uses new message additional input layer decoder agent attention output contextual encoder layer agent sequence hidden state vectors token sent decoder calculate word attention tions use single layer lstm decoder feed hidden state rst agent initial state time step decoder predicts new word summary computes new state attending relevant input context provided agents decoder uses new hierarchical attention mechanism agents word attention distribution bahdanau puted token agent attention tokens paragraph learned rameters decoding step new decoder context calculated agent weighted sum encoder hidden states agent word context vector sents information extracted agent paragraph read decoder decide information relevant lstmlstm current decoding step weighting context vector agent attention yielding document global agent attention distribution figure includes words document sidered vocabulary oov probability distribution extended vocabulary puted agent learned soft selection agents compute agent context vector agt act xed length vector agent context encoding salient information entire ument provided agents nated decoder state fed multi layer perception produce vocabulary distribution vocabulary words time topics generated sentences intact reasonable decoder utilize agents course short sequences sentence decoder signed select agent attend time step introduce contextual agent attention caa prevent frequently switching tween agents previous step agent attention additional information coding step generate distribution words multi agent pointer network similar allow copying candidate words different paragraphs document computing generation probability value agent timestep context vector decoder state decoder input learned scalar truth predicted output depending ing testing time generation probability termines generate word cabulary sampling ing word corresponding agent input paragraph sampling attention bution produces extended vocabulary sum attention instances appears source ment nal distribution extended cabulary sample obtained weighting agent corresponding agent attention values contrast single agent baseline model allows agent vote ferent oov words time equation case word relevant generated summary time collaboratively voted result agent attention probability mixed objective learning train deep communicating agents use mixed training objective jointly optimizes multiple losses describe mle baseline multi agent model uses imum likelihood training sequence tion given truth output sequence human summary word quences given input document mize negative log likelihood target word sequence lmle semantic cohesion encourage sentences summary informative repetition include semantic cohesion loss integrate sentence level semantics learning tive decoder generates output word quence keeps track end sentence delimiter token indices den state vectors end sentence compute cosine similarity secutively generated sentences minimize similarity end sentence hidden states dene semantic cohesion loss lsem nal training objective lmle sem lmle lsem tunable hyperparameter reinforcement learning loss policy gradient methods directly optimize discrete target evaluation metrics rouge non differentiable paulus jaques pasunuru bansal time step word generated model viewed action taken agent sequence generated compared ground truth sequence compute reward model learns self critical training approach rennie learns ploring new sequences comparing best greedily decoded sequence training example output sequences generated sampled probability tion time step baseline output greedily generated argmax decoding training objective minimize lrl loss ensures better exploration model learns generate sequences receive higher rewards compared baseline creasing overall reward expectation model mixed loss training mle loss learn better language model guarantee better results global performance measures similarly optimizing loss increase reward gathered expense diminished readability uency erated summary paulus nation objectives yield improved task specic scores maintaining uency lmixed lrl tunable hyperparameter ance objective functions pre train models mle loss switch mixed loss add semantic cohesion loss term lmixed sem sem analyze impact training intermediate rewards introduce based rewards opposed end summary wards differential rouge metrics generating diverse sentences warding sentences based scores obtained end generated summary compute cremental rouge scores generated sentence sentences rewarded increase rouge contribute sequence suring current sentence contributed novel information overall summary experimental setup datasets conducted experiments marization datasets cnn dailymail nallapati hermann new york times nyt sandhaus replicate preprocessing steps paulus tain data splits anonymize named entities dca models initialize number agents training partition document agents agent paragraphs additional tails found appendix training details training testing truncate article tokens limit length summary tokens ing tokens test time distribute truncated articles agents multi agent models preserving paragraph sentences possible datasets limit output vocabulary size frequent tokens training set train contextual layers dca models layers provide additional mance gains term equation sem term mle mixed training additional details provided appendix evaluation evaluate system unigram recall bigram recall rouge longest common quence select mle models lowest negative log likelihood models highest rouge scores sample validation data evaluate test use pyrouge pypi python org pypi model summarunner nallapati graph based attention tan pointer generator pointer generator coverage controlled summarization xed values fan intra attention paulus intra mle pgen comm agent pgen comm agent pgen comm agent dca pgen comm agents dca mpgen comm agents dca mpgen comm caa agents dca mpgen comm caa agents rouge table comparison results cnn daily mail test set variants rouge best model models bolded model intra attention paulus intra attention paulus intra mle pgen comm agent pgen comm agent pgen comm agent dca pgen comm agents dca mpgen comm agents dca mpgen comm caa agents dca mpgen comm caa agents rouge table comparison results new york times test set variants rouge best model models bolded set test time use beam search width models generate nal predictions baselines compare dca models previously published models summarunner nallapati graph based attentional neural model tan rnn based tractive summarizer combines abstractive tures training pointer networks coverage based training summarization intra decoder tention paulus controllable stractive summarization fan allows users dene attributes generated maries uses copy mechanism source entities decoder attention reduce repetition ablations investigate new component model different ablation producing seven different models rst ablations single agent model local coder context encoder pointer network tectures dca encoders trained mle loss model trained additional semantic cohesion sem loss model trained mixed loss end summary rewards rest models use agents crementally add component add semantic cohesion loss add multi agent pointer networks mpgen agent communication finally add tual agent attention caa train mixed loss dca els use pointer networks results quantitative analysis results cnn dailymail nyt datasets table respectively overall models agent encoders pointer generation nication strongest models weaker rouge model paulus man evaluations work showed model received lower readability relevance scores model trained mle indicating additional boost rouge lated summary quality result account best models tive models use mixed loss timize sentence level structure similarity reference summary higher rouge reward learn parameters improve semantic coherence promoting higher abstraction table appendix generated mary examples model agent agent agent rouge table comparison multi agent models varying number agents rouge results model table cnn daily maily dataset single multi agents multi agent models improvements single agent lines cnn dailymail dataset compared mle published baselines improve rouge scores found agent models generally outperformed agent models table cause truncate documents training larger number agents cient multi document summarization independent communicating agents trained multiple agents communication performance dca models ilar single agent baselines communication biggest jump rouge seen cnn dailymail data indicating encoders better identify key facts input avoiding unnecessary details contextual agent attention caa compared model contextualized agent tion model yields better rouge scores stability provided caa helps decoder avoid frequent switches agents dilute topical signal captured encoder repetition penalty neurally generated maries redundant introduced mantic cohesion penalty incremental rewards generate semantically diverse maries baseline model optimized sem loss improves rouge scores baseline similarly model trained reinforcement learning uses sentence based intermediate rewards improves rouge scores datasets human evaluations perform human evaluations establish model rouge improvements correlated human judgments measure municative multi agent network contextual agent attention comparison single agent network communication use lowing evaluation criteria generated maries non redundancy fewer ideas repeated coherence ideas pressed clearly focus main ideas document shared avoiding superuous details overall summary effectively communicates article content focus non redundancy dimensions help quantify pact multi agent communication model coherence helps evaluate impact reward based learning repetition penalty proposed models evaluation procedure randomly selected samples cnn dailymail test set use workers amazon mechanical turk judges evaluate criteria dened judges shown original document ground truth summary model maries asked evaluate summary criteria likert scale worst best ground truth model summaries presented judges random order summary rated judges results averaged examples judges performed head head evaluation common duc style evaluations domly model generated summaries ask human annotators rate summary metrics seeing source document ground truth summaries results human evaluators signicantly prefer summaries generated communicating coders rating task evaluators preferred multi agent summaries single agent cases metrics head head evaluation mans consistently preferred dca summaries generated single agent head head rating evaluation largest improvement dca model cus question indicating model learns generate summaries pertinent details capturing salient information later portions document human turnbull interviewed childhood political stance admitted planned run prime minister tony abbott successfully toppled february leadership spill words primed minister controversially printed cover single malcolm turnbull set feature cover australia bold doubt set tors tongues wagging posing suave blue suit pinstriped shirt contrasting red tie turnbull condent demeanour complimented bold confronting words printed page primed minister malcolm turnbull set run prime minister tony abbott successfully toppled ary leadership spill set feature cover liberal party newsletter multi human daphne selfe modelling fties recently landed new campaign vans single stories year old commands day work daphne selfe shows collaboration footwearsuper brandand theetherealhigh street store uncompromisinggrace daphne said collection appears year old dron stories collection featured story truly relaxed timeless modern twist shoes worn pieces brands collection multi daphne selfe starred campaign vans stories model appears year old dron hair collection commanding day work table comparison human summary best multi agent model summaries cnn dailymail dataset single agent model generates coherent summary focused contains unnecessary details highlighed red misses keys facts multi agent model successfully captures bolded criteria non redundancy coherence focus overall score based head head table head head score based comparison human evaluations random subset cnn dataset single multi agent indicates tistical signicance focus overall communication improves focus investigate multi agent models discover salient concepts comparison gle agent models analyze rouge scores based average attention received agent compute average attention received agent decoding time step generated summary cnn daily mail test corpus bin document summary pairs attention received agent average rouge scores summaries bin figure outlines interesting results summaries generated distributed tention agents yield higher rouge scores indicating attending multiple areas document allows discovery salient concepts later sections text second use bins generate summaries documents bin agent model average rouge scores single agent summaries lower figure average rouge scores summaries binned agent average attention generating summary section agents contribute equally summary score increases responding multi agent summaries indicating cases agent dominates tention communication agents allows model generate focused summaries qualitatively effect table compare human generated maries best single agent model best multi agent model model generates good summaries capture facts human summary able include facts extra details erating relevant diverse summaries related work recent works investigate attention nisms encoder decoder models sharpen agent single agent single received agentmultisinglemulti agent single context decoder focus input encoding luong vinyals bahdanau example ong proposes global local tion networks machine translation ers investigate hierarchical attention networks document classication yang ment classication chen dialog response selection zhou attention mechanisms shown crucial summarization rush zeng nallapati pointer particular networks vinyals help address redundancy saliency generated summaries cheng lapata paulus fan share motivation works work uniquely presents approach based commnet deep communicating agent work sukhbaatar compared prior multi agent works logic puzzles foerster language learning lazaridou mordatch abbeel starcraft games vinyals present rst study framework long text generation finally model related prior works address repetitions generating long text introduce post trained coverage work penalize repeated attentions regions input paulus use intra decoder attention punish generating words contrast propose new mantic coherence loss intermediate based rewards reinforcement learning courage semantically similar generations conclusions investigated problem encoding long text generate abstractive summaries strated use deep communicating agents improve summarization automatic manual evaluation analysis demonstrates improvement improved ability covering salient concepts taining semantic coherence summaries acknowledgements research supported nsf darpa cwc program aro references dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly learning align translate iclr huimin chen maosong sun cunchao yankai lin zhiyuan liu neural sentiment tion user product attention emnlp jianpeng cheng mirella lapata neural summarization extracting sentences words acl sumit chopra michael auli alexander rush abstractive sentence summarization tentive recurrent neural networks naacl abhishek das satwik kottur jos moura dhruv batra stefan lee learning ative visual dialog agents deep reinforcement learning cvpr greg durrett taylor berg kirkpatrick dan klein learning based single document tion compression anaphoricity constraints acl angela fan david grangier michael auli controllable abstractive summarization foerster farquhar afouras nardelli whiteson counterfactual multi agent icy gradients jakob foerster yannis assael nando freitas shimon whiteson learning nicate deep multi agent reinforcement learning nips karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend nips kai hong michel marcus ani nenkova system combination multi document rization emnlp kai hong ani nenkova improving estimation word importance news document summarization extended technical port natasha jaques shixiang dzmitry bahdanau jose miguel hernandez lobato richard turner douglas eck sequence tutor tive tuning sequence generation models control icml angeliki lazaridou nghia pham marco roni multi agent based language learning jiwei tan xiaojun wan jianguo xiao abstractive document summarization based attentional neural model acl vinyals ewalds bartunov georgiev vezhnevets yeo makhzani kuttler agapiou schrittwieser craft new challenge reinforcement learning arxiv preprint oriol vinyals meire fortunato navdeep jaitly pointer networks nips oriol vinyals lukasz kaiser terry koo slav petrov ilya sutskever geoffrey hinton mar foreign language nips yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus google neural macherey chine translation system bridging gap arxiv preprint human machine translation zichao yang diyi yang chris dyer xiaodong alex smola eduard hovy hierarchical attention networks document classication naacl wenyuan zeng wenjie luo sanja fidler raquel urtasun efcient summarization arxiv preprint copy mechanism xiangyang zhou daxiang dong hua shiqi zhao yan xuan liu tian tiview response selection human computer versation emnlp junyi jessy kapil thandani amanda stent role discourse units near extractive summarization proceedings annual meeting special interest group discourse dialog minh thang luong hieu pham christopher manning effective approaches based neural machine translation emnlp christopher manning mihai surdeanu john bauer jenny finkel steven bethard david closky stanford corenlp natural proceedings guage processing toolkit annual meeting association tional linguistics system demonstrations mordatch abbeel grounded compositional populations emergence language multi agent ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments association advancement cial intelligence ramakanth pasunuru mohit bansal forced video captioning entailment rewards emnlp romain paulus caiming xiong richard socher deep reinforced model abstractive marization iclr jeffrey pennington richard socher pher manning glove global vectors word representation empirical methods ral language processing steven rennie etienne marcheret youssef mroueh jarret ross vaibhava goel self critical arxiv sequence training image captioning preprint alexander rush sumit chopra jason weston neural attention model abstractive tence summarization emnlp evan sandhaus new york times annotated pus linguistic data consortium philadelphia abigale peter liu christopher manning gettothepoint summarization generatornetworks acl sainbayar sukhbaatar arthur szlam rob fergus learning multiagent communication propagation nips ilya sutskever oriol vinyals quoc sequence sequence learning neural works advances neural information ing systems limit input output vocabulary size frequent tokens training set initialize word embeddings glove vectors pennington tune training train adam learning rate mle models models tune gamma hyper parameter mixed loss erating dca models value yielded best gains train models iterations took days agents days agents encoder parameters tune avoid repetition prevent decoder generating trigram test following paulus dition predicted vocabulary ken unk replace likely origin choosing source word largest cascaded attention arg max generated summary examples appendix provides example documents test set comparisons human generated golden summaries summaries produced models baseline single agent model trained loss model table best multi agent optimized mixed model loss model table red highlights indicate details appear summary models generated red indicates factual errors summary green highlights indicate key facts human gold summary models manage capture supplementary material stats avg tokens document avg tokens summary total train doc summ pair total validation doc summ pair total test doc summ pair input token length output token length agent input token length agent agent input token length agent agent input token length agent cnn nyt table summary statistics cnn dailymail new york times nyt datasets datasets cnn dailymail cnn dailymail dataset lapati hermann lection online news articles sentence summaries use data splits nallapati lier work anonymized entities replacing named entity unique identier minican opted non anonymized version new york times nyt dataset mainly train extractive rization systems hong nenkova hong durrett recently abstractive marization task paulus nyt dataset sandhaus collection articles lished use scripts provided extract process nyt dataset modications order replicate pre processing steps sented paulus similar paulus sorted documents publication date chronological order rst training idation testing use pointer supervision replacing named entities abstract type person tion organization misc stanford named entity recognizer manning contrast anonymize nyt dataset reduce pre processing training details train models nvidia gpu machine set hidden state size coders decoders datasets document model abbey clancy helping target breast cancer striking sultry pose new charity campaign winner strictly come dancing joins singer foxes victoria secret angel lily donaldson model alice dellal new series pictures photographer simon emmett fashion targets breast cancer clancy looks chic shows famous legs wearing plain white shirt abbey clancy leads glamour joins forces famous friends target breast cancer striking sultry pose new charity campaign model mother year old daughter sophia footballer husband peter crouch said mum makes proud campaign funds vital work ensuring generation young women afraid diagnosis breast cancer wearing support want campaign holding heaven singer foxes looks foxy cropped stripy jeans abbey says proud campaign funds vital work ensuring generation young women afraid diagnosis breast cancer victoria secret angel lily donaldson industry years adds glamour charity campaign holding heaven singer foxes dons stripy jeans campaign says honoured said honoured taking year fashion targets breast cancer campaign awesome heritage fashion huge life taking inspire women wear support join ght breast cancer head proud year campaign far raised breakthrough breast cancer research funding year range clothes accessories produced conjunction high street partners river island warehouse topshop laura ashley debenhams superga baukjen cambridge satchel company viewed online www fashiontargetsbreastcancer org lookbook campaign stars alice dellal far raised breakthrough breast cancer research funding human gold single agent baseline multi agent models abbey lily joined alice dellal singer foxes women pictured wearing support abbey says proud campaign funds vital work campaign raised breakthrough breast cancer research strictly come dancing joins singer foxes victoria secret angel lily donaldson model alice dellal new series pictures photographer simon emmett fashion targets breast cancer clancy looks chic shows famous legs wearing plain white shirt abbey says proud campaign funds vital work ensuring generation young women afraid diagnosis breast cancer campaign raised breakthrough breast cancer research table example multi agent models demonstrate extractive behaviors select sentences different sections document single model extracts second sentences multi agent model successfully selects salient sentences sentences document specically sentence attributed fact agents successfully encode salient aspects distributed distant sections document interesting result agent model shows extractive behaviour example successfully selects salient sentences single agent model includes superuous details document michelle pfeiffer latest hollywood star preparing hit small screen oscar nominated star known roles iconic lms scarface dangerous liaisons andthe age innocence teamed katie couric pitch new television comedy morning news program involved project pfeiffer attached star diane english creator murphy brown scroll video michelle pfeiffer left set star new television comedy morning news program produced katie couric right series created diane english candice bergen murphy brown female news anchor according variety pfeiffer role morning news anchor making similar real life role couric played host today years couric serve executive producer help ensure series strikes realistic notes creator project english previously brains brown starring candice bergen centered female news anchor ran seasons winning emmys english serve writer producer showrunner program ladies currently talks hbo showtime amc netix amazon pick program couric serve executive producer drawing experience anchor today years pfeiffer biggest stars television joining group includes house cards stars robin wright kevin spacey true detective leads matthew mcconaughey woody harrelson lady gaga recently announced appearing season american horror story actress kept low prole past years mother handful lms time recently appeared alongside robert niro mob comedy family michelle pfeiffer set star new television comedy morning news program katie couric serve executive producer drawing experience anchor today years series created diane english murphy brown female news anchor ladies currently talks hbo showtime amc netix amazon pick program oscar nominated star known roles iconic lmssuch scarface dangerous liaisons age innocence teamed katie couric pitch new vision comedy morning news program involved project pfeiffer attached star diane english creator murphy brown human gold single agent baseline multi agent michelle pfeiffer set star new comedy morning news program couric series created diane english creator murphy brown pfeiffer biggest stars serve executive producer showrunner project table baseline model generates non coherent summary references main character michelle fer ambiguous way end generated summary contrast multi agent model cessfully captures main character including key facts interesting feature multi agent model showcases simplication property accounts strength abstraction specically simplied bold long sentence document starting couric generated salient words document everton manager roberto martinez forced defend penalty asco club ross barkley missed spot win burnley goodison park untried barkley inexplicably took minute kick awarded foul david jones aaron lennon leighton baines scored penalties attempts premier league dispute team mates time brought memories everton match west brom january kevin mirallas grabbed ball baines penalty missed ross barkley steps minute penalty despite presence leighton baines pitch barkley effort saved byburnley goalkeeper tom heaton goodison park martinez insisted barkley rights request penalty taking duties saturday romelu lukaku pitch taken happy players penalties let depend feel moment argued everton manager baines left scored penalties attempts premier league ross showed incredible responsibility love seeing players control big moments leighton happy given responsibility barkley penalty struck corner burnley goalkeeper tom heaton dived right save fortunately young england player prove costly mirallas went score goal game minutes everton boss roberto martinez issues instructions players break play burnley everton defeated burnley goodison park saturday kevin mirallas scored goal game minute ross barkley earlier missed minute penalty leighton baines scored penalties attempts season everton manager roberto martinez forced defend penalty asco club human gold ross barkley missed spot win burnley goodison park untried barkley inexplicably took minute kick awarded foul david jones aaron lennon leighton baines scored penalties attempts premier league everton beat burnley goodison park premier league ross barkley steps minute penalty missed barkley scored penalties attempts pitch single agent baseline multi agent table single agent model generates summary superuous details facts clearly expressed able capture statistics player correctly penalties attempts missed player scored goal game kevin mirallas hand multi agent model able generate concise summary key facts similar single agent model missed capture player scored goal game interestingly document contains word defeated multi agent model chose use beat instead exist original document
