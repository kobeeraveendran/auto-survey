abstractive text summarization based language model conditioning locality modeling dmitrii aksenov julian moreno schneider peter bourgonje robert schwarzenberg leonhard hennig georg rehm dfki gmbh alt moabit berlin germany rstname abstract explore extent knowledge pre trained language model benecial task abstractive summarization end experiment conditioning encoder decoder transformer based neural model bert language model addition propose new method bert windowing allows chunk wise processing texts longer bert window size explore locality modeling explicit restriction calculations local context affect summarization ability transformer introducing dimensional convolutional self attention rst layers encoder results models compared baseline state art models cnn daily mail dataset additionally train model swisstext dataset demonstrate usability german models outperform baseline rouge scores datasets superiority manual qualitative analysis keywords summarisation language modeling information extraction information retrieval bert locality modeling introduction text summarization nlp task real world applications increasing unstructured information text form calls methods cally extract relevant information documents present condensed form eld marization different paradigms recognised mensions extractive abstractive single document multi document extractive summarization sentences words extracted text carry important information directly presenting sult summary abstractive summarization methods paraphrase text changing text aim generate exible consistent summaries thermore single document summarization works gle documents multi document summarization deals multiple documents produces single summary paper concentrate single document abstractive summarization recent abstractive models utilize neural network based sequence sequence proach training models calculate tional probability summary given input sequence maximizing loss function typically cross entropy approaches based encoder decoder work encoder encodes input sequence vector representation decoder produces new mary given draft summary summary generated previous iterations layer decoder generator maps hidden states ken probabilities use state art transformer sequence sequence tasks built primarily attention mechanism vaswani attempt improve performance abstractive text marization improving language encoding ties model recent results shown main contribution transformer multi layer tecture allowing self attention replaced technique signicant drop performance domhan following egy develop model introduces convolution vanilla self attention allowing better encode cal dependencies tokens overcome data sparsity problem use pre trained language model encoding encoder decoder setup ates contextualized representation input sequence specically use bert directional context conditioning multilingualism state art scores tasks devlin furthermore propose new method allows applying bert longer texts main contributions paper designing new abstractive text summarization models based ideas conditioning pre trained guage model application convolutional self attention layers encoder proposing method encoding input sequence windows ates bert input allows processing longer input texts evaluating performance models english german language conducting ablation study cnn dail mail swisstext datasets comparing state art methods related work pre trained language models traditionally non contextualized embedding vectors pre training neural based nlp models mikolov pennington recently trained language models exploiting contextualized dings elmo bert xlnet raised bar nlp tasks peters radford devlin yang recent tempts use models text summarization process sequences maximum tokens strated suitability achieving new state art sults zhang liu liu lapata neural abstractive text summarization neural approach abstractive summarization largely adopted state art models shi signicant contribution pointer generator work uses special layer decoder network able generate tokens dictionary extract input text uses coverage vector mechanism pay attention kens covered previous iterations example earlier work adapting reinforcement learning pure model described paulus achieved high rouge scores duced unreadable summaries combination cal cross entropy optimization achieved high scores inating unreliability problem liu best knowledge rst use model summarization decoder extraction model tion compression techniques increase size sequence zhang incorporate bert transformer based model use stage dure exploiting mask learning strategy attempt improve abstractive summarization models corporating extractive model example use key information guide network guide summary generation process summarization gehrmann extractive model crease precision pointer generator mechanism strand research adapts existing models cope long text cohan present aware attention model introduces hierarchy attention mechanism calculating additional attention vector sections input text subramanian showed language model trained combination original text extractive summaries erated model golden summary achieve results comparable standard encoder decoder based marization models approach text summarization model based transformer architecture architecture adopts original model vaswani decoder use pointer generator formula increase extractive pabilities network later refer architecture copytransformer probability copying specic word source document psof probability generation word calculated tive summarization model pgen probability copying instead generation convolutional self attention transformer like self attention network hierarchical multi layer architecture figure model overview ments shown architecture tends learn cal information rst layers sentence level patterns middle semantics upper layers raganato tiedemann tenney tage approach attention operation considers tokens equally important tic information concentrated certain local areas problem usually specied problem locality modeling syntactic information help identifying important words phrases benecial focus attention regions successful approach locality modeling task called convolutions local self attention networks yang essentially problem dealt application dimensional convolution self attention operation network lower layers strengthens dependencies neighboring elements makes model distance aware searches level patterns sequence words restricts attention scope window neighboring elements convolution applied attention illustrated mulas region centered position query attention convolution extended dimensional area taking interactions features learned ferent attention heads transformer account original transformer head independently models distinct set linguistic properties dependencies tokens raganato tiedemann applying dimensional convolution second dimension index attention head explicitly allow head interact learned features adjacent sub spaces shortcoming original implementation rst heads interact assumed adjacent assume considering heads sub spaces periodically increase model effectiveness applying circular convolution second dimension section evaluate original sion modication window region heads stands union keys values different subspaces convolutional self attention shown effective machine translation nlp tasks knowledge applied text summarization problem experiments ported paper created implementation local attention convolutional self attention work transformer supports modes having size kernels system parameters yang incorporate convolutional attention transformer encoder positioning place self attention lower layers tion low level modeling capabilities encoder provides strong boost model tion accuracy text summarization task bert conditioned encoder main task encoder remember mantic syntactic information input text decoder generate output knowledge transfer language model oretically improve ability remember important formation larger corpus training phase compared corpus text marization training phase condition encoder bert language model encoder conditioning forward strategy recommended bert based model placing pre trained language model encoder embeddings layer embeddings system context dependent decided tune encoder bert sake memory time economy instead follow general recommendations concatenating hidden states layers bert dimensional embedding vector vlin use variations bert based encoder rst model uses bert encode sequence second model feeds bert generated embeddings vanilla transformer encoder bert windowing key features approach ability overcome length limitations bert allowing deal longer documents bert maximum supported sequence length smaller average size texts summarization datasets method relies known method ing knowledge tokens traditional sense called wordpiece tokens devlin bert based models abstractive text rization research figure apply bert dows texts strides generate matrices matrix embedding window combine reverse operation vectors ping positions averaged summing dividing number overlapping vectors result matrix embeddings shape hidden size times length text drawback approach reduce size context resulted vector calculated based maximum twice window size number tokens split text equal size windows aggravate consistency input sentences split arbitrary manner adjacent windows despite drawback assume procedure improve accuracy encoder trained non truncated texts set window size maximum size tokens stride consider stride size optimal trade average context size tational requirements model number windows trade ensure token context initial nal tokens tokens context figure integration bert generated contextual sentations windows bert conditioned decoder decoder pre training applied similar way main difference instead nal output bert use word embedding matrix sitions reason decoder generated probability distribution conditioned complete text previous summary draft output bert implicitly assumes consistent completed input zhang context independent embeddings represent minimum set features meaningful prediction custom transformer decoder stacked bert bert based model similar stage bert zhang bertsumabs liu pata differs usage hidden states bert create contextualized representation presence pointer generator capabilities process figure different approaches integration bert conditioning convolutional self attention method rouge copytransformer conv conv circular conv table ablation study model convolutional attention cnn daily mail dataset kernel sizes long texts figure schema sic model bert conditioned convolutional attention encoder bert conditioned decoder integration bert convolutional self attention evaluated different ways integrating conditioning convolutional self attention model encoder figure stacking approach comprises feeding generated embeddings convolutional self attention transformer encoder potential problem proach convolutional self attention assumed benecial applied lower layers locality modeling feature help modeling local dencies syntax time bert chical model layers target high level patterns sequences semantics assume plication network detecting low level patterns bert output undermine generalization abilities concatenation considerations raised develop second approach concatenation split convolutional self attention transformer encoder networks rst uses convolutional self attention second inal self attention identical transformer encoder feed original sequences bert convolutional self attention network parallel sulting embedding vectors concatenated fed transformer encoder way model locality lower layers encoder cost smaller depth network assuming number layers datasets aim develop system works independent way assumes upstream components available respective language language independent multi lingual version bert summarization datasets english use english evaluation additionally include german check model applied language cnn daily mail experiments mainly conducted cnn daily mail dataset hermann nallapati contains collection news articles paired sentence summaries published cnn daily mail websites dataset standard training summarization models use non anonymized data training recent state art models raw dataset consists separate text les representing single article summary use data preprocessed version provided gehrmann training pairs validation pairs test pairs align data vocabulary bert enized bpe based wordpiece tokenizer vlin samples bert training data prepended special token follow figure effect window size model rouge transformer copytransformer bert encoder transformer decoder bert transformer encoder transformer decoder bert transformer encoder bert transformer decoder transformer text bert transformer encoder transformer decoder text transformer copytransformer bert transformer encoder transformer decoder bert transformer encoder bert transformer decoder transformer text bert transformer encoder transformer decoder text table ablation study bert based model truncated original cnn daily mail dataset model rouge table ablation study bert based model truncated original swisstext dataset add source text dataset resulting dataset average lengths article summary tokens respectively experiments use clipped version training validation datasets article truncated tokens experiments bert windowing use text version swisstext dataset evaluate efciency model multi lingual multi domain environment conduct series ments german swisstext dataset dataset created german text summarization challenge swiss text analytics conference swisstext zhaw designed explore ent ideas solutions abstractive tion german texts best knowledge rst long document summarization dataset man language publicly available data tracted german wikipedia represents biographical articles denitions concepts dataset tokenized multilingual wordpiece tokenizer devlin preprocessed way cnn daily mail dataset split training validation testing sets containing samples respectively average length source sequence tokens makes dataset suitable experiments windowing experiments system built opennmt library training use cross entropy loss adam optimizer noam decay method kingma tion dropout label smoothing uation calculate scores rouge library rouge evaluation sequences wordpiece tokens locality modeling evaluate effect convolution self attention introduce rst layer encoder use kernel sizes yang iments accelerate training process use small model hidden size self attention heads layers encoder decoder models trained training steps coverage penalty baseline use implementation contrast use attention layer decoder train new generator layer scratch results presented table volutions tokens attention heads improve rouge scores standard convolution outperformed lar convolution rouge percent respectively investigated effect window size dimensional convolution rouge scores figure contrast ndings machine translation found size returns best result summarization task bert conditioning hyperparameters set equal optimal architecture bert based stractive summarizer conducted ablation study table ones experiments convolutional self attention different cnn daily main dataset test models bert decoder transformer decoder transformer transformer decoder version bert experiments bert base baseline use transformer pointer erator results observe bert improves efciency model encoder decoder bert encoder tive produce embeddings standard transformer encoder solely encoder pointer generator model outperformed copytransformer baseline rouge evaluate bert windowing method conducted experiments text approach outperforms baseline proves method fully applied texts longer tokens nal formance model lower model trained truncated text pattern observed baselines assumed relates specics dataset prone having important information rst sentence text swisstext data use multilingual version bert base evaluated models bert transformer encoder transformer transformer decoders table introduction bert transformer increased rouge scores percent spectively time usage bert decoder decreased overall score assume reason multilingual bert language independence embedding matrix outputs precise contextualized representations undermines benets summarization task non truncated texts usage bert transformer encoder increased rouge scores percent furthermore gives higher scores pared model truncated texts strates usability bert windowing lar dataset assume difference performance cnn daily mail datasets reects difference distribution useful information text ticularly swisstext dataset spread uniformly cnn daily mail dataset ducted small experiment comparing average rouge score golden summary head tail document taking rst sentences correlates length gold summary datasets difference taking head tail swisstext dataset rouge respectively smaller cnn daily mail rouge respectively rms hypothesis integration strategies evaluate integration strategies trained els respective bert based baselines models encoder transformer layers volutional transformer layer placed bert parallel respectively table method stacking provide signicant improvement introduction convolutional attention increased percent dropped rouge remained considering domains imally correlates human assessment section dismiss method concatenation strategy volution shown efcient increasing rouge scores percent rms hypothesis locality modeling cient applied non contextualized word representations unfortunately model failed outperform stacking baseline conclude concatenating architecture undermines performance transformer model convolutional self attention benecial pre trained guage models decided train nal models separately model comparison nal comparison model state art methods conducted experiments cnn daily mail dataset set hidden state number transformer layers encoder layers number self attention heads baseline smaller original copytransformer gehrmann reason performs slightly worse table bert conditioning encoder decoder sizes convolution kernels set networks trained training steps single nvidia geforce gtx generation summary beam search algorithm beam size set finally generated summaries detokenized quences words separated spaces bert based model set minimum length generated summary found restriction model prone generate shorter sequences test dataset model outperformed baseline rouge better scores stage bert worse stage sumabs models convolutional copytransformer use tional self attention rst layers encoder increased rouge furthermore present rst publicly available mark swissdata dataset table comparability model include results method integration model rouge stacking concatenation copytransformer copytransformer table different strategies integrating language models convolutional self attention cnn daily mail dataset method rouge bilstm pointer generator coverage intra attention paulus copytransformer gehrmann summarization gehrmann stage bert zhang stage bert zhang intra attention paulus key information guide network sentence rewriting chen bansal bertsumabs liu lapata copytransformer implementation convolutional copytransformer enc dec table rouge scores models cnn daily mail test set rst section shows different state art models second section presents models baseline method rouge copytransformer implementation convolutional copytransformer enc table rouge scores models swisstext test set eters equal cnn daily mail baseline conditioning encoder networks trained truncated texts training steps results convolutional showed efciency cnn daily mail dataset outperforming baseline percent rouge bert based model achieved highest scores qualitative analysis rouge evaluation valid method quality assessment perceive need additional manual evaluation best solution conduct grained study models outputs manually ing terms semantic coherence grammaticality time consuming nature evaluation reverted qualitative analysis comparing summaries generated different models figure includes reference summary generated different models comparing rst sentence vanilla transformer model performed worse copying original sentence omitting characters word meteorological model convolution copied sentence spelling ror finally bert based model succeeded erate right token meteorological bert based model summary conveys meaning gold summary convolutional generates transformer sentences present gold summary overall given bigger model time found smaller model copy mechanism achieved higher scores rouge needs explored future work example models provided summary extractive ture bert based model shows level abstractiveness merging parts sentences single second summary sentence far gold summary sentence way paraphrases original text given particular example models demonstrate explicit ments abstractive summarization remains ing paraphrasing capabilities state art systems low models guaranteed duce summaries follow initial order quence events discussion summarization evaluation rouge lin widely adopted metric evaluating automatic text summarization approaches evaluation comparison set system generated candidate summaries gold dard summary availability corresponding ware performance contributed popularity han goharian despite adoption studies metric faced key criticisms main criticism rouge account meaning expressed sequences ric developed based assumption high ity generated candidate summary share words single human gold standard summary sumption relevant extractive stractive summarization different terminology paraphrasing express meaning han goharian results metric ing low scores summary matching gold dard surface level allows cheating metric generating ungrammatical nonsensical gold summary researchers developing computer write weather forecasts takes meteorological data writes report designed mimic human process known natural language generation lrb nlg rrb prototype system tested bbc website later year transformer researchers london edinburgh developed computer collateological information puter generated weather updates tested scientists heriot watt university university college london project successful prototype system tested generating local weather reports bbc website currently bbc website features reports written meteorologists convolutional transformer researchers london edinburgh developed computer collate meterological information produce forecasts written human uses process known natural language generation lrb nlg rrb computer generated weather updates tested scientists heriot watt university university college london project successful prototype system tested generating local weather reports bbc website bert transformer researchers london edinburgh developed computer collate meteorological information produce forecasts written human met ofce data uses process known natural language generation lrb nlg rrb project successful prototype system tested generating local weather reports bbc website figure comparison output models example form cnn daily mail testset surface realisation mistakes highlighted green typical abstractive feature illustrating arranging sentence highlighted blue maries having high rouge scores sjobergh achieved choosing quent bigrams input document rouge adoption relies correlation human rst research duc sessment datasets containing news articles rouge showed high correlation human judgments lin dorr recent research tions suitability rouge settings roy dang duc data tic responsiveness scores systems respond high rouge scores cohan ian demonstrate summarization scientic texts rouge low correlations gold summaries rouge correlates better far ideal case follows result murray showing unigram match tween candidate summary gold summary accurate metric assess quality problem credibility rouge demonstrated systems operated scoring range peyrard different rization evaluation metrics correlate differently human judgements higher scoring range state art systems operate furthermore improvements measured metric necessarily lead provements concern led development new evaluation rics peyrard dene metrics important cepts regard summariazion redundancy vance informativeness line shannon entropy denitions formulate metric tance better correlates human judgments clark propose metric sentence mover larity operates semantic level better correlates human evaluation summarization model trained reinforcement learning metric ward achieved higher scores human based evaluation despite drawbacks broad adoption rouge makes way compare efciency model state art models evaluation system swissdata dataset conrms ciency terms rouge restricted cnn daily mail data conclusion present new abstractive text summarization model incorporates convolutional self attention bert compare performance system baseline competing systems cnn daily mail data set english report improvement state art results rouge scores establish suitability model languages english domains cnn daily mail data set apply model german swisstext data set present scores setup key contribution model ability deal texts longer bert window size limited wordpiece tokens present cascading approach evaluate texts longer window size demonstrate performance dealing longer input texts source code system publicly available functional service based model currently tegrated summarization service platforms lynx moreno schneider qurator rehm european language grid rehm acknowledgements work presented paper received funding european union horizon research tion programme grant agreement lynx german federal ministry education research bmbf project qurator tumskern com axenov bert summ opennmt bibliographical references chen bansal fast abstractive marization reinforce selected sentence rewriting proceedings annual meeting tion computational linguistics volume long pers pages melbourne australia july ciation computational linguistics clark celikyilmaz smith sentence mover similarity automatic evaluation multi sentence texts proceedings annual meeting association computational tics pages florence italy july association computational linguistics cohan goharian revisiting rization evaluation scientic articles available line arxiv cohan dernoncourt kim bui kim chang goharian discourse aware attention model abstractive summarization long documents naacl hlt conroy dang mind gap gers divorcing evaluations summary content linguistic quality proceedings tional conference computational linguistics coling pages manchester august coling organizing committee devlin chang lee toutanova bert pre training deep bidirectional formers language understanding proceedings conference north american chapter association computational linguistics human language technologies volume long short pers pages minneapolis minnesota june association computational linguistics domhan attention need granular analysis neural machine translation tectures proceedings annual meeting association computational linguistics volume long papers pages melbourne tralia july association computational linguistics dorr monz president schwartz jic methodology extrinsic evaluation text summarization rouge correlate ceedings acl workshop intrinsic sic evaluation measures machine translation summarization pages ann arbor michigan june association computational linguistics gehrmann deng rush abstractive summarization proceedings conference empirical methods natural language processing pages hermann kocisky grefenstette espeholt kay suleyman blunsom teaching machines read comprehend ceedings international conference neural information processing systems volume pages cambridge usa mit press kingma adam method stochastic optimization learning representations international conference gao guiding eration abstractive text summarization based key information guide network proceedings conference north american chapter sociation computational linguistics human guage technologies volume short papers pages new orleans louisiana june association putational linguistics lin rouge package automatic uation summaries text summarization branches pages barcelona spain july association computational linguistics liu lapata text summarization pretrained encoders proceedings ence empirical methods natural language cessing international joint conference natural language processing emnlp ijcnlp liu saleh pot goodrich sepassi kaiser shazeer generating wikipedia summarizing long sequences tional conference learning representations liu fine tune bert extractive tion available online arxiv mikolov sutskever chen corrado dean distributed representations words phrases compositionality burges editors advances neural information processing systems pages curran ciates inc moreno schneider rehm montiel ponsoda rodriguez doncel revenko karampatakis khvalchik sageder gracia maganza orchestrating nlp services legal main nicoletta calzolari editors proceedings language resources evaluation ference lrec marseille france european language resources association elra accepted publication submitted version available preprint murray renals carletta tive summarization meeting recordings speech eurospeech european conference speech communication technology lisbon tugal september pages nallapati zhou dos santos xiang abstractive text summarization sequence sequence rnns ings signll conference computational natural language learning pages berlin germany august association computational guistics paulus xiong socher deep forced model abstractive summarization tional conference learning representations pennington socher manning glove global vectors word representation empirical methods natural language processing emnlp pages pers shi keneshloo ramakrishnan reddy neural abstractive text summarization sequence sequence models available online arxiv sjobergh older versions rougeeval marization evaluation system easier fool mation processing management text summarization subramanian pilault pal extractive abstractive neural document tion transformer language models available online arxiv tenney das pavlick bert ers classical nlp pipeline proceedings annual meeting association computational linguistics vaswani shazeer parmar uszkoreit jones gomez kaiser polosukhin attention need proceedings ternational conference neural information ing systems pages usa curran associates inc fan baevski dauphin auli pay attention lightweight dynamic convolutions international conference learning representations yang wang wong chao convolutional self attention networks ceedings conference north yang dai yang carbonell salakhutdinov xlnet generalized toregressive pretraining language understanding wallach editors advances neural tion processing systems pages curran associates inc zhang cai wang pretraining based natural language generation text summarization proceedings conference computational natural language learning conll pages hong kong china november ation computational linguistics zhaw german text summarization challenge swiss text analytics conference available online peters neumann iyyer gardner clark lee zettlemoyer deep alized word representations proceedings conference north american chapter ation computational linguistics human language technologies volume long papers pages new orleans louisiana june association computational linguistics peyrard simple theoretical model tance summarization proceedings nual meeting association computational guistics pages florence italy july ation computational linguistics peyrard studying summarization evaluation metrics appropriate scoring range proceedings annual meeting association putational linguistics pages florence italy july association computational linguistics radford child luan amodei sutskever language models unsupervised multitask learners available online raganato tiedemann analysis encoder representations transformer based machine translation proceedings emnlp shop blackboxnlp analyzing interpreting neural networks nlp pages brussels belgium november association computational linguistics rehm berger elsholz hegele kintzel marheinecke piperidis deligiannis nis gkirtzou labropoulou bontcheva jones roberts hajic hamrlova kacena choukri arranz vasiljevs anvari lagzdin melnika backfried dikici janosik prinz prinz stampler aniola perez silva berro mann renals klejch european language grid overview nicoletta calzolari editors proceedings language sources evaluation conference lrec seille france european language resources ation elra accepted publication rehm bourgonje hegele kintzel schneider ostendorff zaczynska berger grill rauchle rauenbusch rutenburg schmidt wild hoffmann fink schulz seva quantz bottger matthey fricke thomsen paschke qundus hoppe karam weichhardt fillies neudecker gerber labusch rezanezhad schaefer zellhofer siewert bunk pintscher aleynikova heine qurator innovative gies content data curation adrian paschke editors proceedings qurator conference intelligent content solutions berin ceur workshop proceedings volume january liu manning point summarization pointer generator networks proceedings annual meeting tion computational linguistics volume long
