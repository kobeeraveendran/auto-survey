extractive summarization text matching ming zhong pengfei liu yiran chen danqing wang xipeng qiu xuanjing huang shanghai key laboratory intelligent information processing fudan university school computer science fudan university zhangheng road shanghai china edu abstract paper creates paradigm shift regard way build neural extractive rization systems instead following monly framework extracting sentences individually modeling relationship tween sentences formulate extractive summarization task semantic text ing problem source document candidate summaries extracted original text matched semantic space notably paradigm shift tic matching framework grounded comprehensive analysis inherent gap sentence level summary level tractors based property dataset instantiating framework simple form matching model driven state art extractive sult cnn dailymail new level experiments datasets effectiveness matching framework believe power matching based summarization work fully exploited age instantiations future released codes processed dataset generated summaries com maszhongming matchsum introduction task automatic text summarization aims compress textual document shorter highlight keeping salient information original text paper focus extractive rization usually generates semantically grammatically correct sentences dong nallapati computes faster currently neural extractive rization systems score extract sentences smaller semantic unit authors contributed equally corresponding author figure matchsum framework match textual representations document gold mary candidate summaries extracted ument intuitively better candidate summaries semantically closer document gold summary closest original text model relationship sentences select tences form summary cheng lapata nallapati formulate tractive summarization task sequence ing problem solve encoder decoder framework models independent nary decisions sentence resulting high redundancy natural way address problem introduce auto regressive decoder chen bansal jadhav rajan zhou allowing scoring operations different sentences inuence trigram blocking paulus liu pata popular method recently motivation stage selecting tences form summary skip sentence trigram overlapping previously lected sentences surprisingly simple method removing duplication brings remarkable formance improvement cnn dailymail systems modeling relationship sentences essentially sentence level extractors considering semantics documentcandidate summarygold summaryextractsemantic spacebertbertbert entire summary makes inclined select highly generalized sentences ignoring coupling multiple sentences narayan bae utilize reinforcement learning achieve level scoring limited architecture sentence level summarizers better understand advantages tations sentence level summary level proaches conduct analysis benchmark datasets section explore characteristics methods inherent gap approaches datasets motivates propose following summary level method paper propose novel summary level framework matchsum figure alize extractive summarization semantic text matching problem principle idea good summary semantically similar source document unqualied summaries semantic text matching important research problem estimate semantic similarity source target text fragment applied elds tion retrieval mitra question ing yih severyn moschitti natural language inference wang jiang wang ventional approaches semantic text matching learn vector representation text ment apply typical similarity metrics compute matching scores specic extractive summarization pose siamese bert architecture compute similarity source document candidate summary siamese bert leverages pre trained bert devlin siamese network structure bromley hoffer ailon reimers gurevych rive semantically meaningful text embeddings compared cosine similarity good summary highest similarity set candidate summaries evaluate proposed matching framework perform signicance testing range benchmark datasets model outperforms strong baselines signicantly cases improve state art extractive result cnn dailymail design experiments observe gains brought framework summarize contributions follows instead scoring extracting sentences form summary formulate tractive summarization semantic text ing problem propose novel summary level framework approach bypasses difculty summary level optimization contrastive ing good summary mantically similar source document unqualied summaries conduct analysis investigate extractive models summary level tion based property dataset attempt quantify inherent gap sentence level summary level methods proposed framework achieved rior performance compared strong baselines benchmark datasets notably obtain state art extractive result cnn dailymail base version bert seek observe performance gain model comes related work extractive summarization recent research work extractive summarization spans large range approaches work ally instantiate encoder decoder framework choosing rnn zhou transformer zhong wang gnn wang encoder non auto regressive narayan arumae liu auto regressive decoders jadhav rajan liu lapata despite effectiveness models essentially sentence level tors individual scoring process favor est scoring sentence probably optimal form application provides means summary level scoring brings improvement narayan bae efforts limited auto regressive non auto regressive architectures non neural approaches integer linear ming ilp method level scoring wan addition work solve tive summarization semantic perspective fore paper concept coverage gillick quantify phenomenon section favre reconstruction miao som maximize semantic volume gatama stage summarization recent studies alyguliyev galanis droutsopoulos zhang attempted build stage document tion systems specic extractive summarization rst stage usually extract fragments original text second stage select modify basis fragments chen bansal bae follow hybrid extract rewrite architecture policy based bridge networks lebanoff durrett mendes focus compress learning paradigm sive summarization rst train tor content selection model viewed extract match framework employs sentence extractor prune unnecessary information sentence level summary level dataset dependent analysis previous work pointed ness sentence level extractors tematic analysis following questions extractive summarization level extractor better sentence level tor given dataset extractor choose based characteristics data inherent gap extractors section investigate gap sentence level summary level methods benchmark datasets instruct search effective learning framework worth ing sentence level extractor use include redundancy removal process estimate effect level extractor redundancy elimination notably analysis method estimate theoretical fectiveness presented section generalized applicable summary level proach denition refer single document consisting sentences candidate summary cluding sentences extracted ment given document gold summary measure candidate summary culating rouge lin hovy value levels sentence level score sentence represents number sentences denotes average rouge indicates age overlaps sentence gold summary summary level score considers sentences calculates rouge score gold summary pearl summary dene pearl summary summary lower sentence level score higher summary level score denition candidate summary dened pearl summary exists didate summary satises inequality clearly candidate summary pearl summary challenging sentence level summarizers extract best summary best summary refers summary highest summary level score candidate summaries denition summary dened summary satises argmax denotes candidate summaries document ranking best summary document sort candidate descending order based level score dene rank index best summary use mean rouge use approximate method ext table highest scoring sentences form candidate maries datasets source reddit xsum cnn wikihow pubmed multi news social media news news knowledge base scientic paper news type sds sds sds sds sds mds train pairs valid tokens ext test doc sum table datasets overview sds represents single document summarization mds represents multi document summarization data doc sum indicates average length document summary test set respectively ext denotes number sentences extract different datasets appearance pearl summary bring challenges sentence level extractors attempt investigate proportion summary different datasets benchmark datasets detailed description datasets displayed table demonstrated figure observe datasets best summaries highest scoring sentences cally cnn best summaries pearl summary indicating sentence level extractors easily fall local optimization missing better candidate summaries different cnn pubmed able sentence level summarizers best summary sets pearl summary ditionally challenging achieve good mance wikihow multi news summary level learning process datasets evenly distributed appearance pearl summary makes selection best summary complicated conclusion proportion summaries best summaries erty characterize dataset affect choices summarization extractors inherent gap sentence level summary level extractors analysis explicated level method better sentence level method pick pearl summaries improvement bring given specic dataset based denition characterize upper bound level summary level summarization systems document reddit xsum cnn wikihow pubmed multi news figure distribution datasets number candidate summaries document different short text relatively dates use number candidate summaries axis axis represents proportion best summaries rank test set intuitively comes rst means best summary composed sentences highest score summary pearl summary increases gets lower rankings didate summaries sentence level score higher best summary leads ing difculty sentence level extractors inherently unaware pearl summary ing best summary difcult better utilize characteristics data propose summary level framework score extract summary directly specically formulate extractive rization task semantic text matching problem source document candidate maries extracted original text matched semantic space following section detail instantiate proposed ing summarization framework simple siamese based architecture siamese bert inspired siamese network structure bromley construct siamese bert tecture match document candidate summary siamese bert consists berts tied weights cosine similarity layer inference phase unlike modied bert liu bae directly use original bert derive semantically meaningful embeddings document candidate summary need obtain sentence level tion use vector cls token bert layer representation document summary let embeddings document candidate summary similarity score measured order tune siamese bert use margin based triplet loss update weights tuitively gold summary cally closest source document rst principle loss follow candidate summary margin value design pairwise margin loss candidate summaries sort candidate summaries descending order rouge scores gold summary naturally candidate pair larger ranking gap larger margin second principle design loss function figure different datasets max ccd max ccd set candidate summaries tracted quantify potential gain ument calculating difference finally dataset level potential gain tained represents specic dataset number documents dataset figure performance gain summary level method varies dataset improvement imum cnn figure ble performance gain lated length reference summary ferent datasets case short summaries reddit xsum perfect identication pearl summaries lead ment similarly multiple sentences long mary pubmed multi news large degree semantic overlap making improvement summary level method tively small medium length summary cnn wikihow words summary level learning process rewarding discuss performance gain specic models section summarization matching quantitative analysis suggests datasets sentence level extractors redditxsumcnn dmwikihowpubmedmulti represents candidate summary ranked hyperparameter distinguish tween good bad candidate summaries finally margin based triplet loss written reddit xsum cnn wiki pubmed news ext sel size basic idea let gold summary highest matching score time ter candidate summary obtain higher score compared unqualied candidate summary figure illustrate idea inference phase formulate extractive summarization task search best mary candidates extracted document arg max candidates pruning curse combination matching idea intuitive suffers combinatorial explosion problems example determine size candidate summary set score possible candidates ate difculties propose simple candidate pruning strategy concretely introduce content selection module pre select salient sentences ule learns assign sentence salience score prunes sentences irrelevant current document resulting pruned document similar previous work stage summarization content selection module parameterized neural network paper use bertsum liu lapata gram blocking bertext score sentence use simple rule obtain candidates generating combinations sel sentences subject pruned document organize order sentences according original position document form candidate summaries total sel candidate sets experiment datasets order verify effectiveness work obtain convicing explanations perform experiments divergent mainstream datasets follows table details candidate summary ferent datasets ext denotes number sentences prune original document sel denotes number sentences form candidate summary size number nal candidate summaries cnn dailymail hermann commonly summarization dataset modied nallapati contains news ticles associated highlights summaries paper use non anonymized version pubmed cohan collected scientic papers consists long ments modify dataset duction section document abstract section corresponding summary wikihow koupaee wang verse dataset extracted online knowledge base articles span wide range topics xsum narayan sentence summary dataset answer question article summaries ally written typically authors documents dataset multi news fabbri document news summarization dataset tively long summary use truncated version concatenate source documents single input experiments reddit kim highly abstractive dataset collected social media platform use tifu long version reddit regards body text post document summary implementation details use base version bert implement models experiments adam optimizer kingma warming learning rate schedule follows vaswani step step batch size denotes warmup steps choose model model lead oracle match oracle banditsum dong neusum zhou jecs durrett hibert zhang pnbert zhong pnbert bertext bertext bertext liu bertext tri blocking bertsum liu lapata bae bertext bertext tri blocking matchsum bert base matchsum roberta base table results cnn test set model indicates large version bert bertext add additional pointer network pared bertext table little effect mance cause performance degradation use validation set save best checkpoints training record performance best checkpoints test set importantly experimental results listed paper average runs obtain siamese bert model cnn use gpus hours training datasets remove samples document summary truncate document tokens oracle paper calculated truncated datasets details candidate summary different datasets found table experimental results results cnn shown table list strong baselines different learning proaches rst section contains lead acle match prune documents matching match oracle relatively low oracle common baselines marization task means extracting rst eral sentences document summary groundtruth extractive models training oracle groundtruth train matchsum bertext num bertext num matchsum sel matchsum sel matchsum sel bertext num bertext num matchsum sel matchsum sel matchsum sel reddit xsum table results test sets reddit xsum indicates sentences bertext tracts summary sel indicates number sentences choose form candidate summary second section score entire summary lead performance improvement ably relies sentence level summarizers pointer network sequence labeling models select sentences distinguishing semantics ent summaries trigram blocking simple effective heuristic cnn better redundancy removal methods based neural models compared models proposed matchsum outperformed competitors large margin example beats bertext score bert base encoder additionally compared baseline bert large pre trained encoder model matchsum bert base perform better furthermore change encoder base liu mance improved think provement roberta introduced million english news articles pretraining superior performance dataset strates effectiveness proposed matching framework results datasets short summaries reddit xsum heavily evaluated abstractive summarizer short maries evaluate model datasets investigate matchsum achieve improvement dealing model lead oracle match oracle bertext blocking blocking matchsum bert base wikihow pubmed multi news table results test sets wikihow pubmed multi news matchsum beats state art bert model ngram blocking different domain datasets summaries containing fewer sentences compared typical extractive models taking sentence match inal document matchsum degenerates ranking sentences table illustrates degradation bring small ment compared bertext num reddit xsum number sentences increases summary level semantics need taken account matchsum obtain markable improvement compared bertext num reddit xsum addition model maps candidate summary semantic space exibly choose number sentences methods extract xed number tences table advantage leads performance improvement results datasets long summaries summary relatively long level matching complicated harder learn aim compare difference trigram blocking model dealing long summaries table presents trigram blocking works cnn tain stable improvement ngram blocking little effect wikihow multi news causes large performance drop pubmed think reason ngram blocking understand semantics sentences summaries restricts presence entities words obviously suitable scientic domain entities appear multiple times contrary proposed method strong constraints aligns original document summary semantic space experiment results display model robust domains especially wikihow sum beats state art bert model score analysis following analysis driven tions benets matchsum sistent property dataset analyzed section model achieved different formance gains diverse datasets dataset splitting testing typically choose datasets xsum cnn wikihow largest performance gain iment split test set roughly equal numbers parts according described section experiment subset figure shows performance gap tween matchsum bertext smallest best summary summary phenomenon line understanding samples ability summary level extractor discover summaries bring advantages increases performance gap ally tends increase specically benet matchsum cnn highly consistent appearance pearl summary bring improvement subset smallest rises sharply reaches maximum value wikihow similar cnn best summary consists entirely highest scoring sentences performance gap obviously smaller samples xsum xsum cnn wikihow figure datasets splitting experiment split test sets parts according described section axis left right indicates subsets test set value small large axis represents rouge improvement matchsum bertext subset bertext dataset compared inherent gap sentence level summary level extractors dene ratio matchsum learn dataset inherent gap level summary level extractos clear figure value pends figure length gold summary table gold summaries longer upper bound summary level proaches difcult model reach matchsum achieve xsum words summary pubmed multi news summary length exceeds spective summary length similar model performs better datasets summaries instance evenly distributed multi news figure higher obtained pubmed pearl summaries better understanding dataset allows clear awareness strengths itations framework hope analysis provide useful clues future research extractive summarization conclusion formulate extractive summarization task semantic text matching problem propose novel summary level framework match source document candidate summaries semantic space conduct analysis model better characteristic data experimental results matchsum figure different datasets reddit excluded samples test set slightly different trend remains model perform samples largest needs improvement exploration comparison performance improvement matchsum concentrated samples summaries illustrates semantic based summary level model capture sentences particularly good viewed individually forming better summary intuitively comparison datasets provements brought matchsum framework associated inherent gaps presented section better understand relation introduce follows cbe represent candidate mary selected matchsum bertext document respectively indicate improvement matchsum dmwikihowpubmedmulti outperforms current state art extractive model benchmark datasets strates effectiveness method believe power matching based summarization framework fully exploited future forms matching models plored instantiated proposed framework acknowledgment like thank anonymous reviewers valuable comments work ported national key research ment program china national natural science foundation china shanghai nicipal science technology major project zjlab references alyguliyev stage unsupervised proach multidocument summarization automatic control computer sciences kristjan arumae fei liu reinforced tive summarization question focused rewards proceedings acl student research workshop pages sanghwan bae taeuk kim jihoon kim goo lee summary level training sentence rewriting abstractive summarization ings workshop new frontiers marization pages jane bromley isabelle guyon yann lecun eduard sackinger roopak shah signature cation siamese time delay neural network advances neural information processing tems pages yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proceedings annual ing association computational linguistics volume long papers volume pages jianpeng cheng mirella lapata neural marization extracting sentences words proceedings annual meeting sociation computational linguistics volume long papers volume pages arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang nazli goharian discourse aware attention model abstractive summarization long documents proceedings conference north american chapter association tional linguistics human language technologies volume short papers volume pages jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language standing proceedings conference north american chapter association computational linguistics human language nologies volume long short papers pages yue dong yikang shen eric crawford herke van hoof jackie chi kit cheung sum extractive summarization contextual dit proceedings conference pirical methods natural language processing pages alexander richard fabbri irene tianwei suyi dragomir radev multi news large scale multi document summarization dataset acl abstractive hierarchical model pages association computational linguistics dimitrios galanis ion androutsopoulos extractive supervised stage method sentence human language technologies compression annual conference north american chapter association computational guistics pages association tional linguistics dan gillick benoit favre scalable global proceedings model summarization workshop integer linear programming ural language processing pages karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read advances neural comprehend tion processing systems pages elad hoffer nir ailon deep metric learning triplet network international workshop similarity based pattern recognition pages springer aishwarya jadhav vaibhav rajan tive summarization swap net sentences words alternating pointer networks ceedings annual meeting tion computational linguistics volume long papers volume pages byeongchang kim hyunwoo kim gunhee kim abstractive summarization reddit posts multi level memory networks ings conference north american chapter association computational guistics human language technologies volume long short papers pages diederik kingma jimmy adam method stochastic optimization arxiv preprint mahnaz koupaee william yang wang ihow large scale text summarization dataset arxiv preprint logan lebanoff kaiqiang song franck dernoncourt doo soon kim seokhwan kim walter chang scoring sentence singletons fei liu pairs abstractive summarization arxiv preprint chin yew lin eduard hovy matic evaluation summaries gram occurrence statistics proceedings man language technology conference north american chapter association tional linguistics pages yang liu fine tune bert extractive rization arxiv preprint yang liu mirella lapata text proceedings tion pretrained encoders conference empirical methods ural language processing international joint conference natural language processing emnlp ijcnlp pages yinhan liu myle ott naman goyal jingfei dar joshi danqi chen omer levy mike lewis luke zettlemoyer veselin stoyanov roberta robustly optimized bert pretraining proach arxiv preprint alfonso mendes shashi narayan sebastiao miranda zita marinho andre martins shay hen jointly extracting compressing uments summary state representations ceedings conference north ican chapter association computational linguistics human language technologies ume long short papers pages yishu miao phil blunsom language latent variable discrete generative models tence compression proceedings ference empirical methods natural language processing pages bhaskar mitra fernando diaz nick craswell learning match local distributed representations text web search ings international conference world wide web pages international world wide web conferences steering committee ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments thirty aaai conference articial intelligence ramesh nallapati bowen zhou cicero dos santos glar bing xiang tive text summarization sequence sequence rnns conll page shashi narayan shay cohen mirella lapata details summary topic aware convolutional neural networks proceedings treme summarization conference empirical methods natural guage processing pages shashi narayan shay cohen mirella lapata ranking sentences extractive tion reinforcement learning proceedings conference north american ter association computational linguistics human language technologies volume long pers volume pages romain paulus caiming xiong richard socher deep reinforced model abstractive marization arxiv preprint nils reimers iryna gurevych bert sentence embeddings siamese networks proceedings conference empirical methods natural language processing international joint conference ral language processing emnlp ijcnlp pages aliaksei severyn alessandro moschitti learning rank short text pairs convolutional deep neural networks proceedings ternational acm sigir conference research development information retrieval pages acm ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need advances neural information cessing systems pages xiaojun wan ziqiang cao furu wei sujian ming zhou multi document tion discriminative summary reranking arxiv preprint danqing wang pengfei liu yining zheng xipeng qiu xuan jing huang heterogeneous graph neural networks extractive document marization proceedings conference association computational linguistics danqing wang pengfei liu ming zhong jie xipeng qiu xuanjing huang exploring domain shift extractive text summarization arxiv preprint shuohang wang jing jiang learning ral language inference lstm proceedings conference north american ter association computational linguistics human language technologies pages zhiguo wang wael hamza radu florian bilateral multi perspective matching natural guage sentences proceedings national joint conference articial intelligence pages aaai press jiacheng greg durrett neural tive text summarization syntactic compression proceedings conference cal methods natural language processing hong kong china association computational guistics jiacheng zhe gan cheng jingjing discourse aware neural extractive arxiv preprint liu model text summarization wen tau yih ming wei chang christopher meek andrzej pastusiak question answering enhanced lexical semantic models proceedings annual meeting association computational linguistics volume long papers pages dani yogatama fei liu noah smith tractive summarization maximizing semantic proceedings conference ume empirical methods natural language processing pages haoyu zhang yeyun gong yan nan duan jun wang ming gong ming zhou language arxiv preprint eration text summarization pretraining based natural xingxing zhang furu wei ming zhou hibert document level pre training hierarchical bidirectional transformers document tion acl ming zhong pengfei liu danqing wang xipeng qiu xuan jing huang searching tive neural extractive summarization works proceedings ence association computational tics pages ming zhong danqing wang pengfei liu xipeng qiu xuanjing huang closer look data bias neural extractive summarization models emnlp ijcnlp page qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural ment summarization jointly learning score select sentences proceedings annual meeting association computational guistics volume long papers volume pages
