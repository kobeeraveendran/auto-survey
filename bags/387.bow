multichannel lstm cnn telugu technical domain identication sunil gundapu language technologies research centre kcis iiit hyderabad telangana india sunil iiit radhika mamidi language technologies research centre kcis iiit hyderabad telangana india radhika abstract instantaneous growth text tion retrieving domain oriented information text data broad range tions information retrieval natural guage processing thematic keywords compressed representation text usually domain identication plays signicant role machine translation text summarization question answering information extraction sentiment analysis paper posed multichannel lstm cnn ology technical domain identication telugu architecture uated context icon shared task techdocation task tem got score test dataset validation set introduction technical domain identication task matically identifying categorizing set beled text passages documents ing domain categories predened domain category set domain category set consists category labels bio chemistry tion technology computer science management physics domains viewed set text passages test text data treated query system domain cation applications like machine lation summarization question answering task rst step stream applications machine translation decides domain text data afterward machine translation choose resources identied domain majority research work area text classication domain identication english tribution regional languages especially indian languages telugu india old traditional languages categorized vidian language family according list million native telugu speakers ranks sixteenth spoken guages worldwide tried identify domain telugu text data supervised machine learning deep learning techniques work multichannel lstm cnn method outperforms methods provided dataset proach incorporates advantages cnn self attention based bilstm model rest paper structured follows section explains related works domain identication section describes dataset vided shared task section addresses methodology applied task section presents results error analysis nally section concludes paper possible future works related work methods domain identication text categorization indian languages works reported telugu language section survey methodologies approaches address domain identication text categorization murthy explains automatic text gorization special emphasis telugu research work supervised classication naive bayes classier applied ugu news articles text categorization swamy work representing categorizing indian language text documents text mining techniques nearest neighbour naive bayes decision tree classier ethnologue com guides categorization telugu text documents language dependent independent models posed narala durga han introduced model document sication text categorization paper described term frequency ontology based text categorization telugu documents combining lstm cnn robustness liu posed attention based multichannel convolutional neural network text classication network bilstm encodes history future information words cnn capture relations words dataset description dataset provided organizers task techdocation training models data task consists text documents training validation testing hyperparameter tuning validation set provided organizers statistics dataset shown table texts dataset seen figure train data validation data cse phy com tech bio tech mgnt total table dataset statistics figure number samples class proposed approach data preprocessing text passages originally provided telugu script corresponding domain tags text documents noise fore passing text training stage preprocessed following procedure acronym mapping dictionary created acronym mapping dictionary expanded english acronyms acronym ping dictionary find language words english words located telugu words passage index words translate telugu translate english words translate glish words telugu language identied rst stage ing google translation purpose hindi sentence translation serve hindi sentences dataset translated sentences telugu google translation tool noise removal removed unnecessary tokens punctuation marks non utf format tokens single length english tokens text data supervise machine learning algorithms build nest system domain identication started supervised machine learning niques moved deep learning models svm multilayer perceptron linear classier gradient boosting methods performed given training dataset supervised models trained word level gram level character level idf vector representations multichannel lstm cnn architecture started experiments individual lstm gru cnn models different word dings like glove fasstext ensembling cnn self attention lstm model gave better results individual models googletrans readthedocs develop multichannel model domain identication consisting main components rst component long short time ory hochreiter schmidhuber forth lstm advantage lstm dle long term dependencies store global semantics foregoing information variable sized vector second component convolutional neural network lecun henceforth cnn advantage cnn capture gram features text volution lters restricts performance convolutional lters size considering strengths components ensemble lstm cnn model domain tion figure self attention bilstm forward backward hidden states catenate forward backward hidden units merged representations self attention model gives score subword sentence given equation figure multichannel lstm cnn model self attention bilstm classier rst module architecture self attention based bilstm classier employed attention based bilstm model extract semantic sentiment information input text data self attention attention mechanism softmax function gives subword weights sentence outcome module weighted sum den representations subword self attention mechanism built stms architecture gure takes input pre trained embeddings subwords passed telugu fasttext grave word embeddings bilstm layer hidden representation timestep input self attention component suppose input sentence given subwords let represents represents forward hidden state ward hidden state ith position bilstm merged representation obtained combining calculate attention weight normalizing attention score finally compute sentence latent sentation vector equation latent representation vector fed fully connected layer followed softmax layer obtain probabilities plstm convolutional neural network second component cnn consider ordering words context word appears sentence present telugu fasstext subword embeddings bojanowski sentence cnn gure generate required embeddings figure cnn classier initially present sentence ding matrix convolution layer row dimension fasstext subword embedding tor word sentence length perform convolution operations convolution layer different kernel sizes purpose kernel sizes capture contexts varying lengths extract local features word window output convolution layers passed responding max pooling layers max pooling layer preserve word order bring important features feature map change original max pooling layer convolution neural network word preserving max pooling layer preserve putted sentences word order order persevering max pooling layer reduces number features preserving order words max pooling layer output concatenated fed fully connected layer followed softmax layer obtain softmax probabilities pcnn cnn bilstm models softmax abilities aggregated gure element wise product obtain nal ities inal pcnn plstm tried gregation techniques like average maximum imum element wise addition element wise multiplication combine lstm cnn els probabilities element wise product gave better results techniques results error analysis rst started experiments machine ing algorithms kinds idf ture vector representations svm mlp gave pretty good results validation dataset failed bio chemistry management data points cnn model fasstext word embeddings confused computer nology cse data points table maybe reason confusion datapoints similar syntactic level self attention based bilstm model forms cnn model physics cse puter technology data points performs worse bio chemistry management data points observe training set data samples belong physics cse computer technology domains remaining data owned remaining domain labels suming imbalanced training set problems misclassication chemistry management domain samples tried handle data skewing problem smote chawla technique work observing cnn bilstm model results ensemble models better results ensemble multichannel lstm cnn model outperforms previous models ing recall weighted score development dataset recall score test dataset table validation data test data model accuracy precision recall score accracy precision recall score svm cnn bilstm multichannel organizers system table comparison model results sepp hochreiter jurgen schmidhuber long short term memory neural computation lecun generalization network design strategies zhenyu liu haiwei huang chaohong shengfei lyu multichannel cnn tion text classication arxiv kavi murthy automatic categorization telugu news articles swapna narala rani ramakrishna telugu text categorization language models global journal computer science technology swamy hanumanthappa jyothi indian language text representation categorization supervised learning algorithm international conference intelligent puting applications pages kelvin jimmy ryan kiros kyunghyun cho aaron courville salakhutdinov zemel yoshua bengio attend tell neural image caption generation visual attention icml validation data test data precision recall accuracy table performance multichannel system conclusion paper proposed multichannel approach integrates advantages cnn lstm model captures local global dependencies sentiment sentence approach gives better results individual cnn lstm supervised machine learning algorithms ugu techdocation dataset discussed previous section handle data imbalance problem efciently future work improve performance unambiguous cases cse computer nology domains references piotr bojanowski edouard grave armand joulin tomas mikolov enriching word arxiv preprint tors subword information nitesh chawla kevin bowyer lawrence hall kegelmeyer smote synthetic minority sampling technique artif intell res jair durga govardhan ontology based text categorization telugu documents edouard grave piotr bojanowski prakhar gupta mand joulin tomas mikolov learning proceedings word vectors languages international conference language sources evaluation lrec
