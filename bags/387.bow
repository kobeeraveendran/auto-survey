multichannel lstm cnn for telugu technical domain identication sunil gundapu language technologies research centre kcis iiit hyderabad telangana india sunil iiit ac in radhika mamidi language technologies research centre kcis iiit hyderabad telangana india radhika ac in e f l c s c v v i x r a abstract with the instantaneous growth of text tion retrieving domain oriented information from the text data has a broad range of tions in information retrieval and natural guage processing thematic keywords give a compressed representation of the text usually domain identication plays a signicant role in machine translation text summarization question answering information extraction and sentiment analysis in this paper we posed the multichannel lstm cnn ology for technical domain identication for telugu this architecture was used and uated in the context of the icon shared task techdocation task h and our tem got of the score on the test dataset and on the validation set introduction technical domain identication is the task of matically identifying and categorizing a set of beled text passages documents to their ing domain categories from a predened domain category set the domain category set consists of category labels bio chemistry tion technology computer science management physics and other these domains can be viewed as a set of text passages and test text data can be treated as a query to the system domain cation has many applications like machine lation summarization question answering this task would be the rst step for most stream applications i e machine translation it decides the domain for text data and afterward machine translation can choose its resources as per the identied domain majority of the research work in the area of text classication and domain identication has been done in english there has been well below tribution for regional languages especially indian languages telugu is one of india s old traditional languages and it is categorized as one of the vidian language family according to the list there are about million native telugu speakers and it ranks sixteenth most spoken guages worldwide we tried to identify the domain of telugu text data using various supervised machine learning and deep learning techniques in our work our multichannel lstm cnn method outperforms the other methods on the provided dataset this proach incorporates the advantages of cnn and self attention based bilstm into one model the rest of the paper is structured as follows section explains some related works of domain identication section describes the dataset vided in the shared task section addresses the methodology applied in the task section presents the results and error analysis and nally section concludes the paper as well as possible future works related work several methods for domain identication and text categorization have been done on indian languages and few of the works have been reported on the telugu language in this section we survey some of the methodologies and approaches used to address domain identication and text categorization murthy explains the automatic text gorization with special emphasis on telugu in his research work supervised classication using the naive bayes classier has been applied to ugu news articles for text categorization swamy et al work on representing and categorizing indian language text documents using text mining techniques k nearest neighbour naive bayes and decision tree classier ethnologue com guides categorization of telugu text documents using language dependent and independent models posed by narala et al durga and han introduced a model for document sication and text categorization in their paper described a term frequency ontology based text categorization for telugu documents combining lstm and cnn s robustness liu et al posed attention based multichannel convolutional neural network for text classication in their network bilstm encodes the history and future information of words and cnn capture relations between words dataset description we used the dataset provided by the organizers of task h of techdocation for training the models the data for the task consists of text documents for training for validation for testing for hyperparameter tuning we used the validation set provided by the organizers the statistics of the dataset are shown in table and the amount of texts for the dataset can be seen in figure train data validation data cse phy com tech bio tech mgnt other total table dataset statistics figure number of samples per class proposed approach data preprocessing the text passages have been originally provided in the telugu script with the corresponding domain tags the text documents have some noise so fore passing the text to the training stage they are preprocessed using the following procedure acronym mapping dictionary we created an acronym mapping dictionary expanded the english acronyms using the acronym ping dictionary find language words sometimes english words are co located with telugu words in the passage we nd the index of those words to translate into telugu translate english words translate the glish words into the telugu language which are identied in the rst stage of ing google s translation was used for this purpose hindi sentence translation we can serve a few hindi sentences in the dataset we translated those sentences into telugu using google translation tool noise removal removed the unnecessary tokens punctuation marks non utf format tokens and single length english tokens from the text data supervise machine learning algorithms to build the nest system for domain identication we started with supervised machine learning niques then moved to deep learning models svm multilayer perceptron linear classier gradient boosting methods performed very well on the given training dataset these supervised models trained on the word level n gram level and character level tf idf vector representations multichannel lstm cnn architecture we started experiments with individual lstm gru cnn models with different word dings like glove and fasstext ever ensembling of cnn with self attention lstm model gave better results than individual models googletrans readthedocs io en we develop a multichannel model for domain identication consisting of two main components the rst component is a long short time ory hochreiter and schmidhuber forth lstm the advantage of lstm can dle the long term dependencies but does not store the global semantics of foregoing information in a variable sized vector the second component is a convolutional neural network lecun henceforth cnn the advantage of cnn can capture the n gram features of text by using volution lters but it restricts the performance due to convolutional lters size by considering the strengths of these two components we ensemble the lstm and cnn model for domain tion figure self attention bilstm the forward and backward hidden states we catenate the forward and backward hidden units to get the merged representations hi hi the self attention model gives a score ei to each subword i in the sentence s as given by below equation figure multichannel lstm cnn model ei kt i kn self attention bilstm classier the rst module in architecture is self attention based bilstm classier we employed this attention xu et al based bilstm model to extract the semantic and sentiment information from the input text data self attention is an attention mechanism in which a softmax function gives each subword s weights in the sentence the outcome of this module is a weighted sum of den representations at each subword the self attention mechanism is built on stms architecture see gure and it takes input as pre trained embeddings of the subwords we passed the telugu fasttext grave et al word embeddings to a bilstm layer to get hidden representation at each timestep which is the input to the self attention component suppose the input sentence s is given by the subwords wn let h represents the h represents the forward hidden state and ward hidden state at ith position in bilstm the merged representation ki is obtained by combining then we calculate the attention weight ai by normalizing the attention score ai finally we compute the sentence s latent sentation vector h using below equation the latent representation vector h is fed to a fully connected layer followed by a softmax layer to obtain probabilities plstm convolutional neural network the second component is cnn which consider the ordering of the words and the context in which each word appears in the sentence we present telugu fasstext subword embeddings bojanowski et al of a sentence to cnn see gure to generate the required embeddings figure cnn classier initially we present a d s sentence ding matrix to the convolution layer each row is a dimension fasstext subword embedding tor of each word and s is sentence length we perform convolution operations in the convolution layer with three different kernel sizes and the purpose behind using various kernel sizes was to capture contexts of varying lengths and to extract local features around each word window the output of convolution layers was passed to responding max pooling layers the max pooling layer is used to preserve the word order and bring out the important features from the feature map we change the original max pooling layer in the convolution neural network with the word preserving k max pooling layer to preserve the putted sentences word order the order persevering max pooling layer reduces the number of features while preserving the order of these words the max pooling layer output is concatenated together fed to a fully connected layer followed by a softmax layer to obtain softmax probabilities pcnn the cnn and bilstm models softmax abilities are aggregated see gure using an element wise product to obtain the nal ities pf inal pcnn plstm we tried various gregation techniques like average maximum imum element wise addition and element wise multiplication to combine lstm and cnn els probabilities but an element wise product gave better results than other techniques results and error analysis we rst started our experiments with machine ing algorithms with various kinds of tf idf ture vector representations svm mlp gave pretty good results on the validation dataset but failed on bio chemistry and management data points and cnn model with fasstext word embeddings seemed to be confused between the computer nology and cse data points see table maybe the reason behind this confusion is that both datapoints quite similar at the syntactic level the self attention based bilstm model forms the cnn model on physics cse and puter technology data points though it performs worse on the bio chemistry and management data points if we observe the training set of the data samples belong to physics cse and computer technology domains remaining of data owned by the remaining domain labels so we were suming that an imbalanced training set was also one of the problems for misclassication of chemistry and management domain samples we tried to handle the this data skewing problem with smote chawla et al technique but it is does nt work very well after observing the cnn and bilstm model results we ensemble these two models for better results the ensemble multichannel lstm cnn model outperforms all our previous models ing a recall of with the weighted score of on the development dataset and a recall of with an score of on the test dataset see table validation data test data model accuracy precision recall score accracy precision recall score svm cnn bilstm multichannel organizers system table comparison between various model results sepp hochreiter and jurgen schmidhuber long short term memory neural computation y lecun generalization and network design strategies zhenyu liu haiwei huang chaohong lu and shengfei lyu multichannel cnn with tion for text classication arxiv kavi murthy automatic categorization of telugu news articles swapna narala b p rani and k ramakrishna telugu text categorization using language models global journal of computer science and technology m n swamy m hanumanthappa and n jyothi indian language text representation and categorization using supervised learning algorithm international conference on intelligent puting applications pages kelvin xu jimmy ba ryan kiros kyunghyun cho aaron c courville r salakhutdinov r zemel and yoshua bengio show attend and tell neural image caption generation with visual attention in icml validation data test data precision recall accuracy table performance of multichannel system conclusion in this paper we proposed a multichannel approach that integrates the advantages of cnn and lstm this model captures local global dependencies and sentiment in a sentence our approach gives better results than individual cnn lstm and supervised machine learning algorithms on the ugu techdocation dataset as discussed in the previous section we will handle the data imbalance problem efciently in future work and we will improve the performance of the unambiguous cases in cse and computer nology domains references piotr bojanowski edouard grave armand joulin and tomas mikolov enriching word arxiv preprint tors with subword information nitesh chawla kevin bowyer lawrence hall and w kegelmeyer smote synthetic minority over sampling technique j artif intell res jair a durga and a govardhan ontology based text categorization telugu documents edouard grave piotr bojanowski prakhar gupta mand joulin and tomas mikolov learning in proceedings word vectors for languages of the international conference on language sources and evaluation lrec
