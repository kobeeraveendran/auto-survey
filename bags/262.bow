impressive performance randomly weighted encoders summarization tasks jonathan jaehong christopher institute learning algorithms polytechnique montreal cifar chair pilault jaehong com abstract work investigate performance untrained randomly initialized encoders general class sequence sequence els compare performance fully trained encoders task tive summarization hypothesize dom projections input text representational power encode chical structure sentences semantics documents trained decoder duce abstractive text summaries cally demonstrate architectures trained randomly initialized encoders perform competitively respect equivalent chitectures fully trained encoders capacity encoder improves overall model tion closes performance gap tween untrained randomly initialized trained encoders knowledge rst time general sequence sequence models attention assessed trained randomly projected representations stractive summarization introduction recent state art natural language ing nlp models operate directly raw text sidestepping typical prepossessing steps classical nlp use hand crafted tures young typically assumed engineered features needed critical parts language modeled directly encoded word sentence representations deep neural networks dnn instance searchers attempted evaluate ability recurrent neural networks rnn represent cal structural compositional semantics linzen hupkes lake roni study morphological learning equal contribution order determined coin machine translation belinkov dalvi diagnostic methods proposed analyze linguistic properties xed length vector hold ettinger adi kiela relatively little known exact properties learned encoded sentence document tations training general linguistic structures shown important nlp strubell mccallum knowing information comes architectural bias trained weights meaningful signing better performing models recently demonstrated randomly parameterized combinations pre trained word embeddings comparable performance fully trained sentence embeddings wieting kiela experiments question gains trained modern sentence embeddings random ods showing random encoders perform close state art sentence embeddings eting kiela challenged assumption sentence embeddings greatly improved training encoder follow wieting kiela generalize approaches complex sequence sequence learning larly abstractive text summarization tigate aspects random encoders hierarchical recurrent encoder decoder hred architecture untrained domly initialized encoders fully trained encoders work seek answer main questions effective untrained randomly initialized hierarchical rnns ing document structure semantics untrained encoders close performance trained encoders challenging task long text summarization tasks capacity encoder decoder affect quality ated summaries trained untrained coders answer questions analyse plexity rouge scores random hred fully trained hreds hidden sizes nlp classication tasks random embeddings shown useful eting kiela testing efcacy conditional language generation task main contribution present empirical evidence random projections sent text hierarchy achieve results par fully trained representations ful pretrained word embeddings dom hierarchical representations input text perform similarly trained hierarchical tations empirically demonstrate general models attention gap random encoder trained encoder comes smaller increasing size tions nally provide evidence validate optimization training networks properly best knowledge rst time analysis formed general class tion challenging task long text marization related work fixed random weights neural networks random neural network minsky ridge dened neural network weights initialized randomly randomly trained optimized particular task random neural networks studied training optimization cedures infeasible tional resources time shown low dimensional problems feed forward neural networks ffnn xed random weights achieve comparable accuracy smaller dard deviations compared network trained gradient backpropagation schmidt inspired work extreme learning machines elm proposed huang elm gle layer ffnn output weights learned simple generalized inverse ations hidden layer output matrices sequent theoretical studies demonstrated randomly generated hidden weights elm maintains universal approximation bility equivalent fully trained ffnn huang works explored effects randomness vision tasks stationary els work explore randomness nlp tasks autoregressive models similar ideas developed rnns echo state networks esn jaeger generally reservoir computing krylov krylov core dynamics input sequence modeled large reservoir random untrained weights state mapped output space trainable readout layer esn leverage vian architectural bias rnns able encode input history recurrent random projections esn comparable generalization elm generally known robust non linear time series prediction problems research randomness autoregressive models context encoder decoder architectures nlp random encoders deep architectures fixed random weights studied encode types input data computer vision shown random kernels convolutional neural networks cnn perform reasonably object recognition tasks jarrett works highlighted importance setting random weights cnn found mance network explained choice cnn architecture instead timized weights saxe similarly random encoder architectures ural language processing nlp deeply vestigated wieting kiela context sentence embeddings ments pre trained word embeddings passed different randomly weighted trained encoders bag random embedding jections random long short term memory work hochreiter schmidhuber lstm echo state network esn dom sentence representation passed learnable decoder solve senteval conneau kiela downstream tasks sentiment analysis question type product reviews jectivity opinion polarity paraphrasing ment semantic relatedness interestingly performance random sentence figure architecture random encoder summarization model weight parameters sentence document encoder lstms randomly initialized xed parameters word embeddings word sentence level attention decoder lstms learned training blue parts architecture encoder recurrent neural networks weights randomly initialized trained orange parts decoder lstms weights trained encoder context vector ground truth target summary token predicted token tions close modern sentence dings infersent conneau skipthought kiros authors gued effectiveness current sentence bedding methods benet largely representational power pre trained word beddings random encoding text deployed solve nlp classication tasks random encoding conditional text ation studied approach hypothesize markovian representation word history position text provided randomly parameterized encoders rich achieve comparable results fully trained networks difcult nlp tasks work compare hierarchical recurrent encoder decoder hred models randomly xed encoder ones fully trained normal end end manner abstractive summarization tasks particular examine performances random encoder models varying coder decoder capacity isolate root causes performance gaps trained coders random untrained encoders provide analysis gradient ows relative weight changes training model hierarchical recurrent encoder decoder model similar nallapati model consists encoders sentence encoder document encoder sentence encoder recurrent neural network encodes quence input words sentence xed size sentence representation specically hidden state recurrent neural work sentence encoding embeddings input token length corresponding sentence sentence encoder shared sentences input document sequence sentence codings passed document encoder recurrent neural network denotes encoding sentence number sentences input ument decoder recurrent neural network generates target summary token token model encoder abstractive model nallapati attn vocab attn vocab pointer generator network hred attn pointer hred attn pointer hred attn pointer hred attn pointer trained hierarchical gru trained lstm trained lstm trained lstm trained lstm random lstm random lstm esn identity lstm rouge table results cnn daily mail test dataset rouge scores condence interval reported ofcial rouge script capture relevant context source ument decoder leverages cal attention mechanism nallapati concretely decoder computes level attention weight sentence coder states input tokens decoder tains sentence level attention weight ument encoder hidden states nal attention weight integrates sentence level tention capture salient input word sentence levels denote word level attention weight tokens input ument respectively sentence level tion weight sentence input ument returns index sentence word position total number tokens input document pointer generator architecture enables coder copy words directly source ument use pointer generator allows model effectively deal vocabulary tokens additionally decoder coverage prevent summarization model generating phrase multiple times detailed scription pointer generator decoder age found cohan experiment analysis experiments aim demonstrate random encoding reach similar performances trained encoding conditional natural guage generation task appreciate bution rst describe experimental architectural setup deep diving sults cnn daily mail hermann nallapati summarization task fully trainable chical encoder models use directional lstms sentence document encoder trained lstm different types untrained random hierarchical encoders investigate random lstm random directional lstms rand lstm sentence encoder weight document ces biases initialized uniformly hidden size lstms identity lstm similar lstm hidden weights biases matrices set identity random lstm esn random directional lstm sentence encoder ized way echo state network esn document encoder weights sampled randomly normal distribution architecture random encoder rization model depicted figure current networks including echo state network single layer note tied beddings press wolf source word beddings learned random trainable encoders important setting periments aim isolate effect trained data preprocessing code provided com abisee cnn dailymail enc dec trained random random lstm lstm lstm esn table test perplexity trained random chical encoder models cnn daily mail dataset lower better note enc encoder hidden size dec decoder hidden size percentages relative perplexity degradation random encoder models respect associated trained lstm encoder model encoders trained word embeddings experiments single directional lstm decoder generally follow standard hyperparameters suggested cohan guide readers appendix details training evaluation steps performance random encoder models table shows rouge scores lin monly performance metric summarization trained untrained random encoder els note hierarchical random encoder models random lstm random lstm esn obtain rouge scores close trained models gap trained dom hierarchical encoders point rouge scores hierarchical random encoders outperform competitive baseline pati terms cited model uses pre trained dings respect trained lstm random lstm achieves rouge scores gap similar rouge tested identity lstm idea role trained word embeddings play performance identity lstm creates sentences representations accumulating trained word embeddings equation measure representational power random projection encoder compare rouge scores random hierarchical encoder models identity lstm model notice random encoders greatly identity lstm encoder brought closer gauging effectiveness randomly projected recurrent encoder hidden states lation word embeddings impact increasing capacity table shows test perplexity random archical encoder models different encoder decoder hidden sizes chose base ysis perplexity instead rouge isolate model performance effect word pling methods beam search quality overall predicted word ties shown increased model capacity leads lower test perplexity implies ter target word prediction improvement performance equal models notice random encoders close mance gap fully trained counterpart encoder hidden size increases instance vary encoder hidden size random lstm relative perplexity gap trained lstm diminishes pattern aligns previous work wieting kiela authors covered performance random sentence encoder converged trained mensionality sentence embeddings increased perform similar experiments coder hidden size varies xing encoder hidden size rst expected hidden size trained coder play larger role enhancing model performance random encoder shown table perplexity random encoder models largest encoder hidden size close random encoder models largest decoder hidden size formance gaps previously mentioned guration respect fully trained small conclusions draw result increasing capacity random encoder closes performance gap tween fully trained random encoder models second increasing number parameters random encoder yields similar improvements performance gap random identity tialization pronounced input word beddings trained document encoder sentence encoder figure relative weight change legend indicates different combinations encoder decoder hidden sizes update encoders trained lstm gure increasing number parameters trainable decoder illustrates important advantages random encoder terms number parameters train gradient trainable encoders suspect smaller performance gap bigger encoder decoder hidden size arise optimization issues rnns bengio hochreiter vanishing gradient problem verify rameters large trained models learned erly analyze distribution weight eters gradients model results rst notice networks different capacity different scale rameter gradient values makes sible directly compare gradient distributions models different capacity amine relative weight changes trained encoder lstms follows weight parameter encoder lstm number parameters coder lstm weight index number training updates relative encoder weight change depicted figure iterations observe difference relative weight changes tween small large encoder models sentence document encoder weights hidden sizes similar patterns training iterations details distributions weight parameters gradients refer reader appendix added ing curves appendix trained models converged given eters trained lstm properly timized conclude trained weights contribute signicantly model performance conditional natural language generation task summarization conclusion future work comparative study analyzed mance untrained randomly initialized encoders complex task classication ing kiela concretely performance random hierarchical encoder models uated challenging task abstractive marization shown untrained dom encoders able capture hierarchical structure documents tion qualiy comparable fully trained models provided empirical evidence increasing model capacity hances performance model closes gap random fully trained chical encoders future works investigate effectiveness random encoders nlp tasks machine translation question answering acknowledgements like thank devon hjelm ioannis mitliagkas catherine lefebvre useful comments minh dao helping ures work partially supported ivado excellence scholarship canada research excellence fund references yossi adi einat kermany yonatan belinkov ofer lavi yoav goldberg fine grained ysis sentence embeddings auxiliary tion tasks corr yonatan belinkov nadir durrani fahim dalvi san sajjad james glass ral machine translation models learn ogy corr bengio simard frasconi ing long term dependencies gradient descent difcult trans neur netw arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang nazli goharian discourse aware attention model abstractive summarization long documents corr alexis conneau douwe kiela senteval evaluation toolkit universal sentence tions arxiv preprint alexis conneau douwe kiela holger schwenk loic barrault antoine bordes supervised learning universal sentence representations arxiv preprint natural language inference data fahim dalvi nadir durrani hassan sajjad yonatan belinkov stephan vogel understanding improving morphological learning neural machine translation decoder ijcnlp john duchi elad hazan yoram singer adaptive subgradient methods online learning journal machine stochastic optimization learning research allyson ettinger ahmed elgohary philip resnik probing semantic evidence composition means simple classication tasks ings workshop evaluating vector space representations nlp pages berlin germany association computational tics karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend proceedings international conference neural mation processing systems volume pages cambridge usa mit press sepp hochreiter yoshua bengio paolo frasconi gradient recurrent nets difculty learning long term dependencies kolen kremer editors field guide dynamical current networks ieee press sepp hochreiter jurgen schmidhuber long short term memory neural comput guang bin huang lei chen chee kheong siew universal approximation incremental constructive feedforward networks random den nodes trans neur netw zhu huang siew extreme learning machine new learning scheme ward neural networks ieee international joint conference neural networks ieee cat volume pages vol dieuwke hupkes sara veldhoen willem zuidema visualisation diagnostic siers reveal recurrent recursive neural corr networks process hierarchical structure herbert jaeger echo state approach analysing training recurrent neural erratum note bonn germany german national research center information ogy gmd technical report kevin jarrett koray kavukcuoglu marcaurelio zato yann lecun best stage architecture object recognition ieee international conference computer vision iccv pages douwe kiela alexis conneau allan jabri imilian nickel learning visually grounded sentence representations corr ryan kiros yukun zhu ruslan salakhutdinov richard zemel raquel urtasun antonio torralba sanja fidler skip thought vectors advances neural information processing systems pages krylov krylov reservoir computing journal echo state network classier training physics conference series brenden lake marco baroni systematic years tional skills sequence sequence recurrent works corr bin yibin xuewen rong son echo state network extreme learning chine nonlinear prediction journal tional information systems chin yew lin looking good metrics automatic summarization evaluation samples ntcir tal linzen emmanuel dupoux yoav goldberg assessing ability lstms learn sensitive dependencies corr marvin minsky oliver selfridge ing random nets ieee transactions tion theory ramesh nallapati bowen zhou caglar gulcehre bing xiang abstractive text rization sequence sequence rnns yond arxiv preprint press lior wolf output arxiv embedding improve language models preprint andrew saxe pang wei koh zhenghao chen neesh bhand bipin suresh andrew random weights unsupervised proceedings ture learning national conference international conference machine learning pages usa omnipress schmidt kraaijveld duin feedforward neural networks random weights proceedings iapr international conference pattern recognition vol ence pattern recognition methodology tems pages abigail peter liu christopher manning point summarization generator networks corr emma strubell andrew mccallum tax helps elmo understand semantics syntax relevant deep neural architecture srl proceedings workshop relevance linguistic structure neural architectures nlp pages melbourne australia tion computational linguistics john wieting douwe kiela training required exploring random encoders sentence classication corr tom young devamanyu hazarika soujanya poria erik cambria recent trends deep ieee learning based natural language processing computational intelligence magazine appendix training evaluation dimensionality embeddings beddings trained scratch vocabulary size limited training strain document length tokens summary length tokens batch size learning rate adagrad duchi initial accumulator value optimization maximum gradient norm set training performed epochs test time set maximum number generated tokens beam search beam size decoding evaluate qualities ated summaries use standard rouge ric lin report standard rouge scores learning curves figure learning curves able random encoder summarization models different encoder decoder hidden sizes note gap training validation plexity trained random encoder els smaller encoder decoder hidden size increases weight gradient distribution figure present distributions model parameters gradients fully trainable models different capacities note models different capacities different scale bution models smaller encoder hidden size tend larger scale parameter gradient values figure training perplexity trained random encoder summarization models different encoder decoder hidden sizes enc denotes encoder dec denotes decoder numbers parentheses corresponding hidden sizes figure validation perplexity trained random encoder summarization models different encoder decoder hidden sizes enc denotes encoder dec denotes decoder numbers parentheses corresponding hidden sizes weight weight weight gradient gradient gradient figure distribution weight parameters gradients document encoder weight weight weight gradient gradient gradient figure distribution weight parameters gradients sentence encoder
