automatic extractive text summarization single audit reports machine learning vivian chou harvard medical school boston usa com leanna kent elder research inc arlington usa leanna com joel gngora elder research inc arlington usa joel com sam ballerini elder research inc arlington usa sam com carl hoover clarkson university potsdam usa edu abstract rapid growth text data motivated development machine learning based automatic text summarization strategies concisely capture essential ideas larger text study aimed devise extractive summarization method single audits assess recipients federal grants compliant program requirements use federal funding currently voluminous audits manually analyzed officials oversight risk management prioritization purposes automated summarization potential streamline processes analysis focused findings section single audits spanning following text preprocessing glove embedding sentence level means clustering performed partition sentences topic establish importance sentence audit key summary sentences extracted proximity cluster centroids summaries judged non expert human evaluation compared human generated summaries rouge metric goal fully automate summarization audits human input required stages large variability audit writing style content context examples human inputs include number clusters choice discard certain clusters based content relevance definition sentence overall approach progress automated extractive summaries audits future work focus automation improving summary consistency work highlights inherent difficulty subjective nature automated summarization real world application keywords extractive summarization unsupervised machine learning text mining clustering single audits introduction text summarization process shortening long pieces text coherent summary containing main points document summarization subfield natural language processing nlp garnered increasing attention recent years rapid growth based data automated summarization methods utilized century generate abstracts domain specific technical papers human written largely sufficient summaries remained applications decades early nlp researchers foresaw continued growth knowledge necessitate efficient consistent summarization scale achieved manually information explosion century stimulated broad use automated summarization technical documents far greater variety texts terms subject format summary defined text conveys important information original longer half original usually significantly abstract paper simple example summary summarization achieved major approaches extractive abstractive extractive summarization selects important sentences phrases original text combines verbatim summary abstractive summarization generates sentences novo abstraction closely resembles human cognition challenging implement necessarily produce clearly superior summaries despite relative simplicity extractive summarization produce highly effective summaries widely utilized focus extractive summarization references summarization refer extractive methods ideas applicable abstractive summarization readily applied method summarization latent dirichlet allocation lda represents corpus collection generative topics words methodologies assumptions lda flexible ability produce informative summaries limited definition topic motivating use alternative strategies certain contexts describe alternative utilizes word embedding clustering bulk work like nlp workflows lies preprocessing methodology present method sentences phrases embedded single multi dimensional mean vector representations grouped cosine similarity finally document corpus sentences phrases nearest centroid group defined important document extractive summarization methodology fundamental endeavor specific summarization strategy vary depending source text desired output available resources variables mechanistic details aside level nearly extractive summarization approaches built fairly universal set principles initial conversion text intermediate numerical representation followed ranking selection important units sentences paragraphs practice workflow break cleanly discretely instance single technique accomplish numerical representation ranking hand given step encompass multiple techniques combination algorithms rank select sentences steps subdivided separate ranking selection small variations notwithstanding virtually summarization predicated major tasks utilized document summarization works typical step numerical representation necessary convert text computer readable format calculate quantitative features downstream algorithms pioneering statistical calculations word phrase frequency sentence position presence key phrases term frequency inverse nowadays older methods remain use sophisticated methods preferred popular approach represent words vectors word embeddings concept word embeddings dates significant breakthroughs arguably occurred introduction glove embeddings mainstays summarization nlp recently novel embedding methods elmo bert xlnet provided innovative alternatives frequency idf following numerical representation sentences ranked basis importance sentences selected data labeled supervised semi supervised machine learning train classifier predicts importance sentence supervised supervised learning achieve desirable results datasets unlabeled largely precluding methods possible hand label data labeled training data needed generally makes approach laborious semi supervised methods require labeled data extractive summarization relies instead unsupervised learning approaches include graph based algorithms popular textrank roots google pagerank algorithm unsupervised deep learning clustering methods means broad category unsupervised methods summarization summary evaluation approaches task summarization problem evaluate quality eventual output warrants careful consideration challenges arise concrete implementation evaluation method broader question quality entails questions include resolve trade sufficient brevity content important factors readability coherence alongside essential elements grammar naturally answers questions subjective highly circumstantial influenced intended purpose summaries variables actual utility summary end user decisive factor information available longer timescale evaluation complicated general requirement significant human input instance duc tac summarization conferences employed human judges grade summaries labor intensive approaches broadly practical alternatively automated evaluation methods implemented packages recall oriented understudy gisting evaluation rouge precursor bleu calculate precision recall scores reflect quality summary rouge similar methods require considerable human effort system generated summary compared human written reference summary calculate required metrics utility rouge somewhat conditional suitable corpus document accompanied pre existing abstract academic journal articles abstract like text introductory blurbs wikipedia articles additional metadata rouge contingent quality reference summary subject questions criteria outlined constitutes summary good quality recognition limitations alternate evaluation techniques developed latent semantic analysis lsa measure similarity system generated summaries original source text ultimately problem evaluation remains headed hydra aspect automatic summarization right technique likely highly context specific iii summarization single audits rich resource federal grant agencies current work focuses summarization corpus single audits capture qualities activities grant recipients detailed audits significant financial economic motivations better understanding grant recipients grants largest categories spending united states government projected award billion grants ensure proper use funds single audits database harvester census gov organizations expend federal grants year assess program compliance analysis recipient federal award financial transactions expenditures internal control systems federal assistance received audit period single audit represents valuable resource decision making risk management grant agencies instance grants managers utilize information single audits decide award grant based applicant past performance grant oversight awarded statements conducted financial perform records manually extracting information single audits nontrivial length audits makes cumbersome human reader locate desired information beneficial identify frame relevant information compressed easily digestible format volume data presents additional barrier study spans comprises documents million pages inevitably grow time automation necessary accomplish corpus wide analyses reasonable timescale ensure consistent unbiased outcomes related work single audits current work builds earlier project elder research inc devised automated method quantify severity audit associated numerical risk score end single audits downloaded form corpus audits pages long hundreds pages pertinent information contained section titled schedule findings questioned costs typically spans pages method devised extract relevant findings pages comprise pages corpus enables analysis focus solely essential data sentiment analysis extracted findings performed generate risk score audit report altogether work makes possible quickly assess grantees straightforward intuitive metric facilitates prioritization grantees interest deemed risky motivation overview current approach previous work elder research offers powerful method answer question high risk raises natural question entities high risk attempt address question extracting text provides explanations choice answer automatic summarization opposed simply manual summaries driven volume data corpus selected audits comprises documents problem exacerbates considers rest audits existence new documents added year year interested smaller subset documents manual summarization reasonable unlikely human group humans maintain consistent quality objectivity documents current approach desired output document summary consisting best illustrate broader context examples sentences desirable extract help elucidate reasoning negative assessment grantee grantee unable provide proper documentation student school missed corrected cause stemmed turnover staff ultimately lapse procedures relevant documents accessed data prepared modeling preparation entailed standard specific text preprocessing steps followed word sentence embeddings pre trained glove vectors sentence vectors entire corpus pooled train means clustering model iteratively optimized summaries document generated extracting important subsequently evaluated human system metrics sentences deemed initial task extract findings sections exclude pages audits reduce noise excluded substantial number entire reports contain findings typically pertained high performing grantees elicit critique reduced length reports abbreviated documents containing findings reference document refer abbreviated documents filtering approach helps simplify problem induce selection bias subsequent analysis evaluated sample performance testing data exploration document initially represented single string tokenized divided split word level needed ensure canonically equivalent strings binary representation nfkd unicode normalization performed text prior preprocessing gain intuition corpus simple univariate analysis document lengths performed string document divided constituent sentences natural language toolkit nltk sentence tokenizer note tokenization stage purposes measuring document length tokenized string eventual preprocessing sentence counts analyzed yielding notable observations mean document length sentences corpus skewed shorter documents mode sentences followed documents sentences shorter documents extra long sentences longest document sentences randomly selected read fixed number documents length bracket short medium long extra long gain sense organization content orthography stylistic conventions observations documents contained brief filler sentences page headers footers state new mexico denver housing center section headings condition cause page numbers documents contained expository statements historical background relevant risk statements predominated longer documents dilute relevant information information concentrated short medium documents shortest documents corresponded grantees satisfactory audits contain sentences findings questioned costs stage removal criteria roman numerals lack universal formatting proved challenging analysis section headings appear cause cause inline separately lists appear parenthetical phrases extra whitespace non alphanumeric regular expression table regular expressions text cleaning text preprocessing performed combination standard specific preprocessing table ensure predictable consistent results standard preprocessing reduces noise associated virtually text data punctuation frequently occurring stopwords specialized nature single audits contained idiosyncrasies diction syntax format structure warranted additional preprocessing modification standard steps manual preprocessing occurred major stages fig stage removed clearly irrelevant sentences preserved orthography generate human readable intermediate text stage produced fully processed sentences embedding ranking selection performed processed sentences corresponding intermediate sentences produce readable summary output initial preprocessing generate readable intermediates outset document represented single string page headers section headings removed based length looked substrings occurred newline breaks removed substrings characters avoid removal dangling lines removed short substrings started uppercase letter non alphabetical character document divided nltk sentence tokenizer isolate section headings sentence contained colon dash split character additional elements removed regular expressions regex table table standard corpus specific preprocessing standard preprocessing stemming lemmatization gram modeling performed convert text lower case remove extra whitespace remove stopwords ntlk list punctuation symbols corpus specific remove page headers short sentences remove roman numerals remove parenthetical phrases negation stopwords fig breakdown preprocessing stages subsequent steps document stored list strings identifiable index intermediate text contained typical english orthography capitalization punctuation stopwords sentences suitable embedding remained easily readable retained later use final touches sanity check entire lower cased text removed punctuation regex table final step stopword removal performed tandem glove embedding section stopword removal accomplished predefined nltk stopword list corpus key modification retain words words add value certain corpora important corpus audits focus lack failure fulfill certain requirement words important determining problems detected preprocessing documents corpus zero length lists ensure preprocessing removing key information original preprocessed text lengths compared rationale preprocessing functioning properly lists originate documents short little substantial content expected documents produced lists sentences shorter originally sentences long average longest documents contained filler text headers results indicated preprocessing sensible lists valid reflections original data artifacts exuberant preprocessing necessary case filtering information ideal automated process induce selection bias remaining documents stayed non preprocessing interestingly corpus showed increased sentence counts likely splitting longer sentences unlikely problematic information merely partitioned lost regardless length increase uncommon corpus showed reduced length following preprocessing cleaned text sentence count original text word sentence embedding glove completion preprocessing stages fully cleaned ready embedding glove method chosen based recognized effectiveness nlp tasks prior success glove corpus ease speed implementation trained vectors decisive factor time constraints natural extension use methods bert custom glove require modifications glove zip file containing vectors trained english language wikipedia gigaword nlp stanford edu projects downloaded obtained individual word tokens word level embedding splitting sentences whitespace non stopword word appropriate word vector retrieved single vector sentence obtained taking arithmetic mean element constituent word vectors vector zeros automatically assigned rare sentences comprised solely stopwords string occurred sentences entire embedding process subsequent means clustering section performed vectors vectors conferred obvious advantage intensive vectors remainder work vii sentence ranking selection means ranking selection vectorized sentences accomplished means clustering chosen intuitive interpretation context algorithm partitions observations sentences clusters topics assigning observation nearest centroid fig centroids clusters iteratively updated minimize intra cluster variation cluster sum squares importance sentence correlates proximity cluster centroid allowing identification sentences cluster topic fig overall fig implementation details hyperparameter tuning train means model entire vectorized sentences pooled stored flattened list vector mapped text representation based unique index list clustering performed scikit learn nltk implementations means selection cosine similarity distance metric key means hyperparameter distance metric assigning centroids clusters euclidean distance common default cosine similarity preferred text metric choice modeling calculations scikit learn means implementation allow ready use metric euclidean distance circumvent trained scikit learn means model vectors normalized unit length based principle squared euclidean distance cosine similarity unit vectors nltk means implementation allows specification distance metric normalization necessary scikit learn model denote squared euclidean distance cosine metric expansion squared euclidean distance yty yty fig simplified visualization means clustering rank select sentences color coded clusters correlate distinct topics sentences selected importance individual topic selection cluster number major hyperparameter representing number clusters topics utilized elbow method means run range sum squared errors model plotted function reduce spatial temporal costs scikit learn mini batch means model run resulting plot indicated optimal values tested empirically running normal scikit learn means model cluster sizes performing best ranking selection sentences means algorithm outputs centroids represent means observations given cluster distance observation respective centroid calculated generated matrix observation centroid distances given document sentences selected cluster fig overall proximity fig processes described algorithmically sentences cluster consider sentences cluster sentences belong cluster cluster rank sentences proximity cluster centroid save index repeat clusters result list sentence indices length sentences overall begin matrix storing proximity sentence document cluster centroid sort sentences distance select sentences result list sentence indices length document length whichever smaller roughly scaled original document length sentences rounding sentences sentences sentences rounding values chosen empirically attempt balance sufficient information brevity obtain summaries roughly length obtained cluster method output document list indices output list lists summaries generated extracting intermediate sentence preprocessing stage corresponding indices preliminary results filtering clusters relevance desired text summary successfully generated document following steps preliminary evaluation reading randomly sampled summaries revealed despite preprocessing summaries contained filler text page headers section headings persistence text despite preprocessing likely inconsistent formatting documents preventing detection filler text established criteria summaries contained extraneous text valid sentences filtered preprocessing nonetheless added little value summaries sentences included limited logistical information lists dates amounts individual financial transactions information value certain contexts greatly improve understanding grantee risk preferable omit sentences eventual summaries reduce extraneous information summaries clusters selectively discarded based content criteria filter clusters cluster topic determined examining sentences closest centroid uniformity content determined examining random sample sentences cluster ultimately clusters produced learn nltk models discarded leaving useful clusters original certain clusters decision straightforward instance scikit learn nltk models produced cluster contained exclusively page headers section headings cluster contained exclusively dates discarded clusters characterized excessive financial detail dollar amounts transactions overly long sentences sentences standard verbiage audits add information specific individual grantees choice discard certain clusters cases straightforward clusters entirely composed irrelevant text universal content clusters far uniform clusters contained predominantly irrelevant information contain non trivial quantity relevant similarly clusters useful contain sentences little value arguably discarding unambiguous clusters risks significant loss information complicated question relevant irrelevant instance decided sentences stating exact dollar amounts transactions excessively detailed argued financial details provide clues grantee risk retained short cluster filtering challenging step rife ambiguities trade offs current manual approach potential improvement thorough analysis inter cluster distances cluster density silhouette scores given major effect cluster filtering summary quality section greater investment examining different clusters methods prioritize information likely significant benefit fig distinct summaries produced different combinations means implementations sentence selection methods steps text output evaluated read throughs graduate level data analyst domain expert audits preliminary evaluation apparent removal irrelevant clusters significantly changed summary content despite concerns information loss summaries appeared greater proportion relevant content change implemented despite potential trade offs mitigate possible loss information cluster method reduction available clusters sentences chosen cluster number sentences selected overall followed criteria sets summaries ultimately generated fig summaries evaluated isolation overall quality relative determine method conferred obvious advantages end medium length documents sentences prior summarization chosen randomly documents evaluated manually rouge metric general summary sets appeared comparable cluster methods performing slightly better fig conspicuous advantage learn fig nltk implementations fig viii human evaluation manual evaluation performed criteria brevity summaries reasonable length high throughput reading selection criteria imposed length limit summaries longer desired instance sentence summary impressive original text sentences occasional long sentences inflated summary lengths unpredictable way information summary contain information understand grantee risk summary successfully omit unnecessary information explicit wording transparent accessible information benchmark questions formal rubric scoring system human evaluation relied largely intuitive sense good poor found sampled summaries varying quality best case summaries fulfilled criteria worst case summaries composed entirely irrelevant information summaries functionally useless matter fulfilled criteria commonly summaries functional optimal broad scenarios mutually exclusive apparent multiple findings single summary contained multiple distinct findings resulting logically confusing text details final section dilution information desired information present hidden sentences desired requiring reader actively filter unnecessary information oblique phrasing ideally desired information stated explicitly grantee compliant action performed practice summaries contained statements recommended grantees perform action compliant possible infer statement information require additional effort reader tenable summaries pertaining scenarios acceptable contain desired information caveat likely yield suboptimal user experience scenario multiple findings final section detail problematic potential missing misleading information significant challenges separating documents findings problem multiple findings resolved work valuable future endeavor rouge evaluation summaries evaluated manually evaluated rouge summary human reference written overlaps system generated summaries corresponding ideal human reference calculated reduce noise calculations mentioned preprocessing steps stopword filtering punctuation removal lower casing performed human system summaries stemming nltk porterstemmer rouge produced key metrics recall human reference summary captured system summary precision system summary actually needed relevant despite popularity general effectiveness rouge method particular application human evaluation considered informative rouge shortcomings rouge attributed general pitfalls method additional issues noted recall precision scores poor compared typically desired making difficult confidently assess summary quality shortcoming set reference summaries available constricted definition ideal reality rarely single ground truth text valid summaries use multiple reference summaries written different people likely improve rouge evaluation finally rouge performed limited documents certain length sentences ill advised extrapolate results significant extent provided potential future directions based current assessment method invaluable solicit expert user feedback ensure method achieves technical soundness desired functionality short term modifications comparatively minor modifications radically change overarching logic approach word embeddings noted alternatives glove methods section difficult precisely predict results method improve output speculate customized embeddings better capture specialized language audits glove vectors currently use trained wikipedia gigaword newswire corpus audit documents differ greatly texts likely words meanings contexts captured pre trained glove vectors fully reflect usage words audits custom vectors lda vector representations alleviate problem tailor cluster number filtering document length built uniformly applied single means model documents corpus certainly optimal corresponding topics decent approximation number topics average length document sentences unlikely appropriate number document nearly sentences cases clusters likely needed capture different topics furthermore current method uses narrow range output lengths broad range raw texts sentence summary desired document sentences barely summary original sentences brief document hundreds thousands sentences future train multiple models explore clustering algorithms including density based hierarchical approaches long term summarize distinct findings noted problematic scenario single multiple distinct findings collapsed summary situation summary human reader seemingly illogical disjointed occurs summaries currently generated document single document multiple distinct findings intellectually findings effectively separate despite originating grant recipient concern independent largely non overlapping causes effects possible consequences condensing multiple findings summary include misleading information example cause finding attributed effect separate finding oftentimes context summary detect happens lack context summaries spread thin multiple findings instance findings require multiple sentences fully explained shortcoming current method relevant sentences selected compared imperfect summary outcomes issue potential particularly noteworthy misinformation summary reads awkwardly considered successful information relevant accurate summary useful information usually clearly identifiable non functional misinformation arise mixed findings easy detect address issues potential future direction extract distinct findings audit document generate single summary single class findings approach added advantage addressing concerns related length outlined realistically extracting findings complex difficult task require investment significant time effort successful likely noticeable positive effect summarization xii conclusion overall approach progress automated extractive summaries audits custom text preprocessing glove embedding means clustering selection heuristics strategy successfully extracted summaries technically sound algorithmic manner large volume federal grant audits future aims include greater automation exploratory steps involved human loop criteria aims include improving consistency results validation technically practical setting work highlights inherent difficulty subjectivity machine learning based automated summarization real world application demonstrates value reducing non automated steps reducing validation subjectivity assessing true sample results expert input order improve output acknowledgment authors like thank robert han ryan mcgibony guidance feedback like thank advisor david van vactor harvard medical school supporting pursuit project separate dissertation work funded elder research internal research development project references luhn automatic creation literature abstracts ibm res dev vol edmundson new methods automatic extracting acm vol baxendale machine index technical literature experiment ibm res dev vol torres moreno automatic text summarization matter automatic text summarization xxiii radev hovy mckeown introduction special issue summarization comput linguist vol hahn mani challenge automatic summarization computer long beach calif november blei jordan latent dirichlet allocation mach learn res vol jones statistical interpretation term specificity application retrieval doc vol collobert weston general deep architecture nlp icml pdf bengio ducharme vincent jauvin neural probabilistic language model mach learn res aug mikolov chen corrado dean distributed representations words phrases compositionality neural inf process syst vol mikolov chen corrado dean efficient estimation word representations vector space pennington socher manning glove global vectors word representation proc conf empir methods nat lang process emnlp vol peters deep contextualized word representations proc conf north american chapter assoc comput linguist human language technologies volume long papers devlin chang lee toutanova bert language training deep bidirectional transformers understanding arxiv prepr oct yang dai yang carbonell salakhutdinov xlnet generalized autoregressive pretraining language understanding arxiv prepr wong extractive summarization supervised semi supervised learning proc int conf comput linguist vol mihalcea tarau textrank bringing order texts proc conf empir methods nat lang process page brin anatomy large scale hypertextual web search engine vol verma nidhi extractive summarization deep learning arxiv prepr aug yousefi azar hamey text summarization unsupervised deep learning expert syst appl vol feb macqueen methods classification analysis multivariate observations proc berkeley symp math stat probab vol miller leveraging bert extractive text summarization lectures arxiv prepr renu kunal review opinion data summarization means clustering latent semantic analysis int res sci technol twinandilla adhy surarso kusumaningrum multi document summarization means latent dirichlet allocation lda significance sentences procedia comput sci vol agrawal gupta extraction based approach text summarization means clustering int sci res publ vol prathima divakar automatic extractive text summarization means clustering int comput sci eng vol jun jain bewoor patil context sensitive text summarization means clustering algorithm int soft comput eng vol lin rouge package automatic evaluation summaries assoc comput linguist papineni roukos ward zhu bleu method automatic evaluation machine translation proc annu meeting assoc comput linguist acl vol steinberger jeek evaluation measures text summarization comput informatics vol
