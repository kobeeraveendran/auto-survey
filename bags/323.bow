unsupervised abstractive dialogue summarization tete tetes xinyuan ruiyi manzil amr inc university research com abstract high quality dialogue summary paired data expensive produce domain sensitive making abstractive dialogue summarization challenging task work propose rst unsupervised abstractive dialogue rization model tete tetes sutat unlike standard text summarization dialogue marization method consider speaker scenario speakers ferent roles goals language styles tete tete customer agent versation sutat aims summarize speaker modeling customer utterances agent utterances separately taining correlations sutat consists conditional generative module supervised summarization modules ditional generative module contains coders decoders variational toencoder framework dependencies latent spaces captured encoders decoders vised summarization modules equipped sentence level self attention mechanisms erate summaries tions experimental results sutat superior unsupervised dialogue rization automatic human tions capable dialogue classication single turn conversation generation introduction tete tetes conversations pants widely studied importance component dialogue analysis instance tetes customers agents contain mation contact centers understand lems customers improve solutions agents time consuming ers track progress going long uninformative utterances matically summarizing tete tete shorter customer agent looking hamilton lodge bridge sure chesterton road postcode customer book people nights ning tuesday reference number agent customer thank tuesday agent assist today customer thank needed agent welcome time customer summary agent summary like book hotel cambridge tuesday booked hotel reference number help table example sutat generated summaries version retaining main points save vast human resources number potential real world applications summarization models categorized classes extractive abstractive extractive methods select sentences phrases input text abstractive methods attempt ate novel expressions requires advanced ability paraphrase condense information spite easier extractive summarization preferred dialogues limited ity capture highly dependent conversation ries produce coherent discourses abstractively summarizing dialogues attracted recent research interest goo chen pan yuan liu existing abstractive dialogue rization approaches fail address main lems dialogue carried tiple speakers different roles goals language styles taking example contact center customers aim propose lems agents aim provide solutions leads different semantic contents choices vocabularies existing methods process dialogue utterances text tion accommodating multi speaker nario second high quality annotated data readily available dialogue summarization main expensive produce topic descriptions instructions commonly gold references general lack information speakers methods use auxiliary information dialogue acts goo chen semantic scaffolds yuan key point sequences liu help summarization adding burden data annotation edge previous work focused vised deep learning abstractive dialogue marization propose sutat unsupervised abstractive dialogue summarization approach specically tete tetes paper use example agent customer represent speakers tete tetes better understanding addition summarization sutat alogue classication single turn conversation generation accommodate speaker scenario tat processes utterances customer agent separately conditional generative module inspired zhang latent spaces contained variational autoencoder vae framework conditional generative ule includes encoders map customer terance corresponding agent utterance latent representations decoders construct utterances jointly separate encoders decoders enables sutat model ences language styles vocabularies customer utterances agent utterances pendencies latent spaces captured making agent latent variable conditioned customer latent variable compared standard autoencoders learn tic representations input utterances vae based conditional generative module learn variational distributions gives model pressive capacity exibility correlation latent spaces encoders decoders ditional generative module pervised summarization modules generate tomer summaries agent summaries divergent meansum chu liu combined multi document representation simply computed averaging encoded input texts sutat employs setence level self attention anism vaswani highlight nicant utterances neglect uninformative ones incorporate copying factual details source text proven useful supervised summarization dialogue maries usually written person point view sutat simplies problem ing summaries consistent utterances pronouns table shows example sutat generated summaries experiments conducted dialogue datasets multiwoz budzianowski taskmaster byrne assumed access utterances datasets annotations including dialogue acts descriptions instructions automatic human evaluations sutat outperforms unsupervised baseline methods dialogue marization capability tat dialogue classication generated maries single turn conversation generation methodology sutat consists conditional generative ule unsupervised summarization modules let denote set customer utterances denote set agent utterances dialogue aim generate customer summary agent summary utterances figure shows entire architecture sutat given customer utterance consecutive agent utterance conditional generative ule embeds encoders obtain latent variables variational tent spaces reconstruct utterances decoders latent space agent latent variable conditioned customer latent variable decoding generated tomer utterances conditioned generated agent utterances design resembles tete carries agent responses customer requests dependent encoded utterances dialogue inputs unsupervised summarization modules employ sentence level self attention mechanism figure block diagram sutat architectures connected blue dashed line red arrow represents conditional relationship latent spaces utterances embeddings highlight informative ones combine weighted dings summary representation drawn low variance latent space combined utterance embedding decoded summary decoder partial copy mechanism process require annotations data conditional generative module build conditional generative module sivae based framework zhang capture dependencies latent spaces goal module train coders decoders customer utterances agent utterances maximizing evidence lower bound lgen log log log variational posterior distribution approximates true posterior distribution lower bound includes reconstruction losses kullback leibler divergences tween priors variational posteriors assuming priors posteriors gaussian apply reparameterization trick kingma welling compute divergences closed forms represent customer encoder agent coder customer decoder agent decoder correlation latent spaces captured making agent latent variable conditioned customer latent variable dene customer prior standard gaussian agent prior gaussian mean variance functions process resembles tete tete contact centers carries response agent conditioned customer says encoding given customer utterance sequence rst encode terance embedding bidirectional lstm graves transformer encoder vaswani lstm takes hidden states contextual representations ing sequence directions transformer encoder produces contextual representations dimensions word embeddings customer encoderagent encodercustomerdecoderagentdecodercustomer latent spaceagent latent spacecustomerutterances xagentutterances ycustomerutterancesagentutterancescustomer decoder sentence levelself attentionpartialcopysxzxzysummaryrepresentationcustomerlatent sentence levelself attentionpartialcopysysummaryrepresentationagentlatent generative moduleunsupervised summarization moduleunsupervised summarization moduleencoded customer utterancesencoded agent utterances customer utterance embedding obtained averaging contextual representations similarly obtain agent utterance bedding customer latent variable rst pled agent latent variable sampled gaussian parameters puted separate linear projections lineary lineary decoding rst decode agent terance lstm sutskever transformer decoder vaswani decoded sequence latent variable generate customer utterance lstm decoder transformer decoder embeddings viously decoded sequence decoded tations feedforward layers compute vocabulary distributions learnable parameters vocabulary sizes customer utterances agent utterances unsupervised summarization module given encoded utterances dialogue unsupervised summarization module learns erate summary semantically similar input utterances trained components conditional generative module sentence level self attention utterances like greetings small talk contribute content dialogue employ sentence level self attention mechanism built multi head attention vaswani highlight signicant utterances dialogue multi head attention partitions queries keys values heads dimensions calculates scaled dot product attention linear projections heads headi kwk vwv trainable rameters scaled dot product attention outputs weighted sum values softmax qkt sutat sentence level self attention achieved making queries keys values set encoded agent customer utterances dialogue self attention module assigns weights input utterances nicant informative ones higher weights output weighted combined utterance bedding highlights informative utterances dialogue summary generation summary tions sampled latent spaces taking weighted combined utterance tations inputs limit novelty generated summary set variances latent spaces close zero containing key information dialogue decoded customer summary agent summary decoders conditional generative module makes generated summaries similar utterances pronouns language styles encode generated summaries encoders compare utterance embeddings age cosine distance constrain summaries semantically close input utterances summarization modules trained maximizing similarity loss lsum denotes cosine distance summarization modules prone produce inaccurate factual details design simple effective partial copy mechanism employs extractive summarization tricks address problem automatically list factual information data dates locations names numbers decoder predicts word factual mation list copy mechanism replaces word containing factual information input utterances multiple factual tion words dialogue highest predictive possibility chosen note partial copy mechanism need trained activated training training process objective function optimize weighted sum reconstruction loss equation similarity loss equation lgen controls weights objectives sutat involves encoding generated agent utterance help generating customer terance equation encoding ated summary compare utterance dings equation directly sampling multinomial distribution argmax differentiable operation use soft argmax trick chen approximate ministic sampling scheme annealing parameter adam kingma adopted stochastic optimization jointly train model parameters maximizing equation step adam samples mini batch dialogues updates parameters zhang related works cardie abstractive dialogue tion recently explored success sequence sequence neural networks pan propose enhanced interaction dialogue encoder transformer pointer decoder marize dialogues summarize modal meetings encoder decoder ture approaches design additional nisms neural summarization model leverage auxiliary information dialogue acts goo chen key point sequences liu semantic scaffolds yuan supervised methods use concise topic descriptions instructions gold references high quality annotated dialogue summaries readily available unsupervised summarization extractive summarization models require summary paired data instead tackle sentence selection problem textrank cea tarau lexrank erkan radev encode sentences nodes graph select representative ones mary zheng lapata rossiello advance textrank lexrank bert devlin compute tence similarity replacing idf weights embeddings respectively abstractive summarization approaches focus ing unsupervised sentence compression scale texts fevry phang baziotis west ted yang proposes transformer based architecture pretraining large scale data meansum chu liu generates multi document summary decoding average encoding input texts autoencoder rization module interactive brainskas amplayo lapata extend meansum hierarchical variational toencoder denoising noised synthetic dataset methods accommodate multi speaker scenario dialogues experimental details dialogue summarization early dialogue marization works mainly focus extractively marizing statistical machine learning ods galley xie wang perform experiments variants tat equipped lstm encoders coders sutat lstm equipped transformer encoders decoders sutat tran model lexrank meansum copycat vae sutat lstm sutat tran multiwoz taskmaster customer customer ablation study lstm encoders decoders sutat sutat att sutat copy agent agent table rouge scores multiwoz taskmaster test sets dataset experiments conducted dialogue datasets budzianowski byrne tiwoz consists goal oriented human written dialogues customers agents spanning domains booking hotels booking taxis label multi label periment split dataset dialogues training testing tion taskmaster consists goal oriented dialogues including spoken written dialogues work use written logues created human workers based scenarios outlined tasks ordering pizza ordering movie tickets dataset split dialogues training testing validation baselines validate effectiveness sutat compare variants following baselines unsupervised extractive summarization methods lexrank erkan radev rossiello unsupervised abstractive summarization methods meansum chu liu copycat brainskas dition train vanilla text vae model bowman unsupervised summarization module baseline rst work summarizes speaker dialogue modications need baselines fair parisons model vised summarization baseline models adapt speaker scenario tete tetes train models baseline customer terances agent utterances testing customer summaries agent summaries erated trained models baseline separately automatic human evaluation concatenated classication experiment settings tune parameters sutat tion set vae based text generative models fer posterior collapse model learns ignore latent variable bowman employ term annealing dropping words decoding avoid posterior collapse annealing initial weights terms gradually increase weights training progresses reach threshold rate increase set respect total number batches word dropout rate decoding latent variable size customer agent latent variables controls weights objective functions equation set word embedding size rectional lstm encoder lstm decoder number hidden layers hidden unit size transformer encoder coder number hidden layers number heads multi head attention set number heads sentence level self attention hidden unit size mlps annealing parameter soft argmax equation set ing training learning rate batch size maximum number epoch sutat implemented pytorch trained nvidia tesla gpu reference summaries work dene dialogue summary summarizing speaker dialogue annotated dataset available validate effectiveness sutat compare baselines follow setting chu liu collect abstractive summaries subset dataset workers sented dialogues multiwoz dialogues taskmaster asked write maries best summarize content sentiment speaker asked ers write summaries speaker want book hotel instead customer wants book hotel length summary tence collected summaries reference summaries testing model tuning reference summaries cover domains datasets released later results conduct majority experiments superiority sutat unsupervised dialogue summarization use labeled reference maries rouge score based automatic tion human evaluation compare line methods demonstrate ness sutat analyzing language modeling results generated summaries perform dialogue classication addition sutat capable single turn conversation ation unsupervised dialogue summarization automatic evaluation rouge lin standard summarization metric measure face word alignment generated summary reference summary experiments use rouge sure word overlap bigram overlap longest common sequence respectively table shows rouge scores sutat variants baselines proposed sutat lstm encoders decoders outperforms baselines datasets sutat lstm performs better sutat transformer rouge scores reason transformer decoders strong encoders weakened training general unsupervised abstractive models perform better unsupervised tive models compared unsupervised abstractive summarization baselines equipped lstm encoders decoders sutat lstm big performance improvement believe sutat accommodates speaker nario tete tetes utterances speaker correlations better modeled addition evaluate reconstruction mances language modeling based methods perplexity ppl check posterior lapse vae based methods gence results multiwoz taskmaster shown table seen sutat tran better ppl scores ing methods datasets showing decoders effective reconstructing tences consequently powerful coders sutat tran smaller divergences lead posterior collapse coders tend ignored human evaluation human evaluation generated summaries conducted quantify qualitative results model sample alogues labeled reference summaries multiwoz taskmaster test set sampled dialogues summaries generated unsupervised abstractive approaches meansum copycat vae lstm sutat tran recruit workers rank generated summaries reference maries best worst based criteria informativeness summary present main points dialogue concise version readability summary matically correct structured correlation customer summary correlated agent summary dialogue average ranking scores shown ble sutat lstm achieves best informativeness correlation results datasets sutat tran good mances demonstrating ability sutat generating informative coherent dialogue summaries general sutat models better human evaluation scores baseline model multiwoz agent customer ppl taskmaster agent customer ppl meansum copycat vae sutat lstm sutat tran ppl ppl table language modeling results multiwoz taskmaster lower better ppl multiwoz read taskmaster read corr model reference info meansum copycat vae sutat lstm sutat tran corr info table human evaluation results informativeness readability correlation generated summaries model multiwoz taskmaster meansum copycat vae sutat unsupervised sutat supervised table auc scores domain classcation erated summaries multiwoz multi label taskmaster single label els especially correlation scores sults close reference summaries cause sutat exploits dependencies customer latent space agent latent space results generating correlated tomer summaries agent summaries ablation study perform ablations date component sutat removing variational latent spaces sutat encoded utterances directly ding removing sentence level self attention mechanism sutat att removing tial copy mechanism sutat copy use lstm encoders decoders ablation els results ablation study table removed components play role tat removing latent spaces biggest inuence summarization performance cating variational latent space necessary support design makes agent latent variable dependent customer latent variable performance drop removing level self attention mechanism shows weighted combined utterance embedding better simply taking mean encoded utterances removing partial copy smallest quality drop taking dialogue example ble partial copy mechanism sutat generate following summaries customer summary book hotel cambridge tuesday like agent summary booked hotel reference number lzludtvi help generated summaries wrong reference number crucial mation summary classication summaries good dialogue summary reect key points utterances perform dialogue sication based dialogue domains test lidity generated summaries encode generated customer summary agent summary trained encoders model concatenated features dialogue classication way logue features obtained unsupervisedly train separate linear classier encoded summaries use sutat lstm encoders decoders task shown table sutat outperforms baselines dialogue classication indicating sutat erated summaries better comprehension domain information dialogue perform supervised classication sutat features train linear classier cross entropy loss combined equation new objective customer yes yes multiple sports places visit sorry locations center town like different area customer yes book group people thursday booking successful ence number minorhoq agent agent customer looking place stay west cheap need internet hotels moderate price range care expand criteria agent table examples single turn conversations ated conditional generative module sutat function parameters jointly optimized seen table supervised cation results high multiwoz taskmaster demonstrating effectiveness sutat single turn conversation generation design conditional generative module sutat enables generating novel single turn sations sampling customer latent variable standard gaussian sampling agent latent variable sutat produce realistic looking novel dialogue pairs customer decoder agent decoder table shows examples novel single turn conversations generated sutat randomly sampled latent variables alogue pairs closely correlated meaning dependencies latent spaces cessfully captured conclusion propose sutat unsupervised abstractive dialogue summarization model accommodating speaker scenario tete tetes marizing data annotations conditional generative module models tomer utterances agent utterances separately encoders decoders taining correlations variational latent spaces unsupervised summarization module sentence level self attention mechanism highlight informative utterances mary representations containing key information dialogue decoded decoders conditional generative module help partial copy mechanism generate customer summary agent summary experimental results superiority sutat unsupervised dialogue summarization capability dialogue tasks references reinald kim amplayo mirella lapata supervised opinion summarization noising proceedings annual meeting denoising association computational linguistics acl christos baziotis ion androutsopoulos ioannis stas alexandros potamianos seq differentiable sequence sequence sequence autoencoder unsupervised abstractive sentence compression proceedings conference north american chapter association computational linguistics naacl samuel bowman luke vilnis oriol vinyals drew dai rafal jozefowicz samy gio generating sentences continuous space proceedings conference tational natural language learning conll arthur brainskas mirella lapata ivan titov unsupervised opinion summarization proceedings copycat review generation annual meeting association tional linguistics acl pawe budzianowski tsung hsien wen hsiang tseng inigo casanueva stefan ultes osman madan milica gaic multiwoz scale multi domain wizard dataset oriented dialogue modelling proceedings conference empirical methods natural guage processing emnlp bill byrne karthik krishnamoorthi chinnadhurai sankar arvind neelakantan daniel duckworth semih yavuz ben goodrich amit dubey andy cedilnik kyu young kim realistic diverse dialog dataset proceedings conference empirical methods natural language processing emnlp liqun chen yizhe zhang ruiyi zhang chenyang tao zhe gan haichao zhang bai dinghan shen changyou chen lawrence carin improving sequence sequence learning proceedings international mal transport conference learning representations iclr eric chu peter liu meansum neural model unsupervised multi document abstractive summarization proceedings international conference machine learning icml jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language ing proceedings conference north american chapter association tional linguistics naacl gnes erkan dragomir radev lexrank graph based lexical centrality salience text summarization journal articial intelligence search thibault fevry jason phang vised sentence compression denoising encoders proceedings conference putational natural language learning conll michel galley skip chain conditional dom eld ranking meeting utterances proceedings conference tance pirical methods natural language processing emnlp chih wen goo yun nung chen tive dialogue summarization sentence gated modeling optimized dialogue acts ieee ken language technology workshop slt alex graves navdeep jaitly abdel rahman hamed hybrid speech recognition deep bidirectional lstm ieee workshop automatic speech recognition understanding diederik kingma jimmy adam method stochastic optimization proceedings international conference learning sentations iclr gaetano rossiello pierpaolo basile giovanni meraro centroid based text summarization compositionality word embeddings proceedings multiling workshop marization summary evaluation source types genres abigail peter liu christopher manning point summarization proceedings annual generator networks meeting association computational guistics acl ilya sutskever oriol vinyals quoc sequence sequence learning neural networks advances neural information processing tems neurips ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention advances neural information need cessing systems neurips wang claire cardie independent abstract generation focused meeting summarization proceedings annual ing association computational linguistics acl peter west ari holtzman jan buys yejin choi bottlesum unsupervised self supervised sentence summarization information tleneck principle proceedings conference empirical methods natural language ing emnlp diederik kingma max welling proceedings encoding variational bayes international conference learning tions iclr shasha xie yang liu hui lin evaluating effectiveness features sampling tive meeting summarization ieee spoken language technology workshop slt manling lingyu zhang heng richard radke meeting summaries topic abstractive multi modal meeting summarization proceedings annual meeting tion computational linguistics acl ziyi yang chenguang zhu robert gmyr michael zeng xuedong huang eric darve ted pretrained unsupervised summarization model theme modeling denoising arxiv preprint chin yew lin rouge package automatic evaluation summaries acl workshop text summarization branches lin yuan zhou abstractive dialog marization semantic scaffolds arxiv preprint chunyi liu peng wang jiang zang jieping automatic dialogue summary generation customer service acm sigkdd international conference knowledge discovery data mining rada mihalcea paul tarau textrank ing order text proceedings conference empirical methods natural language ing emnlp haojie pan junpei zhou zhou zhao yan liu deng cai min yang end end dialogue description generation arxiv preprint xinyuan zhang yitong dinghan shen lawrence carin diffusion maps textual advances neural network embedding mation processing systems neurips xinyuan zhang yang siyang yuan dinghan shen lawrence carin syntax infused tional autoencoder text generation ings annual meeting association computational linguistics acl hao zheng mirella lapata sentence trality revisited unsupervised summarization proceedings annual meeting tion computational linguistics acl
