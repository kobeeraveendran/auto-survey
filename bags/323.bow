unsupervised abstractive dialogue summarization for tete a tetes xinyuan ruiyi manzil amr inc university research com p e s l c s c v v i x r a abstract high quality dialogue summary paired data is expensive to produce and domain sensitive making abstractive dialogue summarization a challenging task in this work we propose the rst unsupervised abstractive dialogue rization model for tete a tetes sutat unlike standard text summarization a dialogue marization method should consider the speaker scenario where the speakers have ferent roles goals and language styles in a tete a tete such as a customer agent versation sutat aims to summarize for each speaker by modeling the customer utterances and the agent utterances separately while taining their correlations sutat consists of a conditional generative module and two supervised summarization modules the ditional generative module contains two coders and two decoders in a variational toencoder framework where the dependencies between two latent spaces are captured with the same encoders and decoders two vised summarization modules equipped with sentence level self attention mechanisms erate summaries without using any tions experimental results show that sutat is superior on unsupervised dialogue rization for both automatic and human tions and is capable of dialogue classication and single turn conversation generation introduction tete a tetes conversations between two pants have been widely studied as an importance component of dialogue analysis for instance a tetes between customers and agents contain mation for contact centers to understand the lems of customers and improve the solutions by agents however it is time consuming for ers to track the progress by going through long and sometimes uninformative utterances matically summarizing a tete a tete into a shorter customer agent i am looking for the hamilton lodge in bridge sure it is at chesterton road postcode customer please book it for people nights ning on tuesday done your reference number is agent customer thank you i will be there on tuesday agent is there anything more i can assist you with today customer thank you that s everything i needed agent you are welcome any time customer summary agent summary i would like to book a hotel in cambridge on tuesday i have booked you a hotel the reference number is can i help you with anything else table an example of sutat generated summaries version while retaining its main points can save a vast amount of human resources and has a number of potential real world applications summarization models can be categorized into two classes extractive and abstractive extractive methods select sentences or phrases from the input text while abstractive methods attempt to ate novel expressions which requires an advanced ability to paraphrase and condense information spite being easier extractive summarization is often not preferred in dialogues for its limited ity to capture highly dependent conversation ries and produce coherent discourses therefore abstractively summarizing dialogues has attracted recent research interest goo and chen pan et al yuan and yu liu et al however existing abstractive dialogue rization approaches fail to address two main lems first a dialogue is carried out between tiple speakers and each of them has different roles goals and language styles taking the example of a contact center customers aim to propose lems while agents aim to provide solutions which leads them to have different semantic contents and choices of vocabularies most existing methods process dialogue utterances as in text tion without accommodating the multi speaker nario second high quality annotated data is not readily available in the dialogue summarization main and can be very expensive to produce topic descriptions or instructions are commonly used as gold references which are too general and lack any information about the speakers moreover some methods use auxiliary information such as dialogue acts goo and chen semantic scaffolds yuan and yu and key point sequences liu et al to help with summarization adding more burden on data annotation to our edge no previous work has focused on vised deep learning for abstractive dialogue marization we propose sutat an unsupervised abstractive dialogue summarization approach specically for tete a tetes in this paper we use the example of agent and customer to represent the two speakers in tete a tetes for better understanding in addition to summarization sutat can also be used for alogue classication and single turn conversation generation to accommodate the two speaker scenario tat processes the utterances of a customer and an agent separately in a conditional generative module inspired by zhang et al where two latent spaces are contained in one variational autoencoder vae framework the conditional generative ule includes two encoders to map a customer terance and the corresponding agent utterance into two latent representations and two decoders to construct the utterances jointly separate encoders and decoders enables sutat to model the ences of language styles and vocabularies between customer utterances and agent utterances the pendencies between two latent spaces are captured by making the agent latent variable conditioned on the customer latent variable compared to using two standard autoencoders that learn tic representations for input utterances using the vae based conditional generative module to learn variational distributions gives the model more pressive capacity and more exibility to nd the correlation between two latent spaces the same encoders and decoders from the ditional generative module are used in two pervised summarization modules to generate tomer summaries and agent summaries divergent from meansum chu and liu where the combined multi document representation is simply computed by averaging the encoded input texts sutat employs a setence level self attention anism vaswani et al to highlight more nicant utterances and neglect uninformative ones we also incorporate copying factual details from the source text that has proven useful in supervised summarization see et al dialogue maries are usually written in the third person point of view but sutat simplies this problem by ing the summaries consistent with the utterances in pronouns table shows an example of sutat generated summaries experiments are conducted on two dialogue datasets multiwoz budzianowski et al and taskmaster byrne et al it is assumed that we can only access utterances in the datasets without any annotations including dialogue acts descriptions instructions both automatic and human evaluations show sutat outperforms other unsupervised baseline methods on dialogue marization we further show the capability of tat on dialogue classication with generated maries and single turn conversation generation methodology sutat consists of a conditional generative ule and two unsupervised summarization modules let x xn denote a set of customer utterances and y denote a set of agent utterances in the same dialogue our aim is to generate a customer summary and an agent summary for the utterances in x and y figure shows the entire architecture of sutat given a customer utterance and its consecutive agent utterance y the conditional generative ule embeds them with two encoders and obtain latent variables zx and zy from the variational tent spaces then reconstruct the utterances from zx and zy with two decoders in the latent space the agent latent variable is conditioned on the customer latent variable during decoding the generated tomer utterances are conditioned on the generated agent utterances this design resembles how a a tete carries out the agent s responses and the customer s requests are dependent on each other the encoded utterances of a dialogue are the inputs of the unsupervised summarization modules we employ a sentence level self attention mechanism figure block diagram of sutat architectures connected by a blue dashed line are the same the red arrow represents the conditional relationship between two latent spaces on the utterances embeddings to highlight the more informative ones and combine the weighted dings a summary representation is drawn from the low variance latent space using the combined utterance embedding which is then decoded into a summary with the same decoder and a partial copy mechanism the whole process does not require any annotations from the data conditional generative module we build the conditional generative module in a sivae based framework zhang et al to capture the dependencies between two latent spaces the goal of the module is to train two coders and two decoders for customer utterances and agent utterances y by maximizing the evidence lower bound lgen log zx log log y where q is the variational posterior distribution that approximates the true posterior distribution the lower bound includes two reconstruction losses and two kullback leibler kl divergences tween the priors and the variational posteriors by assuming priors and posteriors to be gaussian we can apply the reparameterization trick kingma and welling to compute the kl divergences in closed forms zx zx and represent customer encoder agent coder customer decoder and agent decoder the correlation between two latent spaces are captured by making the agent latent variable zy conditioned on the customer latent variable zx we dene the customer prior to be a standard gaussian n i the agent prior is also a gaussian n where the mean and the variance are functions of zx this process resembles how a tete a tete at contact centers carries out the response of an agent is conditioned on what the customer says encoding given a customer utterance sequence wt we rst encode it into an terance embedding ex using bidirectional lstm graves et al or a transformer encoder vaswani et al the bi lstm takes the hidden states hi h i as contextual representations by h i ing a sequence from both directions h h the transformer encoder produces the contextual representations that have the same dimensions as word embeddings wt wt customer encoderagent encodercustomerdecoderagentdecodercustomer latent spaceagent latent spacecustomerutterances xagentutterances ycustomerutterancesagentutterancescustomer decoder sentence levelself attentionpartialcopysxzxzysummaryrepresentationcustomerlatent sentence levelself attentionpartialcopysysummaryrepresentationagentlatent generative moduleunsupervised summarization moduleunsupervised summarization moduleencoded customer utterancesencoded agent utterances the customer utterance embedding ex is obtained by averaging over the contextual representations similarly we can obtain the agent utterance bedding ey the customer latent variable zx is rst pled from n x x using ex then the agent latent variable zy is sampled from zx n y y using ey and the gaussian parameters x x y and y are puted with separate linear projections x y lineary zx x y lineary ey zx decoding we rst decode zy into the agent terance from the using lstm sutskever et al or a transformer decoder vaswani et al the decoded sequence and the latent variable zx are then used in zx to generate the customer utterance in the lstm decoder y zy zx y y while in the transformer decoder i zy i where y i and i are the embeddings of the viously decoded sequence the decoded tations are put in feedforward layers to compute the vocabulary distributions y and y wt i zy i zx y y by wt bx where wx wy rl and by rl are learnable parameters and are the vocabulary sizes for customer utterances and agent utterances unsupervised summarization module given the encoded utterances of a dialogue an unsupervised summarization module learns to erate a summary that is semantically similar to the input utterances using trained components from the conditional generative module sentence level self attention some utterances like greetings or small talk do not contribute to the content of a dialogue therefore we employ a sentence level self attention mechanism which is built upon multi head attention vaswani et al to highlight the most signicant utterances in a dialogue the multi head attention partitions the queries q keys k and values v into h heads along their dimensions d and calculates h scaled dot product attention for the linear projections of the heads k v headi i kwk i vwv where wo wq wk and wv are trainable rameters the scaled dot product attention outputs a weighted sum of values k v softmax v qkt in sutat the sentence level self attention is achieved by making the queries keys and values all be the set of encoded agent customer utterances of a dialogue the self attention module assigns weights on the input utterances such that more nicant and informative ones have higher weights the output is a weighted combined utterance bedding ex or ey that highlights more informative utterances from the dialogue summary generation summary tions and sy are sampled from the latent spaces taking the weighted combined utterance tations ex and ey as inputs to limit the amount of novelty in the generated summary we set the variances of the latent spaces close to zero so that x and sy y and sy containing key information from the dialogue are decoded into a customer summary and an agent summary using the same decoders from the conditional generative module which makes the generated summaries similar to the utterances in pronouns and language styles we re encode the generated summaries into ex and ey with the same encoders and compare them with each of the utterance embeddings using age cosine distance to constrain the summaries to be semantically close to input utterances the summarization modules are trained by maximizing a similarity loss n n lsum y where d denotes the cosine distance however the summarization modules are prone to produce inaccurate factual details we design a simple but effective partial copy mechanism that employs some extractive summarization tricks to address this problem we automatically make a list of factual information from the data such as dates locations names and numbers whenever the decoder predicts a word from the factual mation list the copy mechanism replaces it with a word containing factual information from the input utterances if there are multiple factual tion words in the dialogue the one with the highest predictive possibility will be chosen note that this partial copy mechanism does not need to be trained and is not activated during training training process the objective function we optimize is the weighted sum of the reconstruction loss in equation and the similarity loss in equation l lgen where controls the weights of two objectives sutat involves re encoding the generated agent utterance to help with generating the customer terance in equation and re encoding the ated summary to compare with utterance dings in equation directly sampling from the multinomial distribution with argmax is a differentiable operation so we use the soft argmax trick chen et al to approximate the ministic sampling scheme yi y where is the annealing parameter adam kingma and ba is adopted for stochastic optimization to jointly train all model parameters by maximizing equation in each step adam samples a mini batch of dialogues and then updates the parameters zhang et al related works cardie abstractive dialogue tion has been recently explored due to the success of sequence to sequence neural networks pan et al propose an enhanced interaction dialogue encoder and a transformer pointer decoder to marize dialogues li et al summarize modal meetings on another encoder decoder ture some approaches design additional nisms in a neural summarization model to leverage auxiliary information such as dialogue acts goo and chen key point sequences liu et al and semantic scaffolds yuan and yu however these supervised methods can only use concise topic descriptions or instructions as gold references while high quality annotated dialogue summaries are not readily available unsupervised summarization many extractive summarization models do not require summary paired data and instead they tackle a sentence selection problem textrank cea and tarau and lexrank erkan and radev encode sentences as nodes in a graph to select the most representative ones as a mary zheng and lapata and rossiello et al advance upon textrank and lexrank by using bert devlin et al to compute tence similarity and replacing tf idf weights with embeddings respectively in abstractive summarization some approaches focus on ing unsupervised sentence compression with scale texts fevry and phang baziotis et al west et al while ted yang et al proposes a transformer based architecture with pretraining on large scale data meansum chu and liu generates a multi document summary by decoding the average encoding of the input texts where the autoencoder and the rization module are interactive brainskas et al and amplayo and lapata extend meansum by using a hierarchical variational toencoder and denoising a noised synthetic dataset however none of these methods accommodate the multi speaker scenario in dialogues experimental details dialogue summarization early dialogue marization works mainly focus on extractively marizing using statistical machine learning ods galley xie et al wang and we perform experiments with two variants of tat one equipped with lstm encoders and coders sutat lstm and the other equipped with transformer encoders and decoders sutat tran model lexrank meansum copycat vae sutat lstm sutat tran multiwoz taskmaster customer r l r l r l customer ablation study with lstm encoders and decoders sutat ls sutat att sutat copy agent agent r l table rouge scores on the multiwoz and taskmaster test sets dataset the experiments are conducted on two dialogue datasets budzianowski et al and byrne et al tiwoz consists of goal oriented human written dialogues between customers and agents spanning over domains such as booking hotels booking taxis of them are label and of them are multi label in the periment we split the dataset into and dialogues for training testing and tion taskmaster consists of goal oriented dialogues including spoken and written dialogues in this work we only use the written logues which is created by human workers based on scenarios outlined for one of the six tasks such as ordering pizza ordering movie tickets the dataset is split into and dialogues for training testing and validation baselines to validate the effectiveness of sutat we compare the two variants against the following baselines unsupervised extractive summarization methods lexrank erkan and radev and rossiello et al unsupervised abstractive summarization methods meansum chu and liu and copycat brainskas et al in dition we train a vanilla text vae model bowman et al with our unsupervised summarization module as another baseline since we are the rst work that summarizes for each speaker in a dialogue some modications need to be made on baselines to make fair parisons with our model to make the vised summarization baseline models adapt to the two speaker scenario in tete a tetes we train two models for each baseline with either customer terances or agent utterances during testing the customer summaries and agent summaries are erated by the two trained models of each baseline which are used either separately for automatic and human evaluation or concatenated together for the classication experiment settings we ne tune the parameters of sutat on the tion set vae based text generative models can fer from posterior collapse where the model learns to ignore the latent variable bowman et al we employ kl term annealing and dropping out words during decoding to avoid posterior collapse for kl annealing the initial weights of the kl terms are and then we gradually increase the weights as training progresses until they reach the kl threshold of the rate of this increase is set to with respect to the total number of batches the word dropout rate during decoding is the latent variable size is for both customer and agent latent variables that controls weights of two objective functions in equation is set to the word embedding size is for the rectional lstm encoder and lstm decoder the number of hidden layers is and the hidden unit size is for the transformer encoder and coder the number of hidden layers is and the number of heads in the multi head attention is set to the number of heads in the sentence level self attention is also the hidden unit size of the mlps in is the annealing parameter for soft argmax in equation is set to ing training the learning rate is the batch size is and the maximum number of epoch is sutat is implemented in pytorch and trained using a nvidia tesla gpu with gb reference summaries in this work we dene the dialogue summary as summarizing for each speaker in a dialogue and there is no such annotated dataset available to validate the effectiveness of sutat and compare with baselines we follow the setting in chu and liu to collect abstractive summaries for a subset of each dataset workers were sented with dialogues from multiwoz and dialogues from taskmaster and asked to write maries that best summarize both the content and the sentiment for each speaker we asked ers to write your summaries as if your were the speaker e i want to book a hotel instead of the customer wants to book a hotel and keep the length of the summary no more than one tence the collected summaries are only used as reference summaries for testing and not used for model tuning these reference summaries cover all domains in both datasets and will be released later results we conduct the majority of experiments to show the superiority of sutat on unsupervised dialogue summarization we use the labeled reference maries for rouge score based automatic tion and human evaluation to compare with line methods we further demonstrate the ness of sutat by analyzing the language modeling results and using generated summaries to perform dialogue classication in addition we show that sutat is capable of single turn conversation ation unsupervised dialogue summarization automatic evaluation rouge lin is a standard summarization metric to measure the face word alignment between a generated summary and the reference summary in the experiments we use and rouge l to sure the word overlap bigram overlap and longest common sequence respectively table shows the rouge scores for two sutat variants and the baselines as we can see our proposed sutat with lstm encoders and decoders outperforms all other baselines on both datasets sutat lstm performs better than sutat transformer on rouge scores the reason could be that transformer decoders are too strong so the encoders are weakened during training in general the unsupervised abstractive models perform better than unsupervised tive models compared with other unsupervised abstractive summarization baselines equipped with lstm encoders and decoders sutat lstm has a big performance improvement we believe this is because sutat accommodates the two speaker nario in tete a tetes so that the utterances from each speaker and their correlations are better modeled in addition we evaluate reconstruction mances of the language modeling based methods with perplexity ppl and check the posterior lapse for the vae based methods with kl gence the results for multiwoz and taskmaster are shown in table as can be seen sutat tran has much better ppl scores than other ing methods on both datasets showing the former decoders are effective at reconstructing tences consequently due to the powerful coders sutat tran has smaller kl divergences which can lead to posterior collapse where the coders tend to be ignored human evaluation human evaluation for the generated summaries is conducted to quantify the qualitative results of each model we sample alogues that are labeled with reference summaries from the multiwoz and taskmaster test set each with the sampled dialogues summaries are generated from the unsupervised abstractive approaches meansum copycat vae lstm and sutat tran we recruit three workers to rank the generated summaries and reference maries from the best to the worst based on three criteria informativeness a summary should present the main points of the dialogue in a concise version readability a summary should be matically correct and well structured correlation the customer summary should be correlated to the agent summary in the same dialogue the average ranking scores are shown in ble as we can see sutat lstm achieves the best informativeness and correlation results on both datasets while sutat tran also has good mances further demonstrating the ability of sutat on generating informative and coherent dialogue summaries in general the two sutat models have better human evaluation scores than baseline model multiwoz agent customer kl ppl taskmaster agent customer kl ppl meansum copycat vae sutat lstm sutat tran ppl kl ppl kl table language modeling results on multiwoz and taskmaster lower is better for ppl multiwoz read taskmaster read corr model reference info meansum copycat vae sutat lstm sutat tran corr info table human evaluation results on informativeness readability and correlation of generated summaries model multiwoz taskmaster meansum copycat vae sutat unsupervised sutat supervised table auc scores for domain classcation with erated summaries where multiwoz is multi label and taskmaster is single label els especially on correlation scores where the sults are close to reference summaries this is cause sutat exploits the dependencies between the customer latent space and the agent latent space which results in generating more correlated tomer summaries and agent summaries ablation study we perform ablations to date each component of sutat by removing the variational latent spaces sutat ls so the encoded utterances are directly used for ding removing the sentence level self attention mechanism sutat att and removing the tial copy mechanism sutat copy we use lstm encoders and decoders for all ablation els the results for ablation study in table show that all the removed components play a role in tat removing the latent spaces has the biggest inuence on the summarization performance cating that the variational latent space is necessary to support our design which makes the agent latent variable dependent on the customer latent variable the performance drop after removing the level self attention mechanism shows that using weighted combined utterance embedding is better than simply taking the mean of encoded utterances removing the partial copy has the smallest quality drop however taking the dialogue example in ble without the partial copy mechanism sutat can generate the following summaries customer summary book a hotel in cambridge on tuesday i would like to agent summary i have booked you a hotel the reference number is lzludtvi can i help you with anything else the generated summaries are the same except for the wrong reference number which is crucial mation in this summary classication with summaries a good dialogue summary should reect the key points of the utterances we perform dialogue sication based on dialogue domains to test the lidity of generated summaries first we encode the generated customer summary and agent summary into ex and ey using the trained encoders of each model which are then concatenated as features of the dialogue for classication in this way the logue features are obtained unsupervisedly then we train a separate linear classier on top of the encoded summaries we use sutat with lstm encoders and decoders for this task as shown in table sutat outperforms other baselines on dialogue classication indicating the sutat erated summaries have better comprehension of domain information in the dialogue we can also perform supervised classication by using and sy from sutat as features to train a linear classier the cross entropy loss is combined with equation as the new objective customer yes yes are there any multiple sports places that i can visit in sorry there are none locations in the center of town would you like a different area customer yes please book for the same group of people at on thursday your booking was successful and your ence number is minorhoq agent agent customer hi i am looking for a place to stay the west should be cheap and does nt need to have internet there are no hotels in the moderate price range would you care to expand other criteria agent table examples of single turn conversations ated by the conditional generative module of sutat function where all parameters are jointly optimized as can be seen in table the supervised cation results are as high as on multiwoz and on taskmaster further demonstrating the effectiveness of sutat single turn conversation generation the design of the conditional generative module in sutat enables generating novel single turn sations by sampling the customer latent variable from the standard gaussian zx n i and then sampling the agent latent variable zy sutat can produce realistic looking novel dialogue pairs using the customer decoder and agent decoder table shows three examples of novel single turn conversations generated by sutat using randomly sampled latent variables we can see that the alogue pairs are closely correlated meaning the dependencies between two latent spaces are cessfully captured conclusion we propose sutat an unsupervised abstractive dialogue summarization model accommodating the two speaker scenario in tete a tetes and marizing them without using any data annotations the conditional generative module models the tomer utterances and agent utterances separately using two encoders and two decoders while taining their correlations in the variational latent spaces in the unsupervised summarization module a sentence level self attention mechanism is used to highlight more informative utterances the mary representations containing key information of the dialogue are decoded using the same decoders from the conditional generative module with the help of a partial copy mechanism to generate a customer summary and an agent summary the experimental results show the superiority of sutat for unsupervised dialogue summarization and the capability for more dialogue tasks references reinald kim amplayo and mirella lapata supervised opinion summarization with noising and in proceedings of the annual meeting denoising of the association for computational linguistics acl christos baziotis ion androutsopoulos ioannis stas and alexandros potamianos seq differentiable sequence to sequence to sequence autoencoder for unsupervised abstractive sentence compression in proceedings of the conference of the north american chapter of the association for computational linguistics naacl samuel r bowman luke vilnis oriol vinyals drew m dai rafal jozefowicz and samy gio generating sentences from a continuous space in proceedings of the conference on tational natural language learning conll arthur brainskas mirella lapata and ivan titov unsupervised opinion summarization as in proceedings of the copycat review generation annual meeting of the association for tional linguistics acl pawe budzianowski tsung hsien wen bo hsiang tseng inigo casanueva stefan ultes osman madan and milica gaic multiwoz a scale multi domain wizard of oz dataset for oriented dialogue modelling proceedings of the conference on empirical methods in natural guage processing emnlp bill byrne karthik krishnamoorthi chinnadhurai sankar arvind neelakantan daniel duckworth semih yavuz ben goodrich amit dubey andy cedilnik and kyu young kim toward a realistic and diverse dialog dataset in proceedings of the conference on empirical methods in natural language processing emnlp liqun chen yizhe zhang ruiyi zhang chenyang tao zhe gan haichao zhang bai li dinghan shen changyou chen and lawrence carin improving sequence to sequence learning via in proceedings of the international mal transport conference on learning representations iclr eric chu and peter j liu meansum a neural model for unsupervised multi document abstractive summarization in proceedings of the international conference on machine learning icml jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language ing in proceedings of the conference of the north american chapter of the association for tional linguistics naacl gnes erkan and dragomir r radev lexrank graph based lexical centrality as salience in text summarization journal of articial intelligence search thibault fevry and jason phang vised sentence compression using denoising encoders in proceedings of the conference on putational natural language learning conll michel galley a skip chain conditional dom eld for ranking meeting utterances by in proceedings of the conference on tance pirical methods in natural language processing emnlp chih wen goo and yun nung chen tive dialogue summarization with sentence gated modeling optimized by dialogue acts in ieee ken language technology workshop slt alex graves navdeep jaitly and abdel rahman hamed hybrid speech recognition with deep bidirectional lstm in ieee workshop on automatic speech recognition and understanding diederik p kingma and jimmy ba adam a method for stochastic optimization in proceedings of the international conference on learning sentations iclr gaetano rossiello pierpaolo basile and giovanni meraro centroid based text summarization through compositionality of word embeddings in proceedings of the multiling workshop on marization and summary evaluation across source types and genres abigail see peter j liu and christopher d manning get to the point summarization with in proceedings of the annual generator networks meeting of the association for computational guistics acl ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural networks in advances in neural information processing tems neurips ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all in advances in neural information you need cessing systems neurips lu wang and claire cardie independent abstract generation for focused meeting summarization in proceedings of the annual ing of the association for computational linguistics acl peter west ari holtzman jan buys and yejin choi bottlesum unsupervised and self supervised sentence summarization using the information tleneck principle in proceedings of the conference on empirical methods in natural language ing emnlp diederik p kingma and max welling in proceedings of the encoding variational bayes international conference on learning tions iclr shasha xie yang liu and hui lin evaluating the effectiveness of features and sampling in tive meeting summarization in ieee spoken language technology workshop slt manling li lingyu zhang heng ji and richard j radke keep meeting summaries on topic abstractive multi modal meeting summarization in proceedings of the annual meeting of the tion for computational linguistics acl ziyi yang chenguang zhu robert gmyr michael zeng xuedong huang and eric darve ted a pretrained unsupervised summarization model with theme modeling and denoising arxiv preprint chin yew lin rouge a package for automatic evaluation of summaries in acl workshop on text summarization branches out lin yuan and zhou yu abstractive dialog marization with semantic scaffolds arxiv preprint chunyi liu peng wang jiang xu zang li and jieping ye automatic dialogue summary generation for customer service in acm sigkdd international conference on knowledge discovery and data mining rada mihalcea and paul tarau textrank ing order into text in proceedings of the conference on empirical methods in natural language ing emnlp haojie pan junpei zhou zhou zhao yan liu deng cai and min yang end end dialogue description generation arxiv preprint xinyuan zhang yitong li dinghan shen and lawrence carin diffusion maps for textual in advances in neural network embedding mation processing systems neurips xinyuan zhang yi yang siyang yuan dinghan shen and lawrence carin syntax infused tional autoencoder for text generation in ings of the annual meeting of the association for computational linguistics acl hao zheng and mirella lapata sentence trality revisited for unsupervised summarization in proceedings of the annual meeting of the tion for computational linguistics acl
