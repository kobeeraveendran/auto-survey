deep transfer reinforcement learning text summarization yaser keneshloo discovery analytics center virginia tech edu naren ramakrishnan discovery analytics center virginia tech edu chandan reddy discovery analytics center virginia tech edu abstract deep neural networks data hungry models face diculties attempting train small text datasets transfer learning potential solution eectiveness text domain explored areas image analysis paper study problem transfer learning text summarization discuss existing state art models fail generalize unseen datasets propose reinforcement learning framework based self critic policy gradient approach achieves good generalization state art results variety datasets extensive set experiments ability proposed framework tune text summarization model training samples best knowledge rst work studies transfer learning text summarization provides generic solution works unseen data keywords transfer learning text summarization self critic reinforcement learning introduction text summarization process summarizing long document sentences capture recent years essence document searchers news article datasets cnn newsroom main resource building evaluating text summarization models models suer critical problem model trained specic dataset works dataset instance model trained cnn dataset tested newsroom dataset result poorer trained directly newsroom dataset lack generalization ability current state art models main motivation work problem arises situations need perform summarization specic dataset ground truth summaries exist dataset collecting ground truth summaries expensive time consuming recourse situation simply apply pre trained summarization model generate summaries data discussed paper approach fail satisfy basic requirements task fails generate high quality summaries analysis work known news related datasets text summarization expect model trained datasets perform news related dataset contrary shown table fast model trained cnn holds state art result text summarization task cnn test dataset score according measure reach metric newsroom test data performance fall observation shows models suer poor generalization capability paper rst study extent current state art models vulnerable alizing datasets discuss transfer ing help alleviating problems addition propose solution based ment learning achieves good generalization formance variety summarization datasets ditional transfer learning usually works pre training model large dataset tuning dataset testing result dataset proposed method shown fig able achieve good results variety datasets tuning model single dataset removes requirement ing separate transfer models dataset best knowledge rst work ies transfer learning problem text rization provides solution rectifying eralization issue arises current state art summarization models addition conduct experiments demonstrate ability proposed method obtain state art results datasets small amounts ground truth data rest paper organized follows copyright siam unauthorized reproduction article prohibited table pointer generator fast models trained cnn dataset tested cnn newsroom datasets pointer generator rouge cnn newsroom fast figure standard transfer learning settings model pre trained network layers transferred model tuned nally tested contrary proposed method transferrl uses create model works variety test datasets tion describes transfer learning methods recent research related problem section presents proposed model transfer learning text tion section shows experimental results pares benchmark datasets section concludes discussion related work recently surge development deep learning based methods building models ability transfer generalize similar problems transfer learning studied domain image processing utility nlp problems thoroughly investigated section review works transferring trained models works underlying model rst trained specic dataset pre trained model problem dataset method depending underlying model transform dierent types neural network layers pre trained model transfer model examples transferable layers word embedding layer convolutional layers cnn model fully connected hidden layers nally output layer yosinski studied eect transferring dierent layers deep neural network found lower level layers learn general features higher level layers capture specic characteristic problem hand researchers demonstrated transfer low level high level neural layers cnn recently semwal idea ferring network layers text classication aside transferring network layers experimented freezing tuning layers transfer concluded tuning transfer layers provide better result studied context named entity recognition problem posed method falls category study eect transferring network layers pose new training model training text rization models reinforcement learning techniques knowledge distillation knowledge tion refers class techniques trains small network transferring knowledge larger work techniques typically quire building models devices limited tional resources usually models teacher larger model student smaller model goal transfer knowledge teacher dent recently researchers idea create models meta learning shot ing shot learning domain tation image classication problems eect types models nlp tasks studied studied building generalized models recently cann released challenge called decathlon nlp aims solving dierent nlp problems single unied model main intuition hind model comprehend impact ferring knowledge dierent nlp tasks building generalized model works task model outperforms state art models specic tasks fails reach line results tasks like text summarization observe poor results generalized works google framework text summarization vast research work topic text summarization ing deep neural networks works range fully extractive methods completely stractive ones earliest works neural networks extractive summarization nallapati proposed framework ranking technique extract salient copyright siam unauthorized reproduction article prohibited tence input hand tive summarization rush rst time attention sequence sequence model problem headline generation improve performance models pointer generator model proposed fully handling vocabulary oov words model later improved coverage nism models suer mon problem known exposure bias refers fact training model trained feeding ground truth input decoder step test rely output erate token training typically cross entropy loss test metrics rouge bleu evaluate model tackle problem researchers suggested models scheduled sampling forcement learning based approaches recently authors investigated methods try rst perform extractive summarization selecting salient sentences document classier apply language model paraphrasing model selected sentences obtain nal abstractive summarization models discussed paper shown table capability generalize datasets perform specic dataset target data pre training process proposed model paper propose transfer learning methods problem text summarization experiments consider dierent datasets source dataset train pre trained model target dataset dataset tune pre trained model following idea transferring layers pre trained model rst proposed model transfers dierent layers trained model trained tunes propose method uses novel reinforcement learning framework train transfer model training signals received transferring network layers ious network layers deep neural network instance model cnn encoder lstm decoder cnn layers hidden decoder ers trained tune word embedding representation key layer model use pre trained word embeddings glove training let drive inference word beddings let model tune trained word embedding training model summary transfer embedding layer convolutional layer cnn hidden ers lstm output layer text summarization transfer learning problem way understand eect layers tune freeze layers model transfer report best performing model gested best performance realized layers pre trained model transferred model led tune fore follow practice let transferred model tune trainable variables model shown later experimental result section way transferring network layers provides strong baseline text summarization performance posed reinforced model close baseline main problems approach source dataset contain large number training samples lot training samples able tune pre trained model generalize distribution pre trained model parameters ful transfer learning method requires large number samples problematic specically cases target dataset small tuning model cause tting reasons propose model uses reinforcement learning tune model based reward obtained target dataset transfer reinforcement learning ferrl section explain proposed inforcement learning based framework knowledge transfer text summarization basic ing summarization mechanism work pointer generator model pointer generator reason choose pointer generator model basis framework ability handle vocabulary oov words necessary transfer learning note specic vocabulary generated train pre trained model use dierent vocabulary set tuning stage indexing words change words second according experiments use instead target dataset avoid confusing subscript time instance word index rst dataset index second dataset copyright siam unauthorized reproduction article prohibited words cnn room datasets words common datasets model trained datasets oovs tuning step framework able handle oov words elegantly strate signicantly poor results transfer nave approach resolving problem use shared set vocabulary words model suer inability generalize datasets dierent vocabulary set pointer generator shown fig pointer generator model comprises series lstm encoders blue boxes lstm decoders green boxes let consider dataset dataset contains documents summaries document represented series words xte encoder takes ding word input generates output state decoder hand takes state encoder hte starts generating output size based current state decoder truth summary word step decoding attention vector context vector output distribution pvocab calculated follows wssj sof fij pvocab sof ijhi trainable model rameters concatenation operator ple sequence sequence model attention use pvocab calculate cross entropy loss pvocab captures distribution words vocabulary generate lot oov words ing decoding step pointer generator model igates issue switching mechanism chooses word vocabulary tain probability original document attention distribution probability follows wccj wssj wxxj jpvocab figure pointer generator self critic policy gradient cross entropy loss calculated follows lce log shows training parameters returns word embedding specic token mentioned section main problems cross entropy loss exposure bias curs inconsistency decoder input training test model trained loss generalization power quired transfer learning model trained generate samples distribution heavily relies ground truth input distribution input data changes likely happen transfer learning dataset trained model essentially calibrate transferred layer achieve good result target dataset avoid problems propose reinforcement learning framework slowly removes dependency model training loss increases reliance model output reinforcement learning objective training focus minimizing negative pected reward directly minimizing loss allows framework use model output training helps ing model based metric decoding rouge achieve training following objective minimized minimize lrl trainable model parameters word oov pvocab model rely attention values select right token nal probability calculated sample tokens drawn output policy practice usually sample sequence tokens calculate expectation derivative copyright siam unauthorized reproduction article prohibited log log figure proposed transferrl framework encoder decoder units shared source target datasets loss function given follows lrl minimization improved adding baseline reward text summarization baseline reward come separate network called critic network reward sequence coming greedy selection work consider greedy summary objective sequence baseline minimize training follows lrl log represents greedy selection time model known self critic policy gradient approach model uses greedy output create baseline model uses sampled sentence target training ground truth sentence given objective model focuses samples better greedy selection training penalizing worse greedy selection transfer reinforcement learning model trained suer exposure bias perform poorly transfer learning setting fact model trained tion source dataset transferred target dataset aims generate samples according distribution source dataset need model remembers distribution source dataset tries learn adapt distribution target dataset overall based framework proposed paper shown fig step select random mini batch feed shared encoder units decoder starts decoding mini batch decoding completed model generates sentence based greedy selection sampling output distribution finally calculate ferrl loss according propagate error according trade parameter thick thin dashed lines plot shows eect extent model needs rely propagating error let consider sequences drawn greedy tion sampling source dataset target dataset tively dene transfer loss function variables follows log controls trade self critic loss samples drawn source dataset target dataset means train model samples source dataset means model trained samples target dataset seen decoder state context vector shared source target samples use shared embedding trained source dataset datasets input data given encoder come source target datasets practice objective loss activates good pre trained model obtained follow practice rst pre train model source dataset activate transfer loss combining loss loss parameter follows ixed experimental results performed range experiments understand dynamics transfer learning investigate best practices obtaining generalized model text summarization section discuss insights gained experiments evaluations rouge scores test data rouge scores condence interval reported ocial rouge similar multi task learning frameworks decanlp use measure comparing result transfer learning datasets taking average score measure org project copyright siam unauthorized reproduction article prohibited table basic statistics datasets experiments newsroom cnn train val test avg summary sentences avg words summary datasets addition introduce weighted average score takes account size dataset weight averaging datasets use widely datasets text summarization experiments rst datasets newsroom cnn daily mail training models duc duc datasets test generalization capability model table shows basic statistics datasets datasets news article accompanied written summaries cover wide range challenges transfer learning instance model trained newsroom dataset likely generate long summary sentence cnn dataset model required generate smaller summary sentences experiments use newsroom cnn vice versa training setup experiment run model epochs pre training epochs transfer process extra epochs coverage mechanism use batch size training encoder reads rst words decoder generates summary words encoder decoder units hidden size embedding dimension set learn word embedding training models words dataset vocabulary test use beam search size use adagrad optimize models initial learning rate pre training coverage linearly decrease learning rate based epoch numbers epoch set zero start training increased linearly gets end training training use scheduled sampling sampling probability equal value use framework build model eect dataset size discuss insights gained starting ing eect data size pre training according experiments shown table average model trained newsroom dataset source dataset better performance models use cnn surprising result deep ral networks data hungry models typically work best provided large number samples rst experiment table table uses newsroom dataset training model surprisingly performs good dataset discussed earlier performance datasets poor common vocabulary mentioned tion way avoid excessive oov words transfer learning datasets use common cabulary train model common vocabulary set model trained vocabulary perform datasets suers poor generalization unseen datasets demonstrate combine articles cnn newsroom training datasets create single unied dataset table table train model loss common set vocabulary result experiment shown experiment table table shown comparing results experiment combining datasets decrease performance newsroom test datasets increase mance cnn test data ing generalization ability method datasets performs worse proposed method witnessed comparing average scores weighted average scores proposed model model erage method improves compared method according weighted average score transferring layers experiment discuss eect transferring dierent layers pre trained model transfer learning generator model described section embedding matrix encoder decoder model parameters choices layers use transfer learning page limitations results presented arxiv version paper space constraints report results setup refer readers arxiv version paper copyright siam unauthorized reproduction article prohibited table results newsroom cnn test data shows dataset pre training target dataset stands newsroom dataset denotes cnn dataset method column shows use loss transferring layers transferrl trl loss training use coverage mechanism experiments result proposed method shown method loss loss newsroom cnn table normalized weighted normalized rouge scores table method avg score loss loss weighted avg score experiment pre train model transfer learning replace continue training model loss shown tables way transferring network layers provides strong baseline comparing formance proposed method results simple transferring layers provide signals model adapt new data distribution discussed earlier tion way transfer learning tends completely forget pre trained model distribution entirely changes nal model distribution dataset tuning eect observed ble comparing result experiments shown table transfer learning formance drops newsroom test dataset based increases cnn dataset based proposed method tries remember distribution pre trained model parameter slowly changes distribution model according distribution coming target dataset performs better simple transfer learning datasets shown paring result experiments table shows proposed model performs better nave transfer learning test datasets eect zeta mentioned section trade emphasizing training samples drawn controlled parameter eect transfer learning clip value train separate model objective basically means treat samples coming source target datasets equally training experiments start zero increase linearly till end training clip nal value table shows result experiment simplicity sake provide result proposed model achieved clipping results comparing results setups average increasing value yield better results clipping value instance according average weighted average score increase rouge scores clip comparing cnn score table clipping value denitely hurt performance model shows equal attention distribution coming datasets hand surprising component avoiding clipping performance source dataset increases transfer learning small datasets discussed section transfer learning good uations goal summarization dataset little ground truth summaries purpose conducted set experiments test proposed model transfer learning datasets target datasets seen small improvement results omitted section copyright siam unauthorized reproduction article prohibited table result transferrl clipping newsroom cnn test datasets average weighted average scores table result transfer learning newsroom pre training tuning method newsroom cnn avg score weighted avg score table result transfer learning methods newsroom dataset pre training tuning underlined result shows improvement statistically signicant compared proposed model method experiments randomly pick dataset training set validation dataset rest dataset test data generate articles training dataset respectively similar experiments paper use cnn use newsroom transfer learning size datasets models trained tions tuning best model selected according validation set tables depict results experiment shown tables simply transfer network layers performs slightly better statistically higher cording condence interval proposed model proposed model achieve far better result shown tables results achieved tuning pre trained model datasets close ones achieved table case dataset proposed method table achieves better sults ones shown table shows ability proposed framework generalizing seen datasets note unlike experiments proposed model table information data distribution performs better datasets table comparing best performing model state art multi task learning frameworks cnn dataset according average rouge scores result reported original paper decanlp average rouge proposed model generalized models pare performance proposed model recent methods multi task learning text summarization decanlp recent frameworks use multi task learning following setup works focus models trained cnn datasets report average rouge scores best performing model table compares result proposed approach methods conclusion paper tackled problem transfer ing text summarization studied problem dierent perspectives transfer network layers pre trained model proposing ment learning framework borrows insights self critic policy gradient strategy oers tematic mechanism creates trade reliance source target dataset ing training extensive set experiments demonstrated generalization power posed model unseen test datasets reach state art results datasets best knowledge rst work studies transfer learning text summarization oers lution beats state art models generalizes unseen datasets acknowledgments work supported national science foundation grants references cnn results excluded space constraints caruana deep nets need reader refer arxiv version paper deep nips pages copyright siam unauthorized reproduction article prohibited bengio vinyals jaitly shazeer scheduled sampling sequence prediction current neural networks nips pages pages narayan cohen lapata ranking tences extractive summarization reinforcement learning naacl hlt bertinetto henriques valmadre torr vedaldi learning feed forward shot ers nips pages papineni roukos ward zhu bleu method automatic evaluation machine translation acl pages chen bansal fast abstractive rization reinforce selected sentence rewriting acl volume pages paulus xiong socher deep reinforced model abstractive summarization arxiv preprint donahue jia vinyals homan zhang tzeng darrell decaf deep convolutional activation feature generic visual recognition icml pages duan andrychowicz stadie schneider sutskever abbeel zaremba shot imitation learning nips pages ganin ustinova ajakan germain larochelle laviolette marchand lempitsky domain adversarial training neural networks journal machine learning research gehrmann deng rush arxiv preprint summarization abstractive grusky naaman artzi newsroom dataset million summaries diverse extractive strategies naacl hlt hermann kocisky grefenstette holt kay suleyman blunsom ing machines read comprehend nips pages keneshloo shi ramakrishnan reddy deep reinforcement learning sequence sequence models kryscinski paulus xiong socher improving abstraction text summarization arxiv preprint jiang shang paraphrase generation deep reinforcement learning arxiv preprint lin neural adaptation layers emnlp cross domain named entity recognition pages lin rouge package automatic evaluation summaries proc workshop text rization branches mccann keskar xiong socher natural language decathlon multitask learning question answering nallapati zhai zhou summarunner recurrent neural network based sequence model extractive summarization documents aaai pages nallapati zhou dos santos gulcehre xiang abstractive text summarization signll sequence sequence rnns pennington socher manning glove emnlp global vectors word representation pages ponti vulic glavas mrksic korhonen adversarial propagation zero shot cross lingual transfer word vector specialization emnlp pages acl ravi larochelle optimization model shot learning iclr rush chopra weston neural tention model abstractive sentence summarization emnlp pages sachan xie xing eective use bidirectional language modeling medical named entity recognition arxiv santoro bartunov botvinick stra lillicrap meta learning icml pages augmented neural networks liu manning point summarization pointer generator networks acl volume pages semwal yenigalla mathur nair practitioners guide transfer learning text classication convolutional neural networks sdm pages siam shi keneshloo ramakrishnan summarization arxiv preprint reddy sequence sequence models neural abstractive text snell swersky zemel prototypical networks shot learning nips pages tzeng homan saenko darrell versarial discriminative domain adaptation cvpr volume page vaswani bengio brevdo chollet gomez gouws jones kaiser ner parmar neural chine translation yosinski clune bengio lipson transferable features deep neural networks nips pages zhou yang wei huang zhou zhao neural document summarization jointly learning score select sentences acl pages acl copyright siam unauthorized reproduction article prohibited supplemental material section provide details experimental analysis table document represents specic table main paper copyright siam unauthorized reproduction article prohibited method cov table table main paper results newsroom cnn test data shows dataset pre training target dataset stands newsroom stands cnn dataset method column shows use loss transferring layers transferrl trl loss training experiment run dierent setups coverage mechanism represented use coverage mechanism experiments result proposed method shown newsroom cnn loss loss loss loss loss loss yes yes yes yes yes yes yes table table main paper normalized weighted normalized scores table method cov avg score loss loss loss loss loss loss yes yes yes yes yes yes yes weighted avg score table table main paper result transferrl clipping newsroom cnn test data average weighted average scores method cov yes yes yes yes newsroom cnn copyright siam unauthorized reproduction article prohibited table table main paper normalized weighted normalized scores table method cov avg score yes yes yes yes weighted avg score table table main paper result transfer learning methods newsroom pre training tuning underlined result shows improvement statistically signicant compared proposed model method cov yes yes yes yes table table main paper result transfer learning methods newsroom pre training tuning method cov yes yes yes yes copyright siam unauthorized reproduction article prohibited
