n j g l s c v v x r deep transfer reinforcement learning text summarization yaser keneshloo discovery analytics center virginia tech edu naren ramakrishnan discovery analytics center virginia tech edu chandan k reddy discovery analytics center virginia tech vt edu abstract deep neural networks data hungry models face diculties attempting train small text datasets transfer learning potential solution eectiveness text domain explored areas image analysis paper study problem transfer learning text summarization discuss existing state art models fail generalize unseen datasets propose reinforcement learning framework based self critic policy gradient approach achieves good generalization state art results variety datasets extensive set experiments ability proposed framework ne tune text summarization model training samples best knowledge rst work studies transfer learning text summarization provides generic solution works unseen data keywords transfer learning text summarization self critic reinforcement learning introduction text summarization process summarizing long document sentences capture recent years essence document searchers news article datasets cnn dm newsroom main resource building evaluating text summarization models models suer critical problem model trained specic dataset works dataset instance model trained cnn dm dataset tested newsroom dataset result poorer trained directly newsroom dataset lack generalization ability current state art models main motivation work problem arises situations need perform summarization specic dataset ground truth summaries exist dataset collecting ground truth summaries expensive time consuming recourse situation simply apply pre trained summarization model generate summaries data discussed paper approach fail satisfy basic requirements task fails generate high quality summaries analysis work known news related datasets text summarization expect model trained datasets perform news related dataset contrary shown table fast rl model trained cnn dm holds state art result text summarization task cnn dm test dataset f score according measure reach metric newsroom test data performance fall observation shows models suer poor generalization capability paper rst study extent current state art models vulnerable alizing datasets discuss transfer ing help alleviating problems addition propose solution based ment learning achieves good generalization formance variety summarization datasets ditional transfer learning usually works pre training model large dataset ne tuning dataset testing result dataset proposed method shown fig able achieve good results variety datasets ne tuning model single dataset removes requirement ing separate transfer models dataset best knowledge rst work ies transfer learning problem text rization provides solution rectifying eralization issue arises current state art summarization models addition conduct experiments demonstrate ability proposed method obtain state art results datasets small amounts ground truth data rest paper organized follows copyright siam unauthorized reproduction article prohibited table pointer generator fast rl models trained cnn dm dataset tested cnn dm newsroom datasets pointer generator rouge cnn dm newsroom l fast rl l figure standard transfer learning settings model pre trained ds network layers transferred model ne tuned dg nally tested dg contrary proposed method transferrl uses dg create model works variety test datasets tion describes transfer learning methods recent research related problem section presents proposed model transfer learning text tion section shows experimental results pares benchmark datasets section concludes discussion related work recently surge development deep learning based methods building models ability transfer generalize similar problems transfer learning tl studied domain image processing utility nlp problems thoroughly investigated section review works transferring trained models works underlying model rst trained specic dataset pre trained model problem dataset method depending underlying model transform dierent types neural network layers pre trained model transfer model examples transferable layers word embedding layer convolutional layers cnn model fully connected fc hidden layers nally output layer yosinski et al studied eect transferring dierent layers deep neural network found lower level layers learn general features higher level layers capture specic characteristic problem hand researchers demonstrated transfer low level high level neural layers cnn tl recently semwal et al idea ferring network layers text classication aside transferring network layers experimented freezing ne tuning layers transfer concluded ne tuning transfer layers provide better result tl studied context named entity recognition problem posed method falls category study eect transferring network layers pose new co training model training text rization models reinforcement learning techniques knowledge distillation knowledge tion refers class techniques trains small network transferring knowledge larger work techniques typically quire building models devices limited tional resources usually models teacher larger model student smaller model goal transfer knowledge teacher dent recently researchers idea create models meta learning shot ing shot learning domain tation image classication problems eect types models nlp tasks studied studied building generalized models recently cann et al released challenge called decathlon nlp aims solving dierent nlp problems single unied model main intuition hind model comprehend impact ferring knowledge dierent nlp tasks building generalized model works task model outperforms state art models specic tasks fails reach line results tasks like text summarization observe poor results generalized works google s framework text summarization vast research work topic text summarization ing deep neural networks works range fully extractive methods completely stractive ones earliest works neural networks extractive summarization nallapati et al proposed framework ranking technique extract salient copyright siam unauthorized reproduction article prohibited tence input hand tive summarization rush et al rst time attention sequence sequence model problem headline generation improve performance models pointer generator model proposed fully handling vocabulary oov words model later improved coverage nism models suer mon problem known exposure bias refers fact training model trained feeding ground truth input decoder step test rely output erate token training typically cross entropy loss test metrics rouge bleu evaluate model tackle problem researchers suggested models scheduled sampling forcement learning based approaches recently authors investigated methods try rst perform extractive summarization selecting salient sentences document classier apply language model paraphrasing model selected sentences obtain nal abstractive summarization models discussed paper shown table capability generalize datasets perform specic dataset target data pre training process proposed model paper propose transfer learning methods problem text summarization experiments consider dierent datasets ds source dataset train pre trained model dg target dataset dataset ne tune pre trained model following idea transferring layers pre trained model rst proposed model transfers dierent layers trained model trained ds ne tunes dg propose method uses novel reinforcement learning rl framework train transfer model training signals received ds dg transferring network layers ious network layers deep neural network instance model cnn encoder lstm decoder cnn layers hidden decoder ers trained ds ne tune dg word embedding representation key layer model use pre trained word embeddings glove training ds let ds drive inference word beddings let model ne tune trained word embedding training model summary transfer embedding layer convolutional layer cnn hidden ers lstm output layer text summarization transfer learning problem way understand eect layers tune freeze layers model transfer report best performing model gested best performance realized layers pre trained model ds transferred model led ne tune dg fore follow practice let transferred model ne tune trainable variables model shown later experimental result section way transferring network layers provides strong baseline text summarization performance posed reinforced model close baseline main problems approach source dataset ds contain large number training samples dg lot training samples able ne tune pre trained model generalize distribution pre trained model parameters ful transfer learning method requires large number samples ds dg problematic specically cases target dataset small ne tuning model cause tting reasons propose model uses reinforcement learning ne tune model based reward obtained target dataset transfer reinforcement learning ferrl section explain proposed inforcement learning based framework knowledge transfer text summarization basic ing summarization mechanism work pointer generator model pointer generator reason choose pointer generator model basis framework ability handle vocabulary oov words necessary transfer learning note specic vocabulary generated ds train pre trained model use dierent vocabulary set ne tuning stage dg indexing words change words second according experiments use dg instead dt target dataset avoid confusing t subscript time instance word index rst dataset index second dataset copyright siam unauthorized reproduction article prohibited k words cnn dm room datasets k words common datasets model trained datasets k oovs ne tuning step framework able handle oov words elegantly strate signicantly poor results transfer nave approach resolving problem use shared set vocabulary words ds dg model suer inability generalize datasets dierent vocabulary set pointer generator shown fig pointer generator model comprises series lstm encoders blue boxes lstm decoders green boxes let consider dataset d dn dataset contains n documents summaries document represented series te words e xte xt v encoder takes ding word xt input generates output state ht decoder hand takes state encoder e hte starts generating output size t te y based current state decoder st truth summary word yt step decoding j attention vector j context vector cj output distribution pvocab calculated follows wssj vt sof fij j cj pvocab sof cj ijhi wh ws trainable model rameters concatenation operator ple sequence sequence model attention use pvocab calculate cross entropy loss pvocab captures distribution words vocabulary generate lot oov words ing decoding step pointer generator model igates issue switching mechanism chooses word vocabulary tain probability original document attention distribution probability follows j wccj wssj wxxj j jpvocab j p ij figure pointer generator w self critic policy gradient cross entropy ce loss calculated follows lce log p st x t shows training parameters e returns word embedding specic token mentioned section main problems cross entropy loss exposure bias curs inconsistency decoder input training test model trained ce loss generalization power quired transfer learning model trained generate samples distribution heavily relies ground truth input distribution input data changes likely happen transfer learning dataset trained model essentially calibrate transferred layer achieve good result target dataset avoid problems propose reinforcement learning framework slowly removes dependency model training ce loss increases reliance model output reinforcement learning objective rl training focus minimizing negative pected reward directly minimizing ce loss allows framework use model s output training helps ing model based metric decoding rouge achieve rl training following objective minimized minimize lrl t t t p wc wx trainable model parameters word xj oov pvocab model rely attention values select right token nal probability calculated eq t sample tokens drawn output policy p e p p t practice usually sample sequence tokens calculate expectation derivative copyright siam unauthorized reproduction article prohibited log p t t lt rl log p t figure proposed transferrl framework encoder decoder units shared source ds target datasets dg loss function given follows lrl e t p minimization improved adding baseline reward text summarization baseline reward come separate network called critic network reward sequence coming greedy selection p work consider greedy t summary objective sequence baseline minimize rl training follows lrl t log p st t yt represents greedy selection time t model known self critic policy gradient approach model uses greedy output create baseline model uses sampled sentence target training ground truth sentence given objective model focuses samples better greedy selection training penalizing worse greedy selection transfer reinforcement learning model trained eq suer exposure bias perform poorly transfer learning setting fact model trained tion source dataset transferred target dataset aims generate samples according distribution source dataset need model remembers distribution source dataset tries learn adapt distribution target dataset overall rl based framework proposed paper shown fig step select random mini batch ds dg feed shared encoder units decoder starts decoding mini batch decoding completed model generates sentence based greedy selection sampling output distribution finally calculate ferrl loss according eq propagate error according trade o parameter thick thin dashed lines plot shows eect extent model needs rely ds dg propagating error let consider sequences drawn greedy tion sampling source dataset ds target dataset dg ys g tively dene transfer loss function variables follows s yg ys s log p t yg g s st xs ug g st xg controls trade o self critic loss samples drawn source dataset target dataset means train model samples source dataset means model trained samples target dataset seen eq decoder state st context vector shared source target samples use shared embedding trained source dataset es datasets input data given encoder e xs xg come source target datasets practice rl objective loss activates good pre trained model obtained follow practice rst pre train model source dataset activate transfer rl loss eq combining loss ce loss eq parameter follows lm ixed lt rl experimental results performed range experiments understand dynamics transfer learning investigate best practices obtaining generalized model text summarization section discuss insights gained experiments evaluations rouge l scores test data rouge scores condence interval reported ocial rouge similar multi task learning frameworks decanlp use measure comparing result transfer learning datasets taking average score measure org project copyright siam unauthorized reproduction article prohibited table basic statistics datasets experiments newsroom cnn dm train val test avg summary sentences avg words summary datasets addition introduce weighted average score takes account size dataset weight averaging datasets use widely datasets text summarization experiments rst datasets newsroom cnn daily mail training models duc duc datasets test generalization capability model table shows basic statistics datasets datasets news article accompanied written summaries cover wide range challenges transfer learning instance model trained newsroom dataset likely generate long summary sentence cnn dm dataset model required generate smaller summary sentences experiments use newsroom ds cnn dm dg vice versa training setup experiment run model epochs pre training epochs transfer process extra epochs coverage mechanism use batch size training encoder reads rst words decoder generates summary words encoder decoder units hidden size embedding dimension set learn word embedding training models k words dataset vocabulary test use beam search size use adagrad optimize models initial learning rate pre training rl coverage linearly decrease learning rate based epoch numbers t epoch set zero start rl training increased linearly gets end training rl training use scheduled sampling sampling probability equal value use framework build model eect dataset size discuss insights gained starting ing eect data size pre training according experiments shown table average model trained newsroom dataset source dataset ds better performance models use cnn dm ds surprising result deep ral networks data hungry models typically work best provided large number samples rst experiment table table uses newsroom dataset training model surprisingly performs good dataset discussed earlier performance datasets poor common vocabulary mentioned tion way avoid excessive oov words transfer learning datasets use common cabulary ds dg train model common vocabulary set model trained vocabulary perform datasets suers poor generalization unseen datasets demonstrate combine articles cnn dm newsroom training datasets create single unied dataset table table train model ce loss eq common set vocabulary result experiment shown experiment table table shown comparing results experiment combining datasets decrease performance newsroom test datasets increase mance cnn dm test data ing generalization ability method datasets performs worse proposed method witnessed comparing average scores weighted average scores proposed model model erage method improves compared method according weighted average score transferring layers experiment discuss eect transferring dierent layers pre trained model transfer learning generator model described section embedding matrix encoder decoder model parameters choices layers use transfer learning page limitations results presented arxiv version paper space constraints report results setup refer readers arxiv version paper copyright siam unauthorized reproduction article prohibited table results newsroom cnn dm test data ds shows dataset pre training dg target dataset n stands newsroom dataset c denotes cnn dm dataset method column shows use ce loss transferring layers tl transferrl trl loss training use coverage mechanism experiments result proposed method shown ds dg method n n n c c ce loss ce loss tl newsroom rl cnn dm rl rl rl table normalized weighted normalized rouge f scores table ds dg method avg score n n n c c ce loss ce loss tl rl weighted avg score rl experiment pre train model ds transfer learning replace ds dg continue training model ce loss shown tables way transferring network layers provides strong baseline comparing formance proposed method results simple transferring layers provide signals model adapt new data distribution discussed earlier tion way transfer learning tends completely forget pre trained model distribution entirely changes nal model distribution dataset ne tuning eect observed ble comparing result experiments shown table transfer learning formance drops newsroom test dataset based increases cnn dm dataset based proposed method tries remember distribution pre trained model parameter slowly changes distribution model according distribution coming target dataset performs better simple transfer learning datasets shown paring result experiments table shows proposed model performs better nave transfer learning test datasets eect zeta mentioned section trade o emphasizing training samples drawn ds dg controlled parameter eect transfer learning clip value train separate model objective basically means treat samples coming source target datasets equally training experiments start zero increase linearly till end training clip nal value table shows result experiment simplicity sake provide result proposed model achieved clipping results comparing results setups average increasing value yield better results clipping value instance according average weighted average score increase rouge l scores clip comparing cnn dm score table clipping value denitely hurt performance dg model shows equal attention distribution coming datasets hand surprising component avoiding clipping performance source dataset increases transfer learning small datasets discussed section transfer learning good uations goal summarization dataset little ground truth summaries purpose conducted set experiments test proposed model transfer learning datasets target datasets seen small improvement results omitted section copyright siam unauthorized reproduction article prohibited table result transferrl clipping newsroom cnn dm test datasets average weighted average scores table result transfer learning newsroom pre training ne tuning dg ds n n method tl rl newsroom cnn dm avg score weighted avg score rl rl table result transfer learning methods newsroom dataset pre training ne tuning underlined result shows improvement tl statistically signicant compared proposed model dg ds n n method tl rl e dg experiments randomly pick dataset training set validation dataset rest dataset test data generate articles training dataset respectively similar experiments paper use cnn dm use newsroom ds dg transfer learning size datasets models trained tions ne tuning best model selected according validation set tables depict results experiment shown tables simply transfer network layers performs slightly better statistically higher cording condence interval proposed model proposed model achieve far better result shown tables results achieved ne tuning pre trained model datasets close ones achieved table case dataset proposed method table achieves better sults ones shown table shows ability proposed framework generalizing seen datasets note unlike experiments proposed model table information data distribution performs better datasets table comparing best performing model state art multi task learning frameworks cnn dm dataset according average rouge l f scores result reported original paper decanlp average rouge proposed model generalized models pare performance proposed model recent methods multi task learning text summarization decanlp recent frameworks use multi task learning following setup works focus models trained cnn dm datasets report average rouge l f scores best performing model table compares result proposed approach methods conclusion paper tackled problem transfer ing text summarization studied problem dierent perspectives transfer network layers pre trained model proposing ment learning framework borrows insights self critic policy gradient strategy oers tematic mechanism creates trade o reliance source target dataset ing training extensive set experiments demonstrated generalization power posed model unseen test datasets reach state art results datasets best knowledge rst work studies transfer learning text summarization oers lution beats state art models generalizes unseen datasets acknowledgments work supported national science foundation grants references cnn dm results excluded space constraints j ba r caruana deep nets need reader refer arxiv version paper deep nips pages copyright siam unauthorized reproduction article prohibited s bengio o vinyals n jaitly n shazeer scheduled sampling sequence prediction current neural networks nips pages pages s narayan s b cohen m lapata ranking tences extractive summarization reinforcement learning naacl hlt l bertinetto j f henriques j valmadre p torr vedaldi learning feed forward shot ers nips pages k papineni s roukos t ward w zhu bleu method automatic evaluation machine translation acl pages y chen m bansal fast abstractive rization reinforce selected sentence rewriting acl volume pages r paulus c xiong r socher deep reinforced model abstractive summarization arxiv preprint j donahue y jia o vinyals j homan n zhang e tzeng t darrell decaf deep convolutional activation feature generic visual recognition icml pages y duan m andrychowicz b stadie o j ho j schneider sutskever p abbeel w zaremba shot imitation learning nips pages y ganin e ustinova h ajakan p germain h larochelle f laviolette m marchand v lempitsky domain adversarial training neural networks journal machine learning research s gehrmann y deng m rush arxiv preprint summarization abstractive m grusky m naaman y artzi newsroom dataset million summaries diverse extractive strategies naacl hlt k m hermann t kocisky e grefenstette l holt w kay m suleyman p blunsom ing machines read comprehend nips pages y keneshloo t shi n ramakrishnan c k reddy deep reinforcement learning sequence sequence models w kryscinski r paulus c xiong r socher improving abstraction text summarization arxiv preprint z li x jiang l shang h li paraphrase generation deep reinforcement learning arxiv preprint b y lin w lu neural adaptation layers emnlp cross domain named entity recognition pages c lin rouge package automatic evaluation summaries proc workshop text rization branches b mccann n s keskar c xiong r socher natural language decathlon multitask learning question answering r nallapati f zhai b zhou summarunner recurrent neural network based sequence model extractive summarization documents aaai pages r nallapati b zhou c dos santos c gulcehre b xiang abstractive text summarization signll sequence sequence rnns j pennington r socher c d manning glove emnlp global vectors word representation pages e m ponti vulic g glavas n mrksic korhonen adversarial propagation zero shot cross lingual transfer word vector specialization emnlp pages acl s ravi h larochelle optimization model shot learning iclr m rush s chopra j weston neural tention model abstractive sentence summarization emnlp pages d s sachan p xie e p xing eective use bidirectional language modeling medical named entity recognition arxiv santoro s bartunov m botvinick d stra t lillicrap meta learning icml pages augmented neural networks p j liu c d manning point summarization pointer generator networks acl volume pages t semwal p yenigalla g mathur s b nair practitioners guide transfer learning text classication convolutional neural networks sdm pages siam t shi y keneshloo n ramakrishnan c k summarization arxiv preprint reddy sequence sequence models neural abstractive text j snell k swersky r zemel prototypical networks shot learning nips pages e tzeng j homan k saenko t darrell versarial discriminative domain adaptation cvpr volume page vaswani s bengio e brevdo f chollet n gomez s gouws l jones kaiser n ner n parmar al neural chine translation j yosinski j clune y bengio h lipson transferable features deep neural networks nips pages q zhou n yang f wei s huang m zhou t zhao neural document summarization jointly learning score select sentences acl pages acl copyright siam unauthorized reproduction article prohibited supplemental material section provide details experimental analysis table document represents specic table main paper copyright siam unauthorized reproduction article prohibited dm dg method cov table table main paper results newsroom cnn dm test data ds shows dataset pre training dg target dataset n stands newsroom c stands cnn dm dataset method column shows use ce loss transferring layers tl transferrl trl loss training experiment run dierent setups coverage mechanism represented use coverage mechanism experiments result proposed method shown newsroom cnn dm ce loss ce loss ce loss ce loss tl tl ce loss ce loss tl tl rl rl rl yes yes yes yes yes yes yes rl n n n n n n n n n n c c c c c c c c c c table table main paper normalized weighted normalized f scores table ds dg method cov avg score n n n c n c n c n c c c c n c n c n c n ce loss ce loss ce loss ce loss tl tl ce loss ce loss tl tl yes yes yes yes yes yes yes rl weighted avg score rl table table main paper result transferrl clipping newsroom cnn dm test data average weighted average scores dm dg method cov n n c c n n c c c c n n c c n n yes yes yes yes newsroom rl cnn dm rl rl rl copyright siam unauthorized reproduction article prohibited table table main paper normalized weighted normalized f scores table ds dg method cov avg score n c n c c n c n n c n c c n c n rl yes yes yes yes weighted avg score rl table table main paper result transfer learning methods newsroom pre training ne tuning underlined result shows improvement tl statistically signicant compared proposed model dg ds n n n n c c c c method cov yes yes yes yes tl tl tl tl rl table table main paper result transfer learning methods newsroom pre training ne tuning dg ds n n n n c c c c method cov yes yes yes yes tl tl tl tl rl copyright siam unauthorized reproduction article prohibited
