podsumm podcast audio summarization aneesh vartakavi amanmeet garg gracenote inc diverse nature scale specificity podcasts present unique challenge content discovery systems listeners rely text descriptions episodes provided podcast creators discover new content factors like presentation style narrator production quality significant indicators subjective user preference difficult quantify reflected text descriptions provided podcast creators propose automated creation podcast audio summaries aid content discovery help listeners quickly preview podcast content investing time listening entire episode paper present method automatically construct podcast summary guidance text domain method performs key steps audio text transcription text summary generation motivated lack datasets task curate internal dataset find effective scheme data augmentation design protocol gather summaries annotators fine tune model augmented dataset perform ablation study method achieves rouge scores dataset hope results inspire future research direction ccs concepts information systems speech audio search computing methodologies cross validation applied computing sound music computing additional key words phrases podcasts speech summarization neural networks introduction recent surge popularity podcasts presents big opportunity unique set challenges existing content discovery recommendation systems podcasts usually require active attention listener extended periods unlike listening music subjective attributes speaker presentation style type humor production quality influence listener preference hard discern text description video domain movie trailers allow viewer preview content subjective decision watch film frequent release schedule podcasts production trailers episode impractical audio summaries shown promise improving performance spoken document search algorithms propose method create short podcast audio summaries automated manner summaries inform listener topics podcast subjective attributes like presentation style production quality podcasts present unique set challenges audio based summarization algorithm example podcasts usually focus spoken word content contain overlapping speech multiple speakers free form speech audio effects background music advertisements supervised learning algorithm operating audio domain identify nature audio segment able judge importance require large training data manually annotated listening audio multiple passes difficult time consuming process podcasts largely contain spoken word content summarization performed text domain transcript episode work present summarization podsumm method obtain podcast audio summaries guided text domain podsumm works transcribing spoken content podcast identifying important sentences transcript finally stitching respective audio segments introduce protocol create internal dataset specific task summary introduce authors contributed equally research concept podcast audio summaries aid content discovery summaries allow listener rapidly preview episode investing time listening entire episode vartakavi garg related work text summarization neural models consider task classification problem neural encoder creates latent representation sentences followed classifier scoring sentences importance creating summary rising popularity deep neural networks transformers pre trained language models particularly transformer models bert shown promise wide range nlp tasks bert express semantics document obtain sentence level representation recent approaches text summarization like presumm matchsum leverage bert achieve state art performance benchmark datasets present promising avenue development expansion application domains speech summarization speech summarization requires directly processing audio streams providing snippets create combined audio summary prior solutions task modelled problem feature classification problem speech text training problem graph clustering problem neural extractive summarization reinforcement learning hierarchical modeling sequence sequence modeling shown promising results limited variety data automated speech summarization open research problems multi party speech spontaneous speech handling disfluencies podcast summarization limited research automated methods podcast audio summarization diversity narrative nature podcasts spontaneous speech music audio effects advertisements present challenges existing speech summarization methods address issue pose podcast audio summarization problem multi modal data summarization create audio summary podcast guidance text domain method podsumm architecture podsumm method comprises sequence steps starting original audio stream resulting audio summary obtained output figure stage process automatic speech recognition asr generates transcript process text segment podcast transcript sentences subsequently use fine tuned text summarization model select important sentences inclusion final summary discuss stage detail automatic speech recognition asr methods perform task speech text transcription handle complexities related varied accents prosody acoustic features speaker demographics quality asr transcriptions varies significantly depends underlying training data leveraged publicly available podsumm podcast audio summarization fig block diagram podsumm method modules shelf solution aws transcribe allowed limit errors focus core modules pipeline text processing audio transcripts obtained section contain tuples text individual words punctuation marks start end timestamps audio confidence score text prediction use spacy segment text sentences corresponding start end times audio additionally force sentence break pause seconds words occurs helps better handle cases asr method missed punctuation mark frequently occurs music played speech segments text summary generation build text summaries selecting appropriate sentences transcript leveraging advances field extractive text summarization choose presumm model builds bert obtain sentence level encoding stacks inter sentence transformer layers capture document level features summarization find presumm model pre trained cnn dailymail dataset produce adequate summaries podcasts motivated lack research datasets task created dataset fine tune model podcasts described section extractive presumm model performs summarization document sentences sentm assigning score senti indicating exclusion inclusion summary model trained binary classification entropy loss capture difference prediction ground truth label amazon com usage linguistic audio generation predictions text summarization model include sentence indices respective scores stored sentence offsets audio representing selected sentences stitched obtain vartakavi garg audio summary dataset creation address lack datasets task podcast summarization curate dataset support development evaluation method selected unique podcast series different genres selecting average episodes series dataset contains total hours podcasts average duration minutes episode built annotation tool presented annotator sequence sentences transcript episode metadata podcast feed including original audio episode sentence paired respective audio segment derived offsets segment additionally annotation tool dynamically generated audio text summaries based annotator selection enabling verify choices annotator instructed follow protocol outlined read provider submitted description available listen audio podcast episode understand context core message select set sentences represent summary podcast raters requested select continuous sequences sentences possible minimize cuts audio keeping total summary listen newly created sentence summary repeat steps necessary submit annotations length seconds satisfactory summary obtained resulting annotations include set sentence indices selected annotator suitable candidates create summary resource limitations episode annotated single annotator unable compute inter annotator agreement discarding outliers find took minutes seconds minutes seconds annotate single episode collected total episodes average selected sentences summary model training begin presumm model pre trained cnn dailymail dataset steps provided authors report strong performance fine tune model podcast dataset steps described noticed overfitting training set pre trained model allows position embeddings length deemed sufficient application annotations dataset contained tokens longer episodes model checkpoints saved evaluated test set steps best performing model checkpoint ablation experiments report system performance predicting summaries new unseen data obtain predicted scores sentence subsequently sentences selected rank ordered candidates create final summary com nlpyang presumm podsumm podcast audio summarization evaluation metrics cross validation experiment repeated fold data augmentation report precision recall measure rouge scores metrics selected measure ability model produce summaries overlapping words comparison reference recall prediction precision average measure rouge metric signifies unigram word overlap bigram consecutive word overlap longest common sequence overlap current dataset consists total podcast episodes number small comparison datasets cnn dailymail data label pairs mitigate effect sampling bias report mean standard deviation rouge metrics fold cross validation experiment model trained training split samples performance reported test split episodes process perform data augmentation compensate relatively small size dataset increase generalization ability model observe previews advertisements included summary similar podcasts episodes describe method automatically find segments repetitive content augmentation procedure find indices sentences transcript occur episodes dataset clean indices merge near indices large set remove outliers repetitive content segments stored use augmentations generate augmented output episode repetitive content replace prepend transcript randomly selected repetitive segment create new data sample transcript add new samples total augmented data size samples training set fold ablation studies effect number candidate sentences similar presumm select sentences highest scores predictions study effect varying number sentences selected represent summary rank ordered candidates model prediction experiment varied scores reported effect data augmentation data augmentation applied training alters repetitive content preceding sentences relevant summary test effect data augmentation scheme model performance performed fine tuning experiment data augmentation report system performance metrics results summarize results ablation studies table outlined section report mean standard deviation measure metrics fold cross validation experiment similar prior work use simple baseline lead select leading sentences document summary com abisee cnn dailymail vartakavi garg metric baseline fine tuning presumm presumm presumm aug ablation sentences presumm presumm presumm presumm rouge table results baseline fold cross validation experiment ablation experiments presumm method measure metrics pre trained presumm model model fine tuned podsumm dataset reported test set fold summary statistics metric reported mean std dev folds find performs slightly worse presumm model pre trained cnn dailymail dataset fine tuning fine tuning dataset presumm find significant improvements measure metrics baseline model fine tuning model augmentation presumm aug improves performance demonstrating model performance task improves small task specific data augmentation ablation study find selecting sentences produced best results compared display distribution sentence indices figure ground truth data distribution indicates initial sentences related podcast summary task corroborated relatively high performance baseline relative lead scores model fine tuning biased select sentences beginning document likely property cnn dailymail dataset distributions fine tuning aug closer ground truth distributions reflected metrics tails models appear follow distribution model fine tuning highlights need analysis model development large dataset account possible variations underlying data present example transcript model predictions presumm table presumm aug table model fine tuning selects lot sentences relevant episode table true positive sentences green false positive sentences blue false negative red sentences repeated content sentences magenta falsely predicted model cyan demonstrates method able correctly identify important sentences podcast transcription transcript shows errors accumulated system variations spoken words org mistranscribed dot org incorrect sentence segmentation errors like complicate downstream text processing example reader identify false positive sentences example system identified incorrect sentence segmentation podsumm podcast audio summarization fig selected sentence index normalized count sentences dataset ground truth predictions presumm fine tuned presumm fine tuned presumm aug hey real quick start california coming way live month february super excited finally come southern california oaks talking race tickets head npr prisons dot org iowa tomorrow night friday night tickets available okay hey npr politics podcast january tamara keith cover white house aisha roscoe cover white house susan davis cover congress senate convene court impeachment today senate impeachment trial continuing questions answers senators asking questions house managers president legal team answering questions fact going things happen time stamp aisha wondering stood today lot questions getting idea limit president elected president lawyers representing alan dershowitz argument presidents think election public interest actions kind help reelection long illegal like senators probing limits far argument table presumm output correct predictions green false negatives red false positive sentences blue sentences detected repetitive content magenta falsely predicted model cyan discussion work proposed podsumm method automatically generate audio summaries podcasts guidance text domain method involves transcribing audio followed text processing text summarization audio summary generated stitching audio segments correspond sentences selected text summarization resulting model fine tuned dataset performed better lead baseline model trained cnn dailymail dataset method contains sequence steps performance module directly influences final produced audio summaries paper heavily leverage prior work different fields believe custom modules bring significant advantages example sentence segmentation model robust transcription errors missing punctuation background music allow leverage cheaper accurate asr solutions research needed develop understand effects individual modules specific podcasts truthno ftftft aug vartakavi garg hey real quick start california coming way live month february super excited finally come southern california oaks talking race tickets head npr prisons dot org iowa tomorrow night friday night tickets available okay hey npr politics podcast january tamara keith cover white house aisha roscoe cover white house susan davis cover congress senate convene court impeachment today senate impeachment trial continuing questions answers senators asking questions house managers president legal team answering questions fact going things happen time stamp aisha wondering stood today lot questions getting idea limit president elected president lawyers representing alan dershowitz argument presidents think election public interest actions kind help reelection long illegal like senators probing limits far argument point question senator susan collins maine republican republicans including senators crepeau blunt rubio remember questions submitted writing chief justice reads aloud table presumm aug output correct predictions green false negatives red false positive sentences blue sentences detected repetitive content magenta falsely predicted model cyan proposed method showed improved performance fine tuning dataset recognize smaller size restrict generalization ability model unseen data manual annotation large corpus podcast data task prohibitively expensive techniques like data augmentation alleviate extent conclusion acknowledgments media technology lab gracenote references present novel method create audio summaries podcasts guidance text domain discuss strengths limitations work establishes proof working principle sets direction future development fully learned automated method podcast speech summarization look forward newer methods emerging research community leading improved listener experience authors thank josh morris counsel chinting guidance asr joseph renner jeff scott gannon gesiriech zafar rafi feedback manuscript contributions team members jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language understanding proceedings conference north american chapter association computational linguistics human language technologies volume long short papers mattia antonino gangi robert enyedi alessandra brusadin marcello federico robust neural machine translation clean noisy speech transcripts arxiv preprint sadaoki furui kikuichi yousuke shinnaka chiori hori speech speech speech text summarization international workshop language understanding agents real world interaction citeseer nikhil garg benoit favre korbinian reidhammer dilek hakkani clusterrank graph based method meeting summarization tenth annual conference international speech communication association podsumm podcast audio summarization awni hannun carl case jared casper bryan catanzaro greg diamos erich elsen ryan prenger sanjeev satheesh shubho sengupta adam coates andrew deep speech scaling end end speech recognition arxiv karl moritz hermann tom koisk edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend proceedings international conference neural information processing systems volume montreal canada mit press cambridge usa yaser keneshloo tian shi naren ramakrishnan chandan reddy deep reinforcement learning sequence sequence models ieee transactions neural networks learning systems chin yew lin rouge package automatic evaluation summaries text summarization branches association computational linguistics barcelona spain aclweb org anthology tzu liu shih hung liu berlin chen hierarchical neural summarization framework spoken documents icassp ieee international conference acoustics speech signal processing icassp ieee yang liu mirella lapata text summarization pretrained encoders proceedings conference empirical methods natural language processing international joint conference natural language processing emnlp ijcnlp ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents proceedings thirty aaai conference artificial intelligence san francisco california usa aaai press abigail peter liu christopher manning point summarization pointer generator networks corr org damiano spina johanne trippas lawrence cavedon mark sanderson extracting audio summaries support effective spoken document search journal association information science technology ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need advances neural information processing systems yuxiang baotian learning extract coherent summary deep reinforcement learning thirty second aaai conference shasha xie hui lin yang liu semi supervised extractive speech summarization training algorithm eleventh annual conference artificial intelligence international speech communication association xingxing zhang mirella lapata furu wei ming zhou neural latent extractive document summarization proceedings conference empirical methods natural language processing ming zhong pengfei liu yiran chen danqing wang xipeng qiu xuanjing huang extractive summarization text matching arxiv preprint
