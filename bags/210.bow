searching effective neural extractive summarization works ming zhong pengfei liu danqing wang xipeng qiu xuanjing huang shanghai key laboratory intelligent information processing fudan university school computer science fudan university zhangheng road shanghai china edu abstract recent years seen remarkable cess use deep neural networks text summarization clear derstanding perform improved paper seek better understand neural extractive summarization systems benet ferent types model architectures able knowledge learning schemas tionally effective way improve current frameworks achieve state art result cnn dailymail large margin based observations ses hopefully work provide clues future research extractive marization source code available project introduction recent years seen remarkable success use deep neural networks text tion celikyilmaz jadhav rajan far research utilizing neural network text tion revolved architecture ing zhou chen bansal gehrmann despite success remains poorly stood perform comings limits ability design ter architectures rapid development neural architectures calls detailed empirical study analyzing understanding existing models paper primarily focus extractive summarization computationally cient generate grammatically ent summaries nallapati seek authors contributed equally corresponding author com fastnlp fastnlp com better understand neural network based proaches task benet different types model architectures transferable edge learning schemas improved architectures architecturally better mance usually comes cost standing system date know little functionality neural component differences peters raises following typical tions choice different ral architectures cnn rnn transformer ence performance summarization tem components matters specic dataset current models suffer engineering problem understanding questions help choose suitable architectures different application scenarios motivate ward powerful frameworks external transferable knowledge ing schemas clearly improvement racy performance merely shift feature engineering structure neering exible ways incorporate nal knowledge mikolov peters devlin learning schemas introduce extra instructive constraints paulus arumae liu rst steps answers following questions type pre trained models supervised unsupervised pre training friendly summarization task architectures explored exhaustively push state art results new level introducing external transferable knowledge changing learning schema comprehensive study perspective content sec learning schemas sup reinforce structure dec enc pointer seqlab lstm transformer knowledge exter glove bert news inter random table outline experimental design dec enc represent decoder encoder respectively sup denotes supervised learning news means vised pre training knowledge alytical perspectives rst build testbed summarization system training ing environment constructed ing environment design different tion models analyze inuence performance specically models differ types architectures encoders cnn lstm transformer vaswani coders auto non auto regressive ternal transferable knowledge glove ton bert devlin newsroom grusky different learning schemas supervised learning forcement learning peer internal working mechanism testing cases provide sufcient ation scenarios testing environment cretely present multi domain test sentence shufing test analyze models different metrics repetition sentence length position bias additionally developed provide better understanding characteristics ferent datasets empirically main observations rized architecturally speaking models regressive decoder prone achieving ter performance non auto regressive coder lstm likely suffer architecture overtting problem transformer robust success extractive summarization tem cnn dailymail heavily relies ability learn positional information sentence unsupervised transferable knowledge useful supervised transferable regressive indicates decoder rent prediction knowledge previous predictions edge easily inuenced domain shift problem effective way improve rent system achieving state art sult cnn dailymail large margin help unsupervised transferable knowledge score result enhanced introducing reinforcement learning score hopefully detailed empirical study vide hints follow researchers design better architectures explore new art results right direction related work work connected following threads work nlp research task oriented neural networks interpreting knowing internal working mechanism neural network easy hobble performance task reached bottleneck recently peters investigate different learning works inuence properties learned tualized representations different work paper focus dissecting neural models text summarization similar work kedzie studies deep learning models perform context selection terms typical marization architectures domains compared work comprehensive study different analytic aspects example additionally investigate able knowledge inuence extractive tion popular neural architecture come inconsistent clusions analyzing auto regressive coder importantly paper shows existing systems improved achieved state art performance cnn dailymail extractive summarization recent work attempt explore different neural nents combinations build end end learning model specically work tiate encoder decoder framework ing recurrent neural networks cheng lapata nallapati zhou encoder auto regressive decoder chen bansal jadhav rajan zhou non auto regressive decoder isonuma narayan arumae liu decoder based pre trained word representations mikolov pennington use transformer extractive summarization missing issue addition work uses reinforcement ing technique narayan chen bansal provide direct optimization goals work improves performance summarization system different perspectives hensive study remains missing testbed text summarization analyze neural summarization system pose build training testing environment different text cases models rstly erated different training settings evaluated different testing tings introduction train testing testbed rst description text rization task description existing methods extractive summarization rectly choose output salient sentences phrases original document formally given document consisting tences objective extract subset tences tic training hyper parameter testing phase additionally sentence tains words generally existing extractive rization systems abstracted ing framework consisting major modules sentence encoder document encoder coder rst sentence encoder utilized convert sentence sentential sentation sentence representations contextualized document encoder finally decoder extract subset tences based contextualized sentence resentations schemas sentence encoder instantiate sentence encoder cnn layer kim explore options sentence encoder strong evidence vious work kedzie shows ferences existing sentence encoder matter nal performance document encoder given sequence sentential representation duty document encoder contextualize sentence obtaining contextualized representations achieve goal investigate lstm based structure transformer structure proven effective achieved state art results nlp tasks notably let model best tural bias stacking deep layers allowed lstm layer long short term memory work lstm proposed hochreiter schmidhuber specically address sue learning long term dependencies proven effective wide range nlp tasks text classication liu semantic matching rocktaschel liu text summarization rush machine translation sutskever transformer layer transformer vaswani essentially feed forward attention architecture achieves pairwise interaction attention mechanism recently transformer achieved great success nlp tasks vaswani dai appealing know neural module performs text summarization task decoder decoder extract subset sentences original document based alized representations ing architecture decoders divide regressive non auto regressive versions investigated paper setup training environment objective step provide typical diverse testing cases models terms model architectures transferable knowledge learning sequence labeling seqlab models formulate extractive summarization task sequence labeling problem equipped non auto regressive decoder formally given document consisting sentences summaries extracted predicting quence label document represents tence document included summaries pointer network pointer representative auto regressive decoder pointer network based decoder shown superior performance tractive summarization chen bansal jadhav rajan pointer network lects sentence attention mechanism glimpse operation vinyals extracts sentence pointer network aware previous predictions external transferable knowledge success neural network based models nlp tasks attributed shift feature engineering structural ing exible ways incorporate external knowledge mikolov peters devlin common form external transferable knowledge rameters pre trained corpora investigate different pre trained models inuence summarization system following pre trained knowledge tion unsupervised transferable knowledge typical unsupervised transferable knowledge explored paper context independent word embeddings mikolov pennington contextualized word embeddings peters devlin state art results new level large number nlp taks recently supervised pre trained knowledge unsupervised pre trained knowledge utilize parameters networks pre trained summarization datasets value tigation know transferability ent dataset achieve rst pre train model newsroom dataset grusky largest datasets tains samples different domains tune model target domains vestigate learning schemas utilizing external knowledge provides way seek new state art results tive introducing extra data additionally ternative way resorting change learning schema model paper plore different learning schemas inuence tractive summarization system comparing pervised learning reinforcement learning setup testing environment testing environment provide sufcient evaluation scenarios internal working mechanism testing models detailed deception rouge following previous work text marization evaluate performance ferent architectures standard rouge scores lin pyrouge cross domain evaluation present domain evaluation testing model evaluated multi domain datasets based cnn dailymail newsroom detail multi domain datasets descried tab repetition design repetition score test different architectures behave diversely avoiding generating unnecessary lengthy peated information use percentage peated grams extracted summary measure word level repetition calculated repn count count number grams uniq eliminate gram cation closer word based repetition score lower repeatability words summary meaningful ground truth distribution positional bias study datasets different affects different chitectures achieve design positional bias describe uniformity ground truth tribution different datasets python org pypi lated posbias divide article parts choose articles cnn dailymail newsroom sentences average denotes probability rst golden label articles sentence length sentence length affect different metrics extent count erage length sentence extracted different decoders explore decoder perceive length information tences sentence shufing attempt explore impact sentence position information ent structures shufe orders sentences observe robustness different architectures order sentences experiment datasets instead evaluating model solely single dataset care testing els perform different types data lows know current models suffer engineering problem domains train valid test cnn dailymail nytimes washingtonpost foxnews theguardian nydailynews wsj usatoday table statistics multi domain datasets based cnn dailymail newsroom cnn dailymail cnn dailymail question answering dataset hermann nallapati commonly summarization dataset consists line news articles paired human generated summaries sentences average data prepossessing use data anonymized version replace named entities newsroom recently newsroom structed grusky contains million articles summaries extracted major news publications years regard diversity sources diversity summarization styles select seven tions largest number data different domains cross domain evaluation large scale data newsroom choose dataset transfer experiment training settings different learning schemas utilize cross entropy loss function reinforcement learning method close chen bansal small difference use precision reward extracted sentence instead value rouge hird columns scope methods interactions different words sentence context independent word representations glove directly utilize initialize words sentence tuned training phase bert truncate article kens feed feature based bert gradient concatenate layers dimensional token embedding passing mlp experimental observations analysis ndings analyses terms architectures external transferable knowledge analysis decoders understand differences decoder pointer seqlab probing behaviours different testing environments domains tab observe models pointer based decoder prone achieving better performance based decoder specically datasets models pointer based decoder perform seqlab domains achieves comparable results domains example nytimes washingtonpost model dec enc cnn nytimes washingtonpost foxnews seqlab pointer lstm transformer lstm transformer lead oracle lead oracle seqlab pointer lstm transformer lstm transformer dec enc theguardian nydailynews wsj usatoday table results different architectures different domains enc dec represent document coder decoder respectively lead means extract rst sentences summary usually competitive lower bound oracle represents ground truth extracted greedy algorithm nallapati usually upper bound number parentheses denotes sentences extracted testing choose lead lower bound domain experiments use obtain word representations theguardian domains pointer passes seqlab improvment attempt explain difference lowing perspectives repetition domains need extract multiple sentences summary rst mains tab pointer aware ous prediction makes reduce plication grams compared seqlab shown fig models pointer higher repetition scores models qlab extracting sentences cates pointer capture word level mation previous selected sentences positive effects subsequent decisions positional bias domains need extract sentence summary mains tab pointer performs better seqlab shown fig mance gap decoders grows positional bias different datasets increases example tab domains low value positional bias seqlab achieves closed performance pointer performance gap grows processing domains value positional bias consequently seqlab sensitive positional bias contrast impairs performance datasets sentence length pointer shows ity capture sentence length information based previous predictions seqlab fig models pointer tend choose longer sentences rst sentence greatly reduce length tence subsequent extractions ison models seqlab tend extract sentences similar length ability allows pointer adaptively change length extracted sentences achieving better performance regardless sentence multiple sentences required analysis encoders section analysis coders lstm transformer different testing environments domains tab following servations transformer outperform lstm datasets nydailynews relatively large margin lstm beats transformer domains closed improvements ing different training phases mains hyper parameters transformer lstm sets layers dimensions pointer layers dimensions seqlab repetition score positional bias average length figure different behaviours decoders seqlab pointer different testing environment shows repetition scores different architectures extracting sentences cnn dailymail shows relationship positional bias abscissa denotes positional bias different datasets denotes average rouge difference decoders different encoders shows average length sentence extracted different architectures nallapati narayan table results transformer seqlab different proportions sentence embedding sitional embedding cnn dailymail input transformer sentence embedding plus tional half table tains models similar performance know positional information models transformer obtains lower crease lstm suggesting transformer robust disentangling testing transformer provides effective way disentangle position tent information enables design cic experiment investigating role positional information plays shown tab dynamically regulate ratio sentence embedding tional embedding coefcients surprisingly utilizing sitional embedding model told sentences document contains vaswani input transformer word embedding plus positional embedding design different proportions carry disentangling test figure results different document encoders pointer normal shufed cnn dailymail denotes decrease performance tences document shufed parameters phenomena suggest lstm easily suffers architecture overtting problem compared transformer additionally experimental setting transformer cient train times faster lstm equipped seqlab decoder obtains better performance pared lstm reason think non local bias wang transformer shufed testing settings shufe orders sentences training set test set keeps unchanged compare models different encoders lstm transformer results seen fig generally signicant drop performance number layers searches sion searches model dec enc baseline glove bert newsroom seqlab pointer lstm transformer lstm transformer table results different architectures different pre trained knowledge cnn dailymail enc dec represent document encoder decoder respectively model achieve comparable existing models contrast positional information moved performance dropped large gin experiment shows success extractive summarization heavily relies ability learning positional information cnn dailymail benchmark dataset current work models chen bansal dong zhou jadhav rajan lstm lstm lstm bert lstm bert analysis transferable knowledge different types transferable knowledge inuences summarization models table evaluation cnn dailymail half table currently state art models lower half models unsupervised pre training line obtain word sentations solely based training set cnn dailymail shown tab independent word representations tribute current models models equipped bert cited observe performances types architectures improved large margin model cnn lstm pointer specically achieved new state art surpassing existing models dramatically supervised pre training cases models benet pre trained ters learned newsroom dataset model cnn lstm pointer fails performance decreased understand phenomenon following explanations transferring process cnn dailymail newsroom suffers domain shift lem distribution golden labels sitions changed observation fig shows cnn lstm pointer sitive ordering change obtaining lower performance bert work investigate ferent ways bert gure bert brought improvement tive summarization system rst usage feed individual tence bert obtain sentence representation contain contextualized tion model gets high score feed entire article bert obtain token representations tence representation mean pooling model performance soared score experiment indicates bert provide powerful sentence embedding key factor extractive summarization textualized information type tion bears positional relationship tences proven critical tractive summarization task learning schema complementarity supervised learning text tion reinforcement learning recently introduce constraints paper explore advanced techniques evaluated anonymized version complementary rst choose based model lstm pointer lstm pointer bert reinforcement learning duced aiming optimize models shown tab observe performance largely improved bert applying ment learning performance improved indicates plementarity architecture transferable knowledge reinforcement learning conclusion different paper seek better understand neural extractive summarization systems benet types model tectures transferable knowledge learning schemas detailed observations provide hints follow researchers design powerful learning frameworks acknowledgment thank jackie chi kit cheung peng qian useful comments discussions like thank anonymous reviewers valuable comments research work ported national natural science foundation china hai municipal science technology sion hai municipal science technology major zjlab references kristjan arumae fei liu reinforced tractive summarization question focused proceedings acl student wards search workshop pages asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents proceedings abstractive summarization conference north american chapter association computational linguistics man language technologies volume long pers volume pages jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings annual meeting association computational linguistics volume long papers volume pages zihang dai zhilin yang yiming yang william cohen jaime carbonell quoc ruslan salakhutdinov transformer language modeling longer term dependency jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language ing arxiv preprint yue dong yikang shen eric crawford herke van hoof jackie chi kit cheung sum extractive summarization contextual dit proceedings conference pirical methods natural language processing pages sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference cal methods natural language processing pages max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies proceedings conference north american chapter association computational linguistics man language technologies volume long pers volume pages karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend advances ral information processing systems pages sepp hochreiter jurgen schmidhuber neural computation long short term memory masaru isonuma toru fujino junichiro mori yutaka matsuo ichiro sakata extractive marization multi task learning document proceedings classication ence empirical methods natural language processing pages aishwarya jadhav vaibhav rajan tive summarization swap net sentences words alternating pointer networks ceedings annual meeting tion computational linguistics volume long papers volume pages yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proceedings annual ing association computational linguistics volume long papers volume pages chris kedzie kathleen mckeown hal daume iii content selection deep learning models proceedings summarization ference empirical methods natural language processing pages yoon kim works sentence classication convolutional neural arxiv preprint technologies volume long papers volume pages chin yew lin rouge package matic evaluation summaries text summarization branches pengfei liu xipeng qiu jifan chen xuanjing huang deep fusion lstms text proceedings annual tic matching meeting association computational guistics volume long papers volume pages pengfei liu xipeng qiu xuanjing huang recurrent neural network text classication multi task learning proceedings ijcai pages pengfei liu xipeng qiu xuanjing huang adversarial multi task learning text tion proceedings annual meeting association computational linguistics ume long papers volume pages tomas mikolov kai chen greg corrado efcient estimation word arxiv preprint frey dean representations vector space ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments thirty aaai conference articial intelligence ramesh nallapati bowen zhou cicero dos santos glar bing xiang tive text summarization sequence sequence rnns conll page shashi narayan shay cohen mirella lapata ranking sentences extractive tion reinforcement learning proceedings conference north american ter association computational linguistics human language technologies volume long pers volume pages romain paulus caiming xiong richard socher deep reinforced model abstractive marization arxiv preprint jeffrey pennington richard socher christopher manning glove global vectors word representation proceedings ence empirical methods natural language cessing emnlp pages matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word resentations proceedings conference north american chapter association computational linguistics human language matthew peters mark neumann luke zettlemoyer wen tau yih dissecting contextual word embeddings architecture representation proceedings conference cal methods natural language processing pages tim rocktaschel edward grefenstette karl moritz hermann tomas phil blunsom reasoning entailment neural attention arxiv preprint alexander rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages abigail peter liu christopher manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers volume pages ilya sutskever oriol vinyals quoc sequence sequence learning neural works advances neural information ing systems pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need advances neural information cessing systems pages oriol vinyals samy bengio manjunath kudlur order matters sequence sequence sets arxiv preprint xiaolong wang ross girshick abhinav gupta kaiming non local neural networks proceedings ieee conference computer vision pattern recognition pages yuxiang baotian learning extract coherent summary deep reinforcement learning thirty second aaai conference articial telligence qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural ment summarization jointly learning score select sentences proceedings annual meeting association computational guistics volume long papers volume pages
