searching for effective neural extractive summarization what works and what s next ming zhong pengfei liu danqing wang xipeng qiu xuanjing huang shanghai key laboratory of intelligent information processing fudan university school of computer science fudan university zhangheng road shanghai china edu abstract the recent years have seen remarkable cess in the use of deep neural networks on text summarization however there is no clear derstanding of why they perform so well or how they might be improved in this paper we seek to better understand how neural extractive summarization systems could benet from ferent types of model architectures able knowledge and learning schemas tionally we nd an effective way to improve current frameworks and achieve the state the art result on cnn dailymail by a large margin based on our observations and ses hopefully our work could provide more clues for future research on extractive marization source code will be available on and our project introduction recent years has seen remarkable success in the use of deep neural networks for text tion see et al celikyilmaz et al jadhav and rajan so far most research utilizing the neural network for text tion has revolved around architecture ing zhou et al chen and bansal gehrmann et al despite their success it remains poorly stood why they perform well and what their comings are which limits our ability to design ter architectures the rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models in this paper we primarily focus on extractive summarization since they are computationally cient and can generate grammatically and ent summaries nallapati et al and seek to these two authors contributed equally corresponding author com fastnlp fastnlp com better understand how neural network based proaches to this task could benet from different types of model architectures transferable edge and learning schemas and how they might be improved architectures architecturally the better mance usually comes at the cost of our standing of the system to date we know little about the functionality of each neural component and the differences between them peters et al which raises the following typical tions how does the choice of different ral architectures cnn rnn transformer ence the performance of the summarization tem which part of components matters for specic dataset do current models suffer from the over engineering problem understanding the above questions can not only help us to choose suitable architectures in different application scenarios but motivate us to move ward to more powerful frameworks external transferable knowledge and ing schemas clearly the improvement in racy and performance is not merely because of the shift from feature engineering to structure neering but the exible ways to incorporate nal knowledge mikolov et al peters et al devlin et al and learning schemas to introduce extra instructive constraints paulus et al arumae and liu for this part we make some rst steps toward answers to the following questions which type of pre trained models supervised or unsupervised pre training is more friendly to the summarization task when architectures are explored exhaustively can we push the state of the art results to a new level by introducing external transferable knowledge or changing another learning schema to make a comprehensive study of above l u j l c s c v v i x r a perspective content sec id learning schemas sup reinforce structure dec enc pointer seqlab lstm transformer knowledge exter glove bert news inter random table outline of our experimental design dec and enc represent decoder and encoder respectively sup denotes supervised learning and news means vised pre training knowledge alytical perspectives we rst build a testbed for summarization system in which training and ing environment will be constructed in the ing environment we design different tion models to analyze how they inuence the performance specically these models differ in the types of architectures encoders cnn lstm transformer vaswani et al coders auto non auto regressive ternal transferable knowledge glove ton et al bert devlin et al newsroom grusky et al and different learning schemas supervised learning and forcement learning to peer into the internal working mechanism of above testing cases we provide sufcient ation scenarios in the testing environment cretely we present a multi domain test sentence shufing test and analyze models by different metrics repetition sentence length and position bias which we additionally developed to provide a better understanding of the characteristics of ferent datasets empirically our main observations are rized as architecturally speaking models with regressive decoder are prone to achieving ter performance against non auto regressive coder besides lstm is more likely to suffer from the architecture overtting problem while transformer is more robust the success of extractive summarization tem on the cnn dailymail heavily relies on the ability to learn positional information of the sentence unsupervised transferable knowledge is more useful than supervised transferable regressive indicates that the decoder can make rent prediction with knowledge of previous predictions edge since the latter one is easily inuenced by the domain shift problem we nd an effective way to improve the rent system and achieving the state of the art sult on cnn dailymail by a large margin with the help of unsupervised transferable knowledge score and this result can be further enhanced by introducing reinforcement learning score hopefully this detailed empirical study can vide more hints for the follow up researchers to design better architectures and explore new of the art results along a right direction related work the work is connected to the following threads of work of nlp research task oriented neural networks interpreting without knowing the internal working mechanism of the neural network it is easy for us to get into a hobble when the performance of a task has reached the bottleneck more recently peters et al investigate how different learning works inuence the properties of learned tualized representations different from this work in this paper we focus on dissecting the neural models for text summarization a similar work to us is kedzie et al which studies how deep learning models perform context selection in terms of several typical marization architectures and domains compared with this work we make a more comprehensive study and give more different analytic aspects for example we additionally investigate how able knowledge inuence extractive tion and a more popular neural architecture former besides we come to inconsistent clusions when analyzing the auto regressive coder more importantly our paper also shows how existing systems can be improved and we have achieved a state of the art performance on cnn dailymail extractive summarization most of recent work attempt to explore different neural nents or their combinations to build an end to end learning model specically these work tiate their encoder decoder framework by ing recurrent neural networks cheng and lapata nallapati et al zhou et al as encoder auto regressive decoder chen and bansal jadhav and rajan zhou et al or non auto regressive decoder isonuma et al narayan et al arumae and liu as decoder based on pre trained word representations mikolov et al pennington et al however how to use transformer in extractive summarization is still a missing issue in addition some work uses reinforcement ing technique narayan et al wu and hu chen and bansal which can provide more direct optimization goals although above work improves the performance of summarization system from different perspectives yet a hensive study remains missing a testbed for text summarization to analyze neural summarization system we pose to build a training testing environment in which different text cases models are rstly erated under different training settings and they are further evaluated under different testing tings before the introduction of our train testing testbed we rst give a description of text rization task description existing methods of extractive summarization rectly choose and output the salient sentences or phrases in the original document formally given a document d dn consisting of n tences the objective is to extract a subset of tences r rm from d m is tic during training while is a hyper parameter in testing phase additionally each sentence tains words generally most of existing extractive rization systems can be abstracted into the ing framework consisting of three major modules sentence encoder document encoder and coder at rst a sentence encoder will be utilized to convert each sentence into a sentential sentation then these sentence representations will be contextualized by a document encoder to finally a decoder will extract a subset of tences based on these contextualized sentence resentations schemas sentence encoder we instantiate our sentence encoder with cnn layer kim we do nt explore other options as sentence encoder since strong evidence of vious work kedzie et al shows that the ferences of existing sentence encoder do nt matter too much for nal performance document encoder given a sequence of sentential representation dn the duty of document encoder is to contextualize each sentence therefore obtaining the contextualized representations sn to achieve this goal we investigate the lstm based structure and the transformer structure both of which have proven to be effective and achieved the state of the art results in many other nlp tasks notably to let the model make the best of its tural bias stacking deep layers is allowed lstm layer long short term memory work lstm was proposed by hochreiter and schmidhuber to specically address this sue of learning long term dependencies which has proven to be effective in a wide range of nlp tasks such as text classication liu et al semantic matching rocktaschel et al liu et al text summarization rush et al and machine translation sutskever et al transformer layer transformer vaswani et al is essentially a feed forward attention architecture which achieves pairwise interaction by attention mechanism recently transformer has achieved great success in many other nlp tasks vaswani et al dai et al and it is appealing to know how this neural module performs on text summarization task decoder decoder is used to extract a subset of sentences from the original document based on alized representations sn most ing architecture of decoders can divide into regressive and non auto regressive versions both of which are investigated in this paper setup for training environment the objective of this step is to provide typical and diverse testing cases models in terms of model architectures transferable knowledge and learning sequence labeling seqlab the models which formulate extractive summarization task as a sequence labeling problem are equipped with non auto regressive decoder formally given a document d consisting of n sentences dn the summaries are extracted by predicting a quence of label yi for the document where yi represents the i tence in the document should be included in the summaries pointer network pointer as a representative of auto regressive decoder pointer network based decoder has shown superior performance for tractive summarization chen and bansal jadhav and rajan pointer network lects the sentence by attention mechanism using glimpse operation vinyals et al when it extracts a sentence pointer network is aware of previous predictions external transferable knowledge the success of neural network based models on nlp tasks can not only be attributed to the shift from feature engineering to structural ing but the exible ways to incorporate external knowledge mikolov et al peters et al devlin et al the most common form of external transferable knowledge is the rameters pre trained on other corpora to investigate how different pre trained models inuence the summarization system we take the following pre trained knowledge into tion unsupervised transferable knowledge two typical unsupervised transferable knowledge are explored in this paper context independent word embeddings mikolov et al pennington et al and contextualized word embeddings peters et al devlin et al have put the state of the art results to new level on a large number of nlp taks recently supervised pre trained knowledge besides unsupervised pre trained knowledge we also can utilize parameters of networks pre trained on other summarization datasets the value of this tigation is to know transferability between ent dataset to achieve this we rst pre train our model on the newsroom dataset grusky et al which is one of the largest datasets and tains samples from different domains then we ne tune our model on target domains that we vestigate learning schemas utilizing external knowledge provides a way to seek new state of the art results from the tive of introducing extra data additionally an ternative way is resorting to change the learning schema of the model in this paper we also plore how different learning schemas inuence tractive summarization system by comparing pervised learning and reinforcement learning setup for testing environment in the testing environment we provide sufcient evaluation scenarios to get the internal working mechanism of testing models next we will make a detailed deception rouge following previous work in text marization we evaluate the performance of ferent architectures with the standard and rouge l scores lin by using pyrouge cross domain evaluation we present a domain evaluation in which each testing model will be evaluated on multi domain datasets based on cnn dailymail and newsroom detail of the multi domain datasets is descried in tab repetition we design repetition score to test how different architectures behave diversely on avoiding generating unnecessary lengthy and peated information we use the percentage of peated n grams in extracted summary to measure the word level repetition which can be calculated as repn where count is used to count the number of grams and uniq is used to eliminate n gram cation the closer the word based repetition score is to the lower the repeatability of the words in summary it is meaningful the ground truth distribution of positional bias to study the whether datasets is different and how it affects different chitectures to achieve this we design a positional bias to describe the uniformity of ground truth tribution in different datasets which can be python org pypi lated as posbias we divide each article into k parts we choose because articles from cnn dailymail and newsroom have sentences by average and denotes the probability that the rst golden label is in part i of the articles sentence length sentence length will affect different metrics to some extent we count the erage length of the k th sentence extracted from different decoders to explore whether the decoder could perceive the length information of tences sentence shufing we attempt to explore the impact of sentence position information on ent structures therefore we shufe the orders of sentences and observe the robustness of different architectures to out of order sentences experiment datasets instead of evaluating model solely on a single dataset we care more about how our testing els perform on different types of data which lows us to know if current models suffer from the over engineering problem domains train valid test cnn dailymail nytimes washingtonpost foxnews theguardian nydailynews wsj usatoday table statistics of multi domain datasets based on cnn dailymail and newsroom cnn dailymail the cnn dailymail question answering dataset hermann et al ed by nallapati et al is commonly used for summarization the dataset consists of line news articles with paired human generated summaries sentences on average for the data prepossessing we use the data with anonymized version as see et al which does nt replace named entities newsroom recently newsroom is structed by grusky et al which contains million articles and summaries extracted from major news publications across years we regard this diversity of sources as a diversity of summarization styles and select seven tions with the largest number of data as different domains to do the cross domain evaluation due to the large scale data in newsroom we also choose this dataset to do transfer experiment training settings for different learning schemas we utilize cross entropy loss function and reinforcement learning method close to chen and bansal with a small difference we use the precision of as a reward for every extracted sentence instead of the value of rouge l hird columns show the scope and methods of interactions for different words wi in a sentence for context independent word representations glove we directly utilize them to initialize our words of each sentence which can be ne tuned during the training phase for bert we truncate the article to kens and feed it to a feature based bert without gradient concatenate the last four layers and get a dimensional token embedding after passing through a mlp experimental observations and analysis next we will show our ndings and analyses in terms of architectures and external transferable knowledge analysis of decoders we understand the differences between decoder pointer and seqlab by probing their behaviours in different testing environments domains from tab we can observe that models with pointer based decoder are prone to achieving better performance against based decoder specically among these eight datasets models with pointer based decoder perform seqlab on six domains and achieves comparable results on the other two domains for example in nytimes washingtonpost model r l r l r l r l dec enc cnn dm nytimes washingtonpost foxnews seqlab pointer lstm transformer lstm transformer lead oracle lead oracle seqlab pointer lstm transformer lstm transformer dec enc theguardian nydailynews wsj usatoday table results of different architectures over different domains where enc and dec represent document coder and decoder respectively lead means to extract the rst k sentences as the summary usually as a competitive lower bound oracle represents the ground truth extracted by the greedy algorithm nallapati et al usually as the upper bound the number k in parentheses denotes k sentences are extracted during testing and choose lead k as a lower bound for this domain all the experiments use to obtain word representations and theguardian domains pointer passes seqlab by at least improvment we attempt to explain this difference from the lowing three perspectives repetition for domains that need to extract multiple sentences as the summary rst two mains in tab pointer is aware of the ous prediction which makes it to reduce the plication of n grams compared to seqlab as shown in fig models with pointer always get higher repetition scores than models with qlab when extracting six sentences which cates that pointer does capture word level mation from previous selected sentences and has positive effects on subsequent decisions positional bias for domains that only need to extract one sentence as the summary last six mains in tab pointer still performs better than seqlab as shown in fig the mance gap between these two decoders grows as the positional bias of different datasets increases for example from the tab we can see in the domains with low value positional bias such as seqlab achieves closed performance against pointer the performance gap grows when processing these domains with value positional bias consequently seqlab is more sensitive to positional bias which by contrast impairs its performance on some datasets sentence length we nd pointer shows the ity to capture sentence length information based on previous predictions while seqlab does nt we can see from the fig that models with pointer tend to choose longer sentences as the rst sentence and greatly reduce the length of the tence in the subsequent extractions in ison it seems that models with seqlab tend to extract sentences with similar length the ability allows pointer to adaptively change the length of the extracted sentences thereby achieving better performance regardless of whether one sentence or multiple sentences are required analysis of encoders in this section we make the analysis of two coders lstm and transformer in different testing environments domains from tab we get the following servations transformer can outperform lstm on some datasets nydailynews by a relatively large margin while lstm beats transformer on some domains with closed improvements besides ing different training phases of these eight mains the hyper parameters of transformer keep while for lstm many sets of layers dimensions for pointer and layers dimensions for seqlab repetition score positional bias average length figure different behaviours of two decoders seqlab and pointer under different testing environment a shows repetition scores of different architectures when extracting six sentences on cnn dailymail shows the relationship between r and positional bias the abscissa denotes the positional bias of six different datasets and r denotes the average rouge difference between the two decoders under different encoders c shows average length of k th sentence extracted from different architectures d d nallapati et al narayan et al r l table results of transformer with seqlab using different proportions of sentence embedding and sitional embedding on cnn dailymail the input of transformer is sentence embedding plus tional the bottom half of the table tains models that have similar performance with former that only know positional information models however transformer obtains lower crease against lstm suggesting that transformer are more robust disentangling testing transformer provides us an effective way to disentangle position and tent information which enables us to design a cic experiment investigating what role positional information plays as shown in tab we dynamically regulate the ratio between sentence embedding and tional embedding by two coefcients and surprisingly we nd even only utilizing sitional embedding the model is only told how many sentences the document contains vaswani et al the input of transformer is word embedding plus positional embedding so we design the above different proportions to carry out the disentangling test figure results of different document encoders with pointer on normal and shufed cnn dailymail r denotes the decrease of performance when the tences in document are shufed parameters are above phenomena suggest that lstm easily suffers from the architecture overtting problem compared with transformer additionally in our experimental setting transformer is more cient to train since it is two or three times faster than lstm when equipped with seqlab decoder former always obtains a better performance pared with lstm the reason we think is due to the non local bias wang et al of transformer shufed testing in this settings we shufe the orders of sentences in training set while test set keeps unchanged we compare two models with different encoders lstm transformer and the results can be seen in fig generally there is signicant drop of performance about these two number of layers searches in and sion searches in model r l r l r l r l dec enc baseline glove bert newsroom seqlab pointer lstm transformer lstm transformer table results of different architectures with different pre trained knowledge on cnn dailymail where enc and dec represent document encoder and decoder respectively our model can achieve on which is comparable to many existing models by contrast once the positional information is moved the performance dropped by a large gin this experiment shows that the success of such extractive summarization heavily relies on the ability of learning the positional information on cnn dailymail which has been a benchmark dataset for most of current work models r l chen and bansal dong et al zhou et al jadhav and rajan lstm pn lstm pn rl lstm pn bert lstm pn bert rl analysis of transferable knowledge next we show how different types of transferable knowledge inuences our summarization models table evaluation on cnn dailymail the top half of the table is currently state of the art models and the lower half is our models unsupervised pre training here as a line is used to obtain word sentations solely based on the training set of cnn dailymail as shown in tab we can nd that independent word representations can not tribute much to current models however when the models are equipped with bert we are cited to observe that the performances of all types of architectures are improved by a large margin the model cnn lstm pointer specically has achieved a new state of the art with on surpassing existing models dramatically supervised pre training in most cases our models can benet from the pre trained ters learned from the newsroom dataset ever the model cnn lstm pointer fails and the performance are decreased we understand this phenomenon by the following explanations the transferring process from cnn dailymail to newsroom suffers from the domain shift lem in which the distribution of golden labels sitions are changed and the observation from fig shows that cnn lstm pointer is more sitive to the ordering change therefore obtaining a lower performance why does bert work we investigate two ferent ways of using bert to gure out from where bert has brought improvement for tive summarization system in the rst usage we feed each individual tence to bert to obtain sentence representation which does not contain contextualized tion and the model gets a high score of however when we feed the entire article to bert to obtain token representations and get the tence representation through mean pooling model performance soared to score the experiment indicates that though bert can provide a powerful sentence embedding the key factor for extractive summarization is textualized information and this type of tion bears the positional relationship between tences which has been proven to be critical to tractive summarization task as above learning schema and complementarity besides supervised learning in text tion reinforcement learning has been recently used to introduce more constraints in this paper we also explore if several advanced techniques be and evaluated on the anonymized version complementary with each other the we and rst choose based model lstm pointer lstm pointer bert then the reinforcement learning are duced aiming to further optimize our models as shown in tab we observe that even though the performance of has been largely improved by bert when applying ment learning the performance can be improved further which indicates that there is indeed a plementarity between architecture transferable knowledge and reinforcement learning conclusion from different in this paper we seek to better understand how neural extractive summarization systems could benet types of model tectures transferable knowledge and learning schemas our detailed observations can provide more hints for the follow up researchers to design more powerful learning frameworks acknowledgment we thank jackie chi kit cheung peng qian for useful comments and discussions we would like to thank the anonymous reviewers for their valuable comments the research work is ported by national natural science foundation of china no and hai municipal science and technology sion and hai municipal science and technology major zjlab references kristjan arumae and fei liu reinforced tractive summarization with question focused in proceedings of acl student wards search workshop pages asli celikyilmaz antoine bosselut xiaodong he and yejin choi deep communicating agents for in proceedings of the abstractive summarization conference of the north american chapter of the association for computational linguistics man language technologies volume long pers volume pages jianpeng cheng and mirella lapata neural summarization by extracting sentences and words in proceedings of the annual meeting of the association for computational linguistics volume long papers volume pages zihang dai zhilin yang yiming yang william w cohen jaime carbonell quoc v le and ruslan salakhutdinov transformer xl language modeling with longer term dependency jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language ing arxiv preprint yue dong yikang shen eric crawford herke van hoof and jackie chi kit cheung sum extractive summarization as a contextual dit in proceedings of the conference on pirical methods in natural language processing pages sebastian gehrmann yuntian deng and alexander rush bottom up abstractive summarization in proceedings of the conference on cal methods in natural language processing pages max grusky mor naaman and yoav artzi newsroom a dataset of million summaries with diverse extractive strategies in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies volume long pers volume pages karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom teaching chines to read and comprehend in advances in ral information processing systems pages sepp hochreiter and jurgen schmidhuber neural computation long short term memory masaru isonuma toru fujino junichiro mori yutaka matsuo and ichiro sakata extractive marization using multi task learning with document in proceedings of the classication ence on empirical methods in natural language processing pages aishwarya jadhav and vaibhav rajan tive summarization with swap net sentences and in words from alternating pointer networks ceedings of the annual meeting of the tion for computational linguistics volume long papers volume pages yen chun chen and mohit bansal fast tive summarization with reinforce selected sentence rewriting in proceedings of the annual ing of the association for computational linguistics volume long papers volume pages chris kedzie kathleen mckeown and hal daume iii content selection in deep learning models of in proceedings of the summarization ference on empirical methods in natural language processing pages yoon kim works for sentence classication convolutional neural arxiv preprint technologies volume long papers volume pages chin yew lin rouge a package for matic evaluation of summaries text summarization branches out pengfei liu xipeng qiu jifan chen and xuanjing huang deep fusion lstms for text in proceedings of the annual tic matching meeting of the association for computational guistics volume long papers volume pages pengfei liu xipeng qiu and xuanjing huang recurrent neural network for text classication with multi task learning in proceedings of ijcai pages pengfei liu xipeng qiu and xuanjing huang adversarial multi task learning for text tion in proceedings of the annual meeting of the association for computational linguistics ume long papers volume pages tomas mikolov kai chen greg corrado and efcient estimation of word arxiv preprint frey dean representations in vector space ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based quence model for extractive summarization of ments in thirty first aaai conference on articial intelligence ramesh nallapati bowen zhou cicero dos santos c a glar and bing xiang tive text summarization using sequence to sequence rnns and beyond conll page shashi narayan shay b cohen and mirella lapata ranking sentences for extractive tion with reinforcement learning in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long pers volume pages romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization arxiv preprint jeffrey pennington richard socher and christopher manning glove global vectors for word representation in proceedings of the ence on empirical methods in natural language cessing emnlp pages matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer deep contextualized word resentations in proceedings of the conference of the north american chapter of the association for computational linguistics human language matthew peters mark neumann luke zettlemoyer and wen tau yih dissecting contextual word embeddings architecture and representation in proceedings of the conference on cal methods in natural language processing pages tim rocktaschel edward grefenstette karl moritz hermann tomas and phil blunsom reasoning about entailment with neural attention arxiv preprint alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of the tence summarization conference on empirical methods in natural guage processing pages abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proceedings of the nual meeting of the association for computational linguistics volume long papers volume pages ilya sutskever oriol vinyals and quoc vv le sequence to sequence learning with neural works in advances in neural information ing systems pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in advances in neural information cessing systems pages oriol vinyals samy bengio and manjunath kudlur order matters sequence to sequence for sets arxiv preprint xiaolong wang ross girshick abhinav gupta and kaiming he non local neural networks in proceedings of the ieee conference on computer vision and pattern recognition pages yuxiang wu and baotian hu learning to extract coherent summary via deep reinforcement learning in thirty second aaai conference on articial telligence qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao neural ment summarization by jointly learning to score and select sentences in proceedings of the annual meeting of the association for computational guistics volume long papers volume pages
