summary renement through denoising nikola i nikolov alessandro calmanovici richard h r hahnloser institute of neuroinformatics university of zurich and eth zurich switzerland niniko dcalma ethz ch l u j l c s c v v i x r a abstract we propose a simple method for processing the outputs of a text rization system in order to rene its all quality our approach is to train to text rewriting models to correct mation redundancy errors that may arise during summarization we train on thetically generated noisy summaries ing three different types of noise that troduce out of context information within each summary when applied on top of extractive and abstractive summarization baselines our summary denoising models yield metric improvements while reducing redundancy introduction text summarization aims to produce a shorter formative version of an input text while tractive summarization only selects important tences from the input abstractive tion generates content without explicitly re using whole sentences nenkova et al in recent years a number of successful approaches have been proposed for both extractive nallapati et al narayan et al and abstractive chen and bansal gehrmann et al rization paradigms despite these successes many state of the art systems remain plagued by overly high output redundancy see et al see ure which we set out to reduce in this paper we propose a simple method ure section for post processing the outputs of a text summarization system in order to prove their overall quality our approach is to train dedicated text to text rewriting models to correct available at ninikolov summary denoising figure overview of our approach to summary denoising we alter ground truth summaries to generate a noisy dataset on which we train denoising models to restore the original summaries errors that may arise during summarization cusing specically on reducing information dancy within each individual summary to achieve this we synthesize from clean summaries noisy summaries that contain diverse information dancy errors such as sentence repetition and of context information section in our experiments section we show that denoising yields metric improvements and reduces redundancy when applied on top of several tive and abstractive baselines the generality of our method makes it a useful post processing step applicable to any summarization system that dardizes the summaries and improves their all quality ensuring fewer redundancies across the text background post processing of noisy human or generated text is a topic that has recently been gathering interest automatic error correction rozovskaya and roth xie et al aims to improve the grammar or spelling of a text in machine translation automatic post ing of translated outputs chatterjee et al is commonly used to further improve the translation quality standardise the translations or adapt them ground truth summarynoisy synthesize noisy summaries denoisingmodel noisemodelclean summarynoisy train apply summary denoising model to a different domain isabelle types of noise in xie et al authors synthesize matically incorrect sentences from correct ones using backtranslation sennrich et al which they use for grammar error correction they enforce hypothesis variety during decoding by adding noise to beam search another work that is close to ours is fevry and phang where authors introduce redundancy on the word level in order to build an unsupervised sentence sion system in this work we take a similar proach but instead focus on generating tion redundancy errors on the sentence rather than the word level approach our approach to summary renement consists of two steps first we use a dataset of clean ground truth summaries to generate noisy summaries ing several different types of synthetic noise ond we train text rewriting models to correct and denoise the noisy summaries restoring them to their original form the learned denoising models are then used to post process and rene the outputs of a summarization system generating noisy summaries to generate noisy datasets we rely on an ing parallel dataset of articles and clean ground truth summaries s sj we iterate over each of the summaries and perturb them with noise according to a sentence noise distribution pnoise pn pnoise denes the bility of adding noise to a specic number of tences within each summary from up to a imum of n noisy sentences with pnoise for all experiments in this work we use pnoise in order to ensure tency meaning that of our noisy summaries contain no noisy sentences while contain initial experiments showed one noisy sentence that distributions which enforce larger or smaller amounts of noise lead to stronger or weaker noising effects our choice of noise distribution showed good results on the majority of systems that we tested we leave a more rigorous tion of the choice of distribution to future work in addition to adding noise we generate noisy summaries for each clean summary by picking multiple random sentences to noise this step creases the dataset size while introducing variety we experiment with three simple types of noise all of which introduce information redundancy into a summary our aim is to train denoising models that minimize repetitive or peripheral formation within summaries repeat picks random sentences from the mary and repeats them at the end repetition of phrases or even whole sentences is a problem commonly observed in text generation with rnns see et al which motivates efforts to detect and minimize repetitions replace picks random sentences from the mary and replaces them with the closest sentence from the article this type of noise helps the model to learn to rene sentences from the generated summaries paraphrasing sentences when they are too long or contain redundant information extra picks random sentences from the article paraphrases them and inserts them into the mary preserving the order of the sentences as they appear in the article with this type of noise a model learns to delete sentences which are out of context or contain redundant tion to paraphrase the sentences we use the tence paraphrasing model from chen and bansal trained on matching sentence pairs from the cnn daily mail dataset mixture mixes all the above noise types formly into a single dataset keeping the same dataset size as for the individual noise types with mixture we explore whether the benets of each noise type can be combined into a single model experimental set up dataset we use the cnn daily mail hermann et al of news articles and maries in the form of bullet points and follow the preprocessing pipeline from chen and bansal we use the standard split of the dataset consisting of news summary pairs for ing and pairs for validation we follow tion to generate noisy versions of the datasets to be used during training during testing instead of clean summaries that contain noisy sentences we input summaries produced by existing tive or abstractive summarization systems com cnn dailymail denoising the lexrank system denoising the rnn ext system figure metric results l and repeat rate on denoising extractive summarization systems the axis in all plots is the number of extracted sentences human is the result of the ground truth summaries only for the repeat rate denoising models for all of our denoising periments we use a standard bidirectional lstm encoder decoder model sutskever et al with hidden units and an attention anism bahdanau et al and train on the subword level sennrich et al capping the vocabulary size to tokens for all we train all models until convergence ing the adam optimizer kingma and ba in addition to our neural denoising els we implement a simple denoising baseline overlap based on unigram overlap between sentences in a summary overlap deletes tences which overlap more than with any other sentence in the summary and can therefore be considered as redundant i si evaluation we report the l rics lin we also report the repeat rate nikolov et al which is the average unigram overlap o of each sentence in a text with the remainder of the text where denotes the complement of sentence since the repeat rate measures the overlapping information across all sentences in a summary lower values signify that a summary contains many unique tences while higher values indicate potential formation repetition or redundancy within a mary use the fairseq library com pytorch fairseq empirically found that this threshold is sufciently high to prevent unnecessary deletion and sufciently low to detect near identical sentences results extractive summarization we experiment with denoising two extractive tems lexrank erkan and radev is an supervised graph based approach which measures the centrality of each sentence with respect to the other sentences in the document rnn ext is a more recent supervised lstm sentence extractor module from chen and bansal trained on the cnn daily mail dataset it extracts sentences from the article sequentially both extractive tems require the number of sentences to be tracted to be given as a hyperparameter in our periments we test with summary lengths ranging from to the results on extractive summarization are in figure for lexrank and figure for ext where we plot the metric scores for varying numbers of extracted sentences for each of the two systems for both lexrank and rnn ext we observe rouge improvements after denoising over the baseline systems without denoising the repeat and replace methods yielded more modest improvements of rouge l points performing comparably to the simple overlap the most effective noise types are baseline extra and mixture yielding improvements of up to rouge l points for lexrank and up to rouge l points for rnn ext the superior performance to overlap indicates that the average sentence count of a summary in the cnn daily mail dataset is rateno denoisingoverlapreplacerepeatextramixturehumanno rateno denoisingoverlapreplacerepeatextramixturehumanno denoisingoverlapreplacerepeatextramixturehuman system human article article rnn rnn rnn rnn rnn rnn rnn rl rnn rl rnn rl rnn rl rnn rl rnn rl denoising approach rouge l repeat mixture overlap repeat replace extra mixture overlap repeat replace extra mixture sent tok table results on denoising abstractive summarization repeat is the repeat rate while sent and tok are the average numbers of sentences or tokens in the summaries best rouge results for each model are in bold human is the result of the ground truth summaries while article uses the original article as the summary figure number of sentence repetitions before and after denoising tional denoising operations learned by our models see figure are benecial and can lead to more polished summaries that also may contain tive elements the gains from denoising are greater for longer summaries of more than two sentences long summaries are more likely to be affected by dancy for shorter summaries denoising might lead to deletion of important information thus noising needs to be applied more carefully in such cases furthermore for all sentence lengths and noise types we observe a reduction in the peat rate after denoising demonstrating that our approach is effective at reducing redundancy in table we additionally include the result from using the whole articles article as input to our mixture model denoising is effective in this case indicating that our approach may be promising for developing abstractive tion systems that are fully unsupervised similar to recent work in unsupervised sentence compression fevry and phang abstractive summarization for abstractive summarization we test two tems the rst is a standard lstm decoder model with an attention mechanism rnn identical to our denoising network from the second rnn rl is a section of the art abstractive system proposed in chen and bansal that combines extractive and abstractive summarization using reinforcement learning we train rnn ourselves while for rl we use the outputs provided by the authors our metric results from denoising abstractive summarization are in table in figure we also compute the approximate number of sentence etitions on the test set by calculating the number of sentences that overlap signicantly with at least one other sentence in the summary for the rnn model the repeat noise helps to remove repetition halving our repetition ric while boosting the rouge scores this sult is similar to our much simpler overlap baseline based on sentence deletion the other noise types help to reduce redundancy bringing the repeat rate closer to that of human maries this however comes at the cost of a decrease in rouge for rnn rl while ing helps to reduce repetition none of our noise types managed to yield rouge improvements one reason for this may be that this model ready comes with a built in mechanism for ing redundancy which relies on sentence ing chen and bansal however as shown in figure and in our example in table this model still generates many more sentence in tions than found in human summaries all our approach is effective at reducing dant information in abstractive summaries of repeating sentencesno denoisingmixtureno denoisingmixturehumanrnnrnn rlhuman a rnn ext extractive system extracting sentences rnn abstractive system figure types of denoising operations applied to an extractive left and an abstractive right system averaged over our test set ever this comes with a potential loss of tion which can lead to a reduction in rouge thus our denoising methods are currently better suited for extractive than for absctractive rization our work therefore calls for the ment of novel types of synthetic noise that target abstractive summarization system replace noise where dinorah santana the player s agent said her client had rejected the offer of a three year contract extension is paraphrased to the player s agent said she had rejected the offer of a three year contract or even a combination of deletion and rewriting e rnn rl system repeat noise analysis of model outputs conclusion in figure we quantify the types of operations deletion or modication of one or more tences or no change our denosing models formed on the summaries produced by the tive rnn ext figure and abstractive rnn tem figure the replace and repeat noises are the most conservative leaving over of the summaries unchanged extra is the most prone to delete sentences while repeat and replace are most prone to modify sentences we see a similar pattern for both extractive and stractive summarization with an increase of tion for longer summaries produced by the tive system this indicates that our approach bly learns to switch between operations depending on the properties of the noisy input summary in table we show example outputs from noising extractive and abstractive summaries duced for a sports article from our test set all baseline summarization systems produced outputs for example the rst that contain redundancy three sentences generated by the rnn system and the and sentences produced by the rl system are almost identical to denoise the summaries our models used diverse operations such as deletion of one or two sentences e rnn system repeat noise rewriting e rnn rl we proposed a general framework for improving the outputs of a text summarization system based on denoising our approach is independent of the type of the system and is applicable to both stractive and extractive summarization paradigms it could be useful as a post processing step in a text summarization pipeline ensuring that the maries meet specic standards related to length or quality our approach is effective at reducing tion repetition present in existing summarization systems and can even lead to rouge ments especially for extractive summarization denoising abstractive summarization proved to be more challenging and our simple noise types did not yield signicant rouge improvements for a state of the art system our focus in future work will therefore be to estimate better models of the noise present in abstractive summarization to reduce information redundancy without a loss in quality as well as to target other aspects such as the grammaticality or cohesion of the summary extrano changedeletionmodificationdeletion of of outputsmixtureextrano changedeletionmodificationdeletion of of outputsmixture ground truth dani alves has spent seven seasons with the catalan giants alves has four spanish titles to his name with barcelona the brazil defender has also won the champions league twice with barca rnn rnn rnn rl the brazilian has been unable to no denoising dani alves has been unable to agree a new deal with catalan club agree a new deal with catalan club alves has been unable to agree a new deal with catalan club alves has been linked with a ber of clubs including manchester united and manchester city set to leave no denoising dani alves looks barcelona this summer alves has enjoyed seven successful years at barcelona alves has been unable to agree a deal with the catalan club agree a new deal dinorah santana the player s agent said her client had rejected the offer of a three year contract extension the year old has been unable to no denoising set to leave looks dani alves barcelona this summer after his sentative conrmed the brazilian back had rejected the club s nal tract offer alves has enjoyed seven successful years at barcelona winning four ish titles and the champions league twice but the year old has been unable to agree a new deal with the catalan club and will leave the nou camp this summer dinorah santana the player s agent and ex wife said at a press ence on thursday that her client had rejected the offer of a three year tract extension which was dependent on the player taking part in per cent of matches for the club replace same same same the player s agent and ex wife said at a press conference on thursday that her client had rejected the offer of a three year contract extension repeat same same same same extra same same the year old has been unable to agree a new deal with the catalan club and will leave the nou camp this mer deleted mixture same same same deleted replace same same same same repeat same deleted deleted same extra same same same deleted replace same same same same the player s agent said she had jected the offer of a three year contract repeat same same deleted alves has been unable to agree a new deal same extra same same same same deleted mixture same deleted deleted same mixture same same same same deleted table examples for denoising extractive and abstractive summarization same indicates a summary sentence has been unchanged while deleted indicates sentence deletion in brackets denotes the score while rep denotes the repeat rate acknowledgments we acknowledge support from the swiss national science foundation grant references dzmitry bahdanau kyunghyun cho and yoshua gio neural machine translation by jointly arxiv preprint learning to align and translate rajen chatterjee matteo negri raphael rubino and marco turchi findings of the wmt shared task on automatic post editing in ings of the third conference on machine tion shared task papers pages yen chun chen and mohit bansal fast tive summarization with reinforce selected sentence rewriting in proceedings of acl gunes erkan and dragomir r radev lexrank graph based lexical centrality as salience in text summarization journal of articial intelligence search thibault fevry and jason phang vised sentence compression using denoising in proceedings of the conference encoders on computational natural language learning sociation for computational linguistics pages org anthology sebastian gehrmann yuntian deng and alexander rush bottom up abstractive summarization in proceedings of the conference on cal methods in natural language processing ciation for computational linguistics pages org anthology karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom teaching chines to read and comprehend in advances in ral information processing systems pages p isabelle domain adaptation of mt systems through automatic postediting proc machine translation summit mt summit xi diederick p kingma and jimmy ba adam a method for stochastic optimization in international conference on learning representations iclr chin yew lin rouge a package for matic evaluation of summaries text summarization branches out ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based quence model for extractive summarization of ments in thirty first aaai conference on articial intelligence the conference of shashi narayan shay b cohen and mirella ata ranking sentences for extractive in marization with reinforcement learning ceedings of the north american chapter of the association for putational linguistics human language nologies volume long papers association for computational linguistics pages ani nenkova sameer maskey and yang liu in proceedings of the automatic summarization annual meeting of the association for putational linguistics tutorial abstracts of acl association for computational linguistics page nikola nikolov michael pfeiffer and richard loser data driven summarization of tic articles in proceedings of the eleventh tional conference on language resources and uation lrec european language resources association elra paris france alla rozovskaya and dan roth grammatical error correction machine translation and classiers in proceedings of the annual meeting of the association for computational linguistics volume long papers volume pages abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proceedings of the nual meeting of the association for computational linguistics volume long papers volume pages rico sennrich barry haddow and alexandra birch improving neural machine translation els with monolingual data in proc of acl ciation for computational linguistics pages rico sennrich barry haddow and alexandra birch neural machine translation of rare words in proc of acl association with subword units for computational linguistics pages ilya sutskever oriol vinyals and quoc vv le sequence to sequence learning with neural works in advances in neural information ing systems pages ziang xie guillaume genthial stanley xie andrew ng and dan jurafsky noising and denoising natural language diverse backtranslation for mar correction in proceedings of the ence of the north american chapter of the ation for computational linguistics human guage technologies volume long papers sociation for computational linguistics pages
