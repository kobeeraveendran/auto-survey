summary renement denoising nikola nikolov alessandro calmanovici richard hahnloser institute neuroinformatics university zurich eth zurich switzerland niniko dcalma ethz abstract propose simple method processing outputs text rization system order rene quality approach train text rewriting models correct mation redundancy errors arise summarization train thetically generated noisy summaries ing different types noise troduce context information summary applied extractive abstractive summarization baselines summary denoising models yield metric improvements reducing redundancy introduction text summarization aims produce shorter formative version input text tractive summarization selects important tences input abstractive tion generates content explicitly sentences nenkova recent years number successful approaches proposed extractive nallapati narayan abstractive chen bansal gehrmann rization paradigms despite successes state art systems remain plagued overly high output redundancy ure set reduce paper propose simple method ure section post processing outputs text summarization system order prove overall quality approach train dedicated text text rewriting models correct available ninikolov summary denoising figure overview approach summary denoising alter ground truth summaries generate noisy dataset train denoising models restore original summaries errors arise summarization cusing specically reducing information dancy individual summary achieve synthesize clean summaries noisy summaries contain diverse information dancy errors sentence repetition context information section experiments section denoising yields metric improvements reduces redundancy applied tive abstractive baselines generality method makes useful post processing step applicable summarization system dardizes summaries improves quality ensuring fewer redundancies text background post processing noisy human generated text topic recently gathering interest automatic error correction rozovskaya roth xie aims improve grammar spelling text machine translation automatic post ing translated outputs chatterjee commonly improve translation quality standardise translations adapt ground truth summarynoisy synthesize noisy summaries denoisingmodel noisemodelclean summarynoisy train apply summary denoising model different domain isabelle types noise xie authors synthesize matically incorrect sentences correct ones backtranslation sennrich use grammar error correction enforce hypothesis variety decoding adding noise beam search work close fevry phang authors introduce redundancy word level order build unsupervised sentence sion system work similar proach instead focus generating tion redundancy errors sentence word level approach approach summary renement consists steps use dataset clean ground truth summaries generate noisy summaries ing different types synthetic noise ond train text rewriting models correct denoise noisy summaries restoring original form learned denoising models post process rene outputs summarization system generating noisy summaries generate noisy datasets rely ing parallel dataset articles clean ground truth summaries iterate summaries perturb noise according sentence noise distribution pnoise pnoise denes bility adding noise specic number tences summary imum noisy sentences pnoise experiments work use pnoise order ensure tency meaning noisy summaries contain noisy sentences contain initial experiments showed noisy sentence distributions enforce larger smaller amounts noise lead stronger weaker noising effects choice noise distribution showed good results majority systems tested leave rigorous tion choice distribution future work addition adding noise generate noisy summaries clean summary picking multiple random sentences noise step creases dataset size introducing variety experiment simple types noise introduce information redundancy summary aim train denoising models minimize repetitive peripheral formation summaries repeat picks random sentences mary repeats end repetition phrases sentences problem commonly observed text generation rnns motivates efforts detect minimize repetitions replace picks random sentences mary replaces closest sentence article type noise helps model learn rene sentences generated summaries paraphrasing sentences long contain redundant information extra picks random sentences article paraphrases inserts mary preserving order sentences appear article type noise model learns delete sentences context contain redundant tion paraphrase sentences use tence paraphrasing model chen bansal trained matching sentence pairs cnn daily mail dataset mixture mixes noise types formly single dataset keeping dataset size individual noise types mixture explore benets noise type combined single model experimental set dataset use cnn daily mail hermann news articles maries form bullet points follow preprocessing pipeline chen bansal use standard split dataset consisting news summary pairs ing pairs validation follow tion generate noisy versions datasets training testing instead clean summaries contain noisy sentences input summaries produced existing tive abstractive summarization systems com cnn dailymail denoising lexrank system denoising rnn ext system figure metric results repeat rate denoising extractive summarization systems axis plots number extracted sentences human result ground truth summaries repeat rate denoising models denoising periments use standard bidirectional lstm encoder decoder model sutskever hidden units attention anism bahdanau train subword level sennrich capping vocabulary size tokens train models convergence ing adam optimizer kingma addition neural denoising els implement simple denoising baseline overlap based unigram overlap sentences summary overlap deletes tences overlap sentence summary considered redundant evaluation report rics lin report repeat rate nikolov average unigram overlap sentence text remainder text denotes complement sentence repeat rate measures overlapping information sentences summary lower values signify summary contains unique tences higher values indicate potential formation repetition redundancy mary use fairseq library com pytorch fairseq empirically found threshold sufciently high prevent unnecessary deletion sufciently low detect near identical sentences results extractive summarization experiment denoising extractive tems lexrank erkan radev supervised graph based approach measures centrality sentence respect sentences document rnn ext recent supervised lstm sentence extractor module chen bansal trained cnn daily mail dataset extracts sentences article sequentially extractive tems require number sentences tracted given hyperparameter periments test summary lengths ranging results extractive summarization figure lexrank figure ext plot metric scores varying numbers extracted sentences systems lexrank rnn ext observe rouge improvements denoising baseline systems denoising repeat replace methods yielded modest improvements rouge points performing comparably simple overlap effective noise types baseline extra mixture yielding improvements rouge points lexrank rouge points rnn ext superior performance overlap indicates average sentence count summary cnn daily mail dataset rateno denoisingoverlapreplacerepeatextramixturehumanno rateno denoisingoverlapreplacerepeatextramixturehumanno denoisingoverlapreplacerepeatextramixturehuman system human article article rnn rnn rnn rnn rnn rnn rnn rnn rnn rnn rnn rnn denoising approach rouge repeat mixture overlap repeat replace extra mixture overlap repeat replace extra mixture sent tok table results denoising abstractive summarization repeat repeat rate sent tok average numbers sentences tokens summaries best rouge results model bold human result ground truth summaries article uses original article summary figure number sentence repetitions denoising tional denoising operations learned models figure benecial lead polished summaries contain tive elements gains denoising greater longer summaries sentences long summaries likely affected dancy shorter summaries denoising lead deletion important information noising needs applied carefully cases furthermore sentence lengths noise types observe reduction peat rate denoising demonstrating approach effective reducing redundancy table additionally include result articles article input mixture model denoising effective case indicating approach promising developing abstractive tion systems fully unsupervised similar recent work unsupervised sentence compression fevry phang abstractive summarization abstractive summarization test tems rst standard lstm decoder model attention mechanism rnn identical denoising network second rnn section art abstractive system proposed chen bansal combines extractive abstractive summarization reinforcement learning train rnn use outputs provided authors metric results denoising abstractive summarization table figure compute approximate number sentence etitions test set calculating number sentences overlap signicantly sentence summary rnn model repeat noise helps remove repetition halving repetition ric boosting rouge scores sult similar simpler overlap baseline based sentence deletion noise types help reduce redundancy bringing repeat rate closer human maries comes cost decrease rouge rnn ing helps reduce repetition noise types managed yield rouge improvements reason model ready comes built mechanism ing redundancy relies sentence ing chen bansal shown figure example table model generates sentence tions found human summaries approach effective reducing dant information abstractive summaries repeating sentencesno denoisingmixtureno denoisingmixturehumanrnnrnn rlhuman rnn ext extractive system extracting sentences rnn abstractive system figure types denoising operations applied extractive left abstractive right system averaged test set comes potential loss tion lead reduction rouge denoising methods currently better suited extractive absctractive rization work calls ment novel types synthetic noise target abstractive summarization system replace noise dinorah santana player agent said client rejected offer year contract extension paraphrased player agent said rejected offer year contract combination deletion rewriting rnn system repeat noise analysis model outputs conclusion figure quantify types operations deletion modication tences change denosing models formed summaries produced tive rnn ext figure abstractive rnn tem figure replace repeat noises conservative leaving summaries unchanged extra prone delete sentences repeat replace prone modify sentences similar pattern extractive stractive summarization increase tion longer summaries produced tive system indicates approach bly learns switch operations depending properties noisy input summary table example outputs noising extractive abstractive summaries duced sports article test set baseline summarization systems produced outputs example rst contain redundancy sentences generated rnn system sentences produced system identical denoise summaries models diverse operations deletion sentences rnn system repeat noise rewriting rnn proposed general framework improving outputs text summarization system based denoising approach independent type system applicable stractive extractive summarization paradigms useful post processing step text summarization pipeline ensuring maries meet specic standards related length quality approach effective reducing tion repetition present existing summarization systems lead rouge ments especially extractive summarization denoising abstractive summarization proved challenging simple noise types yield signicant rouge improvements state art system focus future work estimate better models noise present abstractive summarization reduce information redundancy loss quality target aspects grammaticality cohesion summary extrano changedeletionmodificationdeletion outputsmixtureextrano changedeletionmodificationdeletion outputsmixture ground truth dani alves spent seven seasons catalan giants alves spanish titles barcelona brazil defender won champions league twice barca rnn rnn rnn brazilian unable denoising dani alves unable agree new deal catalan club agree new deal catalan club alves unable agree new deal catalan club alves linked ber clubs including manchester united manchester city set leave denoising dani alves looks barcelona summer alves enjoyed seven successful years barcelona alves unable agree deal catalan club agree new deal dinorah santana player agent said client rejected offer year contract extension year old unable denoising set leave looks dani alves barcelona summer sentative conrmed brazilian rejected club nal tract offer alves enjoyed seven successful years barcelona winning ish titles champions league twice year old unable agree new deal catalan club leave nou camp summer dinorah santana player agent wife said press ence thursday client rejected offer year tract extension dependent player taking cent matches club replace player agent wife said press conference thursday client rejected offer year contract extension repeat extra year old unable agree new deal catalan club leave nou camp mer deleted mixture deleted replace repeat deleted deleted extra deleted replace player agent said jected offer year contract repeat deleted alves unable agree new deal extra deleted mixture deleted deleted mixture deleted table examples denoising extractive abstractive summarization indicates summary sentence unchanged deleted indicates sentence deletion brackets denotes score rep denotes repeat rate acknowledgments acknowledge support swiss national science foundation grant references dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly arxiv preprint learning align translate rajen chatterjee matteo negri raphael rubino marco turchi findings wmt shared task automatic post editing ings conference machine tion shared task papers pages yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proceedings acl gunes erkan dragomir radev lexrank graph based lexical centrality salience text summarization journal articial intelligence search thibault fevry jason phang vised sentence compression denoising proceedings conference encoders computational natural language learning sociation computational linguistics pages org anthology sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference cal methods natural language processing ciation computational linguistics pages org anthology karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend advances ral information processing systems pages isabelle domain adaptation systems automatic postediting proc machine translation summit summit diederick kingma jimmy adam method stochastic optimization international conference learning representations iclr chin yew lin rouge package matic evaluation summaries text summarization branches ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments thirty aaai conference articial intelligence conference shashi narayan shay cohen mirella ata ranking sentences extractive marization reinforcement learning ceedings north american chapter association putational linguistics human language nologies volume long papers association computational linguistics pages ani nenkova sameer maskey yang liu proceedings automatic summarization annual meeting association putational linguistics tutorial abstracts acl association computational linguistics page nikola nikolov michael pfeiffer richard loser data driven summarization tic articles proceedings eleventh tional conference language resources uation lrec european language resources association elra paris france alla rozovskaya dan roth grammatical error correction machine translation classiers proceedings annual meeting association computational linguistics volume long papers volume pages abigail peter liu christopher manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers volume pages rico sennrich barry haddow alexandra birch improving neural machine translation els monolingual data proc acl ciation computational linguistics pages rico sennrich barry haddow alexandra birch neural machine translation rare words proc acl association subword units computational linguistics pages ilya sutskever oriol vinyals quoc sequence sequence learning neural works advances neural information ing systems pages ziang xie guillaume genthial stanley xie andrew dan jurafsky noising denoising natural language diverse backtranslation mar correction proceedings ence north american chapter ation computational linguistics human guage technologies volume long papers sociation computational linguistics pages
