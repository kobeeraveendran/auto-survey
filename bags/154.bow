entity commonsense representation neural abstractive summarization reinald kim amplayo seonjae lim seung won hwang yonsei university seoul south korea rktamplayo sun lim abstract major proportion text summary includes important entities found original text entities build topic mary hold commonsense formation linked knowledge base based observations investigates usage linked entities guide decoder neural text summarizer generate concise better summaries end leverage shelf entity linking system els extract linked entities propose module easily attachable sequence sequence model transforms list entities vector representation topic mary current available els ciently effective possibly introducing solved ambiguities irrelevant entities resolve imperfections els coding entities selective disambiguation pooling entity vectors tion applying simple sequence model attention mechanism base model signicant ments performance gigaword sentence title cnn long document multi sentence highlights summarization datasets rouge points introduction text summarization task generate shorter concise version text preserving meaning original text task vided subtask based approach tractive abstractive summarization tive summarization task create summaries pulling snippets text form nal text combining form summary abstractive summarization asks generate maries scratch restriction use amplayo lim authors equal tribution names arranged alphabetically figure observations linked entities summaries summaries mainly composed entities entities represent topic mary entity commonsense learned large corpus available words original text limitations extractive summarization incoherent texts unnatural methodology yao research trend shifted abstractive summarization sequence sequence models sutskever attention mechanism bahdanau found great success generating stractive summaries single sentence chopra long document multiple sentences chen generating summaries necessary determine main topic sift essary information omitted sequence models tendency include information relevant found original text result cise summaries concentrates wrongly relevant topics problem especially severe summarizing longer texts paper propose use entities found original text infer summary topic los angeles dodgersacquired south koreanright hander jae seofrom new york metson wednesdayin player swap input textkoreasseoheaded dodgersfrom metssummarytopic entitydistributionlos angeles dodgers korean woongnew york mets gating aforementioned problem specically leverage linked entities extracted ploying readily available entity linking system importance linked entities rization intuitive explained ing figure example ure aside auxiliary words construct tence summary mainly composed linked entities extracted original text second depict main topic mary probability distribution relevant ties list entities finally leverage entity commonsense learned separate large knowledge base wikipedia end present method fectively apply linked entities sequence sequence models called module easily attached sequence sequence based summarization model module encodes entities extracted original text entity linking system els constructs vector representing topic summary generated informs decoder constructed topic vector imperfections current els extracted linked entities ambiguous coarse considered relevant summary solve issue entity encoders lective disambiguation constructing topic vectors attention experiment datasets gigaword cnn varying lengths ing module sequence sequence model attention mechanism signicantly increases performance datasets compared state art models dataset model obtains comparable mance gigaword dataset texts short outperforms competing models cnn dataset texts longer thermore provide analysis model effectively uses extracted linked entities duce concise better summaries usefulness linked entities summarization subsections present detailed guments empirical previously examined evidences observations possible issues linked entities extracted entity linking system els generating abstractive summaries purpose use opment sets gigaword dataset provided rush cnn dataset vided hermann mental data quantitative evidence refer readers figure running example observations discussed section tions usefulness linked entities abstractive summarization summaries mainly composed linked entities extracted original text ample seen summary contains words refer different entities fact noun phrases summary mention linked entity experimental data tract linked entities original text pare noun phrases found mary report noun phrases gigaword cnn datasets spectively contain linked entity conrms observation second linked entities represent topic summary dened multinomial distribution entities graphically shown example probabilities refer relevance entities entities viously represent topics newman utilized controlled cabulary main topics document hulpus example tity jae seo relevant subject summary entity south korean relevant tant constructing summary use entity sense learned continuous vector representation separate larger corpus yamada ample know entities los les dodgers new york mets american baseball teams jae seo baseball player associated teams use formation generate coherent summaries extracted linked ties covered pre trained experimental data proving observation com idio possible issues despite usefulness linked entities extracted els issues low precision rates hasibi design challenges training datasets ling issues summarized parts ambiguity coarseness extracted entities ambiguous example entity south korean biguous refer south korean person south korean language experimental data tract entities based frequency entities extracted randomly selected texts check biguation pages wikipedia discover entities entities picked random disambiguation pages shows entities prone ambiguity problems second linked entities mon considered entity duce errors irrelevance summary example wednesday erroneous wrongly linked entity wednesday night baseball swap irrelevant linked correctly entity trade sports common irrelevant generating summaries experimental data randomly select data instances tag correctness relevance extracted tities labels correct evant correct somewhat relevant rect irrelevant incorrect results tagged respectively shows large incorrect vant entities model solve issues described present module easily attached sequence sequence based stractive summarization model encodes linked entities extracted text forms single topic vector vector ultimately concatenated decoder hidden state vectors module contains ules specically issues presented tity linking systems entity encoding ule selective disambiguation pooling submodule attention overall architecture illustrated figure consists entity ing system els sequence sequence tention mechanism model module note proposed module ily attached sophisticated abstractive marization models zhou tan based traditional decoder framework consequently produce better results code base model available base model base model employ basic decoder rnn neural machine lation bahdanau text tion nallapati tasks employ layer bidirectional gru bigru current unit encoder bigru consists forward backward gru results sequences forward backward hidden states tively gru gru forward backward hidden states concatenated hidden state vectors tokens nal states forward backward gru nated create nal text representation vector encoder values calculated layer second layer rst layer nal text representation vectors projected fully connected layer passed decoder initial hidden states decoder use layer directional gru attention time step previous token previous hidden state previous context vector passed gru calculate new hidden state shown equation gru wikipedia org wiki south com korean figure architecture proposed sequence sequence model module context vector computed additive attention mechanism bahdanau matches current decoder state encoder state importance score scores passed softmax pool encoder states weighted sum nal pooled vector context vector shown equations globally disambiguating encoder way disambiguate entity entities putting importance entities nearer purpose employ based model globally disambiguate entities specically use bigru concatenate forward backward hidden state vectors new entity vector uahi ihi finally previous token current context vector current decoder state generate current word softmax layer decoder vocabulary shown wcct wsst sof entity encoding submodule performing entity linking input text ing els receive sequential list linked entities arranged based location text embed entities dimensional vectors entities contain ambiguity necessary resolve applying base model based idea ambiguous entity disambiguated neighboring entities introduce kinds disambiguating encoders gru gru locally disambiguating encoder way disambiguate entity rect neighbors entity putting importance value entities far ploy cnn based model locally disambiguate entities specically convolution operation lter matrices rhd lter size window words different sizes produces new ture vectors shown non linear function convolution operation reduces number entities differently depending lter size prevent loss information produce feature vectors pad entity list dynamically lter size number paddings lter size refers number entities disambiguate middle entity finally concatenate feature vectors los angeles dodgers acquired south korean right hander jae seofrom new york mets wednesday player swap input textentity linking systemthelosangelesdodgersacquiredsouth sequence sequence attention start koreasseokoreasseoheaded attention mechanismbi encoder selective modulepooling firm attention mation wxei entity encoding submodule trated figure ultimately submodule outputs disambiguated entity vectors pooling submodule entity vectors pooled create gle topic vector represents topic summary possible pooling technique use soft attention vectors determine importance value vector matching entity vector text vector text encoder context vector entity vectors pooled weighted sum problem soft tention considers entity vectors constructing topic vector tities important necessary ing summaries number tities erroneous irrelevant reported section soft attention gives non negligible important scores entities adds essary noise construction topic vector pooling submodule instead uses tention mechanism consider entities constructing topic vector differentiable way follows uas sparse aiei functions gets indices vectors sparse creates sparse vector values sparse vector added original tance score vector create new importance use represent figure entity encoding submodule selective disambiguation applied entity left ure represents submodule right gure represents choices disambiguating encoders different new entity vector question disambiguating encoder better debate argued local context appropriate lau claimed additionally global context helps wang rnn based encoder good smartly makes use entities perform bad entities introduces noise far entity disambiguation cnn based encoder good minimizes noise totally ignoring far entities ambiguating determining ate lter sizes needs engineering overall argue input text short tence encoders perform comparably wise input text long document cnn based encoder performs better selective disambiguation obvious entities need disambiguated correctly linked adequately biguated entity disambiguated entity context specic suitable summarization task tity encoding submodule uses selective mechanism decides use biguating encoder ducing selective disambiguation gate nal entity vector calculated linear globally disambiguating encoder disambiguating encoder score vector new vector important scores non entities softmax plied gives small negligible zero values non entities value depends lengths input text mary increases ity attention soft attention cide empirically section extending base model module extends base model follows nal text representation vector context vector constructing topic vector pooling submodule topic vector concatenated decoder hidden state vectors concatenated vector nally create output vector wcci related work recent success neural network els competitive results stractive summarization neural attention model rst applied task easily achieving art performance multiple datasets rush model extended instead use recurrent neural network decoder chopra model tended use rnn encoder decoder work enhancements lexical statistical features nallapati current state art performance achieved selectively encoding words process ing salient information zhou neural abstractive summarization models explored summarize longer ments word extraction models ously explored performing worse sentence extraction models cheng lapata erarchical attention based recurrent neural works applied task owing idea multiple sentences ument nallapati finally based models proposed enable models traverse text content grasp overall meaning chen current state art performance achieved graph based attentional neural model considering key tors document summarization saliency uency novelty tan dataset gigaword cnn table dataset statistics previous studies summarization tasks entities preprocessing stage anonymize dataset nallapati mitigate vocabulary problems tan linked entities summarization properly explored rst use linked entities improve performance summarizer experimental settings datasets use widely tion datasets different text lengths use annotated english gigaword dataset rush dataset receives rst sentence news article input use headline title gold standard mary development dataset large randomly selected pairs development dataset use held test dataset rush comparison second use cnn dataset released hermann dataset receives news cle input use human generated multiple sentence highlight gold standard summary original dataset modied processed specically document rization task nallapati addition previously provided datasets extract linked entities ceccarelli open source els links text snippets found given text entities contained wikipedia use default recommended parameters stated website summarize statistics datasets table implementation datasets reduce size input output entity cabularies suggested replace frequent words isti cnr unk use pennington pre trained tors initialize word entity vectors grus set state size cnn set feature maps respectively attention tuned culating perplexity model starting smaller values stopping perplexity model comes worse previous model liminary tuning showed gigaword dataset cnn dataset best choices use dropout srivastava non linear connections dropout rate set batch sizes gigaword cnn datasets respectively training stochastic gradient descent mini batches adadelta update rule constraint hinton perform early stopping subset given development dataset use beam search size generate summary baselines gigaword dataset pare models following abstractive baselines rush tuned version abs uses attentive cnn coder nnlm decoder ati rnn sequence sequence model lexical statistical features encoder luong nmt luong layer lstm encoder decoder model elman chopra uses attentive cnn encoder elman rnn decoder seass zhou uses bigru encoders gru decoders selective encoding cnn dataset compare models following extractive abstractive baselines strong baseline extracts rst sentences document summary lexrank extracts texts lexrank erkan radev gru non hierarchical layer sequence sequence abstractive line distraction chen uses sequence sequence abstractive model distraction based networks gba tan graph based attentional neural tive model baseline results beam search gathered previous papers stanford edu com idio model base luong nmt ras elman seass table results gigaword dataset length variants rouge model base lexrank gru distraction gba table results cnn dataset length rouge metric compare nal model base model base variants model selective disambiguation soft tention results report rouge scores datasets competing models rouge scores lin report results gigaword cnn dataset table ble respectively gigaword dataset texts short best model achieves ble performance current state art cnn dataset texts longer best model outperforms previous models emphasize module easily attachable better models expect improve model gold base mean table human evaluations gigaword dataset bold faced values best red colored values worst values evaluation metric performance overall achieves signicant improvement baseline model base points increase gigaword dataset points increase cnn dataset fact variants gain improvements baseline plying leveraging linked entities improves performance summarizer model variants cnn based encoder lective disambiguation attention performs best automatic evaluation gigaword dataset shows cnn rnn variants similar performance break tie models conduct man evaluation gigaword dataset struct annotators read input sentence rank competing summaries rst according relevance uency original summary gold models base compute proportion ing model mean rank model results reported table model best mean rank followed gold base respectively perform anova post hoc tukey tests cnn ant signicantly better rnn variant base model rnn variant perform cnn variant contrary automatic rouge evaluation terestingly cnn variant produces better signicant difference summaries gold summaries posit fact article title correspond summary rst sentence selective disambiguation entities effectiveness selective disambiguation gate selecting entities disambiguate table shows total different amples entities highest lowest values rst example sentence tains entity united states linked country entity correct linked entity united states davis cup team given high value hand sentence linked correctly country united states given low value second example vides similar scenario sentence linked entity gold linked entity gold medal sentence linked correctly chemical element mer case received high value case received low value entities summary topic finally provide sample dataset table case study comparing nal model uses attention variant uses soft attention baseline model base attention weights soft models gigaword example base model generated servations informative summary mentioning ico state rst edition second soft model produced factually wrong summary ing guadalajara mexican state actually city model able solve problem focusing important entities eliminating possible noise unk crucial entities country club ness selective disambiguation ple entity state corrected mean entity mexican state relevant selected cnn example line model generated erroneous summary argue length text long decoder guided topics focus soft model generated better summary cuses wrong topics specically iran nuclear program making summary eral quick read original article tells main topic article political parties arguing deal iran entity nuclear appeared lot article makes soft model wrongly focus nuclear entity model produced relevant summary focusing original gold baseline entities soft firm original gold baseline soft firm western mexico host rst edition dollar ochoa invitation tournament nov club ochoa foundation said statement wednesday mexico host lorena ochoa golf tournament guadalajara host ochoa tournament tournament gigaword dataset example guadalajara country club lorena ochoa state unk mexico state guadalajara host ochoa ochoa invitation jalisco mexican state host rst edition ochoa invitation lorena ochoa golf cnn dataset example url cnn politics netanyahu iran deal index html netanyahu says option standing better deal political sparring continues deal iran netanyahu says country unk cheating country unk cheating netanyahu says country unk cheating bad deal says says says plan country unk cheating country unk cheating says country unk cheating country unk cheating benjamin netanyahu think alternative standing netanyahu tells cnn says roll iran nuclear ambitions roll iran nuclear program new netanyahu think alternative standing netanyahu says obama comments come democrats republicans spar framework announced week lift western sanctions iran table examples gigaword cnn datasets corresponding summaries generated competing models tagged text marked bold preceded sign red color represents attention scores given entity report attention scores entities gigaword example conciseness linked entities cnn example text linked entity wikipedia org wiki andy roddick got better dmitry tursunov straight sets friday assuring states lead defending champions russia davis cup nal sir alex ferguson revealed friday david beckham states surprised knew midelder return england come manchester united linked entity wikipedia org wiki gold following medal standing olympic winter games tabulated team silver bronze unk opened lower monday dollars ounce friday closing rate table examples highest lowest disambiguation gate values example entities united states gold tagged text marked bold preceded sign litical entities republicans democrats fact important elements attended create mary topic vector conclusion proposed leverage linked entities prove performance sequence sequence models neural abstractive summarization task linked entities guide decoding cess based summary topic sense learned knowledge base duced module easily attachable model encoder decoder framework applies linked entities summarizer encoding entities tive disambiguation pooling summary topic vector attention nism showed applying basic sequence sequence model achieve improvements base model sequently achieve comparable performance complex summarization models acknowledgement like thank anonymous viewers valuable feedback work supported microsoft research stitute information communications ogy promotion iitp grant funded korea government msit ment explainable humanlevel deep machine learning inference framework hwang corresponding author references dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly arxiv preprint learning align translate diego ceccarelli claudio lucchese salvatore lando raffaele perego salvatore trani dexter open source framework entity linking proceedings sixth international workshop exploiting semantic annotations information retrieval acm pages qian chen xiaodan zhu zhenhua ling wei hui jiang distraction based neural works document summarization arxiv preprint jianpeng cheng mirella lapata neural summarization extracting sentences words arxiv preprint sumit chopra michael auli alexander rush abstractive sentence summarization tentive recurrent neural networks proceedings conference north american ter association computational linguistics human language technologies pages gunes erkan dragomir radev lexrank graph based lexical centrality salience text journal articial intelligence summarization research faegheh hasibi krisztian balog svein erik reproducibility tagme tity linking system european conference formation retrieval springer pages karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend advances ral information processing systems pages geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutdinov improving neural networks preventing arxiv preprint adaptation feature detectors ioana hulpus conor hayes marcel karnstedt derek greene unsupervised graph based topic labelling dbpedia proceedings sixth acm international conference web search data mining acm pages jey han lau paul cook timothy baldwin unimelb topic modelling based word sense duction web snippet clustering naacl hlt pages chin yew lin rouge package matic evaluation summaries text tion branches proceedings shop barcelona spain volume xiao ling sameer singh daniel weld design challenges entity linking transactions association computational linguistics minh thang luong hieu pham christopher manning effective approaches based neural machine translation arxiv preprint ramesh nallapati bowen zhou caglar gulcehre bing xiang abstractive text rization sequence sequence rnns yond arxiv preprint david newman chaitanya chemudugunta padhraic smyth mark steyvers analyzing ties topics news articles statistical topic models isi springer pages yuan qiong kai feng cao yosi mass dafna sheinwald hui jia zhu shao sheng cao semantic documents relatedness proceedings cept graph representation ninth acm international conference web search data mining acm pages jeffrey pennington richard socher christopher manning glove global vectors word representation proceedings ence empirical methods natural language cessing emnlp pages alexander rush sumit chopra jason neural attention model arxiv preprint ston stractive sentence summarization abigail peter liu christopher point summarization arxiv preprint ning pointer generator networks nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov dropout simple way prevent neural networks overtting journal machine learning search ilya sutskever oriol vinyals quoc sequence sequence learning neural works advances neural information ing systems pages jiwei tan xiaojun wan jianguo xiao abstractive document summarization proceedings based attentional neural model annual meeting association putational linguistics volume long papers volume pages jing wang mohit bansal kevin gimpel brian ziebart clement sense topic model word sense induction unsupervised data enrichment transactions association computational linguistics kelvin jimmy ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel yoshua bengio attend tell neural image caption generation visual international conference machine tention learning pages ikuya yamada hiroyuki shindo hideaki takeda yoshiyasu takefuji learning distributed resentations texts entities knowledge base arxiv preprint jin yao xiaojun wan jianguo xiao cent advances document summarization edge information systems pages qingyu zhou nan yang furu wei ming zhou selective encoding abstractive sentence summarization arxiv preprint
