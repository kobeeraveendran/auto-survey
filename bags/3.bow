catching drift probabilistic content models applications generation summarization regina barzilay computer science lab mit mit edu lillian lee department computer science cornell university cornell edu abstract consider problem modeling tent structure texts specic main terms topics texts address order topics appear rst present effective knowledge lean method learning content models annotated documents utilizing novel tation algorithms hidden markov els apply method plementary tasks information ordering tractive summarization experiments incorporating content models plications yields substantial improvement previously proposed methods publication info hlt naacl ceedings main conference introduction development application computational els text structure central concern natural guage processing document level analysis text ture important instance work ous research sought characterize texts terms domain independent rhetorical elements schema items mckeown rhetorical relations mann thompson marcu focus work equally fundamental domain dependent dimension structure text content use term content corresponds roughly notions topic topic change desire models specify example articles earthquakes typically contain information quake strength location casualties descriptions casualties usually precede rescue efforts manually determine topics given domain distributional view learning directly annotated texts analysis word distribution patterns idea dates harris claimed types word recurrence patterns characterize types discourse advantages distributional perspective clude drastic reduction human effort tion topics occur human expert explicitly modeled aid applications course success distributional approach depends existence recurrent patterns trary document collections patterns variable easily detected statistical means research shown texts domain tend exhibit high similarity wray cognitive psychologists long posited similarity accidental arguing formulaic text structure facilitates readers comprehension recall bartlett paper investigate utility specic content models representing topics topic shifts content models hidden markov models hmms states correspond types information characteristic domain terest earthquake magnitude previous quake occurrences state transitions capture possible information presentation orderings domain rst describe efcient knowledge lean method learning set topics relations tween topics directly annotated documents technique incorporates novel adaptation standard hmm induction algorithm tailored task modeling content apply techniques based content models complex text processing tasks consider formation ordering choosing sequence present pre selected set items tial step concept text generation multi document summarization text synthesis problems formulaic necessarily equivalent simple automated approaches offer advantages manual techniques especially needs model domains experiments content models outperform lapata state art ordering method wide margin domain performance metric gap centage points second consider extractive compression document choosing rization subsequence sentences task velop new content model based learning algorithm sentence selection resulting summaries yield match human written output compares vorably achieved standard leading sentences baseline success content models mentary tasks demonstrates exibility ness indicates sufciently expressive represent important text properties observations taken fact content models ceptually intuitive efciently learnable raw ument collections suggest formalism prove useful broader range applications considered exploring options ing line future research related work knowledge rich methods models employing manual crafting typically complex representations content generally captured types knowledge rambow kittredge domain edge earthquakes magnitudes independent communication knowledge scribing event usually entails specifying location domain communication knowledge reuters earthquake reports conclude listing previous formalisms exemplifying edge types dejong scripts mckeown schemas rambow domain specic schemas respectively contrast models based tributional view content freely incorporate information categories long formation manifested recurrent pattern comparison formalisms mentioned content models constitute relatively impoverished tion actually contributes ease learned empirical results effective despite simplicity recent work duboue mckeown propose method learning content planner tion texts domain specic knowledge base method applies domains knowledge base supplied qualify domain knowledge earthquakes hearst knowledge lean approaches distributional models content appeared frequency search text segmentation topic based language beeferman modeling florian yarowsky chen iyer ostendorf gildea hofmann khudanpur methods fact employ learning content models closely related techniques proposed literature section details language modeling research goal predict text probabilities tends treat topic useful auxiliary variable central concern example topic based distributional information ally interpolated standard non topic based gram models improve probability estimates work contrast treats content primary entity particular induction algorithms designed explicit goal modeling document content differ standard baum welch algorithm learning hidden markov models content models instances hmms model construction employ iterative estimation procedure ternates creating clusters text spans similar word distributions serve representatives document topics computing models word distributions topic changes clusters derived formalism preliminaries treat texts sequences pre dened text spans presumed convey mation single topic specifying text span length denes granularity induced topics concreteness follows refer sentences text spans experiments paragraphs clauses potentially employed instead working assumption texts given domain generated single content model tent model hmm state corresponds distinct topic generates sentences relevant topic according state specic language model note standard gram language models fore considered degenerate single state content models state transition probabilities probability changing given topic turing constraints topic shifts use forward algorithm efciently compute generation ity assigned document content model clarity omit minor technical details use dummy initial nal states section describes free parameters chosen athens seismological institute said temblor center located kilometers miles south capital seismologists pakistan northwest frontier province said temblor epicenter kilometers miles north provincial capital peshawar temblor centered kilometers miles west provincial capital kunming kilometers miles southwest beijing bureau seismologist said figure samples earthquake articles sentence cluster corresponding descriptions location viterbi algorithm quickly likely model state sequence generated given ment rabiner details implementation use bigram language els probability word sentence def generated state estimating state bigram bilities described topic induction initial previous work florian yarowsky iyer ostendorf khudanpur initialize set ics distributionally construed partitioning sentences documents given domain specic collection clusters create clusters complete link clustering measuring sentence similarity cosine metric word bigrams features figure shows example output given knowledge documents discuss new irrelevant content create etcetera cluster merging clusters containing fewer sentences assumption clusters consist outlier sentences use denote number clusters results determining states emission probabilities tion probabilities given set ters etcetera cluster construct content model corresponding states refer insertion state state bigram probabilities induce state sentence emission probabilities timated smoothed counts corresponding cluster psi def frequency word sequence occurs sentences cluster barzilay lee proper names bers dates temporarily replaced generic tokens help ensure clusters contain sentences describing event type actual event vocabulary want insertion state model digressions unseen topics novel step forcing language model complementary states setting psm def maxi psi puv maxi note contents etcetera cluster ignored stage state transition probability estimates arise considering sentences article tributed clusters specically clusters let number documents sentence immediately precedes let number documents taining sentences states use following smoothed estimate probability transitioning viterbi estimation initial clustering ignores sentence order contextual clues indicate sentences high lexical similarity actually different topics instance reuters articles earthquakes frequently nish mentioning previous quakes means sentence temblor injured dozens beginning report probably highly salient included summary sentence end piece probably refers different event omitted natural way incorporate ordering information iterative estimation model parameters content model provides information transition structure like viterbi proach iyer ostendorf cluster tences placing new cluster corresponds state likely erated according viterbi decoding ing data use new clustering input procedure estimating hmm parameters described cluster estimate cycle repeated clusterings stabilize reach predened number iterations evaluation tasks apply techniques described tasks stand benet models content changes topic information ordering text generation formation selection single document summarization complementary tasks rely joint model functionalities ability order set pre selected information bearing items ability selection extracting ordered quence information bearing items representative sequence information ordering information ordering task essential synthesis applications including concept text tion multi document summarization ing range discourse stylistic factors inuence ordering process infeasible mains probabilistic content models provide means handling important aspects problem strate point utilizing content models select propriate sentence orderings simply use content model trained documents domain interest selecting ordering presented candidates content model assigns highest probability extractive summarization content models single document summarization ordering issue task tests ability content models adequately represent domain topics independently ordering topics usual strategy employed domain specic marizers humans determine priori types information originating documents included stories earthquakes number victims radev mckeown systems avoid need white manual analysis learning content selection rules collection articles paired authored summaries learning algorithms ically focus sentence features coarse structural features position graph kupiec content model based summarization algorithm combines advantages approaches hand learns required formation annotated document summary pairs hand operates abstract global level making use topical structure entire document algorithm trained follows given content model acquired articles method scribed section need learn topics resented content model states appear summaries rst step employ viterbi gorithm tag summary sentences sentences original articles viterbi topic label topic state likely generated state training set articles contained topic sentences single document summary follow order appearance original document domain average standard vocabulary length deviation earthquakes clashes drugs finance accidents size type table corpus statistics length sentences cabulary size type token ratio computed placement proper names numbers dates compute probability state generates sentences appear summary ability estimated simply counting number document summary pairs parallel training data originating document summary contain sentences assigned topic malizing count number articles taining sentences topic produce summary new article gorithm rst uses content model viterbi decoding assign article sentences topic algorithm selects states chosen appear topic article sentences highest probability generating summary sentence estimated sentences input article corresponding states placed output summary evaluation experiments data evaluation purposes created corpora domains earthquakes clashes armies rebel groups drug related criminal offenses nancial reports summaries aviation accidents specically rst collections consist articles north american news corpus gathered tdt style ment clustering system fth consists narratives national transportation safety board database previously employed jones thompson event identication experiments set articles training content model cles testing development set parameter tuning table presents information ticle length measured sentences determined sentence separator reynar ratnaparkhi vocabulary size token type ratio domain sentences prioritize summarization probability topic state break ties order appearance document sls csail mit struct parameter estimation document computed training algorithm free parameters indirectly control number states induced tent model parameters smoothing bigram probabilities tuned separately main corresponding held development set ing powell grid search press ter values selected optimize system performance information ordering found domains optimal models based sharper language models optimal number states ranged ordering experiments metrics intent ordering experiments test content models assign high probability ceptable sentence arrangements stumbling block performing kind evaluation data ordering quality set sentences document sequenced different ways single text ate length ask humans evaluate tunately know original sentence order oso source document acceptable prefer algorithms assign high probability relative bulk possible permutations observation motivates rst ation metric rank received oso mutations given document sentences sorted probabilities model consideration signs best possible rank worst additional difculty encountered setting evaluation wanted compare algorithms lapata state art tem method consider permutations rank metric computed compensate report oso prediction rate measures percentage test cases model consideration gives highest probability oso possible permutations expect good model predict oso fair fraction time furthermore provide assessment quality predicted orderings follow lapata employing kendall sure ordering differs oso underlying assumption reasonable tence orderings fairly similar specically permutation sentences section discussion relation ordering summarization task number swaps adjacent tences necessary arrange oso metric ranges inverse orders identical orders results unseen test texts exhaustively enumerated sentence permutations ranked content model corresponding domain compared results bigram guage model baseline improved version state art probabilistic ordering method pata trained data lapata method rst learns set pairwise ordering preferences based features noun verb dependencies given new set sentences latest version method applies viterbi style tion algorithm choose permutation satisfying preferences lapata personal communication table gives results ordering test son experiments content models outperform tives universally wide margin conjecture difference performance stems ability content models capture global ument structure contrast algorithms local taking account relationships tween adjacent word pairs adjacent sentence pairs respectively interesting observe method achieves better results despite having access guistic information incorporated lapata method fair techniques designed larger corpus aggravate data sparseness problems feature rich method table gives details rank results content models showing rank scores tributed instance earthquakes main oso permutations test documents drugs accidents domains proved relatively challenging method cases oso rank exceed given maximal possible rank domains exceeds million believe model good job ordering task computed learning curves different mains shown figure surprisingly formance improves size training set domains gure shows relative difculty content model point view different domains remains constant varying set sizes interestingly easiest domains finance optimal permutation complete domain system rank content earthquakes lapata bigram content lapata bigram content lapata bigram content lapata bigram content lapata bigram oso pred clashes drugs finance accidents table ordering results averages test cases domain rank range earthquakes clashes drugs finance accidents table percentage cases content model assigned oso rank given range earthquakes thought mulaic redundant token type ratios table domains words repeated frequently average summarization experiments evaluation summarization algorithm driven questions summaries produced acceptable quality terms selected content content model representation provide tional advantages locally focused methods address rst question compare summaries created system lead baseline extracts rst sentences original text spite simplicity results annual ment understanding conference duc evaluation gest single document summarization systems beat baseline address question consider summarization system learns extraction rules directly parallel corpus texts summaries kupiec system earthquake clashes drugs finance accidents training set size figure ordering task performance terms oso prediction rate function number documents training set rization framed sentence level binary tion problem sentence labeled available boostexter system schapire singer summary tures considered sentence unigrams location text beginning dle end relationships sentences explicitly modeled making system good basis comparison evaluated summarization system quakes domain texts domain condensed version written journalists summaries consequently easily aligned sentences original articles document summary pairs half randomly selected training half testing thirty documents like large number comparable size training corpora competitive system evaluations mentioned average ber sentences texts summaries respectively total sentences test documents training sets runtime provided systems ument desired output length length sentences corresponding shortened version resulting summaries judged tion component sentences appeared human written summary input text results table conrm hypothesis benets content models text summarization model outperforms sentence level feature set yielded best results possibilities tried dropped phrases rarely clause system content based sentence classier words location leading sentences extraction accuracy table summarization task results model size ordering summarization table content model performance earthquakes function model size ordering oso prediction rate summarization extraction accuracy ber states metrics induce similar ranking models fact size model yields mance tasks experiments limited domain correlation results ing optimizing parameters task promises yield good performance ndings provide support hypothesis content models helpful specic tasks serve effective representations text structure general content model lead conclusions training set size number summary source pairs figure summarization performance extraction racy earthquakes function training set size focused classier lead baseline furthermore learning curves shown figure indicate method achieves good performance small subset parallel training data fact accuracy method training data higher sentence level classier training set clearly performance gain demonstrates effectiveness content models summarization task relation ordering summarization methods somewhat orthogonal tasks ordering summarization evaluate quality model paradigm interesting ask parameterization model cases specically looked results different model topologies induced varying number model states tests experimented earthquakes data domain evaluate summarization performance exerted direct control number states utilizing cluster size threshold order create exactly states specic value merged smallest clusters clusters remained table shows performance different sized content models respect summarization task ordering task oso prediction rate ordering results sensitive paper present unsupervised method induction content models capture constraints topic selection organization texts ticular domain incorporation models ing summarization applications yields substantial provement previously proposed methods sults indicate distributional approaches widely model inter sentential phenomena cessfully applied capture text level relations cally validating long standing hypothesis word distribution patterns strongly correlate discourse patterns text specic domains important future direction lies studying respondence domain specic model domain independent formalisms rst tomatically annotating large corpus texts course relations rhetorical parser marcu soricut marcu able rate domain independent relationships transition structure content models study uncover interesting connections domain specic stylistic constraints generic principles text organization literature discourse frequently modeled hierarchical structure suggests tic context free grammars hierarchical hidden markov models fine applied ing content structure future plan investigate bootstrap induction hierarchical models ing labeled data derived content models like explore domain independent course constraints guide construction hierarchical models acknowledgments grateful mirella lapata providing results system data dominic jones cindi thompson supplying document collection thank eli lay sasha blair goldensohn eric breck claire cardie yejin choi marcia davidson pablo duboue noemie elhadad luis gravano julia hirschberg sanjeev danpur jon kleinberg oren kurland kathy mckeown daniel marcu art munson smaranda muresan cent pang becky passoneau owen rambow ves stoyanov chao wang anonymous reviewers helpful comments conversations portions work rst author postdoctoral fellow cornell university paper based work supported national science tion grants itr alfred sloan research fellowship opinions ndings conclusions recommendations expressed authors essarily reect views national science tion sloan foundation references bartlett remembering study experimental social psychology bridge university press barzilay barzilay lee learning paraphrase unsupervised approach multiple sequence alignment hlt naacl main proceedings beeferman beeferman berger text segmentation exponential ferty models proceedings emnlp chen chen seymore rosenfeld topic adaptation language modeling unnormalized exponential models proceedings icassp volume dejong overview frump system lehnert ringle eds strategies natural language processing lawrence erlbaum associates hillsdale new jersey duboue duboue statistical acquisition content selection rules natural language generation proceedings emnlp fine singer tishby hierarchical hidden markov model analysis plications machine learning florian florian yarowsky dynamic non local language modeling erarchical topic based adaptation proceedings acl harris discourse guage kittredge lehrberger eds guage studies language restricted semantic domains walter gruyter berlin new york hearst multi paragraph tation expository text proceedings acl iyer iyer ostendorf modeling long distance dependence language topic mixtures dynamic cache models ings icslp jones jones identifying events similarity son context proceedings conll kittredge kittredge korelsky bow need domain communication language computational intelligence kupiec kupiec pedersen chen trainable document summarizer mani maybury eds advances automatic summarization mit press cambridge lapata probabilistic text turing experiments sentence ordering ceeding acl mann mann son rhetorical structure theory tional theory text organization text marcu ing natural language texts acl eacl rhetorical proceedings mckeown text ation discourse strategies focus straints generate natural language text bridge university press cambridge press press teukolsky terling flannery numerical recipes art scientic computing cambridge sity press second edition rabiner tutorial hidden markov models selected applications speech recognition proceedings ieee radev radev generating natural language summaries multiple line sources computational guistics rambow domain cation knowledge fifth international workshop natural language generation reynar reynar maximum entropy approach parkhi identifying sentence boundaries proceedings fifth conference applied natural language processing gildea gildea hofmann topic based language models proceedings eurospeech schapire schapire singer boostexter boosting based system text categorization machine learning soricut soricut marcu sentence level discourse parsing syntactic lexical hlt naacl proceedings information white white korelsky cardie pierce wagstaff multi document summarization information extraction ings hlt conference wray formulaic language lexicon cambridge university press cambridge khudanpur building topic dependent maximum entropy guage model large corpora proceedings icassp volume
