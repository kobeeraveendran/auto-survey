a m l c s c v s c v i x r a catching the drift probabilistic content models with applications to generation and summarization regina barzilay computer science and ai lab mit mit edu lillian lee department of computer science cornell university cornell edu abstract we consider the problem of modeling the tent structure of texts within a specic main in terms of the topics the texts address and the order in which these topics appear we rst present an effective knowledge lean method for learning content models from annotated documents utilizing a novel tation of algorithms for hidden markov els we then apply our method to two plementary tasks information ordering and tractive summarization our experiments show that incorporating content models in these plications yields substantial improvement over previously proposed methods publication info hlt naacl ceedings of the main conference pp introduction the development and application of computational els of text structure is a central concern in natural guage processing document level analysis of text ture is an important instance of such work ous research has sought to characterize texts in terms of domain independent rhetorical elements such as schema items mckeown or rhetorical relations mann and thompson marcu the focus of our work however is on an equally fundamental but domain dependent dimension of the structure of text content our use of the term content corresponds roughly to the notions of topic and topic change we desire models that can specify for example that articles about earthquakes typically contain information about quake strength location and casualties and that descriptions of casualties usually precede those of rescue efforts but rather than manually determine the topics for a given domain we take a distributional view learning them directly from un annotated texts via analysis of word distribution patterns this idea dates back at least to harris who claimed that various types of word recurrence patterns seem to characterize various types of discourse advantages of a distributional perspective clude both drastic reduction in human effort and tion of topics that might not occur to a human expert and yet when explicitly modeled aid in applications of course the success of the distributional approach depends on the existence of recurrent patterns in trary document collections such patterns might be too variable to be easily detected by statistical means ever research has shown that texts from the same domain tend to exhibit high similarity wray cognitive psychologists have long posited that this similarity is not accidental arguing that formulaic text structure facilitates readers comprehension and recall bartlett in this paper we investigate the utility of specic content models for representing topics and topic shifts content models are hidden markov models hmms wherein states correspond to types of information characteristic to the domain of terest e earthquake magnitude or previous quake occurrences and state transitions capture possible information presentation orderings within that domain we rst describe an efcient knowledge lean method for learning both a set of topics and the relations tween topics directly from un annotated documents our technique incorporates a novel adaptation of the standard hmm induction algorithm that is tailored to the task of modeling content then we apply techniques based on content models to two complex text processing tasks first we consider formation ordering that is choosing a sequence in which to present a pre selected set of items this is an tial step in concept to text generation multi document summarization and other text synthesis problems in our formulaic is not necessarily equivalent to simple so automated approaches still offer advantages over manual techniques especially if one needs to model several domains experiments content models outperform lapata s state of the art ordering method by a wide margin for one domain and performance metric the gap was centage points second we consider extractive the compression of a document by choosing rization a subsequence of its sentences for this task we velop a new content model based learning algorithm for sentence selection the resulting summaries yield match with human written output which compares vorably to the achieved by the standard leading n sentences baseline the success of content models in these two mentary tasks demonstrates their exibility and ness and indicates that they are sufciently expressive to represent important text properties these observations taken together with the fact that content models are ceptually intuitive and efciently learnable from raw ument collections suggest that the formalism can prove useful in an even broader range of applications than we have considered here exploring the options is an ing line of future research related work knowledge rich methods models employing manual crafting of typically complex representations of content have generally captured one of three types of knowledge rambow kittredge et al domain edge e that earthquakes have magnitudes independent communication knowledge e that scribing an event usually entails specifying its location and domain communication knowledge e that reuters earthquake reports often conclude by listing previous formalisms exemplifying each of these edge types are dejong s scripts mckeown s schemas and rambow s domain specic schemas respectively in contrast because our models are based on a tributional view of content they will freely incorporate information from all three categories as long as such formation is manifested as a recurrent pattern also in comparison to the formalisms mentioned above content models constitute a relatively impoverished tion but this actually contributes to the ease with which they can be learned and our empirical results show that they are quite effective despite their simplicity in recent work duboue and mckeown propose a method for learning a content planner from a tion of texts together with a domain specic knowledge base but our method applies to domains in which no such knowledge base has been supplied does not qualify as domain knowledge because it is not about earthquakes per hearst knowledge lean approaches distributional models of content have appeared with some frequency in search on text segmentation and topic based language beeferman et al modeling florian and yarowsky chen et al iyer and ostendorf gildea and hofmann wu and khudanpur the methods we in fact employ for learning content models are quite closely related to techniques proposed in that literature see section for more details however language modeling research whose goal is to predict text probabilities tends to treat topic as a useful auxiliary variable rather than a central concern for example topic based distributional information is ally interpolated with standard non topic based n gram models to improve probability estimates our work in contrast treats content as a primary entity in particular our induction algorithms are designed with the explicit goal of modeling document content which is why they differ from the standard baum welch or em algorithm for learning hidden markov models even though content models are instances of hmms model construction we employ an iterative re estimation procedure that ternates between creating clusters of text spans with similar word distributions to serve as representatives of within document topics and computing models of word distributions and topic changes from the clusters so derived formalism preliminaries we treat texts as sequences of pre dened text spans each presumed to convey mation about a single topic specifying text span length thus denes the granularity of the induced topics for concreteness in what follows we will refer to sentences rather than text spans since that is what we used in our experiments but paragraphs or clauses could potentially have been employed instead our working assumption is that all texts from a given domain are generated by a single content model a tent model is an hmm in which each state s corresponds to a distinct topic and generates sentences relevant to that topic according to a state specic language model ps note that standard n gram language models can fore be considered to be degenerate single state content models state transition probabilities give the probability of changing from a given topic to another thereby turing constraints on topic shifts we can use the forward algorithm to efciently compute the generation ity assigned to a document by a content model and the clarity we omit minor technical details such as the use of dummy initial and nal states section describes how the free parameters k t and are chosen the athens seismological institute said the temblor s center was located kilometers miles south of the capital seismologists in pakistan s northwest frontier province said the temblor s epicenter was about kilometers miles north of the provincial capital peshawar the temblor was centered kilometers miles west of the provincial capital of kunming about kilometers miles southwest of beijing a bureau seismologist said figure samples from an earthquake articles sentence cluster corresponding to descriptions of location viterbi algorithm to quickly nd the most likely model state sequence to have generated a given ment see rabiner for details in our implementation we use bigram language els so that the probability of an n word sentence def wn being generated by a state s is n estimating the state bigram q bilities is described below in topic induction as initial previous work florian and yarowsky iyer and ostendorf wu and khudanpur we initialize the set of ics distributionally construed by partitioning all of the sentences from the documents in a given domain specic collection into clusters first we create k clusters via complete link clustering measuring sentence similarity by the cosine metric using word bigrams as features figure shows example output then given our knowledge that documents may sometimes discuss new irrelevant content as well we create an etcetera cluster by merging together all clusters containing fewer than t sentences on the assumption that such clusters consist of outlier sentences we use m to denote the number of clusters that results determining states emission probabilities and tion probabilities given a set cm of m ters where cm is the etcetera cluster we construct a content model with corresponding states sm we refer to sm as the insertion state for each state si i m bigram probabilities which induce the state s sentence emission probabilities are timated using smoothed counts from the corresponding cluster psi def where is the frequency with which word sequence y occurs within the sentences in cluster ci and v is the barzilay and lee proper names bers and dates are temporarily replaced with generic tokens to help ensure that clusters contain sentences describing the same event type rather than same actual event vocabulary but because we want the insertion state sm to model digressions or unseen topics we take the novel step of forcing its language model to be complementary to those of the other states by setting psm def maxi i m psi puv maxi i m note that the contents of the etcetera cluster are ignored at this stage our state transition probability estimates arise from considering how sentences from the same article are tributed across the clusters more specically for two clusters c and c let be the number of documents in which a sentence from c immediately precedes one from c and let be the number of documents taining sentences from c then for any two states and sj i m we use the following smoothed estimate of the probability of transitioning from si to sj cj m viterbi re estimation our initial clustering ignores sentence order however contextual clues may indicate that sentences with high lexical similarity are actually on different topics for instance reuters articles about earthquakes frequently nish by mentioning previous quakes this means that while the sentence the temblor injured dozens at the beginning of a report is probably highly salient and should be included in a summary of it the same sentence at the end of the piece probably refers to a different event and so should be omitted a natural way to incorporate ordering information is iterative re estimation of the model parameters since the content model itself provides such information through its transition structure we take an em like viterbi proach iyer and ostendorf we re cluster the tences by placing each one in the new cluster ci i m that corresponds to the state most likely to have erated it according to the viterbi decoding of the ing data we then use this new clustering as the input to the procedure for estimating hmm parameters described above the cluster estimate cycle is repeated until the clusterings stabilize or we reach a predened number of iterations evaluation tasks we apply the techniques just described to two tasks that stand to benet from models of content and changes in topic information ordering for text generation and formation selection for single document summarization these are two complementary tasks that rely on joint model functionalities the ability to order a set of pre selected information bearing items and the ability to do the selection itself extracting from an ordered quence of information bearing items a representative sequence information ordering the information ordering task is essential to many synthesis applications including concept to text tion and multi document summarization while ing for the full range of discourse and stylistic factors that inuence the ordering process is infeasible in many mains probabilistic content models provide a means for handling important aspects of this problem we strate this point by utilizing content models to select propriate sentence orderings we simply use a content model trained on documents from the domain of interest selecting the ordering among all the presented candidates that the content model assigns the highest probability to extractive summarization content models can also be used for single document summarization because ordering is not an issue in this this task tests the ability of content models to adequately represent domain topics independently of whether they do well at ordering these topics the usual strategy employed by domain specic marizers is for humans to determine a priori what types of information from the originating documents should be included e in stories about earthquakes the number of victims radev and mckeown some systems avoid the need white et al for manual analysis by learning content selection rules from a collection of articles paired with authored summaries but their learning algorithms ically focus on within sentence features or very coarse structural features such as position within a graph kupiec et al our content model based summarization algorithm combines the advantages of both approaches on the one hand it learns all required formation from un annotated document summary pairs on the other hand it operates on a more abstract and global level making use of the topical structure of the entire document our algorithm is trained as follows given a content model acquired from the full articles using the method scribed in section we need to learn which topics resented by the content model s states should appear in our summaries our rst step is to employ the viterbi gorithm to tag all of the summary sentences and all of the sentences from the original articles with a viterbi topic label or v topic the name of the state most likely to have generated them next for each state s such that at least three full training set articles contained v topic sentences in a single document summary follow the order of appearance in the original document domain average standard vocabulary length deviation earthquakes clashes drugs finance accidents size type table corpus statistics length is in sentences cabulary size and type token ratio are computed after placement of proper names numbers and dates s we compute the probability that the state generates sentences that should appear in a summary this ability is estimated by simply counting the number of document summary pairs in the parallel training data such that both the originating document and the summary contain sentences assigned v topic s and then malizing this count by the number of full articles taining sentences with v topic s to produce a summary of a new article the gorithm rst uses the content model and viterbi decoding to assign each of the article s sentences a v topic next the algorithm selects those states chosen from among those that appear as the v topic of one of the article s sentences that have the highest probability of generating a summary sentence as estimated above sentences from the input article corresponding to these states are placed in the output summary evaluation experiments data for evaluation purposes we created corpora from ve domains earthquakes clashes between armies and rebel groups drug related criminal offenses nancial reports and summaries of aviation accidents specically the rst four collections consist of ap articles from the north american news corpus gathered via a tdt style ment clustering system the fth consists of narratives from the national transportation safety board s database previously employed by jones and thompson for event identication experiments for each such set articles were used for training a content model cles for testing and for the development set used for parameter tuning table presents information about ticle length measured in sentences as determined by the sentence separator of reynar and ratnaparkhi vocabulary size and token type ratio for each domain there are more than sentences we prioritize them by the summarization probability of their v topic s state we break any further ties by order of appearance in the document sls csail mit struct parameter estimation document is computed as our training algorithm has four free parameters two that indirectly control the number of states in the induced tent model and two parameters for smoothing bigram probabilities all were tuned separately for each main on the corresponding held out development set ing powell s grid search press et al the ter values were selected to optimize system performance on the information ordering we found that across all domains the optimal models were based on sharper language models e the optimal number of states ranged from to ordering experiments metrics the intent behind our ordering experiments is to test whether content models assign high probability to ceptable sentence arrangements however one stumbling block to performing this kind of evaluation is that we do not have data on ordering quality the set of sentences from an n document can be sequenced in n different ways which even for a single text of ate length is too many to ask humans to evaluate tunately we do know that at least the original sentence order oso in the source document must be acceptable and so we should prefer algorithms that assign it high probability relative to the bulk of all the other possible permutations this observation motivates our rst ation metric the rank received by the oso when all mutations of a given document s sentences are sorted by the probabilities that the model under consideration signs to them the best possible rank is and the worst is n an additional difculty we encountered in setting up our evaluation is that while we wanted to compare our algorithms against lapata s state of the art tem her method does nt consider all permutations see below and so the rank metric can not be computed for it to compensate we report the oso prediction rate which measures the percentage of test cases in which the model under consideration gives highest probability to the oso from among all possible permutations we expect that a good model should predict the oso a fair fraction of the time furthermore to provide some assessment of the quality of the predicted orderings themselves we follow lapata in employing kendall s which is a sure of how much an ordering differs from the oso the underlying assumption is that most reasonable tence orderings should be fairly similar to it specically for a permutation of the sentences in an n section for discussion of the relation between the ordering and the summarization task n where is the number of swaps of adjacent tences necessary to re arrange into the oso the metric ranges from inverse orders to identical orders results for each of the unseen test texts we exhaustively enumerated all sentence permutations and ranked them using a content model from the corresponding domain we compared our results against those of a bigram guage model the baseline and an improved version of the state of the art probabilistic ordering method of pata both trained on the same data we used lapata s method rst learns a set of pairwise ordering preferences based on features such as noun verb dependencies given a new set of sentences the latest version of her method applies a viterbi style tion algorithm to choose a permutation satisfying many preferences lapata personal communication table gives the results of our ordering test son experiments content models outperform the tives almost universally and often by a very wide margin we conjecture that this difference in performance stems from the ability of content models to capture global ument structure in contrast the other two algorithms are local taking into account only the relationships tween adjacent word pairs and adjacent sentence pairs respectively it is interesting to observe that our method achieves better results despite not having access to the guistic information incorporated by lapata s method to be fair though her techniques were designed for a larger corpus than ours which may aggravate data sparseness problems for such a feature rich method table gives further details on the rank results for our content models showing how the rank scores were tributed for instance we see that on the earthquakes main the oso was one of the top ve permutations in of the test documents even in drugs and accidents the domains that proved relatively challenging to our method in more than of the cases the oso s rank did not exceed ten given that the maximal possible rank in these domains exceeds three million we believe that our model has done a good job in the ordering task we also computed learning curves for the different mains these are shown in figure not surprisingly formance improves with the size of the training set for all domains the gure also shows that the relative difculty from the content model point of view of the different domains remains mostly constant across varying set sizes interestingly the two easiest domains finance the optimal such permutation is np complete i e t a r n o i t c e r p o s o domain system rank content earthquakes lapata n a bigram content lapata n a bigram content lapata n a bigram content n a lapata bigram content lapata n a bigram oso pred clashes drugs finance accidents table ordering results averages over the test cases domain rank range earthquakes clashes drugs finance accidents table percentage of cases for which the content model assigned to the oso a rank within a given range and earthquakes can be thought of as being more mulaic or at least more redundant in that they have the token type ratios see table that is in these domains words are repeated much more frequently on average summarization experiments the evaluation of our summarization algorithm was driven by two questions are the summaries produced of acceptable quality in terms of selected content and does the content model representation provide tional advantages over more locally focused methods to address the rst question we compare summaries created by our system against the lead baseline which extracts the rst sentences of the original text spite its simplicity the results from the annual ment understanding conference duc evaluation gest that most single document summarization systems can not beat this baseline to address question we consider a summarization system that learns extraction rules directly from a parallel corpus of full texts and their summaries kupiec et al in this system earthquake clashes drugs finance accidents training set size figure ordering task performance in terms of oso prediction rate as a function of the number of documents in the training set rization is framed as a sentence level binary tion problem each sentence is labeled by the available boostexter system schapire and singer as being either in or out of the summary the tures considered for each sentence are its unigrams and its location within the text namely beginning third dle third and end third hence relationships between sentences are not explicitly modeled making this system a good basis for comparison we evaluated our summarization system on the quakes domain since for some of the texts in this domain there is a condensed version written by ap journalists these summaries are mostly consequently they can be easily aligned with sentences in the original articles from sixty document summary pairs half were randomly selected to be used for training and the other half for testing while thirty documents may not seem like a large number it is comparable to the size of the training corpora used in the competitive system evaluations mentioned above the average ber of sentences in the full texts and summaries was and respectively for a total of sentences in each of the test and full documents of the training sets at runtime we provided the systems with a full ument and the desired output length namely the length in sentences of the corresponding shortened version the resulting summaries were judged as a whole by the tion of their component sentences that appeared in the human written summary of the input text the results in table conrm our hypothesis about the benets of content models for text summarization our model outperforms both the sentence level feature set yielded the best results among the several possibilities we tried were dropped one or two phrases or more rarely a clause y c a r u c c a n o i a z i r a m m u s system content based sentence classier words location leading n sentences extraction accuracy table summarization task results model size ordering summarization table content model performance on earthquakes as a function of model size ordering oso prediction rate summarization extraction accuracy ber of states both metrics induce similar ranking on the models in fact the same size model yields top mance on both tasks while our experiments are limited to only one domain the correlation in results is ing optimizing parameters on one task promises to yield good performance on the other these ndings provide support for the hypothesis that content models are not only helpful for specic tasks but can serve as effective representations of text structure in general content model lead conclusions training set size number of summary source pairs figure summarization performance extraction racy on earthquakes as a function of training set size focused classier and the lead baseline furthermore as the learning curves shown in figure indicate our method achieves good performance on a small subset of parallel training data in fact the accuracy of our method on one third of the training data is higher than that of the sentence level classier on the full training set clearly this performance gain demonstrates the effectiveness of content models for the summarization task relation between ordering and summarization methods since we used two somewhat orthogonal tasks ordering and summarization to evaluate the quality of the model paradigm it is interesting to ask whether the same parameterization of the model does well in both cases specically we looked at the results for different model topologies induced by varying the number of model states for these tests we experimented with the earthquakes data the only domain for which we could evaluate summarization performance and exerted direct control over the number of states rather than utilizing the cluster size threshold that is in order to create exactly m states for a specic value of m we merged the smallest clusters until m clusters remained table shows the performance of the different sized content models with respect to the summarization task and the ordering task using oso prediction rate while the ordering results seem to be more sensitive to the in this paper we present an unsupervised method for the induction of content models which capture constraints on topic selection and organization for texts in a ticular domain incorporation of these models in ing and summarization applications yields substantial provement over previously proposed methods these sults indicate that distributional approaches widely used to model various inter sentential phenomena can be cessfully applied to capture text level relations cally validating the long standing hypothesis that word distribution patterns strongly correlate with discourse patterns within a text at least within specic domains an important future direction lies in studying the respondence between our domain specic model and domain independent formalisms such as rst by tomatically annotating a large corpus of texts with course relations via a rhetorical parser marcu soricut and marcu we may be able to rate domain independent relationships into the transition structure of our content models this study could uncover interesting connections between domain specic stylistic constraints and generic principles of text organization in the literature discourse is frequently modeled using a hierarchical structure which suggests that tic context free grammars or hierarchical hidden markov models fine et al may also be applied for ing content structure in the future we plan to investigate how to bootstrap the induction of hierarchical models ing labeled data derived from our content models we would also like to explore how domain independent course constraints can be used to guide the construction of the hierarchical models acknowledgments we are grateful to mirella lapata for providing us the results of her system on our data and to dominic jones and cindi thompson for supplying us with their document collection we also thank eli lay sasha blair goldensohn eric breck claire cardie yejin choi marcia davidson pablo duboue noemie elhadad luis gravano julia hirschberg sanjeev danpur jon kleinberg oren kurland kathy mckeown daniel marcu art munson smaranda muresan cent ng bo pang becky passoneau owen rambow ves stoyanov chao wang and the anonymous reviewers for helpful comments and conversations portions of this work were done while the rst author was a postdoctoral fellow at cornell university this paper is based upon work supported in part by the national science tion under grants itr im and and by an alfred p sloan research fellowship any opinions ndings and conclusions or recommendations expressed above are those of the authors and do not essarily reect the views of the national science tion or sloan foundation references f c bartlett remembering a study in experimental and social psychology bridge university press barzilay and r barzilay l lee learning to paraphrase an unsupervised approach using multiple sequence alignment in hlt naacl main proceedings beeferman et al d beeferman a berger j text segmentation using exponential ferty models in proceedings of emnlp chen et al s f chen k seymore r rosenfeld topic adaptation for language modeling using unnormalized exponential models in proceedings of icassp volume g dejong an overview of the frump system in w g lehnert m h ringle eds strategies for natural language processing lawrence erlbaum associates hillsdale new jersey duboue and p a duboue k r own statistical acquisition of content selection rules for natural language generation in proceedings of emnlp et al s fine y singer n tishby the hierarchical hidden markov model analysis and plications machine learning florian and r florian d yarowsky dynamic non local language modeling via erarchical topic based adaptation in proceedings of the acl z harris discourse and guage in r kittredge j lehrberger eds guage studies of language in restricted semantic domains walter de gruyter berlin new york m hearst multi paragraph tation of expository text in proceedings of the acl iyer and r iyer m ostendorf modeling long distance dependence in language topic mixtures vs dynamic cache models in ings of icslp jones and d r jones c a identifying events using similarity and son context in proceedings of conll kittredge et al r kittredge t korelsky o bow on the need for domain communication language computational intelligence kupiec et al j kupiec j pedersen f chen a trainable document summarizer in i mani m t maybury eds advances in automatic summarization mit press cambridge ma m lapata probabilistic text turing experiments with sentence ordering in ceeding of the acl mann and w c mann s a son rhetorical structure theory toward a tional theory of text organization text d marcu ing of natural language texts acl eacl the rhetorical in proceedings of the k r mckeown text ation using discourse strategies and focus straints to generate natural language text bridge university press cambridge uk press et al w h press s a teukolsky w t terling b p flannery numerical recipes in c the art of scientic computing cambridge sity press second edition l rabiner a tutorial on hidden markov models and selected applications in speech recognition proceedings of the ieee radev and d r radev k r own generating natural language summaries from multiple on line sources computational guistics o rambow domain cation knowledge in fifth international workshop on natural language generation reynar and j reynar a a maximum entropy approach to parkhi identifying sentence boundaries in proceedings of the fifth conference on applied natural language processing gildea and d gildea t hofmann topic based language models using em in proceedings of eurospeech schapire and r e schapire y singer boostexter a boosting based system for text categorization machine learning soricut and r soricut d marcu sentence level discourse parsing using syntactic and lexical the hlt naacl in proceedings of information white et al m white t korelsky c cardie v ng d pierce k wagstaff multi document summarization via information extraction in ings of the hlt conference a wray formulaic language and the lexicon cambridge university press cambridge wu and j wu s khudanpur building a topic dependent maximum entropy guage model for very large corpora in proceedings of icassp volume
