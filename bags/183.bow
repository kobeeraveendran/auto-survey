n a j l c s c v v i x r a neural related work summarization with a joint context driven attention mechanism yongzhen xiaozhong zheng of maritime economics and management dalian maritime university dalian china of informatics computing and engineering indiana university bloomington bloomington in usa group hangzhou china com edu iu edu abstract conventional solutions to automatic related work summarization rely heavily on engineered features in this paper we develop a neural data driven summarizer by ing the paradigm in which a joint context driven attention mechanism is posed to measure the contextual relevance within full texts and a heterogeneous raphy graph simultaneously our motivation is to maintain the topic coherency between a related work section and its target document where both the textual and graphic contexts play a big role in characterizing the ship among scientic publications accurately experimental results on a large dataset show that our approach achieves a considerable provement over a typical summarizer and ve classical summarization baselines introduction in scientic elds scholars need to contextualize their contribution to help readers acquire an derstanding of their research papers for this pose the related work section of an article serves as a pivot to connect prior domain knowledge in which the innovation and superiority of current work are displayed by a comparison with ous studies while citation prediction can assist in drafting a reference collection nallapati et al consuming all these papers is still a rious job where authors must read every source document carefully and locate the most relevant content cautiously as a solution in saving authors efforts tomatic related work summarization is tially a topic biased multi document problem cong and kan which relies heavily on human engineered features to retrieve snippets corresponding author from the references most recently neural works enable a data driven architecture to sequence for natural language ation bahdanau et al where an coder reads a sequence of words sentences into a context vector from which a decoder yields a sequence of specic outputs nonetheless pared to scenarios like machine translation with an end to end nature aligning a related work section to its source documents is far more challenging to address the summarization alignment mer studies try to apply an attention mechanism to measure the saliency novelty of each candidate word sentence tan et al with the aim of locating the most representative content to retain primary coverage however toward summarizing a related work section authors should be more ative when organizing text streams from the ence collection where the selected content ought to highlight the topic bias of current work rather than retell each reference in a compressed but anced fashion this motivates us to introduce the contextual relevance and characterize the ship among scientic publications accurately generally speaking for a pair of documents a larger lexical overlap often implies a higher ilarity in their research backgrounds yet such a hypothesis is not always true when sampling tent from multiple relevant topics take as an example from viewpoint of the abstract ilarity those references investigating information retrieval latent semantic model or through data mining could be of more tance in correlation and should be greatly sampled for the related work section but in reality this ticle spends a bit larger chunk of texts about to elaborate deep learning during the ture review which is quite difcult for machines deep structured semantic models for web search using clickthrough data huang et al to grasp the contextual relevance therein in tion other situations like emerging new concepts also suffer from the terminology variation or phrasing in varying degrees in this study we utilize a heterogeneous ography graph to embody the relationship within a scalable scholarly database over the recent past there is a surge of interest in exploiting diverse lations to analyze bibliometrics ranging from erature recommendation yu et al to topic evolvement jensen et al in a graphical sense interconnected papers transfer the credit among each other directly indirectly through ious patterns such as paper citation author laboration keyword association and releasing on series of venues which constitutes the graphic context for outlining concerned topics nately a variety of edge types may pollute the formation inquiry where a slice of edges are not so important as the others on sampling content meanwhile most existing solutions in mining erogeneous graphs depend on the human sion e hyperedge bu et al and ath swami et al this is usually not easy to access due to the complexity of graph schemas our contribution is threefold first we explore the edge type usefulness distribution eud on a heterogeneous bibliography graph which ables the relationship discovery between any pair of papers for sampling the interested tion second we develop a novel rizer for the automatic related work tion where a joint context driven attention anism is proposed to measure the contextual evance within both textual and graphic contexts third we conduct experiments on papers with native related work sections and tal results show that our approach outperforms a typical summarizer and ve classical summarization baselines signicantly related work this study touches on several strands of research within automatic related work summarization and summarizer as follows the idea of creating a related work section tomatically is pioneered by cong and kan who design two rule based strategies to extract sentences for general and detailed topics tively subsequently hu and wan ploit probabilistic latent semantic indexing to authors cong and kan hu and wan widyantoro and amin chen and hai number of papers table data scales of previous studies on automatic related work summarization topic biased split candidate texts into different parts then apply several regression models to learn the importance of each sentence similarly widyantoro and amin transform the marization problem into classifying rhetorical egories of sentences where each sentence is resented as a feature vector containing word quency sentence length and most recently chen and hai construct a graph of sentative keywords in which a minimum steiner tree is gured out to guide the summarization as nding the least number of sentences to cover in general compared the discriminated nodes to traditional summaries the automatic related work summarization receives less concerns over the past however these existing solutions not work without manual intervention which its the application scale to an extremely small size see table the earliest summarizer stems from rush et al which utilizes a feed forward network for compressing sentences and later is expanded by chopra et al with a current neural network rnn on this basis nallapati et al c and chen et al both present a set of rnn based models to dress various aspects of abstractive tion typically cheng and lapata pose a general summarizer where an encoder learns the representation of documents while a decoder generates each word sentence ing an attention mechanism with further search nallapati et al extend the tence compression by trying a hierarchical tion architecture and a limited vocabulary during the decoding phase next narayan et al leverage the side information as an attention cue to locate focus regions for summaries recently inspired by pagerank tan et al introduce a graph based attention mechanism to tackle the saliency problem nonetheless these methods all discuss the single document scenario which is far from the nature of automatic related work rization in this study derived from the general summarizer of cheng and lapata we pose a joint context driven attention mechanism to measure the contextual relevance within full texts and a heterogeneous bibliography graph neously to our best knowledge we make the rst attempt to develop a neural data driven solution for the automatic related work summarization and the practice of using the joint context as an tion cue is also less explored to date besides this study is launched on a dataset with up to pers which is much greater than previous studies and makes our results more convincing text since summarization via word word generation is not mature at present cheng and lapata nallapati et al tan et al we adopt the extractive tial fashion for our summarizer where a related work section is created by extracting and linking sentences from a reference collection meanwhile this study follows the mode of cong and kan who assume that the collection is given as part of the input and do not consider the citation sentences of each reference methodology problem formulation to adapt the paradigm we formulate the automatic related work summarization into a quential text generation problem as follows given an unedited paper t target document and its n size reference collection rt rt n we draw up a related work section for t by ing sentences from rt to be specic each ence source document will be traversed one time sequentially and without loss of generality in the descending order of their signicance to t sequently all sentences to be selected are nated into an m length sequence st st m to feed the summarizer for each candidate sentence j once being visited a label yt st j will be determined synchronously based on whether or not this sentence should be covered into the output our objective is to maximize the likelihood probability of observed labels yt yt m under rt st and summarizer parameters as shown below max log j rt st m x random walk on heterogeneous bibliography graph author coauthor written by cite contribute join paper relevant publish venue contribute keyword investigate contribute figure heterogeneous bibliography graph prior works have illustrated that one of the most promising channels for information dation is the community network guo and liu in this study we verify this hypothesis toward the content sampling of scientic rization by investigating heterogeneous relations among different kinds of objects such as papers authors keywords and venues z for measuring the relationship among tic publications we introduce a directed graph g v e to contain various bibliographical nections as shown in figure which involves four objects and ten edge types in total each edge ej i e is assigned a value to cate the transition probability between two nodes vj v where i r returns the known edge type usefulness of ej i and r is a normalizing weight for most of edge types we model the weight as one divided by the number of outgoing links of the same kind but ing the contribution category the weight eling is accomplished by pagerank with priors white and smyth note that different edge types usually take very uneven importance in one particular task yu et al and it is quite cult to enable the classical heterogeneous graph mining without expert dened paths for random walk bu et al swami et al in this study we propose an unsupervised proach to capture the connectivity diversity by introducing an optimal eud for navigating dom walkers on the heterogeneous bibliography graph given a target document t the optimized usefulness assignment can help those walkers lock a top n recommendation rt to best match the erence collection rt as shown in eq on this basis a well performing algorithm grover and leskovec is adopted to duct an unsupervised random walk to vectorize ery node v v into a dimensional ding rd so that any edge e e can be calculated therefrom specically we employ evolutionary algorithm ea to tune the eud which enjoys advantages over conventional ent methods in both convergence speed and racy arg max log j rt eud n x t x ea setup we use an array of real numbers to code an individual in the population where denotes the usefulness of j th edge type given an eud pagerank page runs on graph to infer the relative importance of each node for each target document and a tness tion is applied to judge how well this eud es locating the ground truth references as eq j rt n in which if rt j belongs to rt then returns the ranking of rt j within rt and otherwise a big penalty coefcient to prevent irrelevant erences to be recommended like most other timizations this procedure starts with a randomly generated population max pt p j n j ea operator we choose the operator from ferential evolution das and suganthan to generate offsprings for each individual the basic idea is to utilize the difference between different individuals to disturb each trial object first three distinct individuals are sampled randomly from current population to create a as shown in eq where r ant xvar dicates the scaling factor next xvar is crossed to build a hybrid one xhyb with a trial object xtri as eq in which denotes the crossover factor and u represents an uniform random number at last the tnesses of xtri are compared and the better one will be saved as the offspring into a new round of evolution and xhyb j xvar j f j j xhyb xvar j xtri j if u c otherwise neural extractive summarization as figure shows we model our rizer with a hierarchical encoder and an based decoder as described below hierarchical encoder our encoder consists of two major layers namely a convolutional ral network cnn and a long short term ory rnn specically the cnn deals with word level texts to derive level meanings which are then taken as inputs to the rnn for handling longer range dependency within lager units like a paragraph and even a whole paper this conforms to the nature of ment that is composed from words sentences and higher levels of abstraction narayan et al where each word wt j i can be represented by a j i rd dimensional embedding ous studies have illustrated the strength of cnn in presenting sentences because of its capability to learn compressed expressions and address tences with variable lengths kim first a convolution kernel k rdqd is applied to each possible window of q words to construct a list of feature maps as consider a sentence of p words j wt gt j i tanh k i where rd denotes the bias term next over time pooling collobert et al is formed on all generated features to obtain the tence embedding as max gt where i denotes the i th row of matrix given a sequence of sentences st st m we then take the rnn to yield an equal length array of hidden states in which lstm has proved to leviate the vanishing gradient problem when ing long sequences hochreiter and schmidhuber each hidden state can be viewed as a cal representation with focusing on current and former sentences together which is updated as ht j lstm in practice we use multiple kernels with ous widths to produce a group of embeddings for j ht rd t t t t t thi t t hidden state node embedding t t attention j context vector t t tyi t t binary decision trn t j th j m attention based decoder t j ty j tym max over time pooling convolution average ts feature map sentence embedding t j t j t j word embedding hierarchical encoder figure framework of our summarizer each sentence and average them to capture the information inside different n grams as figure bottom shows the sentence st j involves six words and two kernels of widths two orange and three green abstract a set of ve and four ture maps respectively meanwhile since cal structure theory mann and thompson points out that association must exist in any two parts of coherent texts rnn is only applicable to manage the sentence relation within a single ument because we can not expect the dependency between two sections from different references attention based decoder our decoder labels each sentence st j as sequentially according to whether it is salient or novel enough plus if vant to the target document t or not as shown in figure top the binary decision yt j is made by j and the context vector ht both the hidden state ht j from an attention mechanism grey background in particular this attention red dash line is acted as an intermediate stage to determine which tences to highlight so as to provide the contextual information for current decision bahdanau et al given ht ht m this decoder returns the probability of yt as below rt st sigmoid j ht j ht j aj iht i m x j ht r denotes a fully connected where j and ht layer with as input the concatenation of ht j and aj i is the attention weight indicating how much the supporting sentence st i contributes to extracting the candidate one st j apart from saliency and novelty two traditional attention factors chen et al tan et al we focus on the contextual relevance within both textual and graphic contexts to distinguish the relationship from near to far as shown in eq and eq to be specic htt i i to st sents the saliency of st j dtt i cates the novelty of st i to the dynamic output dt j to t from the textual context i refers to the relevance from the graphic context more i denotes the relevance of st i wsht j wnht concretely w rd characterizes the learnable matrix returns the average of hidden states from t and return the node dings of both t and the source document that ht i belongs to respectively note that and represent two distinct embedding spaces where the former reects the lexical collocations of pus and the latter embodies the connectivity terns of associated graph aj wsht htt i saliency dtt j wnht i novelty i dt j rt st ht i x the basic idea behind our attention mechanism if a supporting sentence more is as follows sembles a candidate one or overlaps less with the dynamic output or is more relevant to the target document then it can provide more contextual formation to facilitate current decision on being extracted or not thereby taking a higher weight in the generated context vector this innovative tention will guide our goal related work section to maximize the representativeness of selected tences saliency novelty while minimizing the semantic distance to the target document vance this is consistent with the way that ars consume a reference collection with the max objective in their minds experiment experimental setup this section presents the experimental setup for assessing our approach including dataset used for training and testing implementation details contrast methods and evaluation metrics dataset we conduct experiments on a created from the acm digital library where data and full texts are derived from pdf les to be detailed this dataset includes papers help outcome we while com readers share copyrighted the experiment is reproduce the of information experiment data removed part the authors keywords and venues in total note that we ignore the keyword with frequency below a certain threshold and adopt greedy matching of guo et al to generate pseudo keywords for papers lacking topic tions for each target document the references are traversed by the descending order of the cited number in related work section primary and in full paper secondary successively we rst ply a series of pre processings such as lowercasing and stemming to standardize candidate sentences then remove those which are too short long or words on this basis a total of papers are selected to evaluate our approach each containing more than references found in the dataset and a related work section of at least words but as for the heterogeneous bibliography graph all source data have to be imported to sure the structural integrity of communities sides this graph should be constructed year year to preclude the effect of later publications on earlier ones implementation we use tensorow for mentation where both the dimensions of ding and hidden state are equally for the cnn mikolov et al is utilized to initialize the word embeddings which can be further tuned during the training phase while we follow the work of kim to ply a list of kernels with widths as for the rnn each lstm module is set to one single layer and all input documents are padded to the same length along with a mark to indicate the real number of sentences based on these settings we train our summarizer using adam with the default in kingma and ba and perform mini batch cross entropy training with a batch of one target document for epochs to create training data for our summarizer each reference needs to be annotated with the ground i e candidate sentences are truth in advance tagged with for indicating summary worthy or not specically we follow a heuristic practice of cao et al and nallapati et al to compute score lin and hovy for each sentence in terms of the native related work sections gold standards next those tences with high scores are chosen as the positive samples and the rest as the negative ones such that the total score of selected sentences is imized with respect to the gold standard as for testing we relax the number of sentences to be lected and focus on the classication probability from eq in this study cross validation is plied to split the dataset into ten parts equally at random in which nine are used for training and the other one for testing evaluation we adopt the widely used toolkit rouge lin and hovy to evaluate the summarization performance automatically in ticular we report and gram and bigram overlapping as a way to assess the informativeness and rouge l the longest common subsequence as a means to assess the uency in terms of xed bytes of gold standards to validate the proposed attention nism we compare our approach denoted as p against six variants including p void a plain summarizer without tions p s use the saliency as an only leverage both the saliency tion factor p and novelty p incorporate the relevance from the textual context p gain the relevance from the graphic context of a geneous citation graph p utilize the heterogeneous bibliography graph but with each edge type the same usefulness in addition we also select six representative summarization methods as a benchmark group the rst one is the general summarizer by cheng and lapata denoted as net which employs an attention mechanism to tract sentences directly after reading them lowing are ve classical generic solutions ing luhn luhn a heuristic rization based on word frequency and tion mmr carbonell and goldstein a diversity based re ranking to produce summaries lexrank erkan et al a graph based summary technique inspired by pagerank and hits sumbasic nenkova and vanderwende a frequency based summarizer with plication removal nltksum acanfora et al a natural language tookit implementation for summarization for clarity luhn lexrank and sic are analogous to the work of hu and wan which extracts sentences scoring the and they are also trasted in the latest studies on neural ers chen et al tan et al while mmr often serves as a part post processing in signicance of existing techniques to avoid the redundancy cohan and goharian and we introduce nltksum to investigate the impact of ical semantic analysis to the automatic related work summarization note that former studies specially for this task require extensive human volvements see table thus we can not apply them to such a large dataset of this study results and discussion table reports the evaluation comparison over rouge metrics from the top half all scores pear a gradual upward trend with incorporation of saliency novelty relevance from both textual and graphic contexts and eud into consideration one after another which demonstrates the validity of our attention mechanism for summarizing related work sections to be specic we further reach the following conclusions p void vs p s vs p both saliency and novelty are two effective factors to locate the quired content for summaries which is consistent with prior studies p vs p contextual relevance does contribute to address the alignment between a lated work section and its source documents p vs p textual context alone can not provide entire evidence to characterize the relationship among scientic publications exactly p vs p heterogeneous liography graph involves richer contextual mation than a homogeneous citation graph p vs p eud plays an indispensable role in organizing accurate tual relevance on a heterogeneous graph figure number of extracted words on each reference cluster under different attention factors continuing the dssm figure visualizes the number of extracted words on each reference methods p void p s p p p p p luhn mmr lexrank sumbasic nltksum pointernet rouge l indicates wilcoxon signed rank test compared with p table rouge evaluation on papers from acm digital library under different attention factors it can be seen that only after adding the relevance pecially that from the graphic context into tions our summarizer can correctly sample the content from deep learning yellow line and eliminate that originated from other sources by a big margin green line as this example falls into the methodology transferring a host of its volved word collocations are not idiomatic binations yet such as deep neural network occurs with clickthrough data that is more quently related to latent semantic analysis at that time which results in a somewhat biased tual context by contrast the graphic context will suffer less from this bias because it characterizes the connectivity patterns real time setup instead of n gram statistics thus offering a more robust measure for the contextual relevance the bottom half of table illustrates the ority of our approach over six representative marization methods above all luhn lexrank and mmr three summarizers that simply exploit shallow text features word frequency and ciated sentence similarity to measure either nicance or redundancy fall far behind the plain variant p void which partly reects the strength of paradigm in summarizing a related work section second with combination of nicance and redundancy sumbasic achieves a drastic increase on and a mild raise on pack the references cited in the same subsection of the related work section as one reference cluster respectively but it still can not improve rouge l marginally this is because simple text statistics can not present deeper levels of ral language understanding to catch larger grained units of co occurrence third nltksum benets from a nltk library so as to access cal semantic supports thereby having the best formativeness and among the ve generic baselines and meanwhile a parable uency rouge l with our approach finally as a deep learning solution although pointernet takes both hidden states and previously labeled sentences into account at each decoding step it focuses on only current and just one vious sentences lacking a comprehensive eration on saliency novelty and more importantly the contextual relevance p to better verify the summarization mance we also conduct a human evaluation on papers containing more than references in the dataset we assign a number of raters to pare each generated related work section against the gold standard and judge by three independent aspects as how compliant is the related work section to the target document how intuitive is the related work section for readers to grasp the key content how useful is the related work section for researchers to prepare their nal ature reviews note that we do not allow any ties during the comparison and each property is sessed with a point scale of worst to best table displays how often raters rank each summarizer as the and so on in terms of methods luhn mmr lexrank sumbasic nltksum pointernet p mean ranking table human evaluation proportion on papers with more than references in the dataset best to worst specically our approach comes the on of the time which is followed by nltksum that is considered the best on of the time almost half of ours and net with quite equal proportions on each ing furthermore the other four summarizers count for obviously lower ratings in general to attain the statistical signicance one way sis of variance anova is performed on the tained ratings and the results show that our proach is better than all six contrast methods nicantly p which means that the clusion drawn by table is sustained conclusion in this paper we highlight the contextual vance for the automatic related work tion and analyze the graphic context to terize the relationship among scientic tions accurately we develop a neural data driven summarizer by leveraging the paradigm where a joint context driven attention mechanism is proposed to measure the contextual relevance within full texts and a heterogeneous phy graph simultaneously extensive experiments demonstrate the validity of the proposed attention mechanism and the superiority of our approach over six representative summarization baselines in future work an appealing direction is to ganize the selected sentences in a logical ion e by leveraging a topic hierarchy tree to determine the arrangement of the related work section cong and kan we also would like to take the citation sentences of each ence into consideration which is another concise and universal data source for scientic tion chen and hai cohan and goharian at the end of this paper we believe that extractive methods are by no means the nal lutions for literature review generation due to giarism concerns and we are going to put forward a fully abstractive version in further studies acknowledgement we would like to thank the anonymous reviewers for their valuable comments this work is partially supported by the national science foundation of china under grant no references joseph acanfora marc evangelista david keimig and myron su natural language ing generating a summary of ood disasters cell dzmitry bahdanau kyunghyun cho and yoshua gio neural machine translation by jointly arxiv preprint learning to align and translate dzmitry bahdanau jan chorowski dmitriy serdyuk philemon brakel and yoshua bengio to end attention based large vocabulary speech the ieee in proceedings of recognition icassp international conference on acoustics speech and signal processing shanghai china pages jiajun bu shulong tan chun chen can wang hao wu lijun zhang and xiaofei he music ommendation by unied hypergraph combining cial media information and music content in ceedings of the acm sigmm international ference on multimedia amsterdam netherlands pages ziqiang cao wenjie li sujian li furu wei and ran li attsum joint learning of focusing and summarization with neural attention arxiv preprint jaime carbonell and jade goldstein the use of mmr diversity based reranking for reordering uments and producing summaries in proceedings of the international acm sigir conference on research and development in information trieval new york usa pages jingqiang chen and zhuge hai summarization in proceedings of related work through citations of the ieee skg international conference on semantics knowledge and grids beijing china pages qian chen xiaodan zhu si wei si wei and hui jiang distraction based neural networks for in proceedings of the acm modeling documents ijcai international joint conference on articial intelligence new york usa pages jianpeng cheng and mirella lapata neural summarization by extracting sentences and words in proceedings of the acl annual meeting of the association for computational linguistics berlin germany sumit chopra michael auli and alexander m rush abstractive sentence summarization with in proceedings tentive recurrent neural networks of the naacl conference of the north american chapter of the association for computational guistics san diego usa pages arman cohan and nazli goharian tic article summarization using citation context arxiv preprint and article s discourse structure pages ronan collobert jason weston michael karlen ray kavukcuoglu and pavel kuksa natural language processing almost from scratch journal of machine learning research duy vu hoang cong and min yen kan towards in automated related work summarization ceedings of the acm coling international conference on computational linguistics beijing china pages swagatam das and ponnuthurai nagaratnam than differential evolution a survey of the state of the art ieee transactions on evolutionary computation erkan radev and r dragomir lexrank based lexical centrality as salience in text rization journal of qiqihar junior teachers lege aditya grover and jure leskovec scalable feature learning for networks in ings of the acm sigkdd international ference on knowledge discovery and data mining san francisco usa pages chun guo and xiaozhong liu automatic ture generation on heterogeneous graph for music in proceedings of the recommendation ternational acm sigir conference on research and development in information retrieval ago chile pages chun guo jinsong zhang and xiaozhong liu scientic metadata quality enhancement for arly publications ischools sepp hochreiter and jrgen schmidhuber long short term memory neural computation yue hu and xiaojun wan automatic ation of related work sections in scientic papers in proceedings of the an optimization approach acl emnlp conference on empirical methods in natural language processing doha qatar pages po sen huang xiaodong he jianfeng gao li deng alex acero and larry heck learning deep structured semantic models for web search using in proceedings of the clickthrough data acm cikm international conference on tion knowledge management san francisco usa pages scott jensen xiaozhong liu yingying yu and stasa milojevic generation of topic evolution trees from heterogeneous bibliographic networks nal of informetrics yoon kim convolutional neural networks for sentence classication eprint arxiv diederik kingma and jimmy ba adam a method for stochastic optimization computer ence chin yew lin and eduard hovy matic evaluation of summaries using n gram occurrence statistics in proceedings of the naacl the annual conference of the north american chapter of the association for computational guistics stroudsburg usa pages h p luhn the automatic creation of literature abstracts ibm corp william c mann and sandra a thompson rhetorical structure theory toward a functional ory of text organization text talk tomas mikolov kai chen greg corrado and jeffrey dean efcient estimation of word tations in vector space computer science ramesh nallapati bing xiang and bowen zhou sequence to sequence rnns for text rization in proceedings of the international ence on learning representations workshop track san juan puerto rico ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based sequence model for extractive summarization of documents arxiv preprint ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre and bing xiang abstractive text summarization using arxiv preprint to sequence rnns and beyond ramesh m nallapati amr ahmed eric p xing and william w cohen joint latent topic models in proceedings of the for text and citations acm sigkdd international conference on edge discovery and data mining las vegas usa pages shashi narayan nikos papasarantopoulos shay b cohen and mirella lapata neural tive summarization with side information arxiv preprint ani nenkova and lucy vanderwende the pact of frequency on summarization microsoft search l page the pagerank citation ranking ing order to the web online manuscript stanford digital libraries working paper alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of the acl tence summarization emnlp conference on empirical methods in ural language processing lisbon portugal pages ananthram swami ananthram swami and thram swami scalable resentation learning for heterogeneous networks in proceedings of the acm sigkdd national conference on knowledge discovery and data mining halifax canada pages jiwei tan xiaojun wan jianguo xiao jiwei tan aojun wan and jianguo xiao abstractive document summarization with a graph based tional neural model in proceedings of the acl annual meeting of the association for tional linguistics vancouver canada pages scott white and padhraic smyth algorithms in for estimating relative importance in networks proceedings of the acm sigkdd international conference on knowledge discovery and data ing washington usa pages dwi h widyantoro and imaduddin amin tation sentence identication and classication for in proceedings of related work summarization the icacsis international conference on advanced computer science and information systems pages yingying yu xiaozhong liu and zhuoren jiang random walk and feedback on scholarly network in proceedings of the acm national workshop on graph search and beyond santiago chile pages
