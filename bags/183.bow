n j l c s c v v x r neural related work summarization joint context driven attention mechanism yongzhen xiaozhong zheng maritime economics management dalian maritime university dalian china informatics computing engineering indiana university bloomington bloomington usa group hangzhou china com edu iu edu abstract conventional solutions automatic related work summarization rely heavily engineered features paper develop neural data driven summarizer ing paradigm joint context driven attention mechanism posed measure contextual relevance texts heterogeneous raphy graph simultaneously motivation maintain topic coherency related work section target document textual graphic contexts play big role characterizing ship scientic publications accurately experimental results large dataset approach achieves considerable provement typical summarizer ve classical summarization baselines introduction scientic elds scholars need contextualize contribution help readers acquire derstanding research papers pose related work section article serves pivot connect prior domain knowledge innovation superiority current work displayed comparison ous studies citation prediction assist drafting reference collection nallapati et al consuming papers rious job authors read source document carefully locate relevant content cautiously solution saving authors efforts tomatic related work summarization tially topic biased multi document problem cong kan relies heavily human engineered features retrieve snippets corresponding author references recently neural works enable data driven architecture sequence natural language ation bahdanau et al coder reads sequence words sentences context vector decoder yields sequence specic outputs nonetheless pared scenarios like machine translation end end nature aligning related work section source documents far challenging address summarization alignment mer studies try apply attention mechanism measure saliency novelty candidate word sentence tan et al aim locating representative content retain primary coverage summarizing related work section authors ative organizing text streams ence collection selected content ought highlight topic bias current work retell reference compressed anced fashion motivates introduce contextual relevance characterize ship scientic publications accurately generally speaking pair documents larger lexical overlap implies higher ilarity research backgrounds hypothesis true sampling tent multiple relevant topics example viewpoint abstract ilarity references investigating information retrieval latent semantic model data mining tance correlation greatly sampled related work section reality ticle spends bit larger chunk texts elaborate deep learning ture review difcult machines deep structured semantic models web search clickthrough data huang et al grasp contextual relevance tion situations like emerging new concepts suffer terminology variation phrasing varying degrees study utilize heterogeneous ography graph embody relationship scalable scholarly database recent past surge interest exploiting diverse lations analyze bibliometrics ranging erature recommendation yu et al topic evolvement jensen et al graphical sense interconnected papers transfer credit directly indirectly ious patterns paper citation author laboration keyword association releasing series venues constitutes graphic context outlining concerned topics nately variety edge types pollute formation inquiry slice edges important sampling content existing solutions mining erogeneous graphs depend human sion e hyperedge bu et al ath swami et al usually easy access complexity graph schemas contribution threefold explore edge type usefulness distribution eud heterogeneous bibliography graph ables relationship discovery pair papers sampling interested tion second develop novel rizer automatic related work tion joint context driven attention anism proposed measure contextual evance textual graphic contexts conduct experiments papers native related work sections tal results approach outperforms typical summarizer ve classical summarization baselines signicantly related work study touches strands research automatic related work summarization summarizer follows idea creating related work section tomatically pioneered cong kan design rule based strategies extract sentences general detailed topics tively subsequently hu wan ploit probabilistic latent semantic indexing authors cong kan hu wan widyantoro amin chen hai number papers table data scales previous studies automatic related work summarization topic biased split candidate texts different parts apply regression models learn importance sentence similarly widyantoro amin transform marization problem classifying rhetorical egories sentences sentence resented feature vector containing word quency sentence length recently chen hai construct graph sentative keywords minimum steiner tree gured guide summarization nding number sentences cover general compared discriminated nodes traditional summaries automatic related work summarization receives concerns past existing solutions work manual intervention application scale extremely small size table earliest summarizer stems rush et al utilizes feed forward network compressing sentences later expanded chopra et al current neural network rnn basis nallapati et al c chen et al present set rnn based models dress aspects abstractive tion typically cheng lapata pose general summarizer encoder learns representation documents decoder generates word sentence ing attention mechanism search nallapati et al extend tence compression trying hierarchical tion architecture limited vocabulary decoding phase narayan et al leverage information attention cue locate focus regions summaries recently inspired pagerank tan et al introduce graph based attention mechanism tackle saliency problem nonetheless methods discuss single document scenario far nature automatic related work rization study derived general summarizer cheng lapata pose joint context driven attention mechanism measure contextual relevance texts heterogeneous bibliography graph neously best knowledge rst attempt develop neural data driven solution automatic related work summarization practice joint context tion cue explored date study launched dataset pers greater previous studies makes results convincing text summarization word word generation mature present cheng lapata nallapati et al tan et al adopt extractive tial fashion summarizer related work section created extracting linking sentences reference collection study follows mode cong kan assume collection given input consider citation sentences reference methodology problem formulation adapt paradigm formulate automatic related work summarization quential text generation problem follows given unedited paper t target document n size reference collection rt rt n draw related work section t ing sentences rt specic ence source document traversed time sequentially loss generality descending order signicance t sequently sentences selected nated m length sequence st st m feed summarizer candidate sentence j visited label yt st j determined synchronously based sentence covered output objective maximize likelihood probability observed labels yt yt m rt st summarizer parameters shown max log j rt st m x random walk heterogeneous bibliography graph author coauthor written cite contribute join paper relevant publish venue contribute keyword investigate contribute figure heterogeneous bibliography graph prior works illustrated promising channels information dation community network guo liu study verify hypothesis content sampling scientic rization investigating heterogeneous relations different kinds objects papers authors keywords venues z measuring relationship tic publications introduce directed graph g v e contain bibliographical nections shown figure involves objects edge types total edge ej e assigned value cate transition probability nodes vj v r returns known edge type usefulness ej r normalizing weight edge types model weight divided number outgoing links kind ing contribution category weight eling accomplished pagerank priors white smyth note different edge types usually uneven importance particular task yu et al cult enable classical heterogeneous graph mining expert dened paths random walk bu et al swami et al study propose unsupervised proach capture connectivity diversity introducing optimal eud navigating dom walkers heterogeneous bibliography graph given target document t optimized usefulness assignment help walkers lock n recommendation rt best match erence collection rt shown eq basis performing algorithm grover leskovec adopted duct unsupervised random walk vectorize ery node v v dimensional ding rd edge e e calculated therefrom specically employ evolutionary algorithm ea tune eud enjoys advantages conventional ent methods convergence speed racy arg max log j rt eud n x t x ea setup use array real numbers code individual population denotes usefulness j th edge type given eud pagerank page runs graph infer relative importance node target document tness tion applied judge eud es locating ground truth references eq j rt n rt j belongs rt returns ranking rt j rt big penalty coefcient prevent irrelevant erences recommended like timizations procedure starts randomly generated population max pt p j n j ea operator choose operator ferential evolution das suganthan generate offsprings individual basic idea utilize difference different individuals disturb trial object distinct individuals sampled randomly current population create shown eq r ant xvar dicates scaling factor xvar crossed build hybrid xhyb trial object xtri eq denotes crossover factor u represents uniform random number tnesses xtri compared better saved offspring new round evolution xhyb j xvar j f j j xhyb xvar j xtri j u c neural extractive summarization figure shows model rizer hierarchical encoder based decoder described hierarchical encoder encoder consists major layers convolutional ral network cnn long short term ory rnn specically cnn deals word level texts derive level meanings taken inputs rnn handling longer range dependency lager units like paragraph paper conforms nature ment composed words sentences higher levels abstraction narayan et al word wt j represented j rd dimensional embedding ous studies illustrated strength cnn presenting sentences capability learn compressed expressions address tences variable lengths kim convolution kernel k rdqd applied possible window q words construct list feature maps consider sentence p words j wt gt j tanh k rd denotes bias term time pooling collobert et al formed generated features obtain tence embedding max gt denotes th row matrix given sequence sentences st st m rnn yield equal length array hidden states lstm proved leviate vanishing gradient problem ing long sequences hochreiter schmidhuber hidden state viewed cal representation focusing current sentences updated ht j lstm practice use multiple kernels ous widths produce group embeddings j ht rd t t t t t thi t t hidden state node embedding t t attention j context vector t t tyi t t binary decision trn t j th j m attention based decoder t j ty j tym max time pooling convolution average ts feature map sentence embedding t j t j t j word embedding hierarchical encoder figure framework summarizer sentence average capture information inside different n grams figure shows sentence st j involves words kernels widths orange green abstract set ve ture maps respectively cal structure theory mann thompson points association exist parts coherent texts rnn applicable manage sentence relation single ument expect dependency sections different references attention based decoder decoder labels sentence st j sequentially according salient novel plus vant target document t shown figure binary decision yt j j context vector ht hidden state ht j attention mechanism grey background particular attention red dash line acted intermediate stage determine tences highlight provide contextual information current decision bahdanau et al given ht ht m decoder returns probability yt rt st sigmoid j ht j ht j aj iht m x j ht r denotes fully connected j ht layer input concatenation ht j aj attention weight indicating supporting sentence st contributes extracting candidate st j apart saliency novelty traditional attention factors chen et al tan et al focus contextual relevance textual graphic contexts distinguish relationship near far shown eq eq specic htt st sents saliency st j dtt cates novelty st dynamic output dt j t textual context refers relevance graphic context denotes relevance st wsht j wnht concretely w rd characterizes learnable matrix returns average hidden states t return node dings t source document ht belongs respectively note represent distinct embedding spaces reects lexical collocations pus embodies connectivity terns associated graph aj wsht htt saliency dtt j wnht novelty dt j rt st ht x basic idea attention mechanism supporting sentence follows sembles candidate overlaps dynamic output relevant target document provide contextual formation facilitate current decision extracted taking higher weight generated context vector innovative tention guide goal related work section maximize representativeness selected tences saliency novelty minimizing semantic distance target document vance consistent way ars consume reference collection max objective minds experiment experimental setup section presents experimental setup assessing approach including dataset training testing implementation details contrast methods evaluation metrics dataset conduct experiments created acm digital library data texts derived pdf les detailed dataset includes papers help outcome com readers share copyrighted experiment reproduce information experiment data removed authors keywords venues total note ignore keyword frequency certain threshold adopt greedy matching guo et al generate pseudo keywords papers lacking topic tions target document references traversed descending order cited number related work section primary paper secondary successively rst ply series pre processings lowercasing stemming standardize candidate sentences remove short long words basis total papers selected evaluate approach containing references found dataset related work section words heterogeneous bibliography graph source data imported sure structural integrity communities sides graph constructed year year preclude effect later publications earlier ones implementation use tensorow mentation dimensions ding hidden state equally cnn mikolov et al utilized initialize word embeddings tuned training phase follow work kim ply list kernels widths rnn lstm module set single layer input documents padded length mark indicate real number sentences based settings train summarizer adam default kingma ba perform mini batch cross entropy training batch target document epochs create training data summarizer reference needs annotated ground e candidate sentences truth advance tagged indicating summary worthy specically follow heuristic practice cao et al nallapati et al compute score lin hovy sentence terms native related work sections gold standards tences high scores chosen positive samples rest negative ones total score selected sentences imized respect gold standard testing relax number sentences lected focus classication probability eq study cross validation plied split dataset parts equally random training testing evaluation adopt widely toolkit rouge lin hovy evaluate summarization performance automatically ticular report gram bigram overlapping way assess informativeness rouge l longest common subsequence means assess uency terms xed bytes gold standards validate proposed attention nism compare approach denoted p variants including p void plain summarizer tions p s use saliency leverage saliency tion factor p novelty p incorporate relevance textual context p gain relevance graphic context geneous citation graph p utilize heterogeneous bibliography graph edge type usefulness addition select representative summarization methods benchmark group rst general summarizer cheng lapata denoted net employs attention mechanism tract sentences directly reading lowing ve classical generic solutions ing luhn luhn heuristic rization based word frequency tion mmr carbonell goldstein diversity based ranking produce summaries lexrank erkan et al graph based summary technique inspired pagerank hits sumbasic nenkova vanderwende frequency based summarizer plication removal nltksum acanfora et al natural language tookit implementation summarization clarity luhn lexrank sic analogous work hu wan extracts sentences scoring trasted latest studies neural ers chen et al tan et al mmr serves post processing signicance existing techniques avoid redundancy cohan goharian introduce nltksum investigate impact ical semantic analysis automatic related work summarization note studies specially task require extensive human volvements table apply large dataset study results discussion table reports evaluation comparison rouge metrics half scores pear gradual upward trend incorporation saliency novelty relevance textual graphic contexts eud consideration demonstrates validity attention mechanism summarizing related work sections specic reach following conclusions p void vs p s vs p saliency novelty effective factors locate quired content summaries consistent prior studies p vs p contextual relevance contribute address alignment lated work section source documents p vs p textual context provide entire evidence characterize relationship scientic publications exactly p vs p heterogeneous liography graph involves richer contextual mation homogeneous citation graph p vs p eud plays indispensable role organizing accurate tual relevance heterogeneous graph figure number extracted words reference cluster different attention factors continuing dssm figure visualizes number extracted words reference methods p void p s p p p p p luhn mmr lexrank sumbasic nltksum pointernet rouge l indicates wilcoxon signed rank test compared p table rouge evaluation papers acm digital library different attention factors seen adding relevance pecially graphic context tions summarizer correctly sample content deep learning yellow line eliminate originated sources big margin green line example falls methodology transferring host volved word collocations idiomatic binations deep neural network occurs clickthrough data quently related latent semantic analysis time results somewhat biased tual context contrast graphic context suffer bias characterizes connectivity patterns real time setup instead n gram statistics offering robust measure contextual relevance half table illustrates ority approach representative marization methods luhn lexrank mmr summarizers simply exploit shallow text features word frequency ciated sentence similarity measure nicance redundancy fall far plain variant p void partly reects strength paradigm summarizing related work section second combination nicance redundancy sumbasic achieves drastic increase mild raise pack references cited subsection related work section reference cluster respectively improve rouge l marginally simple text statistics present deeper levels ral language understanding catch larger grained units co occurrence nltksum benets nltk library access cal semantic supports having best formativeness ve generic baselines parable uency rouge l approach finally deep learning solution pointernet takes hidden states previously labeled sentences account decoding step focuses current vious sentences lacking comprehensive eration saliency novelty importantly contextual relevance p better verify summarization mance conduct human evaluation papers containing references dataset assign number raters pare generated related work section gold standard judge independent aspects compliant related work section target document intuitive related work section readers grasp key content useful related work section researchers prepare nal ature reviews note allow ties comparison property sessed point scale worst best table displays raters rank summarizer terms methods luhn mmr lexrank sumbasic nltksum pointernet p mean ranking table human evaluation proportion papers references dataset best worst specically approach comes time followed nltksum considered best time half net equal proportions ing furthermore summarizers count obviously lower ratings general attain statistical signicance way sis variance anova performed tained ratings results proach better contrast methods nicantly p means clusion drawn table sustained conclusion paper highlight contextual vance automatic related work tion analyze graphic context terize relationship scientic tions accurately develop neural data driven summarizer leveraging paradigm joint context driven attention mechanism proposed measure contextual relevance texts heterogeneous phy graph simultaneously extensive experiments demonstrate validity proposed attention mechanism superiority approach representative summarization baselines future work appealing direction ganize selected sentences logical ion e leveraging topic hierarchy tree determine arrangement related work section cong kan like citation sentences ence consideration concise universal data source scientic tion chen hai cohan goharian end paper believe extractive methods means nal lutions literature review generation giarism concerns going forward fully abstractive version studies acknowledgement like thank anonymous reviewers valuable comments work partially supported national science foundation china grant references joseph acanfora marc evangelista david keimig myron su natural language ing generating summary ood disasters cell dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly arxiv preprint learning align translate dzmitry bahdanau jan chorowski dmitriy serdyuk philemon brakel yoshua bengio end attention based large vocabulary speech ieee proceedings recognition icassp international conference acoustics speech signal processing shanghai china pages jiajun bu shulong tan chun chen wang hao wu lijun zhang xiaofei music ommendation unied hypergraph combining cial media information music content ceedings acm sigmm international ference multimedia amsterdam netherlands pages ziqiang cao wenjie li sujian li furu wei ran li attsum joint learning focusing summarization neural attention arxiv preprint jaime carbonell jade goldstein use mmr diversity based reranking reordering uments producing summaries proceedings international acm sigir conference research development information trieval new york usa pages jingqiang chen zhuge hai summarization proceedings related work citations ieee skg international conference semantics knowledge grids beijing china pages qian chen xiaodan zhu si wei si wei hui jiang distraction based neural networks proceedings acm modeling documents ijcai international joint conference articial intelligence new york usa pages jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings acl annual meeting association computational linguistics berlin germany sumit chopra michael auli alexander m rush abstractive sentence summarization proceedings tentive recurrent neural networks naacl conference north american chapter association computational guistics san diego usa pages arman cohan nazli goharian tic article summarization citation context arxiv preprint article s discourse structure pages ronan collobert jason weston michael karlen ray kavukcuoglu pavel kuksa natural language processing scratch journal machine learning research duy vu hoang cong min yen kan automated related work summarization ceedings acm coling international conference computational linguistics beijing china pages swagatam das ponnuthurai nagaratnam differential evolution survey state art ieee transactions evolutionary computation erkan radev r dragomir lexrank based lexical centrality salience text rization journal qiqihar junior teachers lege aditya grover jure leskovec scalable feature learning networks ings acm sigkdd international ference knowledge discovery data mining san francisco usa pages chun guo xiaozhong liu automatic ture generation heterogeneous graph music proceedings recommendation ternational acm sigir conference research development information retrieval ago chile pages chun guo jinsong zhang xiaozhong liu scientic metadata quality enhancement arly publications ischools sepp hochreiter jrgen schmidhuber long short term memory neural computation yue hu xiaojun wan automatic ation related work sections scientic papers proceedings optimization approach acl emnlp conference empirical methods natural language processing doha qatar pages po sen huang xiaodong jianfeng gao li deng alex acero larry heck learning deep structured semantic models web search proceedings clickthrough data acm cikm international conference tion knowledge management san francisco usa pages scott jensen xiaozhong liu yingying yu stasa milojevic generation topic evolution trees heterogeneous bibliographic networks nal informetrics yoon kim convolutional neural networks sentence classication eprint arxiv diederik kingma jimmy ba adam method stochastic optimization computer ence chin yew lin eduard hovy matic evaluation summaries n gram occurrence statistics proceedings naacl annual conference north american chapter association computational guistics stroudsburg usa pages h p luhn automatic creation literature abstracts ibm corp william c mann sandra thompson rhetorical structure theory functional ory text organization text talk tomas mikolov kai chen greg corrado jeffrey dean efcient estimation word tations vector space computer science ramesh nallapati bing xiang bowen zhou sequence sequence rnns text rization proceedings international ence learning representations workshop track san juan puerto rico ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents arxiv preprint ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre bing xiang abstractive text summarization arxiv preprint sequence rnns ramesh m nallapati amr ahmed eric p xing william w cohen joint latent topic models proceedings text citations acm sigkdd international conference edge discovery data mining las vegas usa pages shashi narayan nikos papasarantopoulos shay b cohen mirella lapata neural tive summarization information arxiv preprint ani nenkova lucy vanderwende pact frequency summarization microsoft search l page pagerank citation ranking ing order web online manuscript stanford digital libraries working paper alexander m rush sumit chopra jason weston neural attention model abstractive proceedings acl tence summarization emnlp conference empirical methods ural language processing lisbon portugal pages ananthram swami ananthram swami thram swami scalable resentation learning heterogeneous networks proceedings acm sigkdd national conference knowledge discovery data mining halifax canada pages jiwei tan xiaojun wan jianguo xiao jiwei tan aojun wan jianguo xiao abstractive document summarization graph based tional neural model proceedings acl annual meeting association tional linguistics vancouver canada pages scott white padhraic smyth algorithms estimating relative importance networks proceedings acm sigkdd international conference knowledge discovery data ing washington usa pages dwi h widyantoro imaduddin amin tation sentence identication classication proceedings related work summarization icacsis international conference advanced computer science information systems pages yingying yu xiaozhong liu zhuoren jiang random walk feedback scholarly network proceedings acm national workshop graph search santiago chile pages
