supmmd a sentence importance model for extractive summarization using maximum mean discrepancy umanga alexander patrick aditya krishna menon lexing national university canberra act australia google research new york ny united states bista alex mathews lexing edu au com t c o l c s c v v i x r a abstract most work on multi document summarization has focused on generic summarization of mation present in each individual document set however the under explored setting of update summarization where the goal is to identify the new information present in each set is of equal practical interest e presenting readers with updates on an evolving news topic in this work we present supmmd a novel technique for generic and update summarization based on the maximum mean discrepancy from kernel two sample testing supmmd combines both supervised learning for salience and vised learning for coverage and diversity ther we adapt multiple kernel learning to make use of similarity across multiple information sources e text features and knowledge based concepts we show the efcacy of supmmd in both generic and update summarization tasks by meeting or exceeding the current state of the art on the and datasets introduction multi document summarization is the problem of producing condensed digests of salient information from multiple sources such as articles concretely suppose we are given two sets of articles denoted set a and set b on a related topic e climate change the pandemic separated by publication timestamp or geographic region we may then identify three possible instantiations of multi document summarization see figure i generic summarization where the goal is to summarize a set a or b individually comparative summarization where the goal is to summarize a set b against another set a while highlighting the differences iii update summarization where the goal is both generic summarization of set a and comparative summarization of set b versus a most existing work on this topic has focused on the generic summarization task however figure different summarization tasks generic comparative c update two sets of articles set a and b are denoted by red and blue circles respectively summary prototypes are bold circles and information coverage of each tasks is lled with respective colors update summarization is of equal practical interest intuitively the comparative aspect of this setting aims to inform a user of new information on a topic they are already familiar with extractive multi document summarization methods can be unsupervised or supervised pervised methods typically dene salience or erage using a global model of sentence sentence similarity methods based on retrieval goldstein et al centroids radev et al graph centrality erkan and radev or utility maximization lin and bilmes gillick and favre have been well explored however sentence salience also depends on surface features e position length presence of cue words effectively capturing these requires supervised models specic to the dataset and task a body of work has incorporated such information through supervised learning for example based on point processes kulesza and taskar learning important words hong and nenkova graph neural networks yasunaga et al and support vector regression varma et al these supervised methods have either a separate model for learning and inference leading to a disconnect between learning sentence salience and sentence selection varma et al yasunaga et al hong and nenkova or are designed specically for generic summarization kulesza and taskar in this work we propose supmmd which has a single model of learning salience and inference and can be applied to generic and comparative summarization we make the following contributions we present supmmd a novel technique for both generic and update summarization that combines supervised learning for salience and unsupervised learning for coverage and diversity supmmd has a single model for learning and inference we adapt multiple kernel learning cortes et al into our model which allows similarity across multiple information sources e text features and knowledge based concepts to be used we show that supmmd meets or exceeds the state of the art in generic and update tion on the and datasets literature review multi document summarization can be extractive where salient pieces of the original text such as sentences are selected to form the summary or abstractive where a new text is generated by paraphrasing important information the former is popular as it often creates semantically and matically correct summaries nallapati et al in this work we focus on generic and update document summarization in the extractive setting most extractive summarizers have two nents sentence scoring and selection a variety of unsupervised and supervised methods have been developed for the former unsupervised sentence scorers are based on centroids radev et al graph centrality erkan and radev retrieval relevance goldstein et al word statistics nenkova and vanderwende topic models haghighi and vanderwende or concept coverage gillick and favre lin and bilmes supervised techniques include using a graph based neural network yasunaga et al learning sentence quality from point processes kulesza and taskar combining word importances hong and nenkova combining sentence and phrase importances cao et al or employing a mixture of submodular functions lin and bilmes sentence selection methods can be broadly egorized as greedy methods goldstein et al radev et al erkan and radev nenkova and vanderwende cao et al haghighi and vanderwende hong and nenkova kulesza and taskar cao et al varma et al which produce approximate solutions by iteratively selecting the sentences with the maximal score or exact integer linear programming ilp based methods gillick and favre cao et al some greedy methods use an objective which belongs to a special class of set functions called submodular functions lin and bilmes kulesza and taskar which have good approximation guarantees under greedy optimization nemhauser et al there has been limited research into update and comparative summarization notable prior work includes maximizing concept coverage using ilp gillick et al learning sentence scores using a support vector regressor varma et al and temporal content ltering zhang et al bista et al cast the comparative summarization problem as classication and use mmd gretton et al in this work we adapt their method to learn sentence importances driven by surface features summarization as classication we review a perspective introduced by bista et al where summarization is viewed as classication and provide a brief introduction to maximum mean discrepancy mmd both these ideas form the basis of our subsequent method generic summarization as classication let v be t topics of articles that we wish to summarize for a topic t we wish to select summary sentences st bista et al lated summarization as selecting prototypes that minimize the accuracy of a powerful classier between sentences in the input and summary the intuition is that a powerful classier should not be able to distinguish between the sentences from articles and summary sentences formally we pick st argmax t s sst where st l comprise subsets of v t with upto l words and y is the accuracy of the best possible classier that distinguishes between elements in sets x and y we shall shortly realize this using mmd comparative summarization as competing binary classication for comparative summarization between two sets a and b bista et al introduced an additional term into giving rise to competing goals for the classier it should not be able to distinguish between the summaries and sentences from set b but should be able to distinguish between the maries and sentences from set a formally let v t b be the set of sentences in set b v t a be the sentences in set to compare set a then for suitable we seek st the summary sentences of set b st argmax t b t a sst the hyperparameter controls the relative tance of accurately representing articles in set b versus not representing the articles in set a maximum mean discrepancy mmd the mmd is a kernel based measure of the distance between two distributions more formally denition let h be a reproducing kernel hilbert space rkhs with associated kernel let f be the set of functions h x r in the unit ball of h where x is a topological space then the mmd between distributions p q is the maximal difference in expectations of functions from f under p q gretton et al q sup hf e xp e yq a small mmd value indicates that p q are similar given nite samples x pn and y qm an empirical estimate of the mmd denoted as y can be computed as y nm y y mmd for summarization the mmd corresponds to the minimal achievable loss of a centroi based kernel classier rumbudur et al consequently we use s to approximate the s in and using a suitable kernel k that measures the similarity of two sentences intuitively this selects summaries s which best represent the distribution of original sentences v note that if we expand s as per and later in the rst term is irrelevant for optimization the second and third term capture the coverage and diversity of the summary tences without any supervision hence this is an unsupervised summarization the supmmd method we start by developing a technique for incorporating sentence importance into mmd for the purpose of generic multi document extractive summarization we then extend this method to comparative summarization and incorporate multiple different kernels to use a diverse sets of features from mmd to weighted mmd that cover unsupervised mmd bista et al selects relevant representative sentences concepts while retaining diversity the notion of representativeness is based on a global model of sentence sentence similarity however this notion of representativeness is not necessarily well matched to the selection of salient information salience of a sentence may be determined by surface features such as position in the article or number of words for example news articles are often written such that sentences at the start of a article have the characteristics of a summary kedzie et al learning a notion of salience that is specic to the summarization task and dataset requires supervised training thus we extend the mmd model by porating supervised sentence importance weighting let v s x be independent samples drawn from the distributions of article sentences p and summary sentences q on the space of all sentences x we dene non negative importance functions q parameterized by learnable parameters we restrict these functions so that epf p v and eqf q s equipped with f we may modify mmd such that the importance of sentences which are good summary candidates is increased denition the weighted mmd q between p q is q sup hf p e p q note that classic mmd is a special case of where f in practice the supremum over all h is impossible to compute directly we thus derive an alternative form for equation lemma for is equivalently p q in the above x f is a canonical feature mapping of sentences and summaries from x to rkhs the derivation which mirrors a similar derivation for mmd gretton et al is given in the appendix importance function we use log linear models as importance functions as they are a common choice of sentence tance kulesza and taskar and easy to t when training data is scarce formally the log linear importance function is where is the surface features of sentence v we can dene the empirical estimates nt s of the importance functions p v as v and q nt v s nt where nt is the number of sentences and mt is the number of summary sentences in topic t training generic summarization the parameters of the log linear importance function must be learned from data so we dene a loss function based on weighted mmd let v t be the t training tuples then the loss of topic t is the square of importance weighted empirical mmd between sentences and summary sentences from within the topic st t st where the weighted trick to equation gives see appendix t st is an empirical estimate of q applying the kernel lt nt nt t v s v nt t s nt mt s mt equation is the loss for a single topic but during training we will instead minimize the average loss over all topics in the training set i e t st intuitively we learn min the parameters by minimizing an importance weighted distance between sentences and ground truth summary sentences over all topics t training comparative summarization we now extend the learning task to comparative marization using the competing binary classiers idea of bista et al specically we replace the accuracy terms in equation with the square of weighted mmd given the t comparative training tuples v t then the objective is to minimize a b v t min b a t t t b st t a st note there are two sets of importance parameters b a one for each of the two document sets multiple kernel learning we employ multiple kernel learning mkl to make use of data from multiple sources in our mmd summarization framework we adapt two stage kernel learning cortes et al where different kernels are linearly combined to maximize the alignment with the target kernel of the classication problem since mmd can be interpreted as classiability sriperumbudur et al mkl ts neatly into our mmd based summarization objective intuitively mkl should identify a good combination of kernels for building a classier that separates summary and non summary sentences untkt let be p kernel functions for topic t let kt i be the kernel matrix according to kernel function ki and kt iunt be the centered kernel matrix with unt i nt let yt be the ground truth summary labels with yt i iff i st the target kernel represents the ideal notion of similarity between sentences the non negative kernel weights w which lead to the optimal alignment with the target kernel are given by cortes et al min where mt rpp has mt i has ai rs and at rp the kernel function must be characteristic for mmd to be a valid metric muandet et al most popular kernels used for bag of words like text features including tf idf the linear kernel y and the cosine kernel y are not characteristic budur et al fortunately the exponential kernel y y is characteristic for any kernel steinwart hence we use the normalized exponential kernel combined with the cosine kernel y inference given a learned importance function f we may nd the best set of summary sentences st for generic summarization via st argmax t st sst similarly for the comparative task with learned importance functions we seek st as to extract relevant text and then perform sentence and word tokenization for duc we clean the text using various regular expressions the details of which are provided in our code release we train punktsentencetokenizer to detect sentence boundaries and use the standard nltk bird word tokenizer for the tac dataset we use the preprocessing pipeline employed by gillick et al this enables a cleaner comparison with the state of the art icsi gillick et al method on the tac dataset for all datasets we keep the sentences between and words per yasunaga et al argmax sst t b st t a st a feature representations both these inference problems are budgeted maximization problems which are often solved by greedy algorithms lin and bilmes the generic unsupervised summarization task is submodular and monotone under certain tions kim et al so greedy algorithms have good theoretical guarantees nemhauser et al while our supervised variants do not have these guarantees we nd that greedy optimization nonetheless leads to good solutions experimental setup we include guidance on applying supmmd and the details required to reproduce our experiments datasets we use four standard multi document tion benchmark datasets and dataset statistics are provided in table each of these datasets has multiple topics where each topic in turn has multiple news articles and four human written summaries in one setting we use as the training set and as test set and in another setting we use as the training set and as the test set both settings are common in the literature the duc datasets can be used for generic summarization while tac being an update summarization task can be used for both generic set a and comparative summarization set b data preprocessing and preparation the duc and tac datasets are provided as collections of xml documents so it is necessary nist gov data html our method requires two different sets of sentence features text features which are used to compute the sentence sentence similarity as part of the kernel and surface features which are used in learning the sentence importance model text features each sentence has three different feature sentations unigrams bigrams and entities the unigrams are stemmed words with stop words from the nltk english list removed the bigrams are a combination of stemmed unigrams and bigrams the entities are dbpedia concepts extracted using dbpedia spotlight mendes et al we use a term frequency inverse sentence frequency tf isf neto et al tation for all text features tf isf has been used extensively in multi document summarization dias et al alguliev et al wan et al surface features we use surface features for the duc dataset and for the tac dataset position there are ve position features four indicators denote the or a later position of the sentence in the article the nal feature gives the position relative to the length of the article counts there are two count features the number of words and number of nouns we use the spacy part of speech tagging to nd nouns tsf this is the sum of the ts isf scores for unigrams composing the sentence for sentence this is ws s where is the inverse sentence frequency of unigram w and s is the term frequency of w in s com benob icsisumm io dataset topics sents avg summ sents avg summ sents oracle ours oracle liu and lapata a b a b table dataset statistics and oracle performance we report the number of topics in each dataset along with the number of sentences after preprocessing we show the rouge scores of our oracle method and the one by liu and lapata with average number of sentence in summary from each method btsf the boosted sum of ts isf scores for unigrams composing the sentence specically we compute ws s where we boost the score of unigrams w that appear in the rst sentence of the article as in the generic summarization for comparative summarization as used by gillick et al unigrams that do not appear in the rst sentence of the article have lexrank the lexrank score erkan and radev computed on the bigrams cosine similarity for the tac datasets we additionally use an indicator whether the sentence begins a paragraph this is provided by the cessing pipeline from icsi gillick et al qsim the fraction of topic description unigrams present in each sentence these topic descriptions are only available for tac oracle extraction both duc and tac provide four human written summaries for each topic since our goal is tive summarization with supervised training we need to know which sentences in the articles could be used to construct the summaries in the training set the article sentences that best match the abstractive summaries are called the oracles st algorithm oracle extraction function v t h t r l st while s argmax sv st st return st l do h t extraction score h t against the human maries h t until a word budget l is reached we only include sentences between to words as gested by yasunaga et al and set a budget of words to ensure our oracle summaries are within words consistent with the evaluation in contrast to liu and lapata which uses only recall score lin our method balances both and recall scores using the harmonic mean and explicitly accounts for sentence length grid search on the validation sets shows that the optimal value for r is across different datasets and summarization tasks as reported in table on average our method produces oracles consisting of more sentences and with higher and scores compared to oracles from liu and lapata this is consistent across all datasets implementation details supervised variants use an regularized log linear model of importance trained using the oracles as ground truth we selected the number of training epochs using fold cross validation we then tune the other hyperparameters on the training set the hyperparameters of the generic tion task are a parameter of the kernel the regularization weight for the log linear importance function and r which denes the length dependent scaling factor in greedy selection lin and bilmes the comparative objective has an additional hyperparameter which controls the comparativeness more implementation details are provided in the appendix we will make implementation publicly evaluation settings our extraction algorithm algorithm is spired by liu and lapata we greedily select sentences s which provide the maximum gain in to evaluate our methods we use the rouge lin metric the choice for evaluating both com computationalmedia supmmd generic summarization hong and nenkova cho et al yasunaga et al kulesza and taskar and update summarization varma et al gillick and favre zhang et al li et al rouge metrics have been shown to correlate with human judgments lin in generic summarization task our recent work bista et al shows that human judgments are consistent with the automatic metrics for evaluating comparative summaries both duc and tac evaluations use the rst words of the generated summary our evaluation setup mirrors hong et al this allows us to compare performance with the state of the art methods they reported and other works also evaluated using this as is standard for the datasets we report and recall scores for datasets both set a and b we adopt the evaluation settings from the so we can compare against the three best performing systems in the as is standard for the dataset we report and rouge recall scores baselines we select the top performing methods from a recent benchmark paper hong et al to serve as baselines and report rouge scores from the benchmark paper they are icsi an integer linear programming method that maximizes coverage gillick et al dpp a determinantal point process method that learns sentence quality and maximizes diversity kulesza and taskar submodular a method based on a learned mixture of submodular functions lin and bilmes a method base on topic ing conroy et al regsum a method that focuses on learning word importance hong and nenkova lexrank a popular graph based sentence scoring method erkan and radev we also include recent deep learning methods evaluated using the same setup as hong et al and report rouge scores from the individual papers dppsim an extension to the dpp model which learns the sentence sentence similarity using a rouge with args a nist summarization args a capsule network cho et al himap a recurrent neural model that employs a modied pointer generator component fabbri et al and a model that uses a graph convolution network combined with a recurrent neural network to learn sentence saliency yasunaga et al as baselines for the dataset we use the top three systems in the competition for each task resulting in four systems altogether to the best of our knowledge these systems are the current state of the art we report the rouge scores from the competition the systems are icsi with two variants sys uses integer linear programming to maximize coverage of concepts gillick et al and sys which additionally uses sentence compression to generate new candidate sentences iit uses a support vector regressor to predict sentence rouge scores varma et al ictcas temporal method zhang et al and icl a manifold ranking based method li et al ltering content a experimental results we compare our methods with the baselines on the a and b datasets we present several variants of our method to analyze the effects of different components and modeling choices we report the performance of unsupervised mmd unsupmmd which does not explicitly consider sentence importance for our supervised method supmmd we report the performance with a bigram kernel supmmd and combined kernels supmmd mkl we also evaluated the impact of our oracle extraction method by replacing it with the extraction method suggested by liu and lapata in supmmd alt oracles meanwhile supmmd mkl compress presents the result of applying sentence compression gillick et al to our model generic summarization the performance of our methods on the generic summarization task are shown in table on the dataset all supmmd variants exceed the state of the art when evaluated with and perform similarly to the best existing methods when evaluated with a icsi gillick et al dpp kulesza and taskar submodular lin and bilmes conroy et al regsum hong and nenkova lexrank erkan and radev dpp sim cho et al himap fabbri et al yasunaga et al unsupmmd supmmd alt oracle supmmd supmmd mkl compress supmmd mkl table results on generic multi document summarization task our best system supmmd mkl outperforms the previous best system icsi on score by while the dpp baseline achieves the highest score on it has a relatively low score which suggests it is optimized for unigram performance at the cost of bigram performance supmmd mkl strikes a better balance scoring the best in and second best in on the generic summarization task in table our supmmd mkl model outperforms the state of the art icsi model on both and rouge specically supmmd mkl scores in while the best icsi variant scores in supervised modeling models using supervised training to identify important sentences tially outperform the unsupervised method supmmd in fact unsupmmd is the lowest ing method across all metrics and datasets this strongly indicates that a degree of supervision is essential to perform well in this task and that the importance function is a suitable way to adapt the unsupmmd model to supervised training over we observe a strong correlation between the the relative position of a sentence and the score given by supmmd this observation is consistent with previous works kedzie et al and strates that supmmd has learned to use the surface features to capture salience further details of ture correlations are provided in the appendix oracle extraction our oracle extraction technique for transforming abstractive training data to extractive training data helps supmmd methods achieve higher rouge performance an alternative technique developed by liu and lapata gillick et al gillick et al varma et al zhang et al unsupmmd supmmd alt oracle supmmd supmmd mkl compress supmmd mkl table results on generic multi document summarization task set a and implemented in supmmd alt oracle gives lower performance than our technique for example on supmmd alt oracle has a of and of while supmmd has a of and a of thus the advantages of our proposed cle extraction method are substantial and consistent across multiple datasets and evaluation metrics multiple kernel learning we observe that combining multiple kernels helps the performance of supmmd models on the generic summarization task supmmd mkl which combines both bigram and entity kernels has a of on while supmmd only uses the bigrams kernel and scores in multiple kernels show even clearer gains in the a dataset sentence compression incorporated into the post processing steps of supmmd mkl compress does not clearly improve the results over supmmd mkl on a compression clearly reduces performance and on supmmd mkl compress has a higher score but a lower score than supmmd mkl incorporating compression into the summarization pipeline is an appealing direction for future work comparative summarization the results for the comparative summarization task on the b dataset are shown in table our supervised mmd variants supmmd and mmd mkl both outperform the state of the art baseline icsi in rouge but fall short in it would be hard to claim that either method is superior in this instance however it does show that supmmd which uses a substantially different approach to that of icsi provides an alternative state of the art thus supmmd further b conclusion icsi sys gillick et al icsi sys gillick et al iiit sys varma et al icl sys li et al unsupmmd supmmd alt oracle supmmd supmmd mkl compress supmmd mkl table results on comparative document summarization task set b maps out the set of techniques that are useful for comparative summarization as per the generic summarization task both our supervised training method and oracle extraction method are essential for achieving good performance in and rouge we also identify sentence position and btsf as important features for sentence salience see the appendix multiple kernels as in supmmd mkl has relatively little effect reducing the score to from the slightly higher achieved by supmmd a similar small decrease is seen for rouge manual inspection shows that the summaries from supmmd and supmmd mkl methods are largely identical with differences primarily on topic which covers political movements in nepal the key entities in this topic are not resolved accurately by dbpedia spotlight contributing additional noise and affecting the mkl approach model variants we have tested an additional variant of our model for comparative summarization which denes two different importance functions one for each of the two document sets a and b see for details in contrast supmmd has a single importance function shared between document sets i e in equation a b performed substantially worse than supmmd in both metrics for example supmmd has a of while has a of we conjecture that a single importance function performs better when training data is relatively scarce because it reduces the number of parameters and simplies the learning problem techniques for tying together the eters for both importance functions such as with a hierarchical bayesian model are left as future work in this work we present supmmd a novel technique for update summarization based on the maximum mean discrepancy supmmd combines supervised learning for salience and unsupervised learning for coverage and diversity further we adapt multiple kernel learning to exploit multiple sources of similarity e text features and knowledge based concepts we show the efcacy of supmmd in both generic and update tion tasks on two standard datasets when compared to the existing approaches we also show that the portance model we introduce on top of our existing unsupervised mmd bista et al improves the summarization performance substantially on both generic and comparative summarization tasks for future work we leave the task of rating embeddings features such as bert devlin et al and evaluating with large generic multi document summarization dataset news fabbri et al acknowledgments this work is supported in part by data to decisions crc and arc discovery project this research is also supported by use of the nectar research cloud a collaborative tralian research platform supported by the national collaborative research infrastructure strategy we thank minjeong shin for helpful feedback and suggestions references rasim m alguliev ramiz m aliguliyev makrufa s hajirahimova and chingiz a mehdiyev mcmr maximum coverage and minimum redundant text summarization model expert systems with applications steven bird nltk the natural language toolkit in proceedings of the coling acl teractive presentation sessions pages sydney australia association for computational linguistics umanga bista alexander patrick mathews minjeong shin aditya krishna menon and lexing xie comparative document summarisation via classication in the thirty third aaai conference on articial intelligence aaai the thirty first innovative applications of articial intelligence conference iaai the ninth aaai symposium on educational advances in articial intelligence eaai honolulu hawaii usa january february pages aaai press ziqiang cao furu wei li dong sujian li and ming zhou ranking with recursive neural networks and its application to multi document summarization in proceedings of the twenty ninth aaai conference on articial intelligence page aaai press natural language processing pages boulder colorado association for computational linguistics daniel gillick benoit favre dilek hakkani tr bernd bohnet yang liu and shasha xie the icsi utd summarization system at tac in tac sangwoo cho logan lebanoff hassan foroosh and fei liu improving the similarity measure of determinantal point processes for extractive multi document summarization in proceedings of the annual meeting of the association for putational linguistics pages florence italy association for computational linguistics jade goldstein mark kantrowitz vibhu mittal and jaime carbonell summarizing text documents sentence selection and evaluation metrics in ings of the annual international acm sigir conference on research and development in tion retrieval sigir page new york ny usa association for computing machinery john conroy sashka t davis jeff kubina yi kai liu dianne p oleary and judith d schlesinger multilingual summarization dimensionality reduction and a step towards optimal term coverage in proceedings of the multiling workshop on multilingual multi document summarization pages soa bulgaria association for computational linguistics j b conway a course in functional analysis second edition volume of graduate texts in mathematics springer verlag new york corinna cortes mehryar mohri and afshin tamizadeh two stage learning kernel rithms in proceedings of the annual tional conference on machine learning icml jacob devlin ming wei chang kenton lee and bert pre training kristina toutanova transformers for language of deep bidirectional in proceedings of the understanding ference of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota association for computational linguistics the north american chapter of gal dias elsa alves and jos gabriel pereira lopes topic segmentation algorithms for text summarization and passage retrieval an exhaustive in proceedings of the national evaluation conference on articial intelligence volume page aaai press gnes erkan and dragomir r radev lexrank graph based lexical centrality as salience in text summarization j artif int res alexander fabbri irene li tianwei she suyi li and dragomir radev multi news a large scale multi document summarization dataset and tive hierarchical model in proceedings of the annual meeting of the association for computational linguistics pages florence italy association for computational linguistics dan gillick and benoit favre a scalable global model for summarization in proceedings of the workshop on integer linear programming for arthur gretton karsten m borgwardt malte j rasch bernhard schlkopf and alexander smola a kernel two sample test aria haghighi and lucy vanderwende exploring content models for multi document summarization in proceedings of human language technologies the annual conference of the north american chapter of the association for computational linguistics pages boulder colorado association for computational linguistics kai hong john conroy benoit favre alex kulesza hui lin and ani nenkova a repository of state of the art and competitive baseline summaries in proceedings for generic news summarization of the ninth international conference on language resources and evaluation pages reykjavik iceland european languages resources association elra kai hong and ani nenkova improving the estimation of word importance for news in proceedings of the document summarization conference of the european chapter of the association for computational linguistics pages gothenburg sweden association for computational linguistics chris kedzie kathleen mckeown and hal daum iii content selection in deep learning models in proceedings of the of summarization conference on empirical methods in natural language processing pages brussels belgium association for computational linguistics been kim rajiv khanna and oluwasanmi koyejo examples are not enough learn to criticize criticism for interpretability in proceedings of the international conference on neural information processing systems page red hook ny usa curran associates inc alex kulesza and ben taskar determinantal now point processes for machine learning publishers inc hanover ma usa sujian li wei wang and yongwei zhang tac update summarization of icl in tac yujia li kevin swersky and richard zemel in generative moment matching networks ceedings of the international conference on international conference on machine learning volume page jmlr org chin yew lin rouge a package for automatic in text summarization evaluation of summaries branches out pages barcelona spain association for computational linguistics hui lin and jeff bilmes multi document marization via budgeted maximization of submodular functions in human language technologies the annual conference of the north american the association for computational chapter of linguistics pages los angeles california association for computational linguistics hui lin and jeff bilmes a class of submodular functions for document summarization in ings of the annual meeting of the association for computational linguistics human language technologies pages portland oregon usa association for computational linguistics hui lin and jeff bilmes learning mixtures of submodular shells with application to document summarization page arlington virginia usa auai press dong c liu and jorge nocedal on the limited memory bfgs method for large scale optimization mathematical programming yang liu and mirella lapata text summarization in proceedings of the with pretrained encoders conference on empirical methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china association for computational linguistics pablo n mendes max jakob andrs garca silva and christian bizer dbpedia spotlight shedding light on the web of documents in proceedings of the international conference on semantic systems i semantics page new york ny usa association for computing machinery krikamol muandet kenji fukumizu bharath budur bernhard schlkopf al kernel mean embedding of distributions a review and beyond foundations and trends in machine learning ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based sequence model for extractive summarization of documents in proceedings of the thirty first aaai conference on articial intelligence page aaai press george l nemhauser laurence a wolsey and marshall l fisher an analysis of tions for maximizing submodular set functions i mathematical programming ani nenkova and lucy vanderwende the impact of frequency on summarization microsoft research redmond washington tech rep msr joel larocca neto alexandre d santos celso aa kaestner neto alexandre d santos al document clustering and text summarization ganapati p patil and calyampudi r rao weighted distributions and size biased sampling with applications to wildlife populations and human families biometrics pages dragomir r radev hongyan jing magorzata stys and daniel tam centroid based summarization of multiple documents information processing management bharath k sriperumbudur kenji fukumizu arthur gretton gert r g lanckriet and bernhard schlkopf kernel choice and classiability for rkhs embeddings of probability distributions in proceedings of the international conference on neural information processing systems page red hook ny usa curran associates inc bharath k sriperumbudur arthur gretton kenji fukumizu bernhard schlkopf and gert rg lanckriet hilbert space embeddings and metrics on probability measures journal of machine learning research ingo steinwart on the inuence of the kernel on the consistency of support vector machines journal of machine learning research vasudeva varma prasad pingali rahul katragadda sai krishna surya ganesh kiran sarvabhotla harish garapati hareen gopisetty vijay bharath reddy kranthi reddy al iiit hyderabad at tac in tac xiaojun wan jianwu yang and jianguo xiao towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction in proceedings of the annual meeting of the association of computational linguistics pages prague czech republic association for computational linguistics michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan and dragomir radev graph based neural multi document marization in proceedings of the conference on computational natural language learning conll pages vancouver canada association for computational linguistics manzil zaheer sashank reddi devendra sachan satyen kale and sanjiv kumar adaptive methods for nonconvex optimization in s bengio h wallach h larochelle k grauman n bianchi and r garnett editors advances in neural information processing systems pages curran associates inc jin zhang pan du hongbo xu and xueqi cheng ictgrasper at temporal preferred update summarization in proceedings of the second text analysis conference tac gaithersburg maryland usa november a background theory on kernels and mmd in this section we provide a brief overview of kernels and maximum mean discrepancy mmd for a detailed overview we refer readers to to muandet et al and gretton et al from which this brief overview is taken a positive denite kernels and kernel trick denition a a function k x x r is called positive denite kernel if it is symmetric i e x yx and gram trix is positive denite i e nn cnr i xj theorem a if a kernel is positive denite there exists a feature map x h such that x yx this is known as the kernel trick in machine learning the feature space h is called a ing kernel hilbert space rkhs and the kernel k is also known as reproducing kernel a reproducing kernel hilbert space denition a an rkhs is a hilbert space of functions where all function evaluations are bounded i e xx hh in an rkhs function evaluation where x h are canonical feature map associated with rkhs h and a rkhs is fully characterized by its reproducing kernel k or a positive denite nel k uniquely determines a rkhs and vice versa hence which is known as the riesz representer theorem conway a more on mmd recall that f is a class of rkhs functions within the unit ball i e h h suppose h admits a feature map x h then per gretton et al we may solve the supremum in equation as q a hence mmd is computed as the distance tween the mean feature embeddings under each distribution for a suitable kernel based feature space gretton et al eq a involves explicitly evaluating the arbitrarily high dimensional features instead the kernel trick allows efcient computation of q by evaluating just pairwise kernels supposing h has induced kernel k we have q e e y e xp yq y a a characteristic kernel for a distribution p and kernel with feature map x h the kernel mean map is p a kernel k is characteristic if the map p is injective a characteristic kernel ensures mmd is if and only if p q i e no information is lost in mapping the distribution into the rkhs muandet et al examples of characteristic kernels for rd y include the gaussian kernel and laplace kernel y mmd with the gaussian kernel is equivalent to comparing all moments between two distributions li et al b proof of lemma the weighted mmd q where f contains functions h x r within unit ball rkhs h is dened as sup hf e vp p e sq q recall f is a non negative importance weighting function then according to patil and rao the weighted probability density p of p is p v v and q and similarly q for q p the weighted mmd is and q since we restrict s we have thus sup hf e vp e sq sup e vp e sq since in an rkhs this simplies to h e vp h e sq q e sq p e sq sup e vp e vp where the penultimate step follows from the dual norm the proof is similar to mmd in gretton et al c empirical estimate of q first q can be expanded as e vp e h p sq q e p p p q q v e vp sq q e s applying the kernel trick a e p p p q v e vp sq q e s q our loss of generic summarization t st is t st recalling nt and mt lt nt nt t v s v nt t s nt mt s mt mt d training details we train generic summarization model with full batch lbfgs liu and nocedal with learning rate we train comparative rization model using yogi optimizer zaheer et al wikipedia org wiki with a mini batch size of topics learning rate and decreasing the learning rate by half every epochs we choose the number of training epochs by validating across folds with early stopping we set the patience to epochs for early stopping with lbfgs optimizer and epochs with yogi optimizer we tune the other hyperparameters on the training set and the optimal hyperparameters of best model supmmd mkl and searched space are shown in table the kernel combination weights w are also shown in table the kernel combination weights w are written in order unigrams bigrams and entities hyp a b r epoch w table optimal hyperparameters their search space and mkl combination weights on each dataset e additional results in this section we provide some additional results e correlation with rouge score dataset supmmd lexrank supmmd lexrank table correlation of sentence importance scores with normalized sentence rouge scores we analyze the correlation between normalized rouge recall scores of the sentences and sentence scores from supmmd and lexrank the ized rouge score of each sentence is dened as as shown in table we nd that supmmd has a slightly high correlation with sentence rouge scores this suggests that supmmd is better in capturing sentence importance for summarization e feature correlations we analyze the correlation between various surface features and sentence importance scores from supmmd and lexrank erkan and radev as a b feature supmmd lexrank supmmd lexrank supmmd lexrank position tsf btsf words nouns table correlation of some features with sentence scores from supmmd and lexrank eigenvector centrality method set a set b icsi a fourth day of thrashing thunderstorms began to take a heavier toll on southern california with at least three deaths blamed on the rain as ooding and mudslides forced road closures and emergency crews carried out harrowing rescue operations downtown los angeles has had more than inches of rain since jan more than its average rainfall for an entire year including inches a record meteorologists say southern fornia has not been hit by this much rain in nearly years the disaster was the latest caused by rain and snow that has battered california since dec supmmd downtown los angeles has had more than inches of rain since jan more than its average rainfall for an entire year including inches a record a fourth day of thrashing thunderstorms began to take a heavier toll on southern california with at least three deaths blamed on the rain as ooding and mudslides forced road closures and emergency crews carried out harrowing rescue operations the roads in los angeles county were equally frustrating part of a rain saturated hillside gave way sending a mississippi like torrent of earth and trees onto four blocks of this oceanfront town and killing two men californians braced for even more rain as they gled to recover from storms that have left at least nine people dead triggered mudslides and tornadoes and washed away roads and runways the record inches centimeters was set in mudslides forced amtrak to suspend train service tween los angeles and santa barbara through at least thursday a winter storm pummeled southern fornia for the third straight day claiming the lives of three people and raising fears of mudslides even as homes around the region were evacuated staff writers rick orlov and lisa mascaro contributed to this story storms have caused million million in damage to los angeles county roads and facilities since the beginning of the year multi million dollar homes collapsed and mudslides trapped residents in their homes as a heavy rains that have claimed three lives pelted los angeles for the fth straight day in scenes reminiscent of the aftermath of the northridge earthquake years ago this month los angeles area residents faced gridlocked freeways and roads day while cleanup crews cleared mud rubble and bris left from a two week siege of rain a shattering storm slammed southern california for a sixth straight day tuesday triggering mudslides and tornadoes and forcing more road closures but ers predicted it would wane wednesday before a new storm moves in sunday night table example summaries of topic containing articles about rains and mudslides in southern california we highlight few phrases in bold which could help us to identify the difference between set a and b summaries from icsi and supmmd methods suggest that set a contains articles describing events from earlier days of the disaster and set b contains articles from later stage of the disaster shown in table supmmd has higher correlation with relative position signifying the importance of position of sentence in summary sentences lexrank has a higher correlations with the number of words number of nouns and tfisf scores of the sentences which is expected as lexrank is an eigenvector centrality of sentence sentence similarity matrix this suggest supmmd is able to learn that rst few sentences are important in news summarization similar result is reported by kedzie et al where they show that the rst few sentences are important in creating summary of news articles e example summary we present the update summaries set a and b of topic which contains articles about rains and mudslides in southern california in table
