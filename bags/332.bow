supmmd sentence importance model extractive summarization maximum mean discrepancy umanga alexander patrick aditya krishna menon lexing national university canberra act australia google research new york ny united states bista alex mathews lexing edu au com t c o l c s c v v x r abstract work multi document summarization focused generic summarization mation present individual document set explored setting update summarization goal identify new information present set equal practical interest e presenting readers updates evolving news topic work present supmmd novel technique generic update summarization based maximum mean discrepancy kernel sample testing supmmd combines supervised learning salience vised learning coverage diversity ther adapt multiple kernel learning use similarity multiple information sources e text features knowledge based concepts efcacy supmmd generic update summarization tasks meeting exceeding current state art datasets introduction multi document summarization problem producing condensed digests salient information multiple sources articles concretely suppose given sets articles denoted set set b related topic e climate change pandemic separated publication timestamp geographic region identify possible instantiations multi document summarization figure generic summarization goal summarize set b individually comparative summarization goal summarize set b set highlighting differences iii update summarization goal generic summarization set comparative summarization set b versus existing work topic focused generic summarization task figure different summarization tasks generic comparative c update sets articles set b denoted red blue circles respectively summary prototypes bold circles information coverage tasks lled respective colors update summarization equal practical interest intuitively comparative aspect setting aims inform user new information topic familiar extractive multi document summarization methods unsupervised supervised pervised methods typically dene salience erage global model sentence sentence similarity methods based retrieval goldstein et al centroids radev et al graph centrality erkan radev utility maximization lin bilmes gillick favre explored sentence salience depends surface features e position length presence cue words effectively capturing requires supervised models specic dataset task body work incorporated information supervised learning example based point processes kulesza taskar learning important words hong nenkova graph neural networks yasunaga et al support vector regression varma et al supervised methods separate model learning inference leading disconnect learning sentence salience sentence selection varma et al yasunaga et al hong nenkova designed specically generic summarization kulesza taskar work propose supmmd single model learning salience inference applied generic comparative summarization following contributions present supmmd novel technique generic update summarization combines supervised learning salience unsupervised learning coverage diversity supmmd single model learning inference adapt multiple kernel learning cortes et al model allows similarity multiple information sources e text features knowledge based concepts supmmd meets exceeds state art generic update tion datasets literature review multi document summarization extractive salient pieces original text sentences selected form summary abstractive new text generated paraphrasing important information popular creates semantically matically correct summaries nallapati et al work focus generic update document summarization extractive setting extractive summarizers nents sentence scoring selection variety unsupervised supervised methods developed unsupervised sentence scorers based centroids radev et al graph centrality erkan radev retrieval relevance goldstein et al word statistics nenkova vanderwende topic models haghighi vanderwende concept coverage gillick favre lin bilmes supervised techniques include graph based neural network yasunaga et al learning sentence quality point processes kulesza taskar combining word importances hong nenkova combining sentence phrase importances cao et al employing mixture submodular functions lin bilmes sentence selection methods broadly egorized greedy methods goldstein et al radev et al erkan radev nenkova vanderwende cao et al haghighi vanderwende hong nenkova kulesza taskar cao et al varma et al produce approximate solutions iteratively selecting sentences maximal score exact integer linear programming ilp based methods gillick favre cao et al greedy methods use objective belongs special class set functions called submodular functions lin bilmes kulesza taskar good approximation guarantees greedy optimization nemhauser et al limited research update comparative summarization notable prior work includes maximizing concept coverage ilp gillick et al learning sentence scores support vector regressor varma et al temporal content ltering zhang et al bista et al cast comparative summarization problem classication use mmd gretton et al work adapt method learn sentence importances driven surface features summarization classication review perspective introduced bista et al summarization viewed classication provide brief introduction maximum mean discrepancy mmd ideas form basis subsequent method generic summarization classication let v t topics articles wish summarize topic t wish select summary sentences st bista et al lated summarization selecting prototypes minimize accuracy powerful classier sentences input summary intuition powerful classier able distinguish sentences articles summary sentences formally pick st argmax t s sst st l comprise subsets v t upto l words y accuracy best possible classier distinguishes elements sets x y shall shortly realize mmd comparative summarization competing binary classication comparative summarization sets b bista et al introduced additional term giving rise competing goals classier able distinguish summaries sentences set b able distinguish maries sentences set formally let v t b set sentences set b v t sentences set compare set suitable seek st summary sentences set b st argmax t b t sst hyperparameter controls relative tance accurately representing articles set b versus representing articles set maximum mean discrepancy mmd mmd kernel based measure distance distributions formally denition let h reproducing kernel hilbert space rkhs associated kernel let f set functions h x r unit ball h x topological space mmd distributions p q maximal difference expectations functions f p q gretton et al q sup hf e xp e yq small mmd value indicates p q similar given nite samples x pn y qm empirical estimate mmd denoted y computed y nm y y mmd summarization mmd corresponds minimal achievable loss centroi based kernel classier rumbudur et al consequently use s approximate s suitable kernel k measures similarity sentences intuitively selects summaries s best represent distribution original sentences v note expand s later rst term irrelevant optimization second term capture coverage diversity summary tences supervision unsupervised summarization supmmd method start developing technique incorporating sentence importance mmd purpose generic multi document extractive summarization extend method comparative summarization incorporate multiple different kernels use diverse sets features mmd weighted mmd cover unsupervised mmd bista et al selects relevant representative sentences concepts retaining diversity notion representativeness based global model sentence sentence similarity notion representativeness necessarily matched selection salient information salience sentence determined surface features position article number words example news articles written sentences start article characteristics summary kedzie et al learning notion salience specic summarization task dataset requires supervised training extend mmd model porating supervised sentence importance weighting let v s x independent samples drawn distributions article sentences p summary sentences q space sentences x dene non negative importance functions q parameterized learnable parameters restrict functions epf p v eqf q s equipped f modify mmd importance sentences good summary candidates increased denition weighted mmd q p q q sup hf p e p q note classic mmd special case f practice supremum h impossible compute directly derive alternative form equation lemma equivalently p q x f canonical feature mapping sentences summaries x rkhs derivation mirrors similar derivation mmd gretton et al given appendix importance function use log linear models importance functions common choice sentence tance kulesza taskar easy t training data scarce formally log linear importance function surface features sentence v dene empirical estimates nt s importance functions p v v q nt v s nt nt number sentences mt number summary sentences topic t training generic summarization parameters log linear importance function learned data dene loss function based weighted mmd let v t t training tuples loss topic t square importance weighted empirical mmd sentences summary sentences topic st t st weighted trick equation gives appendix t st empirical estimate q applying kernel lt nt nt t v s v nt t s nt mt s mt equation loss single topic training instead minimize average loss topics training set e t st intuitively learn min parameters minimizing importance weighted distance sentences ground truth summary sentences topics t training comparative summarization extend learning task comparative marization competing binary classiers idea bista et al specically replace accuracy terms equation square weighted mmd given t comparative training tuples v t objective minimize b v t min b t t t b st t st note sets importance parameters b document sets multiple kernel learning employ multiple kernel learning mkl use data multiple sources mmd summarization framework adapt stage kernel learning cortes et al different kernels linearly combined maximize alignment target kernel classication problem mmd interpreted classiability sriperumbudur et al mkl ts neatly mmd based summarization objective intuitively mkl identify good combination kernels building classier separates summary non summary sentences untkt let p kernel functions topic t let kt kernel matrix according kernel function ki kt iunt centered kernel matrix unt nt let yt ground truth summary labels yt iff st target kernel represents ideal notion similarity sentences non negative kernel weights w lead optimal alignment target kernel given cortes et al min mt rpp mt ai rs rp kernel function characteristic mmd valid metric muandet et al popular kernels bag words like text features including tf idf linear kernel y cosine kernel y characteristic budur et al fortunately exponential kernel y y characteristic kernel steinwart use normalized exponential kernel combined cosine kernel y inference given learned importance function f nd best set summary sentences st generic summarization st argmax t st sst similarly comparative task learned importance functions seek st extract relevant text perform sentence word tokenization duc clean text regular expressions details provided code release train punktsentencetokenizer detect sentence boundaries use standard nltk bird word tokenizer tac dataset use preprocessing pipeline employed gillick et al enables cleaner comparison state art icsi gillick et al method tac dataset datasets sentences words yasunaga et al argmax sst t b st t st feature representations inference problems budgeted maximization problems solved greedy algorithms lin bilmes generic unsupervised summarization task submodular monotone certain tions kim et al greedy algorithms good theoretical guarantees nemhauser et al supervised variants guarantees nd greedy optimization nonetheless leads good solutions experimental setup include guidance applying supmmd details required reproduce experiments datasets use standard multi document tion benchmark datasets dataset statistics provided table datasets multiple topics topic turn multiple news articles human written summaries setting use training set test set setting use training set test set settings common literature duc datasets generic summarization tac update summarization task generic set comparative summarization set b data preprocessing preparation duc tac datasets provided collections xml documents necessary nist gov data html method requires different sets sentence features text features compute sentence sentence similarity kernel surface features learning sentence importance model text features sentence different feature sentations unigrams bigrams entities unigrams stemmed words stop words nltk english list removed bigrams combination stemmed unigrams bigrams entities dbpedia concepts extracted dbpedia spotlight mendes et al use term frequency inverse sentence frequency tf isf neto et al tation text features tf isf extensively multi document summarization dias et al alguliev et al wan et al surface features use surface features duc dataset tac dataset position ve position features indicators denote later position sentence article nal feature gives position relative length article counts count features number words number nouns use spacy speech tagging nd nouns tsf sum ts isf scores unigrams composing sentence sentence ws s inverse sentence frequency unigram w s term frequency w s com benob icsisumm io dataset topics sents avg summ sents avg summ sents oracle oracle liu lapata b b table dataset statistics oracle performance report number topics dataset number sentences preprocessing rouge scores oracle method liu lapata average number sentence summary method btsf boosted sum ts isf scores unigrams composing sentence specically compute ws s boost score unigrams w appear rst sentence article generic summarization comparative summarization gillick et al unigrams appear rst sentence article lexrank lexrank score erkan radev computed bigrams cosine similarity tac datasets additionally use indicator sentence begins paragraph provided cessing pipeline icsi gillick et al qsim fraction topic description unigrams present sentence topic descriptions available tac oracle extraction duc tac provide human written summaries topic goal tive summarization supervised training need know sentences articles construct summaries training set article sentences best match abstractive summaries called oracles st algorithm oracle extraction function v t h t r l st s argmax sv st st return st l h t extraction score h t human maries h t word budget l reached include sentences words gested yasunaga et al set budget words ensure oracle summaries words consistent evaluation contrast liu lapata uses recall score lin method balances recall scores harmonic mean explicitly accounts sentence length grid search validation sets shows optimal value r different datasets summarization tasks reported table average method produces oracles consisting sentences higher scores compared oracles liu lapata consistent datasets implementation details supervised variants use regularized log linear model importance trained oracles ground truth selected number training epochs fold cross validation tune hyperparameters training set hyperparameters generic tion task parameter kernel regularization weight log linear importance function r denes length dependent scaling factor greedy selection lin bilmes comparative objective additional hyperparameter controls comparativeness implementation details provided appendix implementation publicly evaluation settings extraction algorithm algorithm spired liu lapata greedily select sentences s provide maximum gain evaluate methods use rouge lin metric choice evaluating com computationalmedia supmmd generic summarization hong nenkova cho et al yasunaga et al kulesza taskar update summarization varma et al gillick favre zhang et al li et al rouge metrics shown correlate human judgments lin generic summarization task recent work bista et al shows human judgments consistent automatic metrics evaluating comparative summaries duc tac evaluations use rst words generated summary evaluation setup mirrors hong et al allows compare performance state art methods reported works evaluated standard datasets report recall scores datasets set b adopt evaluation settings compare best performing systems standard dataset report rouge recall scores baselines select performing methods recent benchmark paper hong et al serve baselines report rouge scores benchmark paper icsi integer linear programming method maximizes coverage gillick et al dpp determinantal point process method learns sentence quality maximizes diversity kulesza taskar submodular method based learned mixture submodular functions lin bilmes method base topic ing conroy et al regsum method focuses learning word importance hong nenkova lexrank popular graph based sentence scoring method erkan radev include recent deep learning methods evaluated setup hong et al report rouge scores individual papers dppsim extension dpp model learns sentence sentence similarity rouge args nist summarization args capsule network cho et al himap recurrent neural model employs modied pointer generator component fabbri et al model uses graph convolution network combined recurrent neural network learn sentence saliency yasunaga et al baselines dataset use systems competition task resulting systems altogether best knowledge systems current state art report rouge scores competition systems icsi variants sys uses integer linear programming maximize coverage concepts gillick et al sys additionally uses sentence compression generate new candidate sentences iit uses support vector regressor predict sentence rouge scores varma et al ictcas temporal method zhang et al icl manifold ranking based method li et al ltering content experimental results compare methods baselines b datasets present variants method analyze effects different components modeling choices report performance unsupervised mmd unsupmmd explicitly consider sentence importance supervised method supmmd report performance bigram kernel supmmd combined kernels supmmd mkl evaluated impact oracle extraction method replacing extraction method suggested liu lapata supmmd alt oracles supmmd mkl compress presents result applying sentence compression gillick et al model generic summarization performance methods generic summarization task shown table dataset supmmd variants exceed state art evaluated perform similarly best existing methods evaluated icsi gillick et al dpp kulesza taskar submodular lin bilmes conroy et al regsum hong nenkova lexrank erkan radev dpp sim cho et al himap fabbri et al yasunaga et al unsupmmd supmmd alt oracle supmmd supmmd mkl compress supmmd mkl table results generic multi document summarization task best system supmmd mkl outperforms previous best system icsi score dpp baseline achieves highest score relatively low score suggests optimized unigram performance cost bigram performance supmmd mkl strikes better balance scoring best second best generic summarization task table supmmd mkl model outperforms state art icsi model rouge specically supmmd mkl scores best icsi variant scores supervised modeling models supervised training identify important sentences tially outperform unsupervised method supmmd fact unsupmmd lowest ing method metrics datasets strongly indicates degree supervision essential perform task importance function suitable way adapt unsupmmd model supervised training observe strong correlation relative position sentence score given supmmd observation consistent previous works kedzie et al strates supmmd learned use surface features capture salience details ture correlations provided appendix oracle extraction oracle extraction technique transforming abstractive training data extractive training data helps supmmd methods achieve higher rouge performance alternative technique developed liu lapata gillick et al gillick et al varma et al zhang et al unsupmmd supmmd alt oracle supmmd supmmd mkl compress supmmd mkl table results generic multi document summarization task set implemented supmmd alt oracle gives lower performance technique example supmmd alt oracle supmmd advantages proposed cle extraction method substantial consistent multiple datasets evaluation metrics multiple kernel learning observe combining multiple kernels helps performance supmmd models generic summarization task supmmd mkl combines bigram entity kernels supmmd uses bigrams kernel scores multiple kernels clearer gains dataset sentence compression incorporated post processing steps supmmd mkl compress clearly improve results supmmd mkl compression clearly reduces performance supmmd mkl compress higher score lower score supmmd mkl incorporating compression summarization pipeline appealing direction future work comparative summarization results comparative summarization task b dataset shown table supervised mmd variants supmmd mmd mkl outperform state art baseline icsi rouge fall short hard claim method superior instance supmmd uses substantially different approach icsi provides alternative state art supmmd b conclusion icsi sys gillick et al icsi sys gillick et al iiit sys varma et al icl sys li et al unsupmmd supmmd alt oracle supmmd supmmd mkl compress supmmd mkl table results comparative document summarization task set b maps set techniques useful comparative summarization generic summarization task supervised training method oracle extraction method essential achieving good performance rouge identify sentence position btsf important features sentence salience appendix multiple kernels supmmd mkl relatively little effect reducing score slightly higher achieved supmmd similar small decrease seen rouge manual inspection shows summaries supmmd supmmd mkl methods largely identical differences primarily topic covers political movements nepal key entities topic resolved accurately dbpedia spotlight contributing additional noise affecting mkl approach model variants tested additional variant model comparative summarization denes different importance functions document sets b details contrast supmmd single importance function shared document sets e equation b performed substantially worse supmmd metrics example supmmd conjecture single importance function performs better training data relatively scarce reduces number parameters simplies learning problem techniques tying eters importance functions hierarchical bayesian model left future work work present supmmd novel technique update summarization based maximum mean discrepancy supmmd combines supervised learning salience unsupervised learning coverage diversity adapt multiple kernel learning exploit multiple sources similarity e text features knowledge based concepts efcacy supmmd generic update tion tasks standard datasets compared existing approaches portance model introduce existing unsupervised mmd bista et al improves summarization performance substantially generic comparative summarization tasks future work leave task rating embeddings features bert devlin et al evaluating large generic multi document summarization dataset news fabbri et al acknowledgments work supported data decisions crc arc discovery project research supported use nectar research cloud collaborative tralian research platform supported national collaborative research infrastructure strategy thank minjeong shin helpful feedback suggestions references rasim m alguliev ramiz m aliguliyev makrufa s hajirahimova chingiz mehdiyev mcmr maximum coverage minimum redundant text summarization model expert systems applications steven bird nltk natural language toolkit proceedings coling acl teractive presentation sessions pages sydney australia association computational linguistics umanga bista alexander patrick mathews minjeong shin aditya krishna menon lexing xie comparative document summarisation classication thirty aaai conference articial intelligence aaai thirty innovative applications articial intelligence conference iaai ninth aaai symposium educational advances articial intelligence eaai honolulu hawaii usa january february pages aaai press ziqiang cao furu wei li dong sujian li ming zhou ranking recursive neural networks application multi document summarization proceedings ninth aaai conference articial intelligence page aaai press natural language processing pages boulder colorado association computational linguistics daniel gillick benoit favre dilek hakkani tr bernd bohnet yang liu shasha xie icsi utd summarization system tac tac sangwoo cho logan lebanoff hassan foroosh fei liu improving similarity measure determinantal point processes extractive multi document summarization proceedings annual meeting association putational linguistics pages florence italy association computational linguistics jade goldstein mark kantrowitz vibhu mittal jaime carbonell summarizing text documents sentence selection evaluation metrics ings annual international acm sigir conference research development tion retrieval sigir page new york ny usa association computing machinery john conroy sashka t davis jeff kubina yi kai liu dianne p oleary judith d schlesinger multilingual summarization dimensionality reduction step optimal term coverage proceedings multiling workshop multilingual multi document summarization pages soa bulgaria association computational linguistics j b conway course functional analysis second edition volume graduate texts mathematics springer verlag new york corinna cortes mehryar mohri afshin tamizadeh stage learning kernel rithms proceedings annual tional conference machine learning icml jacob devlin ming wei chang kenton lee bert pre training kristina toutanova transformers language deep bidirectional proceedings understanding ference association computational linguistics human language technologies volume long short papers pages minneapolis minnesota association computational linguistics north american chapter gal dias elsa alves jos gabriel pereira lopes topic segmentation algorithms text summarization passage retrieval exhaustive proceedings national evaluation conference articial intelligence volume page aaai press gnes erkan dragomir r radev lexrank graph based lexical centrality salience text summarization j artif int res alexander fabbri irene li tianwei suyi li dragomir radev multi news large scale multi document summarization dataset tive hierarchical model proceedings annual meeting association computational linguistics pages florence italy association computational linguistics dan gillick benoit favre scalable global model summarization proceedings workshop integer linear programming arthur gretton karsten m borgwardt malte j rasch bernhard schlkopf alexander smola kernel sample test aria haghighi lucy vanderwende exploring content models multi document summarization proceedings human language technologies annual conference north american chapter association computational linguistics pages boulder colorado association computational linguistics kai hong john conroy benoit favre alex kulesza hui lin ani nenkova repository state art competitive baseline summaries proceedings generic news summarization ninth international conference language resources evaluation pages reykjavik iceland european languages resources association elra kai hong ani nenkova improving estimation word importance news proceedings document summarization conference european chapter association computational linguistics pages gothenburg sweden association computational linguistics chris kedzie kathleen mckeown hal daum iii content selection deep learning models proceedings summarization conference empirical methods natural language processing pages brussels belgium association computational linguistics kim rajiv khanna oluwasanmi koyejo examples learn criticize criticism interpretability proceedings international conference neural information processing systems page red hook ny usa curran associates inc alex kulesza ben taskar determinantal point processes machine learning publishers inc hanover ma usa sujian li wei wang yongwei zhang tac update summarization icl tac yujia li kevin swersky richard zemel generative moment matching networks ceedings international conference international conference machine learning volume page jmlr org chin yew lin rouge package automatic text summarization evaluation summaries branches pages barcelona spain association computational linguistics hui lin jeff bilmes multi document marization budgeted maximization submodular functions human language technologies annual conference north american association computational chapter linguistics pages los angeles california association computational linguistics hui lin jeff bilmes class submodular functions document summarization ings annual meeting association computational linguistics human language technologies pages portland oregon usa association computational linguistics hui lin jeff bilmes learning mixtures submodular shells application document summarization page arlington virginia usa auai press dong c liu jorge nocedal limited memory bfgs method large scale optimization mathematical programming yang liu mirella lapata text summarization proceedings pretrained encoders conference empirical methods natural language processing international joint conference natural language processing emnlp ijcnlp pages hong kong china association computational linguistics pablo n mendes max jakob andrs garca silva christian bizer dbpedia spotlight shedding light web documents proceedings international conference semantic systems semantics page new york ny usa association computing machinery krikamol muandet kenji fukumizu bharath budur bernhard schlkopf al kernel mean embedding distributions review foundations trends machine learning ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents proceedings thirty aaai conference articial intelligence page aaai press george l nemhauser laurence wolsey marshall l fisher analysis tions maximizing submodular set functions mathematical programming ani nenkova lucy vanderwende impact frequency summarization microsoft research redmond washington tech rep msr joel larocca neto alexandre d santos celso aa kaestner neto alexandre d santos al document clustering text summarization ganapati p patil calyampudi r rao weighted distributions size biased sampling applications wildlife populations human families biometrics pages dragomir r radev hongyan jing magorzata stys daniel tam centroid based summarization multiple documents information processing management bharath k sriperumbudur kenji fukumizu arthur gretton gert r g lanckriet bernhard schlkopf kernel choice classiability rkhs embeddings probability distributions proceedings international conference neural information processing systems page red hook ny usa curran associates inc bharath k sriperumbudur arthur gretton kenji fukumizu bernhard schlkopf gert rg lanckriet hilbert space embeddings metrics probability measures journal machine learning research ingo steinwart inuence kernel consistency support vector machines journal machine learning research vasudeva varma prasad pingali rahul katragadda sai krishna surya ganesh kiran sarvabhotla harish garapati hareen gopisetty vijay bharath reddy kranthi reddy al iiit hyderabad tac tac xiaojun wan jianwu yang jianguo xiao iterative reinforcement approach simultaneous document summarization keyword extraction proceedings annual meeting association computational linguistics pages prague czech republic association computational linguistics michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan dragomir radev graph based neural multi document marization proceedings conference computational natural language learning conll pages vancouver canada association computational linguistics manzil zaheer sashank reddi devendra sachan satyen kale sanjiv kumar adaptive methods nonconvex optimization s bengio h wallach h larochelle k grauman n bianchi r garnett editors advances neural information processing systems pages curran associates inc jin zhang pan du hongbo xu xueqi cheng ictgrasper temporal preferred update summarization proceedings second text analysis conference tac gaithersburg maryland usa november background theory kernels mmd section provide brief overview kernels maximum mean discrepancy mmd detailed overview refer readers muandet et al gretton et al brief overview taken positive denite kernels kernel trick denition function k x x r called positive denite kernel symmetric e x yx gram trix positive denite e nn cnr xj theorem kernel positive denite exists feature map x h x yx known kernel trick machine learning feature space h called ing kernel hilbert space rkhs kernel k known reproducing kernel reproducing kernel hilbert space denition rkhs hilbert space functions function evaluations bounded e xx hh rkhs function evaluation x h canonical feature map associated rkhs h rkhs fully characterized reproducing kernel k positive denite nel k uniquely determines rkhs vice versa known riesz representer theorem conway mmd recall f class rkhs functions unit ball e h h suppose h admits feature map x h gretton et al solve supremum equation q mmd computed distance tween mean feature embeddings distribution suitable kernel based feature space gretton et al eq involves explicitly evaluating arbitrarily high dimensional features instead kernel trick allows efcient computation q evaluating pairwise kernels supposing h induced kernel k q e e y e xp yq y characteristic kernel distribution p kernel feature map x h kernel mean map p kernel k characteristic map p injective characteristic kernel ensures mmd p q e information lost mapping distribution rkhs muandet et al examples characteristic kernels rd y include gaussian kernel laplace kernel y mmd gaussian kernel equivalent comparing moments distributions li et al b proof lemma weighted mmd q f contains functions h x r unit ball rkhs h dened sup hf e vp p e sq q recall f non negative importance weighting function according patil rao weighted probability density p p p v v q similarly q q p weighted mmd q restrict s sup hf e vp e sq sup e vp e sq rkhs simplies h e vp h e sq q e sq p e sq sup e vp e vp penultimate step follows dual norm proof similar mmd gretton et al c empirical estimate q q expanded e vp e h p sq q e p p p q q v e vp sq q e s applying kernel trick e p p p q v e vp sq q e s q loss generic summarization t st t st recalling nt mt lt nt nt t v s v nt t s nt mt s mt mt d training details train generic summarization model batch lbfgs liu nocedal learning rate train comparative rization model yogi optimizer zaheer et al wikipedia org wiki mini batch size topics learning rate decreasing learning rate half epochs choose number training epochs validating folds early stopping set patience epochs early stopping lbfgs optimizer epochs yogi optimizer tune hyperparameters training set optimal hyperparameters best model supmmd mkl searched space shown table kernel combination weights w shown table kernel combination weights w written order unigrams bigrams entities hyp b r epoch w table optimal hyperparameters search space mkl combination weights dataset e additional results section provide additional results e correlation rouge score dataset supmmd lexrank supmmd lexrank table correlation sentence importance scores normalized sentence rouge scores analyze correlation normalized rouge recall scores sentences sentence scores supmmd lexrank ized rouge score sentence dened shown table nd supmmd slightly high correlation sentence rouge scores suggests supmmd better capturing sentence importance summarization e feature correlations analyze correlation surface features sentence importance scores supmmd lexrank erkan radev b feature supmmd lexrank supmmd lexrank supmmd lexrank position tsf btsf words nouns table correlation features sentence scores supmmd lexrank eigenvector centrality method set set b icsi fourth day thrashing thunderstorms began heavier toll southern california deaths blamed rain ooding mudslides forced road closures emergency crews carried harrowing rescue operations downtown los angeles inches rain jan average rainfall entire year including inches record meteorologists southern fornia hit rain nearly years disaster latest caused rain snow battered california dec supmmd downtown los angeles inches rain jan average rainfall entire year including inches record fourth day thrashing thunderstorms began heavier toll southern california deaths blamed rain ooding mudslides forced road closures emergency crews carried harrowing rescue operations roads los angeles county equally frustrating rain saturated hillside gave way sending mississippi like torrent earth trees blocks oceanfront town killing men californians braced rain gled recover storms left people dead triggered mudslides tornadoes washed away roads runways record inches centimeters set mudslides forced amtrak suspend train service tween los angeles santa barbara thursday winter storm pummeled southern fornia straight day claiming lives people raising fears mudslides homes region evacuated staff writers rick orlov lisa mascaro contributed story storms caused million million damage los angeles county roads facilities beginning year multi million dollar homes collapsed mudslides trapped residents homes heavy rains claimed lives pelted los angeles fth straight day scenes reminiscent aftermath northridge earthquake years ago month los angeles area residents faced gridlocked freeways roads day cleanup crews cleared mud rubble bris left week siege rain shattering storm slammed southern california sixth straight day tuesday triggering mudslides tornadoes forcing road closures ers predicted wane wednesday new storm moves sunday night table example summaries topic containing articles rains mudslides southern california highlight phrases bold help identify difference set b summaries icsi supmmd methods suggest set contains articles describing events earlier days disaster set b contains articles later stage disaster shown table supmmd higher correlation relative position signifying importance position sentence summary sentences lexrank higher correlations number words number nouns tfisf scores sentences expected lexrank eigenvector centrality sentence sentence similarity matrix suggest supmmd able learn rst sentences important news summarization similar result reported kedzie et al rst sentences important creating summary news articles e example summary present update summaries set b topic contains articles rains mudslides southern california table
