automatic text document summarization semantic based analysis thesis submitted jawaharlal nehru university award degree doctor philosophy chandra shekhar yadav school computer systems sciences jawaharlal nehru university new delhi india july dedicated parents school computer systems sciences jawaharlal nehru university new delhi declaration certify thesis entitled automatic text document summarization semantic based analysis submitted school computer systems sciences jawaharlal nehru university new delhi partial fulfillment requirements award degree doctor philosophy record bonafide work carried supervision dr aditi sharan thesis contains words length exclusive tables figures bibliographies matter embodied thesis submitted university institution award degree diploma chandra shekhar yadav enrollment school computer systems sciences jawaharlal nehru university new india iii school computer systems sciences jawaharlal nehru university new delhi certificate certify thesis entitled automatic text document summarization semantic based analysis submitted mr chandra shekhar yadav school computer systems sciences jawaharlal nehru university new delhi award degree doctor philosophy research work carried supervision dr aditi sharan supervisor dr aditi sharan dean prof d k lobiyal school computer systems sciences school computer systems sciences jawaharlal nehru university jawaharlal nehru university new india new india iv v acknowledgments goal achievable guidance glad express sincere gratitude utmost regards supervisor dr aditi sharan guidance helpful discussions intellectual inputs thesis work worthy extensive research experiences helpful thesis important thing helping nature friendly behavior contributed important share fulfillment work methodology philosophy problem solving methods suggested great help work grateful teachers sc ss jnu especially prof k k bharadwaj prof c p katti prof karmeshu prof d p vidyarthi prof s n minz prof r k agrawal prof d k lobiyal prof t v vijay kumar dr buddha singh guidance support motivation feel depressed rejection hard comments research papers like express thanks dean sc ss jnu support pursue work school extend thanks school administration especially chandra sir meena maam ashok sir librarian sc ss library dr b r ambedkar central library jnu supporting work carried lab healthy friendly work culture regular lab discussions exchange ideas opinions thankful dr hazra imran ubc canada ms manju lata joshi dr nidhi malik dr jagendra singh dr mayank saini dr sifatullah siddiqi dr sonia mr vikrant vaish mr ashish mrs sheeba siqqique ms bhawana gupta support stay lab sc ss jnu special thanks mr rakesh kumar miss payal biswas encouraging helping writing thesis wonderful stay lab lifetime memorable students helpful supportive courageous thankful seniors juniors fellow students friends like thank dr kapil gupta nit k dr neetesh kumar iiitm gwalior dr vipin mgcu dr navjot mnnit dr yogendra meena du dr raza abbas haidri du dr gaurav baranwal bhu mr prem yadav dte mr harendra vi pratap du mr sumit kumar mr krishan veer du dr dinesh kumar cuh mr utkarsh nit d mr dhirendra dtu people encouraged motivate feel depressed work thankful director sliet dr shailendra jain colleagues computer science department dr major singh h o d cse dr damanpreet singh dr birmohan singh dr manoj sachan dr sanjeev singh mrs gurjinder kaur dr vinod verma mr jaspal sigh mr manminder singh mr rahul gautam mrs preetpal kaur dr sanjeev singh electrical sliet mr pankaj das electronics sliet dr amit rai chemical sliet mr jonny singla mechanical sliet thanks wonderful family bearing blessed love compassion supplications support family members period deepest gratitude goes selfless love care indulgence life thesis simply impossible heart goes reverence respected father mr padam singh yadav dear mother mrs suvita devi blessings sacrifices tremendous affection patience indebted sisters mrs rinkesh yadav younger brother mr kesari singh yadav unconditional love blind faith continuous motivation tough time finally like express thanks person directly indirectly related work like thank university grant commission ugc council scientific industrial research csir india research fellowship period chandra shekhar yadav enrollment school computer systems sciences jawaharlal nehru university new india vii abstract advent web data wen increased million folds recent years web data generated data stored years important data format text answer user queries internet overcome problem information overload possible solution text document summarization reduces query access time optimize document results according specific user s requirements summarization text document categorized abstractive extractive work direction extractive summarization extractive summarized result subset original documents objective content coverage lea redundancy work based extractive approaches approach statistical features semantic based features include sentiment feature idea cached view emotion plays important role effectively conveys message play vital role text document summarization second work extractive summarization dimensions based latent semantic analysis document represented form matrix rows represent concepts cover different dimensions columns represents documents lsa ability mapped concept space lsa hold synonyms relations mapping concepts based svd decomposition concepts entropy find informative concepts sentences lsa hold polysemy relations extend work wordnet relations handle relationships words sentences work lexical network created informative sentences extracted based extend lexical chain based work designed optimization function optimize content coverage redundancy length constraints nature function linear constraints linear applied integer linear programming find solution viii contents declaration iii certificate acknowledgments vi abstract viii list figures xii list tables xiv chapter introduction automatic text document summarization introduction flavors summarization extractive abstractive summarization single multi document summarization query focused generic summarization personalized summarization guided summarization indicative informative critical summary state art approach summarization corpus description corpus description hybrid approach duc dataset summary evaluation content based task based readability thesis objective chapter hybrid approach single text document summarization statistical sentiment features introduction literature work background random forest binary logistic regression features text document summarization location feature ix aggregation similarity feature frequency feature centroid feature sentiment feature summarization procedure algorithm detailed approach description experiment results experiment experiment experiment experiment concluding remark chapter new latent semantic analysis entropy based approach automatic text document summarization introduction background introduction latent semantic analysis lsa summarization introduction entropy information related work gongliu model murray model sj model model model proposed model working lsa example mincorrelation model lsacs model lsass model proposed model measure redundancy summary experiment results concluding remark chapter lexnetwork based summarization study impact wsd techniques similarity threshold lexnetwork introduction related work background word sense disambiguation wsd lexical chain lexalytics algorithms centrality proposed work experiments results performance different centrality measure selected lesk algorithm different threshold impact wsd technique threshold different centrality measures comparisons lexalytics algorithms overall performance concluding remark chapter modeling automatic text document summarization multi objective optimization introduction literature work related work baseline mdr model background linear programming proposed multi objective optimization model outline proposed model detail description proposed model experiments results cosine based criteria minimize redundancy b lexical based criteria reduce redundancy co relation analysis different centrality based proposed model concluding remark chapter conclusion future work references xi list figures showing inshorts app s news google snippet example step process text document summarization sentence length y axis vs sentence number x axis optimal summaries scus figure random forest algorithm figure algorithm summarization opinosis generated summary precision curve recall curve showing f score f score summary figure comparative performance model model vt matrix showing mechanism lsa matrix decomposed u lsa based summarization procedure lsa decomposition matrix reduced dimension vector representation d space u vt matrix d space representing words sentences showing improved performance proposed proposed showing improved performance entropy based proposed rouge score entropy based system showing number words increasing rouge score decreasing example lexical chain creation dataset available elhadad showing normalized degree centrality undirected graph showing graph nodes edges showing node degree regular graph steps summarization graphical form directed graph refers sentence corresponding weighted edges sentences shown performance evaluation rouge score different centrality measures adapted lesk wsd similarity threshold performance evaluation rouge score different centrality measures cosine lesk wsd similarity threshold performance evaluation rouge score different centrality measures cosine lesk wsd similarity threshold performance evaluation rouge score different centrality measures simple lesk wsd similarity threshold performance evaluation rouge score different centrality measures simple lesk wsd similarity threshold impact wsd technique similarity threshold subgraph based centrality impact wsd technique similarity threshold page rank centrality measure xii impact wsd technique similarity threshold bonpow based centrality impact wsd technique similarity threshold ness centrality impact wsd technique similarity threshold closeness centrality impact wsd technique similarity threshold alpha centrality alpha semantrica lexalytics vs proposed system improved showing overall performance system comparative study graphical method solution w t figure outline summarizer system module process automatic text summarization showing procedure stepwise example duc healthcare data set file number txt figure comparative performance proposed mdr baseline model different scatter plots showing different directions strengths correlation figure graph showing comparative performance different relevance based measure sign denoting hybridization different features xiii list tables different features scores total score sentences system generated summary proposed algorithm microsoft system generated summary mead system generated summary summary generated algorithm considered system summary summary model summary summary generated algorithm system summary summary model summary summary generated algorithm system summary summary model summary summary generated different system summary human generated summary model summary summary generated different system considered system summary generated summary model summary f score summary different rouge score summary generated different approaches different rouge score shown duc dataset table showing performance model table showing performance o model frequency based words documents matrix w reduced space w processed retaining positive related sentences rowsum calculated find probability reduced space r information contained concept corresponding sentence xiv w reduced space w processed keeping positive related sentences column sum find measure probability information contained sentence reduced space r rouge score previously proposed models performance proposed models proposed proposed performance model entropy based model generally increasing summary length rouge score increasing reducing showing average information contains summary generated different systems average n gram presents different systems generated summary summarization m stands model showing sentences txt duc dataset creation lexical network showing lexical semantic relations present sentences performance different centrality measures adapted lesk wsd similarity threshold performance different centrality measures adapted lesk wsd similarity threshold performance different centrality measures cosine lesk wsd similarity threshold performance different centrality measures cosine lesk wsd similarity threshold performance different centrality measures simple lesk wsd similarity threshold xv performance different centrality measures simple lesk wsd similarity threshold impact wsd technique similarity threshold subgraph based centrality impact wsd technique similarity threshold eigen value based centrality impact wsd technique similarity threshold eigen value based centrality impact wsd technique similarity threshold page rank centrality measure impact wsd technique similarity threshold bonpow based centrality impact wsd technique similarity threshold closeness centrality comparison semantrica lexalytics algorithm difference centrality based measure adapted lesk wsd showing improved system w semantrica comparison semantrica lexalytics algorithm difference centrality based measure cosine lesk wsd showing improved system w semantrica comparison semantrica lexalytics algorithm difference centrality based measure simple lesk wsd showing improved system w semantrica performance centrality based measures wsd technique similarity threshold precision recall f score model mdr model subgraph centrality precision recall f score model mdr model pagerank centrality precision recall f score proposed model centrality based measure subgraph centrality precision recall f score proposed model centrality based measure betweenness centrality xvi precision recall f score proposed model centrality based measure pagerank precision recall f score proposed model centrality based measure evencentrality precision recall f score proposed model centrality based measure precision recall f score proposed model centrality based measure closeness precision recall f score proposed model centrality based measure bonpow showing pairwise pearson s correlation coefficient symmetric showing pairwise spearman s correlation coefficient symmetric showing pairwise kendall s correlation coefficient symmetric precision recall f score proposed model relevance decided hybridizing subgraph bonpow precision recall f score proposed model relevance decided hybridizing subgraph betweenness xvii chapter introduction automatic text document summarization introduction internet defined worldwide interconnection individual networks operated government industry academia private parties originally internet served interconnect laboratories engaged government research expanded serve millions users multitude purposes parts world december million internet users present world population june million people world population according ibm marketing cloud study data internet created people businesses devices data factories pumping incredible amounts information web day size digital data understood given statistics number tweets minute increased tweets minute ii minute facebook comments posted statuses updated photos uploaded iii google searches conducted worldwide minute day iv worldwide texts sent minute v new data producing social media users day vi million tweets day according internet mobile association india imai livemint s article published march number internet users india expected reach million june million december information increasing day day leads information overload termed information glut data smog information overload occurs input system exceeds processing capacity decision makers fairly limited cognitive processing capacity consequently information overload occurs likely reduction decision quality occur information overload dealt certain level representation concise information information web textual efficient text summarizer concisely represent textual information web radev hovy mckeown outlined summary text produced texts conveys important information original longer half original usually significantly simple definition captures important aspects summaries produced single document multiple documents summaries preserve important information summaries short explained alguliev aliguliyev mehdiyev automatic text document summarization task interdisciplinary research area computer science including artificial intelligence statistics data mining linguistic psychology torres moreno defined automatic summary text generated software coherent contains significant relevant information source text sakai sparck jones defines summary reductive transformation source text summary text extraction generation mani maybury states summary document containing text units words terms sentences paragraphs present source document text summarization applications accounting research efficient utilization results text document summarization based real life system ultimate research assistant developed hoskinson system performs text mining internet search work takale kulkarni shah highlighted applications text document summarization search engine google system newsblaster proposed mckeown et al automatically collects cluster categorize summarize news different websites like cnn reuters provides facility users browse results hovy lin introduced summarist system create robust text summarization system system works phases describe form equation like summarization topic identification interpretation generation application summarization news summarization inshorts app shown figure google snippet generation showing inshorts app s news google snippet example broadly summarization task categorized type extractive summarization abstractive summarization abstractive summarization focus human like summary extractive summarization based extractive entities entities sentence subpart sentence phrase word till extractive based summarization relies standard features like sentence position sentence length frequency words combination tf idf score selection candidate word nouns verbs cue words numbers present text uppercase bold letter words sentiment words sentences aggregate similarity centrality goal feature based summarization mix different strategy find salient sentences include summary process shown figure step process text document summarization flavors summarization overlapping summarization techniques categorize fairly section trying present reasonable overview existing techniques extractive abstractive summarization broadly summarization task categorized type abstractive summarization extractive summarization abstractive summarization human like summary actual goal text document summarization defined mani maybury wan abstractive summarization needs things information fusion sentences compression reformation actual challenge abstractive summarization generation new sentences new phrases produced summary retain meaning source document according balaji geetha parthasarathi abstractive summarization requires semantic representation data inference rules natural language generation proposed semi supervised bootstrapping approach identify relevant components abstractive summary generation study goldstein mittal carbonell callan state human generated summary varies person person reason maybe setup human mind domain knowledge interest particular domain extractive summarization based extractive entities entities sentence subpart sentence phrase word till work extractive summarization extraction easy based scoring criteria words sentences phrases evaluation extractive summary easy based word counts word sequences work focused extractive based technique single multi document summarization according ou khoo goh single document summarization defined process representing main content document multi document summarization process representing main content set related documents topic instead document possible approaches multi document summarization approach combine documents single document apply single document summary second possible approach generates summary document combine summary document later perform single document summarization combined summary multi document summary according sood important note concatenation individual single document summaries necessarily produce multi document summary issue later approach generate single summary combine generate summary process relative sentences position changes coherence lost research gap opens new dimensions research query focused generic summarization text document contain topics like social economic development political views common people s views environment entertainment interested angle need specific text given text fill requirement user query q document d certain similarity system return desired documents called query focused summarization tombros sanderson proposed query focused summarization text document develop information retrieval system white jose ruthven extended use web document summarization combining features including text formatting page query dependent features rationale approach concur words query included generated summary type summarizer system generic summarizer regardless user need personalized summarization according dolbear et al personalization defined technology enables summarizer system harmonize differently available contents applications user interaction modalities user stated system s learned preferences main objective personalization enable system content offerings closely targeted user desires achieved different methods like content filtering extract contents appropriate user preferences set available content recommendations provides content user based criteria include user previous acceptance related content consumption related content peer group guided summarization extension query focused summarization instead single question set question guided summarization seen template based summarization template set question fired text document system returns summary form question answers method works developer good domain knowledge accurate predictor question shortly consider disaster example set question theme based cause accident killed critical normal condition relief measure political visit held compensation paid effective people leads production focused summaries concerning questions raised indicative informative critical summary hahn mani defined kinds summary indicative summaries follow classical information retrieval approach provide content alert users relevant sources users read depth informative summaries act substitutes source mainly assembling relevant novel factual information concise structure critical summaries reviews containing informative gist incorporate opinion statements content add value bringing expertise bear available source critical summary gettysburg address gettsyburg address short greatest american speeches ending words especially powerful government people people people shall perish earth state art approach summarization according state approaches summarization procedure classified following limited linguistic structure cohesion introduced halliday hasan captures intuition technique sticking different textual unit text cohesion achieve use semantically related terms like coreference conjunctions ellipsis different cohesion building devices lexical cohesion easily identifiable frequent type important source flow informative content centroid cluster approach documents divided units units document paragraphs sentences based criteria clusters created famous criteria s like cosine ngt vector based similarity clustering summarizer system picks unit cluster considered representative cluster later added summary approach applied single documents machine learning generally talk extractive summarization based statically based feature linguistic features hybridization machine learning based summarization effective learns features weights given data later learned weight test data required labeled data multi objective main concern summary informative length constraints informative constraints designed reducing redundancy increasing coverage approach authors designed function way reduce redundancy increase coverage length constraints later function optimized different techniques corpus description chapter dataset created details dataset mentioned section experiment repeated standard dataset duc rest works relies dataset details duc dataset described section corpus description hybrid approach june multi day cloudburst centered north indian state uttarakhand caused devastating floods landslides country worst natural disaster parts western nepal tibet himachal pradesh haryana delhi uttar pradesh india experienced flood casualties occurred uttarakhand july according figures provided uttarakhand government people presumed dead uttrakhand flood corpus self designed taken newspapers ex hindu times india dataset published paper c s yadav sharan joshi chandra shekhar yadav sharan showing statistically linguistic statics dataset statistical statistics total sentences document length document stop word removed total number distinct words minimum sentence length words maximum sentence length words average sentence length experiment sql stopword list available mysql com doc en fulltext stopwords html seeing interpret sentences length sentences length sentence length y axis vs sentence number x axis linguistic statistics linguistic analyzing number nature significant entities like nn nnp nns dt jj jjr jjs vb vbn vbd vbz vbg vbp different abbreviation stands nn noun singular mass nns nounplural proper noun singular nnps proper noun plural vb verb vbd verb past tense vbg verb gerund vbn verb past participle vbp verb non person singular vbz verb person singular jj adjective jjr adjective comparative jjs adjective superlative dt determinant note means x entity type count duc dataset document sets produced data text retrieval conference trec disks question answering track dataset includes data wall street journal ap newswire san jose mercury news financial times la times disk fbis disk set average documents words maximum length defined single text document abstract document words long multi document abstract divided parts according words long document divided sets set categories following single natural disaster event created seven day window single event domain created seven day window multiple distinct events single type limit time window documents contain biographical information single individual summary evaluation find good summary lot work decide quality summary challenging task dimensions like length constraints different writing styles lexical usage e context research goldstein et al conclude human judgment quality summary varies person person little overlap sentences picked people human judgment usually find concurrence quality given summary confusing e tedious measure quality text summary summary evaluation based task based explained sub sections content based measure evaluate summary presence textual units e n gram peer summary standard summary example content based measures rouge blue content based pyramid rouge evaluation researchers recall oriented understudy gisting evaluation rouge introduced c lin duc officially adopted summarization evaluation model rouge compares system generated summary different model summaries considered rouge effective approach measure document summarizes widely accepted rouge measure overlaps words system summary standard summary gold summary human summary overlapping words measured based n gram co occurrence statistics n gram defined continuous sequence n words multiple rouge metrics defined different value n different models like lcs weighted rouge s su lower upper case matching stemming standard rouge n defined n stands length n gram number n grams present reference summaries maximum number n grams co occurring system summary set reference summaries rouge measures generally gives basic score precision recall f score score sufficient indicator summarizer performance variation rouge rouge n rouge l w rouge s rouge su evaluation rouge measure l w s su taken task generate single document summary words evaluating summary words mentioned earlier abstract dataset category single event domain created seven day window guidelines recall precision defined following simple f score evaluation results showing f score f score given harmonic mean precision recall representing fuzz f score pr rouge n measures n grams uni gram bi gram tri gram higher order n gram overlap rouge l measure lcs largest common subsequence advantage rouge l rouge n nt require consecutive matches nt define n gram length prior x reference summary y candidate summary length m n respectively lcs based precision recall f score defined equation duc document understanding conference set large quantity variant rouge s s stands skipping bigram skip bigram allows maximum words gap lexical units understood example phrase cat hat skip bigrams following cat cat cat hat hat hat y number skip bigram matches x y c combination function control relative importance skip bigram based precision recall score given equation measure rouge su measures skip bigram count peer summary standard summary find similarity summaries measure sensitive word order considering consecutive matches uni gram matches included measure credit candidate sentence sentence word pair co occurring reference recall precision f measure calculated following manner bleu pyramid bleu method proposed automatic evaluation machine translation system primary programming task bleu implementer compare n grams candidate n grams reference translation count number matches matches position independent matches better candidate translation kinds summary generated system generated e peer summary human generated summary e reference summary reference summary content units scu find set words pyramid constructed evaluation method peer summary contributor e lexical unit summary scu matched scu pyramid advantage pyramid method evaluates summary tells idea summary chosen best result method obtained unigram overlap similarity single link clustering process evaluate summary user required reference summaries optimal summaries scus figure higher weight scu placed pyramid weighted scus weight placed reflecting fact fewer scus probable summaries compare task based try measure prospect summaries certain task mention important tasks document categorization information retrieval question answering given text document develop summarizer system concise summary let summary s according task based evaluation fire queries s example question answering task given set question measure precision recall queries response decide quality summary summarizer system readability text analysis conference tac automatically evaluating summaries peers aesop task focus developing automatic metrics measure summary content system level tac new task introduced evaluate participant ability measure summary readability level summarizers individual summaries measure readability summaries accessed based linguistic based criteria grammaticality correctness nonredundancy lexical units referential clarity focus summary text structure coherence humans evaluated peer summaries based linguistic questions assigned different score point scale represents worst best summary thesis objective thesis divided chapters work extractive summarization techniques focus semantic features summarization chapter introduction text summarization second chapter proposing hybrid model single text document summarization model extraction based approach combination statistical semantic technique hybrid model depends linear combination statistical measures sentence position tf idf aggregate similarity centroid semantic measure work impact sentiment feature summary generation find optimal feature weight better results comparison generate different system summaries proposed work mead system microsoft system opinosis system human generated summary evaluation summary content based measure rouge chapter proposing models based approaches sentence selection relies lsa proposed model sentences extracted right singular matrix maintain diversity summary second proposed model based shannon entropy score latent concept second approach sentence approach extracted based highest entropy work propose new measure measure redundancy text fourth chapter present lexical network based new method ats work divided different objectives objective construct lexical network second objective constructing lexical network use different centrality measures decide importance sentences wsd intermediate task text analysis objective analysis performance centrality measure changing change wsd technique intermediate step cosine similarity threshold post processing step fifth chapter present optimization based criteria automatic text document summarization based steps preprocessing sentences output goes second stage concern lexical network creation output network importance sentences given betweenness centrality score final module decide optimization criteria combination centrality lexical network solve objective criterion use ilp integer linear programming find solution e sentences extract summary aligned means stated objectives thesis follows proposing statistical semantic feature based hybrid model text document summarization proposing lsa based model captures linguistic feature text proposing new summary evaluate measure based information contains maintain syntactic semantic property text create lexical network find based previous objective create new objective function optimize expect lexical unit summarization better summary chapter hybrid approach single text document summarization statistical sentiment features introduction chapter proposing hybrid method single text document summarization linear combination statistical features proposed past new kind semantic feature sentiment analysis idea include sentiment analysis feature summary generation derived concept emotions play important role communication effectively convey message play vital role text document summarization comparison different system summaries mead system microsoft system opinosis system human generated summary evaluation content based measure rouge literature work till research direction extractive summarization based approaches extractive summarization important task find informative sentences subpart sentence phrase include extractive elements summary presenting work categories early category work recent work early work document summarization started single text document luhn proposed frequency based model frequency words plays crucial role decide importance sentence given document work baxendale introduced position based statistical model research found starting ending sentences informative summary generation position based measure works newspapers summarization better scientific research paper documents continuation position based work edmundson suggests sentences paragraphs sentences paragraph assigned higher weights sentences document kupiec pedersen chen assigned relatively higher weight paragraphs paragraphs document radev jing sty tam followed different positional value position e pi ith sentence calculated equation n representing number sentences document represents ith sentence position sentence inside text cmax score sentence maximum centroid value radev blair goldensohn zhang proposed mead system single document summarization sentence score depends features centroid position sentence features find importance sentence decided sum features position score proposed linear monotonic decreasing function ganapathiraju carbonell yang considered keyword occurrence feature understanding keywords document represent theme document title keywords indicative theme assigned higher score location uppercase word feature containing proper names included summary generation indicative phrases like report short length sentences sentence pronoun reduce score sentence generally included summary rambow shrestha chen lauridsen proposed method e mail summarization based conventional feature common authors new features conventional features absolute position centroid based idf length sentence jagadeesh pingali varma divided features type sentence level word level sentences level features include position sentences given document presence verbs sentences referring pronouns sentences length sentence terms number words word level features include term frequency tf word length parts speech tag familiarity word analysis smaller words higher frequency occur frequently larger words negate effect considered word length feature summarization feature familiarity word derived wordnet relations familiarity indicate ambiguity word author words familiarity given higher weight sigmoid function calculate importance word given features named entity tag occurrence headings subheadings font style score sentence given combining features work widely considered leading sentences important compared preceding sentences according ouyang li lu zhang true actual data importance sentences varies according user user writing style instead sentences position focus word position claims word position features superior traditional sentence position features defined different word position features direct proportion inverse proportion geometric sequence binary function finally score sentence given found features karanikolas galiotou defined features category term weighting position keyword based term weighting sentence weighting comprises different ways local global weighting term weighting proposed different ways shown following tij representing weight jth term document di fij frequency jth term document di max fi frequency frequent term document di fi sum frequencies index terms existing document di standard approach followed introduced new feature ridf residual idf residual idf jth term document di defined difference observed idf expected idf assumption terms follow poisson distribution sentences position followed new kind model proposed news articles algorithm hariharan score method considers paragraph location sentence location paragraph sp number paragraphs document p position paragraph sip number sentences paragraph ssip sentence position inside paragraph kind feature title words keywords final score given combining features scores feature luo zhuang shi position sentence length sentence likelihood sentence number thematic words number low frequency words lsa score sentence number gram keywords number words appearing sentences entropy sentence relevance sentence defined relevance measures intra sentence relationships sentences entropy based feature denotes quantity information implied sentence mentioned long sentences likely cover number aspects document compared short sentences long sentence comparative entropy short length sentence large entropy sentence possibly implies large converge shimada tadano endo proposed method multi aspects review summarization based evaluative sentence extraction proposed features ratings aspects tf idf value number mentions similar topic ratings aspects divided different levels low high rating given zhang li gao ouyang considered following words summarization features like cue words phrases abbreviations acronyms non cue words opinion words vulgar words emoticons twitter specific symbols rt tofighy raj javad features like word frequency keywords sentence headline word cue word cue word cue word paragraph sentence location sentence length sentence position scores following method gives equal importance second second position score represented following padmalahari kumar prasad proposed feature keyword based work keyword nouns determined keyword found morphological analysis noun phrase extraction clustering scoring second feature position based feature term frequency calculated unigram bigram frequency nouns considered computing bigram frequencies fourth feature length word fifth feature parts speech tag tags ranked assigned weights based information contribution sentence linguistic features proper noun pronouns rautray balabantaray bhardwaj proposed features score sentence feature title feature based similarity overlapping sentence document title divide total number words sentences title second feature sentence length longer sentences given weight compared small length sentences feature frequency based fourth feature position based depends positions paragraph paragraph s position fifth feature aggregate similarity sixth feature based counting proper nouns seventh thematic word score based word frequency numerical data based score roul sahoo goel length sentence weight sentence given tf idf sentence density presence named entities sentence belong number categories like names organization locations quantities presence phrases sentence summary investigation conclusion paper describes important best hardly significantly particular relative offset sentence sentences located beginning end document tends imperative carry relevant information like definitions conclusions type sentences receive score presence title words sentence score sentence given common textual units document title total number words title presence specially emphasized text e quoted text sentence generally situated double quotation marks receives score score presence upper case letters sentence uppercase words phrases usually refer important acronyms like names places sentences received score sentence density represented ration total count keywords sentence total count words including stop words sentence section describing different model weight learning section divided dataset training testing random forest predictive modeling machine learning technique ensemble classifier decision models ensemble models combine results different models versatile algorithm capable performing regression classification performs implicit feature selection according breiman random forests combination tree predictors tree depends values random vector sampled independently distribution trees forest algorithm random forest presented figure boulesteix janitza kruppa knig characteristic algorithm large number trees generated technique issue overfitting convergent background random forest figure random forest algorithm binary logistic regression y binary response variable dependent feature yi true condition satisfies yi false xk set independent features xi discrete continuous combination xi observed value independent ith observation learn model represented equation shows parameter estimation logistic regression parameter estimation maximizing equation features text document summarization proposing hybrid model salient sentence extraction single text document summarization based types features statistical features e location frequency tf idf aggregate similarity centroid semantic feature sentiment feature section presenting detailed features description sentence selection approach location feature baxendale introduced position feature work manual later measure widely sentence scoring author concluded leading sentences article important model given explained n total number sentences model n aggregation similarity feature kim kim hwang defined aggregate similarity score sentence sum similarities sentence vectors document vector space model given wik defined binary weight ok kth word ith sentence similarity measure plays important role text document summarization literature proposed different similarity measure affects outcome implementation cosine similarity based criteria let sentences vector wim wjm standard cosine similarity si sj given value j vary m frequency feature early work document summarization started single document summarization luhn ibm author proposed frequency based model frequency word plays crucial role decide importance word sentence given document method traditional method tf idf measure defined e tf stands term frequency idf inverse document frequency tfi term frequency ith word document nd represents total number documents idfi document frequency ith word data set implementation calculate importance word wi tf considering sentence document idf entire document dataset centroid feature radev et al defined centroid set words statistically important cluster documents centroids identify salient sentences cluster classify relevant documents centroid score ci sentence si computed sum centroid scores cw words appeared particular sentence presented sentiment feature previous sections mentioned statistical measures elaborating semantic based feature calling feature semantic feature set things related defined mani maybury semantic summary generation shallow level analysis deep level analysis shallow approach analysis sentence level syntactic important note word level analysis semantic level deep analysis sentential semantic level representation approach e sentiment feature semantic low level analysis entity level finding sentiment score sentence found entities present sentence find sentiment scores entity sum entity sentiment score e sentiment strength sentiment entity neutral scorning zero entity sentiment positive considering adding find total score sentence sentiment score negative multiplying minus covert positive score adding score find total score given reason considering negative score positive score interested sentiment strength positive negative e sentiment score entity means sentiment entity negative strength detail procedure explained section s fifth feature representing mode e summarization procedure summarization approach based salient sentence selection extraction importance sentence decided combined score given sum statistical measures semantic measure section explaining approach algorithm work section detail explanation basically work summarization divided passes sentence scoring sentence extraction evaluation algorithm algorithm divided passes sentence scoring according linear combinations different measures salient sentence extraction summary generation evaluation summary pass sentence scoring input documents output scored sentences step score sentence given different measures outcome m n matrix m sentences n measures aggregate cosine similarity position sentiment sentence centroid score sentence tfidf step normalized columns matrix step add features sentence calling sum score sentence step sort according score highest score representing significant sentence pass algorithm redundancy input number sentences descending according total score output extracted sentences step summary scored sentence step number sentences parameter initialization summary similarity threshold l required length summary ith sentence length summary l summary ith sentence step rearrange summary sentences given source document maintaining cohesiveness pass evaluation summary input different summaries standard summaries peer summaries output precision recall f score step generate different summary different length mead microsoft opinosis human humans proposed algorithm experiment experiment precision recall f measure model summary mead microsoft opinosis peer summary proposed algorithm model summary human generated summary peer summary mead microsoft opinosis proposed method step rouge n rouge l rouge w set rouge s rouge su measure find figure algorithm summarization detailed approach description describing detail approach described section pass sentence scoring extraction algorithm defined section things covered gives main idea algorithm micro points needs specify sum linear combination different measures statistically dependent e aggregate similarity position tf idf centroid fifth measure semantically dependent e sentiment demonstration working model shown feature position sentences position important indicator informative sentence analyzed leading sentences contain important information implementation results shown s second column second feature tf idf standard formula defined previous section normalized tf idf score given columns feature aggregate similarity cosine score sentence vector calculated sum similarities sentence vectors document vector space model significance find sentences highly similar sentences representing sentences vector space find vector cosine similarity sentences defined standard formula normalized aggregate cosine similarity column scores centroid position sentiment need normalized score normalization values means adjusting values measured different scales notionally common scale removes chance biased w t values implementation column normalization instead matrix normalization normalization column vector xn xi ith element column n size column b let given matrix size column values normalization column column b normalized matrix case fourth feature centroid based radev et al defined centroid set words arestatisticall important cluster documents approach mead centroid score output input centroid value sentence given summation word centroid score present sentence fifth feature sentiment score novelty work find feature dependent alchemy api available alchemyapi considered finding sentiment score semantic approach fall shallow level approach defined section sentence words define kinds sentiment neutral negative positive neutral sentiment value means words sentence sentiment score zero important note easy find sentiment score based cue word like good bad pleasant complexity text words limitation nlp possible find correct sentiment score possible detect sentiment hidden sentiments overall working fifth feature understood clearly following documents document naac accredited jnu cgpa point scale grade highest grade naac sentiment neutral document jnu ranked times higher education asia brics ranking sentiment document positive score naac stands national assessment accreditation council naac brics stands nations brazil russia india china south africa representing positive news jnu sentiment neutral sentiment positive score discover approach find correct sentiment hidden sentiment results displayed sentiment results implementation find sentiment score sentence alchemy api finding entities present sentence sentiment score add entity sentiment score example consider document number bjp spokesperson prakash javadekar said party president entities follow prakash javadekar person rajnath singh person uttarakhand state county bjp company president jobtitle triplet x y z representing x entity y entity type z sentiment score sentiment score sentence add sentiment score sentence result row column sentiment score value considering positive sentiment scores entity sentiment score negative multiplying convert positive score obvious goal procedure equal importance magnitude let consider childhood story fox grapes fox grapes story consider sentences given document document important story things grapes document sweat juicy grapes quench thirst document probably sour alchemy system find sentiment sentences sentiment positive score sentiment negative magnitude important reasons consider value interested find sentiment strength negative positive important add negative score find total score value reduce step finding total score sentence adding scores total score represented given implementation n detail result individual score given column total score scores different features scores total score sentences sent position score tf idf aggregate cosine sim centroid score sentiment score sum pass redundancy remove redundancy model proposed sarkar topmost sentence according total score defined equation add summary added sentence summary similarity threshold algorithm described section s input pass number sentences sorted according descending total score need initialize parameter desired summary parameter like summary initially given similarity threshold l desired length summary system l stands maximum length desired summary limitation length sentences guarantee minimum maximum length summary add sentence summary summary length l similarity new sentence summary output step summary minimal redundancy length nearly equal l position sentence zigzag lost sequence cohesiveness maintain sequence need rearrange sentences according given initial index representing summary generated system similarity threshold desired summary length define arbitrary l number words percentage summary required chosen small large like sentences summary depend total score words summary depends total scores shown objective minimize redundancy note calculating sentence summary eliminating stopwords stopwords play big role increase similarity sentences different stopwords list different similarity score mead microsoft model generated summary different length shown truth model microsoft generated summary summarizer inbuilt inside microsoft office package observed microsoft summarizer reducing redundancy sentence similar sentences shown pass evaluation goldstein et al concluded things human judgment quality summary varies person person human judgment usually find concurrence quality given summary difficult judge quality summary evaluation summary need summaries system generated summary summary user generated model summary standard summary generate different model summary approaches text data set people tell write summary words generate summary mead tool approach taking linear combination position centroid score model summary generated opinosis ganesan zhai han summary given figure microsoft system evaluate summary rouge evaluation package duc adopts rouge official evaluation metric single text document summarization multi document summarization rouge finds recall precision f score evaluation results based gram co occurrence statistic rouge measures system generated summary machine summary overlaps standard summary human summaries model summary n gram contiguous sequence n words evaluation adopting different measures rouge rouge n rouge w rouge l rouge s rouge su experiment results section presenting experiments experiment took summary generated algorithm discussed section system generate summary summary model summary second experiment comparing different system generated summary w t human summary experiment showing significance sentiment feature fourth experiment finding best feature weight combination regression random forest experiment explained section s pass created types model summary human summary gave data set persons summarize based experience instruction summarize words length limitations user experiences generated summary varies words length mead microsoft opinosis system different system generated summaries given opinosis summarizer abstractive type figure giving summarization result length summary generated opinosis system presenting different summaries generated different systems system generated summary proposed algorithm microsoft system generated summary mead system generated summary opinosis generated summary experiment took summary generated algorithm discussed section system generate summary summary model summary step find different rouge scores rouge l rouge w rouge s rouge defined c lin rey followed like sankarasubramaniam ramanathan ghosh rouge scores given measures things recall precision f score system generated summary model summary reference summary comparing system generated summary model summary length summary result given length respectively limitation space providing tables showing f measure different model summaries length nearly summary length nearly simple term define high precision means algorithm retrieved substantially relevant irrelevant high recall means algorithm returns relevant result precision recall wiki summary length getting high precision f score w t mead reference summary high recall w t microsoft generated summary summary generated algorithm considered system summary summary model summary measure mead microsoft opiosis summary r p f r p f r p f rouge l rouge w rouge s rouge su summary generated algorithm system summary summary model summary measure mead opiosis summary r p f r p f rouge l rouge w rouge s rouge su summary generated algorithm system summary summary model summary measure mead mead microsoft microsoft opiosis summary r p f r p f r p f r p f r p f rouge l rouge w rouge s precision curve recall curve showing f score experiment second experiment comparing different system generated summary w t human summary words model reference summary human generated summary summaries system generated summary e mead microsoft opinosis algo system generated summary result shown representing different rouge scores summary length summary generated different system summary human generated summary model summary measure method mead opinious user r p f r p f r p f r p f rouge l rouge w rouge s rouge su summary generated different system considered system summary human generated summary model summary measure mead user summary r p f r p f r p f r p f rouge l rouge w rouge s rouge su getting high f score comparison mead microsoft system opinosis system rouge w mead s opinosis rouge w f score represented getting high precision compare mead opinosis microsoft system leading rouge l rouge w rouge s rouge su getting high recall comparison mead higher recall comparison opinosis microsoft measures opinosis getting higher system representing comparison different system generated summary length f measure comparison length summary representing f score mead system microsoft perform better term recall system performing better compared opinosis method gets higher precision compare opinsis rouge score p higher precision achieved compared mead getting low f score compares mead microsoft system higher w t opinosis f score summary f score summary experiment experiment showing significance sentiment feature purpose experiment sentiment score performs significant role salient sentence extraction generate good quality summary limited words like words tedious task experiment considered different features tried combinations features combination trying prove feature playing significant role summarization number features n feature total number combinations experiment trying combinations calling approaches generating summary single stand feature based summary summary sentiment score playing role generate approximate word summary evaluate summary taken generated summaries gold reference summary motivated duc task evaluating words summary let stands tf idf feature stands aggregate similarity score stands position based feature stands centroid based feature stands sentiment based score feature showing collective score tf idf aggregate similarity position based features different rouge score summary generated different approaches approach measures rouge l rouge w rouge s rouge su approach measures rouge l rouge w rouge s rouge su presenting different features combination find summary seeing conclude position based feature approach highlighted green performing best human reference summary summary extractive type extractive type summary available address evaluation contains sentences words model score position giving higher preference leading sentences widely considered position based score perform cases example scientific article need features clear taking sentiment feature d features getting improved summary rouge score means accurate summary conclusion experiment approaches sentiment feature added times getting improved summary adding sentiment feature example collective features adding sentiment feature getting improved results highlighted red color position based feature performing best approaches reason given depend position based feature need features approach e aggregate position add sentiment score approach e highlighted blue color performance reduced position based score preferred e position based feature dominating approach results obtained human summaries reference summary summary obtains different approaches consider system summary document available c s yadav al remove biases evaluate world summary use e initial word summary evaluate rouge w taken performed experiment duc dataset results shown found incorporating sentiment feature performance improved experiment performed stop words different rouge score shown duc dataset rouge location location sentiment location stop word removed location sentiment stopword removed rouge l rouge w rouge s rouge s experiment experiment finding best optimal feature weights improved results decide feature weights supervised random forest logistic regression internal estimates monitor error strength correlation evaluate variable importance experiment performed duc dataset showing correlation independent features visualizing correlation correlation features showing correlation feature summarization divided data training testing develop model find best feature weight combination regard developed models random forest logistic regression features weights suggested corresponding model currency table showing optimal feature weights different models model location tf idf centroid random forest aggregate similarity sentiment model accuracy logistics regression table logistic regression model sentiment score weight highest feature weight signifies importance sentiment highest logistic regression based model sentiment feature weight second highest models random forest logistic regression logistic regression suggesting feature weights corresponding location tf idf centroid aggregate similarity sentiment model accuracy testing represented corresponding equation table showing performance model p r f int p int r int f rouge l rouge w rouge s rouge su table showing performance o model p r f int p int r int f rouge l rouge w rouge s rouge su model simple linear model proposed tested previous sections self data set section implemented model duc dataset experiment implemented previous model new model feature weight learning regression analysis results attached table performance model decided based precision p recall r f score f confidence interval shows probability lying values p r f score given range figure shows comparative performance analysis based content based rouge measure tables figure proof significant change results model performance model rouge l rouge w rouge s su p model r model f model p model r model f model figure comparative performance model model concluding remark work presented hybrid model text document summarization based linear combination different statistical measures semantic measures hybrid approach considered statistical measures like sentence position centroid tf idf semantic approach sentiment analysis based word level analysis sentiment score sentence given sum sentiment score entity present sentence getting polarities entity neutral negative positive entity sentiment negative multiplying score treat positive score reason want select sentence strong sentiment present negative positive importance calculate score importance sentence added scores sentence pick highest scoring sentence step similarity summary sentences lower threshold maintain diversity added summary stopping criteria summary length constraints generate summaries different length methods like mead microsoft opinosis human generated summary evaluation rouge measure chapter experiments approach took proposed algorithm based generated summary system summary model summary experiment shown getting high precision time signifies covered relevant results second experiment compared different system generated summary mead microsoft opinosis algorithm model summary human generated find explained algorithm performed generated summary time mead system generates summary leading way getting higher recall comparatively mead experiment section shown adding sentiment score feature getting improved results compare sentiment score experiment showing sentiment score contribution extraction appropriate sentences fourth experiment divided data training testing proposed feature weighted approach random forest regression logistic regression model giving better accuracy logistic regression producing better model feature weights assigned regression model new experiment performed model improvement feature weight analysis chapter new latent semantic analysis entropy based approach automatic text document summarization introduction chapter proposing latent semantic analysis lsa based model text summarization work proposed models based approaches proposed model sentences extracted right singular matrix maintain diversity summary second proposed model based shannon entropy score latent concept second approach sentences approach extracted based highest entropy advantage models length dominating model giving better results low redundancy new models entropy based summary evaluation criteria proposed tested showing entropy based proposed model statistically closer s standard gold summary work dataset taken document understanding background section presenting introduction lsa advantages limitation lsa lsa summarization find information content principle information theory introduction latent semantic analysis text mining direction early application lsa started deerwester dumais furnas landauer harshman objective indexing text document applications automatic text document summarization objective convert given document d matrix representation matrix term document matrix elements aij document designed combining local global weight represents weighted term frequency ith term ith lsa vector space approach involves projection given matrix matrix amn usually represented reduced dimension r denoted ar m lsa input matrix decomposed showing mechanism lsa matrix decomposed u vt matrix u mn column orthonormal matrix columns called left singular vector n n nn diagonal covariance matrix diagonal elements non negative singular values sorted descending order vt right singular matrix orthonormal matrix size nn columns called right singular vector let r matrix matrix s properties express r r representing diagonal elements n methods decomposition factorization available like ulv low rank orthogonal semi discrete decomposition svd lsa based svd decomposition reason simultaneously svd decompose matrix orthogonal factors represent types documents vector representation types documents achieved second svd sufficiently capture adjusting representation types documents vector space choosing number dimensions computation svd manageable large data set interpretation applying svd term sentence matrix transformation point view svd derives mapping m dimensional space spanned weighted term frequency vectors r dimensional singular semantic point view svd derives latent semantic structure document represented matrix obvious reasons lsa limitations viewpoints vector space implementations like advantage textual units including documents word ability mapped concept space concept space cluster words documents easy find clusters coincide retrieve documents based words second concept space immensely fewer k dimensions compared designed vice versa original matrix m e lsa inherently global algorithm lsa usefully combined local algorithm useful limitations lsa considers frobenius norm gaussian distribution fit problems problem data follow poisson distribution e depends word counts studied s m katz lsa handle polysemy multiple meanings lsa depend singular value decomposition svd computationally expensive hard update case new documents required add lsa summarization lsa algebraic statistical technique svd given document understood independent concepts svd method models relationships words sentences beauty lsa find semantically similar words sentences capability noise reduction leads improved accuracy lsa based summarization presented comprises major steps input matrix creation svd sentence extraction selection briefly explained section note document sentence interchangeable sentences documents lsa based summarization procedure input matrix creation papers input matrix creation based bag word approach word count approach easy create widely accepted input matrix considered combination local weight lij global local weight criteria define weight range document sentence paragraph set paragraphs global weight decided documents let want use bag word approach common local weighting methods mentioned n number documents input matrix creation svd u s sentence selection extraction s local weight approach global weight approach binary term exists binary term document document term frequency lij tfij number occurrence term document j log tfij gf idf gfi total number times term occur number occurrence term collection number documents document j term occurs semantic matrix size location aij connect location afg semantic connection available words m j g n semantic connection examples synonyms anatomy polysemy hypernymy hyponym gfi total number times term occur collection tfij number occurrence term document j n total documents pij probability occurrence ith term jth document local global weight model decomposition lsa uses svd matrix decomposition input matrix m n number words number sentences svd decomposes matrix decomposed matrixes left singular u described words concepts m n size matrix n n size represents scaling values v matrix n n size sentences concepts sentence selection right singular matrix gives information sentences concepts want information sentence use v detailed selection method presented related work authors like gong liu s v sentence selection introduction entropy information entropy originally defined shannon study information contained transmitted message information theory entropy considered measure information missing transmitted information received information called shannon entropy definition information entropy express discrete set probabilities example case transmitted message probabilities p xi probabilities particular message xi actually transmitted entropy message system measure information present message let message m n symbols mn present message m transmitted source s destination d suppose source s transmits symbols mi n probabilities respectively symbol mi repeated t times probability occurrence mi given tpi t independent observations total information given assumed symbols emitted independently average information symbols t calculated case entropy function related work section mentioning previous different models comparison purpose models based lsa based decomposition model presented pros cons gongliu model model proposed gong liu authors vt matrix sentence selection according row vt representing topic concept corresponding topic selects sentence highest corresponding value topic added extracted sentences summary process repeated required length summary achieved observed following advantages drawbacks salient drawback new unique approach approach summary depended vt author convenient select rows e reduced dimensions vt maximum selected summary sentences equal reduced dimension r case reduced dimension r way suggested select sentences concepts represent information compared concepts approach extraction sentences belong important concepts let system choose ith concept sentence selection selected concept sentences high values like gong approach choose sentence know highly related second sentence respected approach vt selected concepts equal importance importance known concepts independent expected sentences extracted concepts independent ideally possible text data especially linking entities stopwords pronoun words present text simple words want diversity summary matrix noiseless data e removing stop words better output approach depends input matrix murray model model proposed murray renals carletta vt s matrix sentences selection instead selecting sentence vt depends highest index value selects number sentences based matrix s number sentence selection concept depends getting percentage related singular values sum values reduced space r observed following advantage drawbacks overcomes problem gong liu approach selects sentence concept starting values matrix play significant role dominate salient drawback sj model steinberger jeek proposed model document summarization v matrix transpose vt sentence selection add sentence summary author finds length sentence s v reduced space r given s matrix multiplied v matrix emphasis topics e property s sorted decreasing order topic multiplied higher value compare later occur topics observed following advantage drawbacks salient drawbacks sentence selection based new reduced space consideration preferred concepts highest length sentence long length sentences dominate calculating si explicit criteria increase diversity summary sentences related concepts preprocessing authors considered sentence unrelated sentences model model cross method proposed ozsoy alpaslan cicekli vt matrix preprocessed represents core sentences remove sentences index value mean vt matrix multiplied matrix importance topics shown calculatedces length calculated adding columns vt matrix e highest length sentence added summary process repeated required length summary observed following advantages drawbacks drawback diversity summary considered e explicit criteria length dominating summary e long length sentences extracted added summary model topic based approach proposed ozsoy et al preprocessing cross approach w t vt e vt representing core sentences topic reduces space r instead sentence length approach explained sj model found important concept based concepts sentence selection important concept found concept concept matrix matrix formed summing cell values common concepts observed following advantage drawbacks salient drawback finding main topic based concept concept matrix giving better results higher concept score showing concept related concepts following gong liu approach sentence selection e select sentence concept chosen concept k sentence selection concept sentences high values gong approach choosing sentence example case concept k related sentence si sentence know highly related respected sentences related concepts preprocessing author zero showing respective sentence concept unrelated said con n sign b showing preferred b property considered extracted sentences assumed implicitly diverse proposed model sentence selection vt matrix combination diagonal matrix vt matrix know concepts vt assumed extracted sentence minimum similarity matrix independent inherently singular values sorted order assumed gong liu concepti preferred approach followed steinberger jeek cross approach ozsoy al ozsoy cicekli alpaslan considering phenomenon sentence selection based longest length sentence given respectively approach topics concept highest strength chosen proposing different approaches based vector concept relatedness termed proposed second approach based entropy explored represented explaining model approaches taking example section representing document d creating matrix words sentences matrix applying svd decomposition represented adecomposed use different ways generate summary section taking document d creating matrix word frequency based matrix computing vt section proposed section proposed section contains proposed working lsa example section representing lsa based e sentences numbered corresponding word sentences matrix represented table example taken cc reason widely available example expresses property lsa reduced dimensions e representation given document term document matrix lsa dimensional space shown link graph representation let consider example given sentences neatest little guide stock market investing investing dummies edition stock market returns little book value investing value investing graham buffett investing real estate edition stock investing dummies investors miss little book common sense investing way guarantee fair share rich dad guide investing rich invest poor middle class rich dad advisors abc real estate investing secrets finding hidden profits considered document d equal u u si representing ith sentence considering underlined words book dads dummies estate guide investing market real rich stock value set words finally based presence presence words sentences input matrix word sentence matrix constructed given rows representing presence words different sentence frequency columns representing presence words example keyword selection arbitrary keywords selection follow approaches proposed beliga metrovi martini ipi sharan siddiqi singh s s s s s s s s s index words book dads dummies estate guide investing market real rich stock value titles frequency based words documents matrix svd u vt reduced space r given composition matrix u denotes left decomposed matrix v right decomposed matrix s matrix non negative diagonal matrix lsa decomposition matrix reduced dimension mincorrelation model model based vt right decomposed matrix explained gong liu approach author vt matrix sentence selected concept fix number decide k sentences extract concept selecting concept gong liu instead selecting sentence highly related concept select sentences way related second related concept e related means higher value corresponding entry concept sentence related means value corresponding entry concept sentence approach cover different topics sentences concept graphically procedure understood dimensional space vectors vector maintaining angular distance x axis corresponding x axis want select vectors choice closed x axis related corresponding x axis want select vectors choice interested selecting vectors selection set application reference documents total number vector n objective co relation similarity measure measure work co relation measured co occurrence matrix find vt lsa vector representation d space objective function denoted return set vector sentences minimal co related given example understood figure co relation considered angular distance vectors lsacs model lsa based concept selection lsacs based based entropy technique model find concepts contain information information contained concept decided based entropy detail information entropy presented section concepts hidden latent high entropy showing degree freedom means particular concept related number dimensions concepts sentences concepts hidden svd latent consideration consider dimensions x y z latent concept interested find dimension representative finding shandon entropy proposed shannon sentences represented triangle sign blue color words represented star sign red color underlined words example x y z axis assumed like hidden latent possible physical diagram represented arrow sign red color presenting proposed algorithm objective find latent axis x y z represents information u vt matrix d space representing words sentences input document d e content words sentences paragraphs predefined length summary initially similarity threshold output summary decompose given document d sentences sn extract keywords use keywords given document d construct matrix e term sentence matrix design ways form master sentence s tf idf approach perform svd sentence reduced space r right singular matrix vt compute w matrix columns w representing sentences rows representing concepts latent similar gong approach ii case similar approach preprocess w selecting core sentences corresponding concepts strikeout related row w compute representing probability sentences sentence j appear concept find information contains concept shannon s entropy method follow section select informative concept cp reduce space w matrix selected concept cp select sentence si related cp l lmax similarity si sj e sj summary sentences add sj summary l goto choose sentence representing preprocessing given document d step creations matrix approach followed step svd decomposition performed matrix svd decomposed matrix matrices right singular matrix represents information words left singular matrix informs sentences covariance matrix decide importance parameters e importance concepts sentences words cases design w based sub approaches given gong liu e steinberger jeek e model w row representing concepts column sentences demonstrating w formation covariance matrix vt example demonstration purpose e reduced dimension representing relatedness sentences corresponding reduced dimension r based negative sign eliminate entries e neglect sentences informative represented step finding probability relatedness sentences concepts w value representing e probability jth sentence ith concept concept sentences represented terms probability xi denotes ith concept shown finding information contained concept given representing freedom concept information concept choose highest informative concept selected concept sentences added summary based certain cosine similarity criteria matched given iteratively repeated length constraints satisfied sentences w reduced space sentences w processed retaining positive related sentences rowsum calculated find probability sentences total entropy w t sentences w t sentences w t sentences reduced space r information contained concept corresponding sentence proposed work interested find concept closely related sentences e relatedness considered term probability valueij indexed location ith concept jth sentence showing concepti related sentencej strength valueij higher value representing concept related particular sentence according proposed approach concept informative related number sentences high score example let concepti related sentencej probability information contain concepti given n representing refined sentences entropy given concept negative value contains information choose sentence selection sentences selected iterative way till required length summary achieved given example order sentences include summary resolve tie sentences concepts come serve fcfs approach maintain diversity set similarity threshold add sentence summary similarity user defined cosine similarity given experimenting found small summary contains diversity resulting low performance set carefully experiments chose interested find cosine similarity summary sentences newly selected sentences add summary k m representing length master sentence j ith jth sentence total n sentences lsass model lsa based sentence selection lsass based entropy based proposed model find sentences contain highest information sentence high entropy showing degree freedom means particular sentence related number dimensions concepts sentences sentences represented sign corresponding label rectangle diagrams words represented sign corresponding label rectangle diagram presenting proposed algorithm n input document d e content words sentences paragraphs predefined length summary initially similarity threshold output summary decompose given document d sentences sn extract keywords use keywords form master sentence s given document d construct matrix e term sentence matrix perform svd follow sentence reduced k preprocessed vt s compute matrix multiplication w vt s column w representing sentences row representing concepts preprocess w select core sentence strikeout negative related sentences column w compute representing probability sentence appear latent r r reduced space find information contains sentence shannon s entropy method follow select informative sentence sj m l lmax similarity si sj summary sentences sj sentence added summary add sj summary l goto goto concerned preprocessing svd decomposition step new matrix w obtained combining right decomposed matrix v covariance matrix w vt reducing dimension rows matrix w signify concepts document column signifies sentences document representing score sentences corresponding step matrix reduced eliminating rows containing negative values negative values signify informative sentences shown interested find probability concepts appear sentences calculating probability value denotes j representing probability concept appear sentence j shown sentence sum probability concepts xi denotes ith concept willing find information contained sentence corresponding concepts computed summing columns corresponding sentence resultant value signifies information contained sentences sentence extracted based information content added summary step continue similarity threshold criteria hold required summary length constraint satisfied sentencej related concepti probability information contained given sentences sentences w reduced space w processed keeping positive related sentences column sum find measure probability sentences w t w w entropy information contained sentence reduced space r write entropy sentence contains information highest entropy extracted added summary measuring similarity summary sentences selected sentences include summary required length summary achieved given example order sentences include summary resolve tie sentences come serve fcfs approach maintain diversity set similarity threshold add sentence summary similarity previous sentences sentence similarity user defined cosine similarity given formula proposed model measure redundancy summary text collection units words summary important characteristics like coverage redundancy summary length coverage redundancy reciprocal increase reduces lot research like alguliev aliguliyev isazade alguliev aliguliyev isazade alguliev et al happen direction obtain optimal solution multi objective optimization approach generating word summary words reference summary available file section calculating information based n gram content entropy information gram gram denotes redundant information text files measures experiments find reach conclusion consider consider gram dominate text file trivial presence experiment files find average information e redundancy dataset s total number documents gram s representing count n grams sth sentence experiment results section presenting wide number experiments following experiment implemented previously proposed models presented section compare performance experiment comparative performance shown proposed models highest efficiency w t rouge score given proposed ozsoy et al lowest ranked model gong liu models based lsa assumed concepts independent extracted sentences best representative document concepts independent number units words common interested coverage redundancy need update sentence selection approach experiment second section original r l r w r s r su rouge score previously proposed models second experiment diversity e redundancy proposed proposed section select k sentences concepts set select concept select sentences concept related unrelated w r t chosen concept latent approach extension follow section presented approach proposed approach performing better respective model w t w t performing better compared previous models results shown showing gain performance proposed approach rouge score r l r w r s r su performance proposed models proposed proposed showing improved performance proposed proposed experiment based entropy based approach second approach defined section section cases vt termed svt termed sole objective model find concept related sentences summary sentences extracted concept latent continuation experiment model termed objective model find sentences strongly related concepts decided based entropy entropy based model performance shown getting improved results w t previously proposed models exception case exception case generating evaluating word length summary long sentences representative information extract long sentences length dominating model performing better entropy based model proposed section section length dominating performance better compare proposed approaches playing better role compare based model user interested length summary true long sentence contains information repetition selection long sentences starts chance redundancy coverage performance negatively impact shown experiment want evergreen model existing choices better selection entropy based model summarization problem performance previous approaches incorporating entropy models shown graph rouge measure r l r w r s r su performance model entropy based model showing improved performance entropy based proposed experiment showing redundancy information contained different summaries generated different previous proposed models proposed approaches information represented words short followed showing increasing number words text rouge score increasing reduced increasing reducing simultaneously follow rouge score formula given shown pair bold red letter represented information terms entropy given information score represented proportional rouge score given obvious trivial case trying redundancy increasing text target summarizer system words l count cov generally increasing summary length rouge score increasing reducing experiment trying measure redundancy information given equation based n gram count entropy function explained section score means redundancy counting n grams gram reason experiment find gram hard reach conclusion following simple approach giving equal weight n grams rouge score entropy based system showing number words increasing rouge score decreasing statistical n large like n contain information respective count increase redundancy increase e coverage experiment find performing better follow contains redundancy low coverage shown model previously proposed models entropy based gold showing average information contains summary generated different systems model previous models proposed entropy based gold peer proposed summaries frequency gram gram gram gram gram gram gram gram gram gram gram gram average count average n gram presents different systems generated summary rouge score directly depends gram redundancy increases rouge score increases previously proposed models best performer redundancy e number n grams performing better greater peer gold summary n grams counts vs instead entropy based proposed method close standard summary closeness texts find golden gram refers n gram present gold summary system gram refers summary generated different systems results shown summarized representing rouge l score calculated different described systems models average count gram gram gram interpret different results like proposed system entropy performing better compare term rouge score system proposed ozsoy et al performing second place redundancy high compared allotherrsystemsm conclude statistically second entropy based proposed approach closer gold peer summary e term performing low redundancy low rouge l gram gram gram model score model score model score model score model score gram rank summarization m stands model concluding remark work proposed new approaches new models automatic text document summarization novel entropy based approach summary evaluation approaches summary generation based svd based decomposition approach right singular matrix vt processing selects concept till required previous approaches focused selecting sentence highest information content approach selecting sentences w t concept highest related concept related concept approach based assumption covering different topics result leads coverage diversity second approach based entropy formulate different models selecting highest informative concept concept selecting summary sentences repeatedly selecting highest informative sentences e sentence related concepts high score advantage entropy based model length dominating models giving better rouge score statistically closer gold summary experiment found rouge score depends count matched words increasing summary length rouge score decreases increasing redundancy rouge score increases pointed rouge score nt measure redundancy e count matched sentences realized need new measure summary evaluation provide tradeoff redundancy countmatch entropy based criteria proposed testing new proposed measure different summary generated previous models proposed models found entropy based summary closer standard summary experiment results clear model works summary evaluation especially higher length summary summary length increases redundancy increases measure measuring redundancy currently giving equal importance n gram theoretically practically weight higher n gram high redundancy information case repetition chapter lexnetwork based summarization study impact wsd techniques similarity threshold lexnetwork introduction chapter presenting lexical chain based method automatic text document summarization work divided objectives objective constructing lexical network nodes representing sentences edges drawn based lexical semantic relations sentences second objective constructing lexical network applying different centrality measures decide importance sentences sentences extracted added summary based measures study objective work based wsd wsd intermediate task text analysis presenting performance centrality measure changing change wsd technique intermediate step cosine similarity threshold post processing step related work morris hirst proposed logical description implementation lexical chain roget thesaurus barzilay elhadad developed text document summarizer lexical chain algorithm exponential time taking silber mccoy followed research barzilay elhadad lexical chains creation proposed linear time algorithm lexical chain creation according kulkarni apte concept lexical chains helps analyze document semantically concept correlation sentences lexical chain text summary generation stages step candidate word selection chain building noun verb second step lexical chain construction chain scoring model represent original document step chain selection chain extraction summary generation literature chain scoring strategy based tf idf distinct number words position text gurevych nahnsen proposed lexical chains construction candidate words selected based pos tagging nouns gonzlez fort wordnet eurowordnet lexical databases proposed algorithm lexical chain construction based global function optimization relaxation labeling authors categorized relations extra strong strong medium strong relations chains distinguished extracted based strong medium lightweight pourvali abadeh proposed algorithm single document summarization based different knowledge source wordnet wikipedia words present wordnet erekhinskaya moldovan different knowledge source wordnet wn extended wordnet xwn extended wordnet knowledge base xwn kb gonzlez fort wordnet eurowordnet lexical databases proposed algorithm lexical chain construction based global function optimization relaxation labeling authors categorized relations extra strong strong medium strong relations chains distinguished extracted based strong medium lightweight y chen wang guan chinese wordnet hownet kulkarni apte wordnet relations y chen liu wang chinese language resources like hownet tongyicicilin stokes lexical cohesion generate short summaries given number news articles different relations distinct weights assigned like extra strong relation repetition assigned strong relation synonym assigned strong relation hypernym hyponym meronym holonym antonym assigned medium strength relation assigned statistical relation assigned author identifies highest scoring noun proper noun chains relations works closely related work vechtomova karamuftuoglu robertson ercan cicekli author considered keywords sort version document summary proposed lexical chain keyword extraction steinberger poesio kabadjov jez anaphoric information linguistic task latent semantic analysis anaphoric task giving better results summarization purpose chandra shekhar yadav sharan c s yadav et al position tf idf centrality positive sentiment negative sentiment based semantic feature centrality j yeh j yeh ke yang meng static information like position positive negative keyword centrality resemblance title generate extractive summary lsa ga integrated work doran stokes carthy dunnion proposed lexical chain based summarization chain score calculated based frequency relationship present words chain member wordnet score word pairs depends sum frequencies words multiplied relationship score synonym relations assigned value specialization generalization proper nouns chain scores strength depends type match assigned exact match partial match fuzzy match sentences ranked according sum scores words sentence later extracted medelyan presented graph based approach computing lexical chains nodes document s terms edges reflecting semantic relations nodes based graph diameter given longest shortest distance different nodes graph concept strong cohesive weakly cohesive moderately cohesive chains computed plaza stevenson daz proposed summarization system biomedical domain represents documents like graph designed concepts relations present umls metathesaurus unified medical language system jdi algorithm personalized pagerank algorithm wsd y chen huang yeh lee represented document graph like structure node sentences edges drawn topical similarity random walk applied summary generation way xiong ji proposed novel hypergraph based vertex reinforced random walk gonzlez fort wordnet eurowordnet lexical databases proposed algorithm lexical chain construction based global function optimization relaxation labeling authors categorized relations extra strong strong medium strong relations chains distinguished extracted based strong medium lightweight plsa lda approaches proposed steinberger et al chiru rebedea ciotec study summarization result based lsa lda lexical chain important results lsa lda shown strongest correlation results lexical chain correlated lsa lda according performing semantic analysis different nlp applications lexical chains complementary lsa lda j yeh static information like position positive keyword negative keyword centrality resemblance title generate extractive summary lsa ga integrated work steinberger et al anaphoric information linguistic task latent semantic analysis showed anaphoric task giving better results summarization purpose background lexical network construction target work construction lexnetwork wsd intermediate task construction network applied different centrality measures score sentences later extract section presenting wsd centrality techniques lexalytics algorithm work word sense disambiguation wsd nature human language exists common problem languages word ambiguity e multiple sense word according context words occurs wsd word sense disambiguation intermediate task technique nlp applications computationally decide sense particular word active use particular sense erekhinskaya moldovan studied ambiguity word originates variety factors like approach representation word sense dependency knowledge source like wordnet roget thesaurus extended wordnet knowledge source knowledge internal external impossible humans machines find correct sense wordnet lists senses word pen pen writing implement point ink flows pen enclosure confining livestock playpen pen portable enclosure babies left play penitentiary pen correctional institution convicted major crimes pen female swan work variant lesk algorithm wsd proposed lesk lesk algorithm based presumption given words disambiguate sense neighborhood word tend share common topic words ambiguous sense relies knowledge source work different algorithms wsd lesk adapted lesk cosine lesk simple lesk algorithm simplified version lesk algorithm compare ordinary dictionary definition called gloss found traditional dictionary oxford advance learner ambiguous word terms contained neighborhood initialization words word vector disambiguate sense need select context window e neighbor word possible sense word disambiguate count words neighborhoods word dictionary definition sense sense considered best possible sense highest number overlapping counts advantage technique non syntactic dependent global information disadvantage use previous sense e word compute time consuming performance varies according selection neighboring words adapted lesk cosine lesk adapted lesk algorithm proposed banerjee pedersen simple lesk algorithm uses knowledge sources standard dictionary gloss definition adapted lesk algorithm place standard dictionary author electronic database wordnet wordnet provides rich hierarchy semantic relations cosine lesk proposed tan vector space distributional space version calculate lesk overlaps known signatures original lesk paper example let ambiguous word deposit different contexts sentence got bank deposit money financial institute withdraw transact money strip river soil deposit resident instead global vector distributional space built corpus locally given ambiguous word given meaning word deposit context sentence vocabulary financial institute deposit withdraw transact money strip river soil resides following vector assigned meaning meaning computed highest similarity closest meaning word deposit given context sentence cheaper compute need corpus fly wordnet storing vector space memory disambiguating text lack memory usage mean meaning vectors computed recomputed disambiguate wasteful computing resources time lexical chain lexical chain sequence related words writing spanning short long distances generally limited lines lexical chain independent grammatical structure text effect list words captures portion cohesive structure text applications lexical chains keyphrase extraction keyword extraction event detection document clustering text summarization explained following showing lexical chain created document available elhadad example rome capital city inhabitant example wikipedia resource web example lexical chain creation dataset available elhadad mechanism create given barzilay elhadad chains constructed words lost meaning text grammatical structure way find particular word belongs sentences sort issues proposed lexicalnetwork overcome problems lexalytics algorithms lexalytics offers text mining software companies enterprises globe available lexalytics system access java python api excel add ons text analytics tasks like named entity extraction finding meaning text classification tagging sentiment analysis context finding summary generation lexical chaining intermediate step mentioned applications lexalytics algorithms heavily rely like summarization lexical chaining nouns selected lexical chain construction nouns related find conceptual lexical chain given content unrelated sentences separate sentences score lexical chain based length chain relationships lexical semantic nouns system location based features considered deciding sentence priority initial sentence informative relation chain construction purpose word antonym synonym meronym hyper holonyms system sentences adjacent text lexically related associated summary generation extracts best sentences chain shifts nonessential sentences summary centrality degree centrality centrality scores graph theory social network analysis decide relative importance node existing approach centrality based measures approached mentioned simplest type centrality degree centrality w n venables team define graph e nodes stands number edges graph g degree centrality node v defined number edges incident node graph g directed undirected g di graph type degree defined degree degree given node v v v degree node v links approaching v degree v counts links node v directs nodes v v degree centrality vertex v given graph g defined general degree centrality normalized centrality n total number vertices graphs shown normalized degree deg showing normalized degree centrality undirected graph b eigen value centrality linear algebra eigenvector square matrix let define vector change direction associated linear transformation alternatively defined v vector eigenvector square matrix av scalar multiple v condition written following scalar known eigenvalue associated eigenvector v advanced version degree centrality eigen vector centrality degree centrality node based simple count number connections eigenvector centrality acknowledges connections equal words eigenvector centrality defined way accords vertex centrality depends number connection e degree node quality connections social life followed connections people influential lend person influence compare connections influential persons denote centrality ith vertex xi allow effect making xi proportional average centralities ith network neighbors newman represented c closeness centrality graph theory closeness centrality measures defined vertex graph g v e vertices shallow short geodesic distances vertices given higher closeness value closeness defined way defined freeman closeness centrality vertex defined inverse average length shortest paths vertices graph given d alpha centrality denote adjacency matrix aij equivalent aij means contributes j s status vector centrality scores written alpha centrality status score individual seen function status choose given upper rewrite denoting transpose matrix short alpha centrality s status linear function node node connected understood examples community power study actor s status value increased nominated receiving nominations compare upper example school student s popularity increased receiving voted students popular w t students solution problem solution eigenvector centrality produce justifiable results networks given ax ax graph showing graph nodes edges possible solution kind problem possible allow individual students score nt depend connection like class school student s popularity depends external status characteristics let e vector exogenous sources status information replace new parameter reflects relative importance endogenous versus exogenous factors determination centrality matrix solution s value position labeled vertex central order xa xb xc xd xe alpha centrality based measure identical measure proposed l katz katz suggested influence measure weighted sum powers adjacency matrix powers gives indirect paths connecting points e betweenness centrality graph e betweenness centrality given freeman measure vertex based shortest path metric vertices v v occur shortest paths tx vertices higher betweenness centrality compare nt graph g vertices betweenness vertex computed denotes counting shortest paths node s node t total number ofpassedst paths node s node t passing vertex v f bonacich s power centrality bonpow let given matrix adjacency matrix bonacich s power centrality proposed bonacich defined attenuation parameter reciprocal largest eigenvalue adjacency matrix constant multiple familiar eigenvector centrality score values behavior measure different particular gives positive negative weight odd walks respectively seen series expansion converges long holds w n venables team magnitude controls influence distant actors ego s centrality score larger magnitudes indicating slower rates decay high rates imply greater sensitivity edge effects interpretively bonacich power measures correspond notion power vertex recursively defined sum power alters power exponent controls nature recursion involved positive values imply vertices powerful alters powerful occurs cooperative relations negative values imply vertices powerful alters weaker occurs competitive antagonistic relations magnitude exponent indicates tendency effect decay long walks e higher magnitudes imply slower decay interesting feature measure relative instability changes exponent magnitude particularly negative case g hub authority hyperlink induced topic search known hubs authorities developed kleinberg type link analysis algorithm rates web pages set web pages consider connected graph algorithm assigns different scores hub authority score pages authority score estimates value content particular page node graph hub score estimates value links edge graph pages words interpret good hub represents web page points pages good authority represent page linked different hubs hits algorithm relies iterative method converges stationary solution node graph assigned non negative scores authority score xi let hub score yi let xi yi initialized arbitrary nonzero value scores update according iterative ways present later weights normalized h subgraph centrality let g v e graph subgraph e hold v v e e estrada rodriguez velazquez centrality measure based participation node subgraphs network work smaller sub graph gives weight compared larger ones makes measure appropriate characterizing network motifs define designates patterns occur network far random networks degree sequence freeman degree centrality considered like direct influence unable cover long term relations word indirect influence network l katz centrality based measures like betweenness centrality closeness centrality measures justifiable connected network path distance unconnected nodes defined eigenvector centrality ec nt depend paths ec measure simulates mechanism node affects neighbors simultaneously bonacich eigenvalue based centrality ec measure consider measure centrality nodes ranked according participation different network subgraphs let regular graphs vertices vertices set forming triangle squares form rest form groups distinguishable according participation different subgraphs ec distinguish showing node degree regular graph subgraph based centrality based number closed walks starting ending node closed walks weighted influence centrality decreases order walk increases technique closed walk associated connected subgraph points measure counts times node takes differently connected subgraphs network smaller subgraphs higher importance corresponding larger subgraph centrality represented graph angles noticed estrada velazquez given according graph walk regular node identical subgraph denote nodes graph g n let m m distinct eigenvalues given adjacency matrix multiplicities respectively basis eigenspace corresponding eigenvalue m defines angle alpha corresponding node u g eigenspace according gleich features rage rank algorithm simplicity guaranteed existence generality uniqueness fast computation technique applicable far origins google web search proposed page brin motwani winograd pagerank network centrality measure proposed koschtzki et al work motivated work assume given page pages tn pointing page called citations parameter d damping factor set usually set defined number links going page pagerank score page given centrality pagerank variation given pagerank technique forms probability distribution web pages graph sum web pages pagerank score proposed work section presenting layout step process including preprocessing lexical network creation sentence raking e computing importance sentence summary generation based required length step preprocessing step required proper sentence read set arbitrary delimiter regular expression like dot followed new line new tab better accuracy transform special form like u s u s deleted bank space form second step takes input output step reads sentences creates lexical network sentences detail presented algorithm section procedure procedure procedure steps summarization step applying different graph based centrality measures finding informative sentences step set constraints like threshold summary length summary generation according user request algorithm section presenting algorithm summary generation main steps basically previous figure section step step merged step algorithm algorithm lexical network construction section presenting summarization algorithm proposed work comprises steps step preprocessing important step text mining improve harm efficiency algorithm step second creating lexical network wsd help lesk algorithm step sentence scoring sentence extraction input collection sentences output extracted sentences procedure preprocessing proper fragmentation sentences set proper delimiter tagging sentences pos tagger procedure document given sentences apply regular expression proper bifurcation sentences proper possible words like mr dr u s proper sentence extraction preprocessing tagging sentences nltk natural language tool kit parser result step stored list format sentence variable randomly accessed e like array procedure lex network creation step initialize matrix step ith actual sentence extract nouns verb called modified sentences contains significant units ith modified sentences locating corresponding sentence takeith unit word ith unit correct sense word ithactual sentence modified sentences takejth unit j relation correct sense jth unit procedure sentence scoring extraction paired sentence centrality score sort paired sentences based centrality score set similarity measure threshold summary extract ith sentence pool step diversity maintained pick jth sentence summary pool ith add ith sentence summary procedure procedure constructing lexical network nn matrix n sentences e picking sentence extracting nouns like proper noun verbs sentence provide information added modified sentence example duc healthcare data set file number txt actualsentence administration task force studying possible health care solutions offer comprehensive proposal taggedsentence administration nn task nn force nn studying vbg possible jj health nn care nn solutions nns offer vb comprehensive jj proposal nn modifiedsentence administration task force studying health care solutions offer proposal actual lexical network construed modifiedsentences finding relation lexical sematic present modifiedsentence find relation present sentences need disambiguate sense word help lesk algorithm lesk algorithm takes arguments word disambiguate sense ith actual sentence based sense algorithm finds hyper hypo synonyms meronym antonym relation present remaining sentences time relation available according relation given weight assign edge sentences time increase relation present sentences showing lexical network connection sentences txt duc dataset sentence lexical semantic similarity available corresponding entries zero vice versa corresponding showing graphical view lexical network node sentences edges corresponding total similarity note split text delimiter dl total number delimiter n split operation number segmentation appear showing sentences txt duc dataset creation network washington health human services secretary louis sullivan called summit chief executives major insurance companies discuss ways paring administrative costs health care speech dr sullivan indicated bush administration likely forth broad legislative proposal overhaul country health advocated focusing ways improve current system administration health officials said meeting insurers tentatively scheduled nov likely commence series discussions players nation billion health care system problems cost access administrative costs like excessive paper work burdened health care system billions dollars unnecessary costs observers care system believe studies price billion year hhs officials believe like billion billion numerous health care reform proposals introduced congress year democrats comments dr sullivan hhs officials yesterday suggest bush administration interested striving systematic revision push limited fixes incremental changes propose grand sweeping speculative scheme dr sullivan said want public debate focus immediate practical options address urgent healthcare concerns options said making health insurance affordable small businesses easing barriers high quality cost effective managed coordinated care researching effectiveness medical procedures encourage use cost effective ones altering tax code increase consumer awareness true cost health care distribute current tax subsidies equally things administration officials looking possibility limiting tax exemption employer paid health insurance premiums increasing availability primary care neediest people reducing administrative costs health care u s spends capita health care country million americans lack health insurance january president bush ordered administration task force study problems health access propose solutions dr sullivan repeated dislike widely discussed health care revision proposals establishing nationwide federally sponsored health care system similar canada mandating employers provide basic health benefits workers pay tax finance public health plan approaches said inflationary smother competition lead rationing waiting lists said experimentation health care reform left states local solutions local problems working philosophy learning local mistakes order avoid harm nation said showing lexical semantic relations present sentences sent graphical form directed graph refers sentence corresponding weighted edges sentences shown procedure getting lexical network applying centrality based measures sentence scoring sentence high centrality score decided important compared low centrality score extract sentence summary generation decide cosine similarity threshold add sentence summary x x summarysentence cosine similarity x work extended impact different wsd techniques different similarity thresholds post processing step find similarity sentences cosine similarity vectors b length n cosine similarity given ai bi component vector b similarity measure c s yadav al chandra shekhar yadav sharan graph theory network analysis decide relative importance node find central figures existing approach centrality based measures example social network decide importance person help centrality based measures degree centrality eigenvector centrality katz centrality pagerank betweenness centrality closeness centrality alpha centrality bonpow centrality hub authority subgraph centrality detailed description different centrality measure experiments results section presenting different experiments performed result analysis presenting performance different centrality measure selected lesk algorithm different threshold impact wsd technique threshold different centrality measures performance different centrality measure selected lesk algorithm different threshold section showing performance different centrality based measures fixed wsd algorithm like adapted lesk cosine lesk simple lesk respectively different cosine similarity threshold post processing selected cosine similarity threshold range limitation space sufficient need interested result threshold adapted lesk th adapted lesk algorithm wsd lexical network creation step set cosine similarity threshold lexical network construction procedure algorithm applied different centrality measure like alpha centrality coded different values corresponding graph interpret alpha centrality performance continuously increasing performance summarizer system strangely reduced eigen value hub authority performing equal highest performance second highest performance subgraph based centrality performance different centrality measures adapted lesk wsd similarity threshold score bonpow close eigen page sub ness ness value authority rank graph rouge l rouge w rouge s rouge su performance evaluation rouge score different centrality measures adapted lesk wsd similarity threshold adapted lesk th adapted lesk algorithm wsd lexical network creation step set similarity threshold lexical network construction procedure algorithm applied different centrality measure like alpha centrality coded different values corresponding graph interpret alpha centrality performance continuously increasing exception performance summarizer system reduced compared eigen value hub authority performing equally second highest performance subgraph based centrality achieves highest performance performance different centrality measures adapted lesk wsd similarity threshold score bon close eigen page sub ness pow ness value authority rank graph rouge l rouge w rouge s rouge su performance evaluation rouge score different centrality measures adapted lesk wsd similarity threshold cosine lesk th cosine lesk algorithm wsd lexical network creation step set cosine similarity threshold lexical network construction procedure algorithm applied different centrality measure like alpha centrality coded different values corresponding graph interpret alpha centrality performance continuously increasing exception performance summarizer system instantly reduced eigen value hub authority performing equal highest page rank second highest subgraph based centrality performance thing notice measures performing performance different centrality measures cosine lesk wsd similarity threshold score bonpow close eigen page sub ness ness value authority rank graph rouge l rouge w rouge s rouge su performance evaluation rouge score different centrality measures cosine lesk wsd similarity threshold cosine lesk th cosine lesk algorithm wsd lexical network creation step set cosine similarity threshold lexical network construction procedure algorithm applied different centrality measure like alpha centrality coded different values corresponding graph interpret alpha centrality performance continuously increasing exception performance summarizer system strangely reduced eigen value hub authority performing equal highest performance second highest performance measured subgraph based centrality performance different centrality measures cosine lesk wsd similarity threshold score bonpow close eigen page sub ness ness value authority rank graph rouge l rouge w rouge s rouge su performance evaluation rouge score different centrality measures cosine lesk wsd similarity threshold simple lesk th simple lesk algorithm wsd lexical network creation step set cosine similarity threshold lexical network construction procedure algorithm applied different centrality measure like alpha centrality coded different values corresponding graph interpret alpha centrality performance continuously increasing exception performance summarizer system strangely reduced eigen value hub authority performing equal highest performance pagerank gives second highest performance subgraph based centrality measures highest performance performance different centrality measures simple lesk wsd similarity threshold score bon close eigen page sub ness pow ness value authority rank graph rouge l rouge w rouge s rouge su performance evaluation rouge score different centrality measures simple lesk wsd similarity threshold simple lesk th simple lesk algorithm wsd lexical network creation step set cosine similarity threshold lexical network construction procedure algorithm applied different centrality measure like alpha centrality coded different values corresponding graph interpret alpha centrality performance continuously increasing exception performance summarizer system strangely reduced eigen value hub authority performing equal highest performance pagerank measures second highest performance subgraph based centrality measures highest performance performance different centrality measures simple lesk wsd similarity threshold score bon close eigen page sub ness pow ness value authority rank graph rouge l rouge w rouge s rouge su performance evaluation rouge score different centrality measures simple lesk wsd similarity threshold impact wsd technique threshold different centrality measures section presenting different wsd technique different threshold impacts summarization subgraph based centrality code stands adapted lesk technique cosine lesk technique simplified lesk algorithm threshold means cosine similarity threshold means cosine similarity threshold subgraph based presenting performance subgraph based centrality different conditions subgraph based centrality emphasis small subgraphs covers long term relations presenting subgraph based centrality measure different wsd techniques thresholds conclude simple wsd technique adapted lesk cosine lesk threshold giving better results followed simple lesk adapted lesk cosine lesk threshold impact wsd technique similarity threshold subgraph based centrality score rouge l rouge w rouge s rouge su impact wsd technique similarity threshold subgraph based centrality eigenvalue centrality based analysis eigen value centrality depends numbers connections quality connection conclude simple wsd technique threshold cosine lesk adapted lesk simple lesk threshold cosine adapted simple lesk performing order higher lower impact wsd technique similarity threshold eigen value based centrality score rouge l rouge w rouge s rouge su impact wsd technique similarity threshold eigen value based centrality page rank page rank simple ensure guaranteed existence solution fast computation forms probability distribution network experiment set dumping factor conclude simple wsd technique threshold simple cosine lesk adapted lesk threshold adapted lesk simple lesk cosine performing order highest lowest impact wsd technique similarity threshold page rank centrality measure score rouge l rouge w rouge s rouge su impact wsd technique similarity threshold page rank centrality measure bonpow bonpow centrality measure importance node decided alters conclude simple wsd technique threshold adapted lesk cosine lesk simple lesk threshold adapted lesk simple lesk cosine performing order highest lowest impact wsd technique similarity threshold bonpow based centrality score rouge l rouge w rouge s rouge su impact wsd technique similarity threshold bonpow based centrality betweenness betweenness centrality measures importance node based factor participation node shortest paths nodes network conclude adapted lesk algorithm threshold performing better adapted lesk algorithm threshold cosine lesk simple lesk cosine lesk simple lesk threshold impact wsd technique similarity threshold ness centrality score rouge l rouge w rouge s rouge su impact wsd technique similarity threshold ness centrality closeness closeness centrality measure importance node based shortest path node nodes conclude threshold cosine lesk simple lesk adapted lesk threshold cosine lesk simple lesk adapted lesk performing order highest lowest impact wsd technique similarity threshold closeness centrality score rouge l rouge w rouge s rouge su impact wsd technique similarity threshold closeness centrality alpha alpha centrality score depends importance neighboring nodes shown different rouge score alpha centrality ranking sentences extraction summarization clear alpha including unpredicted pattern achieved alpha including rouge score increasing exception alpha efficiency reduced note adapted lesk cosine lesk simplified lesk algorithm means cosine similarity threshold means threshold impact wsd technique similarity threshold alpha centrality alpha comparisons lexalytics algorithms semantrica lexalytics system available lexalytics access java python api different tasks like summarization sentiment entity summarization task system depends lexical chain creation lexical chain created semantic lexical relations location gives priority system section describing system proposed lead semantrica performing better times times proposed methods lead shown comparison semantrica lexalytics algorithm difference centrality based measure adapted lesk wsd showing improved system w t semantrica adapted similarity threshold adapted similarity threshold lexalytics eigen value authority page rank subgraph authority page rank subgraph comparison semantrica lexalytics algorithm difference centrality based measure cosine lesk wsd showing improved system w t semantrica score cosine lesk wsd similarity threshold cosine lesk wsd similarity threshold lexalytics eigen value authority page rank subgraph eigenvalue authority page rank subgraph score l w rouge s su l w rouge s su comparison semantrica lexalytics algorithm difference centrality based measure simple lesk wsd showing improved system w t semantrica subgrap h authorit page ran subgrap h simple lesk wsd similarity simple lesk wsd similarity score sematrica lexalytics rouge rouge rouge rouge s rouge authorit y page ran eige n valu e eige n valu e y semantrica lexalytics vs proposed system improved overall performance clear alpha unable decide performance pattern performance increasing exception reduced time subgraph based centrality giving better result reason subgraph measure gives importance subgraph graph smaller subgraph given importance natural large graph subgraphs cycles showing performances wsd techniques different threshold subgraph based centrality performing better look hard interpret x axis clarification x axis different centrality based measure shown stands adapted lesk algorithm cosine lesk simple lesk algorithm shown stands cosine similarity threshold respectively example entry x axis eigenvalue means adapted lesk algorithm wsd cosine similarity threshold set eigenvalue centrality measure highest performance achieved subgraph interpret simplified lesk algorithm similarity threshold subgraph based centrality measured showing overall performance system comparative study presenting performance ranking based centrality measure like subgraph eigen value total different wsd different thresholds centrality measures threshold simple lesk times adapted lesk times cosine lesk time similarly threshold simple lesk times adapted lesk times cosine lesk times performing best w t centrality measure row presented alpha centrality s measures total measures different centrality measures different wsd technique different thresholds result nt particular type wsd better time threshold better compare threshold subgraph based centrality better vs measures performance centrality based measures wsd technique similarity threshold centrality measure wsd lesk threshold reference performance ranking technique simple adapted cosine simple adapted cosine simple cosine adapted cosine adapted simple simple cosine adapted adapted simple cosine adapted cosine simple adapted simple cosine adapted adapted cosine subgraph eigenvalue hub authority page rank bonpow ness closeness alpha best performances methods simple cosine simple cosine simple adapted cosine simple adapted adapted adapted simple cosine simple simple simple simple cosine simple cosine cosine adapted adapted adapted cosine adapted simple adapted adapted cosine concluding remark chapter presented lexical network concept automatic text document summarization little bit different previously proposed lexical chain based techniques previous techniques author concentrate create number lexical chains creates ambiguity chain prefer problem efficiently handle chain scoring techniques lexical chains problem particular word let donald trump coming sentences sentence prefer problem technique consider nearby sentences chain construction e window sentences selected unable handle long term relationship sentences lex net handle long term relationship sentences nodes scored higher score sentence gave priority problems handled way lexical network based number lexical semantic relations decide importance sentences applied centrality based measure lexical network found subgraph based centrality performing best human language highly ambiguous english need find correct sense particular word given context solution ambiguity problem simplified lesk cosine lesk adapted lesk algorithm work study impact wsd techniques cosine similarity threshold space limitation showing results value represents diversity compared high valued diversity good summary maintain diversity relatedness sentences harming good summary property comparison purpose semantrica lexalytics algorithm found system proposed working better times especially subgraph based centrality work reached number conclusions like alpha centrality alpha inclusion performance summarizer system arbitrary inclusion centrality measures different value threshold performance continuously increasing time exception reduced second set cosine similarity suggesting similarity threshold better diversity better summary rouge score hub authority based ranking eigenvalue based centrality fourth subgraph based centrality measure performing better reason higher score small subgraph recognizes small subgraph cover subgraph considered cluster like structure fifth suggesting particular wsd better time chapter modeling automatic text document summarization multi objective optimization introduction text document summary achieved features examples location tf idf length summary relatedness redundancy based scores stand features generate good quality summary hybridized better summary care multiple features solutions possible time taking try optimize functions work presenting multi objective function better summary optimization function maximizes diversity minimizes redundancy constraints defined terms summary length objective function constraints linear ilp integer linear programming find optimized solution e section presenting related work categorized according research area section mentioning mdr model baseline model comparison sentences extract summary literature work proposed mcdonald related work alguliev aliguliyev mehdiyev proposed multiobjective based approach multi document summarization model based reducing redundant information formalize sentence extraction based multi document summarization optimization problem formulation model depends aspects redundancy content coverage summary length shown si ith sentence o mean vector collection d sn sim cosine similarity xij binary variable pair sentences sj selected included summary zero l required length summary li length ith sentence tolerance represented sign model denominator evaluating correlation sentences sj numerator provides coverage main content document collection denominator reduces redundancy summary model sentence sentence sentence document relations alguliev aliguliyev hajirahimova mehdiyev proposed multi document summarization integer linear programming state model presented suitable single multi document summarization system optimizes property redundancy relevance length similarity function based similarity better calculation author calculate si feature vector removing stop words combined objective function fcos fngt stands cosine similarity measure ngt normalized google distance based similarity measure objective function model represented representing objective maximization similarity drawn cosine ngt based summary s document d length constraints representing combined function maximize similarity summary documents minimum summary sentences length constraints represents function f optimize alpha decided user preference f maximized combination cosine ngt based function xij binary variable optimization function w t constraint elaborated following equations ijij sim work alguliev aliguliyev hajirahimova optimized multi document summarization issues aspect coverage redundancy formulation coverage represented redundancy represented representing combined function let xi binary variable xi si selected representing length summary length constraints l summary length li si sentence s length weight w decided based importance coverage redundancy alguliev al proposed constraint driven document summarization constraints defined term diversity summarization sufficient coverage model formulated quadratic integer programming qip problem solve problem discrete pso diversity constraint defined equations covers relation sentence document content coverage defined shows sentence sentence relations si ith sentence o mean vector collection d sn sim cosine similarity xij binary variable pair sentences sj selected included summary l required length summary li length ith sentence diver specifies high diversity summary parameter cont control content coverage summary alguliev aliguliyev isazade proposed modified median problem multi document summarization approach expresses sentence sentence summary document summary subtopics relationships cover aspects summarization relevance content coverage diversity length jointly optimizing aspects formally assumption considering vertices graph potential medians p median defined subset vertices p cardinality sj ith sentence dij distance vertices j xij yi binary variables o representing center collection documents s summary formulation objective find binary assignment x high relevancy best content coverage high diversity w t summary length l p median problem expressed equations model summarization formulated ijijdiversimssxxxii s selected baseline mdr model compare results mcdonald work author multi objective optimization technique optimize criteria ilp objective criteria taken given following simplicity represent document collection simply set textual units documents collection e d tn ti d iff ti dj d let s d set textual units constituting summary representing length constraints sure selection binary variables xi band yij implementation determine relevance pagerank subgraph based centrality objective function function relevance redundancy describe follows relevance summaries contain informative textual units relevant user redundancy summaries contain multiple textual units convey n information length lmax summaries bounded length similarity sentences calculated cosine similarity n total number sentence importance si decide importance ith sentence higher value given higher importance li represent length ith sentences lmax required max length summary xi binary variable indicates corresponding sentence selected summary way yi j binary variables indicating sj included summary background section presenting intermediate techniques followed work handling text easy output depends text handled preprocessing processing preprocessing required lex network lexical network creation preprocessing task remove stop words convert lower remove punctuation remove number remove whitespaces stemming lemmatization wsd word sense disambiguation presenting detailed wsd techniques centrality measures explained previous chapter introduction linear programming explained linear programming linear programming lp called linear optimization method achieve best outcome minimum cost maximum profit mathematical model requirements represented linear relationships linear programming contains parts decision variable optimization function maximize minimize constraints integer programming ip problem mathematical optimization feasibility program variables restricted integers words ilp given objective function constraints integer constraints linear integer programming np hard method solve ilp broadly categorized parts exact algorithms guarantee find optimal solution time complexity exponential examples cutting planes branch bound dynamic programming heuristic algorithms provide suboptimal solution guarantee quality running time guaranteed polynomial provide fast solution approximation algorithms provide solution polynomial time suboptimal solution bound degree optimality let example given want maximize given z subject constraints maximize z y graphical method solving equation w t solution shown point graphical method solution w t optimization function represented solve process follow limitation ilp want solve function given constraints need expand constraints proposed multi objective optimization model section presenting basic outline model modeling based chapter e based lexical chain network outline proposed model figure diagram represented brief introduction figure shown output chapter lexical network input work lexical network creation finding centrality score sentence fetched centrality sore decide importance sentence edged present lexicalnetwork find relatedness want maximize information coverage minimization redundancy input sentences preprocessesing lexical network construction lexical network centrality multi objective optimization criteria sentences selection summary document figure outline summarizer system optimization function given constraints given equation objective function contains parts minus sign guaranteed maximize highest informative sentences second minus sign care redundant sentences eigenvalcentrality si representing importance ith sentence decided centrality measure sj represents weight edge sentence si sj lexicalnetwork xi xj yij binary variables decide sentences summary detail description proposed model proposed work divided modules outline framework shown comprised sentences selection tagging creation set sentences stop word removed e later lexical network creation creating lexical network computing centrality score degree centrality betweenness centrality subgraph centrality performing better past outcome step later summary generation module process automatic text summarization ijiijjstyxyx module text document sentences properly readable system apply regular expression proper handling sentences words tokens like mr dr u s care properly sentence extraction preprocessing tagging sentences nltk natural language tool kit parser explained example sentence identification tagging significant units e verb nouns considered lexical network construction represented modifiedsentences shown module constructing lexical network nn matrix n number sentences e picking sentence extracting nouns verbs sentence provides information added module modified sentence administration task force studying possible health care solutions offer comprehensive proposal actualsentence sentences administration nn task nn force nn studying vbg possible jj health nn care nn solutions nns offer vb comprehensive jj proposal nn administration task force studying health care solutions offer proposal showing procedure stepwise example duc healthcare data set file number txt actual lexical network construed modifiedsentences finding relation lexical semantic present tagged sentence modifiedsentence modifiedsentence find relation present sentences need disambiguate sense word help lesk algorithm lesk algorithm takes argument word disambiguate sense ith actual sentence based sense algorithm finds hyper hypo synonyms meronym antonym relation present remaining sentences time relation available according relation treated importance assign edge sentence time increase relation present sentences getting lexical network represented given sentences table applying centrality based measures sentence scoring sentence high centrality score decided important compared low centrality score output module lexical network centrality score given input module module module getting sentences lexicalnetwork centrality score going optimize function given objective function divided parts section optimization function maximizing coverage maximization coverage linked maximum informative sentences second section sj trying reduce redundancy selected sentences summary equation n total number sentences xi xj yij binary variables xi equals ith sentences selected added summary yij equals donate sentences ith jth present summary equal j objecive criteria represented guarantee add new sentence optimality condition experiments results section presenting different experiments results coverage importance sentences decided proposed centrality based measures importance decided cosine based measure experiment maximizing baseline objective function section proposed model b section objective function relevance decided centrality measures baseline method redundancy decided cosine measure proposed model lexical network considered minimization redundancy tables presented precision p recall r f score f confidence interval example confidence interval confidence significance interpreted like chance actual value unstandardized coefficient cosine based criteria minimize redundancy experiment baseline model trying maximize centrality score minimize cosine similarity given length constraints combined optimization function represented precision recall f score presented confidence paper mcdonald clearly mentioned decide importance sentences implementation subgraph pagerank based centrality measure precision recall f score model mdr model subgraph centrality rouge r cosine p cosine f cosine confidence r confidence p confidence f cosine cosine cosine rouge l rouge rouge s rouge su precision recall f score model mdr model pagerank centrality rouge r cosine p cosine f cosine confidence r confidence p confidence f cosine cosine cosine rouge l rouge rouge s rouge su clear getting improved results b lexical based criteria reduce redundancy experiment proposed model trying maximize relevance score found degree centrality measures minimize lexical similarity getting lexical network w t given constraints combined optimization function represented precision recall f score presented confidence precision recall f score proposed model centrality based measure subgraph centrality rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical rouge l rouge rouge s rouge su precision recall f score proposed model centrality based measure betweenness centrality rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical rouge l rouge rouge s rouge su precision recall f score proposed model centrality based measure pagerank rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical rouge l rouge rouge s rouge su precision recall f score proposed model centrality based measure evencentrality rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical rouge l rouge rouge s rouge su precision recall f score proposed model centrality based measure rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical rouge l rouge rouge s rouge su precision recall f score proposed model centrality based measure closeness rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical rouge l rouge rouge s rouge su precision recall f score proposed model centrality based measure bonpow rouge r lexical p lexical confidence lexical r lexical p lexical f lexical rouge l rouge rouge s rouge su figure comparative performance proposed mdr baseline model table figure proposed model subgraph based centrality measure relevance lexical graph redundancy getting best mode performance second best model mdr based subgraph based centrality measure comparison w mdr based page rank model proposed model s performance improved experiment finding correlation different centrality based measure significance experiment highlights relation centrality measures different centrality measure returns different summary sentences summary summary analysis e kind sentences return different measures experiment gives glimpse summary summary analysis co relation analysis different centrality based proposed model optimization function described changing centrality getting different results finding correlation methods statistics correlation coefficients measure relationship given variables strong poor value return indicates positive correlation indicates negative correlation indicates correlation better relation exists variables study pearson s correlation spearman s correlation kendall s co relation co relation shown following examples available goo gl calculating average correlation r different scatter plots showing different directions strengths correlation pearson correlation known ppmc pearson product moment correlation shows linear relationship sets data ppmc answers possible draw line graph represent data potential problem ppmc able differentiate dependent independent variables expressed y denote classes observation n cardinality showing pairwise pearson s correlation coefficient symmetric authority bonpow closeness evencen hub pagerank subgraph authority bonpow closeness evencen hub pagerank subgraph spearman coefficient goo gl avljmp spearman correlation determines strength direction monotonic relationship variables strength direction linear relationship variables pearson correlation determines represent denotes mean y xi yi ith observation showing pairwise spearman s correlation coefficient symmetric authority bonpow closeness evencen hub pagerank subgraph authority bonpow closeness evencen hub pagerank subgraph kendall advantage kendall test provides relationship variables provides distribution free test independence spearman rank correlation satisfactory testing null hypothesis independence variables difficult interpret null hypothesis rejected kendall rank correlation improves reflecting strength dependence variables compared kendall s rank correlation coefficient depends concordant discordant concordant nc defined yi xi yi pair discordant nd n total number observations nc nd counting concordant discordant showing pairwise kendall s correlation coefficient symmetric authority bonpow closeness evencen hub pagerank subgraph authority bonpow closeness evencen hub pagerank subgraph interpret different measures find importance sentence rank find following pair highly correlated authority evencen authority hub authority pagerank authority subgraph closeness evencen hub evencent pagerank evencen subgraph hub pagerank hub subgraph pagerank subgraph pairs highly related combination better results case use combination experiment hybridizing different centrality measure maximize relevance sentences minimization redundancy lexical network different measures total hybridize combinations possible limited time constraints check combination possible decide better combination considering experiment combine measures minimum correlated highly correlated return results simple combining centrality measures experiment conclude best performer subgraph based centrality requirement better combination select subgraph bonpow subgraph based measures precision recall f score proposed model relevance decided hybridizing subgraph bonpow rouge r lexical p lexical f lexical confidence r lexical p lexical f lexical rouge l rouge rouge s rouge su precision recall f score proposed model relevance decided hybridizing subgraph betweenness confidence rouge r lexical p lexical f lexical r lexical p lexical f lexical rouge l rouge rouge s rouge su table presented precision recall f score hybridized feature figure representing comparative performance hybridize centrality w t independent centrality measure conclude hybridize relevance measure returning better s su rouge l rouge highest performance derived subgraph based centrality figure graph showing comparative performance different relevance based measure sign denoting hybridization different features concluding remark chapter proposed optimization based summarization guarantees maximize coverage minimum redundancy tested model selecting centrality coverage relevance lexical network function redundancy sentences compared approach cosine similarity based redundancy measure sentences approaches proposed baseline relevance measure decided centrality score performed experiments experiments implemented baseline model subgraph pagerank based centrality measure relevance measure cosine similarity matrix redundancy work implemented centrality measure relevance lexical network redundancy found based centrality giving better results baseline model subgraph centrality giving respectable results second experiment found correlation summary results different centrality based measure relevance lexical network redundancy suggested high correlation pair suitable hybridization low correlation pair expected better results experiment hybridized different centrality measures improved results shown tables figures experiment feature combinations possible limitation tested limited possibility chapter conclusion future work work focus important aspect information retrieval s task automatic text document summarization work divided chapters chapter gives brief introduction text summarization evaluating techniques datasets wide range models developed experimented second chapter hybrid approach single text document summarization statistical sentiment features presented linear combination different statistical measures semantic measures hybrid approach taken statistical measures like sentence position centroid tf idf word level analysis based semantic approach sentiment analysis sentiment score sentence computed sum sentiment score entity present sentence entity sentiment score polarities neutral negative positive interested kind sentences high semantic score negative positive entity sentiment negative multiplying minus treat positive score generate summaries different length different approaches like mead microsoft opinosis human based chapter experiments experiment considered summary generate proposed algorithm system summary model summary evaluating shown getting high precision time denotes covered relevant results second experiment compared different system generated summary mead microsoft opinosis algorithm model summary human generated find explained algorithm performed generated summary time mead system generates summary leading way getting higher recall compare mead experiment showing contribution sentiment score selection informative sentences shown adding sentiment score feature getting improved results compare sentiment score initially experiments assigning equal importance feature score sentence took sum feature s score pickup highest score sentence added summary step selecting sentence based highest score add summary similarity summary sentence lower threshold maintain redundancy coverage repeat iterative process desired length summary achieved experiment extended approach suggested tested better combination feature weights parameter estimation regression random forest chapter new latent semantic analysis entropy based approach automatic text document summarization proposed new approaches new models automatic text document summarization novel entropy based approach summary evaluation approaches summary generation based svd based decomposition approach right singular matrix vt processing selects concept till required previous approaches focused selecting sentence highest information content approach selecting sentences w t concept highest related concept related concept approach based assumption covering different topics result leads coverage diversity future increase selection number sentences glass second approach based entropy formulate different models selecting higher informative concept concept selecting summary sentences repeatedly selecting highest informative sentences e sentence related concepts high score advantage entropy based model length dominating models giving better rouge score statistically closer standard gold summary experiment found rouge score depends count matched words increasing summary length rouge score decreases increasing redundancy rouge score increases pointed rouge score nt measure redundancy e count matched sentences realized need new measure summary evaluation provide tradeoff redundancy countmatch entropy based criteria proposed testing new proposed measure different summary generated previous models proposed models find entropy based summary closer standard summary experiment results clear model works summary evaluation especially higher length summary summary length increases redundancy increases measure measuring redundancy currently giving equal importance n gram theoretically practically weight higher n gram high redundancy information case repetition future assign different feature weights better results fourth chapter lexnetwork based summarization study impact wsd techniques similarity threshold lexnetwork presented lexical network concept automatic text document summarization approach little bit different previously proposed lexical chain based techniques previous techniques author concentrate create number lexical chains creates ambiguity chain prefer problem efficiently handle chain scoring techniques lexical chains problem particular word let donald trump coming sentences sentence prefer problem technique consider nearby sentences chain construction e window sentences selected unable handle long term relationship sentences lex net handle long term relationship sentences nodes scored high score sentence given priority problems handled way lexical network based number lexical semantic relations decide importance sentences centrality based measure lexical network human language highly ambiguous english need find correct sense particular word given context solution ambiguity problem simplified lesk cosine lesk adapted lesk algorithm work studied impact wsd techniques cosine similarity threshold value represents divesity compare high valued diversity good summary maintain diversity relatedness sentences harming good summary property comparison purpose semantrica lexalytics algorithm find system proposed working better times number experiments find subgraph based centrality performing best work reached number conclusions alpha centrality alpha inclusion performance summarizer system arbitrary inclusion centrality measures different value threshold performance continuously increasing time exception reduced set cosine similarity suggesting similarity threshold better diversity better summary rouge score hub authority based ranking eigenvalue based centrality subgraph based centrality measure performing better reason higher score small subgraph recognizes small subgraph cover subgraph considered cluster like structure suggesting particular wsd better time lexnetwork creation lexical semantic relations work assigned equal weight assigns relation presents sentences literature different priority assigned relations future considered network creation improve ranking sentences experiment find corelation eigen value based centrality hub based centrality measures present objective work future try answer fifth chapter modeling automatic text document summarization multi objective optimization proposed optimization based summarization guarantee maximize coverage minimum redundancy tested model selecting coverage based score similarity function relatedness sentences compared proposed lexnetwork based approach cosine similarity based redundancy relevance measured centrality based measures approaches work performed experiments experiments implemented baseline model subgraph pagerank based centrality measure relevance measure cosine similarity matrix redundancy work implemented centrality measure relevance lexical network redundancy found subgraph based centrality giving better results baseline model subgraph centrality giving respectable results second experiment found correlation summary results different centrality based measure relevance lexical network redundancy suggested high correlation pair suitable hybridization low correlation pair expected better results experiment hybridized different centrality measures improved results shown tables figures future work extended combinations aggregate based model proposed tested optimal combination soft computing techniques references alguliev r m aliguliyev r m hajirahimova m s expert systems applications gendocsum mclr generic document summarization based maximum coverage redundancy expert systems applications j eswa alguliev r m aliguliyev r m hajirahimova m s mehdiyev c expert systems applications mcmr maximum coverage minimum redundant text summarization model expert systems applications j eswa alguliev r m aliguliyev r m isazade n r docsum differential evolution self adaptive mutation crossover parameters multi document summarization based systems alguliev r m aliguliyev r m isazade n r expert systems applications cdds constraint driven document summarization models expert systems applications j eswa alguliev r m aliguliyev r m isazade n r formulation document summarization nonlinear programming problem computers industrial engineering alguliev r m aliguliyev r m mehdiyev c sentence selection generic document summarization adaptive differential evolution algorithm swarm evolutionary computation j swevo balaji j geetha t v parthasarathi r abstractive summarization hybrid approach compression semantic graphs international journal semantic web information systems ijswis banerjee s pedersen t adapted lesk algorithm word sense disambiguation wordnet international conference intelligent text processing computational linguistics pp springer barzilay r elhadad m lexical chains text summarization baxendale p b machine index technical literature experiment ibm journal research development beliga s metrovi martini ipi s selectivity based keyword extraction method international journal semantic web information systems ijswis bonacich p power centrality family measures american journal sociology boulesteix janitza s kruppa j knig r overview random forest methodology practical guidance emphasis computational biology bioinformatics wiley interdisciplinary reviews data mining knowledge discovery breiman l random forests machine learning chen y huang y yeh c lee l spoken lecture summarization random walk graph constructed automatically extracted key terms twelfth annual conference international speech communication association chen y liu b wang x automatic text summarization based textual cohesion journal electronics china chen y wang x guan y automatic text summarization based lexical chains international conference natural computation pp springer chiru c rebedea t ciotec s comparison lsa lda lexical chains webist pp creation abstracts l automatic creation literature abstracts april deerwester s dumais s t furnas g w landauer t k harshman r indexing latent semantic analysis journal american society information science dolbear c hobson p vallet d fernndez m cantadorz castellsz p personalised multimedia summaries semantic multimedia ontologies pp springer doran w stokes n carthy j dunnion j comparing lexical chain based summarisation approaches extrinsic evaluation gwc edmundson h p n new methods automatic extracting elhadad m title retrieved serv cs bgu ac il subsys html ercan g cicekli lexical chains keyword extraction information processing erekhinskaya t n moldovan d lexical chains wordnet extensions flairs estrada e rodriguez velazquez j subgraph centrality complex networks physical management conference review e freeman l c set measures centrality based betweenness sociometry freeman l c centrality social networks conceptual clarification social networks ganapathiraju k carbonell j yang y relevance cluster size mmr based summarizer report self paced lab information retrieval ganesan k zhai c han j opinosis graph based approach abstractive summarization highly redundant opinions august gleich d f pagerank web siam review goldstein j mittal v carbonell j callan j creating evaluating multi document sentence extract summaries proceedings ninth international conference information knowledge management pp acm gong y liu x creating generic text summaries gonzlez e fort m f new lexical chain algorithm automatic summarization ccia pp gurevych nahnsen t adapting lexical chaining summarize conversational dialogues proceedings recent advances natural language processing conference pp hahn u mani challenges automatic summarization computer halliday m k hasan r cohesion english longman london hariharan s multi document summarization combinational approach international journal computational cognition hoskinson creating ultimate research assistant computer hovy e lin c automated text summarization summarist jagadeesh j pingali p varma v sentence extraction based single document summarization international institute information technology hyderabad india karanikolas n n galiotou e workbench extractive summarizing methods pci katz l new status index derived sociometric analysis psychometrika katz s m distribution content words phrases text language modelling natural language engineering kim j kim j hwang d korean text summarization aggregate similarity proceedings fifth international workshop information retrieval asian languages pp acm kleinberg j m authoritative sources hyperlinked environment journal acm jacm koschtzki d lehmann k peeters l richter s tenfelde podehl d zlotowski o centrality indices network analysis pp springer kulkarni r apte s s automatic text summarization lexical cohesion correlation sentences international journal research engineering technology kupiec j pedersen j chen f trainable document summarizer proceedings annual international acm sigir conference research development information retrieval pp acm lesk m automatic sense disambiguation machine readable dictionaries tell pine cone ice cream cone proceedings annual international conference systems documentation pp acm lin c rouge package automatic evaluation summaries text summarization branches lin c rey m r ouge package automatic evaluation summaries luhn h p automatic creation literature abstracts ibm journal research development luo w zhuang f q shi z effectively leveraging entropy relevance summarization asia information retrieval symposium pp springer mani maybury m t advances automatic text summarization reviewed mark sanderson university sheffield mani maybury m t automatic summarization mcdonald r study global inference algorithms multi document summarization european conference information retrieval pp springer mckeown k barzilay r chen j elson d evans d klavans j sigelman s columbia s newsblaster new features future directions companion volume proceedings naacl demonstrations medelyan o computing lexical chains graph clustering proceedings annual meeting acl student research workshop pp association computational linguistics morris j hirst g lexical cohesion computed thesaural relations indicator structure text computational linguistics murray g renals s carletta j extractive summarization meeting recordings newman m e j mathematics networks new palgrave encyclopedia economics normalisation statistics wikipedia ou s khoo c s g goh d h automatic text summarization digital libraries handbook research digital libraries design development impact pp igi global ouyang y li w lu q zhang r study position information document summarization proceedings international conference computational linguistics posters pp association computational linguistics ozsoy m g alpaslan f n cicekli journal information science june ozsoy m g cicekli alpaslan f n text summarization turkish texts latent semantic analysis proceedings international conference computational linguistics pp association computational linguistics padmalahari e kumar d v n s prasad s automatic text summarization statistical linguistic features successive thresholds advanced communication control computing technologies icaccct international conference pp ieee page l brin s motwani r winograd t pagerank citation ranking bringing order web stanford infolab plaza l stevenson m daz resolving ambiguity biomedical text improve summarization information processing management pourvali m abadeh m s automated text summarization base lexicales chain graph wordnet wikipedia knowledge base arxiv preprint precision recall n radev d r blair goldensohn s zhang z experiments single multi document summarization mead ann arbor radev d r hovy e mckeown k introduction special issue summarization computational linguistics radev d r jing h sty m tam d centroid based summarization multiple documents j ipm rambow o shrestha l chen j lauridsen c summarizing email threads proceedings hlt naacl short papers pp association computational linguistics rautray r balabantaray r c bhardwaj document summarization sentence features international journal information retrieval research ijirr roul r k sahoo j k goel r deep learning domain multi document text summarization international conference pattern recognition machine intelligence pp springer sakai t sparck jones k generic summaries indexing information retrieval proceedings annual international acm sigir conference research development information retrieval pp acm sankarasubramaniam y ramanathan k ghosh s text summarization wikipedia information processing management j ipm sarkar k syntactic trimming extracted sentences improving extractive multi document summarization journal computing shannon c e mathematical theory communication bell system technical journal vol july october sharan siddiqi s singh j keyword extraction hindi documents statistical approach intelligent computing communication devices pp springer shimada k tadano r endo t multi aspects review summarization objective information j sbspro silber h g mccoy k f efficient text summarizer lexical chains proceedings international conference natural language generation volume pp association computational linguistics sood summarization written text conversations international institute information technology india steinberger j jeek k text summarization singular value decomposition international conference advances information systems pp springer steinberger j poesio m kabadjov m jez k uses anaphora resolution summarization j ipm stokes n applications lexical cohesion analysis topic detection tracking domain university college dublin department computer science takale s kulkarni p j shah s k intelligent web search multi document summarization international journal information retrieval research ijirr tan l examining crosslingual word sense disambiguation nanyang technological university nanyang avenue tofighy s m raj r g javad h h s ahp techniques persian text summarization malaysian journal computer science tombros sanderson m advantages query biased summaries information retrieval proceedings annual international acm sigir conference research development information retrieval pp acm torres moreno j automatic text summarization john wiley sons vechtomova o karamuftuoglu m robertson s e document relevance lexical cohesion query terms information processing management w n venables d m s team r c introduction r wan x cross document relationships generic topic focused document summarizations information retrieval white r w jose j m ruthven task oriented study influencing effects biased summarisation web searching information processing management xiong s ji d query focused multi document summarization hypergraph based ranking information processing management yadav c s sharan hybrid approach single text document summarization statistical sentiment features international journal information retrieval research ijirr yadav c s sharan joshi m l semantic graph based approach text mining proceedings international conference issues challenges intelligent computing techniques icict icicict yadav c s sharan kumar r biswas p new approach single text document summarization advances intelligent systems computing vol yeh j ke h yang w meng text summarization trainable summarizer latent semantic analysis information processing management yeh j text summarization trainable summarizer latent semantic analysis q j ipm zhang r li w gao d ouyang y automatic twitter topic summarization speech acts ieee transactions audio speech language processing
