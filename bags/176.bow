automatic text document summarization using semantic based analysis thesis submitted to the jawaharlal nehru university for the award of the degree of doctor of philosophy by chandra shekhar yadav school of computer and systems sciences jawaharlal nehru university new delhi india july i dedicated to my parents school of computer and systems sciences jawaharlal nehru university new delhi declaration this is to certify that the thesis entitled automatic text document summarization using semantic based analysis is being submitted to the school of computer and systems sciences jawaharlal nehru university new delhi in partial fulfillment of the requirements for the award of the degree of doctor of philosophy is a record of bonafide work carried out by me under the supervision of aditi sharan this thesis contains less than words in length exclusive tables figures and bibliographies the matter embodied in the thesis has not been submitted in part or full to any university or institution for the award of any other degree or diploma chandra shekhar yadav enrollment no school of computer and systems sciences jawaharlal nehru university new india iii school of computer and systems sciences jawaharlal nehru university new delhi certificate this is to certify that the thesis entitled automatic text document summarization using semantic based analysis submitted by chandra shekhar yadav to the school of computer and systems sciences jawaharlal nehru university new delhi for the award of degree of doctor of philosophy is a research work carried out by him under the supervision of aditi sharan supervisor aditi sharan dean k lobiyal school of computer and systems sciences school of computer and systems sciences jawaharlal nehru university jawaharlal nehru university new india new india iv v acknowledgments no goal is achievable without guidance for this i am very glad to express my sincere gratitude and utmost regards to my supervisor aditi sharan for his guidance with many helpful discussions and his intellectual inputs to make thesis work worthy his extensive research experiences were very helpful in my thesis the most important thing is his helping nature and friendly behavior that contributed an important share in the fulfillment of this work the methodology philosophy and problem solving methods suggested by him have been a great help in this work and would be afterwards too i am grateful to my all the teachers of sc ss jnu especially bharadwaj katti karmeshu vidyarthi minz agrawal d k lobiyal vijay kumar and buddha singh for their guidance support and motivation whenever i feel depressed due to rejection or hard comments of my research papers i would like to express my thanks to dean sc ss jnu for his support to pursue my work in the school also i extend my thanks to the school administration especially chandra sir meena maam and ashok sir librarian of sc ss library and the ambedkar central library of jnu for supporting me the work was carried out in lab which has a very healthy and friendly work culture the regular lab discussions exchange of ideas and opinions are part of it i am thankful to hazra imran ubc canada manju lata joshi nidhi malik jagendra singh mayank saini sifatullah siddiqi sonia vikrant vaish ashish sheeba siqqique bhawana gupta all the support during my stay in lab sc ss jnu special thanks to rakesh kumar and payal biswas for encouraging and helping me in writing the thesis it was wonderful to stay in the lab and lifetime memorable for me the students in are very much helpful supportive as well as courageous to each other i am thankful to all my seniors juniors fellow students and friends i would like to thank kapil gupta nit k neetesh kumar iiitm gwalior vipin mgcu navjot mnnit yogendra meena du raza abbas haidri du gaurav baranwal bhu prem yadav dte up harendra vi pratap du sumit kumar krishan veer du dinesh kumar cuh utkarsh nit d dhirendra dtu and those people who always encouraged and motivate me whenever i feel depressed during my work i am thankful to director sliet shailendra jain and all my colleagues in computer science department major singh cse damanpreet singh birmohan singh manoj sachan sanjeev singh gurjinder kaur vinod verma jaspal sigh manminder singh rahul gautam preetpal kaur sanjeev singh electrical sliet pankaj das electronics sliet amit rai chemical sliet and jonny singla mechanical sliet thanks to my wonderful family for bearing with me as i am i was blessed with the love compassion supplications and support of my family members during this period my deepest gratitude goes to them for their selfless love care and indulgence throughout my life this thesis was simply impossible without them my heart goes out in reverence to my respected father padam singh yadav and dear mother suvita devi for their blessings sacrifices tremendous affection and patience i am also indebted to my sisters rinkesh yadav and my younger brother kesari singh yadav for their unconditional love and blind faith in me and also for their continuous motivation in my tough time finally i would like to express thanks to each person who is directly or indirectly related to my work also i would like to thank the university grant commission ugc and council of scientific industrial research csir india for its research fellowship during the period chandra shekhar yadav enrollment no school of computer and systems sciences jawaharlal nehru university new india vii abstract since the advent of the web the amount of data on wen has been increased several million folds in recent years web data generated is more than data stored for years one important data format is text to answer user queries over the internet and to overcome the problem of information overload one possible solution is text document summarization this not only reduces query access time but also optimize the document results according to specific user s requirements summarization of text document can be categorized as abstractive and extractive most of the work has been done in the direction of extractive summarization extractive summarized result is a subset of original documents with the objective of more content coverage and lea redundancy our work is based on extractive approaches in the first approach we are using some statistical features and semantic based features to include sentiment as a feature is an idea cached from a view that emotion plays an important role it effectively conveys a message so it may play a vital role in text document summarization the second work in extractive summarization dimensions based on latent semantic analysis in this document are represented in the form of a matrix where rows represent concepts that cover different dimensions and columns represents documents lsa has the ability to be mapped to the same concept space lsa can hold synonyms relations since the mapping of the same concepts based on svd decomposition and concepts of entropy we find most informative concepts and sentences since lsa can not hold polysemy relations so to extend this work we have used wordnet relations to handle all relationships among words sentences in third work a lexical network has been created and most informative sentences extracted based on that to extend lexical chain based work we have designed an optimization function to optimize content coverage and redundancy along with length constraints since the nature of function is linear constraints are also linear so we have applied integer linear programming to find a solution viii contents declaration iii certificate iv acknowledgments vi abstract viii list of figures xii list of tables xiv chapter introduction to automatic text document summarization introduction flavors of summarization extractive and abstractive summarization single and multi document summarization query focused and generic summarization personalized summarization guided summarization indicative informative and critical summary state of art approach in summarization corpus description corpus description for hybrid approach duc dataset summary evaluation content based task based readability thesis objective chapter hybrid approach for single text document summarization using statistical and sentiment features introduction literature work background random forest binary logistic regression features used in text document summarization the location feature ix the aggregation similarity feature frequency feature centroid feature sentiment feature summarization procedure algorithm detailed approach description experiment and results experiment experiment experiment experiment concluding remark chapter a new latent semantic analysis and entropy based approach for automatic text document summarization introduction background introduction to latent semantic analysis lsa and summarization introduction to entropy and information related work gongliu model murray model sj model model model proposed model working of lsa with an example mincorrelation model lsacs model lsass model proposed model to measure redundancy in summary experiment and results concluding remark chapter lexnetwork based summarization and a study of impact of wsd techniques and similarity threshold over lexnetwork introduction related work background word sense disambiguation wsd lexical chain lexalytics algorithms centrality proposed work experiments and results performance of different centrality measure over selected lesk algorithm with different threshold impact of wsd technique and threshold for different centrality measures comparisons with lexalytics algorithms overall performance concluding remark chapter modeling automatic text document summarization as multi objective optimization introduction literature work related work baseline mdr model background linear programming proposed multi objective optimization model outline of proposed model detail description of proposed model experiments and results cosine based criteria to minimize redundancy a b lexical based criteria to reduce redundancy a co relation analysis between different centrality based proposed model concluding remark chapter conclusion and future work references xi list of figures showing inshorts app s news google snippet example three step process for text document summarization sentence length y axis vs sentence number x axis two of six optimal summaries with scus figure random forest algorithm figure algorithm for summarization opinosis generated summary precision curve recall curve showing f score f score summary figure comparative performance of model and model and vt matrix showing mechanism of lsa matrix a decomposed into u lsa based summarization procedure lsa decomposition on a matrix in reduced dimension vector representation in d space u and vt matrix in d space representing words and sentences showing improved performance using our proposed proposed showing improved performance using our entropy based proposed rouge score for the entropy based system showing as the number of words increasing rouge score also decreasing example of lexical chain creation dataset available elhadad showing normalized degree centrality for an undirected graph showing a graph with some nodes and edges showing eight node with degree regular graph steps in summarization in graphical form directed graph refers to sentence from and from corresponding weighted edges between two sentences are shown here performance evaluation using rouge score of different centrality measures adapted lesk as wsd and similarity threshold performance evaluation using rouge score of different centrality measures cosine lesk as wsd and similarity threshold performance evaluation using rouge score of different centrality measures cosine lesk as wsd and similarity threshold performance evaluation using rouge score of different centrality measures simple lesk as wsd and similarity threshold performance evaluation using rouge score of different centrality measures simple lesk as wsd and similarity threshold impact of wsd technique and similarity threshold over subgraph based centrality impact of wsd technique and similarity threshold over page rank as a centrality measure xii impact of wsd technique and similarity threshold over bonpow based centrality impact of wsd technique and similarity threshold over between ness centrality impact of wsd technique and similarity threshold over closeness centrality impact of wsd technique and similarity threshold over alpha centrality alpha semantrica lexalytics vs our proposed system improved only showing overall performance of some system for the comparative study graphical method of solution of and figure outline of summarizer system three module process for automatic text summarization showing procedure stepwise example from duc healthcare data set file number figure comparative performance of proposed and mdr baseline model different scatter plots showing different directions and strengths of correlation figure graph showing comparative performance of different relevance based measure where sign denoting hybridization of different features xiii list of tables different features scores and total score for sentences our system generated summary using proposed algorithm microsoft system generated summary mead system generated summary summary generated by our algorithm considered as system summary another summary as a model summary summary generated by our algorithm as system summary another summary of a model summary summary generated by our algorithm as system summary another summary of a model summary summary generated by different as system summary human generated summary as a model summary summary generated by different system considered as system summary generated summary as a model summary f score summary different rouge score for summary generated using different approaches different rouge score is shown on duc dataset table showing performance of model table showing performance o model frequency based words documents matrix w in reduced space w is processed by retaining only positive related sentences and rowsum is calculated to find the probability in reduced space r information contained by each concept and corresponding sentence xiv w in reduced space w is processed by keeping only positive related sentences and column sum is find to measure the probability information contained by each sentence in reduced space r rouge score of previously proposed models performance of proposed models with proposed or proposed performance of model with entropy based model generally by increasing summary length rouge score is also increasing and sometimes reducing showing average information contains in summary generated by different systems average n gram presents in different systems generated summary summarization of m stands for model showing all sentences of from duc dataset for the creation of lexical network showing lexical and semantic relations present between sentences performance of different centrality measures using adapted lesk as wsd and similarity threshold performance of different centrality measures using adapted lesk as wsd and similarity threshold performance of different centrality measures using cosine lesk as wsd and similarity threshold performance of different centrality measures using cosine lesk as wsd and similarity threshold performance of different centrality measures using simple lesk as wsd and similarity threshold xv performance of different centrality measures using simple lesk as wsd and similarity threshold impact of wsd technique and similarity threshold over subgraph based centrality impact of wsd technique and similarity threshold over eigen value based centrality impact of wsd technique and similarity threshold over eigen value based centrality impact of wsd technique and similarity threshold over page rank as a centrality measure impact of wsd technique and similarity threshold over bonpow based centrality impact of wsd technique and similarity threshold over closeness centrality comparison of semantrica lexalytics algorithm with difference centrality based measure in which adapted lesk used as wsd showing only improved system semantrica comparison of semantrica lexalytics algorithm with difference centrality based measure in which cosine lesk used as wsd showing only improved system semantrica comparison of semantrica lexalytics algorithm with difference centrality based measure in which simple lesk used as wsd showing only improved system semantrica top performance by centrality based measures wsd technique used and similarity threshold precision recall and f score of model mdr model with subgraph centrality precision recall and f score of model mdr model with pagerank centrality precision recall and f score of our proposed model when centrality based measure is subgraph centrality precision recall and f score of our proposed model when centrality based measure is betweenness centrality xvi precision recall and f score of our proposed model when centrality based measure is pagerank precision recall and f score of our proposed model when centrality based measure is evencentrality precision recall and f score of our proposed model when centrality based measure is precision recall and f score of our proposed model when centrality based measure is closeness precision recall and f score of our proposed model when centrality based measure is bonpow showing pairwise pearson s correlation coefficient symmetric showing pairwise spearman s correlation coefficient symmetric showing pairwise kendall s correlation coefficient symmetric precision recall and f score of our proposed model when relevance is decided by hybridizing subgraph and bonpow precision recall and f score of our proposed model when relevance is decided by hybridizing subgraph and betweenness xvii chapter introduction to automatic text document summarization introduction the internet is defined as the worldwide interconnection of individual networks that is being operated by government industry academia and private parties originally the internet served to interconnect laboratories engaged in government research and since it has been expanded to serve millions of users and a multitude of purposes in all parts of the world in december only million internet users were present that was of the world population and in june this became million people that are of world population according to an ibm marketing cloud study of the data on the internet has been created since people businesses and devices have all become data factories that are pumping out incredible amounts of information to the web each day the size of digital data can be understood by the given statistics since the number of tweets each minute has increased to more than tweets per minute in ii every minute on facebook comments are posted statuses are updated and photos are uploaded iii google searches are conducted worldwide each minute of every day iv worldwide texts are sent every minute new data are producing social media users each day vi million tweets per day according to internet and mobile association of india imai and livemint s article published on march the number of internet users in india were expected to reach million by june up from million in december due to all this information is increasing day by day that leads to information overload this is termed as information glut and data smog information overload occurs when the amount of input to a system exceeds its processing capacity decision makers have fairly limited cognitive processing capacity consequently when information overload occurs it is likely that a reduction in decision quality will occur information overload can be dealt with up to a certain level by the representation of concise information as most of the information on the web is textual so efficient text summarizer can concisely represent textual information on the web radev hovy mckeown have outlined a summary as a text that is produced from one or more texts which conveys important information in the original and that is no longer than half of the original and usually significantly less than that this simple definition captures three important aspects the summaries may be produced from a single document or multiple documents the summaries should preserve important information the summaries should be short as explained by alguliev aliguliyev mehdiyev automatic text document summarization is a task of interdisciplinary research area from computer science including artificial intelligence statistics data mining linguistic and psychology torres moreno has defined an automatic summary as a text generated by software that is coherent and contains a significant amount of relevant information from the source text sakai sparck jones defines a summary as a reductive transformation of source text into a summary text by extraction or generation mani maybury states that a summary is a document containing several text units words terms sentences or paragraphs that are not present in the source document text summarization has various applications in accounting research and efficient utilization of results a text document summarization based real life system is ultimate research assistant was developed by hoskinson their system performs text mining on internet search in other work takale kulkarni shah have highlighted the applications of text document summarization in search engine as google another system newsblaster proposed by mckeown al that automatically collects cluster categorize and summarize news from different websites like cnn reuters this provides a facility for users to browse the results hovy lin introduced summarist system to create a robust text summarization system a system that works on three phases which can describe in the form of an equation like summarization topic identification interpretation generation an application of summarization is news summarization by inshorts app that is shown in figure and google snippet generation in showing inshorts app s news google snippet example broadly summarization task can be categorized into two type extractive summarization and abstractive summarization abstractive summarization focus on the human like summary extractive summarization is based on extractive entities entities may be a sentence subpart of a sentence phrase or a word till now this extractive based summarization relies on standard features like sentence position sentence length frequency of words combination of and as tf idf score selection of candidate word as nouns verbs cue words numbers present in the text uppercase bold letter words sentiment of words sentences aggregate similarity centrality the goal of feature based summarization either alone or a mix of different strategy is to find salient sentences which can include in the summary this process can be shown in figure three step process for text document summarization flavors of summarization due to overlapping of summarization techniques these can not categorize fairly in this section we are trying to present a reasonable overview of existing techniques extractive and abstractive summarization broadly summarization task can be categorized into two type abstractive summarization and extractive summarization abstractive summarization is a more human like summary which is the actual goal of text document summarization as defined by mani maybury and wan abstractive summarization needs three things as information fusion sentences compression and reformation the actual challenge in abstractive summarization is generation of new sentences new phrases along with produced summary must retain the same meaning as the same source document has according to balaji geetha parthasarathi abstractive summarization requires semantic representation of data inference rules and natural language generation they have proposed a semi supervised bootstrapping approach to identify relevant components for an abstractive summary generation a study was done by goldstein mittal carbonell callan state that human generated summary also varies from person to person the reason of this is maybe the setup of the human mind domain knowledge and interest in the particular domain extractive summarization is based on extractive entities entities may be a sentence subpart of sentence phrase or a word till now most work is done on extractive summarization because extraction is easy because this is based on some scoring criteria of words sentences phrases and evaluation of extractive summary is easy because it is just based on word counts or word sequences our work is focused on extractive based technique single and multi document summarization according to ou khoo goh single document summarization can be defined as a process of representing the main content of one document and multi document summarization also a process of representing the main content of a set of related documents on a topic instead of only one document there are two possible approaches for multi document summarization in the first approach combine all documents in the single document then apply single document summary the second possible approach generates a summary for each document then combine all summary into one document later perform single document summarization on the combined summary to get a multi document summary whereas according to sood it is important to note that concatenation of individual single document summaries may not necessarily produce a multi document summary since the issue with the later approach first generate single summary and then again combine to generate a summary is that in this process relative sentences position changes and coherence lost and this research gap opens new dimensions in research query focused and generic summarization a text document may contain several topics like social or economic development political views common people s views environment or entertainment someone may be interested only in one angle so there is a need for specific text from all given text to full fill this requirement user may give a query q on document d after certain similarity the system will return desired documents this is called query focused summarization first tombros sanderson have proposed query focused summarization for text document to develop an information retrieval system while white jose ruthven have extended it for use in web document summarization by combining it with other features including text formatting of the page along with query dependent features the rationale behind their approach with which we concur is that the words in the query should be included in the generated summary another type of summarizer system is generic summarizer that is regardless of user need personalized summarization according to dolbear et al personalization can be defined as this is the technology that enables a summarizer system to harmonize between differently available contents its applications as well as user interaction modalities to a user stated and system s learned preferences the main objective of personalization is to enable the system for content offerings to be closely targeted user desires this can be achieved via different methods like content filtering that extract contents appropriate to a user preferences from a set of available content and give recommendations that provides content to a user based on various criteria which may include the user previous acceptance of related content or on the consumption of related content by a peer group guided summarization this is an extension of query focused summarization but instead of the single question there are set of question guided summarization may be seen as template based summarization the template is a set of question that fired on a text document and the system returns a summary in the form of question answers sometimes this method works well if the developer has good domain knowledge and accurate predictor of the question shortly if we consider any disaster example then set of question or theme will be based on the cause of the accident how many killed how many are in critical and normal condition relief measure is any political visit held during this and compensation paid to effective people it leads to the production of the much focused summaries concerning the questions raised indicative informative and critical summary hahn mani has defined several kinds of summary as indicative summaries follow the classical information retrieval approach they provide enough content to alert users to relevant sources which users can then read in more depth informative summaries act as substitutes for the source mainly by assembling relevant or novel factual information in a concise structure critical summaries or reviews besides containing an informative gist incorporate opinion statements on content they add value by bringing expertise to bear that is not available from the source alone a critical summary of the gettysburg address might be the gettsyburg address though short is one of the greatest of all american speeches with its ending words being especially powerful that government of the people by the people for the people shall not perish from the earth state of art approach in summarization according to the state of approaches summarization procedure can be classified into following part this is not limited to that linguistic structure cohesion has introduced by halliday hasan it captures the intuition this is a technique for sticking together different textual unit of the text cohesion can achieve through the use of semantically related terms like coreference conjunctions and ellipsis among the different cohesion building devices lexical cohesion is the most easily identifiable and most frequent type and it can be a very important source for the flow of informative content centroid and cluster in this approach documents are divided into several units units may be document itself paragraphs and sentences based on some criteria some clusters can be created some famous criteria s are like cosine ngt vector based similarity after clustering summarizer system picks one unit from each cluster that is considered representative of that cluster and later that added to summary this approach can be applied for single and documents machine learning generally we talk about extractive summarization it is based on either statically based feature or linguistic features or its hybridization machine learning based summarization is more effective because it learns features weights from given data and later learned weight can be used on test data only required for this is labeled data multi objective the main concern about the summary is that it should be more informative and length constraints informative constraints can be designed by reducing redundancy and increasing coverage so in this approach mostly all authors designed a function in such a way to reduce redundancy increase coverage and length constraints later that function can be optimized using different techniques corpus description in chapter we used a dataset that was created by us details about this dataset are mentioned in section this experiment also repeated on standard dataset duc rest of the works relies on only dataset details about duc dataset is described in section corpus description for hybrid approach on june was a multi day cloudburst centered on the north indian state of uttarakhand caused devastating floods along with landslides and became the country worst natural disaster though some parts of western nepal tibet himachal pradesh haryana delhi and uttar pradesh in india experienced the flood over of the casualties occurred only in uttarakhand as of july according to figures provided by the uttarakhand government more than people were presumed dead uttrakhand flood corpus is self designed taken from various newspapers ex the hindu times of india this dataset is also published in paper yadav sharan joshi chandra shekhar yadav sharan here we are showing some statistically and linguistic statics about our dataset used statistical statistics total no of sentences in document length of the document after stop word removed total number of distinct words minimum sentence length words maximum sentence length words average sentence length is in our experiment we used a sql stopword list which is available at by seeing the we can interpret that sentences are between length and and sentences are between length and sentence length y axis vs sentence number x axis linguistic statistics in linguistic me are analyzing number and nature of significant entities it is like nn nnp nns dt jj jjr jjs vb vbn vbd vbz vbg vbp where different abbreviation stands for nn noun singular mass nns nounplural proper noun singular nnps proper noun plural vb verb vbd verb past tense vbg verb gerund vbn verb past participle vbp verb non person singular vbz verb person singular jj adjective jjr adjective comparative jjs adjective superlative dt determinant note means x is entity type and is its count duc dataset the document sets are produced using data from the text retrieval conference trec disks used in the question answering track in this dataset includes data from wall street journal ap newswire san jose mercury news financial times la times from disk and fbis from disk each set average has ten documents with at least ten words no maximum length is defined there is single text document abstract for each document with around a hundred words long the multi document abstract is divided into four parts according to two hundred one hundred fifty and ten words long each document is divided into four sets and set categories are following single natural disaster event and created within at most a seven day window single event in any domain and created within at most a seven day window multiple distinct events of a single type no limit on the time window documents that contain biographical information mostly about a single individual summary evaluation to find a good summary lot of work done but to decide the quality of the summary still a challenging task due to several dimensions like length constraints different writing styles and lexical usage the context in which used research is done by goldstein et al they conclude that even human judgment of the quality of a summary varies from person to person only little overlap among the sentences picked by people human judgment usually does find concurrence on the quality of a given summary hence it is sometimes confusing tedious to measure the quality of the text summary summary evaluation can be done by based and task based both are explained in the sub sections content based measure evaluate summary on the presence of textual units n gram in peer summary and standard summary example of content based measures is rouge blue content based pyramid rouge for evaluation most of the researchers are using the recall oriented understudy for gisting evaluation rouge introduced by lin and duc has officially adopted this for summarization evaluation model rouge compares system generated summary with different model summaries it has been considered that rouge is an effective approach to measure document summarizes so widely accepted rouge measure overlaps words between the system summary and standard summary gold summary human summary overlapping words are measured based on n gram co occurrence statistics where n gram can be defined as the continuous sequence of n words multiple rouge metrics have been defined for the different value of n and different models like lcs weighted rouge s su with and without lower upper case matching stemming standard rouge n is defined by here n stands for the length of the n gram is the number of n grams present in the reference summaries and the maximum number of n grams co occurring in the system summary the set of reference summaries is rouge measures generally gives three basic score precision recall and f score since the score is not sufficient indicator for summarizer performance so another variation of rouge is rouge n rouge l w rouge s rouge su in our evaluation we are using six rouge measure l w s and su taken since task about to generate single document summary about words so we are evaluating summary of first words as mentioned earlier in the abstract we are using dataset category two which is about a single event in any domain and created within almost a seven day window as per guidelines recall and precision defined by following and since we are using simple f score so in our evaluation we put in our results we are showing only f score f score is given by the harmonic mean of precision and recall in we are representing fuzz f score pr rouge n measures n grams uni gram bi gram tri gram and higher order n gram overlap rouge l measure lcs largest common subsequence the advantage rouge l over rouge n is that it does nt require consecutive matches and this does nt define n gram length in prior if x is reference summary and y is candidate summary and its length is m and n respectively then lcs based precision recall f score can be defined by equation in duc document understanding conference is set to large quantity as another variant is rouge s where s stands for skipping bigram skip bigram allows maximum two words gap between lexical units this can be understood by an example for the phrase cat in the hat then the skip bigrams are following cat in cat the cat hat in the in hat the hat y is the number of skip bigram matches between x and y c is combination function is to control relative importance of and skip bigram based precision recall and score are given by equation another measure is rouge su it measures skip bigram count between peer summary and standard summary to find out the similarity between these two summaries this measure is quite sensitive to word order without considering consecutive matches uni gram matches are also included in this measure to give credit to a candidate sentence if the sentence does not have word pair co occurring with its reference recall precision and f measure are calculated in the following manner bleu pyramid the bleu method was proposed for automatic evaluation of machine translation system the primary programming task for a bleu implementer is to compare n grams of the candidate with the n grams of the reference translation and count the number of matches these matches are position independent the more the matches the better the candidate translation is two kinds of the summary are generated one is system generated peer summary and another human generated the summary reference summary using reference summary content units scu is find out and using a set of the same words pyramid is constructed in this evaluation method peer summary contributor each lexical unit in a summary or scu are matched against scu in the pyramid the advantage of the pyramid method is that it evaluates summary along with it tells the idea of how the summary is chosen best result in this method is obtained with unigram overlap similarity and single link clustering in this whole process to evaluate summary user required many reference summaries two of six optimal summaries with scus in figure higher weight scu is placed on the top of the pyramid and less weighted scus of weight is placed at the bottom this is reflecting the fact that fewer scus are more probable in all the summaries compare to two three and so on task based they try to measure the prospect of using summaries for a certain task we mention the three most important tasks document categorization information retrieval and question answering for a given text document first we have to develop a summarizer system and get a concise summary from it let summary is s now according to task based evaluation we have to fire queries on s for example in question answering task for a given set of question we will measure precision and recall of queries response it will decide the quality of summary and summarizer system readability in text analysis conference tac and automatically evaluating summaries of peers aesop task the focus was on developing automatic metrics that can measure summary content on the system level in tac a new task is introduced to evaluate for participant ability to measure summary readability both on the level of summarizers as well as on individual summaries to measure the readability of the summaries it accessed based on five linguistic based criteria such as grammaticality correctness nonredundancy of lexical units referential clarity focus of summary text and structure and coherence humans evaluated peer summaries based on these five linguistic questions and assigned a different score on a five point scale one to five where one represents worst and five for the best summary thesis objective this thesis is divided into six chapters this work is about extractive summarization techniques with a focus on how semantic features can be used for summarization first chapter about introduction of text summarization in second chapter we are proposing a hybrid model for a single text document summarization this model is an extraction based approach which is a combination of statistical and semantic technique the hybrid model depends on the linear combination of statistical measures sentence position tf idf aggregate similarity centroid and semantic measure in this work we will show the impact of sentiment feature in summary generation and will find an optimal feature weight for better results for comparison we will generate different system summaries using proposed work mead system microsoft system opinosis system and human generated summary evaluation of the summary will be done by content based measure rouge in the third chapter we are proposing three models based on two approaches for sentence selection that relies on lsa in the first proposed model two sentences are extracted from the right singular matrix to maintain diversity in the summary second and third proposed model is based on shannon entropy in which the score of a latent concept in the second approach and sentence in the third approach is extracted based on the highest entropy in this work we will propose a new measure to measure the redundancy in the text in the fourth chapter we will present lexical network based a new method for ats this work is divided into three different objectives in the first objective we will construct a lexical network in the second objective after constructing the lexical network we will use different centrality measures to decide the importance of sentences since wsd is an intermediate task in text analysis so in third objective we will do an analysis how the performance of centrality measure is changing over the change of wsd technique in an intermediate step and cosine similarity threshold in a post processing step in the fifth chapter we will present an optimization based criteria for automatic text document summarization this is based on three steps first preprocessing of sentences and output goes to the second stage that concern about lexical network creation the output of is network and importance of sentences given by betweenness centrality score in the final module we will decide some optimization criteria using a combination of centrality and lexical network to solve this objective criterion we will use ilp integer linear programming to find a solution which sentences to extract in summary aligned with the means stated above the objectives of this thesis are as follows proposing a statistical and semantic feature based hybrid model for text document summarization proposing a lsa based model which also captures the linguistic feature of the text proposing a new summary evaluate measure based on information contains to maintain the syntactic and semantic property of the text create a lexical network to find based on the previous objective create a new objective function to optimize and expect a out lexical unit for summarization better summary chapter hybrid approach for single text document summarization using statistical and sentiment features introduction in this chapter we are proposing a hybrid method for single text document summarization that is a linear combination of statistical features proposed in the past and a new kind of semantic feature that is sentiment analysis the idea is to include sentiment analysis as a feature in summary generation is derived from the concept that emotions play an important role in communication to effectively convey any message hence it can play a vital role in text document summarization for comparison we are using different system summaries as mead system microsoft system opinosis system and human generated summary evaluation is done using content based measure rouge literature work till now most of the research has done in the direction of extractive summarization based approaches in extractive summarization the important the task is to find informative sentences a subpart of sentence or phrase and include these extractive elements into the summary here we are presenting work done in two categories early category work and recent work done the early work in document summarization has started on single text document by luhn he has proposed a frequency based model in which frequency of words plays a crucial role to decide the importance of any sentence in the given document another work of baxendale was introduced a position based statistical model in his research he has found that starting and ending sentences are more informative in summary generation position based measure works well for newspapers summarization but is not better for scientific research paper documents in continuation of position based work edmundson suggests that sentences in the first and last paragraphs and the first and last sentences of each paragraph should be assigned higher weights than other sentences in a document while kupiec pedersen chen have assigned relatively a higher weight to the first ten paragraphs and last five paragraphs in a document but radev jing sty tam have followed different positional value position pi of an ith sentence is calculated using the equation here n is representing the number of sentences in the document i represents the ith sentence position of the sentence inside the text and cmax is the score of the sentence that has the maximum centroid value radev blair goldensohn zhang have proposed mead system for single and document summarization their sentence score depends on three features centroid and position for each sentence these three features find out and importance of a sentence is decided by the sum of all the features the position score which proposed by them is linear and monotonic decreasing function ganapathiraju carbonell yang have considered keyword occurrence as a feature because as per their understanding keywords of the document represent the theme of the document title keywords are also indicative of the theme they have assigned higher score to first and the last location uppercase word feature containing proper names are included for summary generation indicative phrases like this report short length sentences a sentence with the pronoun she they it are used to reduce the score of the sentence and generally not included in the summary rambow shrestha chen lauridsen have proposed a method for e mail summarization that is based on some conventional feature which is common and used by other authors and some new features conventional features are an absolute position centroid based on idf length of sentence jagadeesh pingali varma have divided features into two type sentence level and word level sentences level features include the position of sentences in the given document the presence of the verbs in the sentences referring pronouns in sentences and length of the sentence in terms of a number of words word level features include term frequency tf word length parts of speech tag and familiarity of the word as per their analysis smaller words has higher frequency occur more frequently than the larger words so to negate this effect they considered the word length as a feature for summarization feature the familiarity of the word is derived from the wordnet relations familiarity can indicate the ambiguity of the word as per author words which have less familiarity were given higher weight the sigmoid function is used to calculate the importance of the word given by some other features used by them are named entity tag occurrence in headings or subheadings and font style the score of the sentence is given by combining all the features in most of the work it is widely considered that leading sentences are more important compared to preceding sentences but according to ouyang li lu zhang this is always not true for actual data importance of sentences varies according to the user and user writing style instead of sentences position they focus on word position and claims that word position features are superior to traditional sentence position features they have defined different word position features direct proportion inverse proportion geometric sequence and binary function finally the score to sentence is given by where is found using one of the features karanikolas galiotou have defined features into three category term weighting position and keyword based term weighting is done for sentence weighting and comprises different ways as local and global weighting for term weighting he has proposed three different ways shown in following and here tij is representing the weight of the jth term in the document di fij is the frequency of the jth term in the document di max fi is the frequency of the most frequent term in document di and fi is the sum of frequencies of the index terms existing in document di for and standard approach followed they have introduced new feature ridf that is residual idf residual idf of a jth term in each document di is defined as the difference between the observed idf expected idf under the assumption that the terms follow a poisson distribution to give sentences position they have followed a new kind of model that is proposed in the news articles algorithm hariharan their score method considers both paragraph location and sentence location in the paragraph where sp is the number of paragraphs in the document p is the position of the paragraph sip is the number of sentences in the paragraph and ssip is the sentence position inside the paragraph the third kind of feature was title words keywords the final score is given by combining all the features scores feature used by luo zhuang he shi were position of the sentence the length of the sentence likelihood of the sentence the number of thematic words the number of low frequency words the lsa score of the sentence the number of two gram keywords number of words appearing in other sentences the entropy of the sentence the relevance of the sentence they have defined relevance measures as intra sentence relationships between sentences and entropy based feature denotes the quantity of information implied by the sentence they have mentioned that long sentences are likely to cover a number of aspects in the document compared to short sentences therefore the long sentence has comparative more entropy than a short length sentence hence a large entropy of sentence possibly implies a large converge shimada tadano endo have proposed a method for multi aspects review summarization based on evaluative sentence extraction they proposed three features ratings of aspects tf idf value and the number of mentions with a similar topic ratings of aspects were divided into different levels from low to high and the rating has given between one to five zhang li gao ouyang have considered following words for summarization features like cue words and phrases abbreviations and acronyms non cue words opinion words vulgar words emoticons such as twitter specific symbols and rt tofighy raj javad have used six features like word frequency keywords in the sentence headline word cue word no of cue word in no of cue word in a paragraph sentence location and sentence length to give the sentence position scores they have used the following method which gives equal importance to first and last second and second last position score is represented by the following padmalahari kumar prasad have proposed the first feature is keyword based in their work keyword are nouns and determined using the keyword has found using morphological analysis noun phrase extraction and clustering and scoring second feature are position based and the third feature is term frequency that is calculated using both the unigram and bigram frequency only nouns are considered for computing of bigram frequencies the fourth feature is the length of the word the fifth feature is parts of speech tag in which tags are ranked and assigned weights that are based on the information contribution of the sentence other linguistic features are a proper noun and pronouns rautray balabantaray bhardwaj have proposed eight features to score the sentence the first feature is title feature that is based on similarity overlapping between the sentence the document title divide by a total number of words in sentences and title the second feature is sentence length longer sentences given more weight compared to small length sentences the third feature is frequency based the fourth feature is position based that depends on both positions in a paragraph and paragraph s position the fifth feature is an aggregate similarity the sixth feature is based on counting on proper nouns seventh is thematic word score based on word frequency eight is the numerical data based score roul sahoo goel have used length of the sentence weight of the sentence that is given by tf idf sentence density presence of named entities in the sentence belong to the number of categories like names of organization locations quantities presence of phrases in the sentence as in summary our investigation in conclusion the paper describes important the best hardly significantly in particular relative offset of the sentence sentences that are located at the beginning or towards the end of a document tends to be more imperative as they carry relevant information like definitions and conclusions such type of sentences receive a score either or presence of title words in the sentence score of the sentence is given by common textual units between the document title and total number of words in the title presence of specially emphasized text quoted text in the sentence generally situated within double quotation marks then it receives the score either score else presence of upper case letters in the sentence these uppercase words or phrases are usually to refer to the important acronyms like names and places such sentences have also received either score else sentence density represented by ration of total count of keywords in a sentence and the total count of words which including all stop words of the sentence in this section we are describing a different model used for weight learning in this section we have divided our dataset into training and testing random forest used predictive modeling and machine learning technique it is an ensemble classifier made using many decision models ensemble models combine the results from different models it is a versatile algorithm capable of performing both regression and classification this performs an implicit feature selection according to breiman random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest the algorithm of random forest is presented by figure boulesteix janitza kruppa knig one characteristic of this algorithm is that due to a large number of trees generated in this technique therefore no issue of overfitting and it is always convergent background random forest figure random forest algorithm binary logistic regression if y be a binary response variable or dependent feature yi is true or if condition satisfies otherwise yi is false or and xk is set of independent features xi may be discrete continuous or a combination xi is the observed value of independent ith observation are to learn the model is represented by equation the shows how parameter estimation done by logistic regression parameter estimation is done by maximizing equation features used in text document summarization we are proposing a hybrid model for salient sentence extraction for single text document summarization this is based on two types of features statistical features location frequency tf idf aggregate similarity centroid and semantic feature that is sentiment feature in this section we are presenting detailed features description used in our sentence selection approach the location feature baxendale has introduced a position feature although his work was almost manual later this measure used widely in sentence scoring the author has concluded that leading sentences of an article are important the model given by them is explained by where n is a total number of sentences the used model is where i n and the aggregation similarity feature kim kim hwang have defined aggregate similarity as the score of a sentence is as the sum of similarities with other all sentence vectors in document vector space model it is given by and where wik is defined as the binary weight ok kth word in an ith sentence similarity measure plays an important role in text document summarization in literature it is proposed that different similarity measure affects the outcome in our implementation we are using cosine similarity based criteria let we have two sentences vector wim and standard cosine similarity between si and sj given by value of i and j vary from to frequency feature the early work in document summarization started on single document summarization by luhn at ibm in the the author has proposed a frequency based model the frequency of word plays a crucial role to decide the importance of any word or sentence in a given document in our method we are using the traditional method of tf idf measure defined by tf stands for term frequency idf for inverse document frequency where tfi is the term frequency of the ith word in the document nd represents a total number of documents and idfi is the document frequency of the ith word in the whole data set in our implementation to calculate the importance of word wi for tf we are considering the sentence as a document and for idf entire document as a dataset centroid feature radev et al have defined as a centroid is a set of words that are statistically important to a cluster of documents as such centroids can be used both to identify salient sentences in a cluster and classify relevant documents the centroid score ci for sentence si is computed as the sum of the centroid scores cw i of all words appeared in the particular sentence that is presented in sentiment feature in previous sections we mentioned statistical measures used by us and in this part we are elaborating semantic based feature we are calling this feature as a semantic feature because in this a set of things are related to one other as defined by mani maybury semantic summary generation may be done using shallow level analysis and deep level analysis in the shallow approach to the most analysis done on the sentence level is syntactic but important to note that word level analysis may be semantic level in deep analysis at least a sentential semantic level of representation is done so our approach sentiment feature is semantic and low level analysis because at the entity level for finding sentiment score for a sentence first we have found out all the entities present in the sentence then find sentiment scores of each entity and then do the sum of all entity sentiment score sentiment strength if the sentiment of entity is neutral then we are scorning it as zero if entity sentiment is positive then considering as same and adding to find the total score of a sentence but if the sentiment score is negative we are multiplying it by minus one to covert in positive score then adding this score to find the total score given in reason for considering negative score to positive score is that we are interested only in sentiment strength which may be positive or negative if sentiment score of an entity if it means sentiment of entity is negative and strength is detail procedure is explained in section s fifth feature here a representing mode a a summarization procedure our summarization approach is based on salient sentence selection and extraction the importance of any sentence is decided by the combined score given by the sum of statistical measures and semantic measure in the next section we are explaining our approach algorithm used in this work and section with detail explanation basically our work of summarization has been divided into three passes sentence scoring sentence extraction and evaluation algorithm this algorithm is divided into three passes sentence scoring according to linear combinations of different measures salient sentence extraction summary generation evaluation of summary pass sentence scoring input documents output scored sentences step score the sentence given with five different measures the outcome is m n matrix m no of sentences n no of measures aggregate cosine similarity position sentiment of sentence centroid score of the sentence tfidf step normalized columns of the matrix step add all the features for every sentence we are calling this sum as a score of the sentence step sort according to score the highest score representing most significant sentence pass algorithm for redundancy input number of sentences descending according to total score output extracted sentences step summary scored sentence step for to number of sentences parameter initialization empty summary similarity threshold l required length summary if ith sentence and length summary l then summary ith sentence step rearrange summary sentences as given in source document to maintaining cohesiveness pass evaluation of summary input different summaries as standard summaries and peer summaries output precision recall and f score step generate different summary different length using mead microsoft opinosis human humans and our proposed algorithm for experiment for experiment precision recall and f measure model summary mead microsoft opinosis peer summary our proposed algorithm model summary human generated summary peer summary mead microsoft opinosis our proposed method step used rouge n to rouge l rouge w we set rouge s rouge su measure to find figure algorithm for summarization detailed approach description here we are describing detail approach used as described in section pass sentence scoring and extraction in algorithm defined in section most things are covered and gives the main idea of algorithm still some micro points are needs to specify is the sum of the linear combination of five different measures four are statistically dependent aggregate similarity position tf idf centroid and the fifth measure is semantically dependent sentiment demonstration of working of this model is shown in the first feature is the position of the sentences position is an important indicator for informative sentence it has been analyzed that first or leading sentences mostly contain important information in our implementation we are using results are shown in s second column the second feature is tf idf we are using the standard formula as defined in the previous section normalized tf idf score is given in third columns the third feature is an aggregate similarity cosine score of a sentence vector can be calculated as the sum of similarities with other all sentence vectors in document vector space model the significance of this is to find sentences which are highly similar to all other sentences after representing all sentences in a vector space and then find vector cosine similarity with all other sentences as a defined standard formula by normalized aggregate cosine similarity in column four since other scores centroid position sentiment are between so we need to normalized score normalization of values means adjusting to values measured on different scales to one notionally common scale that removes the chance to be biased some values in our implementation we are just using column normalization instead of matrix normalization normalization of a column vector is done using where xi is the ith element in the column and n is the size of the column b let a is a given matrix which size is and column one and two has does have values between then we are doing normalization of only column one and two but not third column and b is the give normalized matrix in our case the fourth feature is centroid based radev et al defined as centroid as a set of words that arestatisticall important to a cluster of documents in our approach using mead centroid score output as our input the centroid value of a sentence is given by the summation of each word centroid score present in the sentence the fifth feature is sentiment score this is a novelty in our work to find this feature we are dependent on alchemy api which is available at we have considered that it is finding sentiment score is a semantic approach and fall under shallow level approach as defined in section for any sentence or words we can define three kinds of sentiment neutral negative and positive neutral sentiment value means that words or that sentence sentiment score are zero most important to note that it is easy to find sentiment score based on cue word like good bad pleasant but still due to so much complexity in text words limitation of nlp it is not possible to find the correct sentiment score sometimes even it is also not possible to detect sentiment due to hidden sentiments the overall working of the fifth feature can be understood clearly by following documents document naac accredited jnu with the cgpa of on a four point scale of a grade highest grade by naac and sentiment of this is neutral document jnu ranked in top in times higher education asia and brics top ranking and sentiment of this document is positive and the score is naac stands for national assessment and accreditation council naac and brics stands for five nations brazil russia india china and south africa here and both are representing positive news about jnu but the sentiment of is neutral and sentiment of is positive with score still we have to discover an approach which can find correct sentiment hidden sentiment some results are displayed in with sentiment results in our implementation to find the sentiment score of a sentence we are using alchemy api first finding all entities present in the sentence and their sentiment score then we add all entity sentiment score for example consider document number meanwhile bjp spokesperson prakash javadekar has said that party president has five entities as follow prakash javadekar person name rajnath singh person name uttarakhand state county bjp company president jobtitle triplet x y z representing x is entity name y is entity type z sentiment score and to give sentiment score of sentence we add all so sentiment score for sentence but if we see the result in row and column sentiment score the value is here we are considering only positive sentiment scores if the entity sentiment score is negative then by multiplying to convert it into positive score the obvious goal of the procedure is to give equal importance if magnitude is same let consider one the childhood story about the fox and the grapes in the fox and the grapes story if here we consider two sentences given below document and as a document both are important in the story and about the same things grapes document just these sweat and juicy grapes to quench my thirst and document they re probably sour anyway with alchemy system if we find a sentiment of both these sentences then the sentiment of is positive and the score is where the sentiment of is negative with for us the only magnitude is important reasons to consider as value are we are interested to find sentiment strength it may be negative or positive and both are important for us and if we will add a negative score to find the total score then the value will reduce in the next step we are finding the total score of a sentence by adding all scores the total score can be represented by the given below in our implementation to n detail result of the individual score is given in and the last column is the total score of all scores different features scores and total score for sentences sent no position score tf idf aggregate cosine sim centroid score sentiment score sum of all pass redundancy to remove redundancy we have used the same model as proposed by sarkar which the topmost sentence according to total score defined in the equation is add in summary we added next sentence in the summary if the similarity is less than threshold the algorithm is described in section s input in this pass is a number of sentences which are sorted according to descending total score we need to initialize some parameter to get the desired summary parameter like summary initially empty given similarity threshold and l for the desired length summary even in our system l stands for the maximum length of the desired summary but due to the limitation here the length of sentences we can guarantee minimum maximum length summary we will add the next sentence in summary if still summary length is less than l and similarity new sentence summary the output of this step is a summary with minimal redundancy and length nearly equal to l but the position of the sentence is zigzag that lost the sequence and cohesiveness to maintain the sequence we need to rearrange the sentences according to given in the initial index in we are representing the summary generated by our system in which the similarity threshold is and the desired summary length is we can define arbitrary l in a number of words or percentage of summary required here we have chosen small if we put large like or then the sentences which are in summary will depend only on the total score as in in other words the summary only depends on total scores as shown in but our objective is also to minimize redundancy note before calculating sentence summary we are eliminating stopwords stopwords play a big role to increase the similarity between two sentences with different stopwords list we will get different similarity score mead microsoft and our model generated summary with different length are shown below in the truth is we do have the model behind microsoft generated summary this summarizer is inbuilt inside microsoft office package when we observed that microsoft summarizer is not reducing redundancy sentence and are almost similar sentences it is shown in pass evaluation goldstein et al have concluded two things even human judgment of the quality of a summary varies from person to person human judgment usually does find concurrence on the quality of a given summary hence it is difficult to judge the quality of a summary for evaluation of any summary we need two summaries first one is a system generated the summary and another summary is user generated model summary or standard summary to generate different model summary we used three approaches we give our text data set to people and tell them to write a summary in about to to words we generate summary by mead tool in this approach we taking linear combination of position and centroid score with and third model summary is generated by opinosis by ganesan zhai han summary given in figure microsoft system to evaluate summary we are using rouge evaluation package duc adopts rouge for official evaluation metric for both single text document summarization and multi document summarization rouge finds recall precision and f score for evaluation results based on gram co occurrence statistic it rouge measures how much the system generated summary machine summary overlaps with the standard summary human summaries model summary where an n gram is a contiguous sequence of n words in our evaluation we are adopting different measures of rouge as rouge n to rouge w rouge l rouge s and rouge su experiment and results in this section we are presenting four experiments done by us in the first experiment we took our own summary generated by algorithm discussed in section as system generate summary and another summary as a model summary in the second experiment we are comparing different system generated summary human summary in the third experiment we are showing the significance of sentiment feature in fourth experiment we are finding best feature weight combination using regression and random forest experiment as explained in section s third pass we have created four types of the model summary human summary via we gave data set to persons to summarize it based on their experience with instruction to summarize it within to words length due to limitations and user experiences the generated summary varies from to words length mead microsoft and opinosis system different system generated summaries are given in since opinosis summarizer is the abstractive type in figure we are giving summarization result length summary generated by opinosis system presenting different summaries generated by different systems our system generated summary using proposed algorithm microsoft system generated summary mead system generated summary opinosis generated summary in the first experiment we took our own summary generated by algorithm discussed in section as system generate summary and another summary as a model summary in the next step we find different rouge scores to rouge l rouge w where rouge s and rouge as defined by lin rey and followed by others like sankarasubramaniam ramanathan ghosh rouge scores are given in with it measures three things recall precision and f score for any system generated summary and model summary or reference summary we are comparing our system generated summary with other as model summary same length summary the result of this is given in and for length respectively due to the limitation of space we are providing only three tables showing f measure with different model summaries of the length of nearly and our summary length is nearly in simple term we can define high precision means that an algorithm retrieved substantially more relevant than irrelevant and high recall means that an algorithm returns most of the relevant result precision and recall wiki from and for summary length we are getting high precision f score mead reference summary and high recall microsoft generated a summary summary generated by our algorithm considered as system summary another summary as a model summary measure mead microsoft opiosis summary r p f r p f r p f rouge l rouge w rouge s rouge su summary generated by our algorithm as system summary another summary of a model summary measure mead opiosis summary r p f r p f rouge l rouge w rouge s rouge su summary generated by our algorithm as system summary another summary of a model summary measure mead mead microsoft microsoft opiosis summary r p f r p f r p f r p f r p f rouge l rouge w rouge s precision curve recall curve showing f score experiment in the second experiment we are comparing different system generated summary human summary or in other words here model reference summary is a human generated summary and other summaries are system generated summary mead microsoft opinosis our algo are system generated summary the result is shown in and is representing different rouge scores for the summary length of summary generated by different as system summary human generated summary as a model summary measure my method mead opinious user r p f r p f r p f r p f rouge l rouge w rouge s rouge su summary generated by different system considered as system summary human generated summary as a model summary measure my mead user summary r p f r p f r p f r p f rouge l rouge w rouge s rouge su from we can say that we are getting the high f score comparison to mead microsoft system and opinosis system except for rouge w in mead s and opinosis rouge f score is represented in we are getting high precision to compare to mead and opinosis but microsoft system leading in rouge l rouge w rouge s rouge su only we are getting high recall comparison to mead in to and higher recall comparison to opinosis and microsoft in all measures except opinosis getting higher than our system in we are representing a comparison of different system generated summary length using f measure and comparison of length summary and we representing here only f score from we can say that mead system and microsoft perform better in term of recall but our system is performing better compared to opinosis our method gets higher precision to compare to opinsis all rouge score p and higher precision achieved compared to mead except to we are getting low f score compares to mead and microsoft system but higher opinosis f score summary f score summary experiment in the third experiment we are showing the significance of sentiment feature the purpose of this experiment to show is really sentiment score performs a significant role in salient sentence extraction to generate a good quality summary of limited words like hundred words is a tedious task in our experiment we have considered five different features we have tried all combinations of all five features and using this combination we are trying to prove this feature is playing a significant role in summarization if a number of features are n then with at least one feature the total number of combinations so in our experiment we are trying all combinations calling approaches here we are generating a summary using single stand alone feature based summary and summary in which sentiment score is playing a role here first we generate approximate word summary to evaluate this summary we have taken three generated summaries as a gold reference summary motivated by duc task we are evaluating only the first words of the summary let stands for tf idf feature stands for aggregate similarity score stands for position based feature stands for the centroid based feature stands for sentiment based score feature showing a collective score of tf idf aggregate similarity position based features different rouge score for summary generated using different approaches approach measures rouge l rouge w rouge s rouge su approach measures rouge l rouge w rouge s rouge su here we are presenting different features combination to find a summary by seeing we can conclude that position based feature approach highlighted in green is performing best among all this is due to that in all three human reference summary out of five summary three are extractive type extractive type summary is available at address which are used for evaluation contains top sentences and which is almost words and the model which we are using for score position giving higher preference for leading sentences it is widely considered that position based score ca perform well in all cases for example in scientific article so we need some more features from this is clear when we are taking sentiment feature i d along with other features we are getting an improved summary more rouge score means more accurate summary the conclusion of this experiment is that out of approaches when sentiment feature added nine times we are getting an improved summary by adding sentiment feature for example if we take collective features and by adding sentiment feature we are getting improved results highlighted in red color here position based feature performing best among all approaches the reason is given above and we ca depend only on position based feature so we need more features in approach aggregate and position when we add sentiment score done in approach highlighted in blue color the performance is reduced this is due to position based score not preferred position based feature is not dominating here as in approach results are obtained from three human summaries as reference summary and summary obtains from different approaches consider as system summary along with document are available at yadav al to remove biases and evaluate the first world summary we use initial word summary and to evaluate rouge w we have taken we have performed this experiment on duc dataset and after the results are shown in we have found out that after incorporating sentiment feature the performance has been improved this experiment is performed with and without stop words different rouge score is shown on duc dataset rouge location location sentiment location stop word removed location sentiment stopword removed rouge l rouge w rouge s rouge s experiment in this experiment we are finding the best optimal feature weights to get more improved results to decide the feature weights we are using a supervised that is random forest and logistic regression in this internal estimates monitor error strength correlation and it is used to evaluate the variable importance this experiment has been performed on duc dataset in we are showing correlation between independent features those are used by us by visualizing we can say there is no correlation or less correlation between features showing the correlation between feature used for summarization we have divided our data into training and testing and develop a model to find best feature weight combination in this regard we have developed two models random forest and logistic regression in features weights are suggested with corresponding model currency table showing optimal feature weights using different models model location tf idf centroid random forest aggregate similarity sentiment model accuracy logistics regression from table we can say that in logistic regression model sentiment score weight is is the highest feature weight this signifies importance of sentiment is highest here in logistic regression based model sentiment feature weight is second highest between these two models random forest and logistic regression logistic regression suggesting feature weights corresponding to location tf idf centroid aggregate similarity and sentiment with model accuracy so here we are testing the and represented by corresponding equation and table showing performance of model p r f p r f rouge l rouge w rouge s rouge su table showing performance o model p r f p r f rouge l rouge w rouge s rouge su model is simple linear model which is proposed and tested in previous sections on self made data set in this section we have implemented that model on duc dataset in this experiment we have implemented the previous model as and new model after feature weight learning using regression analysis results are attached in table and performance of model is decided based on precision p recall r and f score f and confidence interval shows the probability of lying values of p r f score between given range figure shows comparative performance analysis based on content based rouge measure these tables and and figure give a proof of significant change in results in model over performance of model and rouge l rouge w rouge s su p model r model f model p model r model f model figure comparative performance of model and model concluding remark in this work we have presented a hybrid model for text document summarization which is based on a linear combination of different statistical measures and semantic measures in our hybrid approach we have considered statistical measures like sentence position centroid tf idf as well as a semantic approach doing sentiment analysis that is based on word level analysis sentiment score of a sentence is given as the sum of sentiment score of every entity present in a sentence we are getting three polarities for any entity as neutral negative and positive if any entity sentiment is negative then we are multiplying every score by to treat it as positive score the reason for doing this we want to select a sentence in which strong sentiment is present it may be either negative or positive and both have the same importance for us to calculate the score or importance for a sentence we have just added all the scores for every sentence and pick up the highest scoring sentence in the next step if the similarity between summary and sentences is lower than the threshold to maintain diversity then we have added it in the summary our stopping criteria is summary length constraints to generate several summaries of different length we have used methods like mead microsoft opinosis and human generated summary evaluation is done by rouge measure in this chapter we have done four experiments in the first approach we took our proposed algorithm based generated summary as system summary and all other as a model summary in this experiment it has shown that we are getting high precision almost every time this signifies that we covered most relevant results in the second experiment we have compared different system generated a summary mead microsoft opinosis and our algorithm to the model summary human generated in this we find that our explained algorithm performed well for generated summary for almost every time but in mead system generates a summary leading in some way but here also we are getting higher recall comparatively to mead in the third experiment section we have shown that when we are adding sentiment score as a feature we are getting improved results to compare to without a sentiment score the third experiment is showing that sentiment score has a contribution in the extraction of more appropriate sentences in the fourth experiment we have divided our data into training and testing and proposed a feature weighted approach using random forest and regression the logistic regression model is giving better accuracy since logistic regression is producing better model so feature weights are assigned as per regression model a new experiment again performed to show the model improvement using feature weight analysis chapter a new latent semantic analysis and entropy based approach for automatic text document summarization introduction in this chapter we are proposing latent semantic analysis lsa based model for text summarization in this work we have proposed three models those are based on two approaches in the first proposed model two sentences are extracted from the right singular matrix to maintain diversity in the summary second and third proposed model is based on shannon entropy in which the score of a latent concept in the second approach and sentences in the third approach are extracted based on the highest entropy the advantage of these models is that these are not length dominating model giving better results and low redundancy along with these three new models entropy based summary evaluation criteria are proposed and tested we are also showing that our entropy based proposed model is statistically closer to s standard gold summary in this work we are using dataset taken from document understanding background in this section we are presenting an introduction to lsa advantages and limitation of lsa how lsa can be used for summarization and how to find information content by using the principle of information theory introduction to latent semantic analysis in text mining direction early application of lsa has started by deerwester dumais furnas landauer harshman their objective was indexing of text document now it has been used for various applications in automatic text document summarization first the objective is to convert a given document d into matrix representation matrix a is term document matrix in which elements aij document this can be designed by combining local and global weight as represents the weighted term frequency of ith term in ith in the lsa is a vector space approach that involves the projection of the given matrix matrix amn usually is represented in reduced dimension r denoted by ar such that using lsa input matrix a is decomposed as and showing mechanism of lsa matrix a decomposed into u and vt matrix where u is an mn column orthonormal matrix which columns are called left singular vector n n is a nn diagonal covariance matrix whose diagonal elements are non negative singular values which are sorted in descending order as in vt right singular matrix also an orthonormal matrix with size nn which columns are called right singular vector let r then matrix matrix s properties can be express as where i i r and r is representing diagonal elements n several methods decomposition of factorization is available like ulv low rank orthogonal semi discrete decomposition and svd we are using lsa based on svd decomposition the reason of this is simultaneously svd decompose matrix a into orthogonal factors that represent both types and documents vector representation of both types and documents are achieved and second the svd sufficiently capture for adjusting the representation of types and documents in the vector space by choosing the number of dimensions computation of the svd in manageable for the large data set the interpretation of applying the svd to the term by sentence matrix a can be made from two transformation point of view the svd derives a mapping between the m dimensional space spanned by the weighted term frequency vectors and the r dimensional singular the semantic point of view the svd derives the latent semantic structure from the document represented by matrix a there are some obvious reasons for using lsa but sometimes this has some limitations in viewpoints vector space implementations like this advantage first all the textual units including documents and word have the ability to be mapped to the same concept space in this concept space we can cluster words documents and easy to find out how these clusters coincide so we can retrieve documents based on words and second the concept of space has immensely fewer k dimensions compared to the designed vice versa original matrix m third lsa is an inherently global algorithm but lsa can also be usefully combined with a more local algorithm to be more useful limitations lsa considers a frobenius norm and gaussian distribution which is not fit for all problems in our problem our data follow a poisson distribution depends on word counts as studied by katz lsa ca handle polysemy multiple meanings lsa depend on singular value decomposition svd which is computationally expensive and hard to update if the case of new documents required to add lsa and summarization lsa is an algebraic statistical technique svd over given document can be understood regarding independent concepts svd is a method which models relationships between words and sentences the beauty of lsa is that it can find out semantically similar words and sentences it has the capability of noise reduction which leads to improved accuracy our lsa based summarization is presented in which comprises three major steps input matrix creation svd and sentence extraction selection which is briefly explained in this section note document and sentence are interchangeable for us sentences are documents lsa based summarization procedure input matrix creation in most of the papers input matrix creation is based on a bag of word approach or word count approach because it is easy to create as well as widely accepted input matrix may be considered as a combination of local weight lij and global local weight criteria define weight in the range of document it may be a sentence paragraph or set of paragraphs and global weight is decided among all documents let we want to use a bag of word approach and then some common local weighting methods are mentioned in n number of documents input matrix creation a svd u s sentence selection extraction local weight approach global weight approach binary if term exists in the binary if term in document i document else term frequency lij tfij the number of occurrence of term in document j log tfij is the gf idf where gfi is the total number of times term i occur in number of occurrence of term in the whole collection and is the number of documents in document j which term i occurs semantic in matrix a with size location aij can connect to location afg if there is any semantic connection available between words m i j g semantic connection examples are synonyms anatomy polysemy hypernymy hyponym where gfi is the total number of times term i occur in the whole collection tfij is the number of occurrence of term i in document j n total documents pij probability of occurrence of ith term in jth document local and global weight model decomposition lsa uses svd for matrix decomposition if we have an input matrix a m n number of words a number of sentences then svd decomposes a matrix decomposed into three matrixes such as left singular u described as words concepts with m n size is a matrix with n n size represents scaling values and v matrix n n size sentences concepts sentence selection right singular matrix gives information about sentences and concepts if we want information about sentence we can use detailed about selection method is presented in related work some authors like gong liu used s and v both for sentence selection introduction to entropy and information entropy was originally defined by shannon to study the amount of information contained in the transmitted message in information theory entropy may be considered as the measure of the amount of the information that is missing from transmitted information and received information and is sometimes called as shannon entropy the definition of the information entropy can express regarding a discrete set of probabilities for example in the case of a transmitted message these probabilities p xi were the probabilities that a particular message xi actually transmitted and the entropy of the message system was a measure of how much information is present in the message let in a message m there are n symbols as mn are present this message m transmitted from source s to destination suppose further that the source s transmits the symbols mi i n with probabilities respectively if symbol mi is repeated t times then the probability of occurrence of mi will be given by tpi thus in t independent observations the total information i given by it is assumed that the symbols are emitted independently the average information of symbols i t will be calculated by and in our case we are also using the same entropy function related work in this section we are mentioning five previous different models which have been used for comparison purpose all these five models have based on lsa based decomposition each model is presented along with its pros cons gongliu model this model has been proposed by gong liu authors have used vt matrix for sentence selection according to them each row of vt representing one topic concept and corresponding to that topic selects only one sentence which has the highest corresponding value on that topic they have added extracted sentences in the summary and the process is repeated until the required length summary is achieved we have observed the following advantages and drawbacks in the salient drawback new and unique approach in this approach the summary is depended only on vt as per author convenient they can select some rows from reduced dimensions of vt the maximum selected summary sentences will be equal to the reduced dimension if a case if reduced dimension r then no way is suggested to select more sentences top concepts represent more information compared to bottom concepts sometimes in this approach extraction of sentences that may belong to less important concepts let system choose an ith concept for sentence selection from the selected concept if two sentences have high values like and then gong approach will choose only one sentence as we know both are highly related but still the second sentence is not respected in this approach vt all selected concepts give equal importance but some may not so much importance in the this is well known that concepts are independent of each other so this is expected that those sentences are extracted from concepts are also independent ideally it is not possible in text data especially due to linking of name entities stopwords and pronoun words present in the text in simple words if we want diversity in summary then our matrix a should be noiseless data after removing stop words so the better output from this approach depends only on the input matrix a murray model this model proposed by murray renals carletta have used both vt and s matrix for sentences selection instead of selecting only one sentence from vt which depends on the highest index value selects the number of sentences based on matrix the number of sentence selection from one concept depends on getting the percentage of the related singular values over the sum of all values or in reduced space r we have observed the following advantage and drawbacks in the overcomes the problem of gong liu approach which selects only one sentence from each concept some starting values from matrix will play a significant role and will dominate salient drawback sj model steinberger jeek have proposed another model for document summarization they are also using both and v matrix transpose of vt for sentence selection to add a sentence in summary the author finds the length of a sentence using s and v in reduced space r that is given by s matrix is multiplied by v matrix to give more emphasis on topics by property s is sorted in decreasing order and so the first topic is multiplied with higher value compare with later occur topics we have observed the following advantage and drawbacks in the salient drawbacks sentence selection is based on new reduced space so consideration about only preferred concepts and highest length sentence long length sentences may dominate by calculating si no explicit criteria to increase diversity in the summary even all sentences are somehow related to all the concepts in preprocessing authors have considered some sentence as unrelated sentences model another model cross method was proposed by ozsoy alpaslan cicekli in which vt matrix is preprocessed that represents only core sentences as per remove sentences that have index value less than mean then vt matrix is multiplied with matrix to give importance for topics shown in calculatedces length are calculated by adding columns of vt matrix highest length sentence is added to the summary this process is repeated until the required length summary we have observed the following advantages and drawbacks in the drawback diversity in summary is not considered no explicit criteria length dominating summary long length sentences extracted and added to the summary model another topic based approach has proposed by ozsoy et al in that preprocessing is same as done in cross approach vt vt is representing only core sentences for each topic reduces space r instead of sentence length approach as explained as sj model they have found out important concept based on those concepts sentence selection is made the important concept is found by concept concept matrix this matrix is formed by summing up the cell values that are common to these concepts we have observed the following advantage and drawbacks in the salient drawback finding the main topic based on concept concept matrix is giving better results higher concept score is showing that a concept is much more related to other concepts still following gong liu approach from sentence selection select only one sentence from one concept if some chosen a concept k for sentence selection in this concept if two sentences have high values gong approach choosing only one sentence with one example if there is a case that concept k is related to with sentence si and with sentence as we know these are highly related then also this is not respected even some sentences are somehow related to concepts in preprocessing author put this to zero is same as showing that respective sentence and concept are unrelated this is said that con sign a b showing a is preferred over b property not considered extracted sentences are assumed to be implicitly diverse proposed model sentence selection is done from either vt matrix or with a combination of diagonal matrix and vt matrix as we know the concepts in vt assumed that among extracted sentence have minimum similarity with each other matrix is independent of each other so this is inherently singular values are always in sorted order and this is assumed by gong liu concepti is preferred over but approach followed by steinberger jeek cross approach by ozsoy al and ozsoy cicekli alpaslan are not considering this phenomenon and sentence selection is based on the longest length sentence given by and respectively in both of these approach topics concept with the highest strength is chosen we are proposing two different approaches first one is based on vector concept relatedness and termed as proposed and the second approach is based on entropy that is further explored and represented with and before explaining these model or approaches first we are taking an example in section representing a document as d creating a matrix a of words sentences over that matrix a we are applying svd decomposition and this is represented by adecomposed that can further use in different ways to generate a summary in section first we are taking a document d in creating matrix word frequency based matrix a and computing in section proposed in section proposed and section contains proposed working of lsa with an example in this section first we are representing an lsa based sentences are numbered from to and corresponding word sentences matrix a is represented in table this example is taken from the reason for doing this is it widely available and this example expresses the property of lsa very well in reduced dimensions representation of given document term document matrix is same as lsa in dimensional space only this has been shown at the same link via graph representation in let consider an example with nine given sentences the neatest little guide to stock market investing investing for dummies edition of stock market returns the little book of value investing value investing from graham to buffett and beyond investing in real estate edition stock investing for dummies investors the little book of common sense investing the only way to guarantee your fair share rich dad guide to investing what the rich invest in that the poor and the middle class do not rich dad advisors the abc of real estate investing the secrets of finding hidden profits most here we considered a document d that is equal to u u where si representing ith sentence and considering only underlined words book dads dummies estate guide investing market real rich stock value as a set of words finally based on presence not presence words in sentences input matrix a word sentence matrix is constructed that is given in rows of a representing the presence of words in a different sentence with frequency and columns representing the presence of words in this example keyword selection is arbitrary but for keywords selection we can follow many approaches as proposed by beliga metrovi martini ipi sharan siddiqi singh s s s s s s s s s index words book dads dummies estate guide investing market real rich stock value titles frequency based words documents matrix now svd a u in reduced space r a is given by composition of this matrix in where u denotes is left decomposed matrix and v for the right decomposed matrix s matrix non negative diagonal matrix lsa decomposition on a matrix in reduced dimension mincorrelation model this model is based on and that is using vt right decomposed matrix as explained in gong liu approach author used vt matrix and only one sentence is selected from each concept we can fix number to decide k sentences to extract from each concept in our first we are selecting a concept as done by gong liu then instead of selecting just one sentence that is highly related to that concept select two sentences in such a way that one most related and second least related to that concept more related means higher value corresponding to entry concept sentence less related means less value corresponding to entry concept sentence this approach is to cover two different topics sentences from the same concept graphically this procedure can be understood by in a two dimensional space has four vectors and each vector is maintaining some angular distance from x axis if corresponding to x axis we want to select two vectors then our choice will be because is closed to x axis and is least related to corresponding to x axis we want to select two vectors then our choice is if we are interested in selecting three vectors then the selection set will be in our application are a reference to documents if the total number of the vector is n then objective may give by here co relation may be some similarity measure or other measure but in this work co relation is measured from co occurrence matrix find by vt by lsa vector representation in d space objective function denoted by return set of vector sentences those are minimal co related above given example can be understood by and in figure co relation can be considered as angular distance between vectors lsacs model lsa based concept selection lsacs is based on which is based on entropy technique in this model we have to find out concepts that contain more information how much information is contained in a concept is decided based on entropy detail about information and entropy are presented in section concepts hidden latent with high entropy is showing more degree of freedom that means the particular concept is related to the number of dimensions concepts sentences since concepts are hidden in svd so we can not show these latent but just for consideration if we consider dimensions x y and z as a latent concept then we are interested to find which dimension is more representative anyone and we are finding this using shandon entropy proposed in shannon in sentences are represented using triangle sign blue color from to in to words are represented by star sign red color underlined words in example the x y and z axis which are assumed as like hidden latent and not possible to show in the physical diagram is represented by the arrow sign in red color below we are presenting the proposed algorithm the objective of this to find the one latent axis among x y or z which represents more information u and vt matrix in d space representing words and sentences input document d the content of words sentences paragraphs predefined length of summary initially similarity threshold output summary decompose the given document d into sentences sn extract keywords and use these keywords to for given document d construct a matrix a term sentence matrix a can design by various ways form the master sentence we are using tf idf approach perform the svd on a as for every sentence in reduced space r by using right singular matrix vt and compute w matrix columns of w are representing sentences and rows are representing concepts latent similar to gong approach or ii case similar to approach or preprocess on w by selecting only core sentences corresponding to concepts by strikeout least related for each row in w compute where this is representing the probability of sentences sentence j appear in concept find the information contains in each concept using shannon s entropy method as follow section select most informative concept cp reduce space using w matrix and selected concept cp select a sentence si that is most related to cp if l lmax similarity si sj sj summary sentences add sj to the summary and l goto else choose next sentence in is representing preprocessing on given document in step two creations of matrix a is done by here approach is followed by us in step three svd decomposition is performed on a matrix svd a decomposed the matrix into three matrices in which right singular matrix represents information about words left singular matrix informs about sentences and is covariance matrix that can be used to decide the importance of parameters the importance of concepts sentences words have two cases of how to design w based on two sub approaches given by gong liu and steinberger jeek model in w each row is representing concepts and column for sentences in we are demonstrating of w formation that using covariance matrix and vt in this example for demonstration purpose we are using reduced dimension three is representing the relatedness of sentences to corresponding to to reduced dimension r in based on the negative sign we will eliminate these entries neglect those sentences which are least informative this is represented in in step we are finding the probability of relatedness between sentences and concepts over the value in is representing the probability of jth sentence in ith concept in each concept all sentences are represented in terms of probability such that where xi denotes ith concept this is shown in in finding the information contained in by each concept that is given by the and in is representing freedom of concept or information in the concept in we will choose the highest informative concept and from a selected concept in sentences are added to the summary based on certain cosine similarity criteria matched that is given by this is iteratively repeated until length constraints satisfied sentences w in reduced space sentences w is processed by retaining only positive related sentences and rowsum is calculated to find the probability sentences total entropy sentences sentences sentences in reduced space r information contained by each concept and corresponding sentence in this proposed work we are interested to find a concept that is more closely related to all sentences relatedness is considered in term of probability valueij indexed at location ith concept jth sentence showing that concepti is related to sentencej with strength valueij a higher value is representing that a concept is more related to a particular sentence according to our proposed approach the concept is more informative if that is related to more number of sentences with high score for example let a concepti related to sentencej with probability then the information contain in concepti is given by here n representing refined sentences since entropy is given by the concept that has more negative value contains more information so we will choose for sentence selection and sentences are selected from this in an iterative way till required length summary achieved for given an example the order of sentences to include in the summary will be resolve a tie between two sentences concepts we are using first come first serve fcfs approach to maintain the diversity we have set a similarity threshold to add the next sentence in the summary the similarity may be user defined we are using cosine similarity which is given by by experimenting we have found if is too small then summary contains too much diversity resulting low performance so we have to set it carefully in the experiments we chose we are interested to find cosine similarity between summary sentences and newly selected sentences next to add in summary here to m representing the length of master sentence i j for ith and jth sentence if total n sentences lsass model lsa based sentence selection lsass is also based on entropy based in this proposed model we have to find sentences that contain the highest information a sentence with high entropy is showing more degree of freedom that means the particular sentence is related to the number of dimensions concepts sentences in sentences are represented using sign corresponding label in rectangle diagrams words are represented by sign with the corresponding label in rectangle diagram below we are presenting the proposed algorithm to n input document d the content of words sentences paragraphs predefined length of summary initially similarity threshold output summary decompose the given document d into sentences sn extract keywords and use these keywords to form the master sentence for given document d construct matrix a term sentence matrix perform the svd on a as follow for every sentence in reduced k by using preprocessed vt and s compute matrix multiplication w vt column of w is representing sentences and row representing concepts preprocess w select only core sentence by strikeout negative related sentences for each column in w compute where representing the probability of a sentence to appear in latent i i r r reduced space find the information contains in each sentence using shannon s entropy method as follow select next most informative sentence sj m if l lmax similarity si sj summary sentences and sj sentence to be added in the summary add sj to the summary and l goto else goto in to are concerned about preprocessing and svd decomposition in the next step a new matrix w is obtained by combining the right decomposed matrix v and covariance matrix w vt and further reducing the dimension to the rows of matrix w signify concepts in the document and the column signifies the sentences of the document in we are representing the score of sentences to corresponding to to here in step the matrix is further reduced by eliminating the rows containing negative values here the negative values signify less informative sentences shown in in we are interested to find the probability of concepts to appear in sentences we are calculating the probability value of which denotes by j is representing the probability of a concept i to appear in sentence j shown in for each sentence the sum of the probability of concepts is as here xi denotes ith concept in we are willing to find out information contained in a sentence corresponding to all concepts it is computed by summing the columns corresponding to the sentence the resultant value signifies the information contained in the sentences in the sentence is extracted based on information content and added in the summary this step will continue until similarity threshold criteria hold and required summary length constraint is satisfied if a sentencej related to concepti with probability then the information contained will be given by sentences sentences w in reduced space w is processed by keeping only positive related sentences and column sum is find to measure the probability sentences entropy information contained by each sentence in reduced space r we can re write as so entropy can also give by the sentence that contains more information highest entropy extracted and added in summary after measuring the similarity between summary sentences and selected sentences so we will include in summary and then up to the required length summary is achieved for given an example the order of sentences to include in the summary will be to resolve a tie between two sentences we are using first come first serve fcfs approach to maintain diversity we can set a similarity threshold to add the next sentence in summary if the similarity between previous sentences and the next sentence is less than similarity may be user defined we are using cosine similarity which is given by this formula in proposed model to measure redundancy in summary the text is a collection of units of words the summary has important characteristics like coverage redundancy and summary length coverage and redundancy are reciprocal to each other when we increase other another reduces a lot of research like alguliev aliguliyev isazade alguliev aliguliyev isazade alguliev et al happen in this direction to obtain an optimal solution using multi objective optimization in our approach we are generating just word summary because words reference summary is available for each file in this section we are calculating information based on n gram content entropy information from gram to gram denotes redundant information in text files and it measures using the by experiments we find that to reach some conclusion we have to consider only if if we consider for then gram will dominate in a text file because of trivial presence the experiment is done on files so to find average information redundancy in the dataset we are using where s is a total number of documents gram s representing a count of n grams for sth sentence experiment and results in this section we are presenting a wide number of experiments done by us following in the first experiment we have implemented all previously proposed models and that are presented in section and compare their performance in this experiment its comparative performance in shown in among these five proposed models highest efficiency rouge score is given by that is proposed by ozsoy et al and lowest ranked model is by gong liu since all five models are based on lsa and it is assumed that concepts are independent of each other so extracted sentences also best representative of the document even two concepts are independent there may be a number of units words are common to each other if we are interested in more coverage or less redundancy we need to update our sentence selection approach this is done in experiment second using and in section and original r l r w r s r su rouge score of previously proposed models in the second experiment to get more diversity or less redundancy we have used proposed proposed in section to select k sentences from the same concepts we set first we will select a concept then we select only two sentences from each concept one is most related and one is the most unrelated chosen concept latent this approach is an extension of and follow section so we presented our approach as and using and we can say that our proposed approach is not only performing better to respective model and but also is performing better compared to all previous models results are shown in and is showing how much gain in performance by using the proposed approach rouge score r l r w r s r su performance of proposed models with proposed or proposed showing improved performance using our proposed proposed next third experiment which is based on entropy based approach second approach as defined in section and section have two cases as using vt which is termed as using svt which is termed as the sole objective of this model is to find a concept that is more related to all the sentences and summary sentences are extracted from this concept latent in continuation with the third experiment the next model is that is termed as the objective of this model is to find sentences which are strongly related to all the concepts and that is decided based on entropy all entropy based model performance is shown in and we are getting improved results previously proposed models exception case and exception case we are generating and evaluating only hundred word length summary since long sentences are representative of more information and extract long sentences so this is length dominating model hence performing better the entropy based model proposed by us section and section is not length dominating so the performance of is sometimes better to compare to our proposed approaches playing better role compare to based model but not always when the user is interested in length summary this is true that long sentence contains more information but when repetition selection of long sentences starts there is more chance to have more redundancy and less coverage therefore performance will negatively impact this is shown in the next experiment so if we want evergreen model from these existing choices better will be a selection of an entropy based model for summarization problem the performance of previous approaches and by incorporating entropy in these models is shown by the graph in rouge measure r l r w r s r su performance of model with entropy based model showing improved performance using our entropy based proposed experiment four is showing redundancy and information contained in different summaries generated by different previous proposed models and our proposed approaches since information is represented by words and in short is followed and we are showing that as we are increasing the number of words in a text rouge score also increasing sometimes it is reduced because of is increasing and reducing simultaneously follow rouge score formula given by shown by the pair in the bold red letter we represented information in terms of entropy given in and in the information score represented by and is proportional to the rouge score given in and which is obvious trivial case and we are trying to show that redundancy is also increasing in text along this which is not our target of summarizer system words l count cov generally by increasing summary length rouge score is also increasing and sometimes reducing in this experiment we are trying to measure the redundancy of information which is given by this equation is based on n gram count entropy function explained in section more score means more redundancy we are counting n grams only if gram the reason of this by experiment we find that if we take all gram then hard to reach any conclusion we are following a simple approach giving equal weight to all n grams rouge score for the entropy based system showing as the number of words increasing rouge score also decreasing but statistical as n go large like n then it will contain more information and if the respective count increase than redundancy will increase more coverage will be less from this experiment we find out that even is performing better follow but this contains more redundancy so low coverage shown in and model previously proposed models entropy based gold showing average information contains in summary generated by different systems model previous models proposed entropy based gold peer proposed summaries frequency gram gram gram gram gram gram gram gram gram gram gram gram average count average n gram presents in different systems generated summary since rouge score directly depends on gram if redundancy increases rouge also score increases from previously proposed models best performer has more redundancy more number of n grams hence this is performing better which is too much greater than peer gold summary n grams counts vs instead of this our entropy based proposed method is close to standard summary the closeness of two texts can find using where golden gram refers n gram present in gold summary and system gram refers to summary generated from different systems results are shown in and can be summarized in in this we are representing only rouge l score calculated for different described systems models and an average count of gram gram gram with from we can interpret different results like our proposed system entropy is performing better compare to other in term of rouge score the system proposed by ozsoy et al is performing at second place but redundancy is high compared to allotherrsystemsm from we can conclude that statistically our second entropy based proposed approach is closer to the gold peer summary in term of and and are performing low and redundancy also low rouge l gram gram gram model score model score model score model score model score gram rank summarization of m stands for model concluding remark in this work we have proposed two new approaches three new models for automatic text document summarization and a novel entropy based approach for summary evaluation both the approaches for summary generation is based on svd based decomposition in the first approach we are using right singular matrix vt for processing and selects a concept one by one top to bottom till required previous approaches are focused on selecting only one sentence of the highest information content in our first approach we are selecting two sentences each concept such that is highest related to concept and least related to the concept this approach is based on assumption that by doing this we are covering two different topics as a result it leads to more coverage and diversity the second approach is based on entropy which formulate into two different models and in first we are selecting a highest informative concept and from that concept we are selecting summary sentences in repeatedly we are selecting highest informative sentences a sentence which is related to all the concepts with high score the advantage of the entropy based model is that these are not length dominating models giving a better rouge score statistically closer to the gold summary during experiment we have found out that rouge score depends only on the count of matched words on increasing the summary length sometimes rouge score decreases and on increasing redundancy rouge score also increases we have pointed out that rouge score does nt measure redundancy count matched sentences we have also realized the need for new measure for summary evaluation that provide a tradeoff between redundancy countmatch and entropy based criteria has proposed during testing of new proposed measure on different summary generated by previous models and our proposed models we have found that our entropy based summary is closer to standard summary from the experiment results it is clear that our model works well for summary evaluation especially for higher length summary because as summary length increases redundancy also increases and in this measure we are measuring redundancy currently we are giving equal importance to all n gram but theoretically and practically we should give more weight to higher n gram because of high redundancy of information in case of repetition chapter lexnetwork based summarization and a study of impact of wsd techniques and similarity threshold over lexnetwork introduction in this chapter we are presenting a lexical chain based method for automatic text document summarization this work is divided into three objectives in the first objective we are constructing a lexical network in which nodes are representing sentences and edges drawn based on lexical and semantic relations between these sentences in the second objective after constructing the lexical network we are applying different centrality measures to decide the importance of the sentences sentences have extracted and added in the summary based on these measures the third study third objective done in this work is based on wsd since wsd is an intermediate task in text analysis so we are presenting how the performance of centrality measure is changing over the change of wsd technique in an intermediate step and cosine similarity threshold in post processing step related work morris hirst have proposed a logical description for the implementation of lexical chain using roget thesaurus and barzilay elhadad had developed the first text document summarizer using lexical chain their algorithm was exponential time taking silber mccoy had followed the research of barzilay elhadad for lexical chains creation and proposed a linear time algorithm for lexical chain creation according to kulkarni apte the concept of using lexical chains helps to analyze the document semantically and the concept of correlation of sentences using lexical chain text summary generation has three stages first step candidate word selection for chain building as noun verb second step is lexical chain construction and chain scoring model to represent the original document and the third step is chain selection and chain extraction for summary generation in literature chain scoring strategy is mostly based on tf idf the distinct number of words position in the text gurevych nahnsen have proposed a lexical chains construction in which candidate words are selected based on pos tagging of nouns and used gonzlez fort have used wordnet or eurowordnet lexical databases proposed an algorithm for lexical chain construction which is based on a global function optimization through relaxation labeling authors have categorized relations into an extra strong strong and medium strong relations chains are distinguished extracted based on strong medium and lightweight pourvali abadeh have proposed an algorithm for single document summarization based on two different knowledge source wordnet and wikipedia for words which are not present in the wordnet erekhinskaya moldovan have used different knowledge source wordnet wn extended wordnet xwn and extended wordnet knowledge base xwn kb gonzlez fort have used wordnet or eurowordnet lexical databases proposed an algorithm for lexical chain construction which is based on a global function optimization through relaxation labeling authors have categorized relations into an extra strong strong and medium strong relations chains are distinguished extracted based on strong medium and lightweight chen wang guan have used chinese wordnet kulkarni apte have used wordnet relations chen liu wang have used chinese language resources like hownet and tongyicicilin stokes have used lexical cohesion to generate very short summaries for given number of news articles they have used different relations and distinct weights are assigned like for extra strong relation repetition assigned strong relation synonym assigned strong relation hypernym hyponym meronym holonym antonym assigned medium strength relation assigned statistical relation assigned after that author identifies the highest scoring noun and proper noun chains using the above relations along with these works another closely related work has done by vechtomova karamuftuoglu robertson and ercan cicekli the author has considered keywords and sort version of document summary and proposed lexical chain for keyword extraction steinberger poesio kabadjov jez have used anaphoric information another linguistic task in latent semantic analysis and show anaphoric task giving better results for summarization purpose chandra shekhar yadav sharan yadav et al have used position tf idf centrality positive sentiment and negative sentiment based semantic feature centrality yeh yeh ke yang meng have used static information like position positive and negative keyword centrality and the resemblance to the title to generate an extractive summary along with this lsa and ga also integrated into their work doran stokes carthy dunnion also have proposed lexical chain based summarization in which the chain score is calculated based on the frequency and relationship present between words chain member using wordnet the score of word pairs depends on the sum of the frequencies of the two words and multiplied by the relationship score between them synonym relations have assigned value specialization or generalization and part whole or whole part proper nouns chain scores strength depends on the type of match assigned for an exact match for a partial match and for a fuzzy match sentences have ranked according to the sum of the scores of the words in each sentence and later extracted medelyan has presented a graph based approach for computing lexical chains where nodes are document s terms and edges reflecting some semantic relations between these nodes based on graph diameter given by the longest shortest distance between any two different nodes in the graph concept strong cohesive weakly cohesive and moderately cohesive chains are computed plaza stevenson daz have proposed a summarization system for the biomedical domain that represents documents like a graph designed from concepts and relations present in the umls metathesaurus unified medical language system jdi algorithm and personalized pagerank algorithm used for wsd chen huang yeh lee have represented the document in graph like structure in which node are sentences and edges are drawn by topical similarity and the random walk applied for summary generation in the same way xiong ji have proposed a novel hypergraph based vertex reinforced random walk gonzlez fort have used wordnet or eurowordnet lexical databases proposed an algorithm for lexical chain construction which is based on a global function optimization through relaxation labeling authors have categorized relations into extra strong strong and medium strong relations chains are distinguished extracted based on strong medium and lightweight plsa lda approaches have been proposed by steinberger al chiru rebedea ciotec study summarization result based on lsa lda and lexical chain two most important results from them are lsa and lda have shown the strongest correlation and the results of the lexical chain are not much correlated neither with lsa nor lda therefore according to them during performing semantic analysis for different nlp applications lexical chains may be used as complementary to lsa or lda yeh have used static information like position positive keyword and negative keyword centrality and the resemblance to the title to generate an extractive summary along with this lsa and ga are also integrated into their work steinberger al have used anaphoric information another linguistic task in latent semantic analysis and showed anaphoric task giving better results for summarization purpose background lexical network construction is the first target of this work and during the construction of lexnetwork wsd is an intermediate task after construction of network we have applied different centrality measures to score the sentences and later extract in this section we are presenting wsd centrality techniques and lexalytics algorithm used in this work word sense disambiguation wsd in nature various human language exists and one common problem for all languages is word ambiguity multiple sense of a word according to the context in which words occurs wsd word sense disambiguation is an intermediate task and technique in nlp applications to computationally decide which sense of a particular word is active by its use in a particular sense erekhinskaya moldovan have studied the ambiguity of word originates from a variety of factors like the approach of representation of the word sense and dependency of knowledge source used like wordnet roget thesaurus extended wordnet knowledge source without knowledge either internal or external it would be impossible for both humans and machines to find the correct sense wordnet lists five senses for the word pen pen a writing implement with a point from which ink flows pen an enclosure for confining livestock playpen pen a portable enclosure in which babies may be left to play penitentiary pen a correctional institution for those convicted of major crimes pen female swan in this work we are using a variant of the lesk algorithm for wsd proposed by lesk the lesk algorithm is based on the presumption that the given words to disambiguate the sense and neighborhood of this word will tend to share a common topic since words are ambiguous and their sense relies on knowledge source so in this work we are using three different algorithms for wsd these are lesk adapted lesk and cosine lesk simple lesk algorithm a simplified version of the lesk algorithm is to compare the ordinary dictionary definition called gloss found in a traditional dictionary such as oxford advance learner of an ambiguous word with the terms contained in its neighborhood initialization put words into word vector for which disambiguate sense need select context window a neighbor of the word for every possible sense of the word to disambiguate one should count some words that are in both neighborhoods of that word and in the dictionary definition of that sense the sense that is considered best possible sense which has the highest number of overlapping counts the advantage of this technique is that this is non syntactic and not dependent on global information the disadvantage of this does use previous sense used for next word it will compute again so that it is time consuming and performance varies according to the selection of neighboring words adapted lesk cosine lesk adapted lesk algorithm has been proposed by banerjee pedersen simple lesk algorithm uses knowledge sources as a standard dictionary for gloss definition wherein adapted lesk algorithm in place of standard dictionary author using electronic database wordnet wordnet provides a rich hierarchy of semantic relations the cosine lesk has proposed by tan is a vector space distributional space version to calculate lesk overlaps also known as signatures in the original lesk paper for example let us take and here the ambiguous word is deposit with two different contexts as below in and sentence i got to the bank to deposit my money financial institute and withdraw and transact money the strip by river where soil deposit resident instead of a global vector distributional space built from a corpus we can do it locally for the given ambiguous word so given the meaning word deposit and the context sentence we have the vocabulary financial institute deposit withdraw transact money strip river soil resides i go and the following vector will be assigned to meaning meaning and then and is computed the highest similarity would give us the closest meaning of the word deposit given the context sentence this is cheaper to compute and there no need for a corpus and also it can be done on the fly with wordnet without storing the vector space in memory before disambiguating our text but the lack of memory usage would also mean that meaning vectors are computed and recomputed as we disambiguate and that might be wasteful regarding computing resources and time lexical chain a lexical chain is a sequence of related words in writing spanning short or long distances generally limited to next few lines a lexical chain is independent of the grammatical structure of the text and in effect it is a list of words that captures a portion of the cohesive structure of the text applications of lexical chains are keyphrase extraction keyword extraction event detection document clustering text summarization this can be explained by the following and the showing a lexical chain created for a document available at elhadad example rome capital city inhabitant example wikipedia resource web example of lexical chain creation dataset available elhadad mechanism of how to create has been given in barzilay elhadad chains are constructed between words so it lost the meaning of the text and grammatical structure there is no way to find out that particular word belongs to how many sentences to sort out these issues we have proposed a lexicalnetwork that overcome all these problems lexalytics algorithms lexalytics offers text mining software for companies and enterprises around the globe which is available at this system can access through java or python api and excel add ons text analytics tasks like named entity extraction finding the meaning of text via classification and tagging sentiment analysis context finding and summary generation lexical chaining is an intermediate step in mentioned applications various lexalytics algorithms heavily rely on it like summarization in lexical chaining nouns are selected for lexical chain construction if the nouns are related to each other we can find that the conceptual lexical chain in the given content even when many other unrelated sentences separate those sentences the score of all lexical chain is based on the length of the chain and the relationships lexical or semantic between nouns in this system location based features also considered deciding sentence priority because of an initial sentence always more informative relation used for chain construction purpose are same word antonym synonym meronym hyper holonyms in this system even if those sentences are not adjacent to each other in the text they are lexically related to each other and can thus be associated with each other during summary generation it extracts best sentences from the chain and shifts out nonessential sentences from the summary centrality a degree centrality centrality scores in graph theory or social network analysis to decide the relative importance of a node one existing approach is centrality based measures some used approached are mentioned by us the first and simplest type of centrality is degree centrality venables team if we define a graph e where is some nodes and stands for the number of edges then on graph g degree centrality of a node v can be defined as the number of edges incident towards a node graph g may be directed and undirected if g is di graph then two type of degree is defined as in degree and out degree for a given node v v v in degree of a node v is some links approaching towards the v and out degree of v is the counts of links that the node v directs towards other nodes v v the degree centrality of a vertex v for a given graph g is defined by general degree centrality and normalized centrality n is the total number of vertices in two graphs are shown with normalized degree deg showing normalized degree centrality for an undirected graph b eigen value centrality in linear algebra an eigenvector of a square matrix let a can define as a vector that does not change its direction under the associated linear transformation alternatively it can be defined as if v is a vector then it is an eigenvector of a square matrix a if av is a scalar multiple of v and this condition can be written as the following where is the scalar known as the eigenvalue associated with the eigenvector a more advanced version of the degree centrality is eigen vector centrality degree centrality of a node is just based on a simple count of the number of connections where eigenvector centrality acknowledges that not all connections are equal or in other words the eigenvector centrality defined in this way accords each vertex a centrality that depends both on the number of connection degree of a node and the quality of its connections in social life this is followed connections to people who are themselves influential will lend a person more influence compare to in connections with less influential persons if we denote the centrality of ith vertex by xi then we can allow for this effect by making xi proportional to the average of the centralities of ith network neighbors newman represented in c closeness centrality in graph theory closeness is a one of the centrality measures defined for a vertex in the graph g v e vertices which are shallow short geodesic distances to other vertices have given higher closeness value closeness can be defined in a various way as defined by freeman the closeness centrality of a vertex is defined by the inverse of the average length of the shortest paths to from all the other vertices in the graph given below by d alpha centrality if we denote an adjacency matrix a in which aij equivalent to aij means i contributes in j s status and if is a vector of centrality scores then this can be written by in alpha centrality status score of an individual can be seen as a function of the status of those who choose him given by upper which can rewrite as at is denoting the transpose of matrix a in short alpha centrality s status is a linear function of another node to which node is connected this could be understood with two examples first in a community power study an actor s status value is increased much if he she nominated from those who themselves are receiving more nominations compare to other same as an upper example in a school also a student s popularity is increased more by receiving voted from other students who are themselves more popular other students one solution to this problem is solution eigenvector centrality but this does produce justifiable results for the networks given in and with ax ax graph showing a graph with some nodes and edges a possible solution of this kind problem is possible if we allow every individual here students with some score that does nt depend on its connection to others like in a class or school each student s popularity that depends on its external status characteristics let e be a vector of these exogenous sources of status or information then we can replace the with a new in which the parameter reflects the relative importance of endogenous versus exogenous factors in the determination of centrality from this has the matrix solution in s for any value of position of labeled vertex a is the most central in the order is xa xb xc xd xe for any the alpha centrality based measure is almost identical to the measure has proposed by katz katz has suggested that influence could measure by a weighted sum of all the powers of the adjacency matrix a here powers of a gives indirect paths connecting points e betweenness centrality in a graph e betweenness centrality given by freeman a measure of a vertex is based on the shortest path metric vertices v v those occur in some shortest paths between other tx vertices have higher betweenness centrality compare to those do nt for a graph g with vertices the betweenness for a vertex is computed by in denotes the counting of shortest paths from node s to node t and is the total number ofpassedst paths from node s to node t that is passing via a vertex f bonacich s power centrality bonpow let given matrix a is an adjacency matrix then bonacich s power centrality has proposed by bonacich is defined by where is an attenuation parameter from is the reciprocal of the which is the largest eigenvalue of adjacency matrix a this is to within a constant multiple of the familiar eigenvector centrality score for other values of else than the behavior of the measure is quite different in particular gives positive and negative weight to even and odd walks respectively as can be seen from the series expansion in which converges so long as holds from venables team the magnitude of controls the influence of distant actors on ego s centrality score with larger magnitudes indicating slower rates of decay high rates hence imply a greater sensitivity to edge effects interpretively the bonacich power measures correspond to the notion that the power of a vertex is recursively defined by the sum of the power of its alters the power exponent controls the nature of the recursion involved positive values imply that vertices become more powerful as their alters become more powerful as occurs in cooperative relations while negative values imply that vertices become more powerful only as their alters become weaker as occurs in competitive or antagonistic relations the magnitude of the exponent indicates the tendency of the effect to decay across long walks higher magnitudes imply slower decay one interesting feature of this measure is its relative instability to changes in exponent magnitude particularly in the negative case g hub authority hyperlink induced topic search known as hubs authorities developed by kleinberg is a type of link analysis algorithm that rates web pages a set of web pages can consider as a connected graph the algorithm assigns two different scores hub and authority score for all pages the authority score estimates the value of the content of the particular page on a node in the graph and its hub score estimates the value of its links edge in the graph to all other pages in other words this can interpret as that a good hub represents a web page that points too many other pages and a good authority represent a page that was linked by many different hubs the hits algorithm relies on an iterative method and converges to a stationary solution each node i in the graph is assigned two non negative scores an authority score xi let and a hub score yi let the xi and yi are initialized with any arbitrary nonzero value and scores will update according to iterative ways present in and later the weights are normalized as h subgraph centrality let g v e is a graph then any subgraph e will hold this v v and e estrada rodriguez velazquez a centrality measure which based on the participation of each node in all subgraphs in the network in their work smaller sub graph gives more weight compared to larger ones which makes this measure appropriate for characterizing the network motifs define as designates those patterns that occur in the network far more often than in random networks with the same degree sequence by freeman degree centrality can be considered like as direct influence but this is unable to cover long term relations or in another word indirect influence in the network katz there is another centrality based measures like betweenness centrality closeness centrality but these measures are justifiable in a connected network because the path distance between unconnected nodes is not defined eigenvector centrality ec also do nt depend on to paths ec measure simulates a mechanism in which each node affects all of its neighbors simultaneously bonacich but the eigenvalue based centrality ec measure can not consider as a measure of centrality in which nodes are ranked according to their participation in different network subgraphs let in which is regular graphs with eight vertices vertices set is forming part of a triangle part of three squares and form part of only two and the rest do not form part of any these groups are distinguishable according to their participation in the different subgraphs although ec can not distinguish them showing eight node with degree regular graph subgraph based centrality is based on the number of closed walks starting and ending at the node closed walks are weighted such that their influence on the centrality decreases as the order of the walk increases in this technique all closed walk is associated with a connected subgraph which points out that this measure counts the times that a node takes part in the differently connected subgraphs of the network here smaller subgraphs have higher importance corresponding to larger the subgraph centrality is represented via graph angles as noticed by estrada velazquez given in according to if the graph is walk regular then every node has an identical subgraph denote the nodes of the graph g by let m where m be the distinct eigenvalues of given adjacency matrix a with multiplicities respectively and be the basis of the eigenspace corresponding to the eigenvalue i above defines the angle alpha corresponding to the node u of g and the eigenspace according to gleich due to several features of rage rank algorithm simplicity guaranteed existence generality uniqueness and fast computation make this technique applicable far beyond its origins in google web search proposed by page brin motwani winograd pagerank is used as a network centrality measure proposed by koschtzki et al and our work also motivated from this work if we assume that a given page and other pages tn which pointing towards page called citations the parameter d is a damping factor which can be set between to usually set to is defined as the number of links going out of page the pagerank score of page is given by centrality i pagerank our used variation is given by pagerank technique forms a probability distribution over web pages or in the graph such that the sum of all web pages pagerank score will be one proposed work in this section we are presenting just a layout of our which is four step process including preprocessing lexical network creation sentence raking computing importance of each sentence and summary generation based on required length the first step is preprocessing this step is required for proper sentence read because we can not set an arbitrary delimiter for a regular expression like just or a dot followed by the new line new tab to get the better accuracy we transform some special form like to the deleted just bank space in the form of the second step takes the input from the output of the first step it reads sentences one by one and creates a lexical network among all sentences detail about this is presented in algorithm in next section procedure procedure procedure steps in summarization in the third step we are applying different graph based centrality measures for finding informative sentences and in the last step we set some constraints like threshold summary length for summary generation according to a user request algorithm in this section we are presenting algorithm for a summary generation in three main steps basically four as per previous figure but in this section step and step are merged in step of this algorithm algorithm lexical network construction in this section we are presenting the summarization algorithm proposed by us our work comprises three steps in the first step is preprocessing done which is a most important step in text mining that can improve harm the efficiency of the algorithm in step second we are creating lexical network meanwhile wsd is done with the help of the lesk algorithm and in step sentence scoring and sentence extraction was done input collection of sentences output extracted sentences procedure preprocessing proper fragmentation of sentences set proper delimiter tagging of sentences using pos tagger procedure in the document we are given some sentences if we apply any regular expression for proper bifurcation of sentences is not proper possible because of some words like so for proper sentence extraction we are doing some preprocessing and then we are tagging these sentences using nltk natural language tool kit parser the result of this step is stored in list format in sentence variable which can be randomly accessed like array procedure lex network creation step initialize matrix step for ith i for each actual sentence extract nouns verb called modified sentences contains only significant units for ith modified sentences locating corresponding same sentence takeith unit to word ith unit correct sense word ithactual sentence for modified sentences takejth unit j if relation correct sense jth unit then procedure sentence scoring and extraction paired sentence centrality score sort paired sentences based on centrality score set similarity measure threshold and summary extract ith sentence from pool after step diversity maintained pick jth sentence from summary pool ith add ith sentence to summary procedure in this procedure we are constructing a lexical network of nn matrix where n is some sentences we are picking each sentence on by one and extracting only nouns like a proper noun verbs from this sentence because only these provide information and this is added to the modified sentence example from duc healthcare data set file number actualsentence an administration task force has been studying possible health care solutions but has yet to offer a comprehensive proposal taggedsentence administration nn task nn force nn studying vbg possible jj health nn care nn solutions nns offer vb comprehensive jj proposal nn modifiedsentence administration task force studying health care solutions offer proposal the actual lexical network is construed for modifiedsentences here we are finding is any relation either lexical or sematic is present between two modifiedsentence to find the relation present between sentences we need to disambiguate the sense of the word this is done with the help of lesk algorithm here lesk algorithm takes two arguments one word to disambiguate sense and other is ith actual sentence based on that sense this algorithm finds hyper hypo synonyms meronym and antonym relation present in other remaining sentences at a time only one relation will be available and according to us every relation is given the same weight so we will assign an edge between sentences and each time increase by one if more relation is present between sentences in we are showing lexical network connection between sentences from duc dataset last sentence is empty so there is neither lexical nor semantic similarity available so corresponding to to entries are zero vice versa corresponding to showing a graphical view of this lexical network in which node are sentences and edges are a corresponding total similarity note when we will split any text with delimiter dl if the total number of delimiter are n then after split operation number of segmentation will appear so here showing all sentences of from duc dataset for the creation of network washington health and human services secretary louis sullivan called for a summit with the chief executives of major insurance companies to discuss ways of paring the administrative costs of health care but in a speech here dr sullivan indicated that the bush administration is likely to put forth a broad legislative proposal to overhaul the country health rather he advocated focusing on ways to improve the current system administration health officials said the meeting with insurers tentatively scheduled for nov is likely to commence a series of discussions with players in the nation billion health care system on problems of cost and access administrative costs like excessive paper work have burdened the health care system with billions of dollars in unnecessary costs many observers care system believe some studies have put the price at more than billion a year but hhs officials believe it is more like billion to billion while numerous health care reform proposals have been introduced in congress this year mostly by democrats the comments of dr sullivan and other hhs officials yesterday suggest the bush administration is interested in striving for systematic revision and will push for limited fixes and incremental changes i will not propose another grand sweeping speculative scheme dr sullivan said rather i want a public debate to focus on some immediate practical options that address our most urgent healthcare concerns some of the options he said are making health insurance more affordable to small businesses easing barriers to high quality cost effective managed and coordinated care researching the effectiveness of various medical procedures to encourage the use of the most cost effective ones altering the tax code to increase consumer awareness of the true cost of health care and distribute current tax subsidies more equally among other things administration officials have been looking at the possibility of limiting the tax exemption for employer paid health insurance premiums increasing the availability of primary care to the neediest people reducing the administrative costs of health care the s spends more per capita on health care than any country yet more than million americans lack health insurance in january president bush ordered an administration task force to study problems of health and access but it has yet to propose solutions dr sullivan repeated his dislike for the two most widely discussed health care revision proposals establishing a nationwide federally sponsored health care system similar to canada or mandating employers to either provide basic health benefits to workers or pay a tax to finance a public health plan such approaches he said would be inflationary smother competition and lead to rationing and waiting lists he said experimentation in health care reform should be left to the states local solutions for local problems should be our working philosophy as should learning from local mistakes in order to avoid harm to the nation as a whole he said showing lexical and semantic relations present between sentences sent in graphical form directed graph refers to sentence from and from corresponding weighted edges between two sentences are shown here procedure after getting a lexical network we are applying centrality based measures for sentence scoring sentence with high centrality score is decided more important compared to low centrality score to extract sentence for summary generation we decide cosine similarity threshold we add the next sentence in summary if x x summarysentence cosine similarity x this work is extended with the impact of different wsd techniques and different similarity thresholds in the post processing step to find the similarity between two sentences we are using cosine similarity if we have two vectors a and b each of length n then cosine similarity is given by ai and bi are a component of vector a and b this similarity measure also used in yadav al chandra shekhar yadav sharan in graph theory or network analysis is to decide the relative importance of a node or to find central figures one existing approach is centrality based measures for example in the social network to decide the importance of a person can do with the help of centrality some based measures are degree centrality eigenvector centrality katz centrality pagerank betweenness centrality closeness centrality alpha centrality bonpow centrality hub authority subgraph centrality detailed description of different centrality measure used by us are experiments and results in this section we are presenting different experiments performed by us along with result analysis first we are presenting performance of different centrality measure over selected lesk algorithm with different threshold then impact of wsd technique and threshold for different centrality measures performance of different centrality measure over selected lesk algorithm with different threshold in this section we are showing the performance of different centrality based measures over fixed wsd algorithm like adapted lesk cosine lesk and simple lesk respectively and different cosine similarity threshold in post processing we have selected a cosine similarity threshold range due to the limitation of space and sufficient need we are interested to show result only for and threshold adapted lesk th in we are using adapted lesk algorithm for wsd for lexical network creation and in the third step we set a cosine similarity threshold of after lexical network construction procedure in algorithm we have applied different centrality measure like alpha centrality coded as for different values from the corresponding graph of in we can interpret that in alpha centrality when performance is continuously increasing but when then the performance of summarizer system is strangely reduced eigen value and hub authority are performing equal with the highest performance and second highest performance is done by subgraph based centrality performance of different centrality measures using adapted lesk as wsd and similarity threshold score bonpow between close eigen page sub ness ness value authority rank graph rouge l rouge w rouge s rouge su performance evaluation using rouge score of different centrality measures adapted lesk as wsd and similarity threshold adapted lesk th in we are using adapted lesk algorithm for wsd for lexical network creation and in the third step we set with similarity threshold after lexical network construction procedure in algorithm we have applied different centrality measure like alpha centrality coded as for different values from the corresponding graph of in we can interpret that in alpha centrality when performance is continuously increasing exception but when then the performance of summarizer system is reduced compared to this eigen value and hub authority are performing equally with second highest performance and subgraph based centrality achieves the highest performance performance of different centrality measures using adapted lesk as wsd and similarity threshold score between bon close eigen page sub ness pow ness value authority rank graph rouge l rouge w rouge s rouge su performance evaluation using rouge score of different centrality measures adapted lesk as wsd and similarity threshold cosine lesk th in we are using cosine lesk algorithm for wsd for lexical network creation and in the third step we set with cosine similarity threshold after lexical network construction procedure in algorithm we have applied different centrality measure like alpha centrality coded as for different values from the corresponding graph of and we can interpret that in alpha centrality when performance is continuously increasing exception but when then the performance of summarizer system instantly reduced eigen value and hub authority are performing equal third highest with page rank second highest and subgraph based centrality top performance but the thing here to notice that all these measures are performing almost same performance of different centrality measures using cosine lesk as wsd and similarity threshold score bonpow between close eigen page sub ness ness value authority rank graph rouge l rouge w rouge s rouge su performance evaluation using rouge score of different centrality measures cosine lesk as wsd and similarity threshold cosine lesk th in we are using cosine lesk algorithm for wsd for lexical network creation and in the third step we set a cosine similarity threshold after lexical network construction procedure in algorithm we have applied different centrality measure like alpha centrality coded as for different values from the corresponding graph of and we can interpret that in alpha centrality when performance is continuously increasing exception but when then the performance of summarizer system is strangely reduced eigen value and hub authority are performing equal with the highest performance and second highest performance is measured by subgraph based centrality performance of different centrality measures using cosine lesk as wsd and similarity threshold score bonpow between close eigen page sub ness ness value authority rank graph rouge l rouge w rouge s rouge su performance evaluation using rouge score of different centrality measures cosine lesk as wsd and similarity threshold simple lesk th in we are using simple lesk algorithm for wsd for lexical network creation and in the third step we set a cosine similarity threshold of after lexical network construction procedure in algorithm we have applied different centrality measure like alpha centrality coded as for different values from the corresponding graph of in we can interpret that in alpha centrality when performance is continuously increasing exception but when then the performance of summarizer system is strangely reduced eigen value and hub authority are performing equal with third highest performance pagerank gives the second highest performance and subgraph based centrality measures the highest performance performance of different centrality measures using simple lesk as wsd and similarity threshold score between bon close eigen page sub ness pow ness value authority rank graph rouge l rouge w rouge s rouge su performance evaluation using rouge score of different centrality measures simple lesk as wsd and similarity threshold simple lesk th in we are using simple lesk algorithm for wsd for lexical network creation and in the third step we set a cosine similarity threshold after lexical network construction procedure in algorithm we have applied different centrality measure like alpha centrality coded as for different values from the corresponding graph of in we can interpret that in alpha centrality when performance is continuously increasing exception but when then the performance of summarizer system is strangely reduced eigen value and hub authority are performing equal with third highest performance pagerank measures second highest performance and subgraph based centrality measures the highest performance performance of different centrality measures using simple lesk as wsd and similarity threshold score between bon close eigen page sub ness pow ness value authority rank graph rouge l rouge w rouge s rouge su performance evaluation using rouge score of different centrality measures simple lesk as wsd and similarity threshold impact of wsd technique and threshold for different centrality measures in this section we are presenting different wsd technique and different threshold impacts in summarization over the subgraph based centrality here code stands for adapted lesk technique for cosine lesk and technique for simplified lesk algorithm where threshold means cosine similarity threshold and means cosine similarity threshold is subgraph based here we are presenting the performance of subgraph based centrality in different conditions subgraph based centrality put more emphasis on small subgraphs and this covers long term relations here we are presenting the subgraph based centrality measure with different wsd techniques and thresholds from and we can conclude that simple wsd technique adapted lesk and cosine lesk with threshold is giving a better results followed by simple lesk adapted lesk cosine lesk with threshold impact of wsd technique and similarity threshold over subgraph based centrality score rouge l rouge w rouge s rouge su impact of wsd technique and similarity threshold over subgraph based centrality eigenvalue centrality based analysis eigen value centrality depends on both numbers of connections and quality of the connection from and we can conclude that simple wsd technique with threshold cosine lesk adapted lesk and simple lesk but for threshold cosine adapted simple lesk is performing in an order higher to lower impact of wsd technique and similarity threshold over eigen value based centrality score rouge l rouge w rouge s rouge su impact of wsd technique and similarity threshold over eigen value based centrality page rank page rank is simple ensure the guaranteed existence of a solution and it is fast in computation it forms a probability distribution over a network in our experiment we have set dumping factor from and we can conclude that simple wsd technique with threshold simple cosine lesk and adapted lesk but for threshold adapted lesk simple lesk and cosine are performing in order highest to lowest impact of wsd technique and similarity threshold over page rank as a centrality measure score rouge l rouge w rouge s rouge su impact of wsd technique and similarity threshold over page rank as a centrality measure bonpow in bonpow centrality measure the importance of a node is decided by alters from and we can conclude that simple wsd technique with threshold adapted lesk cosine lesk and simple lesk but for threshold adapted lesk simple lesk and cosine are performing in order highest to lowest impact of wsd technique and similarity threshold over bonpow based centrality score rouge l rouge w rouge s rouge su impact of wsd technique and similarity threshold over bonpow based centrality betweenness betweenness centrality measures the importance of each node based on a factor of participation of node between shortest paths any nodes in a network from and we can conclude that adapted lesk algorithm with a threshold of is performing better than adapted lesk algorithm with threshold cosine lesk simple lesk cosine lesk simple lesk threshold impact of wsd technique and similarity threshold over between ness centrality score rouge l rouge w rouge s rouge su impact of wsd technique and similarity threshold over between ness centrality closeness closeness centrality measure importance of node based on the shortest path from that node to all other nodes from and we can conclude that with threshold cosine lesk simple lesk and adapted lesk for threshold also cosine lesk simple lesk and adapted lesk are performing in order highest to lowest impact of wsd technique and similarity threshold over closeness centrality score rouge l rouge w rouge s rouge su impact of wsd technique and similarity threshold over closeness centrality alpha the alpha centrality score depends on the importance of neighboring nodes in we have shown different rouge score using alpha centrality to for ranking of sentences extraction and summarization from this is clear that when alpha is between to including this is unpredicted and no pattern is achieved but when alpha is to including rouge score is increasing with some exception at alpha but at efficiency is again reduced for all note for adapted lesk for cosine lesk for simplified lesk algorithm means cosine similarity threshold means threshold impact of wsd technique and similarity threshold over alpha centrality alpha comparisons with lexalytics algorithms semantrica lexalytics is a system available at that can access through java and python api for different tasks like summarization sentiment entity the summarization task of this system depends on lexical chain creation the lexical chain is created using semantic and lexical relations location also gives priority in this system in this section we are describing that where system proposed by us lead by semantrica even this is performing better many times and many times our proposed methods lead this is shown in and comparison of semantrica lexalytics algorithm with difference centrality based measure in which adapted lesk used as wsd showing only improved system semantrica adapted similarity threshold adapted similarity threshold lexalytics eigen value authority page rank subgraph authority page rank subgraph comparison of semantrica lexalytics algorithm with difference centrality based measure in which cosine lesk used as wsd showing only improved system semantrica score cosine lesk as wsd and similarity threshold cosine lesk as wsd and similarity threshold lexalytics eigen value authority page rank subgraph eigenvalue authority page rank subgraph score l w rouge s su l w rouge s su comparison of semantrica lexalytics algorithm with difference centrality based measure in which simple lesk used as wsd showing only improved system semantrica subgrap h authorit page ran subgrap h simple lesk as wsd and similarity simple lesk as wsd and similarity score sematrica lexalytics rouge rouge rouge rouge s rouge authorit y page ran eige n valu e eige n valu e y semantrica lexalytics vs our proposed system improved only overall performance from and this is clear that when alpha to we are unable to decide the performance pattern but when performance is increasing sometime exception is and again reduced at most of the time subgraph based centrality is giving a better result the reason of this subgraph measure gives importance to subgraph in the graph the smaller subgraph is given more importance and this is natural that in the large graph there will be some subgraphs cycles in we are showing some top performances of all three wsd techniques for different threshold here and in which subgraph based centrality is performing better it may look hard to interpret x axis for clarification on x axis different centrality based measure are shown stands for adapted lesk algorithm for cosine lesk and for simple lesk algorithm shown only stands for cosine similarity threshold and respectively for example very first entry on x axis is eigenvalue and means we are using adapted lesk algorithm for wsd and cosine similarity threshold set is using eigenvalue centrality measure same as highest performance is achieved by subgraph which can interpret as simplified lesk algorithm is used because of with similarity threshold and subgraph based centrality measured is used showing overall performance of some system for the comparative study in the next from we are presenting performance ranking based on all used centrality measure like subgraph eigen value total eight with different wsd and two different thresholds out of these eight centrality measures for threshold simple lesk times adapted lesk times cosine lesk time and similarly for threshold simple lesk times adapted lesk times cosine lesk times are performing best any centrality measure in the last row of we have presented alpha centrality s top measures out of total measures due to different centrality measures three different wsd technique and two different thresholds from this result we ca nt say that only particular type wsd is better all time but we can say that the threshold is better to compare to threshold and subgraph based centrality is better all other measures top performance by centrality based measures wsd technique used and similarity threshold centrality measure wsd lesk threshold reference performance ranking technique simple adapted cosine simple adapted cosine simple cosine adapted cosine adapted simple simple cosine adapted adapted simple cosine adapted cosine simple adapted simple cosine adapted adapted cosine subgraph eigenvalue and hub authority page rank bonpow between ness closeness alpha top best performances out of methods simple cosine simple cosine simple adapted cosine simple adapted adapted adapted simple cosine simple simple simple simple cosine simple cosine cosine adapted adapted adapted cosine adapted simple adapted adapted cosine concluding remark in this chapter we have presented the lexical network concept for automatic text document summarization which is a little bit different from previously proposed lexical chain based techniques in previous techniques author concentrate to create a number of lexical chains that creates ambiguity which chain to prefer even this problem efficiently can handle with chain scoring techniques still lexical chains have problem that if one particular word let donald trump is coming in two or more sentences then which sentence to prefer another problem with this technique that they consider only nearby sentences for a chain construction window of two or three sentences was selected so this is unable to handle long term relationship between sentences our lex net handle long term relationship between sentences nodes are scored and higher score sentence gave priority so both problems are handled in this way our lexical network is based on the number of lexical and semantic relations to decide the importance of sentences we have applied centrality based measure on lexical network we have found that subgraph based centrality is performing best among all since human language is highly ambiguous here english so we need to find a correct sense of a particular word in a given context the solution to this ambiguity problem done with simplified lesk cosine lesk and adapted lesk algorithm in this work we have done a study on the impact of wsd techniques and cosine similarity threshold due to space limitation we are showing results only with and the less value of represents more diversity compared to high valued more diversity if is less is not good for summary because it will maintain diversity but less relatedness between sentences which is harming good summary property for comparison purpose we have used semantrica lexalytics algorithm in which we have found that system proposed by us is working better many times especially when we are using subgraph based centrality from this work we have reached on number of conclusions like for alpha centrality when alpha is to inclusion the performance of summarizer system is arbitrary up and down but after that to inclusion for all centrality measures for different value of threshold performance is continuously increasing some time exception at and again reduced at second from a set of cosine similarity as we are suggesting that similarity threshold is better to get enough diversity and better summary as per rouge score third hub authority based ranking is the same as eigenvalue based centrality fourth subgraph based centrality measure is performing better to all the reason of this is the higher score for small subgraph which recognizes a small subgraph and this cover various subgraph can be considered as cluster like structure fifth we are not suggesting any particular wsd is better all the time chapter modeling automatic text document summarization as multi objective optimization introduction text document summary may be achieved from many features for examples location tf idf length of summary relatedness redundancy based scores these stand alone features will not generate a good quality summary so it can be hybridized to get better summary when we take care of multiple features then many solutions are possible and it is time taking hence we always try to optimize these functions in this work we are presenting multi objective function to get a better summary our optimization function maximizes diversity and minimizes redundancy here constraints are defined in terms of summary length since objective function and constraints are linear so we are using ilp integer linear programming to find an optimized solution which in section we are presenting related work that is categorized according to the research area and in section we are mentioning the mdr model baseline model used for comparison that sentences to extract for the summary literature work is proposed by mcdonald related work alguliev aliguliyev mehdiyev have proposed a multiobjective based approach for multi document summarization their model is based on reducing redundant information they formalize sentence extraction based multi document summarization as an optimization problem formulation of the model depends on three aspects redundancy content coverage summary length and shown in where si is ith sentence o is the mean vector of the collection d sn sim is cosine similarity xij is a binary variable which is one if a pair of sentences and sj are selected to be included in the summary else zero in l is required length summary li is the length of ith sentence tolerance is represented sign in this model denominator evaluating the correlation between the sentences and sj the numerator provides the coverage of the main content of the document collection while the denominator reduces the redundancy in the summary model is using sentence to sentence and sentence to document relations alguliev aliguliyev hajirahimova mehdiyev have proposed multi document summarization as an integer linear programming they state this model presented by which is suitable for single and multi document summarization this system optimizes three property redundancy relevance and length similarity function which is used here is based similarity for better calculation author calculate si as feature vector by removing stop words in combined objective function fcos and fngt stands for cosine similarity measure and ngt normalized google distance based similarity measure used in the objective function the model is represented by and the is representing objective of maximization of similarity may be drawn by cosine or ngt based between summary s and document d for length constraints representing combined function to maximize similarity between summary and documents but minimum among summary sentences is about length constraints and represents function f to optimize where alpha can be decided as per user preference f is maximized the combination of cosine and ngt based function here xij is a binary variable this optimization function constraint in is elaborated by following equations ijij sim in another work alguliev aliguliyev hajirahimova have optimized multi document summarization issues as an aspect of coverage and redundancy in this formulation coverage is represented by and redundancy is represented by and are representing a combined function let xi be binary variable xi when si is selected is representing length summary length constraints where l is summary length li is si sentence s length weight w is decided based on the importance of coverage or redundancy alguliev al have proposed constraint driven document summarization and the constraints are defined in term of diversity in summarization and sufficient coverage the model is formulated as a quadratic integer programming qip problem to solve the problem discrete pso is used diversity constraint is defined by equations and that covers the relation between a sentence and document and content coverage is defined by and this shows sentence to sentence relations where si ith sentence o is the mean vector of the collection d sn sim is cosine similarity xij is a binary variable which is one if a pair of sentences and sj are selected to be included to in summary else l is the required length summary li is length of ith sentence diver specifies high diversity in summary parameter cont we can control the content coverage in summary alguliev aliguliyev isazade proposed a modified median problem for multi document summarization this approach expresses sentence to sentence summary to document and summary to subtopics relationships they cover four aspects of summarization relevance content coverage diversity length and jointly optimizing all aspects formally their assumption is considering all vertices of a graph as potential medians p median is defined as a subset of vertices with p cardinality sj ith sentence dij distance between vertices i and j xij yi are binary variables o is representing the center of collection documents s is summary in this formulation the objective is to find out the binary assignment x such that high relevancy best content coverage and the less high diversity summary length is at most the p median problem can be expressed by equations the model used for summarization is formulated by and ijijdiversimssxxxii s is selected as baseline mdr model we compare our results with mcdonald work author used a multi objective optimization technique to optimize various criteria and for that they used ilp objective criteria taken by them is given in following for simplicity we represent the document collection simply as the set of all textual units from all the documents in the collection d tn where ti d iff ti dj we let s d be the set of textual units constituting a summary representing length constraints and and sure about the selection of binary variables xi band yij in implementation to determine relevance we are using pagerank and subgraph based centrality in objective function function relevance and redundancy can describe as follows relevance summaries should contain informative textual units that are relevant to the user redundancy summaries should not contain multiple textual units that convey the same n information length lmax summaries are bounded in length the similarity between two sentences is calculated by cosine similarity where n total number of the sentence importance si decide the importance of ith sentence higher value given higher importance li represent the length of ith sentences and lmax required max length summary the xi binary variable indicates whether or not the corresponding sentence is selected in summary in the same way the yi j binary variables indicating whether or not both and sj are included in the summary background in this section we are presenting some intermediate techniques followed in this work handling with text is not easy because output depends on how text is handled how preprocessing and processing done preprocessing is required before lex network lexical network creation some preprocessing task is to remove stop words convert to lower remove punctuation remove the number remove whitespaces stemming lemmatization and wsd word sense disambiguation here we are not presenting detailed wsd techniques and centrality measures because that are explained in the previous chapter introduction to linear programming is explained here linear programming linear programming lp also called linear optimization is a method to achieve the best outcome minimum cost maximum profit in a mathematical model in which requirements are represented by linear relationships in linear programming contains three parts decision variable optimization function to maximize or minimize and constraints while an integer programming ip problem is a mathematical optimization or feasibility program in which some or all of the variables are restricted to be integers in other words ilp in which the given objective function and the constraints other than the integer constraints are linear integer programming is np hard there is various method to solve ilp broadly this can be categorized into three parts exact algorithms that guarantee to find an optimal solution but time complexity may be exponential examples are cutting planes branch and bound and dynamic programming heuristic algorithms provide a suboptimal solution but without a guarantee on its quality running time is not guaranteed to be polynomial but provide a fast solution approximation algorithms provide a solution in polynomial time a suboptimal solution together with a bound on the degree of optimality let an example given by we want to maximize given subject to the constraints maximize z where y using graphical method solving equation in solution is shown at point graphical method of solution of and if the above optimization function is represented by then to solve this process should follow if the limitation of ilp is that when we want to solve the function given in with constraints in and then we need to expand per and constraints as and proposed multi objective optimization model in this section we are presenting a basic outline of our model this modeling is based on chapter based on lexical chain network outline of proposed model in figure a diagram is represented to show a brief introduction in this figure it is shown that how the output of the last chapter lexical network is used as input for this work lexical network creation and finding centrality score of each sentence is fetched from there here centrality sore is used to decide the importance of sentence edged present in lexicalnetwork is used to find relatedness we want to maximize information coverage and minimization of redundancy input sentences preprocessesing lexical network construction lexical network centrality multi objective optimization criteria sentences selection for summary document figure outline of summarizer system optimization function is given by and constraints are given by equation and this objective function contains two parts before minus sign guaranteed to maximize the highest informative sentences and the second part after minus sign is take care of redundant sentences eigenvalcentrality si are representing the importance of ith sentence decided by centrality measure and sj represents the weight of the edge between sentence si and sj from lexicalnetwork xi xj yij are binary variables to decide no two same sentences will be part of the summary detail description of proposed model this proposed work has been divided into three modules the outline of this framework is shown in is comprised sentences selection tagging and creation of another set of sentences with stop word removed that is later used for lexical network creation in we are creating lexical network and computing centrality score using degree centrality and betweenness centrality subgraph centrality performing better in the past the outcome of this step later used in summary generation three module process for automatic text summarization ijiijjstyxyx module in a text document there are many sentences which are not properly readable by the system first we should apply any regular expression for proper handling of sentences so that words tokens like take care properly in sentence extraction we are doing some preprocessing and then we are tagging these sentences using nltk natural language tool kit parser this is explained by an example in after sentence identification tagging is done then only significant units verb and nouns are considered for lexical network construction that are represented in modifiedsentences shown in in this module we are constructing a lexical network of nn matrix where n is a number of sentences we are picking each sentence on by one and extracting only nouns and verbs from this sentence because only these provides information and this is added to the module modified sentence an administration task force has been studying possible health care solutions but has yet to offer a comprehensive proposal actualsentence sentences administration nn task nn force nn studying vbg possible jj health nn care nn solutions nns offer vb comprehensive jj proposal nn administration task force studying health care solutions offer proposal showing procedure stepwise example from duc healthcare data set file number the actual lexical network is construed for modifiedsentences here we are finding is any relation either lexical or semantic is present between two tagged sentence modifiedsentence modifiedsentence to find the relation present between sentences we need to disambiguate the sense of the word this is done with the help of lesk algorithm here lesk algorithm takes two argument word to disambiguate sense and ith actual sentence based on that sense this algorithm finds hyper hypo synonyms meronym and antonym relation present in other remaining sentences at a time only one relation will available and according to us every relation is treated of the same importance so we will assign an edge between sentence and each time increase by one if more relation is present between sentences after getting a lexical network represented by for given sentences in table we are applying centrality based measures for sentence scoring sentence with high centrality score is decided more important compared to low centrality score and output from this module lexical network and centrality score given input in next module module in this module after getting sentences lexicalnetwork and centrality score we are going to optimize our function given by this objective function divided into two parts the first section of optimization function that is is to maximizing coverage maximization of coverage can be linked by maximum informative sentences the second section that is sj in this we are trying to reduce redundancy between selected sentences for the summary in equation n is the total number of sentences xi xj and yij are binary variables xi equals one if ith sentences selected and added in summary yij equals one will donate that sentences ith and jth are present in summary and i is not equal to these objecive criteria are represented by and that will guarantee to add a new sentence with optimality condition experiments and results in this section we are presenting different experiments and results in coverage or importance of sentences is decided by our proposed centrality based measures and in importance is decided by cosine based measure in this experiment we are maximizing the baseline objective function a section and our proposed model b section in both the objective function the relevance is decided by centrality measures in the baseline method redundancy is decided by cosine measure and in the proposed model the lexical network is considered for minimization of redundancy in tables we have presented precision p recall r and f score f with confidence interval for example if confidence interval is with confidence then the significance of this can interpreted like there is a chance that the actual value of unstandardized coefficient is between a cosine based criteria to minimize redundancy in this experiment we are using a baseline model in which we are trying to maximize the centrality score and minimize cosine similarity with given length constraints the combined optimization function is represented by in precision recall and f score are presented with confidence from the paper mcdonald it is not clearly mentioned how to decide the importance of the sentences but in our implementation we are using subgraph and pagerank based centrality measure precision recall and f score of model mdr model with subgraph centrality rouge r cosine p cosine f cosine confidence r confidence p confidence f cosine cosine cosine rouge l rouge rouge s rouge su precision recall and f score of model mdr model with pagerank centrality rouge r cosine p cosine f cosine confidence r confidence p confidence f cosine cosine cosine rouge l rouge rouge s rouge su from and it is clear that we are getting improved results b lexical based criteria to reduce redundancy in this experiment we are using our proposed model in which we are trying to maximize the relevance score that is found out by various degree centrality measures and minimize lexical similarity getting from the lexical network given constraints the combined optimization function is represented by in precision recall and f score are presented with confidence precision recall and f score of our proposed model when centrality based measure is subgraph centrality rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical rouge l rouge rouge s rouge su precision recall and f score of our proposed model when centrality based measure is betweenness centrality rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical rouge l rouge rouge s rouge su precision recall and f score of our proposed model when centrality based measure is pagerank rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical rouge l rouge rouge s rouge su precision recall and f score of our proposed model when centrality based measure is evencentrality rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical rouge l rouge rouge s rouge su precision recall and f score of our proposed model when centrality based measure is rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical rouge l rouge rouge s rouge su precision recall and f score of our proposed model when centrality based measure is closeness rouge r lexical p lexical f lexical confidence r confidence p confidence f lexical lexical lexical rouge l rouge rouge s rouge su precision recall and f score of our proposed model when centrality based measure is bonpow rouge r lexical p lexical confidence lexical r lexical p lexical f lexical rouge l rouge rouge s rouge su figure comparative performance of proposed and mdr baseline model from table and figure we can say that in proposed model when subgraph based centrality is used to measure relevance and lexical graph used for redundancy then we are getting best mode performance the second best model is mdr based with subgraph based centrality measure when comparison mdr based page rank model all the proposed model s performance is improved in this experiment we are finding a correlation between different centrality based measure the significance of this experiment highlights the relation between centrality measures since the different centrality measure returns different summary sentences and we are not doing summary to summary analysis what kind of sentences return by different measures this experiment gives a glimpse of the summary to summary analysis a co relation analysis between different centrality based proposed model our optimization function is described by in that after changing the centrality we are getting different results and here we are finding the correlation between these methods in statistics correlation coefficients are used to measure relationship is between two given variables it may be strong and poor the value return is between to where indicates a positive correlation indicates negative correlation and indicates no correlation or better to say no relation exists between these two variables in our study we used pearson s correlation spearman s correlation kendall s co relation co relation can be shown by the following examples available at we are using calculating the average correlation different scatter plots showing different directions and strengths of correlation pearson correlation it is also known by ppmc pearson product moment correlation it shows the linear relationship between two sets of data ppmc answers are it possible to draw a line graph to represent the data a potential problem with ppmc is that it is not able to differentiate between dependent and independent variables this can be expressed by where and y denote two classes of observation with n cardinality showing pairwise pearson s correlation coefficient symmetric authority between bonpow closeness evencen hub pagerank subgraph authority between bonpow closeness evencen hub pagerank subgraph spearman coefficient as per spearman correlation determines the strength and direction of the monotonic relationship between your two variables rather than the strength and direction of the linear relationship between your two variables which is what pearson correlation determines this can represent by denotes mean of and y and xi and yi is ith observation showing pairwise spearman s correlation coefficient symmetric authority between bonpow closeness evencen hub pagerank subgraph authority between bonpow closeness evencen hub pagerank subgraph kendall the advantage of kendall test is that this not only provides the relationship between variables but also provides distribution free test of independence spearman rank correlation is satisfactory for testing a null hypothesis of independence between two variables but it is difficult to interpret when the null hypothesis is rejected the kendall rank correlation improves upon this by reflecting the strength of the dependence between the variables being compared kendall s rank correlation coefficient depends on concordant discordant concordant nc is defined by yi else xi yi pair are discordant nd and here n is the total number of observations nc and nd are counting of concordant and discordant showing pairwise kendall s correlation coefficient symmetric authority between bonpow closeness evencen hub pagerank subgraph authority between bonpow closeness evencen hub pagerank subgraph using and we can interpret that there are eight different measures to find the importance of sentence and rank it using we find that the following pair are highly correlated as authority evencen authority hub authority pagerank authority subgraph between closeness evencen hub evencent pagerank evencen subgraph hub pagerank hub subgraph and pagerank subgraph those pairs are highly related their combination will not give better results in the case when we use its combination in this experiment we are hybridizing different centrality measure to maximize the relevance of the sentences and minimization of redundancy has done by the lexical network since we have eight different measures so total hybridize combinations are possible in limited time constraints we can not check all the combination possible to decide a better combination we are considering experiment there we will combine those measures which are minimum correlated because highly correlated will return the same results to make it simple here we are combining two centrality measures from experiment we can conclude that the best performer is subgraph based centrality as per our requirement to get a better combination we can select subgraph bonpow and subgraph between based measures precision recall and f score of our proposed model when relevance is decided by hybridizing subgraph and bonpow rouge r lexical p lexical f lexical confidence r lexical p lexical f lexical rouge l rouge rouge s rouge su precision recall and f score of our proposed model when relevance is decided by hybridizing subgraph and betweenness confidence rouge r lexical p lexical f lexical r lexical p lexical f lexical rouge l rouge rouge s rouge su in table and we have presented precision recall and f score of hybridized feature and figure representing comparative performance of hybridize centrality independent centrality measure from and we can conclude that hybridize relevance measure returning better s su and for rouge l rouge highest performance derived by subgraph based centrality figure graph showing comparative performance of different relevance based measure where sign denoting hybridization of different features concluding remark in this chapter we have proposed optimization based summarization that guarantees maximize coverage and minimum redundancy we have tested our model by selecting centrality for coverage relevance and lexical network as a function of redundancy between sentences we have compared our approach with cosine similarity based redundancy measure between sentences in both approaches proposed and baseline relevance measure has decided by centrality score we have performed three experiments in the first experiments we have implemented the baseline model using subgraph and pagerank based centrality measure used for relevance measure and cosine similarity matrix for redundancy in this work we have implemented all the centrality measure as relevance and lexical network for redundancy where we have found out that based centrality is giving better results even in the baseline model when subgraph centrality is used it is giving respectable results in the second experiment we have found out the correlation between the summary results when different centrality based measure used for relevance and lexical network used for redundancy here we have suggested that the high correlation pair is not suitable for hybridization and the low correlation pair is expected for better results in the third experiment we have hybridized different centrality measures and improved results are shown by tables and figures in the third experiment several feature combinations are possible but to our limitation we have tested only limited possibility chapter conclusion and future work this work our focus is on an important aspect of information retrieval s task automatic text document summarization this work is divided into five chapters the first chapter gives a brief introduction about text summarization evaluating techniques and datasets on which this wide range of models developed and experimented in the second chapter hybrid approach for single text document summarization using statistical and sentiment features we have presented a linear combination of different statistical measures and semantic measures in our hybrid approach we have taken statistical measures like sentence position centroid tf idf and word level analysis based semantic approach that is sentiment analysis the sentiment score of a sentence is computed as the sum of the sentiment score of every entity present in the sentence since for any entity sentiment score have three polarities as neutral negative and positive we are more interested in such kind of sentences those have a high semantic score either negative or positive so if entity sentiment is negative then we are multiplying it by minus one to treat it as a positive score to generate several summaries of different length we have used different approaches like mead microsoft opinosis and human based in this chapter we have done four experiments in the first experiment we have considered our summary generate from a proposed algorithm as a system summary and all others as a model summary after evaluating this has shown that we are getting high precision almost every time that denotes we covered most relevant results in the second experiment we have compared different system generated summary mead microsoft opinosis our algorithm to the model summary human generated in this we find that our explained algorithm performed well for generated summary for almost every time but in mead system generates a summary leading in some way but here also we are getting higher recall to compare to mead the third experiment is showing the contribution of sentiment score in the selection of most informative sentences we have shown that when we are adding sentiment score as a feature we are getting improved results to compare to without a sentiment score initially for all experiments have done by assigning equal importance for every feature to score a sentence we took the sum of all the feature s score and pickup highest score sentence and added that into the summary in the next step we are selecting next sentence based on the next highest score and add it to the summary if the similarity between summary and sentence is lower that threshold to maintain redundancy and coverage we will repeat this iterative process until the desired length summary is achieved in experiment four we have extended this approach we have suggested and tested a better combination of feature weights here parameter estimation is done by regression and random forest in third chapter a new latent semantic analysis and entropy based approach for automatic text document summarization we have proposed two new approaches three new models for automatic text document summarization and a novel entropy based approach for summary evaluation both the approaches for summary generation is based on svd based decomposition in the first approach we are using right singular matrix vt for processing and selects a concept one by one top to bottom till required previous approaches are focused on selecting only one sentence of the highest information content in our approach we are selecting two sentences each concept such that is highest related to concept and least related to the concept this approach is based on assumption that by doing this we are covering two different topics as a result it leads to more coverage and diversity in future we can increase the selection of number of sentences at a glass the second approach is based on entropy which formulate into two different models and in first we are selecting a higher informative concept and from that concept we are selecting summary sentences in repeatedly we are selecting highest informative sentences a sentence which is related to all the concepts with a high score the advantage of the entropy based model is that these are not length dominating models giving a better rouge score statistically closer to standard gold summary during an experiment we have found out that rouge score depends only on the count of matched words an increasing the summary length sometimes rouge score decreases and on increasing redundancy rouge score also increases we have pointed out that rouge score does nt measure redundancy count matched sentences we have also realized the need of a new measure for summary evaluation that provide a tradeoff between redundancy countmatch and entropy based criteria are proposed during testing of the new proposed measure on different summary generated by previous models and our proposed models we have find that our entropy based summary is closer to standard summary from the experiment results it is clear that our model works well for summary evaluation especially for higher length summary because as summary length increases redundancy also increases and in this measure we are measuring redundancy currently we are giving equal importance to all n gram but theoretically and practically we should give more weight to higher n gram because of high redundancy of information in case of repetition in future we may assign different feature weights to get better results in fourth chapter lexnetwork based summarization and a study of impact of wsd techniques and similarity threshold over lexnetwork we have presented a lexical network concept for automatic text document summarization this approach is a little bit different from previously proposed lexical chain based techniques in previous techniques author concentrate to create a number of lexical chains that creates ambiguity which chain to prefer even this problem efficiently can handle with chain scoring techniques still lexical chains have problem that if one particular word let donald trump is coming in two or more sentences then which sentence to prefer another problem with this technique that they consider only nearby sentences for a chain construction window of two or three sentences was selected so this is unable to handle long term relationship between sentences our lex net handle long term relationship between sentences nodes are scored and high score sentence given priority so both problems are handled in this way our lexical network is based on the number of lexical and semantic relations to decide the importance of sentences we have used centrality based measure on lexical network since human language is highly ambiguous here english so we need to find a correct sense of a particular word in given context the solution to this ambiguity problem done with simplified lesk cosine lesk and adapted lesk algorithm in this work we have studied the impact of wsd techniques and cosine similarity threshold less value of represents more divesity compare to high valued more diversity if is less is not good for summary because it will maintain diversity but the less relatedness between sentences which is harming good summary property for comparison purpose we are used semantrica lexalytics algorithm in which we find that system proposed by us is working better many times from number of experiments we have find out that subgraph based centrality is performing best among all from this work we have reached on number of conclusions for alpha centrality when alpha is to inclusion the performance of summarizer system is arbitrary up and down but after that to inclusion for all centrality measures for different value of threshold performance is continuously increasing some time exception at and again reduced at from a set of cosine similarity as we are suggesting that similarity threshold is better to get enough diversity and better summary as per rouge score hub authority based ranking is same as eigenvalue based centrality subgraph based centrality measure is performing better to all the reason of this is higher score for small subgraph which recognizes small subgraph and this cover various subgraph can be considered as cluster like structure we are not suggesting any particular wsd is better all time during lexnetwork creation we have used lexical semantic relations and in this work we have assigned equal weight assigns for each relation presents between sentences in literature different priority assigned to all relations in the future that may be considered for network creation and it may improve the ranking of sentences during the experiment we have find out some corelation between eigen value based centrality and hub based centrality measures at present this is not objective of this work why it is in future we will try to answer this in fifth chapter modeling automatic text document summarization as multi objective optimization we have proposed optimization based summarization that guarantee maximize coverage and minimum redundancy we have tested our model by selecting coverage as based score and similarity function as relatedness between sentences we have compared our proposed lexnetwork based approach with cosine similarity based redundancy and relevance has been measured by centrality based measures in both the approaches in this work we have performed three experiments in the first experiments we have implemented the baseline model using subgraph and pagerank based centrality measure used for relevance measure and cosine similarity matrix for redundancy in this work we have implemented all the centrality measure as relevance and lexical network for redundancy where we have found out that subgraph based centrality is giving better results even in the baseline model when subgraph centrality is used it is giving respectable results in the second experiment we have found out the correlation between the summary results when different centrality based measure used for relevance and lexical network used for redundancy here we have suggested that the high correlation pair is not suitable for hybridization and the low correlation pair is expected for better results in the third experiment we have hybridized different centrality measures and improved results are shown by tables and figures in future this work can be extended to other combinations and an aggregate based model may be proposed and tested along with optimal combination using soft computing techniques references alguliev aliguliyev hajirahimova expert systems with applications gendocsum mclr generic document summarization based on maximum coverage and less redundancy expert systems with applications alguliev aliguliyev hajirahimova mehdiyev expert systems with applications mcmr maximum coverage and minimum redundant text summarization model expert systems with applications alguliev aliguliyev isazade docsum differential evolution with self adaptive mutation and crossover parameters for multi document summarization based systems alguliev aliguliyev isazade expert systems with applications cdds constraint driven document summarization models expert systems with applications alguliev aliguliyev isazade formulation of document summarization as a nonlinear programming problem computers industrial engineering alguliev aliguliyev mehdiyev sentence selection for generic document summarization using an adaptive differential evolution algorithm swarm and evolutionary computation balaji geetha v parthasarathi abstractive summarization a hybrid approach for the compression of semantic graphs international journal on semantic web and information systems ijswis banerjee pedersen an adapted lesk algorithm for word sense disambiguation using wordnet in international conference on intelligent text processing and computational linguistics pp springer barzilay elhadad using lexical chains for text summarization baxendale machine made index for technical literature an experiment ibm journal of research and development beliga metrovi martini ipi selectivity based keyword extraction method international journal on semantic web and information systems ijswis bonacich power and centrality a family of measures american journal of sociology boulesteix janitza kruppa knig overview of random forest methodology and practical guidance with emphasis on computational biology and bioinformatics wiley interdisciplinary reviews data mining and knowledge discovery breiman random forests machine learning chen huang yeh lee spoken lecture summarization by random walk over a graph constructed with automatically extracted key terms in twelfth annual conference of the international speech communication association chen liu wang automatic text summarization based on textual cohesion journal of electronics china chen wang guan automatic text summarization based on lexical chains in international conference on natural computation pp springer chiru rebedea ciotec comparison between lsa lda lexical chains in webist pp creation abstracts the automatic creation of literature abstracts april deerwester dumais furnas landauer harshman indexing by latent semantic analysis journal of the american society for information science dolbear hobson vallet fernndez cantadorz castellsz personalised multimedia summaries in semantic multimedia and ontologies pp springer doran stokes carthy dunnion comparing lexical chain based summarisation approaches using an extrinsic evaluation gwc edmundson new methods in automatic extracting elhadad no title retrieved from ercan cicekli using lexical chains for keyword extraction information processing erekhinskaya moldovan lexical chains on wordnet and extensions in flairs estrada rodriguez velazquez subgraph centrality in complex networks physical management conference review e freeman a set of measures of centrality based on betweenness sociometry freeman centrality in social networks conceptual clarification social networks ganapathiraju carbonell yang relevance of cluster size in mmr based summarizer a report self paced lab in information retrieval ganesan zhai han opinosis a graph based approach to abstractive summarization of highly redundant opinions august gleich pagerank beyond the web siam review goldstein mittal carbonell callan creating and evaluating multi document sentence extract summaries in proceedings of the ninth international conference on information and knowledge management pp acm gong liu creating generic text summaries gonzlez fort a new lexical chain algorithm used for automatic summarization in ccia pp gurevych nahnsen adapting lexical chaining to summarize conversational dialogues in proceedings of the recent advances in natural language processing conference pp hahn mani the challenges of automatic summarization computer halliday hasan cohesion in english longman london hariharan multi document summarization by combinational approach international journal of computational cognition hoskinson creating the ultimate research assistant computer hovy lin automated text summarization in summarist jagadeesh pingali varma sentence extraction based single document summarization international institute of information technology hyderabad india karanikolas galiotou a workbench for extractive summarizing methods katz a new status index derived from sociometric analysis psychometrika katz distribution of content words and phrases in text and language modelling natural language engineering kim kim hwang korean text summarization using an aggregate similarity in proceedings of the fifth international workshop on on information retrieval with asian languages pp acm kleinberg authoritative sources in a hyperlinked environment journal of the acm jacm koschtzki lehmann peeters richter tenfelde podehl zlotowski centrality indices in network analysis pp springer kulkarni apte an automatic text summarization using lexical cohesion and correlation of sentences international journal of research in engineering and technology kupiec pedersen chen a trainable document summarizer in proceedings of the annual international acm sigir conference on research and development in information retrieval pp acm lesk automatic sense disambiguation using machine readable dictionaries how to tell a pine cone from an ice cream cone in proceedings of the annual international conference on systems documentation pp acm lin rouge a package for automatic evaluation of summaries text summarization branches out lin rey r ouge a package for automatic evaluation of summaries luhn the automatic creation of literature abstracts ibm journal of research and development luo zhuang he shi effectively leveraging entropy and relevance for summarization in asia information retrieval symposium pp springer mani maybury advances in automatic text summarization reviewed by mark sanderson university of sheffield mani maybury automatic summarization mcdonald a study of global inference algorithms in multi document summarization in european conference on information retrieval pp springer mckeown barzilay chen elson evans klavans sigelman columbia s newsblaster new features and future directions companion volume of the proceedings of naacl demonstrations medelyan computing lexical chains with graph clustering in proceedings of the annual meeting of the acl student research workshop pp association for computational linguistics morris hirst lexical cohesion computed by thesaural relations as an indicator of the structure of text computational linguistics murray renals carletta extractive summarization of meeting recordings newman the mathematics of networks the new palgrave encyclopedia of economics normalisation statistics wikipedia ou khoo goh automatic text summarization in digital libraries in handbook of research on digital libraries design development and impact pp igi global ouyang li lu zhang a study on position information in document summarization in proceedings of the international conference on computational linguistics posters pp association for computational linguistics ozsoy alpaslan cicekli journal of information science june ozsoy cicekli alpaslan text summarization of turkish texts using latent semantic analysis in proceedings of the international conference on computational linguistics pp association for computational linguistics padmalahari kumar prasad automatic text summarization with statistical and linguistic features using successive thresholds in advanced communication control and computing technologies icaccct international conference on pp ieee page brin motwani winograd the pagerank citation ranking bringing order to the web stanford infolab plaza stevenson daz resolving ambiguity in biomedical text to improve summarization information processing management pourvali abadeh automated text summarization base on lexicales chain and graph using of wordnet and wikipedia knowledge base arxiv preprint precision and recall radev blair goldensohn zhang experiments in single and multi document summarization using mead ann arbor radev hovy mckeown introduction to the special issue on summarization computational linguistics radev jing sty tam centroid based summarization of multiple documents rambow shrestha chen lauridsen summarizing email threads in proceedings of hlt naacl short papers pp association for computational linguistics rautray balabantaray bhardwaj document summarization using sentence features international journal of information retrieval research ijirr roul sahoo goel deep learning in the domain of multi document text summarization in international conference on pattern recognition and machine intelligence pp springer sakai sparck jones generic summaries for indexing in information retrieval in proceedings of the annual international acm sigir conference on research and development in information retrieval pp acm sankarasubramaniam ramanathan ghosh text summarization using wikipedia information processing and management sarkar syntactic trimming of extracted sentences for improving extractive multi document summarization journal of computing shannon a mathematical theory of communication bell system technical journal vol july october sharan siddiqi singh keyword extraction from hindi documents using statistical approach in intelligent computing communication and devices pp springer shimada tadano endo multi aspects review summarization with objective information silber mccoy an efficient text summarizer using lexical chains in proceedings of the first international conference on natural language generation volume pp association for computational linguistics sood towards summarization of written text conversations international institute of information technology india steinberger jeek text summarization and singular value decomposition in international conference on advances in information systems pp springer steinberger poesio kabadjov jez two uses of anaphora resolution in summarization stokes applications of lexical cohesion analysis in the topic detection and tracking domain university college dublin department of computer science takale kulkarni shah an intelligent web search using multi document summarization international journal of information retrieval research ijirr tan examining crosslingual word sense disambiguation nanyang technological university nanyang avenue tofighy raj javad ahp techniques for persian text summarization malaysian journal of computer science tombros sanderson advantages of query biased summaries in information retrieval in proceedings of the annual international acm sigir conference on research and development in information retrieval pp acm torres moreno automatic text summarization john wiley sons vechtomova karamuftuoglu robertson on document relevance and lexical cohesion between query terms information processing management venables team and the an introduction to wan using only cross document relationships for both generic and topic focused document summarizations information retrieval white jose ruthven a task oriented study on the influencing effects of biased summarisation in web searching information processing management xiong ji query focused multi document summarization using hypergraph based ranking information processing management yadav sharan hybrid approach for single text document summarization using statistical and sentiment features international journal of information retrieval research ijirr yadav sharan joshi semantic graph based approach for text mining in proceedings of the international conference on issues and challenges in intelligent computing techniques icict yadav sharan kumar biswas a new approach for single text document summarization advances in intelligent systems and computing vol yeh ke yang meng text summarization using a trainable summarizer and latent semantic analysis information processing management yeh text summarization using a trainable summarizer and latent semantic analysis q zhang li gao ouyang automatic twitter topic summarization with speech acts ieee transactions on audio speech and language processing
