selective attention encoders syntactic graph convolutional networks document summarization haiyang yun kun baochang junwen xiangang chuxing beijing china research america mountain view usa university beijing china com edu cn kunhan maobaochang chenjunwen com r m l c s c v v x r abstract abstractive text summarization challenging task need design mechanism effectively extract salient formation source text generate summary parsing process source text contains critical tic semantic structures useful generate accurate summary modeling parsing tree text summarization trivial non linear structure harder deal document includes multiple sentences parsing trees paper propose use graph connect parsing trees sentences document utilize stacked graph convolutional works gcns learn syntactic representation ument selective attention mechanism extract salient information semantic structural aspect erate abstractive summary evaluate approach cnn daily mail text summarization dataset imental results proposed gcns based selective attention approach outperforms baselines achieves state art performance dataset index terms document summarization sequence sequence dependency parsing tree graph convolutional networks introduction document summarization aims generating short uent summary consisting salient information source text existing approaches text summarization divided major types extractive abstractive tractive methods produce summaries extracting important sentences original document abstractive methods produce summaries summarizing salient tion source text representing arbitrary words end end neural framework based sequence sequence models achieved tremendous success text generation tasks machine lation dialogue systems essence method encoder decoder framework rst encodes input sentence low dimensional representation decodes abstract representation based attention researchers apply neural model stractive text summarization ward adopt approach text summarization signicant difference machine lation important component text summarization turing salient information original document generating summary instead aligning input sentence summary explicit information selection text summary proven effective plicit learning approach recent searchers explored selective gate based global ument combining extractive method capture salient information explicitly decoding achieved current state art results models ignore syntactic structure source text help choose important words structure erate accurate summary feature based models represent dependency information hand crafted features face problem sparse feature spaces phasizing explicit importance structure paper explore syntactic graph convolutional networks model non euclidean document structure adopt attention information gate select salient information generating text summarization specically build document level graph heterogeneous types nodes edges formed connecting dependency parsing trees sentences document adopt stacked convolutional neural networks gcns learn local non local syntactic representation document proved effectiveness nlp employ attention mechanism acquire global document representation combining syntactic semantic information use explicit information tion gate based global document representation choose important words generating better summary uate model cnn daily mail text summarization datasets experimental results proposed gcns encoders model outperforms state art line models related work existing approaches document summarization divided major categories extractive abstractive extractive use hierarchical recurrent neural works rnns representations sentences classify importance sentences rank extracted sentences summary generation reinforcement learning extract salient sentences propose new policy gradient method rewrite sentences e presses paraphrases generate concise overall mary propose framework composed hierarchical document encoder based cnns attention based tractor attention external information present new extractive framework joint learning score selecting sentences rstly apply neural networks text summarization local attention based model erate word conditioned input sentence applies framework hierarchical attention text marization proposes graph based attention mechanism summarize salient information document neural models faces vocabulary oov lems vocabulary xed training stage order solve problem point network copynet proposed allow copying words original text generating arbitrary words xed cabulary propose unied model inconsistency loss combine extractive abstractive methods adopt attention alleviate issue point network tending copy long sequences recently focus explicit information selection encoding step lters unnecessary words uses portant words generating summary focus model selecting salient syntactic semantic information algorithm details fig overall architecture proposed model structural document colors denote types nodes edges represented gcns semantic document representation represented bilstm bined attention mechanism document information selective gate lters unnecessary words generating summary semantic syntactic document encoder gcn semantic document encoder given document concatenating sentences long sequence wi ith words document n sequence length document ploy bidirectional long short term memory bilstm encoder bilstm consists forward lstm reads document d backward lstm reads document d wn xi wewi n lst m xi n lst m xi n xi distributed representation token ei bedding matrix concatenate forward hidden state j original word semantic representation j backward hidden state architecture model shown figure section describe proposed model specically consists semantic document encoder syntactic ment encoder gcn attention information gate syntactic document encoder order build document level graph apply parser generate dependency tree lk sentence sk treating syntactic dependency direction label different edge type directions labels edges criminate important words structure comparing experimental results demonstrate essence ing directions labels edges text summarization task link syntactic root node serially adjacent sentence type edges build complete document adjacency matrix apply gcns compute structural sentation word constructed document graph keeps separate parameters edge type stack gcns layers node scious distant neighbors l layers node representation capture local global syntactic tion specically llayer gcn denote hs hs l ith input output vector node lth layer input vector word semantic representation graph convolution operation denoted follows hs l w l j jm j w l j trainable parameters mi set neighbouring nodes ith node set word semantic representation original inputs gcns hs alleviate parsing error furthermore prevent parametrization setting weighs direction label edges separate weights adjacent sentence edges sole bias edge types use output vector node lth layer word structural representation hs l concatenate semantic structural representation informative hs word representation hi attention information gate propose novel global information gate based tion mechanism select salient information concretely adopt attention gregate representation informative words form document vector gate network takes document vector dv word representation input compute selective gate vector gi ai ui bw n aihi uw uw dv n gi ugdv bg ww wg ug trainable weights bw bg trainable bias word ltered gate vector gi important words decoding h hi gi h representation word wi information ltration input word representation decoder generate summary element wise plication decoding stage apply pointer generator network alleviate oov problems coverage network vent prior works furthermore ploy relieve problem ing long sequences pointer generator network nal loss consists negative log likelihood coverage loss evaluations section introduce expermental setup present experimental results experiment setup use cnn daily mail dataset evaluate model consist long text widely text summarization task scripts supplied produce non anonymized version cnn daily mail summarization dataset contains training pairs validation pairs test pairs ments use words source vocabulary obtain syntactic information sentences corpora use stanford parser dependency trees edge labels model takes dimensional hidden states use dimensional word embeddings choose adagrad found work best stochastic gradient descent adadelta momentum adam rmsprop learning rate initialize accumulator value hyper parameter congurations adjust according performance validation set examples randomly sampled validation set inference stage tuning attention mask threshold set weight length penalty coverage loss weighted use layers gcns encoder set beam size experimental results adopt widely rouge pyrouge measures similarity output evaluation metric github com abisee pointer generator com abisee cnn dailymailr stanford edu software lex parser shtml com sebastiangehrmann summary python org pypi methods extractive runner refresh rnn rl neusum abstractive intra info model r l table results abstractive summarizers dm dataset rst section shows extractive baselines second section describes abstractive approaches section presents model rouge scores condence interval methods model gate r l table results removing different components model cnn dataset rouge scores condence interval summary standard reference computing ping n gram unigram bigram longest common subsequence lcs following experiments adopt unigram bigram rouge l longest common subsequence evaluation observed table proposed proach achieves best performance cnn daily datasets state art extractive abstractive baselines comparing architecture erage model signicant improvement rouge l demonstrates effectiveness model combining syntactic semantic information information gate modeling heterogeneous document level graph stacked gcns furthermore study effectiveness component model conduct ablation iments cnn dataset gate denotes remove attention gate use encoded word decode denotes remove gcns attention gate degrades table shows component model improve formance dataset evidently gcns model ture document effectively represent dense dimension feature attention gate adopt attention anism acquire informative document representation bining syntactic semantic information use selective gate detect critical words generating summary conclusions work propose syntactic graph convolutional coders based dependency trees abstractive text rization uses graph convolutional networks gcns learn representation syntactic structure source text adapt attention mechanism combining semantic structural information select important words coding evaluate model cnn daily mail text summarization datasets experimental results proposed gcns encoders model outperforms state art baseline models references ilya sutskever oriol vinyals quoc v le quence sequence learning neural networks advances neural information processing systems pp minh thang luong hieu pham christopher d arxiv preprint manning based neural machine translation approaches effective antoine bordes y lan boureau jason weston arxiv learning end end goal oriented dialog preprint jiatao gu zhengdong lu hang li victor ok li incorporating copying mechanism sequence sequence learning arxiv preprint abigail peter j liu christopher d manning point summarization pointer generator networks arxiv preprint qingyu zhou nan yang furu wei ming zhou selective encoding abstractive sentence rization meeting association tional linguistics pp cnn dataset subset cnn daily dataset widely text summarization achieved state art mance dataset wei li xinyan xiao yajuan lyu yuanzhuo wang improving neural abstractive document summarization explicit information selection modeling ceedings conference empirical methods natural language processing pp jiwei tan xiaojun wan jianguo xiao tive document summarization graph based tional neural model proceedings annual meeting association computational tics volume long papers vol pp wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun unied model tractive abstractive summarization tency loss arxiv preprint kaiqiang song lin zhao fei liu infused copy mechanisms abstractive proceedings international tion ference computational linguistics pp diego marcheggiani ivan titov encoding tences graph convolutional networks semantic role labeling proceedings conference empirical methods natural language processing pp yuhao zhang peng qi christopher d manning graph convolution pruned dependency trees proves relation extraction proceedings conference empirical methods natural language processing pp joost bastings ivan titov wilker aziz diego graph marcheggiani khalil simaan tional encoders syntax aware neural machine lation proceedings conference pirical methods natural language processing pp ramesh nallapati feifei zhai bowen zhou marunner recurrent neural network based sequence model extractive summarization documents aaai pp shashi narayan shay b cohen mirella ranking sentences extractive arxiv preprint learning ata tion reinforcement yen chun chen mohit bansal fast abstractive summarization reinforce selected sentence ing proceedings annual meeting association computational linguistics volume long papers pp shashi narayan ronald cardenas nikos topoulos shay b cohen mirella lapata jiangsheng yu yi chang document modeling nal attention sentence extraction proceedings annual meeting association putational linguistics volume long papers vol pp qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural document marization jointly learning score select tences proceedings annual meeting association computational linguistics volume long papers pp alexander m rush sumit chopra jason weston neural attention model abstractive sentence marization arxiv preprint ramesh nallapati bowen zhou caglar gulcehre bing xiang al abstractive text summarization sequence sequence rnns arxiv preprint oriol vinyals meire fortunato navdeep jaitly pointer networks advances neural information processing systems pp sebastian gehrmann yuntian deng alexander rush abstractive summarization ceedings conference empirical methods natural language processing pp alex graves jurgen schmidhuber framewise phoneme classication bidirectional lstm neural network architectures neural networks vol pp zichao yang diyi yang chris dyer xiaodong alex smola eduard hovy hierarchical attention networks document classication proceedings conference north american chapter association computational linguistics man language technologies pp danqi chen christopher manning fast curate dependency parser neural networks proceedings conference empirical ods natural language processing emnlp pp romain paulus caiming xiong richard socher deep reinforced model abstractive tion arxiv preprint chin yew lin rouge package automatic ation summaries text summarization branches
