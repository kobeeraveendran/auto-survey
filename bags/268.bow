selective attention encoders by syntactic graph convolutional networks for document summarization haiyang yun kun baochang junwen xiangang chuxing beijing china research america mountain view ca usa university beijing china com edu cn kunhan maobaochang chenjunwen com r a m l c s c v v i x r a abstract abstractive text summarization is a challenging task and one need to design a mechanism to effectively extract salient formation from the source text and then generate a summary a parsing process of the source text contains critical tic or semantic structures which is useful to generate more accurate summary however modeling a parsing tree for text summarization is not trivial due to its non linear structure and it is harder to deal with a document that includes multiple sentences and their parsing trees in this paper we propose to use a graph to connect the parsing trees from the sentences in a document and utilize the stacked graph convolutional works gcns to learn the syntactic representation for a ument the selective attention mechanism is used to extract salient information in semantic and structural aspect and erate an abstractive summary we evaluate our approach on the cnn daily mail text summarization dataset the imental results show that the proposed gcns based selective attention approach outperforms the baselines and achieves the state of the art performance on the dataset index terms document summarization sequence sequence dependency parsing tree graph convolutional networks introduction document summarization aims at generating a short and uent summary consisting of the salient information of the source text existing approaches for text summarization are divided into two major types extractive and abstractive tractive methods produce summaries by extracting important sentences from the original document abstractive methods produce the summaries by summarizing the salient tion of the source text and representing by arbitrary words the end to end neural framework based on sequence sequence models have achieved tremendous success in many text generation tasks such as machine lation dialogue systems the essence of method is an encoder decoder framework which rst encodes the input sentence to a low dimensional representation and then decodes the abstract representation based on attention some researchers also apply neural model to stractive text summarization although it is ward to adopt the approach to text summarization there is a signicant difference between it and machine lation the important component of text summarization is turing the salient information of the original document for generating summary instead of aligning between the input sentence and the summary explicit information selection in text summary have proven to be more effective than plicit learning only via the approach recent searchers has explored selective gate based on global ument and combining extractive method to capture salient information explicitly for decoding and achieved the current state of the art results however these models ignore the syntactic structure of the source text which can help choose important words in structure to erate more accurate summary or feature based models represent dependency information by hand crafted features which face the problem of sparse feature spaces and not phasizing the explicit importance of structure in this paper we explore syntactic graph convolutional networks to model non euclidean document structure and adopt attention information gate to select salient information for generating text summarization specically we build a document level graph with heterogeneous types of nodes and edges which are formed by connecting the dependency parsing trees from the sentences in a document we adopt stacked convolutional neural networks gcns to learn the local and non local syntactic representation for a document which have proved the effectiveness in other nlp then we employ attention mechanism to acquire global document representation combining syntactic and semantic information and use explicit information tion gate based on global document representation to choose important words for generating better summary we uate our model to the cnn daily mail text summarization datasets the experimental results show that the proposed gcns encoders model outperforms the state of the art line models related work existing approaches for document summarization are divided into two major categories extractive and abstractive extractive use hierarchical recurrent neural works rnns to get the representations of the sentences and classify the importance of sentences rank extracted sentences for summary generation through a reinforcement learning and extract salient sentences and propose a new policy gradient method to rewrite these sentences i e presses and paraphrases to generate a concise overall mary propose a framework composed of a hierarchical document encoder based on cnns and an attention based tractor with attention over external information present a new extractive framework by joint learning to score and selecting sentences rstly apply neural networks for text summarization by using a local attention based model to erate word conditioned on the input sentence applies framework with hierarchical attention for text marization proposes graph based attention mechanism to summarize the salient information of document however the above neural models all faces out of vocabulary oov lems since the vocabulary is xed at training stage in order to solve this problem point network and copynet have been proposed to allow both copying words from the original text and generating arbitrary words from a xed cabulary propose a unied model via inconsistency loss to combine the extractive and abstractive methods adopt bottom up attention to alleviate the issue of point network tending to copy long sequences recently more and more focus on explicit information selection in encoding step which lters unnecessary words and uses portant words for generating summary the focus of our model is selecting salient syntactic and semantic information algorithm details fig overall architecture of the proposed model the structural document colors denote the types of nodes and edges represented by gcns and the semantic document representation represented by bilstm are bined via attention mechanism to get document information then selective gate lters unnecessary words for generating the summary semantic and syntactic document encoder with gcn semantic document encoder given a document concatenating all sentences into a long sequence where wi is ith words in the document and n is the sequence length of document we ploy a bidirectional long short term memory bilstm as the encoder the bilstm consists of forward lstm which reads the document d from to and backward lstm reads the document d from wn to xi wewi i n he i he i lst m xi i n lst m xi i n where xi is the distributed representation of token ei by bedding matrix we we concatenate every forward hidden he state j to get the original he word semantic representation he i he j with the backward hidden state he i the architecture of our model is shown in figure in this section we describe the proposed model specically which consists of semantic document encoder syntactic ment encoder with gcn and attention information gate syntactic document encoder in order to build a document level graph we apply a parser to generate the dependency tree lk of every sentence sk by treating each syntactic dependency direction and label as a different edge type directions and labels of edges can criminate the important words in structure and our comparing experimental results also demonstrate the essence of ing directions and labels of edges in text summarization task then we link the syntactic root node serially with adjacent sentence type edges to build a complete document adjacency matrix we apply gcns compute the structural sentation of every word on the constructed document graph which keeps separate parameters for each edge type more we stack many gcns layers to make every node scious of more distant neighbors after l layers every node representation can capture local and global syntactic tion specically in the llayer gcn we denote hs hs l i as the ith input and output vector of node i at the lth layer the input vector is the word semantic representation he i a graph convolution operation can be denoted as follows hs l i w l i i j jm i i j where w l i j are the trainable parameters mi is the set of neighbouring nodes of ith node as we set the word semantic representation he i as the original inputs of gcns hs to alleviate the parsing error furthermore we also prevent over parametrization by setting the same weighs of direction and label edges but separate weights for adjacent sentence edges and sole bias for all edge types we use the output vector of node i at the lth layer as word structural representation hs l i we concatenate the semantic and structural representation to get the informative i hs word representation hi he i attention information gate we propose a novel global information gate based on tion mechanism to select the salient information from the put concretely we adopt the attention gregate the representation of those informative words to form a document vector then the gate network takes the document vector dv and the word representation as the input to compute the selective gate vector gi ai ui bw n aihi i uw i uw dv n gi ugdv bg where ww wg ug are the trainable weights and bw bg are the trainable bias then each word can be ltered by the gate vector gi to get important words for decoding h i hi gi where h i is the representation of word wi after information ltration and used as the input word representation for the decoder to generate the summary is element wise plication in the decoding stage we apply pointer generator network to alleviate the oov problems and coverage network to vent as prior works furthermore we also ploy bottom up to relieve the problem of ing very long sequences using pointer generator network the nal loss consists of negative log likelihood and the coverage loss evaluations in this section we introduce the expermental setup and present the experimental results experiment setup we use cnn daily mail dataset to evaluate our model which consist of long text and has been widely used in text summarization task we used scripts supplied by to produce the non anonymized version of the cnn daily mail summarization dataset which contains training pairs validation pairs and test pairs for all ments we use words of the source vocabulary to obtain the syntactic information of the sentences in the corpora we use stanford parser to get dependency trees with edge labels our model takes dimensional hidden states and use dimensional word embeddings we choose adagrad this was found to work best of stochastic gradient descent adadelta momentum adam and rmsprop with learning rate and initialize the accumulator value with for hyper parameter congurations we adjust them according to the performance on the validation set and examples randomly sampled from validation set in inference stage for bottom up after tuning attention mask threshold is set to the weight of length penalty is and coverage loss weighted is we use layers gcns in encoder and set beam size to experimental results we adopt the widely used rouge by pyrouge it measures the similarity of the output evaluation metric github com abisee pointer generator com abisee cnn dailymailr stanford edu software lex parser shtml com sebastiangehrmann bottom up summary python org pypi methods extractive runner refresh rnn rl neusum abstractive intra bottom info our model r l table results of abstractive summarizers on the dm dataset the rst section shows extractive baselines the second section describes abstractive approaches the third section presents our model all our rouge scores have a condence interval with at most methods our model gate r l table results of removing different components of our model on the cnn dataset all our rouge scores have a condence interval with at most summary and the standard reference by computing ping n gram such as unigram bigram and longest common subsequence lcs in the following experiments we adopt unigram bigram and rouge l longest common subsequence for evaluation it can be observed from table that the proposed proach achieves the best performance on the cnn daily datasets over state of the art extractive as well as abstractive baselines comparing with the same architecture of erage our model has signicant improvement on on on on rouge l which demonstrates the effectiveness of our model combining the syntactic and semantic information by information gate and modeling the heterogeneous document level graph via stacked gcns furthermore to further study the effectiveness of each component of our model we conduct several ablation iments on the cnn dataset gate denotes that we remove the attention gate and only use all encoded word to decode denotes that we remove the gcns and the attention gate which degrades into table shows that each component of our model all improve the formance on the dataset evidently gcns model the ture of document effectively and represent it in a dense dimension feature attention gate adopt the attention anism to acquire informative document representation bining syntactic and semantic information then use selective gate to detect critical words for generating the summary conclusions in this work we propose syntactic graph convolutional coders based on dependency trees for abstractive text rization which uses graph convolutional networks gcns to learn the representation of syntactic structure of the source text then adapt the attention mechanism combining semantic and structural information to select important words for coding we evaluate our model to the cnn daily mail text summarization datasets the experimental results show that the proposed gcns encoders model outperforms the state the art baseline models references ilya sutskever oriol vinyals and quoc v le quence to sequence learning with neural networks in advances in neural information processing systems pp minh thang luong hieu pham and christopher d to arxiv preprint manning based neural machine translation approaches effective antoine bordes y lan boureau and jason weston arxiv learning end to end goal oriented dialog preprint jiatao gu zhengdong lu hang li and victor ok li incorporating copying mechanism in sequence sequence learning arxiv preprint abigail see peter j liu and christopher d manning get to the point summarization with pointer generator networks arxiv preprint qingyu zhou nan yang furu wei and ming zhou selective encoding for abstractive sentence rization in meeting of the association for tional linguistics pp cnn dataset is the subset of cnn daily dataset and is also widely used for text summarization we also have achieved state of the art mance in this dataset wei li xinyan xiao yajuan lyu and yuanzhuo wang improving neural abstractive document summarization with explicit information selection modeling in ceedings of the conference on empirical methods in natural language processing pp jiwei tan xiaojun wan and jianguo xiao tive document summarization with a graph based tional neural model in proceedings of the annual meeting of the association for computational tics volume long papers vol pp wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun a unied model for tractive and abstractive summarization using tency loss arxiv preprint kaiqiang song lin zhao and fei liu infused copy mechanisms for abstractive in proceedings of the international tion ference on computational linguistics pp diego marcheggiani and ivan titov encoding tences with graph convolutional networks for semantic role labeling in proceedings of the conference on empirical methods in natural language processing pp yuhao zhang peng qi and christopher d manning graph convolution over pruned dependency trees proves relation extraction in proceedings of the conference on empirical methods in natural language processing pp joost bastings ivan titov wilker aziz diego graph marcheggiani and khalil simaan tional encoders for syntax aware neural machine lation in proceedings of the conference on pirical methods in natural language processing pp ramesh nallapati feifei zhai and bowen zhou marunner a recurrent neural network based sequence model for extractive summarization of documents in aaai pp shashi narayan shay b cohen and mirella ranking sentences for extractive arxiv preprint learning ata tion with reinforcement yen chun chen and mohit bansal fast abstractive summarization with reinforce selected sentence ing in proceedings of the annual meeting of the association for computational linguistics volume long papers pp shashi narayan ronald cardenas nikos topoulos shay b cohen mirella lapata jiangsheng yu and yi chang document modeling with nal attention for sentence extraction in proceedings of the annual meeting of the association for putational linguistics volume long papers vol pp qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao neural document marization by jointly learning to score and select tences in proceedings of the annual meeting of the association for computational linguistics volume long papers pp alexander m rush sumit chopra and jason weston a neural attention model for abstractive sentence marization arxiv preprint ramesh nallapati bowen zhou caglar gulcehre bing xiang al abstractive text summarization using sequence to sequence rnns and beyond arxiv preprint oriol vinyals meire fortunato and navdeep jaitly pointer networks in advances in neural information processing systems pp sebastian gehrmann yuntian deng and alexander rush bottom up abstractive summarization in ceedings of the conference on empirical methods in natural language processing pp alex graves and jurgen schmidhuber framewise phoneme classication with bidirectional lstm and other neural network architectures neural networks vol no pp zichao yang diyi yang chris dyer xiaodong he alex smola and eduard hovy hierarchical attention networks for document classication in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies pp danqi chen and christopher manning a fast and curate dependency parser using neural networks in proceedings of the conference on empirical ods in natural language processing emnlp pp romain paulus caiming xiong and richard socher a deep reinforced model for abstractive tion arxiv preprint chin yew lin rouge a package for automatic ation of summaries text summarization branches out
