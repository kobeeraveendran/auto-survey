r a m l c s c v v i x r a published as a conference paper at iclr amharic abstractive text summarization amr m zaki department of computer engineering ain shams university com mahmoud i khalil department of computer engineering ain shams university mahmoud asu edu eg hazem m abbas department of computer engineering ain shams university hazem asu edu eg abstract text summarization is the task of condensing long text into just a handful of tences many approaches have been proposed for this task some of the very rst were building statistical models extractive methods paice et al capable of selecting important words and copying them to the output ever these models lacked the ability to paraphrase sentences as they simply select important words without actually understanding their contexts nor understanding their meaning here comes the use of deep learning based architectures tive methods chopra et al et al which effectively tries to understand the meaning of sentences to build meaningful summaries in this work we discuss one of these new novel approaches which combines curriculum learning with deep learning this model is called scheduled sampling bengio et al we apply this work to one of the most widely spoken african languages which is the amharic language as we try to enrich the african nlp community with top notch deep learning architectures dataset and word embedding for the amharic language working with the amharic language turned to be quite challenging as african languages are typically known to be low resource languages data for our task was nt easily collected as there is nt an available dataset for our task this is why we had to collect and build our own dataset from scratch data for text summarization is found in form of long text articles and their summaries titles for the english case researchers work on data scrapped from cnn dailynews hermann et al so we used their same approach and scrapped data from well known amharic news websites goolgule ethiopianregistrar com amharic ethsat com com amharic zehabesha com ethiopianregistrar com ethiopianreporter com we scrapped over articles and only used those with long titles about articles word embedding has proved itself as one of the best methods to represent text for deep models one of the most widely used english word embedding models is et al it represents each word with a list of vectors to be easily used in the deep models however no such models were trained for the amharic language this is why we trained our own model for this task com published as a conference paper at iclr in this work we provide both the scrapped news dataset and the trained word embedding as open source to help enrich the african nlp research community text summarization deep learning building blocks text summarization is considered as a time series problem as we are trying to generate the next word given the past words novel deep models rely on some basic blocks in this section we go through these building blocks using lstm with attention since our task is a time series problem rnn models were rst used to address this task however given the long sentence dependencies in natural languages lstm based architectures were used given its memory structure hochreiter schmidhuber our task can actually be seen as mapping between input and output however since they differ in length long input short output based architectures are used nallapati et al to give our models even more human like abilities in summarization bahdanau et al suggested building a deep model on top of the architecture which helped it attend to important words in the input pointer generator model this previously discussed model has a well known problem which is working with unknown of vocab words as it can only be trained on a xed sized vocabulary a solution was proposed by nallapati et al et al which builds a deep model on top of the architecture capable of learning when to copy words and when to generate new ones scheduled sampling one of the problems that the above based architecture suffers from comes from the way it is trained as the model is trained by supplying it both an input long text and a reference short summary while when we test the model we only supply it with the input long text and no reference is given this forms an inconsistency between the training phase and the testing phase as the model has never been trained to depend on its own this problem is called exposure bias ranzato et al a solution proposed by bengio et al helped in solving this problem which included combining curriculum learning with our deep model we start the training normally by supplying both the long training text and the reference summary but when the model becomes mature enough we gradually introduce the model to its own mistakes while training decreasing its dependency on the reference sentence teaching the model to depend on itself in the training phase in other words making the learning problem more difcult while the model matures hence curriculum learning com theamrzaki text summurization abstractive methods tree master amharic figure scheduled sampling architecture published as a conference paper at iclr experiments we have applied the scheduled sampling model on the amharic dataset that we have built we have used google colab as our training framework as it provides us with free gpu and up to gb of ram our model is built over the library keneshloo et al we have modied it to work on and to work with the amharic dataset we evaluate our experiments using well known metrics used for evaluating text summarization these metrics are and et al which measure the amount of n grams that overlap between the reference summary and our generated one as the measure increases the amount of overlap increases indicating a better output we ran our evaluation on test sentences scores were rouge rouge rouge for comparison running scheduled sampling on english well known datasets cnn dailymail dataset achieves of and this discrepancy of the results from the english counterpart comes from the fact that the english dataset is huge articles with long summary compared to our scrapped amharic dataset of articles with short summaries this comes from the fact that collecting english dataset is comparatively much easier than collecting an african one due to the huge amount of available english resources conclusion by building a custom word embedding model for a specic african language we are able to apply any deep model that works on english on that selected african language like what we have proven by our work in our coming work we are willing to experience with other advanced architectures that have recently proven extremely efcient in addressing problems one of these architectures is bert devlin et al which stands for bidirectional encoder representations from transformers it uses a similar encoder decoder architecture but instead of using recurrent based cells it only uses attention self attention one efcient way to use bert is by using an already pre trained model that has been trained on the english dataset cnn dailymail dataset and then apply cross lingual transfer to the amharic dataset we believe that by this we believe that this may actually result in better summaries in spite of the relatively small amharic dataset we hope that by this work we have helped pave the way in applying novel deep learning techniques for african languages we also hope that we have contributed a guideline in applying deep models that can be further used in other nlp tasks published as a conference paper at iclr references dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation by jointly learning to align and translate arxiv preprint samy bengio oriol vinyals navdeep jaitly and noam shazeer scheduled sampling for sequence prediction with recurrent neural networks in proceedings of the international conference on neural information processing systems volume pp cambridge ma usa mit press sumit chopra michael auli and alexander m rush abstractive sentence summarization with attentive recurrent neural networks in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies pp san diego california june association for computational linguistics url aclweb org anthology jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language understanding in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pp minneapolis minnesota june association for computational linguistics url https aclweb org anthology karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend in c cortes n d lawrence d d lee m sugiyama and r garnett eds advances in neural information processing systems pp curran associates inc url nips cc teaching machines to read and comprehend pdf sepp hochreiter and jurgen schmidhuber long short term memory neural comput november issn neco url neco yaser keneshloo tian shi naren ramakrishnan and chandan reddy deep reinforcement learning for sequence to sequence models ieee transactions on neural networks and learning systems tnnls julian kupiec jan o pedersen and francine chen a trainable document summarizer in sigir chin yew lin rouge a package for automatic evaluation of summaries in text summarization branches out pp barcelona spain july association for computational linguistics url aclweb org anthology tomas mikolov g s corrado kai chen and jeffrey dean efcient estimation of word tions in vector space pp ramesh nallapati bowen zhou cicero dos santos c aglar gulcehre and bing xiang abstractive text summarization using sequence to sequence rnns and beyond in proceedings of the signll conference on computational natural language learning pp berlin germany august association for computational linguistics url aclweb org anthology chris d paice constructing literature abstracts by computer techniques and prospects information processing management kishore papineni salim roukos todd ward and wei jing zhu bleu a method for automatic evaluation of machine translation in proceedings of the annual meeting of the association for computational linguistics pp philadelphia pennsylvania usa july association for computational linguistics url aclweb org anthology published as a conference paper at iclr marcaurelio ranzato sumit chopra michael auli and wojciech zaremba sequence level training with recurrent neural networks in international conference on learning representations iclr san juan puerto rico may conference track proceedings url org abigail see peter liu and christoper manning get to the point summarization with generator networks pp
