published conference paper iclr amharic abstractive text summarization amr zaki department computer engineering ain shams university com mahmoud khalil department computer engineering ain shams university mahmoud asu edu hazem abbas department computer engineering ain shams university hazem asu edu abstract text summarization task condensing long text handful tences approaches proposed task rst building statistical models extractive methods paice capable selecting important words copying output models lacked ability paraphrase sentences simply select important words actually understanding contexts understanding meaning comes use deep learning based architectures tive methods chopra effectively tries understand meaning sentences build meaningful summaries work discuss new novel approaches combines curriculum learning deep learning model called scheduled sampling bengio apply work widely spoken african languages amharic language try enrich african nlp community notch deep learning architectures dataset word embedding amharic language working amharic language turned challenging african languages typically known low resource languages data task easily collected available dataset task collect build dataset scratch data text summarization found form long text articles summaries titles english case researchers work data scrapped cnn dailynews hermann approach scrapped data known amharic news websites goolgule ethiopianregistrar com amharic ethsat com com amharic zehabesha com ethiopianregistrar com ethiopianreporter com scrapped articles long titles articles word embedding proved best methods represent text deep models widely english word embedding models represents word list vectors easily deep models models trained amharic language trained model task com published conference paper iclr work provide scrapped news dataset trained word embedding open source help enrich african nlp research community text summarization deep learning building blocks text summarization considered time series problem trying generate word given past words novel deep models rely basic blocks section building blocks lstm attention task time series problem rnn models rst address task given long sentence dependencies natural languages lstm based architectures given memory structure hochreiter schmidhuber task actually seen mapping input output differ length long input short output based architectures nallapati models human like abilities summarization bahdanau suggested building deep model architecture helped attend important words input pointer generator model previously discussed model known problem working unknown vocab words trained xed sized vocabulary solution proposed nallapati builds deep model architecture capable learning copy words generate new ones scheduled sampling problems based architecture suffers comes way trained model trained supplying input long text reference short summary test model supply input long text reference given forms inconsistency training phase testing phase model trained depend problem called exposure bias ranzato solution proposed bengio helped solving problem included combining curriculum learning deep model start training normally supplying long training text reference summary model mature gradually introduce model mistakes training decreasing dependency reference sentence teaching model depend training phase words making learning problem difcult model matures curriculum learning com theamrzaki text summurization abstractive methods tree master amharic figure scheduled sampling architecture published conference paper iclr experiments applied scheduled sampling model amharic dataset built google colab training framework provides free gpu ram model built library keneshloo modied work work amharic dataset evaluate experiments known metrics evaluating text summarization metrics measure grams overlap reference summary generated measure increases overlap increases indicating better output ran evaluation test sentences scores rouge rouge rouge comparison running scheduled sampling english known datasets cnn dailymail dataset achieves discrepancy results english counterpart comes fact english dataset huge articles long summary compared scrapped amharic dataset articles short summaries comes fact collecting english dataset comparatively easier collecting african huge available english resources conclusion building custom word embedding model specic african language able apply deep model works english selected african language like proven work coming work willing experience advanced architectures recently proven extremely efcient addressing problems architectures bert devlin stands bidirectional encoder representations transformers uses similar encoder decoder architecture instead recurrent based cells uses attention self attention efcient way use bert pre trained model trained english dataset cnn dailymail dataset apply cross lingual transfer amharic dataset believe believe actually result better summaries spite relatively small amharic dataset hope work helped pave way applying novel deep learning techniques african languages hope contributed guideline applying deep models nlp tasks published conference paper iclr references dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate arxiv preprint samy bengio oriol vinyals navdeep jaitly noam shazeer scheduled sampling sequence prediction recurrent neural networks proceedings international conference neural information processing systems volume cambridge usa mit press sumit chopra michael auli alexander rush abstractive sentence summarization attentive recurrent neural networks proceedings conference north american chapter association computational linguistics human language technologies san diego california june association computational linguistics url aclweb org anthology jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language understanding proceedings conference north american chapter association computational linguistics human language technologies volume long short papers minneapolis minnesota june association computational linguistics url https aclweb org anthology karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend cortes lawrence lee sugiyama garnett eds advances neural information processing systems curran associates inc url nips teaching machines read comprehend pdf sepp hochreiter jurgen schmidhuber long short term memory neural comput november issn neco url neco yaser keneshloo tian shi naren ramakrishnan chandan reddy deep reinforcement learning sequence sequence models ieee transactions neural networks learning systems tnnls julian kupiec jan pedersen francine chen trainable document summarizer sigir chin yew lin rouge package automatic evaluation summaries text summarization branches barcelona spain july association computational linguistics url aclweb org anthology tomas mikolov corrado kai chen jeffrey dean efcient estimation word tions vector space ramesh nallapati bowen zhou cicero dos santos aglar gulcehre bing xiang abstractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning berlin germany august association computational linguistics url aclweb org anthology chris paice constructing literature abstracts computer techniques prospects information processing management kishore papineni salim roukos todd ward wei jing zhu bleu method automatic evaluation machine translation proceedings annual meeting association computational linguistics philadelphia pennsylvania usa july association computational linguistics url aclweb org anthology published conference paper iclr marcaurelio ranzato sumit chopra michael auli wojciech zaremba sequence level training recurrent neural networks international conference learning representations iclr san juan puerto rico conference track proceedings url org abigail peter liu christoper manning point summarization generator networks
