p e s l c s c v v x r rule abstractive text summarization bullet points tomonori kodaira com graduate school system design tokyo metropolitan university mamoru komachi ac jp graduate school system design tokyo metropolitan university abstract neural network based approaches come widespread abstractive text rization previously proposed models abstractive text summarization addressed problem repetition tents summary itly consider information structure reasons previous models failed account information structure erated summary standard datasets clude summaries variable lengths ing problems analyzing information ow specically manner rst tence related following sentences use dataset containing maries bullet points pose neural network based abstractive marization model considers tion structures generated summaries experimental results mation structure summary trolled improving performance overall summarization introduction summarization achieved proaches extractive abstractive proaches extractive approach involves ing document e sentence phrase word construct summary contrast stractive approach involves generating document summary words necessarily present document grammatical summary abstractive proach involves directly tracting output expressions source text obvious words present source text selected case extractive approach abstractive proaches increasingly popular tomatic summarization tasks previous work rush et al proposed abstractive sentence summarization method volves generating novel words summary based sequence sequence model proposed sutskever et al furthermore recently provements abstractive text summarization method proposed nallapati et al et al proposed model generates uent summaries owing use large scale dataset produce structured summarization trained cnn daily mail datasets annotated structural information work focus generating structured summary document particular summary sentences cnn daily mail datasets include summaries ing number sentences annotated information structures directly considering employ japanese summarization dataset livedoor news size cnn daily mail datasets livedoor news broadcasts news summary sentences easy analyze summaries dataset extractive approach nallapati et al li et al considered yield produce summary bullet points rst annotate dataset information ture train binary classier formation structure summaries build marization sub models dataset finally obtained summarization model selects summary structure based input generates mary according desired structure contributions work follows cnn daily mail datasets particular line model based proposed pati et al improved cluding hybrid pointer generator network erage mechanism model based model describe model greater tail subsection annotated analyzed structure summaries japanese news summarization dataset summaries form sentences proposed model generates summary bullet points related works dataset case abstractive sentence summarization rush et al proposed new summarization method generate abstractive summary sequence sequence model particular achieved state art performance gigaword corpora contrast abstractive text summarization ing cnn daily mail datasets objective output summary article consisting sentences nallapati et al proposed proved summarization model task essentially attention encoder decoder model cluding trick use large vocabulary jean et al switching pointer generator mechanism hierarchical networks proposed new dataset multi sentence summarization established benchmark dataset works address lem information structure generated mary work attempt sider information structure summary ther improve summarization model model currently model proposed et al state art summarization model com attention encoder decoder let input quences tokens article wi output sequences tokens summary yi tional long short term memory lstm network encoder unidirectional lstm decoder sequence encoder den states hi produced encoder step t decoder receives word embedding previous word decoder state st tention distribution calculated bahdanau et al vt wsst ba v wh ws ba learnable parameters attention distribution indicates importance encoder hidden states probability bution time step t furthermore context vector h t computed follows h t ihi context vector concatenated decoder state st input linear layers duce vocabulary distribution pvocab pvocab st h v v b learnable parameters training loss timestep t tive log likelihood target word w t timestep calculate overall loss entire sequense losst log t loss t t losst training previous word reference testing previous word output decoder hybrid pointer generator network et al proposed hybrid pointer generator work combines attention vocabulary distributions particular pointer generator network hybrid sequence sequence attention model section pointer network vinyals et al network ers source target word distributions addresses problem unknown word generation pointer generator model eration probability pgen time step t calculated context vector h t decoder state st decoder input xt pgen hh t wt s st wt xt bg vectors wh ws wx scalar bg able parameters represents sigmoid tion pgen soft switch select word vocabulary distribution pvocab word attention distribution ment authors developed extended vocabulary union vocabulary words source document probability tended vocabulary calculated follows p w pgen wi w w vocabulary oov word addition w exist source words wi w coverage mechanism aside mentioned changes et al improved coverage model tu et al address etition problem model coverage vector ct sum attention distributions previous decoder timesteps saved vector wc learnable parameter vents attention mechanism repeatedly iting location document avoids generating repetitive text addition constructed new loss function incorporate coverage loss penalize repeated attention location losst log p w t ct annotation summary structure dataset crawled pairs japanese articles summaries livedoor news manner similar vious work tanaka et al summaries written human editors particular summaries consist exactly sentences discuss detail later crawled data january december included pairs articles summaries divided pairs training pairs opment pairs test pairs development test pairs extracted data uary december included pairs articles summaries month article tagged category selected primary category selected available gories furthermore articles included special tags keywords key phrases specic category information crawled data news item includes title article shorter title abstractive summary experiments use ticle summary dataset useful information exploited future work summaries livedoor news dataset sist sentences enables lyze structure output annotated maries structure sentences opment test data summaries typically single sentences lengths characters livedoor world business entertainment sports ct annotation ct distribution source document words indicates magnitude attention word source document receives timestep t coverage vector applied attention mechanism follows vt wsst wcct et ba movies foods lifestyle women latest parallel parallel enumeration sequence sequence segmented sents dev test total table results annotation summaries mately summaries tagged allel remainder tagged sequence sequence summary second tences simply indicate examples related rst sentence form sentence addition annotation revealed rst tence similar title second sentences include additional information example rst sentence rst second sentences successfully generated existing models sentence plays roles dataset particular sequence summary sentence based second sentence parallel summary sentence based rst sentence proposed model uses characteristic generate sentence summary structure aware summarization model generate summary considering structure predict structure summary generated generate summary according predicted structure summary structure classication need train structure specic summarization models parallel sequence types training dataset annotated summary types build binary classier summary types summary information label summaries binary classier summaries signed parallel sequence types figure let input sequences tokens summary xi output label l use bidirectional lstm encoder sequence encoder hidden states hi produced encoder furthermore nal hidden states forward backward encoders concatenated finally linear figure parallel sequence summary structures counted width average length summaries test set form pre processing annotation refer original article assign labels preliminary investigation divide summaries types parallel parallel enumeration sequence sequence mented sentences enumeration summary recommended items introduced segmented sentences summary originally longer sentence ticle summaries divided following types parallel sequence ure illustrates difference types summary types rst sentence scribes primary incident second tence contains additional information mary incident case parallel type sentence explains rst sentence content different second sentence contrast case sequence type sentence includes detailed tion second sentence parallel types particular order terms ond sentences sequence types sequences order subject addition omitted japanese sentence s zero subject second sentence s subject annotate sentence summary marked instance requires zero anaphora resolution generate appropriate output analysis table lists results annotation maries livedoor news dataset precision recall parallel sequence table classication results based summaries precision recall parallel sequence table classication results based articles input testing articles training based classication results model proposed summarization model selects structure specic summarization model output nal summary figure summary structure classication mation applied vector h type experiments hf orward hbackward n yparallel bp ysequence bs wp ws bp bs learnable parameters structure specic sub models second struct structure specic summarization models automatically annotated dataset section describe base tion model et al particular pre train structure specic summarization els training data et al regardless summary structures form ne tuning type cally annotated dataset structure aware summarization finally build binary classier summary types case use articles summaries available test data generate summary based type predicted second summary type classier structure specic summarization models decide model generate summary construct summary structure classication model manner similar case summaries specied experiment classication setup articles summaries mented mecab ipadic hidden states represented dimensional matrices word embeddings dimensional vectors size vocabulary includes words appear model trained adagrad duchi et al learning rate annotated portion development data divided training test pairs spectively summary structure biased indicated results table optimize model sampling achieve high cision particular sample data label precision exceeds results summary input table lists test results number classied summaries training data obtained instances automatically labeled parallel quence summaries respectively results article input table lists sults classication summary structures ing articles input accuracy binary sication number instances com mecab coverage r l parallel model r l sequence model r l proposed r l parallel sequence table evaluation results bullets summarization coverage baseline model proposed et al parallel model sequence model sub models proposed method pair pairs r l r l r l table evaluation pairwise alignment leftmost column indicates alignment sentence system summary oracle summary columns list score sentence system summary bolded scores lowest scores sequence data parallel data recall lower parallel data experiment summarization similar et al setup experiment use models baseline et al proposed els hidden states models represented dimensional matrices word embeddings represented dimensional vectors cabulary size words case source target documents furthermore followed et al perform data cleanup operations ticles truncated words ning article training test sets summaries shorter words excluded models trained adagrad duchi et al learning rate gradient clipping maximum gradient norm evaluation metric use scores rouge l lin evaluate output evaluation applied test cases parallel summaries sequence maries respectively tem summary sentence oracle summary evaluate consistency alignment selected maximize average score rouge l condition duplicate result tables list evaluation results parallel sequence test data seen tables proposed models perform baseline model coverage clear table manner similar results table sequence model outperforms ers average evident table model behaves differently explore reason section discussion model comparison values table ave values table scores sequence model said sequence model highest considers previous sentences parallel model generating sentence mary parallel model fails incorporate formation second sentence generating sentence table furthermore align sentence table shows example summarization ave ave ave coverage r l parallel model r l sequence model r l proposed r l evaluation results test data coverage r l parallel model r l sequence model r l proposed r l evaluation results parallel test data coverage r l parallel model r l sequence model r l proposed r l evaluation results sequence test data table evaluation results sentence breakdown scores row computed rst sentence system summary sentence oracle summary rows worked process row ave average scores example annotated parallel type rst sentence reference mentions result game difcult models erate information described article models quence model produces sentence similar erence attributed ability ate second sentence based previous tion effectively models pairwise evaluation largest proportion pair combinations scores high pairs second sentences lel type summary particular order contrast score sentences verse order low e scores shown bold row second column based discussion suggested rst sentence correctly generated der evaluate summary structure appropriately analyze system summary greater detail evaluate performance proposed method pair combination table lists results pairwise evaluation pairs occurred frequently considered summaries case sequence parallel types suitably generated scores pairs higher pairs account second evaluation methods work evaluated generated summaries methods evaluation applied ordinary evaluation discussed section evaluation pair based rouge l score discussed section ordinary evaluation indicates summary mativeness contrast evaluation pair shop text summarization branches pages ramesh nallapati bing xiang bowen zhou abstractive text summarization sequence proceedings sequence rnns ceedings signll conference tational natural language learning conll pages ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments proceedings thirty aaai ference articial intelligence aaai pages alexander m rush sumit chopra jason weston neural attention model abstractive tence summarization proceedings ference empirical methods natural language processing emnlp pages abigail peter j liu christopher d manning point summarization generator networks proceedings annual meeting association computational tics acl pages ilya sutskever oriol vinyals quoc v le quence sequence learning neural networks advances neural information processing systems nips pages shun tanaka ryohei sasano hiroya takamura manabu okumura news summarization constraints summary sentence lengths number sentences proceedings nual meeting association natural language proacessing pages zhaopeng tu zhengdong lu yang liu xiaohua liu hang li modeling coverage neural chine translation proceedings annual meeting association computational tics acl pages oriol vinyals meire fortunato navdeep jaitly advances neural pointer networks formation processing systems nips pages appendix example generated summary shows proposed model generated summary sentences noted generated mary consist order sentence compared oracle summary performing pairwise evaluation rouge l able evaluate order summary appropriately dataset conclusion study constructed dataset focused summaries sentences tated analyzed structure summaries considered dataset particular posed structure aware summarization model bining summary structure classication model summary specic summarization sub models experiment demonstrated proposed model improves summarization mance baseline model future work use category gory tags analyze characteristics gory build specic models improve summarization system references dzmitry bahdanau kyunghyun cho yoshua neural machine translation jointly gio proceedings learning align translate international conference learning tations john duchi elad hazan yoram singer adaptive subgradient methods online learning stochastic optimization journal machine learning research pages sebastien jean kyunghyun cho roland memisevic yoshua bengio large target vocabulary neural machine translation ceedings annual meeting tion computational linguistics national joint conference natural language cessing acl ijcnlp pages chen li xian qian yang liu vised bigram based ilp extractive summarization proceedings annual meeting sociation computational linguistics acl pages chin yew lin rouge package automatic evaluation summaries proceedings source cologne s local paper express praised yuya osako demonstrated power game burg day reported demonstrated power japan representative fw showed trying toss wolfsburg defense team season insufcient scoring ability received erce criticism passionate cologne fans cologne coach stoger dare avoid appointment home game nonetheless head coach stoger manager joerg schmadtke constantly defending osako ful football player hand cf modeste showing team s scorers knew main job cf said stoger wanted appoint resulting positions trial error season s cologne adopted system opening clear play cf asserted osako got goals rst round pokal scored test match international matches rst opportunity season rudnevs injury osako showed success press paper praised osako showed ball keeping ability speed breadth vision scoring ability game cologne s fw team appreciated player achieves performance score day score casteels played blinder wants maintain good performance cf professional career secure xed position reference cologne yuya osako belongs drew wolfsburg local newspaper praised osako tossed wolfsburg defense team cologne s fw team described having performance coverage rouge cologne s local paper express reported yuya osako demonstrated power defends osako wonderful football player wolfsburg game understood clear said parallel model rouge cologne s local paper express recieved erece criticism team scorers fan knew cf main job said head coach stoger blinder season yuya osako said clear play cf sequence model cologne s local paper express reported yuya osako demonstrated power wolfsburg game japan representative fw tried toss wolfsburg defense team praised demonstrated performance understood clear defended table example generated bullet points summarization parallel structure retrieved livedoor com article january
