from standard summarization to new tasks and beyond summarization with manifold information shen xiuying zhaochun dongyan and rui institute of computer technology peking university beijing china academy of articial intelligence of computer science and technology shandong university qingdao china shengao xy chen zhaody edu cn zhaochun edu cn a m l c s c v v i x r a abstract text summarization is the research area aiming at creating a short and condensed version of the inal document which conveys the main idea of the document in a few words this research topic has started to attract the attention of a large nity of researchers and it is nowadays counted as one of the most promising research areas in eral text summarization algorithms aim at using a plain text document as input and then output a mary however in real world applications most of the data is not in a plain text format instead there is much manifold information to be rized such as the summary for a web page based on a query in the search engine extreme long ument e academic paper dialog history and so on in this paper we focus on the survey of these new summarization tasks and approaches in the real world application introduction the rapid growth of world wide web means that document oods spread throughout the internet readers get drown in the sea of documents wondering where to access text marization system aims at generating a condensed version of a document and conveying the main idea to the reader users can save a lot of time by reading the summary instead of the whole document to capture the main idea hence many sites and applications deploy automatic summarization tems and researchers in natural language processing nlp eld also focus on the text summarization task generally speaking there are two types of text rization one is designed for the most common scenario that summarizes a plain text with hundreds of words and the most popular usage is the news summarization the other one on the contrary is designed for generating summary with fold information in which input may be a structured document or document with additional knowledge different from the plain text summarization task these new summarization tasks aim to produce a better and appropriate summary by rating manifold information in many real world applications corresponding author rui yan edu cn figure new summarization tasks introduced in this paper and scenarios for example for a search engine it is better to summarize the web page according to the user s query stead of just summarizing the web page ignoring the query another example is dialog summarization with the ment of online chatting people always chat with people on the web for business or chitchat e on slack or whatsapp especially in the business scenario it is helpful to give the user a summary of what has been talked in the past days fore they starting a new dialog session or give a brief duction to the people who just join the group chat about what has been discussed in this group in contrast to the prosperity of survey on plain text marization task there are no systematic introductions to proaches about how to build an efcient summarization tem which can leverage the manifold information such as structure information of document and additional knowledge in the research community thus in this survey paper we present a literature review for new summarization tasks and their corresponding methods the tasks are listed in figure there are eight summarization tasks introduced in this per stream document timeline document treme long document dialog query based document incorporating reader comment template based multi media in these tasks the rst ve tasks can be ed into summarization task with special incorporating ument structure and the last three tasks can be classied into incorporating additional knowledge into summarization unlike the conventional plain text summarization methods which are built all with hand crafted rules or feature new summrization tasks stream document summarization timeline summarization template based summarization reader aware summarization multi modal summarization extreme long document summarization query based summarization dialog summarization incorporating document structure incorporating additional knowledge table leaderboard of document summarization task on dailymail dataset neural methods duc rouge l extractive methods nallapati et al narayan and et al wu and hu zhang et al liu and lapata abstractive methods see et al paulus and et al hsu and et al celikyilmaz and et al liu and lapata ing recently many researchers begin to develop some driven approaches to build summarization systems since these approaches can leverage the publicly available large scale dataset and the rapid progress of deep learning proaches instead of using time consuming hand crafted ture engineering therefore we believe it is useful and able to summarize recent progress on new summarization tasks this survey paper is partially based on our ous efforts on building summarization models for new marization tasks we will introduce the problem formulation data collection and the proposed methods for these tasks preliminary standard summarization in this section we will introduce some generally used marization frameworks based on conventional and based learning methods these frameworks are used as the basis of the methods for new summarization tasks conventional methods early conventional approaches to extractive summarization include centroid based methods radev and et al lin and et al supervised and semi supervised ods wong and et al tree and graph based ods kikuchi and et al qian and et al morita and et al filippova and et al submodular methods morita and et al li and et al lin and et al and ilp based methods gillick and et al li and et al banerjee and et al less extractive approaches only extract some phrases or tences from the original document as the summary and it can not produce the condensed and uent summary yates and et al on the contrary the conventional stractive summarization methods usually extract some words from document and then reorder and perform motivated transformations to the words dorr and al banko and et al cohn and lapata barzilay and mckeown tanaka and et al however these paraphrase based generation method are easy to produce uent sentences figure techniques used in neural based summarization methods in contrast to the conventional learning methods based approaches provide an end to end method to rization task in this section we introduce some widely used techniques in these methods as shown in figure and we split the these methods into two categories extractive and abstractive most of these works conduct the experiments on a benchmark dataset cnn dailymail and we list the mance in terms of rouge score lin at table first we will introduce the extractive methods which tract sentences from the document as the summary since the extractive methods use the sentence as the basic unit the rst step is to obtain a sentence representation the most common way cheng and lapata narayan and et al is to employ a recurrent neural network rnn or convolutional neural network cnn to encode the words in a sentence and then obtain a vector representation ter obtaining the sentence representation cheng and ata rst propose a framework with encoder decoder using an rnn to tackle the extractive summarization task which uses the encoder to obtain the vector representation of sentences and use the decoder with an attention nism to extract sentence since the encoder decoder marization framework narayan and et al needs two rnn that computes slowly many researchers start to use the sequence labeling framework nallapati et al zhou and et al for this task which use an rnn to read the sentences only once to deeply understand the document researchers incorporate the memory network chen et al into summarization framework which gives the ing ability to the model since previous works use the entropy as the loss function to train the model which has a gap with the testing stage that use the rouge lin score wu and hu propose to use the reinforcement ing method to directly optimize the rouge score in recent two years the pre training techniques growth rapidly in nlp eld researchers employ the language model pre training model like elmo bert into summarization task liu and lapata zhang et al to achieve better mance from table we can nd that liu and lapata achieve the state of the art performance on the cnn mail dataset the sequence to sequence based text generation ods sutskever et al li et al gao et al neural summarization methods sequence to sequence copy mechanism reinforcement learning selective encoding multi agent pre train pre train reinforcement learning sequence labeling encoder decoder abstractive methods extractive methods make generating uent and concise summary sible and nallapati and et al rstly apply the text generation method to the abstractive summarization task next many extensions on the text generation framework are proposed to achieve better performance in generating summary to avoid the out of vocabulary oov problem copy mechanism gu and et al see et al gulcehre and et al is proposed which directly copy the oov words like the name of person place or tion from the source document into the generated summary similarly researchers also use the reinforcement learning method in abstractive summarization paulus and et al wang and et al liu and et al for the same son as the extractive methods to help the summary tion module focus on the salient parts of source document selective encoding zhou and et al hsu and et al are proposed to encode the important semantic parts and ignore the trivial parts encoding the source document is a crucial step in summarization researchers celikyilmaz and et al propose to divide the hard document ing task into several sub tasks and solve it by multiple laborating encoder agents after encoding the document an attention mechanism is used to fuse all the document sentations produced by all the agents and then generate the summary pre training language model helps the model to capture the semantic of text that motivates the researchers to use the it as the encoder of summarization framework searchers employ the pre training technique into document reading module liu and lapata to capture the main idea of the document and achieve the state of the art mance in abstractive summarization task these methods use a plain text as input and they can not utilize the document structure or other easily acquired knowledge to improve the summarization performance summarization by incorporating document structure in this section we will introduce some summarization tasks in which input is structural text instead of a plain text these special structures will help the summarization model to ture the document main idea stream document summarization stream summarization task was rst introduced in tac dang and owczarzak which targets at rizing new documents in a continuously growing text stream such as news events and twitters when the new document rives in a sequence the stream summary needs to be updated along with considering previous information meantime initial works include boudin and et al which poses a scalable sentence scoring method for query oriented update summarization in this method candidate sentences are selected according to a combined criterion of query vance and dissimilarity with previously read sentences lort and alfonseca present an unsupervised tic approach to model novelty in a document collection and apply it to the generation of update summaries ge and et al propose a graph ranking based method burst mation networks as a novel representation of a text stream in this method the graph node is a burst word including tities with the time span of one of its burst periods and an edge between two nodes indicates how strongly they are lated mnasri and et al is the state of the art work on the duc and tac dataset which examines how integrating a semantic sentence similarity into an update summarization system can improve its results current state of the art methods for this task are all based on human engineered and extractive methods hong and et al mnasri and et al nowadays there are many stream data provided on the internet such as tweets focus on the same news topic and news of a big event like a presidential election a natural disaster consequently the abstractive based summarization methods will be a hot search area and will be explored in the near future timeline summarization classic news summarization plays an important role with the exponential document growth on the web many approaches are proposed to generate summaries but seldom ously consider evolutionary characteristics of news plus to traditional summary elements timeline summarization is an important research task which can help users to have a quick understanding of the overall evolution of any given topic it thus attracts much attention from research communities in cent years to solve this task one should rst identify which sub events are salient and then generate a summary the big difference between timeline summarization and stream marization task is whether the model can see all the sub event timeline summarization task is rstly proposed by allan and et al in this paper they propose a method that tracts a single sentence from each event within a news topic later a series of works et al yan et al yan et al zhao et al further investigate the timeline summarization task and all of them are based on conventional learning method to extract sentences from the timeline data for instance et al formulate the timeline summarization task as a balanced optimization lem via iterative substitution the objective function used in this method is measured by four properties relevance erage coherence and diversity in recent years as an tant case of timeline information social media data is used by many timeline summarization research works for example ren et al considered the task of time aware tweets summarization based on a user s history and collaborative social inuences from social circles the previous works are all based on extractive methods which are not as exible as abstractive approaches chen et al rstly propose a key value memory based architecture to store the events described in the line in this key value memory network they use the event time representation as the key and split the value into two slots global value and local value the local value only captures event information from current event and the global value stores the global characteristics of events in different time position finally an rnn based decoder is employed to generate the summary abstractively to train their model they release the rst large scale timeline summarization dataset which contains document summary pairs collected from a cyclopedia website extreme long document summarization different from the previous summarization task in some narios the input document can be very long such as an demic paper or a patent document which is longer than the news article thus summarizing such an extreme long ment is still a challenging problem when using existing marization methods the biggest challenge of this task is to extract the salient information and central idea from a large amount of information first we introduce some benchmark datasets used the treme long document summarization in the era of using conventional machine learning methods in this task all of the researchers use the small scale scientic papers as the dataset teufel and moens teufel and moens liakata et al and the number document summary data pairs is less than in recent years most of the searchers employ the neural based methods which require a large amount of data to train the model thus many scale long document datasets have been proposed and the data comes from wikipedia liu and et al scientic papers cohan and et al patent documents sharma et al liu and et al propose to use a wikipedia web page with all the reference articles and the sults fetched from google as the long text input and there are document summary data pairs in this dataset cohan and et al propose a large scale scientic per summarization dataset which is collected from arxiv and pubmed and it contains document and summary pairs the average document length is and words in arxiv and pubmed respectively which is times longer than the news dataset cnn dailymail sharma et al propose a larger long document summarization dataset patent which is times larger than the pubmed and arxiv it contains us patent document and tion pairs and the average document length is words this dataset is more suitable for abstractive summarization task since the summary has more novel n grams than other long document summarization datasets in the initial works of this task researchers teufel and moens liakata et al use a supervised er to select content from a scientic paper based on some human engineered features then researchers have extended these works by applying more sophisticated classiers to identify more ne grain categories to determine whether a sentence should be included in the summary collins et al directly use the section each sentence appears in as a categorical feature with values like highlight abstract troduction as for the neural network based methods liu and et al rstly use an extractive summarization method to coarsely identify salient information and then employ a ral abstractive model to generate the summary cohan and et al propose a hierarchical model that uses two level rnn to encode the words and sections respectively and then they use an attention decoder to forms a context vector from both word and sentence level information they also duct experiments on arxiv and pubmed datasets and their model outperforms the baseline methods on these datasets xiao and carenini propose an extractive method for this task using both the global context of the whole ment and the local context within the current topic and this method achieves state of the art performance on the previous two datasets dialog summarization in recent years online chatting becomes more and more ular qiu al tao et al gao et al when the chatting history becomes very long it is consuming for people to review all the context before starting a new dialog thus some researchers focus on the task of summarizing the dialog history different from summarizing a document the salient information is scattered in the whole dialog history ganesh and dingliwal rst propose this task and they propose a pipeline method that consists of a sequence beling module to identify the salient utterance and a module with attention and copy mechanism since their dataset is in a small scale they use a news summarization dataset cnn dailymail to train the abstractive tion module and evaluate on a small scale dialog tion dataset with only sessions to leverage the based text generation method gliwa and et al pose the rst large scale dataset samsum for this task ferent from previous papers working on chit chats liu and et al propose a framework to generate a summary for online customer service which can help the staff to know what was happen without going through long and sometimes twisted utterances as another branch of dialog summarization task the ing summarization task is to generate a summary of meeting transcriptions shang and et al propose an vised abstractive meeting summarization using a graph based model and budgeted submodular maximization in recent years people usually hold a meeting using video calls instead of just using audio consequently additional visual tion can be used in meeting summarization such as the ipant s head pose and eye gaze li and et al propose a multi modal encoding framework that incorporates this sual information and employs a topic segmentation method to identify the topic transition in a dialog ow finally they employ the pointer generator see et al network to fusion the encoded information and generate the summary query based summarization in the typical web search scenario the search engine provides a list of web pages associated with their summaries ferent from the traditional document summarization in this scenario the summary should summarize the query focused aspect of the web page instead of the main idea inspire by this application many researchers start to focus on the based summarization task whose goal is to generate a mary that highlights those points that are relevant in the text of a given query in this task most of the methods are based on tional machine learning methods li and li propose a semi supervised graph based model and incorporate the lda topic model into summarization feigenblat and al propose an unsupervised multi document query based marization method using a cross entropy method which is a generic monte carlo framework for solving hard torial optimization problems different from previous tence extraction methods wang and et al employ a sentence compression method which uses three compression strategy rule based sequence based and tree based to duce the summary to avoid generating repeated phrases and increasing the versity of summary nema and et al rstly propose a neural based framework which ensures that context vectors in attention mechanism are orthogonal to each other specically to alleviate the problem of repeating phrases in the summary we treat successive context vectors as a quence and use a modied lstm cell to compute the new state at each decoding time step in decoding steps the tion mechanism is also used to focus on different portions of the query at different time steps summarization by incorporating additional knowledge in some summarization applications there are many ent types of additional knowledge that can be used to help the model enhance the performance the model can age this additional knowledge to capture the main idea of the document or generate more uent summaries reader aware summarization in most of the news websites they provide an area for the readers to post their comments on the news article in most cases the reader comments concentrate on the main idea of the news article thus these comments can be used to help the summarization model to capture the main idea of the news and then the model can generate a better summary with this help in this section we will introduce two kinds of methods which are based on conventional learning methods and neural networks respectively in the beginning et al rstly propose to derstand the input document with the feedback of readers ing a graph based method where they identify three relations topic quotation and mention by which comments can be linked to one another li et al employ a sparse coding based framework for this task which jointly considers news documents and reader comments via an unsupervised data construction strategy next we turn to the methods using neural networks li et al propose a sentence salience estimation framework based on a neural generative model called variational encoder vae in contrast to the previous methods which use sentence extraction methods on a small scale dataset gao et al rst propose a large scale dataset and a neural generative method rasg on this task this dataset contains data samples and each data sample has eral a document a summary and several reader comments the average comments number of a document is the proposed rasg method is a generative adversarial fellow et al based learning method which conducts the interaction between the reader comments and news cle to capture the reader attention distribution on the article and then use the reader focused article information to guide the summary generation process template based summarization to generate a uent summary template based summarization method rst retrieves a summary template and then edits it into the new summary of the current document existing methods can be classied into two categories hard editing and soft editing more specically hard editing methods force the system to generate the summary which is in the same language pattern as the template conversely editing methods can use partial words in the template and generate more exible summaries wang and cardie introduced a template based cused abstractive meeting summarization system their tem rst applies a multiplesequence alignment algorithm to generate templates and extracts all summary worthy relation instances from the document then the templates are lled oya et al propose a with these relation instances hard editing method that employs a multi sentence fusion gorithm in order to generate summary templates since the hard editing methods are not very exible editing methods become popular in recent years due to the velopment of the neural text generation method cao et al employ existing summaries as soft templates to ate a new summary in this method they use an information retrieval system to retrieve summaries of a similar document and then use an attention based generator to fuse the mation from the template and current document wang et al leverages template discovered from training data to softly select key information from each source article to guide its summarization process however this method nores the dependency between the template document and the input document following these works gao et al propose to analyze the dependency and use this dependency to help the model identify which facts in the input document are the salient facts that should be mentioned in the summary furthermore they use the relationship between template ument and template summary to extract the summary in plate that can be reused in generating a new summary gao et al they also propose a large scale dataset contains document and summary pairs in which summaries are all in patternized language and their method achieves state of the art performance on this dataset multi modal summarization with the increase of multi media data on the web many searchers focus on the multi modal summarization task zhu et al li et al chen and zhuge li et al palaskar et al li et al in recent years compared to the traditional text summarization task setting in the multi modal summarization task the visual tion is incorporated along with the input document into the text summarizing process to improve the quality of the erated summary in the beginning we rst introduce some datasets of li et al image based multi modal summarization chen and zhuge propose two large scale modal summarization datasets and each data sample in these datasets contains a source sentence an image collected from the webpage and a summary different from the previous two datasets which output is only text zhu et al propose the rst large scale multi modal input and modal output summarization dataset which input is a ment with several images and the ground truth is a text mary with the most relevant image selected from the input images li et al palaskar et al propose two video based multi modal summarization dataset which tains document video and summary pairs and the number of data sample are and respectively next we will introduce some existing methods of modal summarization task for the image based multi modal summarization task li et al chen and zhuge zhu et al propose to use a based tive model which has image and document encoders to obtain the representations of multi modal input and an rnn based decoder with multi modality attention to generate the mary since there are some abstract concepts in the source document which can not nd a counterpart in the image and not all the visual information is useful for generating mary to avoid introducing noise into summarization li et al propose to use an image attention lter and an image context lter as for the video based summarization palaskar et al employ a hara et al convolutional neural network to model the video frames and then fuse this video information into the using a hierarchical attention mechanism conclusion we have witnessed a rapid surge of summarization studies recently especially the research in the many new rization tasks summarization systems are catching on re the state of the art performance of summarization tasks has been pushed higher and higher in the real world applications most of them are not in a traditional summarization setting and they usually leverage manifold information since scale big data become more easily available in our living era and it requires much time for people to obtain the overall formation for an event we may stand at the entrance of future success in more advanced summarization systems it is our hope that this survey provides an overview of the challenges and the recent progress as well as some future directions in these new summarization tasks which leverages the manifold information acknowledgements we would like to thank the anonymous reviewers for their constructive comments this work was supported by the national key research and development program of china no the national science foundation of china nsfc no and nsfc no rui yan is partially supported as a young fellow of beijing institute of articial intelligence baai references allan and et al james allan and et al temporal summaries of new topics in sigir pages acm banerjee and et al siddhartha banerjee and et al multi document abstractive summarization using ilp based multi sentence compression in ijcai banko and et al michele banko and et al headline generation based on statistical translation in acl barzilay and mckeown regina barzilay and leen mckeown sentence fusion for multidocument news summarization computational linguistics boudin and et al florian boudin and et al a scalable mmr approach to sentence scoring for document update summarization in coling cao et al ziqiang cao wenjie li sujian li and furu wei retrieve rerank and rewrite soft template based neural summarization in acl celikyilmaz and et al asli celikyilmaz and et al deep communicating agents for abstractive tion in naacl chen and zhuge jingqiang chen and hai zhuge abstractive text image summarization using multi modal attentional hierarchical rnn in emnlp chen et al xiuying chen shen gao chongyang tao yan song dongyan zhao and rui yan iterative document representation learning towards summarization with polishing in emnlp chen et al xiuying chen zhangming chan shen gao meng hsuan yu dongyan zhao and rui yan learning towards abstractive timeline summarization in ijcai cheng and lapata jianpeng cheng and mirella pata neural summarization by extracting sentences and words in acl pages cohan and et al arman cohan and et al a discourse aware attention model for abstractive rization of long documents in naacl hlt cohn and lapata trevor cohn and mirella lapata sentence compression as tree transduction j artif intell res collins et al ed collins isabelle augenstein and sebastian riedel a supervised approach to extractive summarisation of scientic papers in conll pages august dang and owczarzak hoa trang dang and karolina owczarzak overview of the tac update tion task in tac delort and alfonseca jean yves delort and enrique alfonseca dualsum a topic model based approach for update summarization in eacl dorr and al bonnie j dorr and al hedge mer a parse and trim approach to headline generation in hlt naacl hong and et al kai hong and et al a repository of state of the art and competitive baseline summaries for generic news summarization in lrec feigenblat and al guy feigenblat and et al supervised query focused multi document summarization using the cross entropy method in sigir filippova and et al katja filippova and et al dency tree based sentence compression in inlg ganesh and dingliwal prakhar ganesh and saket dingliwal abstractive summarization of spoken and ten conversation arxiv gao et al shen gao xiuying chen piji li ming chan dongyan zhao and rui yan how to write learning towards abstractive summaries with patterns in emnlp summarization through prototype editing gao et al shen gao xiuying chen piji li zhaochun ren lidong bing dongyan zhao and rui yan abstractive text summarization by incorporating reader comments in aaai gao et al shen gao zhaochun ren yihong zhao dongyan zhao dawei yin and rui yan product aware answer generation in e commerce question answering in wsdm gao et al shen gao xiuying chen chang liu li liu dongyan zhao rui yan shen gao xiuying chen chang liu li liu dongyan zhao and rui yan learning to respond with stickers a framework of unifying modality in multi turn dialog in www ge and et al tao ge and et al news stream marization using burst information networks in emnlp gillick and et al dan gillick and et al a scalable global model for summarization in ilp gliwa and et al bogdan gliwa and et al samsum corpus a human annotated dialogue dataset for tive summarization in emnlp goodfellow et al ian goodfellow jean abadie mehdi mirza bing xu david warde farley sherjil ozair aaron courville and yoshua bengio erative adversarial nets in advances in neural information processing systems pages gu and et al jiatao gu and et al incorporating ing mechanism in sequence to sequence learning acl page gulcehre and et al caglar gulcehre and et al ing the unknown words hara et al kensho hara hirokatsu kataoka and yutaka satoh can spatiotemporal cnns retrace the tory of cnns and imagenet ieee cvf ence on computer vision and pattern recognition pages hsu and et al wan ting hsu and et al a unied model for extractive and abstractive summarization using inconsistency loss in acl et al meishan hu aixin sun and ee peng lim comments oriented document summarization derstanding documents with readers feedback in sigir kikuchi and et al yuta kikuchi and et al single document summarization based on nested tree structure in acl li and et al jingxuan li and et al multi document summarization via submodularity applied intelligence li and et al chen li and et al using supervised in acl bigram based ilp for extractive summarization li and et al manling li and et al keep meeting summaries on topic abstractive multi modal meeting summarization in acl li and li yanran li and sujian li query focused multi document summarization combining a topic model with graph based semi supervised learning in coling pages august et al piji li lidong bing wai lam huang li and yi liao reader aware multi document tion via sparse coding in ijcai et al haoran li junnan zhu cong ma jiajun zhang and chengqing zong multi modal tion for asynchronous collection of text image audio and video in emnlp et al piji li lidong bing and wai lam reader aware multi document summarization an in hanced model and the rst dataset et al haoran li junnan zhu tianshang liu ajun zhang and chengqing zong multi modal sentence summarization with modality attention and image ltering in ijcai et al h li j zhu c ma j zhang and c zong read watch listen and summarize modal summarization for asynchronous text image audio ieee transactions on knowledge and data and video engineering may et al juntao li lidong bing lisong qiu dongmin chen dongyan zhao and rui yan learning to write stories with thematic consistency and wording elty in aaai liakata et al maria liakata simon dobnik masree saha colin r batchelor and dietrich schuhmann a discourse driven content model for marising scientic articles evaluated in a complex question answering task in emnlp lin and et al chin yew lin and et al from single to multi document summarization a prototype system and its evaluation in acl lin and et al hui lin and et al multi document summarization via budgeted maximization of submodular functions in hlt naacl lin chin yew lin rouge a package for in text summarization matic evaluation of summaries branches out pages july liu and et al linqing liu and et al generative in versarial network for abstractive text summarization aaai liu and et al peter j liu and et al generating wikipedia by summarizing long sequences in iclr liu and et al chunyi liu and et al automatic logue summary generation for customer service in kdd liu and lapata yang liu and mirella lapata text in emnlp summarization with pretrained encoders mnasri and et al maali mnasri and et al taking into account inter sentence similarity for update tion in ijcnlp morita and et al hajime morita and et al subtree extractive summarization via submodular maximization in acl nallapati and et al ramesh nallapati and et al stractive text summarization using sequence to sequence rnns and beyond in conll nallapati et al ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural work based sequence model for extractive summarization of documents in aaai narayan and et al shashi narayan and et al ing sentences for extractive summarization with ment learning in naacl nema and et al preksha nema and et al diversity driven attention model for query based abstractive marization in acl oya et al tatsuro oya yashar mehdad giuseppe carenini and raymond t ng a template based tive meeting summarization leveraging summary and source text relationships in inlg palaskar et al shruti palaskar jindrich libovicky spandana gella and florian metze multimodal tive summarization for videos in acl paulus and et al romain paulus and et al a deep reinforced model for abstractive summarization in iclr qiu al lisong qiu juntao li wei bi dongyan zhao and rui yan are training samples correlated learning to generate dialogue responses with multiple erences in acl radev and et al dragomir r radev and et al centroid based summarization of multiple documents volume pages ren et al zhaochun ren shangsong liang edgar meij and maarten de rijke personalized time aware tweets summarization in sigir pages acm see et al abigail see peter j liu and pher d manning get to the point summarization with pointer generator networks in acl shang and et al guokan shang and et al pervised abstractive meeting summarization with sentence compression and budgeted submodular mization in acl sharma et al eva sharma chen li and lu wang bigpatent a large scale dataset for abstractive and ent summarization in acl sutskever et al ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural networks in nips tanaka and et al hideki tanaka and et al driven sentence revision for broadcast news tion in acl ijcnlp tao et al chongyang tao shen gao mingyue shang wei wu dongyan zhao and rui yan ing towards effective responses with multi head attention mechanism in ijcai teufel and moens simone teufel and marc moens summarizing scientic articles experiments with vance and rhetorical status computational linguistics wang and cardie lu wang and claire cardie domain independent abstract generation for focused meeting summarization in acl pages gust wang and et al lu wang and et al a sentence compression based framework to query focused document summarization in acl pages gust wang and et al li wang and et al a reinforced topic aware convolutional sequence to sequence model for abstractive text summarization in ijcai wang et al kai wang xiaojun quan and rui wang biset bi directional selective encoding with plate for abstractive summarization in acl pages july qian and al xian qian and et al fast joint in emnlp pression and summarization via graph cuts wong and et al kam fai wong and et al tive summarization using supervised and semi supervised learning in coling wu and hu yuxiang wu and baotian hu learning to extract coherent summary via deep reinforcement ing in aaai xiao and carenini wen giuseppe and carenini long extractive summarization of ments by combining global and local context in emnlp xiao et al rui yan liang kong congrui huang xiaojun wan xiaoming li and yan zhang timeline eration through evolutionary trans temporal tion in emnlp pages acl et al rui yan xiaojun wan jahna bacher liang kong xiaoming li and yan zhang tionary timeline summarization a balanced optimization framework via iterative substitution in sigir pages acm et al rui yan xiaojun wan mirella lapata wayne xin zhao pu jen cheng and xiaoming li sualizing timelines evolutionary summarization via in ative reinforcement between text and image streams cikm pages acm yates and et al alexander yates and et al in ner open information extraction on the web naacl zhang et al xingxing zhang furu wei and ming zhou hibert document level pre training of cal bidirectional transformers for document tion in acl zhao et al xin wayne zhao yanwei guo rui yan yulan he and xiaoming li timeline generation with cial attention in sigir pages acm zhou and et al qingyu zhou and et al selective in acl coding for abstractive sentence summarization zhou and et al qingyu zhou and et al neural ment summarization by jointly learning to score and select sentences arxiv zhu et al junnan zhu haoran li tianshang liu yu zhou jiajun zhang and chengqing zong msmo in multimodal summarization with multimodal output emnlp
