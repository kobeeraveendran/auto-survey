diverse beam search increased novelty abstractive summarization cibils andre musat claudiu hossmann andreea baeriswyl michael ecole polytechnique federale de lausanne epfl email rstname ch articial intelligence group swisscom ag email rstname com e f l c s c v v x r abstract text summarization condenses text shorter version retaining important informations abstractive summarization recent development generates new phrases simply copying rephrasing sentences original text recently neural sequence sequence models achieved good results eld abstractive summarization opens new possibilities applications industrial purposes practitioners observe models use large parts original text output summaries making similar extractive frameworks address drawback rst introduce new metric measure summary tracted input text secondly present novel method relies diversity factor computing neural network loss improve diversity summaries generated neural abstractive model implementing beam search nally method makes system extractive improves overall rouge score state art methods points introduction summarization process generating condensed version text contains key information original automatic summarization approaches extractive abstractive called generative extractive summarization focused nding relevant text spans phrases original document copying construct summary nal text consists exclusively passages input process viewed ranking mechanism jin et al second summary generation paradigm generate novel text produce summary generative cess uses words coming vocabulary seen source document human usually stractive summarization considered difcult requires high level competences generalization reformulation compared extractive summarization ensures baseline levels grammaticality accuracy advent powerful deep architectures generative adversarial networks text generation jeswar et al zhang et al pointer works schmidhuber generative summarization largely considered infeasible novel methods opening way new generation summarization systems carry promise near human text generation quality practitioners quickly discarded earlier generative models inability solve problems result look robotic incomprehensible chiey repetition recent model pointer generator network et al pgnet achieves state art term tomatic summarization widely dataset n dailymail hermann et al nallapati et al et al model abstractive sequence sequence neural model uses extractive tive summarization techniques overcome problems plagued previous sequence sequence neural models like repetition inability handle vocabulary oov words pgnet features improvements include attention hybrid mechanism enables model copy words original text pointer work schmidhuber inclusion pointer generation step clear tive impact performance system progress evident quantitatively e rouge scores itatively et al caveat extractive pointer generator component model option copying large swaths original text ing option method self defeating leads overly extractive behavior abstractive method argue drawback inherently common marizers use pointers elements original text work propose way measure tiveness abstractive summarization methods common methods plagiarism detection identify cases summarizer simply pieces large text spans original text vise novel method penalize behaviour use complement traditional evaluations like rouge n rouge l scores goal orthogonal evaluation discourage abstractive models going low hanging fruit copying second contribution method reduce tiveness abstractive summaries based diverse beam search dbs vijayakumar et al previous attempt li et al improve abstractive summaries dbs lacklustre results verse summary superior obtained baseline model propose method combining novelty dbs based summaries dbs based results qualitatively tively better previous state art major sult decrease extractive nature summarizer time increasing rouge l scores cnn daily mail dataset hermann et al method compatible abstractive summarizer uses beam search includes leading methods time writing lapati et al et al paulus et al chopra et al hasselqvist et al related work recently text summarization vast majority extractive wong et al chuang yang cusp major paradigm shift abstractive summarization largely good results tained recent models rst work succeed abstractive rization sequence sequence model ati et al introduced major dataset cnn daily mail use current work addresses multiple issues capturing hierarchy word structure emitting words rare training time independently chopra et al created tive recurrent model yielded good results shared task recently paulus et al created reinforcement model abstractive summarization came new state art cnn daily mail dataset surge interest abstractive summarization research community lead increasingly promising sults practice remaining issues clude use industrial settings inaccurately ing factual details inability deal vocabulary oov words repetition pgnet et al designed architecture tackle issues simultaneously pointing retrieving elements original text successfully alleviates impact tioned ills pgnet tends abuse copying mechanism generated summaries largely extractive address novel problem rst need quantify dene extractiveness abstractive rizer extracted summary copied original text extractiveness akin rism detection measures text extracted plagiarism detection studied problem frequently measures grams frequencies longest common sequence lcs zhang et al anzelmi et al techniques adequate measures extractiveness summary pect summary share lot words ment solution present reduce extractiveness based diverse beam search dbs vijayakumar et al dbs shown effective creating diverse image captions machine translation visual question eration variation classic beam search designed neural sequence models addresses lack versity original algorithm gimpel et al dbs multiple topics logue response generation asghar et al machine et al abstractive tion li et al dbs contributes marginally score performance stractive summarization combine candidate selection algorithm multiple elds imal marginal relevance mmr guo sanner carbonell goldstein mmr algorithm balances relevance diversity multiple set based mation retrieval tasks order use mmr needs compute ilarity sentences options exist recently good results obtained sentence embeddings instance ones based n gram features pagliardini et al produces cic word n gram embedding vectors tively combined sentence embedding similarly word embeddings mikolov et al allow represent semantic relatedness phrases summary generation rst section devoted briey describing baseline model pgnet et al second half describe main contributions hance pgnet order generate extractive summaries baseline model core idea current methods text tion consists leveraging corpus containing source documents summaries pgnet particular learns map input sequence words source ument sequence words summary document implementing known sequence sequence neural network architecture extended tention mechanism bahdanau et al additionally pgnet addresses shortcomings previous els inability handle vocabulary words repetition sankaran et al tu et al main novelty pgnet pointer work schmidhuber aggregates context vector produced attention mechanism decoder state making model able copy words original ument combine fragments output summary generated abstractive fashion ble model use words contained quences original document summary half composed new words outside lcs article address issue dene metric takes parameters lcs account generally nalizes large spans text copied original text merging sentences splitting penalized heavily copying wholly want metric consistent deterministic output malized score finally novel word combinations penalized dene extraction score extraction s sp acss acss set long non overlapping mon sequences summary s document p acss set proportion common sequences e element set length mon sequence divided length summmary rst start nding long common sequences tween summary original article use similar algorithm lcs return long non lapping common sequences extraction score ensures sum scores having distinct common sequences text proportion ized having common sequence text proportion summary consists long common quence document score summary new words encountered article score reference summaries average plagiarism score extraction score test set respectively means reference summaries mainly paraphrasing copying text article expected outlines value ness measure goal automated systems close zero case human standard extraction score allows compare different system rank respect generative capacity improved decoding mechanism solution extractivness problem complementary baseline architecture tive model employs beam search diverse beam search beam search iterative algorithm widely decode rnns nallapati et al et al paulus et al chopra et al hasselqvist et al approximates optimal solutions time step model computes yt yb t set b solutions held start t time step yt arg max yb tvt t s t yi t yj t figure summary generated baseline model pgnet overall extractive parts cut vocabulary e seen ing phase appear input text concretely computing word composing summary bility pt generated based context vector decoder state decoder input probability soft switch choose generating word known vocabulary extracting word original text finally common practice tasks machine translation text summarization pgnet uses beam search algorithm generate summaries generating word composing summary instead greedily taking word highest score algorithm selects b best scoring candidates exploring tiple sequences parallel happens practice output generated heuristic tend stem gle highly valued beam resulting minor perturbations single sequence pgnet practice practice pgnet acts extractive model et al point work time model copies article tences meaning model behave fully extractive model encompasses range abstractive techniques truncating sentences correct shorter versions model feel tractive et al seen figure highlighted portion input text reused summary extractiveness observations motivated current work tackle tractiveness rst step measure methods ready exist simple plagiarism detection based n gram frequencies analysis longest common sequence zhang et al anzelmi et al lcs easily dene plagiarism score normalizing length lcs given summary document length summary metric major aw completely discards rest summary difference summary constructed b beam width t log probability partial solution v vocabulary vt yt v set possible token extensions beams yt bs pick b elements mmr diverse beam search hand decodes diverse lists dividing equally beam size groups ing beam budget e number nodes expanded time step given group enforcing diversity groups beams y g b t arg max yg t t y v g t t b s t t y t yi t yj t diversity term ing dissimilarity group g prior groups token y chosen extend beams group g diversity strength applying variable diversity strength diversity measure observed forcing model generate tiple diverse summaries logically pushed model provise use extractive capabilities different diversity terms present amples following section merging diverse summaries summaries based dbs intrinsically better summaries generated classic beam search contain novelty inferred generating multiple summaries picking merging best sentences summaries lead extractive model better capturing relevant aspects split diverse summaries sentences candidates form good diverse summary pick best sentences rst rank use framework gave good results keyphrase tion sentence embeddings bennani smires et al method based pagliardini et al allows embedding arbitrary length sequences words similarly word embeddings sent semantic relatedness phrases standard similarity measures like cosine euclidean generate document embedding simply concatenating sentences document subsequently treating document single phrase document embedding rank usefulness candidate sentences intuition useful candidate phrases time close document far away use maximal marginal relevance mmr pick candidates mmr information retrieval guo sanner carbonell goldstein balances relevance case similarity original document summarize diversity compute cosine similarity candidates embedding document embedding obtain score measures relevant information candidate phrase contains precisely pick n candidates iteratively m m r arg max max cj k ci c set candidates e sentences k set picked candidates ci d beddings candidate document respectively cossimb similarity measure ized cosine similarity described trade parameter diversity classic ranking choice n diversity factor detailed following section dene ci ci max ckc ck ci ci c c d document embedding c c represent average similarity standard deviation d set candidates c apply kind transformation ity candidate phrases shown following equations ci ci max ck ci ci c ci c ci selecting best sentences construct nal summary contains best elements generated diverse summaries experiments evaluate baseline pgnet diverse generative model pyrouge nallapati et al et al experiments use trained pgnet described original paper et al coverage hidden states encoder decoder lstms dimensional words embeddings words embedding pretrained learned ing training train adagrad duchi et al learning rate initial accumulator value gradient clipping maximum gradient norm regularization training truncate article tokens generating new summaries limit length summary rst set similarly original model python pyrouge figure summaries generated pgnet beam search pgnet baseline model behave fully extractive post processing module output diverse abstractive summary pointer generator network beam search decoder baseline pointer generator network decoder n pointer generator network decoder n reference summaries plagiarism score extraction score table plagiarism score extraction score test set n hyper parameters stay generator network display score reference summaries comparison hyper parameter description selection best set hyper parameters testing exhaustively hyper parameter sive performed experiments small portions dataset randomly picked examples ing set multiple sets hyper parameters rouge l ranked sets hyper parameters picked best set set tested nal test dataset conrm bias hidden small dataset hyper parameter description diverse beam search enforce maximum minimum number tokens summary periments important factors second best set parameters ations important note baseline pgnet uses maximum number tokens summaries ing testing beam width b parameter dene ber nodes expanded time step search high value b means search space larger computationally expensive set b signicantly higher compared pgnet set high beam size means resources explore groups leads turn better candidates trade b number groups g group requires resources g set duces dbs bs hand setting beam size b allows maximum exploration search space opposition having high budget group set group number g beam width group allow model perform evenly baseline rst group generates different summaries diversity strength scalar penalizes summaries look alike precisely species trade joint probability diversity terms high value produces diverse mary excessively high values diversity strength result grammatically incorrect outputs overpower model probability finally beam search dened function outputs vector similarity scores potential beam pletions options exist diversity function hamming diversity current group penalized producing words time cisely compute hamming distance strings normalized length string group n grams diversity current group penalized producing n grams previous group alignment time use hamming diversity diversity strength meaning penalizes selection tokens previous groups proportional number times selected ensures different words ferent times forces model rephrases sentences pointer generator network beam search decoder baseline model pointer generator network decoder n pointer generator network decoder n rouge l table rouge scores test set rouge scores condence interval reported ofcial rouge script n hyper parameters stay add new words dene hyper parameters embedrank number candidates select n versity factor mmr precisely dene n number iteration run mmr algorithm number sentences nal summary factor trade standard relevance ranked list maximal diversity ranking candidates finally parameters embedrank set n rouge l optimization meaning summary sentences long dbs improvements extractiveness rst evaluate impact diverse generative marization prism extractiveness sulting models figure portray original pgnet diverse summaries immediately ous human extractive contains shorter excerpts original text quantify ing compute previously discussed plagiarism score extraction score method results shown table portray parameter combinations diverse summary generator n n firstly models perform considerably better regards metrics plagiarism score means average summaries half tracted text having value instead value extraction score means model core extractive keeps common sequence original text secondly notice reduce extractiveness respectively getting plagiarism score extraction score model having slightly longer summaries counterintuitive result fact ily explainable picking lower number sentences impact diversity fact reduced higher selected candidate counts relevant information spread multiple sentences rouge scores compare rouge results diverse summaries different parameter combinations table summaries evaluated standard rouge metric precisely scores rouge l measures respectively word overlap bigram overlap longest common sequence reference summary generated summary summaries containing n n sentences perform signicantly baseline score gains addition observe exceeding points slight tradeoff extractiveness measure lower extractiveness obtained marginal cost term rouge score notable result rouge l score improvements obtained ously extractiveness decrease making diverse symmary generation win win proposition conclusions future work paper presented ways measuring reducing extractiveness abstractive generative summaries increasing overall quality showed best generative summary architectures pgnet alleviates ills summaries including repetition vocabulary words suffers highly extractive nature outlined previous methods found plagiarism detection suited evaluating generative summaries proposed alternative extractiveness measure showed ness measure correlated human judgment ground truth close minimum extractiveness ues presented alternative decoding mechanism applied abstractive framework uses beam search method leverages multiple recent velopments complementing diverse beam search diverse summary combination mechanism based similarity measures computed sentence embeddings showed advantages diverse summary eration method reduce extractiveness architecture measured separate ways improves overall rouge score signicant finally believe method opens door eral directions research reducing extractiveness abstractive framework alongside improving overall formance hand selecting candidates crucial step optimally improve rouge score hand method mented abstractive summarization frameworks order reduce extractiveness improving overall rouge score references anzelmi et al daniele anzelmi domenico carlone fabio rizzello robert thomsen d m akbar sain plagiarism detection based scam algorithm proceedings international multiconference neers computer scientists asghar et al nabiha asghar pascal poupart xin jiang hang li deep active learning dialogue generation pages bahdanau et al dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate pages bennani smires et al kamil bennani smires claudiu musat martin jaggi andreea hossmann michael baeriswyl embedrank unsupervised keyphrase extraction sentence embeddings carbonell goldstein jaime carbonell jade goldstein use mmr diversity based reranking reordering documents producing summaries ceedings annual international acm sigir ference research development information trieval sigir pages chopra et al sumit chopra michael auli alexander m rush abstractive sentence summarization attentive recurrent neural networks pages chuang yang wesley t chuang jihoon yang extracting sentence segments text tion proceedings annual international acm sigir conference research development mation retrieval sigir pages duchi et al john duchi elad hazan yoram singer adaptive subgradient methods online journal machine ing stochastic optimization learning research gimpel et al kevin gimpel dhruv batra chris dyer gregory shakhnarovich systematic ploration diversity machine translation emnlp guo sanner shengbo guo scott sanner probabilistic latent maximal marginal relevance ceedings international acm sigir conference research development information retrieval pages hasselqvist et al johan helmertz mikael kageback stractive summarization neural networks hasselqvist niklas query based hermann et al karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend pages jin et al feng jin minlie huang xiaoyan zhu comparative study ranking selection strategies multi document summarization coling et al jiwei li monroe dan jurafsky simple fast diverse decoding algorithm neural eration mikolov et al tomas mikolov kai chen greg rado jeffrey dean efcient estimation word resentations vector space pages nallapati et al ramesh nallapati bowen zhou cero nogueira dos santos caglar gulcehre bing ang abstractive text summarization sequence sequence rnns pagliardini et al matteo pagliardini prakhar gupta martin jaggi unsupervised learning sentence beddings compositional n gram features et al romain paulus caiming xiong richard socher deep reinforced model tive summarization rajeswar et al sai rajeswar sandeep subramanian francis dutil christopher pal aaron courville versarial generation natural language sankaran et al baskaran sankaran haitao mi yaser al onaizan abe ittycheriah temporal tention model neural machine translation schmidhuber jurgen schmidhuber pointer works neural networks et al abigail peter j liu pher d manning point summarization pointer generator networks tu et al zhaopeng tu zhengdong lu yang liu xiaohua liu hang li modeling coverage neural machine translation vijayakumar et al ashwin k vijayakumar michael cogswell ramprasath r selvaraju qing sun stefan lee david crandall dhruv batra diverse beam search decoding diverse solutions neural sequence els pages wong et al kam fai wong mingli wu jie li extractive summarization supervised semi supervised learning international conference computational volume association computational linguistics proceedings zhang et al fangfang zhang yoon chan jhi hao wu peng liu sencun zhu rst step algorithm plagiarism detection proceedings international symposium software testing sis issta page zhang et al yizhe zhang zhe gan kai fan zhi chen ricardo henao dinghan shen lawrence carin adversarial feature matching text generation
