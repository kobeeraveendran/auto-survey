diverse beam search increased novelty abstractive summarization cibils andre musat claudiu hossmann andreea baeriswyl michael ecole polytechnique federale lausanne epfl email rstname articial intelligence group swisscom email rstname com abstract text summarization condenses text shorter version retaining important informations abstractive summarization recent development generates new phrases simply copying rephrasing sentences original text recently neural sequence sequence models achieved good results eld abstractive summarization opens new possibilities applications industrial purposes practitioners observe models use large parts original text output summaries making similar extractive frameworks address drawback rst introduce new metric measure summary tracted input text secondly present novel method relies diversity factor computing neural network loss improve diversity summaries generated neural abstractive model implementing beam search nally method makes system extractive improves overall rouge score state art methods points introduction summarization process generating condensed version text contains key information original automatic summarization approaches extractive abstractive called generative extractive summarization focused nding relevant text spans phrases original document copying construct summary nal text consists exclusively passages input process viewed ranking mechanism jin second summary generation paradigm generate novel text produce summary generative cess uses words coming vocabulary seen source document human usually stractive summarization considered difcult requires high level competences generalization reformulation compared extractive summarization ensures baseline levels grammaticality accuracy advent powerful deep architectures generative adversarial networks text generation jeswar zhang pointer works schmidhuber generative summarization largely considered infeasible novel methods opening way new generation summarization systems carry promise near human text generation quality practitioners quickly discarded earlier generative models inability solve problems result look robotic incomprehensible chiey repetition recent model pointer generator network pgnet achieves state art term tomatic summarization widely dataset dailymail hermann nallapati model abstractive sequence sequence neural model uses extractive tive summarization techniques overcome problems plagued previous sequence sequence neural models like repetition inability handle vocabulary oov words pgnet features improvements include attention hybrid mechanism enables model copy words original text pointer work schmidhuber inclusion pointer generation step clear tive impact performance system progress evident quantitatively rouge scores itatively caveat extractive pointer generator component model option copying large swaths original text ing option method self defeating leads overly extractive behavior abstractive method argue drawback inherently common marizers use pointers elements original text work propose way measure tiveness abstractive summarization methods common methods plagiarism detection identify cases summarizer simply pieces large text spans original text vise novel method penalize behaviour use complement traditional evaluations like rouge rouge scores goal orthogonal evaluation discourage abstractive models going low hanging fruit copying second contribution method reduce tiveness abstractive summaries based diverse beam search dbs vijayakumar previous attempt improve abstractive summaries dbs lacklustre results verse summary superior obtained baseline model propose method combining novelty dbs based summaries dbs based results qualitatively tively better previous state art major sult decrease extractive nature summarizer time increasing rouge scores cnn daily mail dataset hermann method compatible abstractive summarizer uses beam search includes leading methods time writing lapati paulus chopra hasselqvist related work recently text summarization vast majority extractive wong chuang yang cusp major paradigm shift abstractive summarization largely good results tained recent models rst work succeed abstractive rization sequence sequence model ati introduced major dataset cnn daily mail use current work addresses multiple issues capturing hierarchy word structure emitting words rare training time independently chopra created tive recurrent model yielded good results shared task recently paulus created reinforcement model abstractive summarization came new state art cnn daily mail dataset surge interest abstractive summarization research community lead increasingly promising sults practice remaining issues clude use industrial settings inaccurately ing factual details inability deal vocabulary oov words repetition pgnet designed architecture tackle issues simultaneously pointing retrieving elements original text successfully alleviates impact tioned ills pgnet tends abuse copying mechanism generated summaries largely extractive address novel problem rst need quantify dene extractiveness abstractive rizer extracted summary copied original text extractiveness akin rism detection measures text extracted plagiarism detection studied problem frequently measures grams frequencies longest common sequence lcs zhang anzelmi techniques adequate measures extractiveness summary pect summary share lot words ment solution present reduce extractiveness based diverse beam search dbs vijayakumar dbs shown effective creating diverse image captions machine translation visual question eration variation classic beam search designed neural sequence models addresses lack versity original algorithm gimpel dbs multiple topics logue response generation asghar machine abstractive tion dbs contributes marginally score performance stractive summarization combine candidate selection algorithm multiple elds imal marginal relevance mmr guo sanner carbonell goldstein mmr algorithm balances relevance diversity multiple set based mation retrieval tasks order use mmr needs compute ilarity sentences options exist recently good results obtained sentence embeddings instance ones based gram features pagliardini produces cic word gram embedding vectors tively combined sentence embedding similarly word embeddings mikolov allow represent semantic relatedness phrases summary generation rst section devoted briey describing baseline model pgnet second half describe main contributions hance pgnet order generate extractive summaries baseline model core idea current methods text tion consists leveraging corpus containing source documents summaries pgnet particular learns map input sequence words source ument sequence words summary document implementing known sequence sequence neural network architecture extended tention mechanism bahdanau additionally pgnet addresses shortcomings previous els inability handle vocabulary words repetition sankaran main novelty pgnet pointer work schmidhuber aggregates context vector produced attention mechanism decoder state making model able copy words original ument combine fragments output summary generated abstractive fashion ble model use words contained quences original document summary half composed new words outside lcs article address issue dene metric takes parameters lcs account generally nalizes large spans text copied original text merging sentences splitting penalized heavily copying wholly want metric consistent deterministic output malized score finally novel word combinations penalized dene extraction score extraction acss acss set long non overlapping mon sequences summary document acss set proportion common sequences element set length mon sequence divided length summmary rst start nding long common sequences tween summary original article use similar algorithm lcs return long non lapping common sequences extraction score ensures sum scores having distinct common sequences text proportion ized having common sequence text proportion summary consists long common quence document score summary new words encountered article score reference summaries average plagiarism score extraction score test set respectively means reference summaries mainly paraphrasing copying text article expected outlines value ness measure goal automated systems close zero case human standard extraction score allows compare different system rank respect generative capacity improved decoding mechanism solution extractivness problem complementary baseline architecture tive model employs beam search diverse beam search beam search iterative algorithm widely decode rnns nallapati paulus chopra hasselqvist approximates optimal solutions time step model computes set solutions held start time step arg max tvt figure summary generated baseline model pgnet overall extractive parts cut vocabulary seen ing phase appear input text concretely computing word composing summary bility generated based context vector decoder state decoder input probability soft switch choose generating word known vocabulary extracting word original text finally common practice tasks machine translation text summarization pgnet uses beam search algorithm generate summaries generating word composing summary instead greedily taking word highest score algorithm selects best scoring candidates exploring tiple sequences parallel happens practice output generated heuristic tend stem gle highly valued beam resulting minor perturbations single sequence pgnet practice practice pgnet acts extractive model point work time model copies article tences meaning model behave fully extractive model encompasses range abstractive techniques truncating sentences correct shorter versions model feel tractive seen figure highlighted portion input text reused summary extractiveness observations motivated current work tackle tractiveness rst step measure methods ready exist simple plagiarism detection based gram frequencies analysis longest common sequence zhang anzelmi lcs easily dene plagiarism score normalizing length lcs given summary document length summary metric major completely discards rest summary difference summary constructed beam width log probability partial solution vocabulary set possible token extensions beams pick elements mmr diverse beam search hand decodes diverse lists dividing equally beam size groups ing beam budget number nodes expanded time step given group enforcing diversity groups beams arg max diversity term ing dissimilarity group prior groups token chosen extend beams group diversity strength applying variable diversity strength diversity measure observed forcing model generate tiple diverse summaries logically pushed model provise use extractive capabilities different diversity terms present amples following section merging diverse summaries summaries based dbs intrinsically better summaries generated classic beam search contain novelty inferred generating multiple summaries picking merging best sentences summaries lead extractive model better capturing relevant aspects split diverse summaries sentences candidates form good diverse summary pick best sentences rst rank use framework gave good results keyphrase tion sentence embeddings bennani smires method based pagliardini allows embedding arbitrary length sequences words similarly word embeddings sent semantic relatedness phrases standard similarity measures like cosine euclidean generate document embedding simply concatenating sentences document subsequently treating document single phrase document embedding rank usefulness candidate sentences intuition useful candidate phrases time close document far away use maximal marginal relevance mmr pick candidates mmr information retrieval guo sanner carbonell goldstein balances relevance case similarity original document summarize diversity compute cosine similarity candidates embedding document embedding obtain score measures relevant information candidate phrase contains precisely pick candidates iteratively arg max max set candidates sentences set picked candidates beddings candidate document respectively cossimb similarity measure ized cosine similarity described trade parameter diversity classic ranking choice diversity factor detailed following section dene max ckc document embedding represent average similarity standard deviation set candidates apply kind transformation ity candidate phrases shown following equations max selecting best sentences construct nal summary contains best elements generated diverse summaries experiments evaluate baseline pgnet diverse generative model pyrouge nallapati experiments use trained pgnet described original paper coverage hidden states encoder decoder lstms dimensional words embeddings words embedding pretrained learned ing training train adagrad duchi learning rate initial accumulator value gradient clipping maximum gradient norm regularization training truncate article tokens generating new summaries limit length summary rst set similarly original model python pyrouge figure summaries generated pgnet beam search pgnet baseline model behave fully extractive post processing module output diverse abstractive summary pointer generator network beam search decoder baseline pointer generator network decoder pointer generator network decoder reference summaries plagiarism score extraction score table plagiarism score extraction score test set hyper parameters stay generator network display score reference summaries comparison hyper parameter description selection best set hyper parameters testing exhaustively hyper parameter sive performed experiments small portions dataset randomly picked examples ing set multiple sets hyper parameters rouge ranked sets hyper parameters picked best set set tested nal test dataset conrm bias hidden small dataset hyper parameter description diverse beam search enforce maximum minimum number tokens summary periments important factors second best set parameters ations important note baseline pgnet uses maximum number tokens summaries ing testing beam width parameter dene ber nodes expanded time step search high value means search space larger computationally expensive set signicantly higher compared pgnet set high beam size means resources explore groups leads turn better candidates trade number groups group requires resources set duces dbs hand setting beam size allows maximum exploration search space opposition having high budget group set group number beam width group allow model perform evenly baseline rst group generates different summaries diversity strength scalar penalizes summaries look alike precisely species trade joint probability diversity terms high value produces diverse mary excessively high values diversity strength result grammatically incorrect outputs overpower model probability finally beam search dened function outputs vector similarity scores potential beam pletions options exist diversity function hamming diversity current group penalized producing words time cisely compute hamming distance strings normalized length string group grams diversity current group penalized producing grams previous group alignment time use hamming diversity diversity strength meaning penalizes selection tokens previous groups proportional number times selected ensures different words ferent times forces model rephrases sentences pointer generator network beam search decoder baseline model pointer generator network decoder pointer generator network decoder rouge table rouge scores test set rouge scores condence interval reported ofcial rouge script hyper parameters stay add new words dene hyper parameters embedrank number candidates select versity factor mmr precisely dene number iteration run mmr algorithm number sentences nal summary factor trade standard relevance ranked list maximal diversity ranking candidates finally parameters embedrank set rouge optimization meaning summary sentences long dbs improvements extractiveness rst evaluate impact diverse generative marization prism extractiveness sulting models figure portray original pgnet diverse summaries immediately ous human extractive contains shorter excerpts original text quantify ing compute previously discussed plagiarism score extraction score method results shown table portray parameter combinations diverse summary generator firstly models perform considerably better regards metrics plagiarism score means average summaries half tracted text having value instead value extraction score means model core extractive keeps common sequence original text secondly notice reduce extractiveness respectively getting plagiarism score extraction score model having slightly longer summaries counterintuitive result fact ily explainable picking lower number sentences impact diversity fact reduced higher selected candidate counts relevant information spread multiple sentences rouge scores compare rouge results diverse summaries different parameter combinations table summaries evaluated standard rouge metric precisely scores rouge measures respectively word overlap bigram overlap longest common sequence reference summary generated summary summaries containing sentences perform signicantly baseline score gains addition observe exceeding points slight tradeoff extractiveness measure lower extractiveness obtained marginal cost term rouge score notable result rouge score improvements obtained ously extractiveness decrease making diverse symmary generation win win proposition conclusions future work paper presented ways measuring reducing extractiveness abstractive generative summaries increasing overall quality showed best generative summary architectures pgnet alleviates ills summaries including repetition vocabulary words suffers highly extractive nature outlined previous methods found plagiarism detection suited evaluating generative summaries proposed alternative extractiveness measure showed ness measure correlated human judgment ground truth close minimum extractiveness ues presented alternative decoding mechanism applied abstractive framework uses beam search method leverages multiple recent velopments complementing diverse beam search diverse summary combination mechanism based similarity measures computed sentence embeddings showed advantages diverse summary eration method reduce extractiveness architecture measured separate ways improves overall rouge score signicant finally believe method opens door eral directions research reducing extractiveness abstractive framework alongside improving overall formance hand selecting candidates crucial step optimally improve rouge score hand method mented abstractive summarization frameworks order reduce extractiveness improving overall rouge score references anzelmi daniele anzelmi domenico carlone fabio rizzello robert thomsen akbar sain plagiarism detection based scam algorithm proceedings international multiconference neers computer scientists asghar nabiha asghar pascal poupart xin jiang hang deep active learning dialogue generation pages bahdanau dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate pages bennani smires kamil bennani smires claudiu musat martin jaggi andreea hossmann michael baeriswyl embedrank unsupervised keyphrase extraction sentence embeddings carbonell goldstein jaime carbonell jade goldstein use mmr diversity based reranking reordering documents producing summaries ceedings annual international acm sigir ference research development information trieval sigir pages chopra sumit chopra michael auli alexander rush abstractive sentence summarization attentive recurrent neural networks pages chuang yang wesley chuang jihoon yang extracting sentence segments text tion proceedings annual international acm sigir conference research development mation retrieval sigir pages duchi john duchi elad hazan yoram singer adaptive subgradient methods online journal machine ing stochastic optimization learning research gimpel kevin gimpel dhruv batra chris dyer gregory shakhnarovich systematic ploration diversity machine translation emnlp guo sanner shengbo guo scott sanner probabilistic latent maximal marginal relevance ceedings international acm sigir conference research development information retrieval pages hasselqvist johan helmertz mikael kageback stractive summarization neural networks hasselqvist niklas query based hermann karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend pages jin feng jin minlie huang xiaoyan zhu comparative study ranking selection strategies multi document summarization coling jiwei monroe dan jurafsky simple fast diverse decoding algorithm neural eration mikolov tomas mikolov kai chen greg rado jeffrey dean efcient estimation word resentations vector space pages nallapati ramesh nallapati bowen zhou cero nogueira dos santos caglar gulcehre bing ang abstractive text summarization sequence sequence rnns pagliardini matteo pagliardini prakhar gupta martin jaggi unsupervised learning sentence beddings compositional gram features romain paulus caiming xiong richard socher deep reinforced model tive summarization rajeswar sai rajeswar sandeep subramanian francis dutil christopher pal aaron courville versarial generation natural language sankaran baskaran sankaran haitao yaser onaizan abe ittycheriah temporal tention model neural machine translation schmidhuber jurgen schmidhuber pointer works neural networks abigail peter liu pher manning point summarization pointer generator networks zhaopeng zhengdong yang liu xiaohua liu hang modeling coverage neural machine translation vijayakumar ashwin vijayakumar michael cogswell ramprasath selvaraju qing sun stefan lee david crandall dhruv batra diverse beam search decoding diverse solutions neural sequence els pages wong kam fai wong mingli jie extractive summarization supervised semi supervised learning international conference computational volume association computational linguistics proceedings zhang fangfang zhang yoon chan jhi hao peng liu sencun zhu rst step algorithm plagiarism detection proceedings international symposium software testing sis issta page zhang yizhe zhang zhe gan kai fan zhi chen ricardo henao dinghan shen lawrence carin adversarial feature matching text generation
