fat albert finding answers large texts semantic similarity attention layer based bert omar amgad anandharaju hari zayed simon fraser university burnaby canada abstract machine based text comprehension signicant research eld natural language processing understanding text context semantics achieved deep learning model trained solve large subset tasks text summarization classication question answering paper focus question answering problem specically multiple choice type questions develop model based bert state art transformer network alleviate ability bert support large text corpus extracting highest inuence sentences semantic larity model evaluations proposed demonstrate outperforms leading models movieqa challenge currently ranked leader board test accuracy finally discuss model shortcomings suggest possible improvements overcome limitations introduction main challenges natural language processing nlp ability machine read understand unstructured text reason answering related questions question answering models applied wide range applications nancial reports customer service health care signicant number datasets race swag created provide ground truth training evaluating specic type models involve multiple choice questions mcqs movieqa dataset challenge attracted large number promising solutions blohm implement attention models based short term memory lstm convolutional neural networks cnns combine model accuracies ensemble aggregation models prone systematic adversarial attacks like linguistic level attacks word sentence level knowledge adversaries black box white box models learn match patterns select right answer performing plausible inferences humans recently implemented bidirectional encoder representations transformers bert pre trained model tackle large subset nlp tasks bert key technical source code publicly available com omossad fat albert leader board toronto edu plot preprint review table movieqa dataset description movies questions avg words avg words avg words train val test total innovation applying bidirectional training transformer popular attention model language modelling paper results language model bidirectionally trained deeper sense language context single direction language models plethora deep learning models incorporated bert tasks improving state art performances notable limitation bert able support large texts include pre trained model maximum number words tokens dealing large texts performance bert severely affected work aim overcome limitations bert analyzing improving accurate predict answer extracted large text movieqa mcq dataset approach relies concept sentence attention extract signicant sentences large corpus able accomplish task pre trained semantic similarity model remainder paper organized follows section describes datasets focusing semantics nature questions highlight proposed model intuition components section evaluate performance model section finally conclude report suggest future modications model section movieqa dataset section overview dataset evaluate model movieqa dataset created generating number mcqs solved specic context extracted plots real movies existing going highest accuracy solving mcqs movie topics models trained use video scenes subtitles scripts movie plots extracted wikipedia leader board challenge divided according source input selected model dataset consists mcqs obtained movies features high semantic diversity question comes set highly plausible answers correct dataset structure semantics movie plots described table average movie plot sentences words sentence average training validation sets labelled correct answer test dataset labelled evaluated challenge submission server large nature plot texts selected dataset demonstrate incorporate bert relatively large texts fat albert model description existing bert mcq codes currently lack support large text documents restricted sequence length tokens furthermore limited compute capabilities restricted tokens according tpus memory able train models number tokens compared gpus model bert mcq select sentences text highly similar question context subsequently instead processing entire corpus bert mcq uses sentences maximum span certain question text needed answer question sentences plot alignments consecutive questions span specic passage text entire text challenge toronto figure fat albert model overview figure semantic similarity network model model consists following semantic similarity classier pre trained sts clinical datasets bert mcq trained movieqa dataset description entire model depicted fig large movie plot text concatenated question answer sequences fed similarity model produce similar sentences question feed questions answers attention plot text similar sentences bert mcq model output model array probabilities having size number possible choices finally select choice having highest probability semantic similarity network use pre trained models based sts clinical datasets similarity measure sentences number variation semantic similarity exists dataset aforementioned ones selected model proven effective application detailed comparison performance models movieqa dataset provided evaluation section model uses bert extract embeddings sentences embeddings undergo number similarity functions cosine similarity qgram distance levenshtein similarity outputs sent fully connected network followed softmax layer provide similarity index selected use combination sts clinical bert models normalizing model probabilities describes cosine similarity index sentences calculated fig highlights contextual structure bert similarity model bert mcq bert mcq model uses pre trained bert transformer network modied tuned movieqa dataset embedding outputs bert passed fully connected layer produce predicted probabilities possible choice pre trained model uses layer hidden heads parameters tuning performed movieqa dataset modifying bert outputs support variable number choices running bert mcq perfectly aligned plot sentences model able achieve validation accuracy model initially developed support sentence completion type questions modied model handle mcqs changing network structure output probabilities choice instead complete text tokens evaluation results evaluate performance model movieqa dataset section indicate results obtained implementation mentioned reference paper differences appear evaluation published results probably changes parameters authors mirrored source codes provide brief case study highlight cases movie plot model succeeds fails respectively evaluation movieqa dataset properly trained bert mcq reach accuracy range movieqa dataset aligned exact sentences having answer performed sentence selection accuracy drops random choice fact bert truncates sentences word count higher maximum sequence length words case semantic similarity model captured similar sentences question hand selected questions average word count sentences entire dataset range acceptable truncate words similar sentences similarity model performance depicted table combined model aggregates output probabilities web clinical models normalization possible improvement model accuracy training movieqa dataset compare accuracy model current movieqa challenge leader board models included validation accuracy test evaluations received movieqa authors submission contribution created ensemble model aggregates results approaches challenge performs majority ruling select label highest probability model uses cnn lstm models previously described bert model main incentive ensemble allow different models correct collaboratively avoid making mistakes table demonstrates accuracy models compared leader board models highlight effect number tokens bert showcase fig training loss models different number tokens main observation order support larger number tokens reduce batch size training results indicate larger tokens lower losses general clear reducing batch size notably affected model accuracy instance fig number tokens equivalent table semantic similarity evaluations train acc val acc web bert clinical bert combined table movieqa dataset evaluations val acc test acc fat albert ensemble fat albert lstm cnn word level cnn included paper leader board obtained evaluations model accuracy increases signicantly compared maximum number tokens model truncates input having tokens compute capability able run model tokens gpus able allocate memory evaluated loss larger number tokens smaller batch size fig loss gradually decreasing number tokens higher stabilizes inputs generally smaller maximum number tokens input sentences padded zeros reach required size tokens case study demonstrate cases extracted movie forrest gump movie plot sentences brief extract shown fig sample questions displayed fig model selects similar sentences question case answer fully interpreted sentences highlighted bold passing sentences instead entire plot bert mcq successfully selects correct choice hand similarity model able select best sentences second example shown fig despite nding accurate sentences specic question model missed informative model subsequently failed select correct answer conclusions future work paper created attention model based semantic similarity overcome bert limitations order solve mcq begin extracting relevant sentences large text reducing complexity problem answering mcq question time writing report latest submission ranked rst movieqa challenge future work plan extend model process input signals provided movieqa dataset like subtitles build powerful models incorporating human like processing mechanisms referential relations entailment answer elimination finally migrating code work tpus higher computational power instead gpus allow handle larger texts avoiding sentence truncation batch size batch size figure bert mcq training loss movieqa dataset figure forrest gump plot figure correct model prediction example figure wrong model prediction example references blohm jagfeld sood comparing attention based convolutional recurrent neural networks success limitations machine reading hension proceedings conference computational natural language learning association computational linguistics devlin chang lee toutanova bert pre training deep bidirectional transformers language understanding corr lai xie liu yang hovy race large scale reading comprehension dataset examinations arxiv preprint tapaswi zhu stiefelhagen torralba urtasun fidler movieqa understanding stories movies question answering ieee conference computer vision pattern recognition cvpr jun ieee zellers bisk schwartz choi swag large scale adversarial proceedings conference dataset grounded commonsense inference empirical methods natural language processing association computational linguistics
