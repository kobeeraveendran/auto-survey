hybrid memnet extractive summarization abhishek kumar singh iiit hyderabad abhishek iiit manish gupta iiit hyderabad manish vasudeva varma iiit hyderabad abstract extractive text summarization extensive research lem field natural language understanding conventional approaches rely manually compiled tures generate summary attempts developing data driven systems extractive summarization end present fully data driven end end deep network hybrid memnet single document tion task network learns continuous unified representation document generating summary jointly captures local global sentential information notion summary worthy sentences experimental results different corpora confirm model shows significant performance gains compared state art baselines ccs concepts information systems summarization information retrieval keywords summarization deep learning natural language acm reference format abhishek kumar singh manish gupta vasudeva varma brid memnet extractive summarization proceedings singapore singapore november pages doi introduction tremendous growth data web increased need retrieve analyze understand large information time consuming motivation concise representation large text retaining core meaning original text led development summarization systems summarization methods broadly classified categories extractive abstractive tive methods aim select salient phrases sentences elements text abstractive techniques focus generating summaries scratch constraint reusing phrases original text author principal applied researcher microsoft permission digital hard copies work personal classroom use granted fee provided copies distributed profit commercial advantage copies bear notice citation page copyrights components work owned acm honored abstracting credit permitted copy republish post servers redistribute lists requires prior specific permission fee request permissions org singapore singapore acm doi successful summarization systems use extractive methods sentence extraction crucial step systems idea find representative subset sentences contains information entire set traditional approaches extractive summarization identify sentences based human crafted features sentence position length words title presence proper nouns content features like term frequency event features like action nouns generally sentences assigned saliency score indicating strength presence features kupiec use binary classifiers select summary worthy sentences conroy oleary gated use hidden markov models introduced graph based algorithms selecting salient sentences recently interest shifted neural network based proaches modeling extractive summarization task kageback employed recursive autoencoder summarize documents yin pei exploit convolutional neural works project sentences continuous vector space select sentences based prestige diversity cost document extractive summarization task recently cheng lapata introduced attention based neural encoder decoder model extractive single document summarization task trained large corpus news articles collected daily mail lar cheng lapta work focused sentential extractive summaries single document deep neural works propose use memory networks convolutional bidirectional long short term memory networks capturing better document representation work propose data driven end end enhanced encoder decoder based deep network summarizes news cle extracting salient sentences figure shows architecture proposed hybrid memnet model model consists ment reader encoder sentence extractor decoder contrary cheng lapata model tion based decoder model uses attention encoder decoder focus learn better document representation incorporates local global document features attention sentences capture notion saliency sentence contrary orthodox method computing tential features model uses neural networks purely data driven approach zhang kim shown successful use convolution neural networks cnn obtaining latent feature representation network applies cnn multiple filters automatically capture latent semantic features long short term memory lstm work applied obtain comprehensive set features known thought vector vector captures overall abstract tation document obtain final document representation concatenating document embeddings obtained lutional lstm conv lstm document embeddings november singapore singapore figure architecture hybrid memnet model memory network final unified document embedding embeddings sentences decoder select salient sentences document experiment conv lstm encoder convolutional bidirectional lstm conv blstm encoder summarize primary contributions propose novel architecture learn better unified document representation combining features memory network features convolutional lstm blstm network investigate application memory network porates attention sentences conv blstm porates gram features sentence level information learning better thought vector rich semantics experimentally proposed method forms basic systems competitive baselines model achieves significant performance gain duc generic single document summarization dataset begin describing network architecture section lowed experimental details including corpus details section analyze system benchmarks section finally conclude work section hybrid memnet model primary building blocks model document encoder captures local grams level mation global sentence level information notion summary worthy sentences decoder attention based sequence sequence decoder document encoder idea learn unified document representation incorporates gram features sentence level information includes notion salience redundancy sentences purpose sum document representations vectors learned convolutional lstm conv lstm hierarchical coding memnet capturing salience redundancy unified document embedding learned joint interaction mentioned models refer network hybrid memnet sentence encoder convolution neural networks encode sentences shown successfully work multiple sentence level classification tasks conventional convolution neural network uses convolution operation word embeddings followed max pooling operation suppose dimensional word embedding ith word sentence concatenation word embeddings volution operation window words filter rmcd yields new features dimensions filter index convolution operation written bias term obtain feature map applying filter possible window words sentence length intention capture prominent features feature map use max time pooling operation acquire set features filter fixed window size single feature vector represented max use multiple convolution nets different filter sizes compute list embeddings summed obtain final sentence vector conv blstm document encoder recurrent neural network rnn suffers vanishing gradient problem long sequences use long term memory lstm network obtain hierarchical document encoding sentence vectors obtained convolutional sentence encoder fed lstm new representation intuitively captures local global sentential information plore lstm network bidirectional lstm network experiments experiments combination convolution network bidirectional lstm blstm performs better case blstm exploits future context sequence processing data directions final unified document encoding sentences vectors convolutional sentence encoder fed decoder model section discuss details encoder decoder modules memnet based document encoder leverage memory network encoder inspired rent attention model solve question answering language convolution max pooling sentence encoder memory encoder decoder document encoder sum sentence embedding sentences words sentence hybrid memnet extractive summarization november singapore singapore modeling task model uses attention mechanism shown capture temporal context case learns document representation captures notion salience redundancy sentences describe model implements single memory hop operation single layer extend multiple hops memory consider input set sentence vectors obtained sentence encoder document let document representation obtained conv lstm model document embedding memnet model entire set transformed memory vectors dimension continuous space learned weight matrix size embedding size sentence similarly input document embedding transformed learned weight matrix dimension obtain internal state compute match memory taking inner product followed softmax follows ezj probability vector inputs corresponding output vector embedding matrix output vector memory computed sum transformed inputs weighted probability vector input follows pici case multiple layer model handle case hop operation memory layers stacked input layer computed follows let output obtained memory unit final unified document representation obtained summing output conv blstm output memnet intuitively notion worthiness sentence captures hierarchical information document decoder decoder uses lstm label sentences sequentially keeping mind individual relevance mutual redundancy taking account encoded document previously labeled sentences labeling sentence encoder hidden states denoted decoder hidden states denoted time step sentence decoder equations follows lst mlp degree decoder assumes previous sentence summary memorized system certain sentence label concatenation given input mlp multi layer perceptron experimental results section paper present experimental setup assessing performance proposed system present details corpora training evaluation implementation details approach datasets purpose training model use daily mail pus task single document marization cheng lapata overall corpus tains training documents validation documents test documents evaluate model use standard single document summarization dataset consists documents evaluate system articles dailymail test set human written highlights gold standard average byte count document article highlight pairs sampled highlights include minimum sentences implementation details use high scored sentences subject standard word limit words generate summaries size beddings word sentence document set respectively list kernel sizes convolutional sentence encoder hop operation performed case memnet encoder lstm parameters domly initialized uniform distribution use batch size documents learning rate momentum parameters use adam optimizer evaluation metrics evaluate quality system summaries rouge unigram overlap bigram overlap means assessing informativeness rouge means assessing fluency baseline methods evaluate system state art baselines select best systems having state art summarization sults duc corpus single document summarization task ilp tgraph urank summarunner deep classifier ilp phrase based extraction model selects salient phrases recombines subject length grammar constraints integer linear programming ilp tgraph graph based tence extraction model urank uses unified ranking multi document summarization use lead standard baseline simply selecting leading sentences document summary neural network based sentence extractor deep classifier uses gru rnn tially accept reject sentence document summary summarunner rnn based extractive summarizer november singapore singapore table rouge evaluation corpus samples daily mail corpus duc rouge lead ilp tgraph urank deep classifier summarunner hybrid memnet hybrid memnet dailymail rouge lead deep classifier summarunner hybrid memnet hybrid memnet results analysis section compare performance system summarization baselines mentioned section table shows results duc test dataset samples daily mail corpus hybrid memnet represents system conv lstm encoder memnet encoder hybrid memnet uses conv blstm encoder memnet encoder evident results system hybrid hybrid memnet outperforms lead ilp baselines large margin encouraging result system access manually crafted features syntactic information sophisticated linguistic constraints case ilp results system performs better sentence ranking mechanism urank achieves significant performance gain deep classifier summarunner explore contribution memnet encoder performance system compare results hybrid memnet note significant performance gain results post hoc tukey tests showed proposed hybrid memnet model significantly better fact memnet learns document representation captures salience estimation sentence attention mechanism prior summary generation notice replacing lstm blstm encoder improves performance system blstm setting able learn richer set semantics exploit notion future context processing sequential data directions lstm able use previous context conclusions work proposed data driven end end deep neural network approach extractive summarization document system makes use combination memory network convolutional bidirectional long short term memory network learn better unified document representation jointly tures gram features sentence level information notion summary worthiness sentences eventually leading ter summary generation experimental results duc daily mail datasets confirm system outperforms state art baselines references ronan collobert jianpeng cheng mirella lapata neural summarization extracting sentences words proc annual meeting association computational linguistics association computer linguistics jason weston leon bottou michael karlen koray kavukcuoglu pavel kuksa natural language processing scratch journal machine learning research aug john conroy dianne oleary text summarization hidden markov models proc annual intl acm sigir conf research development information retrieval acm gunes erkan dragomir radev lexrank graph based lexical ity salience text summarization journal artificial intelligence research elena filatova vasileios hatzivassiloglou event based extractive summarization proc acl workshop summarization barcelona spain mikael kageback olof mogren nina tahmasebi devdatt dubhashi extractive summarization continuous vector space models proc workshop continuous vector space models compositionality eacl citeseer yoon kim convolutional neural networks sentence classification emnlp acl diederik kingma jimmy adam method stochastic tion arxiv preprint julian kupiec jan pedersen francine chen trainable document summarizer proc annual intl acm sigir conf research development information retrieval acm chin yew lin eduard hovy automatic evaluation summaries gram occurrence statistics proc conf north american chapter association computational linguistics human language technology volume association computational linguistics rada mihalcea language independent extractive summarization proc acl interactive poster demonstration sessions acl ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents proc thirty aaai conf artificial intelligence ramesh nallapati bowen zhou mingbo classify select neural architectures extractive document summarization corr ani nenkova lucy vanderwende kathleen mckeown tional context sensitive multi document summarizer exploring factors influence summarization proc annual intl acm sigir conf research development information retrieval acm daraksha parveen hans martin ramsl michael strube topical coherence graph based extractive summarization hava siegelmann eduardo sontag computational power neural nets proc fifth annual workshop computational learning theory acm richard socher eric huang jeffrey pennington andrew pher manning dynamic pooling unfolding recursive autoencoders paraphrase detection nips vol sainbayar sukhbaatar jason weston rob fergus end end memory networks advances neural information processing systems xiaojun wan unified approach simultaneous document multi document summarizations proc intl conf computational linguistics acl kristian woodsend mirella lapata automatic generation story highlights proc annual meeting association computational linguistics association computational linguistics wenpeng yin yulong pei optimizing sentence modeling selection document summarization ijcai xingxing zhang mirella lapata chinese poetry generation recurrent neural networks emnlp
