hierarchical end end model jointly improving text summarization sentiment classication shuming junyang xuancheng key lab computational linguistics school eecs peking university foreign languages peking university shumingma xusun linjunyang edu abstract text summarization sentiment classication aim capture main ideas text different levels text summarization describe text sentences sentiment classication regarded special type summarization summarizes text abstract fashion sentiment class based idea propose hierarchical end model joint learning text tion sentiment classication ment classication label treated summarization text summarization sentiment classication layer text summarization layer archical structure derived experimental results amazon online reviews datasets model achieves better performance strong baseline systems abstractive tion sentiment classication introduction text summarization sentiment classication portant tasks natural language processing text rization aims generating summary major points original text compared extractive summarization selects subset existing words original text form summary abstractive summarization builds ternal semantic representation uses natural language generation techniques create summary closer human express work mainly focus abstractive text summarization sentiment tion assign sentiment label determine attitude opinion inside text known opinion ing deriving opinion attitude speaker text summarization sentiment classication aim mining main ideas text text summarization describes text words sentences specic way ment classication summarizes text labels abstractive way existing models built rization classication abstractive text tion popular model sequence sequence model sutskever rush erating short summary long source text garded mapping long sequence short sequence model consists encoder coder encoder encodes original text tent representation decoder generates summary recent abstractive summarization models ants sequence sequence model chopra sentiment classication cent work uses neural network architecture kim tang lstm cnn generate text embedding use multi layer perceptron mlp dict label embedding previous hole takalikar mane proposes models produce summaries sentiment labels models train summarization sentiment classication independently require rich craft titov mcdonald sentiment lerman aim extracting sentences certain sentiment class original texts work focuses summarization improve sentiment classication work summarization features work work explore rst step improving text summarization sentiment classication end end framework propose hierarchical end end model consists summarization layer sentiment classication layer summarization layer presses original text short sentences ment classication layer summarizes texts sentiment class hierarchical structure establishes close bond text summarization sentiment tion tasks improve pressing texts summarization easier sentiment classier predict sentiment labels shorter text text summarization point important informative words remove redundant misleading information harmful predict timent sentiment classication provide nicant supervision signal text summarization guides summarization component capture sentiment dency original text improve coherence short text original text evaluate proposed model amazon online views datasets experimental results model achieves better performance strong baseline systems summarization sentiment classication contributions paper listed follows treat sentiment classication special type summarization perform sentiment classication text summarization unied model propose multi view attention obtain different representation texts summarization timent classication experimental results shows model outperforms strong baselines train summarization sentiment classication separately proposed model section introduce proposed model details section problem formulation explain overview proposed model section introduce components model section section finally section gives overall loss tion training methods problem formulation given online reviews dataset consists data ples data sample contains original text summary sentiment label original content summary sequences words denote number words quences respectively label denotes sentiment attitude original content lowest rating highest rating model applied learn mapping source text target summary sentiment label purpose simplicity denote data pair rest section word sequence original text word sequence corresponding summary corresponding sentiment label model overview figure shows architecture model model consists components text encoder summary decoder sentiment classier text coder compresses original text context ory directional lstm summary decoder uni directional lstm generates summary tor sentiment vector sequentially tention mechanism querying context memory summary vectors generate summary word generator sentiment vectors time steps collected fed sentiment classier dict sentiment label order capture context mation original text use highway mechanism feed context memory input stars sentiment classifier nice magnetic toy highway summary decoder text encoder toy bought figure overview model classier classier predicts label ing sentiment vectors summary decoder context memory text encoder text encoder goal source text encoder provide series dense representation original text decoder model original text encoder classier directional long short term memory network bilstm produces context memory source text forward backward functions lstm time step forward backward hidden outputs respectively input time step number words sequence convolutional neural network cnn alternative choice encoder bilstm popular sequence sequence learning text generation tasks including abstractive text summarization according experiments bilstm achieves better performance sentiment classication benchmark datasets details comparison cnn bilstm tion summary decoder multi view attention goal summary decoder generate series summary words provides summary information sentiment classier model summary decoder consists uni directional lstm multi view attention mechanism word generator lstm rst generates hidden output conditioned historical information generated summary function lstm time step generated words time step given hidden output implement multi view tention mechanism retrieval summary information sentiment information context memory original text motivation multi view attention model focus different original text summarization classication tion attention mechanism focus informative words describe main points best sentiment sication attention mechanism focus words contains sentimental tendency great bad implementation multi view attention generates summary vector summarization tihi tanh wthi trainable parameter matrix similar mary vector sentiment vector generated attention mechanism following equation different trainable parameters multi view attention regarded independent global attentions learn focus summary aspect sentiment aspect word generator compute probability distribution output words time step given summary vector sof parameters generator word highest probability emitted word erated summary summary aware sentiment classier decoding words end summary model collects sentiment vectors time step concatenate summary sentiment vectors original text representation perform pooling operation obtain sentiment context vector denote highway operation figure denotes operation concatenation rst dimension number words summary number words original text ment context vector fed classier compute probability distribution sentiment label classier layer feed forward network relu activation function label highest probability predicted sentiment label overall loss function training loss function consists parts cross entropy loss summarization sentiment cation log log ground truth words labels probability distribution words labels computed equation jointly minimize losses adam kingma optimizer hyper parameter balance losses set work experiments section evaluate model amazon line review dataset contains online reviews maries sentiment labels rst introduce datasets evaluation metrics experimental details pare model popular baseline systems nally provide analysis discussion model datasets amazon snap review dataset snap dataset stanford network analysis project vided mcauley dataset consists views amazon contains product reviews data amazon including million reviews spanning july includes review content product user information ratings summaries pair view content corresponding summary sentiment label select domains product reviews construct benchmark datasets toys games sports outdoors movie select rst ples dataset validation set following samples test set rest training set evaluation metric abstractive summarization evaluation metric rouge score lin hovy popular summarization evaluation metrics compare ically produced summary reference summaries computing overlapping lexical units including unigram gram trigram longest common subsequence lcs lowing previous work rush use unigram gram lcs evaluation metrics reported tal results sentiment classication evaluation metric label accuracy evaluate accuracy class stanford edu data web amazon html sentiment sentiment classied class class sentiment sentiment itive negative experimental details model parameters vocabularies extracted training sets source contents summaries share ies tune hyper parameters based performance validation sets limit vocabulary frequent words pearing training set set word embedding hidden size toys sports movies datasets respectively word embedding dom initialized learned scratch encoder single layer bidirectional lstm decoder single layer unidirectional lstm classier layer forward network hidden dimension batch size use dropout probability toys sports movies datasets respectively model training use adam kingma optimization method train model hyper parameters adam optimizer set learning rate tum parameters respectively following sutskever train model total epochs start halve ing rate half epoch epochs clip ents pascanu maximum norm baselines abstractive summarization baseline model sequence sequence model abstractive summarization following previous work denote sequence sequence model attention anism attention mechanism att text classication compare model baseline models bilstm cnn baseline models bilstm model uses bidirectional lstm dimension direction uses max pooling lstm hidden states sentence embedding vector uses mlp output layer hidden states output classication result cnn model uses scheme substitutes bilstm layer convolutional network training use dropout mlp use adam optimizer ing rate batch size bilstm clip norm gradients searched parameters wide range aforementioned set hyperparameters yield highest accuracy baseline models exploit tated data summaries sentiment labels fairer comparison implement joint model att bilstm att bilstm annotated bels summaries sentiments train line model compare model model order analyze improvements model given exactly toys games att att bilstm hssc work sports outdoors att att bilstm hssc work movie att att bilstm hssc work table comparison model sequence sequence baseline abstractive summarization amazon snap test sets test sets include domains toys gamse sports outdoors movie note rouge respectively annotated data baseline model att stm share encoder att produces summary lstm decoder bilstm predicts sentiment label mlp tune hyper parameter validation set set word embedding hidden size batch size dropout rate toys sports movies datasets respectively results denote hierarchical summarization sentiment classication model hssc abstractive summarization compare model sequence sequence baseline amazon snap test sets report rouge score model baseline models test sets shown table hssc model large margin att models test sets shows supervision ment labels improves representation original text given exactly annotated data summary sentiment label hssc model improvement att bilstm baseline indicates hssc learns better representation summarization hssc achieves best performance terms rouge baseline models test sets summarization task online review texts difcult complicate rouge scores snap dataset lower summarization datasets duc documents duc datasets originally news website texts formal summaries duc manually selected written snap dataset constructed reviews amazon toys games cnn bilstm bilstm att hssc work sports outdoors cnn bilstm bilstm att hssc work movie cnn bilstm bilstm att hssc work class class class class class class table comparison model sequence sequence baselines sentiment classication amazon snap test sets test sets include domains toys games sports outdoors movie class class denote accuracy class sentiment class sentiment tively original reviews corresponding summaries informal noise sentiment classication compare model popular sentiment tion methods cnn bilstm amazon snap test sets report accuracy grained timent class sentiment test sets shown table slightly improvement cnn baseline showing bilstm better performance represent texts datasets select bilstm encoder model hssc obtains better performance widely baseline models test sets mainly benet labeled data better representation hssc forms att bilstm baseline showing formation summary decoder helps predict ment labels overall hssc achieves best performance terms class accuracy class accuracy baseline models test sets conducted signicance tests based test signicance tests suggest hssc signicant improvement baselines rouge metrics summarization benchmark datasets sentiment classication toys games movies datasets ment classication sports outdoors datasets ablation study order analyze effect components remove components multi view highway order uate performance rest model rst remove multi view attention shown table model multi view attention drop performance class accuracy rouge concluded toys games multi view highway hssc model class sports outdoors multi view highway hssc model class movie multi view highway hssc model class table ablation study class denotes accuracy grained sentiment denotes rouge summarization multi view attention improves performance stractive summarization sentiment classication ther remove highway highway nent benets sentiment classication bot abstractive summarization benet mainly comes fact gradient sentiment classier directly propagated encoder learns better representation original text classication summarization visualization multi view attention shown table present heatmap tion scores examples multi view attention lows model represent text sentiment view summary view order analyze multi view attention captures sentiment information summary information heatmap sentiment view attention summary view attention spectively average attention scores decoder outputs time steps mark high scores deep color low scores light color table conclude sentiment view attention cuses sentimental words best ful great fun comfortable summary view attention concentrates informative words best scribes opinion authors think best movie great book fun sentiment view attention focuses individual words summary view pays attention word sequences sentiment view attention summary view attention share focus tive words showing benet multi view attention related work rush rst proposes abstractive based marization model uses attentive cnn encoder compress texts neural network language model erate summaries chopra explores recurrent structure abstractive summarization deal vocabulary problem nallapati proposes saw movie times theater think best movies best movie christ passion god bless responsible creation powerful daughter years old received christmas gift ready times passed son children enjoy tactile quality monkeys faces helpful learning counting feel enjoyed reading sing song story long read years pretty memorized great book fun mattress narrow comfortable air found balancing act switch positions tried air effect think sleep stay position unfortunately sleep strong vinyl smell away airing sentiment view original text saw movie times theater think best movies best movie christ passion god bless responsible creation powerful daughter years old received christmas gift ready times passed son children enjoy tactile quality monkeys faces helpful learning counting feel enjoyed reading sing song story long read years pretty memorized great book fun mattress narrow comfortable air found balancing act switch positions tried air effect think sleep stay position unfortunately sleep strong vinyl smell away airing summary view original text table visualization multi view attention heatmap sentiment view attention heatmap summary view attention deeper colors means higher attention scores generator pointer model decoder able erate words source texts solves issue incorporating copying mechanism allowing parts summaries copied source contents discusses problem incorporates pointer generator model coverage mechanism builds large corpus chinese social dia short text summarization chen introduces distraction based neural model forces attention mechanism focus difference parts source puts proposes neural model improve semantic relevance source contents predicted summaries work concerning summarization sentiment classication hole takalikar mana propose models produce summaries sentiment labels els train summarization sentiment tion independently require rich hand craft features work improved summarization help classication cao proposes model train summary generator text classier jointly improves performance text summarization titov mcdonald proposes sentiment summarization method extract summary texts given ment class lerman builds new summarizer training ranking svm model set human ence judgments improves performance sentiment summarization different works model improves text summarization sentiment tion require hand craft features conclusions work propose model generate timent labels human like summaries hoping marize opinions coarse grained sentiment labels grained word sequences evaluate proposed model online reviews datasets experimental sults model achieves better performance baseline systems abstractive summarization sentiment classication acknowledgements work supported national natural science foundation china national high nology research development program china program national thousand young talents program sun corresponding author paper references cao ziqiang cao wenjie sujian furu wei improving multi document summarization text classication aaai pages chen qian chen xiaodan zhu zhenhua ling wei hui jiang distraction based neural networks modeling documents ijcai new york july aaai cheng lapata jianpeng cheng mirella pata neural summarization extracting sentences words acl chopra sumit chopra michael auli alexander rush abstractive sentence summarization attentive recurrent neural networks naacl hlt pages jiatao zhengdong hang incorporating copying mechanism victor sequence sequence learning acl mcauley ruining julian mcauley ups downs modeling visual evolution ion trends class collaborative ltering www pages hole takalikar vikrant hole mukta likar real time tweet summarization sentiment ysis game tournament international journal science research baotian qingcai chen fangze zhu lcsts large scale chinese short text tion dataset emnlp pages kim yoon kim convolutional neural networks emnlp pages sentence classication kingma diederik kingma jimmy adam method stochastic optimization corr lerman kevin sentiment goldensohn ryan mcdonald summarization evaluating learning user preferences eacl pages lerman sasha lin hovy chin yew lin eduard hovy automatic evaluation summaries gram occurrence statistics hlt naacl shuming sun jingjing houfeng wang wenjie improving tic relevance sequence sequence learning chinese social media text summarization acl pages shuming sun wei sujian wenjie xuancheng ren query output ating words querying distributed word representations paraphrase generation naacl mane vinod mane suja panicker vidya patil summarization sentiment analysis user health posts pervasive computing icpc international conference pages ieee nallapati ramesh nallapati bowen zhou ccero nogueira dos santos aglar gulcehre bing xiang abstractive text summarization sequence sequence rnns conll pages pascanu razvan pascanu tomas mikolov yoshua bengio difculty training recurrent ral networks icml pages rush alexander rush sumit chopra jason weston neural attention model abstractive sentence summarization emnlp pages abigail peter liu pher manning point summarization acl pages pointer generator networks sun sun xuancheng ren shuming houfeng wang meprop sparsied propagation accelerated deep learning reduced overtting icml pages sun sun bingzhen wei xuancheng ren shuming label embedding network learning label representation soft training deep networks corr sutskever ilya sutskever oriol vinyals quoc sequence sequence learning neural networks nips pages takase sho takase jun suzuki naoaki okazaki tsutomu hirao masaaki nagata neural headline generation abstract meaning representation emnlp pages tang duyu tang bing qin ting liu ument modeling gated recurrent neural network emnlp pages sentiment classication titov mcdonald ivan titov ryan donald joint model text aspect ratings ment summarization acl pages jingjing sun xuancheng ren gan junyang lin binzhen wei wei diversity promoting generative adversarial network corr generating informative diversied text jingjing sun zeng xiaodong zhang xuancheng ren houfeng wang wenjie unpaired sentiment sentiment translation cycled inforcement learning approach acl
