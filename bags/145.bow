m l c s c v v x r hierarchical end end model jointly improving text summarization sentiment classication shuming xu junyang xuancheng key lab computational linguistics school eecs peking university foreign languages peking university shumingma xusun linjunyang edu abstract text summarization sentiment classication aim capture main ideas text different levels text summarization describe text sentences sentiment classication regarded special type summarization summarizes text abstract fashion e sentiment class based idea propose hierarchical end model joint learning text tion sentiment classication ment classication label treated summarization text summarization sentiment classication layer text summarization layer archical structure derived experimental results amazon online reviews datasets model achieves better performance strong baseline systems abstractive tion sentiment classication introduction text summarization sentiment classication portant tasks natural language processing text rization aims generating summary major points original text compared extractive summarization selects subset existing words original text form summary abstractive summarization builds ternal semantic representation uses natural language generation techniques create summary closer human express work mainly focus abstractive text summarization sentiment tion assign sentiment label determine attitude opinion inside text known opinion ing deriving opinion attitude speaker text summarization sentiment classication aim mining main ideas text text summarization describes text words sentences specic way ment classication summarizes text labels abstractive way existing models built rization classication abstractive text tion popular model sequence sequence model sutskever et al rush et al erating short summary long source text garded mapping long sequence short sequence model consists encoder coder encoder encodes original text tent representation decoder generates summary recent abstractive summarization models ants sequence sequence model chopra et al et al sentiment classication cent work uses neural network architecture kim tang et al lstm cnn generate text embedding use multi layer perceptron mlp dict label embedding previous hole takalikar mane et al proposes models produce summaries sentiment labels models train summarization sentiment classication independently require rich craft titov mcdonald sentiment lerman et al aim extracting sentences certain sentiment class original texts work focuses summarization improve sentiment classication work summarization features work work explore rst step improving text summarization sentiment classication end end framework propose hierarchical end end model consists summarization layer sentiment classication layer summarization layer presses original text short sentences ment classication layer summarizes texts sentiment class hierarchical structure establishes close bond text summarization sentiment tion tasks improve pressing texts summarization easier sentiment classier predict sentiment labels shorter text text summarization point important informative words remove redundant misleading information harmful predict timent sentiment classication provide nicant supervision signal text summarization guides summarization component capture sentiment dency original text improve coherence short text original text evaluate proposed model amazon online views datasets experimental results model achieves better performance strong baseline systems summarization sentiment classication contributions paper listed follows treat sentiment classication special type summarization perform sentiment classication text summarization unied model propose multi view attention obtain different representation texts summarization timent classication experimental results shows model outperforms strong baselines train summarization sentiment classication separately proposed model section introduce proposed model details section problem formulation explain overview proposed model section introduce components model section section finally section gives overall loss tion training methods problem formulation given online reviews dataset consists n data ples th data sample xi yi li contains original text xi summary yi sentiment label li original content xi summary yi sequences words li yi xi xi xi yi xi yi li mi denote number words quences xi yi respectively label li k denotes sentiment attitude original content xi lowest rating highest rating k mi model applied learn mapping source text target summary sentiment label purpose simplicity y l denote data pair rest section word sequence original text y word sequence corresponding summary l corresponding sentiment label model overview figure shows architecture model model consists components text encoder summary decoder sentiment classier text coder compresses original text context ory h bi directional lstm summary decoder uni directional lstm generates summary tor sentiment vector sequentially tention mechanism querying context memory summary vectors generate summary word generator sentiment vectors time steps collected fed sentiment classier dict sentiment label order capture context mation original text use highway mechanism feed context memory input stars sentiment classifier nice magnetic toy highway summary decoder text encoder toy bought figure overview model classier classier predicts label ing sentiment vectors summary decoder context memory text encoder text encoder goal source text encoder provide series dense representation original text decoder model original text encoder classier bi directional long short term memory network bilstm produces context memory hl source text xt forward backward functions lstm time step forward backward hidden outputs respectively input t th time step l number words sequence convolutional neural network cnn alternative choice encoder bilstm popular sequence sequence learning text generation tasks including abstractive text summarization according experiments bilstm achieves better performance sentiment classication benchmark datasets details comparison cnn bilstm tion summary decoder multi view attention goal summary decoder generate series summary words provides summary information sentiment classier model summary decoder consists uni directional lstm multi view attention mechanism word generator lstm rst generates hidden output st conditioned historical information generated summary st f function lstm time step generated words t th time step given hidden output st implement multi view tention mechanism retrieval summary information sentiment information context memory h original text motivation multi view attention model focus different original text summarization classication tion attention mechanism focus informative words describe main points best sentiment sication attention mechanism focus words contains sentimental tendency great bad implementation multi view attention generates summary vector summarization t tihi n x hi n hj ti p hi tanh st t wthi wt trainable parameter matrix similar mary vector sentiment vector generated attention mechanism following equation different trainable parameters multi view attention regarded independent global attentions learn focus summary aspect sentiment aspect word generator t compute probability distribution output words t th time step given summary vector sof wg bg parameters generator word highest probability emitted t th word erated summary t bg summary aware sentiment classier decoding words end summary model collects sentiment vectors time step m concatenate summary sentiment vectors original text representation h perform pooling operation obtain sentiment context vector r denote highway operation figure r m hl denotes operation concatenation rst dimension m number words summary l number words original text ment context vector fed classier compute probability distribution sentiment label classier layer feed forward network relu activation function label highest probability predicted sentiment label overall loss function training loss function consists parts cross entropy loss summarization sentiment cation ls x t yt log lc l log yt l ground truth words labels probability distribution words labels computed equation jointly minimize losses adam kingma ba optimizer l ls lc hyper parameter balance losses set work experiments section evaluate model amazon line review dataset contains online reviews maries sentiment labels rst introduce datasets evaluation metrics experimental details pare model popular baseline systems nally provide analysis discussion model datasets amazon snap review dataset snap dataset stanford network analysis project vided mcauley dataset consists views amazon contains product reviews data amazon including million reviews spanning july includes review content product user information ratings summaries pair view content corresponding summary sentiment label select domains product reviews construct benchmark datasets toys games sports outdoors movie tv select rst ples dataset validation set following samples test set rest training set evaluation metric abstractive summarization evaluation metric rouge score lin hovy popular summarization evaluation metrics compare ically produced summary reference summaries computing overlapping lexical units including unigram gram trigram longest common subsequence lcs lowing previous work rush et al hu et al use unigram bi gram l lcs evaluation metrics reported tal results sentiment classication evaluation metric label accuracy evaluate accuracy ve class stanford edu data web amazon html sentiment sentiment classied class class sentiment sentiment itive negative experimental details model parameters vocabularies extracted training sets source contents summaries share ies tune hyper parameters based performance validation sets limit vocabulary frequent words pearing training set set word embedding hidden size toys sports movies datasets respectively word embedding dom initialized learned scratch encoder single layer bidirectional lstm decoder single layer unidirectional lstm classier layer forward network hidden dimension batch size use dropout probability toys sports movies datasets respectively model training use adam kingma ba optimization method train model hyper parameters adam optimizer set learning rate tum parameters respectively following sutskever et al train model total epochs start halve ing rate half epoch epochs clip ents pascanu et al maximum norm baselines abstractive summarization baseline model sequence sequence model abstractive summarization following previous work et al denote sequence sequence model attention anism attention mechanism att text classication compare model baseline models bilstm cnn baseline models bilstm model uses bidirectional lstm dimension direction uses max pooling lstm hidden states sentence embedding vector uses mlp output layer hidden states output classication result cnn model uses scheme substitutes bilstm layer convolutional network training use dropout mlp use adam optimizer ing rate batch size bilstm clip norm gradients searched parameters wide range nd aforementioned set hyperparameters yield highest accuracy baseline models exploit tated data summaries sentiment labels fairer comparison implement joint model att bilstm att bilstm annotated bels summaries sentiments train line model compare model model order analyze improvements model given exactly toys games et al att et al att bilstm hssc work rg l sports outdoors et al att et al att bilstm hssc work rg l movie tv et al att et al att bilstm hssc work rg l table comparison model sequence sequence baseline abstractive summarization amazon snap test sets test sets include domains toys gamse sports outdoors movie tv rg l note rouge l respectively annotated data baseline model att stm share encoder att produces summary lstm decoder bilstm predicts sentiment label mlp tune hyper parameter validation set set word embedding hidden size batch size dropout rate p toys sports movies datasets respectively results denote hierarchical summarization sentiment classication model hssc abstractive summarization compare model sequence sequence baseline amazon snap test sets report rouge score model baseline models test sets shown table hssc model large margin att models test sets shows supervision ment labels improves representation original text given exactly annotated data summary sentiment label hssc model improvement att bilstm baseline indicates hssc learns better representation summarization hssc achieves best performance terms rouge l baseline models test sets summarization task online review texts difcult complicate rouge scores snap dataset lower summarization datasets duc documents duc datasets originally news website texts formal summaries duc manually selected written snap dataset constructed reviews amazon toys games cnn bilstm bilstm att hssc work sports outdoors cnn bilstm bilstm att hssc work movie tv cnn bilstm bilstm att hssc work class class class class class class table comparison model sequence sequence baselines sentiment classication amazon snap test sets test sets include domains toys games sports outdoors movie tv class class denote accuracy ve class sentiment class sentiment tively original reviews corresponding summaries informal noise sentiment classication compare model popular sentiment tion methods cnn bilstm amazon snap test sets report accuracy ve grained timent class sentiment test sets shown table slightly improvement cnn baseline showing bilstm better performance represent texts datasets select bilstm encoder model hssc obtains better performance widely baseline models test sets mainly benet labeled data better representation s hssc forms att bilstm baseline showing formation summary decoder helps predict ment labels overall hssc achieves best performance terms class accuracy class accuracy baseline models test sets conducted signicance tests based t test signicance tests suggest hssc signicant improvement baselines p rouge metrics summarization benchmark datasets p sentiment classication toys games movies tv datasets p ment classication sports outdoors datasets ablation study order analyze effect components remove components multi view highway order uate performance rest model rst remove multi view attention shown table model multi view attention drop performance class accuracy rouge l concluded toys games multi view highway hssc model class rg l sports outdoors multi view highway hssc model class rg l movie tv multi view highway hssc model class rg l table ablation study class denotes accuracy ve grained sentiment rg l denotes rouge l summarization multi view attention improves performance stractive summarization sentiment classication ther remove highway nd highway nent benets sentiment classication bot abstractive summarization benet mainly comes fact gradient sentiment classier directly propagated encoder learns better representation original text classication summarization visualization multi view attention shown table present heatmap tion scores examples multi view attention lows model represent text sentiment view summary view order analyze multi view attention captures sentiment information summary information heatmap sentiment view attention summary view attention spectively average attention scores decoder outputs time steps mark high scores deep color low scores light color table conclude sentiment view attention cuses sentimental words e best ful great fun comfortable summary view attention concentrates informative words best scribes opinion authors e think best movie great book fun sentiment view attention focuses individual words summary view pays attention word sequences sentiment view attention summary view attention share focus tive words showing benet multi view attention related work rush et al rst proposes abstractive based marization model uses attentive cnn encoder compress texts neural network language model erate summaries chopra et al explores recurrent structure abstractive summarization deal vocabulary problem nallapati et al proposes saw movie times theater think best movies best movie christ passion god bless responsible creation powerful lm daughter years old received christmas gift ready times passed son children enjoy tactile quality monkeys faces helpful learning counting feel enjoyed reading sing song story long read years pretty memorized great book fun mattress narrow comfortable t ne air found balancing act switch positions tried air effect think sleep stay position ne unfortunately sleep strong vinyl smell away airing sentiment view original text saw movie times theater think best movies best movie christ passion god bless responsible creation powerful lm daughter years old received christmas gift ready times passed son children enjoy tactile quality monkeys faces helpful learning counting feel enjoyed reading sing song story long read years pretty memorized great book fun mattress narrow comfortable t ne air found balancing act switch positions tried air effect think sleep stay position ne unfortunately sleep strong vinyl smell away airing summary view original text table visualization multi view attention heatmap sentiment view attention heatmap summary view attention deeper colors means higher attention scores generator pointer model decoder able erate words source texts gu et al solves issue incorporating copying mechanism allowing parts summaries copied source contents et al discusses problem incorporates pointer generator model coverage mechanism hu et al builds large corpus chinese social dia short text summarization chen et al introduces distraction based neural model forces attention mechanism focus difference parts source puts ma et al proposes neural model improve semantic relevance source contents predicted summaries work concerning summarization sentiment classication hole takalikar mana et al propose models produce summaries sentiment labels els train summarization sentiment tion independently require rich hand craft features work improved summarization help classication cao et al proposes model train summary generator text classier jointly improves performance text summarization titov mcdonald proposes sentiment summarization method extract summary texts given ment class lerman et al builds new summarizer training ranking svm model set human ence judgments improves performance sentiment summarization different works model improves text summarization sentiment tion require hand craft features conclusions work propose model generate timent labels human like summaries hoping marize opinions coarse grained sentiment labels ne grained word sequences evaluate proposed model online reviews datasets experimental sults model achieves better performance baseline systems abstractive summarization sentiment classication acknowledgements work supported national natural science foundation china national high nology research development program china program national thousand young talents program xu sun corresponding author paper references cao et al ziqiang cao wenjie li sujian li furu wei improving multi document summarization text classication aaai pages chen et al qian chen xiaodan zhu zhenhua ling si wei hui jiang distraction based neural networks modeling documents ijcai new york ny july aaai cheng lapata jianpeng cheng mirella pata neural summarization extracting sentences words acl chopra et al sumit chopra michael auli alexander m rush abstractive sentence summarization attentive recurrent neural networks naacl hlt pages gu et al jiatao gu zhengdong lu hang li incorporating copying mechanism victor o k li sequence sequence learning acl mcauley ruining julian mcauley ups downs modeling visual evolution ion trends class collaborative ltering www pages hole takalikar vikrant hole mukta likar real time tweet summarization sentiment ysis game tournament international journal science research et al baotian hu qingcai chen fangze zhu lcsts large scale chinese short text tion dataset emnlp pages kim yoon kim convolutional neural networks emnlp pages sentence classication kingma ba diederik p kingma jimmy ba adam method stochastic optimization corr lerman et al kevin sentiment goldensohn ryan t mcdonald summarization evaluating learning user preferences eacl pages lerman sasha lin hovy chin yew lin eduard h hovy automatic evaluation summaries n gram occurrence statistics hlt naacl et al shuming ma xu sun jingjing xu houfeng wang wenjie li qi su improving tic relevance sequence sequence learning chinese social media text summarization acl pages et al shuming ma xu sun wei li sujian li wenjie li xuancheng ren query output ating words querying distributed word representations paraphrase generation naacl mane et al vinod l mane suja s panicker vidya b patil summarization sentiment analysis user health posts pervasive computing icpc international conference pages ieee nallapati et al ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre bing xiang abstractive text summarization sequence sequence rnns conll pages pascanu et al razvan pascanu tomas mikolov yoshua bengio difculty training recurrent ral networks icml pages rush et al alexander m rush sumit chopra jason weston neural attention model abstractive sentence summarization emnlp pages et al abigail peter j liu pher d manning point summarization acl pages pointer generator networks sun et al xu sun xuancheng ren shuming ma houfeng wang meprop sparsied propagation accelerated deep learning reduced overtting icml pages sun et al xu sun bingzhen wei xuancheng ren shuming ma label embedding network learning label representation soft training deep networks corr sutskever et al ilya sutskever oriol vinyals quoc v le sequence sequence learning neural networks nips pages takase et al sho takase jun suzuki naoaki okazaki tsutomu hirao masaaki nagata neural headline generation abstract meaning representation emnlp pages tang et al duyu tang bing qin ting liu ument modeling gated recurrent neural network emnlp pages sentiment classication titov mcdonald ivan titov ryan t donald joint model text aspect ratings ment summarization acl pages xu et al jingjing xu xu sun xuancheng ren dp gan junyang lin binzhen wei wei li diversity promoting generative adversarial network corr generating informative diversied text xu et al jingjing xu xu sun qi zeng xiaodong zhang xuancheng ren houfeng wang wenjie li unpaired sentiment sentiment translation cycled inforcement learning approach acl
