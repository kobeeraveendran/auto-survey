a m l c s c v v i x r a a hierarchical end to end model for jointly improving text summarization and sentiment classication shuming xu junyang xuancheng key lab of computational linguistics school of eecs peking university of foreign languages peking university shumingma xusun linjunyang abstract text summarization and sentiment classication both aim to capture the main ideas of the text but at different levels text summarization is to describe the text within a few sentences while sentiment classication can be regarded as a special type of summarization which summarizes the text into a even more abstract fashion a sentiment class based on this idea we propose a hierarchical to end model for joint learning of text tion and sentiment classication where the ment classication label is treated as the further summarization of the text summarization put hence the sentiment classication layer is put upon the text summarization layer and a archical structure is derived experimental results on amazon online reviews datasets show that our model achieves better performance than the strong baseline systems on both abstractive tion and sentiment classication introduction text summarization and sentiment classication are two portant tasks in natural language processing text rization aims at generating a summary with the major points of the original text compared with extractive summarization which selects a subset of existing words in the original text to form the summary abstractive summarization builds an ternal semantic representation and then uses natural language generation techniques to create a summary that is closer to what a human might express in this work we mainly focus on the abstractive text summarization sentiment tion is to assign a sentiment label to determine the attitude or the opinion inside the text it is also known as opinion ing deriving the opinion or the attitude of a speaker both text summarization and sentiment classication aim at mining the main ideas of the text text summarization describes the text with words and sentences in a more specic way while ment classication summarizes the text with labels in a more abstractive way most of the existing models are built for either rization or classication for abstractive text tion the most popular model is the sequence to sequence model sutskever et al rush et al where erating a short summary for the long source text can be garded as a mapping between a long sequence and a short sequence the model consists of an encoder and a coder the encoder encodes the original text into a tent representation and the decoder generates the summary some recent abstractive summarization models are the ants of the sequence to sequence model chopra et al see et al for sentiment classication most of the cent work uses the neural network architecture kim tang et al such as lstm or cnn to generate a text embedding and use a multi layer perceptron mlp to dict the label from the embedding previous hole and takalikar mane et al proposes the models to produce both the summaries and the sentiment labels however these models train the summarization part and the sentiment classication part independently and require rich craft the titov and mcdonald sentiment lerman et al which aim at extracting the sentences with a certain sentiment class from the original texts these work only focuses on the summarization and does not improve the sentiment classication there are also some work about summarization features some work in this work we explore a rst step towards improving both text summarization and sentiment classication within an end to end framework we propose a hierarchical end end model which consists of a summarization layer and a sentiment classication layer the summarization layer presses the original text into short sentences and the ment classication layer further summarizes the texts into a sentiment class the hierarchical structure establishes a close bond between text summarization and sentiment tion so that the two tasks can improve each other after pressing the texts with summarization it will be easier for the sentiment classier to predict the sentiment labels of the shorter text besides text summarization can point out the important and informative words and remove the redundant and misleading information that is harmful to predict the timent the sentiment classication can provide a more nicant supervision signal for text summarization and guides the summarization component to capture the sentiment dency of the original text which can improve the coherence between the short text and the original text we evaluate our proposed model on amazon online views datasets experimental results show that our model achieves better performance than the strong baseline systems on both summarization and sentiment classication the contributions of this paper are listed as follows we treat the sentiment classication as a special type of summarization and perform sentiment classication and text summarization using a unied model we propose a multi view attention to obtain different representation of the texts for summarization and timent classication experimental results shows that our model outperforms the strong baselines that train the summarization and sentiment classication separately proposed model in this section we introduce our proposed model in details in section we give the problem formulation we explain the overview of our proposed model in section then we introduce each components of the model from section to section finally section gives the overall loss tion and the training methods problem formulation given an online reviews dataset that consists of n data ples the i th data sample xi yi li contains an original text xi a summary yi and a sentiment label li both the original content xi and the summary yi are sequences of words li yi xi xi xi yi xi yi where li and mi denote the number of words in the quences xi and yi respectively the label li k denotes the sentiment attitude of the original content xi from the lowest rating to the highest rating mi the model is applied to learn the mapping from the source text to the target summary and the sentiment label for the purpose of simplicity y l is used to denote each data pair in the rest of this section where is the word sequence of an original text y is the word sequence of the corresponding summary and l is the corresponding sentiment label model overview figure shows the architecture of our model our model consists of three components which are the text encoder the summary decoder and the sentiment classier the text coder compresses the original text into the context ory h with a bi directional lstm the summary decoder is a uni directional lstm which then generates a summary tor and a sentiment vector sequentially with the tention mechanism by querying the context memory the summary vectors are used to generate the summary with a word generator the sentiment vectors of all time steps are collected and then fed into the sentiment classier to dict the sentiment label in order to capture the context mation of the original text we use the highway mechanism to feed the the context memory as part of the input of the five stars sentiment classifier nice magnetic toy highway summary decoder text encoder the toy bought it figure the overview of our model classier therefore the classier predicts the label ing to the sentiment vectors of the summary decoder and the context memory of the text encoder text encoder the goal of the source text encoder is to provide a series of dense representation of the original text for the decoder and in our model the original text encoder is a the classier bi directional long short term memory network bilstm which produces the context memory hl from the source text xt where and are the forward and the backward functions of lstm for one time step and are the forward and the backward hidden outputs respectively is the input at the t th time step and l is the number of words in sequence although convolutional neural network cnn is also an alternative choice for the encoder bilstm is more popular for the sequence to sequence learning of text generation tasks including abstractive text summarization besides according to our experiments bilstm achieves better performance in sentiment classication on our benchmark datasets we give the details of the comparison of cnn and bilstm in tion summary decoder with multi view attention the goal of the summary decoder is to generate a series of summary words and provides the summary information for the sentiment classier in our model the summary decoder consists of a uni directional lstm a multi view attention mechanism and a word generator the lstm rst generates the hidden output st conditioned on the historical information of the generated summary st where f is the function of lstm for one time step and is the last generated words at t th time step given the hidden output st we implement a multi view tention mechanism to retrieval the summary information and the sentiment information from the context memory h of the original text the motivation of the multi view attention is that the model should focus on different part of the original text for summarization and classication for tion the attention mechanism should focus on the informative words that describe the main points best for sentiment sication the attention mechanism should focus on the words that contains the most sentimental tendency such as great bad and so on in implementation the multi view attention generates a summary vector for summarization t tihi n x hi n hj ti p hi tanh st t wthi where wt is a trainable parameter matrix similar to the mary vector the sentiment vector is also generated with the attention mechanism following equation and but has different trainable parameters the multi view attention can be regarded as two independent global attentions to learn to focus more on the summary aspect or the sentiment aspect the word generator is used t to compute the probability distribution of the output words at t th time step given the summary vector sof where wg and bg are parameters of the generator the word with the highest probability is emitted as t th word of the erated summary t bg summary aware sentiment classier after decoding the words until the end of the summary the model collects the sentiment vectors of all time step m then we concatenate the summary sentiment vectors and the original text representation h and perform a pooling operation to obtain a sentiment context vector r which we denote as a highway operation in figure r m hl where denotes the operation of concatenation along the rst dimension m is the number of words in the summary and l is the number of words in the original text the ment context vector is then fed into the classier to compute the probability distribution of the sentiment label the classier is a two layer feed forward network with relu as the activation function the label with the highest probability is the predicted sentiment label overall loss function and training the loss function consists of two parts which are the cross entropy loss of summarization and that of sentiment cation ls x t yt log lc l log where yt and l are the ground truth of words and labels and and are the probability distribution of words and labels computed by equation we jointly minimize the two losses with adam kingma and ba optimizer l ls lc where is a hyper parameter to balance two losses we set in this work experiments in this section we evaluate our model on the amazon line review dataset which contains the online reviews maries and sentiment labels we rst introduce the datasets evaluation metrics and experimental details then we pare our model with several popular baseline systems nally we provide the analysis and the discussion of our model datasets amazon snap review dataset snap this dataset is part of stanford network analysis project and is vided by he and mcauley the dataset consists of views from amazon and contains product reviews and data from amazon including million reviews spanning may july it includes review content product user information ratings and summaries we pair each view content with the corresponding summary and sentiment label we select three domains of product reviews to construct three benchmark datasets which are toys games sports outdoors and movie tv we select the rst ples of each dataset as the validation set the following samples as the test set and the rest as the training set evaluation metric for abstractive summarization our evaluation metric is rouge score lin and hovy which is popular for summarization evaluation the metrics compare an ically produced summary with the reference summaries by computing overlapping lexical units including unigram gram trigram and longest common subsequence lcs lowing previous work rush et al hu et al we use unigram bi gram and l lcs as the evaluation metrics in the reported tal results for sentiment classication the evaluation metric is label accuracy we evaluate the accuracy of both ve class sentiment of which the sentiment is classied into class and two class sentiment of which the sentiment is either itive or negative experimental details model parameters the vocabularies are extracted from the training sets and the source contents and the summaries share the same ies we tune the hyper parameters based on the performance on the validation sets we limit the vocabulary to most frequent words pearing in the training set we set the word embedding and the hidden size to and for toys sports and movies datasets respectively the word embedding is dom initialized and learned from scratch the encoder is a single layer bidirectional lstm the decoder is a single layer unidirectional lstm and the classier is a two layer forward network with a hidden dimension the batch size is and we use dropout with probability for toys sports and movies datasets respectively model training we use the adam kingma and ba optimization method to train the model for the hyper parameters of adam optimizer we set the learning rate two tum parameters and respectively and following sutskever et al we train the model for a total of epochs and start to halve the ing rate every half epoch after epochs we clip the ents pascanu et al to the maximum norm of baselines for abstractive summarization our baseline model is the sequence to sequence model for abstractive summarization following the previous work hu et al we denote the sequence to sequence model without the attention anism as and that with the attention mechanism as att for text classication we compare our model with two baseline models bilstm and cnn for the two baseline models the bilstm model uses a bidirectional lstm with the dimension of in each direction and uses max pooling across all lstm hidden states to get the sentence embedding vector and then uses an mlp output layer with hidden states to output the classication result the cnn model uses the same scheme but substitutes bilstm with layer of convolutional network during training we use dropout on the mlp we use adam as the optimizer with a ing rate of and a batch size of for bilstm we also clip the norm of gradients to be we searched parameters in a wide range and nd the aforementioned set of hyperparameters yield the highest accuracy the above baseline models only exploit part of the tated data either summaries or sentiment labels for fairer comparison we also implement a joint model of att and bilstm att bilstm and both the annotated bels of summaries and sentiments are used to train this line model we compare our model with this model in order to analyze the improvements of our model given exactly the toys games et al att et al att bilstm hssc this work rg l sports outdoors et al att et al att bilstm hssc this work rg l movie tv et al att et al att bilstm hssc this work rg l table comparison between our model and the sequence sequence baseline for abstractive summarization on the amazon snap test sets the test sets include three domains toys gamse sports outdoors and movie tv and rg l note and rouge l respectively same annotated data in this baseline model att and stm share the same encoder and the att produces the summary with a lstm decoder while the bilstm predicts the sentiment label with a mlp we tune the hyper parameter on the validation set we set the word embedding and the hidden size to and the batch size is and the dropout rate is p for toys sports and movies datasets respectively results we denote our hierarchical summarization and sentiment classication model as hssc abstractive summarization first we compare our model with the sequence to sequence baseline on the amazon snap test sets we report the rouge score of our model and the baseline models on the test sets as shown in table our hssc model has a large margin over both and att models on all of the three test sets which shows that the supervision of the ment labels improves the representation of the original text moreover given exactly the same annotated data summary sentiment label our hssc model still has an improvement over the att bilstm baseline which indicates that hssc learns a better representation for summarization all hssc achieves the best performance in terms of and rouge l over the three baseline models on the three test sets the summarization task on the online review texts is much more difcult and complicate so the rouge scores on the snap dataset are lower than other summarization datasets such as duc the documents in duc datasets are originally from news website so the texts are formal and the summaries in duc are manually selected and well written the snap dataset is constructed with the reviews on the amazon and toys games cnn bilstm bilstm att hssc this work sports outdoors cnn bilstm bilstm att hssc this work movie tv cnn bilstm bilstm att hssc this work class class class class class class table comparison between our model and the sequence sequence baselines for sentiment classication on the amazon snap test sets the test sets include three domains toys games sports outdoors and movie tv class and class denote the accuracy of ve class sentiment and two class sentiment tively both the original reviews and the corresponding summaries are informal and full of noise sentiment classication we compare our model with two popular sentiment tion methods which are cnn and bilstm on the amazon snap test sets we report the accuracy of ve grained timent and two class sentiment on the test sets as shown in table has a slightly improvement over the cnn baseline showing that bilstm has a better performance to represent the texts on these datasets therefore we select bilstm as the encoder of our model hssc obtains a better performance over the two widely used baseline models on all of the test sets mainly because of the benet of more labeled data and better representation what s more hssc forms the att bilstm baseline showing that the formation from summary decoder helps to predict the ment labels overall hssc achieves the best performance in terms of class accuracy and class accuracy over the three baseline models on the three test sets we have conducted signicance tests based on t test the signicance tests suggest that hssc has a very signicant improvement over all of the baselines with p in all of rouge metrics for summarization in three benchmark datasets p for sentiment classication in both toys games and movies tv datasets and p for ment classication in the sports outdoors datasets ablation study in order to analyze the effect of each components we remove the components of multi view and highway in order and uate the performance of the rest model we rst remove the multi view attention as shown in table the model out multi view attention has a drop of performance on both class accuracy and rouge it can be concluded that the toys games multi view highway hssc full model class rg l sports outdoors multi view highway hssc full model class rg l movie tv multi view highway hssc full model class rg l table ablation study class denotes the accuracy of ve grained sentiment and rg l denotes rouge l for summarization multi view attention improves the performance of both stractive summarization and sentiment classication we ther remove the highway part and nd the highway nent benets not only the sentiment classication bot also the abstractive summarization the benet mainly comes from the fact that the gradient of the sentiment classier can be directly propagated to the encoder so that it learns a better representation of the original text for both classication and summarization visualization of multi view attention as shown in table we present the heatmap of the tion scores of three examples the multi view attention lows the model to represent the text from the sentiment view and from the summary view in order to analyze whether the multi view attention captures the sentiment information and the summary information we give the heatmap of the sentiment view attention and the summary view attention spectively we take the average of the attention scores in the decoder outputs at all time steps and mark the high scores with deep color and the low scores with light color from the table we conclude that the sentiment view attention cuses more on the sentimental words best ful great fun and comfortable the summary view attention concentrates on the informative words that best scribes the opinion of the authors i think that this is one of the best movie and a great book very fun moreover the sentiment view attention focuses more on the individual words while the summary view pays more attention on the word sequences besides the sentiment view attention and the summary view attention share the focus on the tive words showing the benet from the multi view attention related work rush al rst proposes an abstractive based marization model which uses an attentive cnn encoder to compress texts and a neural network language model to erate summaries chopra et al explores a recurrent structure for abstractive summarization to deal with of vocabulary problem nallapati et al proposes a i saw this movie times in the theater and i think that this is one of the best movies ever made and the best movie made about christ and his passion god bless all those responsible for the creation of this powerful lm my daughter who is now years old received this as a christmas gift when she was it has been ready many times and since been passed along to my son who is now my children enjoy the tactile quality of the monkeys faces it is helpful learning counting when there is something they can feel i have always enjoyed reading the sing song story it does not take long to read and after all these years i pretty much have it memorized a great book very fun this mattress is too narrow to be comfortable you t on it ne but because of the air i found that it was a balancing act to switch positions i tried more and less air to no effect i think if you sleep on your back and stay in that position it would be ne but unfortunately that is not how i sleep the strong vinyl smell does go away after airing out though a sentiment view of the original text i saw this movie times in the theater and i think that this is one of the best movies ever made and the best movie made about christ and his passion god bless all those responsible for the creation of this powerful lm my daughter who is now years old received this as a christmas gift when she was it has been ready many times and since been passed along to my son who is now my children enjoy the tactile quality of the monkeys faces it is helpful learning counting when there is something they can feel i have always enjoyed reading the sing song story it does not take long to read and after all these years i pretty much have it memorized a great book very fun this mattress is too narrow to be comfortable you t on it ne but because of the air i found that it was a balancing act to switch positions i tried more and less air to no effect i think if you sleep on your back and stay in that position it would be ne but unfortunately that is not how i sleep the strong vinyl smell does go away after airing out though summary view of the original text table visualization of multi view attention above is the heatmap of the sentiment view attention and below is the heatmap of the summary view attention deeper colors means higher attention scores generator pointer model so that the decoder is able to erate words in source texts gu et al also solves this issue by incorporating copying mechanism allowing parts of the summaries to be copied from the source contents see et al further discusses this problem and incorporates the pointer generator model with the coverage mechanism hu et al builds a large corpus of chinese social dia short text summarization chen et al introduces a distraction based neural model which forces the attention mechanism to focus on the difference parts of the source puts ma et al proposes a neural model to improve the semantic relevance between the source contents and the predicted summaries there are some work concerning with both summarization and sentiment classication hole and takalikar and mana et al propose the models to produce both the summaries and the sentiment labels however these els train the summarization part and the sentiment tion part independently and require rich hand craft features some work has improved the summarization with the help of classication cao et al proposes a model to train the summary generator and the text classier jointly but only improves the performance of the text summarization titov and mcdonald proposes a sentiment summarization method to extract the summary from the texts given the ment class lerman et al builds a new summarizer by training a ranking svm model over the set of human ence judgments and improves the performance of sentiment summarization different from all of these works our model improves both text summarization and sentiment tion and does not require any hand craft features conclusions in this work we propose a model to generate both the timent labels and the human like summaries hoping to marize the opinions from the coarse grained sentiment labels to the ne grained word sequences we evaluate our proposed model on several online reviews datasets experimental sults show that our model achieves better performance than the baseline systems on both abstractive summarization and sentiment classication acknowledgements this work was supported in part by national natural science foundation of china no national high nology research and development program of china program no and the national thousand young talents program xu sun is the corresponding author of this paper references cao et al ziqiang cao wenjie li sujian li and furu wei improving multi document summarization via text classication in aaai pages chen et al qian chen xiaodan zhu zhenhua ling si wei and hui jiang distraction based neural networks for modeling documents in ijcai new york ny july aaai cheng and lapata jianpeng cheng and mirella pata neural summarization by extracting sentences and words in acl chopra et al sumit chopra michael auli and alexander rush abstractive sentence summarization with attentive recurrent neural networks in naacl hlt pages gu et al jiatao gu zhengdong lu hang li and incorporating copying mechanism in victor li sequence to sequence learning in acl he and mcauley ruining he and julian mcauley ups and downs modeling the visual evolution of ion trends with one class collaborative ltering in www pages hole and takalikar vikrant hole and mukta likar real time tweet summarization and sentiment ysis of game tournament international journal of science and research et al baotian hu qingcai chen and fangze zhu lcsts a large scale chinese short text tion dataset in emnlp pages kim yoon kim convolutional neural networks for in emnlp pages sentence classication kingma and ba diederik kingma and jimmy ba adam a method for stochastic optimization corr lerman et al kevin sentiment goldensohn and ryan mcdonald summarization evaluating and learning user preferences in eacl pages lerman sasha lin and hovy chin yew lin and eduard hovy automatic evaluation of summaries using n gram occurrence statistics in hlt naacl al shuming ma xu sun jingjing xu houfeng wang wenjie li and qi su improving tic relevance for sequence to sequence learning of chinese social media text summarization in acl pages al shuming ma xu sun wei li sujian li wenjie li and xuancheng ren query and output ating words by querying distributed word representations for paraphrase generation in naacl mane et al vinod l mane suja s panicker and vidya b patil summarization and sentiment analysis from user health posts in pervasive computing icpc international conference on pages ieee nallapati al ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang abstractive text summarization using sequence sequence rnns and beyond in conll pages pascanu et al razvan pascanu tomas mikolov and yoshua bengio on the difculty of training recurrent ral networks in icml pages rush et al alexander rush sumit chopra and jason weston a neural attention model for abstractive sentence summarization in emnlp pages see et al abigail see peter liu and pher manning get to the point summarization with in acl pages pointer generator networks sun et al xu sun xuancheng ren shuming ma and houfeng wang meprop sparsied back propagation for accelerated deep learning with reduced overtting in icml pages sun et al xu sun bingzhen wei xuancheng ren and shuming ma label embedding network learning label representation for soft training of deep networks corr sutskever et al ilya sutskever oriol vinyals and quoc le sequence to sequence learning with neural networks in nips pages takase et al sho takase jun suzuki naoaki okazaki tsutomu hirao and masaaki nagata neural headline generation on abstract meaning representation in emnlp pages tang et al duyu tang bing qin and ting liu ument modeling with gated recurrent neural network for in emnlp pages sentiment classication titov and mcdonald ivan titov and ryan donald a joint model of text and aspect ratings for ment summarization in acl pages xu al jingjing xu xu sun xuancheng ren dp gan junyang lin binzhen wei and wei li diversity promoting generative adversarial network for corr generating informative and diversied text al jingjing xu xu sun qi zeng xiaodong zhang xuancheng ren houfeng wang and wenjie li unpaired sentiment to sentiment translation a cycled inforcement learning approach in acl
