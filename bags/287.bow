experiments lvt fre transformer model ilshat gibadullin aidar valeev abstract paper experiment large cabulary trick feature rich encoding plied transformer model text rization achieve better results analogous rnn based sequence sequence model tried models improves results deteriorates introduction lot researches additional busting rnn based sequence sequence models novelty lack transformer model model state art machine translation task showing signicant provements models performance transformer model summarization task explicitly investigated motivated try apply couple architecture independent methods transformer model large vocabulary trick feature rich encoding gave improvements models compare obtained results base transformer model results ones tained evaluated nallapati paper laid follows section gives short review related work section describes approaches applied section describes data system section reports results analysis nally section sums related work currently fundamental neural machine translation models competing state art applicable text summarization recurrent convolutional ral networks bahdanau gehring recent attention based model vaswani els consist encoder decoder parts rst encoder decoder lstm ers attention layer ond based convolutions encoder stack consists number ers composed multi head self attention layer feed forward neural network decoder stack layer ate attention layer encoder stack input fault transformer model uses byte pair encoding bpe sennrich encode text data motivated nallapati cided apply large vocabulary trick lvt jean feature rich encoding fre transformer model main idea lvt work subset ulary relevant current processing batch words batch frequent words till limit lvt allows erably lower training time vocabulary large fre encodes additional tion input word parts speech pos named entity tags ner term frequency inverse document frequency idf statistics models bpe based transformer model comparison purposes transformer default settings described vaswani byte pair encoding bpe nrich encode input sequence size sub words lary set embedding layer shared encoder decoder parts model initialized randomly baseline baseline model took transformer model based words vocabulary instead bpe size vocabulary obtained dataset embedding layer separated encoder decoder parts model tion weights layers formed randomly fre hidden fre model uses feature reach encoding technique extend words embedding vector encoder input output coder embedding layer vector concatenation following sub vectors word embedding vector pos word idf word pos vectors represented hot encoding continuous features idf converted categorical values discretizing xed number bins hot encode indicate bin number fall word embedding weights initialized randomly embedding layer decoder transformer baseline model described previous subsection vectors obtained encoder decoder embedding layers mensions equal hidden size transformer model limit dimension sub vectors dimensions encoder embedding layer dimension obtained vectors hidden size model fre linear map hidden fre difference previous described model use additional linear layer bias encoder embedding layer map vector obtained concatenation sub vectors hidden size transformer model dimension vector obtained catenation sub vectors hidden size model fre lvt model change embedding layer decoder transformer model large cabulary trick approach training batch build new batch vocabulary words texts batch required decoder vocabulary size set case lack words vocabulary obtained batch texts tend frequent words weights decoder embedding layer ous models training use modify weights words rent batch vocabulary inference use vocabulary encoder embedding layer previous model experimental setup data gigaword training els consists million article title pairs acquire annotated version annotated add named entity tags named entity nition tools tried stanfordnertagger nanertagger performed poorly corpus lower cased version deduplicate divide data parts validation set sentences training validation les monitor convergence training duc corpus testing els compare results results paper nallapati system setup transformer model vaswani base setting tensorow ofcial experiments evaluate ity summarization recall oriented derstudy gisting evaluation rouge metric lin hardware operations inside model numeric easily parallelizable nvidia gtx gpu memory speed process results firstly trained bpe based transformer model epochs epoch took hours minutes got good results rouge table secondly trained baseline model epochs took hours minutes com alesee abstractive summarization com tensorow models model bpe based baseline fre fre topiary abs ras elman words words rougel table duc test scores models scores upper scores paper pati lower figure validation scores figure rouge validation scores results got worse table thirdly fre hidden trained epochs took results got worse rouge table fourthly tried fre linear map hidden epochs epoch took sults duc got worse rouge table validation scores fre close rouge formed epoch seen ure finally trained epochs took results plummeted rouge table ably epochs embeddings train updating batch figure cross entropy loss blue imately figure based model evaluation line red higher meaning worse computed vocabulary vocabulary unique figure cross entropy loss training model batch conclusion work evaluated default bpe based transformer model transformer words cabulary baseline tried apply fre lvt approaches validation scores showed approaches improvements worsen quality baseline found default bpe based transformer model gives best result evaluated models duc dataset test set compare models models ated nallapati bpe based model outperforms topiary abs models performs worse models posed authors nallapati fre lvt approaches performs worse baseline performs worse based model answering summarization national center sciences tokyo japan june ramesh nallapati bowen zhou ccero nogueira dos santos aglar gulcehre bing xiang abstractive text summarization sequence proceedings sequence rnns signll conference computational natural language learning conll berlin germany august pages acl rico sennrich barry haddow alexandra birch neural machine translation rare words subword units proceedings annual meeting association computational guistics acl august berlin volume long papers ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez lukasz kaiser illia polosukhin attention need corr figure cross entropy loss training bpe based transformer model think fre improvements baseline quality gigaword notated worse original tated gigaword dataset lvt requires iterations converge convergence slow sure increase till baseline results application lvt transformer sense form described shorten training time epoch improve overall result try model trained word embeddings references dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly learning align translate national conference learning representations iclr san diego usa conference track proceedings jonas gehring michael auli david grangier nis yarats yann dauphin volutional sequence sequence learning corr sebastien jean kyunghyun cho roland memisevic yoshua bengio large target vocabulary neural machine translation proceedings annual meeting association computational linguistics international joint conference natural guage processing asian federation natural language processing acl july beijing china volume long papers pages chin yew lin looking good metrics automatic summarization evaluation samples proceedings fourth ntcir workshop research information cess technologies information retrieval question source document schizophrenia patients medication stop imaginary voices heads gained relief researchers repeatedly sent magnetic eld small area brains ground truth summary magnetic pulse series sent brain ease schizophrenic voices bpe based schizophrenia patients gain relief baseline study shows link schizophrenia patients fre study links schizophrenia schizophrenia fre researchers stop people schizophrenia lvt fre nasal implants pose dilemma source document china evacuating people friday land raging yangtze river ofcials preparing sacrice ooding safeguard cities downstream ground truth summary chinese military personnel conducting extensive ood control efforts yangtze bpe based china orders soldiers ght death baseline china orders soldiers ght yangtze oods fre china orders soldiers ght oods fre china orders soldiers ght oods lvt fre china mobilizes soldiers safeguard potable reservoirs source document czech republic hungary compete bids join nato european union hungarian telegraph agency reported today ground truth summary czech republic hungary vow compete nato bid bpe based czech republic hungary compete nato baseline czech hungary compete nato membership fre czech republic hungary compete nato fre hungary czech republic compete nato bids lvt fre prague czechs intend participate joining nato table examples
