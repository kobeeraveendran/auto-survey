g u a l c s c v v i x r a abstractive summarization improved by wordnet based extractive sentences niantao sujian huiling and qibin moe key laboratory of computational linguistics peking university china institute of medical information chinese academy of medical sciences moe information security lab school of software microelectronics peking university china abstract recently the abstractive summarization models have achieved good results on the cnn daily mail dataset still how to prove abstractive methods with extractive methods is a good research direction since extractive methods have their potentials of exploiting various ecient features for extracting important sentences in one text in this paper in order to improve the semantic relevance of abstractive summaries we adopt the wordnet based sentence ranking algorithm to extract the sentences which are most semantically to one text then we design a dual attentional framework to generate summaries with consideration of the extracted information at the same time we bine pointer generator and coverage mechanisms to solve the problems of out of vocabulary oov words and duplicate words which exist in the abstractive models experiments on the cnn daily mail dataset show that our models achieve competitive performance with the state of art rouge scores human evaluations also show that the summaries generated by our models have high semantic relevance to the original text keywords abstractive summarization model dual tion extractive summarization wordnet introduction for automatic summarization there are two main methods extractive and stractive extractive methods use certain scoring rules or ranking methods to select a certain number of important sentences from the source texts for ample proposed to make use of convolutional neural networks cnn to represent queries and sentences as well as adopted a greedy algorithm combined with pair wise ranking algorithm for extraction based on recurrent neural works rnn constructed a sequence classier and obtained the highest extractive scores on the cnn daily mail corpus set at the same time the stractive summarization models attempt to simulate the process of how human niantao xie et al beings write summaries and need to analyze paraphrase and reorganize the source texts it is known that there exist two main problems called oov words and duplicate words by means of abstraction proposed an improved pointer mechanism named pointer generator to solve the oov words as well as came up with a variant of coverage vector called coverage to deal with the duplicate words created the diverse cell structures to handle duplicate words problem based on query based summarization for the rst time a reinforcement learning method based neural network model was raised and obtained the state of the art scores on the cnn daily mail both extractive and abstractive methods have their merits in this paper we employ the combination of extractive and abstractive methods at the sentence level in the extractive process we nd that there are some ambiguous words in the source texts the dierent meanings of each word can be acquired through the synonym dictionary called wordnet first wordnet based lesk algorithm is utilized to analyze the word semantics then we apply the modied sentence ranking algorithm to extract a specied number of sentences according to the sentence syntactic information during the abstractive part based on model we add a new encoder which is derived from the extractive sentences and put the dual attention mechanism for decoding operations as far as we know it is the rst time that joint training of sentence level extractive and abstractive models has been conducted additionally we combine the pointer generator and coverage mechanisms to handle the oov words and duplicate words our contributions in this paper are mainly summarized as follows considering the semantics of words and sentences we improve the sentence ranking algorithm based on the wordnet based simplied lesk algorithm to obtain important sentences from the source texts we construct two parallel encoders from the extracted sentences and source texts separately and make use of dual attentional model for joint training we adopt the pointer generator and coverage mechanisms to deal with oov words and duplicate words problems our results are competitive compared with the state of the art scores our method our method is based on the attentional model which is implemented with reference to and the attention distribution t is calculated as in here we show the architecture of our model which is composed of eight parts as in figure we construct two encoders based on the source texts and extracted sentences as well as take advantage of a dual attentional decoder to generate summaries finally we combine the pointer generator and coverage mechanisms to manage oov and duplicate words problems abstractive summarization with extractive methods fig a dual attentional encoders decoder model with pointer generator network dual attentional model encoders decoder model referring to we use two single layer tional long short term memory bilstm encoders including source and tractive encoders and a single layer unidirectional lstm unilstm decoder in our model as shown in figure for encoding time i the source texts and the i and we extracted information respectively input the word embeddings ws i into h e h s two encoders meanwhile the corresponding hidden layer states i are i and generated at decoding step t the decoder will receive the word embedding from the step t which is obtained according to the previous word in the reference summary during training or provided by the decoder itself when testing next we acquire the state st and produce the vocabulary distribution p yt here we are supposed to calculate i by the following formulas h s also h e i could be obtained as follows h s h s h e h e h s h s i lst m ws i lst m ws i h s h s i h s i h e h e i lst m we i lst m we i h e h e i h i context attention based sentence ranking niantao xie et al dual attention mechanism at the tth step we need not only the previous hidden state but also the context vector cs t obtained by the corresponding attention distribution to gain state st and vocabulary distribution p yt ce cs ce firstly for source encoder we calculate the context vector cs bs are learnable parameters ws way vs ws t in the following i t vst st ws h s i bs s i t i t t cs t s i t h s t e i t ee i t ee t ce t e i t h e t secondly for extractive encoder we utilize the identical method to compute the context vector ce ve we we be are learnable parameters i t vet ee st we h e i be thirdly we get the gated context vector cg t and ce of context vectors the concatenation of cs shown as below is sigmoid function wg bg are learnable parameters by calculating the weighted sum t where the weight is the gate network obtained by t via multi layer perceptron mlp details are and ce gt t ce bg cg t gt cs t gt ce t in the same way we can obtain the hidden state st and predicte the bout are learnable bin wout win wout ity distribution p yt at time t win parameters st lst m win win bin p t sof st wout t bout abstractive summarization with extractive methods wordnet based sentence ranking algorithm to extract the important sentences we adopt a wordnet based sentence ranking algorithm is a lexical database for the english language which groups english words into sets of synonyms called synsets and provides short denitions and usage examples used the simplied lesk approach based on wordnet to extract abstracts we refer to its algorithm and set up our sentence ranking algorithm so as to construct the extractive encoder for sentence xn after ltering out the stop words and ambiguous tokens through wordnet we obtain a reserved subsequence xim since some words contain too many dierent senses which may result in too much calculation we set a window size nwin default value in descending order according to the number of senses of is and sort words as well as keep the rst nsav nsav nwin words left to get next we count the common number of senses of each word as word weight finally we get the sum weights of each sentence and acquire an average sentence weight xsnsav taking a sentence for instance we make an assumption that has two senses ma and mb has two senses mc and md while has two senses me mf currently considering as the keyword we measure the number of common words between a pair of sentences which describe the word senses of and another word table shows all possible matches of the senses of for the two senses of we can separately obtain the sum of co occurrence word pairs for each meaning for ma we obtain countma countac countad countae countaf for mb we gain countmb countbc countbd countbe countbf the signicance corresponding to the higher score countma or countmb is assigned to the the keyword table the number of common words between a pair of sentences pair of sentences ma and mc ma and md mb and mc mb and md ma and me ma and mf mb and me mb and common words in sense description countac countad countbc countbd countae countaf countbe countbf niantao xie et al in this way we re capable of acquiring the average weight of sentence weightavg countxi nsav let s assume that document d xn which contains a total of n sentences we sort them in descending order according to the average weights of sentences and then extract the top ntop sentences default value is pointer generator and coverage mechanisms pointer generator network pointer generator is an eective method to solve the problem of oov words and its structure has been expanded in figure we borrow the method improved by pgen is dened as a switch to decide to generate a word from the vocabulary or copy a word from the source encoder attention distribution we maintain an extended vocabulary including the cabulary and all words in the source texts for the decoding step t and decoder input xt we dene pgen as pgen t wp st wp pvocab p t xt bp p wt pgen s i t i wi where wt is the value of xt and wp wp wp bp are learnable parameters coverage mechanism duplicate words are a critical problem in the model and even more serious when generating long texts like multi sentence texts made some minor modications to the coverage model which is also displayed in figure first we calculate the sum of attention distributions from previous decoder steps t to get a coverage vector covt covs t s then we make use of coverage vector covt to update the attention tion i t vst finally we dene the coverage loss function covlosst for the sake of penalizing st ws i t bs i ws covs the duplicate words appearing at decoding time t and renew the total loss h s covlosst i t covs i t i losst w where w t covlosst t is the target word at tth step w is the primary loss for timestep t during training hyperparameter default value is is the weight for covlosst ws bs are learnable parameters ws ws abstractive summarization with extractive methods experiments dataset cnn daily mail is widely used in the public automatic summarization evaluation which contains online news articles tokens on average paired with multi sentence summaries tokens on average provided the data processing script and we take advantage of it to obtain the non anonymized sion of the the data including training pairs validation pairs and test pairs though used the anonymized version during training steps we nd that of articles are empty so we utilize the remaining pairs for training then we perform the splitting preprocessing for the data pairs with the help of stanford corenlp and convert them into binary les as well as get the vocab le for the convenience of reading data implementation model parameters conguration the corresponding parameters of trolled experimental models are described as follows for all models we have set the word embeddings and rnn hidden states to be dimensional and dimensional respectively for source encoders extractive encoders and decoders contrary to we learn the word embeddings from scratch during training because our training dataset is large enough we apply the optimization nique adagrad with learning rate and an initial accumulator value of as well as employ the gradient clipping with a maximum gradient norm of for the one encoder models we set up the vocabulary size to be for source encoder and target decoder simultaneously we try to adjust the vocabulary size to be then discover that when the model is trained to converge the time cost is doubled but the test dataset scores have slightly dropped in our analysis the models parameters have increased excessively when the vocabulary enlarges leading to overtting during the training process meanwhile for the models with two encoders we adjust the vocabulary size to be each pair of the dataset consists of an article and a multi sentence summary we truncate the article to tokens and limit the summary to tokens for both training and testing time during decoding mode we generate at least words with beam search algorithm data truncation operations not only reduce memory consumption speed up training and testing but also improve the imental results the reason is that the vital information of news texts is mainly concentrated in the rst half part we train on a single geforce gtx gpu with a memory of mib and the batch size is set to be as well as the beam size is for beam search in decoding mode for the dual attentional models without generator we trained them for about two days models with pointer generator niantao xie et al expedite the training the time cost is reduced to about one day when we add coverage the coverage loss weight is set to and the model needs about one hour for training in order to gure out how each part of our models controlled experiments contributes to the test results based on the released of tensorow we have implemented all the models and done a series of experiments the baseline model is a general attentional model the encoder sists of a bilstm and the decoder is made up of an unilstm the second baseline model is our encoders decoder dual attention model which contains two bilstm encoders and one unilstm decoder this model combines the tive and generative methods to perform joint training eectively through a dual attention mechanism for the above two basic models in order to explain how the oov and cate words are treated we lead into the pointer generator and coverage anism step by step for the second baseline the two tricks are only related to the source encoder because we think that the source encoder already covers all the tokens in the extractive encoder for the extractive encoder we adopt two methods for extraction one is the leading three sentences technique which is simple but indeed a strong baseline the other is the modied sentence ranking algorithm based on wordnet that we explain in details in section it considers semantic relations in words and sentences from source texts results rouge is a set of metrics with a software package used for evaluating matic summarization and machine translation results it counts the number of overlapping basic units including n grams longest common subsequences lcs we use a python wrapper to gain and l scores and list the scores in table we carry out the experiments based on original dataset non anonymized version of data for the top three models in table their rouge scores are slightly higher than those executed by except for the rouge l score of attn pgn which is points lower than the former result for the fourth model we did not reproduce the results of and rouge l decreased by an average of points for the four models in the middle we apply the dual attention mechanism to integrate extraction with abstraction for joint training and decoding these model variants own a single pgn or pgn together with cov achieve better results than the corresponding vulgaris attentional models simultaneously we conclude that the extractive encoders play a role among which we obtained higher and scores based on the dual attn abstractive summarization with extractive methods table rouge scores on cnn daily mail non anonymized testing dataset for all the controlled experiment models mentioned above according to the ocial rouge usage description all our rouge scores have a condence interval of at most pgn cov ml rl are abbreviations for pointer generator coverage objective learning and reinforcement learning models with subscript a were trained and tested on the anonymized cnn daily mail dataset as well as with are the state of the art extractive and abstractive summarization models on the anonymized dataset by now models attn attn attn pgn attn pgn cov dual attn pgn wordnet dual attn pgn dual attn pgn cov wordnet dual attn pgn cov summarunner a rl intra attn a ml rl intra attn rouge scores l pgn cov model and achieve a better rouge l score on wordnet attn pgn cov model let s take a look at the ve models at the bottom two of which give the of the art scores for the extractive and generative methods our scores are already comparable to them it is worthy to mention that based on the dual attention our models related to both and wordnet with pgn and cov have exceeded the previous best scores when in fact previous summarunner rl related models are based on anonymized dataset these dierences may cause some deviations in the comparison of experimental results we give some generated summaries of dierent models for one selected test article from figure we can see that the red words represent key information about who what where and when we can match the corresponding keywords in the remaining seven summaries to nd out whether they cover all the signicant points and check if they are expressed in a concise and coherent way it can be discovered from figure that most of the models have lost several vital points and the model dual attn pgn has undergone fairly serious repetition our model wordnet dual attn pgn cov holds the main key information as well as has better readability and semantic correctness reliably niantao xie et al fig summaries for all the models of one test article example related work up to now automatic summarization with extractive and abstractive methods are under fervent research on the one hand the extractive techniques extract the topic related keywords and signicant sentences from the source texts to stitute summaries proposed a model with a hierarchical encoder and attentional decoder to solve extractive summarization tasks at the word and tence levels currently put forward summarunner a rnn based sequence model for extractive summarization and it achieves the previous state of art performance on the other hand abstractive methods establish an intrinsic semantic representation and use natural language generation techniques to duce summaries which are closer to what human beings express applied the combination of model and attention mechanism to machine translation reference six people including one civilian are killed when a car bombexplodes near a police station others are killed when their armored vehicle is attacked on a highway in northern maqdis an isisaffiliate claims responsibility attn an attack on a police station a guard is killed in a car bomb attack comes hours after another operation that the group claims responsibility attacks come as the military announced a departure of several senior military positions pgn six people were killed when a car bomb exploded near the police station in al arish capital of north first lieutenant a sergeant and four conscripts were killed when their armored vehicle was attacked on the highway from al arishto sheikh zuweidin northern maqdishas claimed many attacks against the army and police in sinai cov six people were killed when a car bomb exploded near the police station in al arish capital of north maqdis a sergeant and four conscripts were killed when their armored vehicle was attacked on the highway from al arishto sheikh zuweidin northern third attack sundayon a checkpoint in rafahleft three security personnel injured after unknown assailants opened fire dual pgn six people killed when a car bomb exploded near the police station in al arish capital of north first lieutenant a sergeant and four conscripts were killed when their armored vehicle was attacked on the highway third attack sundayon a checkpoint in rafahleft three security personnel injured dual pgn six people including one civilian were killed when a car bomb exploded near the police station in al arish capital of north people including one civilian were killed when a car bomb exploded near the police station in al arish capital of north sinai authorities say dual cov six people including one civilian were killed when a car bomb exploded near the police station first lieutenant a sergeant and four conscripts were killed when their armored vehicle was attacked on the highway maqdishas claimed many attacks against the army and police in sinai dual cov six people including one civilian were killed when a car bomb exploded near the police station maqdis an isisaffiliate claimed responsibility for the attack maqdishas claimed many attacks against the army and police abstractive summarization with extractive methods tasks for the rst time exploited model to sentence compression to lay the groundwork for subsequent summarization with dierent granularities used encoder decoder with attention method to generate news headlines added a selective gate network to the basic model in order to control which part of the information owed from encoder to decoder raised a model based on graph and attention mechanism to strengthen the positioning of vital tion of source texts so as to solve rare and unseen words proposed the copynet model and pointing mechanism created read again and copy mechanisms made a combination of the basic model with large vocabulary trick lvt feature rich encoder pointer generator and hierarchical attention in addition to pointer generator other tricks of this paper also contributed to the experiment results presented an updated version of pointer generator which proved to be better as for duplicate words for sake of solving problems of over or missing translation came up with a coverage mechanism to avail oneself of historical information for attention calculation while provided a progressive version introduced a series of diverse cell structures to solve the duplicate words so far few papers have considered about the structural or sementic issues at the language level in the eld of summarization presented a novel vised method that made use of a pruned dependency tree to acquire the sentence compression based on a chinese short text summary dataset lcsts and the attentional model proposed to enhance the semantic relevance by calculating the cos similarities of summaries and source texts conclusion in our paper we construct a dual attentional model comprising source and extractive encoders to generate summaries in addition we put forward the modied sentence ranking algorithm to extract a specic number of high weighted sentences for the purpose of strengthening the semantic representation of the extractive encoder furthermore we introduce the pointer generator and coverage mechanisms in our models so as to solve the problems of oov and duplicate words in the non anonymized cnn daily mail dataset our results are close to the state of the art rouge scores moreover we get the highest abstractive scores as well as obtain such summaries that have better readability and higher semantic accuracies in our future work we plan to unify the reinforcement learning method with our abstractive models acknowledgments we thank the anonymous reviewers for their insightful comments on this paper this work was partially supported by national natural science foundation of china and the correspondence author is sujian li niantao xie et al references bahdanau cho bengio neural machine translation by jointly learning to align and translate arxiv preprint cao li li wei li attsum joint learning of focusing and summarization with neural attention arxiv preprint cheng lapata neural summarization by extracting sentences and words arxiv preprint filippova strube dependency tree based sentence compression in ceedings of the fifth international natural language generation conference pp association for computational linguistics gu lu li li incorporating copying mechanism in sequence sequence learning arxiv preprint gulcehre ahn nallapati zhou bengio pointing the unknown words arxiv preprint lin rouge a package for automatic evaluation of summaries text lopyrev generating news headlines with recurrent neural networks arxiv rization branches out preprint ma sun xu wang li su improving semantic relevance for sequence to sequence learning of chinese social media text summarization arxiv preprint nallapati zhai zhou summarunner a recurrent neural network based sequence model for extractive summarization of documents in aaai pp nallapati zhou gulcehre xiang et al abstractive text tion using sequence to sequence rnns and beyond arxiv preprint nema khapra laha ravindran diversity driven attention model for query based abstractive summarization arxiv preprint pal saha an approach to automatic text summarization using wordnet in advance computing conference iacc ieee international pp ieee paulus xiong socher a deep reinforced model for abstractive rization arxiv preprint rush chopra weston a neural attention model for abstractive sentence summarization arxiv preprint see liu manning get to the point summarization with generator networks arxiv preprint tan wan xiao abstractive document summarization with a based attentional neural model in proceedings of the annual meeting of the association for computational linguistics volume long papers vol pp tu lu liu liu li modeling coverage for neural machine translation arxiv preprint zeng luo fidler urtasun ecient summarization with read again and copy mechanism arxiv preprint zhou yang wei zhou selective encoding for abstractive sentence summarization arxiv preprint
