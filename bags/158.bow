g u l c s c v v x r abstractive summarization improved wordnet based extractive sentences niantao sujian huiling qibin moe key laboratory computational linguistics peking university china institute medical information chinese academy medical sciences moe information security lab school software microelectronics peking university china xieniantao edu cn ac cn pku edu abstract recently abstractive summarization models achieved good results cnn daily mail dataset prove abstractive methods extractive methods good research direction extractive methods potentials exploiting ecient features extracting important sentences text paper order improve semantic relevance abstractive summaries adopt wordnet based sentence ranking algorithm extract sentences semantically text design dual attentional framework generate summaries consideration extracted information time bine pointer generator coverage mechanisms solve problems vocabulary oov words duplicate words exist abstractive models experiments cnn daily mail dataset models achieve competitive performance state art rouge scores human evaluations summaries generated models high semantic relevance original text keywords abstractive summarization model dual tion extractive summarization wordnet introduction automatic summarization main methods extractive stractive extractive methods use certain scoring rules ranking methods select certain number important sentences source texts ample proposed use convolutional neural networks cnn represent queries sentences adopted greedy algorithm combined pair wise ranking algorithm extraction based recurrent neural works rnn constructed sequence classier obtained highest extractive scores cnn daily mail corpus set time stractive summarization models attempt simulate process human niantao xie et al beings write summaries need analyze paraphrase reorganize source texts known exist main problems called oov words duplicate words means abstraction proposed improved pointer mechanism named pointer generator solve oov words came variant coverage vector called coverage deal duplicate words created diverse cell structures handle duplicate words problem based query based summarization rst time reinforcement learning method based neural network model raised obtained state art scores cnn daily mail extractive abstractive methods merits paper employ combination extractive abstractive methods sentence level extractive process nd ambiguous words source texts dierent meanings word acquired synonym dictionary called wordnet wordnet based lesk algorithm utilized analyze word semantics apply modied sentence ranking algorithm extract specied number sentences according sentence syntactic information abstractive based model add new encoder derived extractive sentences dual attention mechanism decoding operations far know rst time joint training sentence level extractive abstractive models conducted additionally combine pointer generator coverage mechanisms handle oov words duplicate words contributions paper mainly summarized follows considering semantics words sentences improve sentence ranking algorithm based wordnet based simplied lesk algorithm obtain important sentences source texts construct parallel encoders extracted sentences source texts separately use dual attentional model joint training adopt pointer generator coverage mechanisms deal oov words duplicate words problems results competitive compared state art scores method method based attentional model implemented reference attention distribution t calculated architecture model composed parts figure construct encoders based source texts extracted sentences advantage dual attentional decoder generate summaries finally combine pointer generator coverage mechanisms manage oov duplicate words problems abstractive summarization extractive methods fig dual attentional encoders decoder model pointer generator network dual attentional model encoders decoder model referring use single layer tional long short term memory bilstm encoders including source tractive encoders single layer unidirectional lstm unilstm decoder model shown figure encoding time source texts extracted information respectively input word embeddings ws h e h s encoders corresponding hidden layer states generated decoding step t decoder receive word embedding step t obtained according previous word reference summary training provided decoder testing acquire state st produce vocabulary distribution p yt supposed calculate following formulas h s h e obtained follows h s h s h e h e h s h s lst m ws lst m ws h s h s h s h e h e lst m lst m h e h e h context attention based sentence ranking niantao xie et al dual attention mechanism tth step need previous hidden state context vector cs t obtained corresponding attention distribution gain state st vocabulary distribution p yt ce cs ce firstly source encoder calculate context vector cs bs learnable parameters ws way vs ws t following t vst st ws h s bs s t t t cs t s t h s t e t ee t ee t ce t e t h e t secondly extractive encoder utilize identical method compute context vector ce ve learnable parameters t vet ee st h e thirdly gated context vector cg t ce context vectors concatenation cs shown sigmoid function wg bg learnable parameters calculating weighted sum t weight gate network obtained t multi layer perceptron mlp details ce gt t ce bg cg t gt cs t gt ce t way obtain hidden state st predicte bout learnable bin wout win wout ity distribution p yt time t win parameters st lst m win win bin p t sof st wout t bout abstractive summarization extractive methods wordnet based sentence ranking algorithm extract important sentences adopt wordnet based sentence ranking algorithm lexical database english language groups english words sets synonyms called synsets provides short denitions usage examples simplied lesk approach based wordnet extract abstracts refer algorithm set sentence ranking algorithm construct extractive encoder sentence xn ltering stop words ambiguous tokens wordnet obtain reserved subsequence xim words contain dierent senses result calculation set window size nwin default value descending order according number senses sort words rst nsav nsav nwin words left count common number senses word word weight finally sum weights sentence acquire average sentence weight xsnsav taking sentence instance assumption senses ma mb senses mc md senses currently considering keyword measure number common words pair sentences describe word senses word table shows possible matches senses senses separately obtain sum co occurrence word pairs meaning ma obtain countma countac countad countae countaf mb gain countmb countbc countbd countbe countbf signicance corresponding higher score countma countmb assigned keyword table number common words pair sentences pair sentences ma mc ma md mb mc mb md ma ma mf mb mb common words sense description countac countad countbc countbd countae countaf countbe countbf nltk org howto wordnet html niantao xie et al way capable acquiring average weight sentence weightavg countxi nsav let s assume document d xn contains total n sentences sort descending order according average weights sentences extract ntop sentences default value pointer generator coverage mechanisms pointer generator network pointer generator eective method solve problem oov words structure expanded figure borrow method improved pgen dened switch decide generate word vocabulary copy word source encoder attention distribution maintain extended vocabulary including cabulary words source texts decoding step t decoder input xt dene pgen pgen t wp st wp pvocab p t xt bp p wt pgen s t wi wt value xt wp wp wp bp learnable parameters coverage mechanism duplicate words critical problem model generating long texts like multi sentence texts minor modications coverage model displayed figure calculate sum attention distributions previous decoder steps t coverage vector covt covs t s use coverage vector covt update attention tion t vst finally dene coverage loss function covlosst sake penalizing st ws t bs ws covs duplicate words appearing decoding time t renew total loss h s covlosst t covs t losst w w t covlosst t target word tth step w primary loss timestep t training hyperparameter default value weight covlosst ws bs learnable parameters ws ws abstractive summarization extractive methods experiments dataset cnn daily mail widely public automatic summarization evaluation contains online news articles tokens average paired multi sentence summaries tokens average provided data processing script advantage obtain non anonymized sion data including training pairs validation pairs test pairs anonymized version training steps nd articles utilize remaining pairs training perform splitting preprocessing data pairs help stanford corenlp convert binary les vocab le convenience reading data implementation model parameters conguration corresponding parameters trolled experimental models described follows models set word embeddings rnn hidden states dimensional dimensional respectively source encoders extractive encoders decoders contrary learn word embeddings scratch training training dataset large apply optimization nique adagrad learning rate initial accumulator value employ gradient clipping maximum gradient norm encoder models set vocabulary size source encoder target decoder simultaneously try adjust vocabulary size discover model trained converge time cost doubled test dataset scores slightly dropped analysis models parameters increased excessively vocabulary enlarges leading overtting training process models encoders adjust vocabulary size pair dataset consists article multi sentence summary truncate article tokens limit summary tokens training testing time decoding mode generate words beam search algorithm data truncation operations reduce memory consumption speed training testing improve imental results reason vital information news texts mainly concentrated rst half train single geforce gtx gpu memory mib batch size set beam size beam search decoding mode dual attentional models generator trained days models pointer generator nyu github io niantao xie et al expedite training time cost reduced day add coverage coverage loss weight set model needs hour training order gure models controlled experiments contributes test results based released tensorow implemented models series experiments baseline model general attentional model encoder sists bilstm decoder unilstm second baseline model encoders decoder dual attention model contains bilstm encoders unilstm decoder model combines tive generative methods perform joint training eectively dual attention mechanism basic models order explain oov cate words treated lead pointer generator coverage anism step step second baseline tricks related source encoder think source encoder covers tokens extractive encoder extractive encoder adopt methods extraction leading sentences technique simple strong baseline modied sentence ranking algorithm based wordnet explain details section considers semantic relations words sentences source texts results rouge set metrics software package evaluating matic summarization machine translation results counts number overlapping basic units including n grams longest common subsequences lcs use python wrapper gain l scores list scores table carry experiments based original dataset e non anonymized version data models table rouge scores slightly higher executed rouge l score attn pgn points lower result fourth model reproduce results rouge l decreased average points models middle apply dual attention mechanism integrate extraction abstraction joint training decoding model variants single pgn pgn cov achieve better results corresponding vulgaris attentional models simultaneously conclude extractive encoders play role obtained higher scores based dual attn com tensorflow models tree master research textsum org project abstractive summarization extractive methods table rouge scores cnn daily mail non anonymized testing dataset controlled experiment models mentioned according ocial rouge usage description rouge scores condence interval pgn cov ml rl abbreviations pointer generator coverage objective learning reinforcement learning models subscript trained tested anonymized cnn daily mail dataset state art extractive abstractive summarization models anonymized dataset models attn attn attn pgn attn pgn cov dual attn pgn wordnet dual attn pgn dual attn pgn cov wordnet dual attn pgn cov summarunner rl intra attn ml rl intra attn rouge scores l pgn cov model achieve better rouge l score wordnet attn pgn cov model let s look ve models art scores extractive generative methods scores comparable worthy mention based dual attention models related wordnet pgn cov exceeded previous best scores fact previous summarunner rl related models based anonymized dataset dierences cause deviations comparison experimental results generated summaries dierent models selected test article figure red words represent key information match corresponding keywords remaining seven summaries nd cover signicant points check expressed concise coherent way discovered figure models lost vital points model dual attn pgn undergone fairly repetition model wordnet dual attn pgn cov holds main key information better readability semantic correctness reliably niantao xie et al fig summaries models test article example related work automatic summarization extractive abstractive methods fervent research hand extractive techniques extract topic related keywords signicant sentences source texts stitute summaries proposed model hierarchical encoder attentional decoder solve extractive summarization tasks word tence levels currently forward summarunner rnn based sequence model extractive summarization achieves previous state art performance hand abstractive methods establish intrinsic semantic representation use natural language generation techniques duce summaries closer human beings express applied combination model attention mechanism machine translation reference people including civilian killed car bombexplodes near police station killed armored vehicle attacked highway northern sinai ansarbeital maqdis isisaffiliate claims responsibility attn attack police station guard killed car bomb attack comes hours operation group claims responsibility attacks come military announced departure senior military positions pgn people killed car bomb exploded near police station al arish capital north sinai lieutenant sergeant conscripts killed armored vehicle attacked highway al arishto sheikh zuweidin northern sinai ansarbeital maqdishas claimed attacks army police sinai cov people killed car bomb exploded near police station al arish capital north sinai ansarbeital maqdis sergeant conscripts killed armored vehicle attacked highway al arishto sheikh zuweidin northern sinai attack sundayon checkpoint rafahleft security personnel injured unknown assailants opened fire wordnet dual pgn people killed car bomb exploded near police station al arish capital north sinai lieutenant sergeant conscripts killed armored vehicle attacked highway attack sundayon checkpoint rafahleft security personnel injured dual pgn people including civilian killed car bomb exploded near police station al arish capital north sinai people including civilian killed car bomb exploded near police station al arish capital north sinai authorities wordnet dual cov people including civilian killed car bomb exploded near police station lieutenant sergeant conscripts killed armored vehicle attacked highway ansarbeital maqdishas claimed attacks army police sinai dual cov people including civilian killed car bomb exploded near police station ansarbeital maqdis isisaffiliate claimed responsibility attack ansarbeital maqdishas claimed attacks army police abstractive summarization extractive methods tasks rst time exploited model sentence compression lay groundwork subsequent summarization dierent granularities encoder decoder attention method generate news headlines added selective gate network basic model order control information owed encoder decoder raised model based graph attention mechanism strengthen positioning vital tion source texts solve rare unseen words proposed copynet model pointing mechanism created read copy mechanisms combination basic model large vocabulary trick lvt feature rich encoder pointer generator hierarchical attention addition pointer generator tricks paper contributed experiment results presented updated version pointer generator proved better duplicate words sake solving problems missing translation came coverage mechanism avail oneself historical information attention calculation provided progressive version introduced series diverse cell structures solve duplicate words far papers considered structural sementic issues language level eld summarization presented novel vised method use pruned dependency tree acquire sentence compression based chinese short text summary dataset lcsts attentional model proposed enhance semantic relevance calculating cos similarities summaries source texts conclusion paper construct dual attentional model comprising source extractive encoders generate summaries addition forward modied sentence ranking algorithm extract specic number high weighted sentences purpose strengthening semantic representation extractive encoder furthermore introduce pointer generator coverage mechanisms models solve problems oov duplicate words non anonymized cnn daily mail dataset results close state art rouge scores highest abstractive scores obtain summaries better readability higher semantic accuracies future work plan unify reinforcement learning method abstractive models acknowledgments thank anonymous reviewers insightful comments paper work partially supported national natural science foundation china correspondence author sujian li niantao xie et al references bahdanau d cho k bengio y neural machine translation jointly learning align translate arxiv preprint cao z li w li s wei f li y attsum joint learning focusing summarization neural attention arxiv preprint cheng j lapata m neural summarization extracting sentences words arxiv preprint filippova k strube m dependency tree based sentence compression ceedings fifth international natural language generation conference pp association computational linguistics gu j lu z li h li v o incorporating copying mechanism sequence sequence learning arxiv preprint gulcehre c ahn s nallapati r zhou b bengio y pointing unknown words arxiv preprint lin c y rouge package automatic evaluation summaries text lopyrev k generating news headlines recurrent neural networks arxiv rization branches preprint ma s sun x xu j wang h li w su q improving semantic relevance sequence sequence learning chinese social media text summarization arxiv preprint nallapati r zhai f zhou b summarunner recurrent neural network based sequence model extractive summarization documents aaai pp nallapati r zhou b gulcehre c xiang b al abstractive text tion sequence sequence rnns arxiv preprint nema p khapra m laha ravindran b diversity driven attention model query based abstractive summarization arxiv preprint pal r saha d approach automatic text summarization wordnet advance computing conference iacc ieee international pp ieee paulus r xiong c socher r deep reinforced model abstractive rization arxiv preprint rush m chopra s weston j neural attention model abstractive sentence summarization arxiv preprint liu p j manning c d point summarization generator networks arxiv preprint tan j wan x xiao j abstractive document summarization based attentional neural model proceedings annual meeting association computational linguistics volume long papers vol pp tu z lu z liu y liu x li h modeling coverage neural machine translation arxiv preprint zeng w luo w fidler s urtasun r ecient summarization read copy mechanism arxiv preprint zhou q yang n wei f zhou m selective encoding abstractive sentence summarization arxiv preprint
