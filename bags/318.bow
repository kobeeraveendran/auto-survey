g u l c s c v v x r baseline analysis podcast abstractive summarization chujie zheng edu university delaware usa kunpeng zhang edu university maryland usa harry jiannan wang edu university delaware usa ling fan edu cn tongji university china abstract podcast summary important factor aecting end users ing decisions considered critical feature cast recommendation systems downstream cations existing abstractive summarization approaches mainly built ne tuned models professionally edited texts cnn dailymail news dierent news podcasts longer colloquial conversational noisier contents commercials sponsorship makes automatic podcast summarization extremely challenging paper presents baseline analysis podcast summarization spotify podcast dataset provided trec aims help researchers understand current state art pre trained models build foundation creating better models introduction podcast industry dramatically growing gaining massive market appeal example spotify spent approximately million acquisition gimlet media discovery understanding podcast content gressive compared types media music movie news calls computationally eective methods podcast analysis including automatic summarization rapid development natural language processing especially success attention mechanism transformer chitecture text summarization task received ing attention models proposed achieve good performance especially news summarization eld trained tested known cnn dailymail cnn dm dataset headlines served ground truth summaries short paper dataset study recently released trec spotify podcasts dataset consists podcast episodes audio les transcripts generated google asr episode summaries information dierent news podcasts unique characteristics lengthy multi modal colloquial conversational nosier contents commercials sponsorship makes podcast study aim summarization task challenging share preliminary results data preprocessing line analysis expected empirically tioned data specialty build foundation subsequent cast analyses code pre trained models released trec competition data preprocessing spotify podcast dataset podcast episodes shows produced creators average duration gle episode minutes longest hours shortest seconds trec podcast track nizers form brass set cutting dataset podcast episodes following rules remove episodes descriptions long acters short characters remove duplicate episodes similar descriptions ducting similarity analysis remove episodes descriptions similar responding descriptions means episode description reect episode content brass set impose extra constraints form cleaner dataset follows remove episodes emoji dominated descriptions e scriptions characters removing jis remove episodes longer minutes control length episode descriptions constraint easily altered relaxed necessary remove episodes profanity language episode descriptions remove episodes non english descriptions remove episodes sponsorship advertisement dominated descriptions preprocessing dataset episodes left serves dataset analyses study table details baseline models abstractive summarization task aims automatically generate podcast episode summaries based episode transcripts ground truth summary written podcast creators performance summarization models measured ing rouge score particularly scores rouge l report recall r precision p design simple heuristic baselines model isons baseline select rst k tokens transcript summary com podcast summarization baseline org project dataset preprocessing trec spotify podcasts dataset ltering trec organizer brass set removing episodes emoji dominated descriptions removing episodes longer minutes removing episodes profanity language removing episodes non english descriptions removing episodes sponsorship advertisement dominated descriptions table data preprocessing number episodes episodes baseline select k tokens transcript summary idea baselines beginning end podcast contain important content information performance shown table k varied tween choose maximum value k bert transformer based models discuss section truncate input kens results exhibit obvious pattern longer summary tends capture words measured phrases measured rouge l true summary leads higher recall lower precision key takeaways choosing yields best bined score means tokens words long capture major summarization information ble distribution true summaries average summary length maximal length line highest scores means starting podcasts contains useful related information podcast summaries ending consistent observation podcast episodes overview beginning tell listeners expect sota model experiments section conduct number experiments cast summarization task current state art sota summarization models including bart net specically use pre trained models tune news datasets cnn dailymail datasets preprocessed podcast dataset section goal overview idea performance sota models builds foundation better model innovation experiments conducted machine tesla gpus split processed podcast dataset training validation testing sets random resulting observations training set observations idation testing sets based baseline analysis vious section choose beginning episode scripts input use default settings use tokens bart tokens prophetnet episode paper use distilbart provided hugging face achieves better formance original bart model experiment description creators summarization ground truth ble shows experiment results lowing observations performance sota models comparable baseline models indicates plenty headroom improvements calls research emerging area scores rouge l prophetnet cnn dm dataset ing best scores table podcast dataset huge performance gap implies cast summarization task challenging news headline summarization task podcast s unique teristics aforementioned fine tuning pre trained models cnn dm dataset podcast summarization result lower performance pared vanilla pre trained models e bart net urges think lexicon dierences tween podcast dataset existing datasets marization tasks cnn dm gigaword bigpatent provide sample generated podcast summaries dierent models repository based baseline analysis paper discuss ber directions future research summarization based long narrative structure discussed simple position heuristics sucient long narratives podcast transcripts tion dene narrative structure better podcast summarization interesting worthy topic conversation summarization podcasts conversational colloquial multi people leverage existing search help podcast summarization largely missing multi modal podcast analysis audio les podcasts tain richer information text transcripts music emotion pitch believe multi modal analysis critical podcast understanding play important role podcast summarization ommendation long document transformer leverage recent research potentially use podcast scripts training model baseline baseline p f r p r f rouge l p f r table model performance baseline models model baseline baseline cnn dm podcast cnn dm podcast prophetnet prophetnet cnn dm prophetnet podcast p f r p r f rouge l p f r distilbart use hugging face transformers model sshleifer distilbart use hugging face transformers model small prophetnet use released prophetnet gb checkpoint fine tuned cnn dm dataset fine tuned podcast dataset table performance comparison dierent models conclusion paper present performance podcast tion baselines sota models spotify podcast dataset discuss directions future research eld hope pioneering baseline analysis tion help researchers needed innovation exciting emerging research area references tadas baltrusaitis chaitanya ahuja louis philippe morency modal machine learning survey taxonomy ieee transactions pattern analysis machine intelligence iz beltagy matthew e peters arman cohan longformer document transformer arxiv preprint ann clifton aasish pappu sravana reddy yongze yu jussi karlgren ben carterette rosie jones spotify podcasts dataset arxiv preprint arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang nazli goharian discourse aware attention model abstractive summarization long documents arxiv preprint jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language understanding arxiv preprint prakhar ganesh saket dingliwal abstractive summarization ken written conversation arxiv preprint nikita kitaev lukasz kaiser anselm levskaya reformer cient transformer arxiv preprint mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy ves stoyanov luke zettlemoyer bart ing sequence sequence pre training natural language generation lation comprehension arxiv preprint chin yew lin rouge package automatic evaluation summaries text summarization branches ramesh nallapati bowen zhou caglar gulcehre bing xiang al stractive text summarization sequence sequence rnns arxiv preprint pinelopi papalampidi frank keller lea frermann mirella lapata screenplay summarization latent narrative structure arxiv preprint colin rael noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li peter j liu exploring transfer learning unied text text transformer arxiv preprint alexander m rush sumit chopra jason weston neural attention model abstractive sentence summarization arxiv preprint eva sharma chen li lu wang bigpatent large scale dataset abstractive coherent summarization arxiv preprint arpit sood thanvir p mohamed vasudeva varma topic focused summarization chat conversations european conference information retrieval springer ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser illia polosukhin attention need advances neural information processing systems yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang ming zhou prophetnet predicting future n gram sequence sequence pre training arxiv preprint xiaodan zhu gerald penn summarization spontaneous tions ninth international conference spoken language processing
