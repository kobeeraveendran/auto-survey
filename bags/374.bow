shot text generation pattern exploiting training timo schick hinrich schtze center information language processing lmu munich germany lmu c e d l c s c v v x r abstract patterns generated texts providing pretrained language models simple task descriptions prompts ral language yields impressive shot results wide range text classication tasks combined gradient based learning examples paper underlying idea applied text generation tasks adapt pattern exploiting training pet recently proposed shot approach netuning generative language models text generation tasks text summarization headline generation datasets proposed variant pet gives consistent improvements strong line shot settings introduction pretraining large neural networks language modeling objective led signicant ments nlp peters et al howard ruder radford et al devlin et al improvements possible choosing different pretraining objective closely matches downstream task est examples include casing prediction named entity recognition mayhew et al gap tence generation summarization zhang et al sentence unshufing discourse resentations lee et al approaches reduce training data required perform handful examples available downstream task common word uses nlp shot settings signicant gains possible proceeding way instead making ing similar downstream task reformulate task similar pretraining objective masked language implementation publicly available github com timoschick pet short summary contact questions dear john having trouble internet banking account internet banking accounts setup accessing e mail title internet banking password reset figure texts generated pegasus large ferent patterns input dear john internet banking accounts setup accessing login d main account password reset digits ssn models e devlin et al lewis et al reformulation technique convert puts cloze questions adding text snippet contains form task description form short prompt radford et al schick schtze making pretraining netuning similar approach compelling benet enabling users explain task pretrained model making easier model understand task examples figure demonstrate pretrained language models use text snippets adapt output generation task idea works unsupervised setting radford et al examples simply provided additional context brown et al unfolds potential bined gradient based training handful labeled examples schick schtze particular pattern exploiting training pet approach proposed schick schtze combines task descriptions learning examples performs strongly shot text classication datasets applied classication tasks applicable problems require tion text sequences paper adapt pet train generative models text generation tasks particular propose modications enable tune pretrained pegasus model zhang et al pet evaluate approach diverse set english headline generation text summarization tasks zero shot shot settings pegasus trained adapted version pet clearly outperforms regular netuning summary contributions follows describe pet modied netuning generative language models quence generation tasks training pegasus pet outperforms regular netuning large set tasks training set sizes analyze factors contributing pet s strong performance related work masked language modeling proposed training objective devlin et al variants objective proposed involve generating sequences text including raffel et al bart lewis et al pegasus zhang et al approach based idea rephrase tasks cloze questions commonly probe knowledge contained masked language models e petroni et al wang et al schick schtze ettinger schick schtze propose pet combines idea gradient based learning shot text cation jiang et al schick et al consider problem nding best way rephrase given task cloze question closely related work schick schtze propose modication pet enables eration multiple tokens approach requires text classication objective scale long output sequences approaches shot learning nlp commonly require large sets examples lated tasks gu et al dou et al qian yu parallel data consistency ing et al chen et al highly specialized methods tailored specic task laban et al contrast pet requires additional labeled data provides intuitive terface leverage task specic human knowledge finally proposed variant pet closely related concurrent work et al use prompts keywords controllable text eration high resource settings prex constrained decoding knowles koehn wuebker et al keskar et al pegasus pretraining pegasus zhang et al standard encoder decoder architecture vaswani et al pretrained gap sentences generation objective tailored text rization tasks pretraining objective requires set documents consisting multiple sentences key idea preprocess document picking subset sentences important document replacing tences mask token concatenating removed sentences pseudo summary transformer model trained generate pseudo summary given partially masked ument similar prior work e raffel et al lewis et al ing entire masked document encoder generating output autoregressive ion decoder picking informative sentences informativeness sentence measured score lin tween sentence remaining document m sentences according measure selected zhang et al train variants gasus pegasus base layer model approximately m parameters large layer model m parameters version publicly available experiments based pegasus large pattern exploiting training rst discuss pattern exploiting training pet text classication tasks e problems textual input x mapped single output y nite set y let m masked language model mlm t set tokens t mask token denote set token sequences t pet requires pattern p x t maps inputs cloze questions containing exactly mask verbalizer v y t maps output single token representing meaning probability y given derived score m assigns masked position p shown jiang et al schick schtze pairs p v work ter absence large development set pairs work hard distinguish perform poorly pet alleviates issue enabling neous usage multiple pattern verbalizer pairs combining mechanism similar knowledge distillation hinton et al pair p v separate mlm tuned available training data ensemble mlms annotate set unlabeled examples soft labels single mlm sequence classication head netuned resulting soft labeled dataset serves nal classier pet text generation differences consider signing form pet appropriate ative setting require izer output space consists natural language sentences e y t second encoder decoder architecture generative language models supports subtle adjustments regard application patterns finally need novel strategy combining multiple patterns simply average text quences produced different models ingful way section let p pattern x y y p z t furthermore let y z zm let single mask token z position h m denote concatenation y z yz subsequence yi yj yi consider encoder decoder model m pretrained masked language modeling objective model able compute ity pm y measures extent y plausible substitute mask z assume decomposing joint probability y follows pm y pm yi z n pm yi z obtained m processing z encoder decoder happen know prex y probability remaining sequence yk expressed pm yk n z pm yi z n m encoder decoder language model options compute probability y given pattern p process entire sequence p encoder choose index j h process n coder zj decoder example summary text process x summary encoder decoder compute pm z pm text summary preliminary experiments found tokens belong partially generated output quence e tokens processed decoder stronger impact model s predictions regular input tokens applies pegasus pretrained generate sentences pattern supposed prex sentence generated e short prompt gasus tends simply ignore processed encoder based observation supplement pattern p decoder prex d t given model generated quence observed input accordingly dene probability y given pm y p example discussed corresponds p summary text recent architectures fulll quirement including bart lewis et al raffel et al pegasus zhang et al decoder prex d corresponds ing pattern p text decoder prex d summary netune m set training examples y simply minimizing cross entropy tween y teacher forcing combining patterns multiple pairs pk dk patterns decoder xes rst netune individual model mi pi regular pet combine knowledge distill single model m require set unlabeled examples u need different strategy schick schtze assigning target sequences y y unlabeled example u simply average sequences generated individual models computing single sequence likely according models infeasible efciently require k models memory time instead resort following approach u rst generate output sequence di pi greedy decoding zhang et al resulting set candidate outputs cx k assign score candidate y cx end rst compute normalized log likelihood y pi log divide length y correct length bias boulanger lewandowski et al jean et al obtain total score y average scores according ual patterns exp nal model m trained pairs y u y drawn cx bility proportional train nal model simply maximize p m y note creates strong discrepancy pretraining ing pretraining pegasus processes sequences contain mask token spirit intention pretraining netuning similar possible train trivial pattern p prepends single mask token input use decoder prex task decoder prexes e mail subject aeslc headline gigaword cnn dailymail highlights e mail topic article headline article highlights short summary brief summary table decoder prexes use tion headline generation tasks experiments tasks evaluate pegasus pet subset tasks zhang et al limited compute available choose tasks maximum output length zhang et al tokens specically consider following datasets aeslc zhang tetreault given email body title email predicted gigaword rush et al given rst sentence news article headline generated xsum narayan et al articles ning wide range different topics summarized reddit tifu kim et al summaries generated posts tifu community reddit newsroom grusky et al maries generated articles major publications cnn dailymail hermann et al articles cnn daily mail list highlights generated task use entire test set ation create types training sets taining training examples dition provide unlabeled examples task unlabeled training examples randomly sampled original training set exception newsroom tains examples enable friendly evaluation consider random subset examples dataset use shot datasets zhang et al use xed random seed exact training data recoverable t model aeslc gigaword xsum reddit tifu newsroom cnn dailymail avg pegasus pegasus m pegasus pet pegasus pegasus m pegasus pet pegasus pegasus m pegasus pet table rl scores tasks training set sizes considered results averaged different seed dependent training sets column shows average performance tasks previous work schick schtze shown particular set training ples huge impact model s mance create distinct training sets size task different random seeds scores reported section average scores sets training examples patterns use set patterns tasks combine different coder prexes patterns use text decoder prexes shown table combine pattern decoder prex resulting pairs task setup experiments pet use pegasus large zhang et al lying language model implementation based transformers library wolf et al stated differently experiments formed setup schick schtze single gpu optimizing hyperparameters previous work uses development sets larger training sets multiple orders magnitude e xie et al zhang et al chen et al assuming existence large development sets highly inconsistent real world shot settings contrast schick schtze assume development data determine hyperparameters solely based previous work practical considerations choose middle way create small ment set examples tasks xsum use development set bination single training set containing amples determine hyperparameters tasks training sets ters consistent value derived previous work following zhang et al use mum input length tokens adafactor timizer shazeer stern square root learning rate decay dropout rate label smoothing setting szegedy et al adopt zhang et al s maximum lengths task recommended schick schtze train models steps batch size tried training steps velopment set found major differences performance learning rate tried values schick schtze use zhang et al use found perform best models evaluation follow zhang et al report rougel rl scores lin stemming porter algorithm porter results tasks compare ing approaches netuning pretrained pegasus model pegasus regular netuning procedure described zhang et al pegasus m finetuning single trivial pattern inserts mask token rst word pegasus pet finetuning pet patterns described table shows results zero shot learning shot learning training examples shot settings pet tently outperforms baselines tasks model aeslc xsum newsroom model reddit tifu newsroom cnn dailymail pegasus pegasus m pegasus pet worst best dec prex pegasus pegasus pet pegasus pegasus pet table rl scores baselines variants pet given training examples table rl scores maximum output lengths given training examples resulting average improvement pegasus vs vs pegasus m gives consistent provements regular netuning performs clearly worse pet demonstrating pet model able use task scriptions provided zero shot setting pet outperforms baselines average falls short individual tasks analysis look factors contributing pet s performance detail end ble compares performance best best worst worst performing tern decoder prex pet setting training examples ence performance best worst patterns difference nounced previous work schick schtze notably simple strategy bining patterns performs better best pattern tasks metrics table shows results decoder prex dec prex instead processing entire input encoder given p d p zn zh compute pm y zn pm y zn variant overall forms better pegasus m results clearly pegasus makes use task descriptions processed encoder finally look performance pet function maximum output length hypothesize inuence decoder prex generated tokens decrease distance mean diminishing gains expected pet tasks require longer text sequences generated investigate assumption table shows performance pegasus pegasus pet tasks maximum output lengths values compute gains pet difference mance pegasus pet pegasus average increasing tokens reduces gains pet regular netuning points shows task descriptions provided pet strong impact generated tokens dozens tokens accordingly proposed approach benecial generating long text sequences conclusion shown pattern exploiting training pet transferred text generation tasks introducing concept decoder prexes combining patterns knowledge tion target sequences generated domly chosen patterns modications pretrained pegasus model netuned pet clearly outperforms regular netuning shot settings future work interesting possible pretrained language models understand complex task descriptions simple prompts especially concurrent work weller et al efrat levy suggests case references nicolas boulanger lewandowski yoshua bengio pascal vincent audio chord recognition recurrent neural networks ismir tom b brown benjamin mann nick ryder melanie subbiah jared kaplan prafulla dhariwal arvind neelakantan pranav shyam girish sastry amanda askell sandhini agarwal ariel herbert voss gretchen krueger tom henighan rewon child aditya ramesh daniel m ziegler jeffrey wu clemens winter christopher hesse mark chen eric sigler mateusz litwin scott gray jamin chess jack clark christopher berner sam mccandlish alec radford ilya sutskever dario amodei language models computing research repository shot learners jiaao chen zichao yang diyi yang text linguistically informed interpolation den space semi supervised text classication proceedings annual meeting ciation computational linguistics pages online association computational guistics jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language proceedings conference standing north american chapter association computational linguistics human language technologies volume long short papers pages minneapolis minnesota ation computational linguistics zi yi dou keyi yu antonios anastasopoulos investigating meta learning algorithms low resource natural language understanding tasks proceedings conference empirical methods natural language processing international joint conference natural guage processing emnlp ijcnlp pages hong kong china association tional linguistics avia efrat omer levy turking test language models understand instructions ing research repository allyson ettinger bert lessons new suite psycholinguistic diagnostics language models transactions association computational linguistics max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies proceedings conference north american chapter association computational linguistics man language technologies volume long pers jiatao gu yong wang yun chen victor o k li kyunghyun cho meta learning resource neural machine translation ings conference empirical methods natural language processing pages brussels belgium association computational linguistics junxian wojciech kryscinski bryan mccann nazneen rajani caiming xiong generic controllable text sum computing research repository marization karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read advances neural comprehend tion processing systems volume pages curran associates inc geoffrey hinton oriol vinyals jeff dean distilling knowledge neural network puting research repository jeremy howard sebastian ruder universal language model ne tuning text classication proceedings annual meeting sociation computational linguistics volume long papers pages melbourne australia association computational linguistics sbastien jean orhan firat kyunghyun cho roland memisevic yoshua bengio montreal neural machine translation systems proceedings tenth workshop statistical machine translation pages lisbon tugal association computational linguistics zhengbao jiang frank f xu jun araki graham neubig know language models know transactions association computational linguistics nitish shirish keskar bryan mccann lav r varshney caiming xiong richard socher ctrl conditional transformer language model trollable generation computing research tory byeongchang kim hyunwoo kim gunhee kim abstractive summarization reddit posts multi level memory networks ings conference north american chapter association computational guistics human language technologies volume long short papers pages neapolis minnesota association computational linguistics rebecca knowles philipp koehn neural interactive translation prediction proceedings association machine translation icas pages philippe laban andrew hsi john canny marti hearst summary loop learning write abstractive summaries examples ceedings annual meeting ciation computational linguistics pages online association computational guistics haejun lee drew hudson kangwook lee christopher d manning slm ing discourse language representation tence unshufing computing research repository mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov luke zettlemoyer bart denoising sequence sequence training natural language generation translation comprehension proceedings nual meeting association computational linguistics pages online association computational linguistics chin yew lin rouge package matic evaluation summaries text tion branches pages barcelona spain association computational linguistics stephen mayhew gupta nitish dan roth robust named entity recognition truecasing training proceedings aaai conference articial intelligence shashi narayan shay b cohen mirella lapata nt details summary topic aware convolutional neural networks proceedings treme summarization conference empirical methods natural guage processing pages brussels gium association computational linguistics matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word proceedings resentations ence north american chapter ation computational linguistics human guage technologies volume long papers pages new orleans louisiana association computational linguistics fabio petroni tim rocktschel sebastian riedel patrick lewis anton bakhtin yuxiang wu alexander miller language models edge bases proceedings conference empirical methods natural language processing international joint conference ral language processing emnlp ijcnlp martin f porter algorithm sufx ping page morgan kaufmann publishers inc san francisco usa kun qian zhou yu domain adaptive log generation meta learning proceedings annual meeting association putational linguistics pages florence italy association computational linguistics alec radford karthik narasimhan tim salimans improving language ilya sutskever standing generative pre training alec radford jeff wu rewon child david luan dario amodei ilya sutskever language models unsupervised multitask learners cal report colin raffel noam shazeer adam roberts ine lee sharan narang michael matena yanqi zhou wei li peter j liu exploring limits transfer learning unied text text transformer journal machine learning search alexander m rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages lisbon portugal association computational linguistics timo schick helmut schmid hinrich schtze automatically identifying words serve labels shot text classication proceedings international conference computational linguistics pages barcelona spain online international committee computational linguistics timo schick hinrich schtze exploiting cloze questions shot text classication natural language inference computing research repository timo schick hinrich schtze s size matters small language models shot learners computing research repository timo schick hinrich schtze rare words major problem contextualized embeddings x attentive mimicking proceedings thirty fourth aaai conference articial intelligence noam shazeer mitchell stern adafactor adaptive learning rates sublinear memory cost computing research repository c szegedy v vanhoucke s ioffe j shlens z wojna rethinking inception ieee ture computer vision ence computer vision pattern recognition cvpr pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser illia polosukhin attention need advances neural information cessing systems pages curran ciates inc cunxiang wang shuailong liang yue zhang nan li tian gao sense pilot study sense making planation proceedings annual ing association computational linguistics pages florence italy association computational linguistics orion weller nicholas lourie matt gardner matthew peters learning task tions proceedings conference pirical methods natural language processing emnlp thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi ric cistac tim rault remi louf morgan icz joe davison sam shleifer patrick von platen clara ma yacine jernite julien plu canwen xu teven le scao sylvain gugger mariama drame quentin lhoest alexander rush formers state art natural language ing proceedings conference pirical methods natural language processing system demonstrations pages online ciation computational linguistics joern wuebker spence green john denero saa hasan minh thang luong models inference prex constrained machine translation proceedings annual meeting association computational linguistics volume long papers pages berlin germany sociation computational linguistics qizhe xie zihang dai eduard hovy minh thang ong quoc v le unsupervised data mentation consistency training computing search repository jingqing zhang yao zhao mohammad saleh peter liu pegasus pre training tracted gap sentences abstractive summarization proceedings international conference machine learning volume proceedings machine learning research pages virtual pmlr rui zhang joel tetreault email save life introducing task email subject line generation proceedings annual meeting association computational guistics pages florence italy association computational linguistics
