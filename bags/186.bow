pretraining based natural language generation text summarization haoyu jianjun computer national university defense technology changsha china jjxu edu abstract paper propose novel pretraining based encoder decoder framework generate output sequence based input sequence stage manner encoder model encode input sequence text representations bert decoder stages model rst stage use transformer based decoder generate second stage draft output sequence mask word draft sequence feed bert combining input sequence draft representation generated bert use transformer based decoder predict rened word masked position best knowledge approach rst method applies bert text generation tasks rst step direction evaluate proposed method text summarization task mental results model achieves new state art cnn daily mail new york times datasets introduction text summarization generates summaries input ments keeping salient information important task applied real world applications methods proposed solve text marization problem nallapati zhou gehrmann main text summarization techniques extractive tive extractive summarization generates summary ing salient sentences phrases source text abstractive methods paraphrase restructure sentences compose summary focus abstractive tion work exible generate diverse summaries recently abstractive approaches introduced based neural sequence sequence framework paulus gehrmann based sequence sequence model copy contact author mechanism incorporates coverage vector track control attention scores source text paulus introduce intra temporal attention processes encoder decoder address repetition incoherent problem issues previous abstractive methods methods use left context decoder complete context predicting word utilize pre trained contextualized language models decoder difcult decoder learn summary representations context interactions language modeling recently bert successfully ural language processing tasks textual entailment entity recognition machine reading sions paper present novel natural language generation model based pre trained language models use bert work far know rst work extend bert sequence generation task dress issues previous abstractive methods model design stage decoding process good use bert context modeling ability rst stage generate summary left context decoder second stage mask word summary predict rened word rene decoder improve naturalness generated sequence cooperate reinforcement objective rene decoder main contributions work propose natural language generation model based bert making good use pre trained language model encoder decoder process model trained end end handcrafted features design stage decoder process tecture model generate word summary considering sides context information conduct experiments benchmark datasets cnn daily mail new york times model achieves average rouge cnn daily mail state art new york times dataset model achieves relative improvement background text summarization paper focus single document multi sentence summarization propose supervised abstractive model based neural attentive sequence sequence work consists parts neural network encoder network decoder encoder encodes input sequence intermediate representation decoder predicts word time step given input sequence representation vector previous decoded output goal model maximize probability ating correct target sequences encoding ation process attention mechanism concentrate important positions text learning tive sequence sequence models minimize negative log likelihood generated sequence shown following equation ground truth mary token loss log objective traditional sequence ation models consider direction context coding process cause performance degradation complete context token contains preceding following tokens feeding preceded decoded words decoder model generate unnatural quences example attentive sequence sequence els generate sequences repeated phrases harm naturalness previous works mitigate problem improving attention calculation process paper feeding directional context stead left context better alleviate problem directional pre trained context encoders recently context encoders elmo gpt bert widely nlp tasks models pre trained huge unlabeled corpus generate better contextualized token embeddings approaches built achieve better performance method based bert illustrate cess briey bert consists layers layer rst multi head self attention sub layer linear afne sub layer residual connection self attention sub layer attention scores eij rst calculated output dimension parameter matrices layer outputs context encoding input sequence despite wide usage huge success mismatch problem pre trained context coders sequence sequence models issue pre trained context encoder like bert model token level representations conditioning rection context pre training fed plete sequences left context decoder pre trained language models suffer plete inconsistent context generate good context aware word representations especially ing inference process model section describe structure model learns generate abstractive multi sentence summary given source document based sequence sequence framework built bert rst design rene decoder word level tackle problems described section introduce discrete objective rene decoders reduce exposure bias problem overall structure model illustrated figure problem formulation denote input document represents source token corresponding mary denoted represents mary length given input document rst predict summary draft left context decoder erated summary draft condition context sides rene content summary draft guide constrain rene process summary summary draft generation summary draft based sequence sequence model encoder input document coded representation vectors fed decoder generate summary draft aij eij exp eij exp eik encoder simply use bert encoder rst maps input sequence word embeddings computes document embeddings encoder output denoted following equation output calculated weighted sum previous outputs added previous bert figure model overview represents decoder layer number represents summary length summary draft decoder draft decoder rst introduce bert word bedding matrix map previous summary draft outputs embeddings vectors time step note input sequence decoder complete use bert network predict context vectors introduce layer transformer decoder learn conditional probability transformer encoder decoder multi head attention helps decoder learn soft alignments summary source document time step draft decoder predicts output ability conditioned previous outputs encoder den representations shown generated sequence truncated rst position special token total mary draft decoder progress shown stage figure issue exists contextualized word representations design rene process mitigate approach described sub section copy mechanism summary tokens vocabulary words occurs input document incorporate copy nism based transformer decoder describe briey decoder time step rst calculate attention ability distribution source document linear dot product layer decoder output encoder output shown otwchj exp exp vocab ldec log shows decoder learning objective minimize negative likelihood conditional probability ground truth word summary decoder structure sufcient use bert network decoder training inference complete sentence fed bert module tune bert parameters input distribution different pre train process harms ity generated context representations use embedding matrix difcult decoder fresh parameters learn model representations vocabulary probabilities relative small corpus compared bert huge training corpus word decoder utilize bert ability generate high quality context vectors harm performance calculate copying gate makes soft choice selecting source generating vocabulary parameters calculate weighted sum copy ity generation probability nal predicted ability extended vocabulary set vocabulary words source document nal probability calculated follow vocab summary rene process main reason introduce rene process enhance decoder bert contextualized representations modify encoder reuse process decoder propose new word level rene decoder rene decoder receives generated summary multi head attention decoderbertsummary draft draft outputbertmulti head attention embeddingdocumentsummary draft embeddingsummary outputmask draft input outputs rened summary figure stage shows rst masks word summary draft feeds draft bert generate context vectors finally predicts rened summary word layer transformer decoder draft decoder time step word input summary masked decoder predicts rened word given words summary learning objective process shown ground truth summary word mary word lref ine log view bert contextualized dings rene decoding process provides complete input sequence consistent pre training cesses intuitively process works follows draft decoder writes summary draft based document rene decoder edits draft concentrates word time based source document words design word level rene decoder cess similar cloze task bert pre train cess ability contextual language model decoder generate uent natural quences parameters shared draft decoder rene decoder individual parameters model performance degrades lot reason use teach forcing training level rene decoder learns predict words given ground truth words summary objective similar language model pre train objective probably decoder learn generate rened maries model decoders share ters mixed objective summarization rouge usually tion metric model training objective maximize log likelihood generated sequences mis match harms model performance similar vious work kryscinski add discrete jective model optimize introducing icy gradient method discrete objective summary draft process shown draft summary sampled predicted distribution reward score compared ground truth mary use rouge experiment balance tween optimizing discrete objective generating able sequences mix discrete objective likelihood objective shows nal objective draft process note ldec logp rene process introduce similar objectives learning inference model training objective model sum processes jointly trained teacher forcing gorithm training feed ground truth summary decoder minimize following objective lmodel ldec lref ine test time time step choose predicted word use beam search generate draft summaries use greedy search generate ned summaries experiment settings work models built bertbase larger pre trained model better mance bertlarge published costs time gpu memory use wordpiece embeddings vocabulary bert set layer transformer decoders set attention heads number set fully connected sub layer hidden size train model adam optimizer learning rate use dynamic learning rate training process regularization use dropout srivastava label ing szegedy models set dropout rate label smoothing value set objective factor training set batch size train epochs fewer ing samples training best model selected models based development set performance gpu memory limit use gradient accumulation set cumulate step feed samples step use beam size length penalty generate logical form sequences lter repeated tri grams beam search process ting word probability zero generate tri gram exists existing summary nice method avoid phrase repetition datasets seldom contains repeated tri grams summary tune erated sequences simple rules multi summary sentences exactly content rst remove sentences remove sentences words result datasets evaluate performance model conduct ments cnn daily mail dataset large collection news articles modied summarization ing choose non anonymized version dataset consists training samples test set samples conduct experiments new york dataset consists news articles original dataset applied lrl ldec lrl dec dec ldec nist rouge avg model extractive summmarunner nallapati refresh narayan deepchannel shi rnn ext chen bansal mask global chang neusum abstractive attn inconsistency summarization gehrmann dca celikyilmaz stage stage stage table rouge results models ablations cnn daily mail test set avg calculates average score rouge experiment follow dataset splits pre process settings durrett rst lter samples article text abstract remove samples summaries shorter words choose test set based date examples published january nal dataset contains training samples test samples called summaries longer words tokenize sequences datasets wordpiece tokenizer tokenizing average article length summary length cnn daily mail average article length summary length truncate article length summary length summary length set average golden summary length longer evaluation metrics cnn daily mail dataset report length score rouge rics calculated pyrouge porter mer option following paulus evaluate limited length rouge recall ated summary length ground truth length split summaries sentences semicolons calculate rouge scores results analysis table gives results cnn daily mail dataset pare performance recent approaches model classify groups based extractive abstractive models line ble lists score model python org pypi comparable dca outperforms rouge compared extractive models neusum global achieve slight higher scores model outperforms models scores condence interval improvements statistically signicant ablation analysis lines table conduct tion study model variants analyze importance component use ablation models periments stage sequence sequence model copy mechanism based bert stage adding decoder stage model stage model rene process cooperated objective compare model stage ablation observe model outperforms average rouge suggesting ment objective helps model effectively analyze effect rene process removing stage model observe rene process average rouge score drops ablation study proves module necessary model ments statistically signicant metrics effects summary length evaluate impact summary length model formance compare average rouge score ments model different length ground truth maries figure shows compared pointer generator coverage length interval test set improvements model higher shorter samples conrms ter context representations longer documents model achieve higher performance shown figure compared related work text summarization text summarization models usually classied stractive extractive ones recently extractive models like deepchannel shi rnn chen bansal neusum achieve higher performances designed structures ample deepchannel propose salience estimation network iteratively extract salient sentences zhang train sentence compression model teach latent variable extractive model recent works focus improving abstractive methods gehrmann design content selector determine phrases source document hsu introduce summary tency loss force words attended termined extractive model lower generation extend model abilities information selection network generate informative summaries pre trained language models pre trained word vectors mikolov pennington bojanowski widely nlp tasks recently pre trained language models elmo gpt bert achieved great success nlp problems textual entailment semantic similarity reading comprehension question answering peters radford devlin recent works focus leveraging pre trained language models summarization radford pretrain language model use sentiment kryscinski yser generating reviews goods train language model golden summaries use decoder incorporate prior knowledge work use pre trained language model large scale unlabeled data encoder decoder model designing stage coding structure build competitive model abstractive text summarization conclusion future work work propose stage model based sequence sequence paradigm model utilize bert encoder decoder sides introduce reinforce jective learning process evaluate model benchmark datasets cnn daily mail new york times experimental results compared previous tems approach effectively improves performance experiments conducted tion task model natural language generation tasks machine translation question ation paraphrasing rene decoder mixed tive applied sequence generation tasks investigate future work figure average rouge improvement cnn daily mail test set samples different golden summary length extractive baseline tage model fall golden summary length greater probably truncate long documents golden summaries formation training data intervals train abstractive model simple extractive method fall far model sentences words durrett attn stage table limited length rouge recall results test set additional results table reports experiment results corpus short summary samples ltered erage longer summaries cnn daily mail model needs catch long term dependency sequences erate good summaries rst lines table results baselines introduced durrett lines select rst sentences select rst words original document compare performance model recent models improvements compared intra attn sota carries dataset large margin model provement experiment proves approach outperform competitive methods different data butions references bojanowski piotr bojanowski edouard grave armand joulin tomas mikolov enriching word tors subword information transactions ciation computational linguistics celikyilmaz asli celikyilmaz antoine lut xiaodong yejin choi deep ing agents abstractive summarization arxiv preprint chang ming wei chang kristina toutanova language model kenton lee jacob devlin pre training hierarchical document representations arxiv preprint chen bansal yen chun chen mohit bansal fast abstractive summarization reinforce selected arxiv preprint sentence rewriting devlin jacob devlin ming wei chang ton lee kristina toutanova bert pre training deep bidirectional transformers language ing arxiv preprint durrett greg durrett taylor berg kirkpatrick dan klein learning based single document rization compression anaphoricity constraints proceedings annual meeting tion computational linguistics acl august berlin germany volume long papers gehrmann sebastian gehrmann yuntian deng alexander rush abstractive summarization arxiv preprint jiatao zhengdong hang incorporating copying mechanism victor sequence sequence learning acl hsu wan ting hsu chieh kai lin ying lee kerui min jing tang min sun unied model extractive abstractive summarization ing inconsistency loss arxiv preprint wojciech kryscinski paulus caiming xiong richard socher ing abstraction text summarization romain arxiv preprint wei xinyan xiao yajuan lyu yuanzhuo wang improving neural abstractive document summarization explicit information selection eling emnlp pages mikolov tomas mikolov ilya sutskever kai chen greg corrado jeff dean distributed sentations words phrases ity advances neural information processing systems pages nallapati ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural work based sequence model extractive summarization proceedings thirty aaai documents conference articial intelligence pages narayan shashi narayan shay cohen mirella lapata ranking sentences extractive marization reinforcement learning arxiv preprint romain paulus caiming xiong richard socher palo alto deep reinforced model abstractive summarization iclr pages pennington jeffrey richard socher christopher manning glove global vectors proceedings word representation conference empirical methods natural language processing emnlp pages pennington peters matthew peters mark neumann hit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word sentations arxiv preprint radford alec radford rafal jozefowicz ilya sutskever learning generate reviews ering sentiment corr radford alec radford karthik narasimhan improving language tim salimans ilya sutskever understanding generative pre training abigail peter liu pher manning point summarization proceedings pointer generator networks annual meeting association computational guistics acl pages shi jiaxin shi chen liang lei hou juanzi zhiyuan liu hanwang zhang deepchannel salience estimation contrastive learning tive document summarization corr srivastava nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan nov dropout simple way prevent neural networks overtting journal machine learning research szegedy christian szegedy vincent houcke sergey ioffe jonathon shlens zbigniew wojna rethinking inception architecture computer ieee conference computer vision vision pattern recognition cvpr pages zhang xingxing zhang mirella lapata furu wei ming zhou neural latent extractive document summarization arxiv preprint qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural document summarization jointly learning score select sentences proceedings annual ing association computational linguistics acl melbourne australia july volume long papers pages
