r m l c s c v v x r improving online forums summarization unifying hierarchical attention networks convolutional neural networks sansiri tarnpradab university central florida fereshteh jafariakinabad university central florida kien hua university central florida online discussion forums prevalent easily accessible allowing people share ideas opinions posting messages discussion threads forum threads significantly grow length difficult participants newcomers existing grasp main ideas study aims create automatic text summarizer online forums mitigate problem present framework based hierarchical attention networks unifying bidirectional long short term memory bi lstm convolutional neural network cnn build sentence thread representations forum summarization scheme bi lstm derives representation comprises information sentence thread cnn recognizes high level patterns dominant units respect sentence thread context attention mechanism applied cnn highlight high level representations capture important units contributing desirable summary extensive performance evaluation based datasets real life online forums news dataset reveals proposed model outperforms competitive ccs concepts computing methodologies natural language processing neural networks information systems additional key words phrases multi document summarization extractive summarization hierarchical attention networks sansiri tarnpradab fereshteh jafariakinabad kien hua improving online forums summarization unifying chical attention networks convolutional neural networks march pages online discussion forums embody plethora information exchanged people common interest typically discussion thread initiated user posting message e question suggestion narrative users interested topic join discussion posting messages e answer relevant experience new question thread gains popularity span hundreds messages putting burden newcomers current participants spend extra time understand simply catch code data available com git authors addresses sansiri tarnpradab university central florida ucf edu fereshteh jafariakinabad university central florida fereshteh ucf edu kien hua university central florida ucf edu permission digital hard copies work personal classroom use granted fee provided copies distributed profit commercial advantage copies bear notice citation page copyrights components work owned acm honored abstracting credit permitted copy republish post servers redistribute lists requires prior specific permission fee request permissions org association computing machinery manuscript submitted acm manuscript submitted acm marization data mining representation learning acm reference format introduction tarnpradab et al desirable discussion far automatic forum summarization method generates concise summary highly simple way produce summary identify salient sentences aggregate method naturally aligns concept extractive summarization likewise involves selecting representative units nating according chronological order order determine saliency unit context taken account factor critical summarization process automatic system human tasked selecting sentences document form summary illustration human given thread extract key sentences read thread grasp contextual information select sentences based context compose summary hand arbitrary sentence shown human supplying context thread sentence belongs clear way deciding sentence belong summary previous works shown performance summarizer improved context information document structure model utilize knowledge generate effective representations similar documents forum threads possess hierarchical structure words constitute sentence sentences constitute post posts constitute thread work propose data driven approach based hierarchical attention networks summarize online forums order utilize knowledge forum structure method hierarchically encodes sentences threads obtain sentence thread representations attention mechanism applied place emphasis salient units drawing inspiration humans read comprehend summarize document led network design unifies bidirectional long short term memory bi lstm convolutional neural network cnn scheme bi lstm derives representation comprises information sentence thread long term dependencies cnn recognizes high level patterns dominant units words sentences respect context sentence thread networks combined aim leverage individual strength achieve effective representations compared extensive experimental results verifies effectiveness contributions study follows propose hierarchical attention networks unifies bi lstm cnn obtain representations extractive summarization online forums attention mechanism employed weight important units different previous studies apply attention directly individual words sentences findings suggest applying attention high level features extracted compressed cnn contributes improvements performance demonstrate advantage proposed hybrid model perform comprehensive empirical study result shows proposed approach significantly outperforms range competitive baselines initial study respect sentence level scores rouge evaluation encourages investigation use proposed hybrid network text summarization conduct extensive experiment different pretrained embeddings static contextual investigate effectiveness improving summarization performance proposed approach framed multi document summarization evaluate performance datasets online forums domain news domain remainder paper organized follows review literature related automatic summarization section proposed framework introduced section section provide details dataset manuscript submitted acm improving online forums summarization unifying hierarchical attention networks convolutional neural networks experimental configurations performance studies explain baselines comparative study assess effectiveness proposed model performance results analyzed section finally draw conclusions section related work summarization representation learning extractive summarization study address problem online forums summarization described section major strands research related study including extractive summarization neural network based text mainly kinds methods text summarization extractive summarization abstractive summarization owing effectiveness simplicity extractive summarization approach extensively technique involves segmenting text units e sentences phrases paragraphs nating key subset units derive final summary contrast abstractive approach functions similarly paraphrasing original units hardly preserved output summary study consider extractive summarization approach design deep classifier recognize key sentences summary extractive approach applied data domains forum threads online reviews emails group chats meetings microblogs news news domain articles typically follow clear pattern important point article followed secondary point forth generally observe clear pattern domains aforementioned examples particular forum content created multiple users gist contained different posts necessarily sentence paragraph furthermore user generated content ugc generally contains noise misspellings informal abbreviations choosing sentences summarization challenging work focus summarizing content forum thread given nature forum data framed multi document summarization documents created posted different authors neural network based text summarization large body research applies neural networks involving rnn cnn combination develop improve text summarization example nallapati et al proposed rnn based sequence model entitled summarunner produce extractive summaries layer bidirectional gated recurrent unit gru applied derive document representations layer runs word level derive hidden representation word forward backward directions second layer runs sentence level encode representations sentences document cao et al proposed cnn based summarization system entitled tcsum perform multi document summarization adopting transfer learning concept tcsum demonstrated distributed representation projected text classification model shared summarization model model achieve state art performance handcrafted features needed unified architecture combines rnn cnn summarization task shown success works instance singh et al proposed hybrid memnet data driven end end network single document summarization cnn applied capture latent semantic features lstm applied capture overall representation document final document representation generated concatenating document embeddings manuscript submitted acm tarnpradab et al cnn lstm memory network narayan et al proposed unified architecture frames extractive summarization problem reinforcement learning objective architecture involves lstm cnn encode sentences documents successively model learns rank sentences training network reinforcement learning framework optimizing rouge evaluation metric lines research taken account hierarchical structure document cheng lapata developed framework containing hierarchical document encoder attention based extractor single document summarization hierarchical information shown help derive meaningful representation document zhou et al proposed end end neural network framework generate extractive document summaries essentially authors developed hierarchical encoder bidirectional gated recurrent unit bigru integrates sentence selection strategy scoring model model jointly learn score select sentences usage attention mechanism proven successful applications example wang ling introduced attention based encoder decoder concept summarize opinions authors applied lstm network generate abstracts given input latent representation computed attention based encoder cao et al applied attention concept simulate human attentive reading behavior extractive query focused summarization system called attsum proposed demonstrated capable jointly handling tasks query relevance ranking sentence saliency ranking recent works applied attention mechanism facilitate sentence extraction process form summary narayan et al proposed hierarchical document encoder attention based extractor generate extractive summaries results shown attention model successfully guide representation learning document feng et al presented model entitled aes attention encoder based summarization summarize articles architecture comprises attention based document encoder attention based sentence extractor authors consider unidirectional bidirectional rnn experiment results shown better performance obtained bidirectional rnn network reads sequence original reverse orders helps derive better document representation representation learning representation learning aims acquire representations automatically data plays crucial role natural language understanding nlu natural language processing nlp models particularly pre trained word representations building blocks nlp nlu models shown improve downstream tasks domains text classification machine translation machine comprehension learning high quality word representations challenging approaches developed produce pre trained word embeddings differ model semantics context words based model glove global vectors word representation count based model rely distributional language hypothesis order capture semantics fasttext character based word representation word represented bag character n grams final word vector sum representations advantages fasttext capability handling vocabulary words oov unlike glove classical word embeddings capture semantic syntactic characteristics words extent fail capture polysemy disregard context word appears address polysemous context dependent nature words contextualized word embeddings proposed elmo embeddings language models proposes deep contextualized word representation representation function manuscript submitted acm improving online forums summarization unifying hierarchical attention networks convolutional neural networks input sentence objective function bidirectional language model representations linear combination internal layers bilm weights learnable specific task bert bidirectional encoder representations transformers contextualized word representation trained bidirectional transformers jointly conditioning left right context layers objective function bert masked language model words input sentence randomly masked flair contextualized character level word embedding models words context sequences characters trained character level language model objective summary approaches adopted learn word representation literature differ ways model meaning context choice word embeddings particular nlp tasks matter experimentation evaluation study experimented fasttext elmo bert embeddings integrating embedding layer model embeddings initialize vectors words sentences present forum data substitute random initialization summarization model system tasked extracting representative sentences thread form summary naturally suited formulated supervised learning task consider sentence extraction unit succinctness let sentences thread corresponding labels indicates sentence summary goal find probable tag sequence given thread sentences arg max t t set possible tag sequences independently tag sentence determined section elaborate hierarchical based framework multi document summarization proposed model based hierarchical attention networks han construct sentence thread representations types neural networks bi directional recurrent neural network convolutional neural network combined unified framework maximize capability summarizer nutshell model comprised hierarchical encoders neural attention component sentence extractor encoders generate representations based words sentences forum neural attention mechanism pinpoints meaningful units process finally sentence extractor selects puts key sentences produce summary following boldface letters represent vectors matrices words sentences denoted indices sentence encoder sentence encoder reads input sentence sequence word embeddings returns sentence vector output adopting pipeline architecture process data streaming manner bi directional recurrent neural network followed convolutional neural network constitute sentence encoder furthermore attention mechanism employed generating sentence vector emphasis units contribute meaning sentence strategy sentence encoding illustrated figure elaborate different network components following subsections manuscript submitted acm tarnpradab et al fig illustration sentence encoder input layer given thread sequence sentences sentence sequence words let denote sentence words indexed denotes number words sentence word converted corresponding pretrained embedding figure subsequently fed bidirectional recurrent neural network bidirectional recurrent neural network layer opt bidirectional long short term memory bi lstm effectiveness evidenced previous studies lstm contains input gate forget gate output gate control information coming previous time step flowing time step gating mechanism accommodates long term dependencies allowing information flow hidden representation sustain long period time bi lstm model contains forward pass backward pass eq forward comprises semantic information beginning sentence current time step contrary comprises semantic information current time step end sentence vectors dimension r dimensionality hidden state word level bi lstm finally produces word representation carries contextual concatenating vectors particular information sentence word convolutional layer convolutional layer primarily extract high level features hidden representation obtained preceding layer word sentence compiled form matrix input cnn concretely r convolutional layer composed set filters filter applied window words denotes index filter filter slides input form feature map r feature map obtained manuscript submitted acm improving online forums summarization unifying hierarchical attention networks convolutional neural networks denotes submatrix comprised row row r additive bias rectified linear unit relu applied element wise nonlinear activation function study dimensional max pooling operation performed obtain fixed length vector total max pooled vectors generated given feature length max pooling window size transformed vector half length words meaningful features bigram extracted transformed vector constituted max pooled values concatenated eq resultant feature maps combined final representation r eq attention layer section describe attention mechanism employed attend important units sentence note units refer latent semantic features bigrams unit compression max pooling prior layer introduce trainable vector bigrams capture global bigram saliency vector denoted selected multiplication operation standard basis vector containing zeros position vector projected transformed space generate eq inner product signals importance bigram convert normalized weight softmax function eq finally weighted sum bigram representation computed obtain sentence vector scalar value indicating bigram importance eq fig complete framework proposed summarization model manuscript submitted acm thread encoder tarnpradab et al thread encoder takes input sequence sentence vectors previously encoded sentence encoder illustrated figure choose index sentences thread encoder similar network dimension r architecture sentence encoder summarized eq note vectors dimensionality hidden state sentence level bi lstm eq sentence thread compiled form matrix r eq feature map represented r index cnn filter total number sentences thread filter height eq constituted max pooled values concatenated vector eq max pooling window size representing pair consecutive sentences resultant max pooled vectors combined final representation r eq vector denoted eq sentence level attention mechanism introduces trainable vector encodes salient sentence level content thread vector weighted sum sentence pairs normalized scalar value indicating important sentence pairs thread eq output layer vector representation sentence concatenated corresponding thread representation construct final sentence representation sentence level thread level context taken account classifying sentence final summary learned vector representations fed dense layer sigmoid activation function cross entropy measure network loss sentence extraction impose limit number words final summary total words original thread allowed order extract salient sentences sentences sorted based saliency scores outputted dense layer sorted sentences iteratively added final summary compression limit reached sentences final summary chronologically ordered according appearance original manuscript submitted acm improving online forums summarization unifying hierarchical attention networks convolutional neural networks thread sentences selected supervised summarization models tend redundant apply additional constraint include sentence summary contains new bigrams comparison existing bigrams final summary henceforth refer approach hierarchical hybrid deep neural network hybrid network short complete framework hybrid network illustrated figure section description datasets experiments followed details training set created present experiment configurations list hyperparameters explored achieve best performing model provide brief description baselines performance study subsequently introduction metrics evaluating performance summarization system experiment dataset proposed approach applicable multi document summarization online forums dataset perform experiments news data datasets trip advisor reddit newsroom study crawled online forums news articles major publications statistics datasets provided table brief description follows trip advisor trip forum data collected bhatia et al study total tripadvisor threads originally annotated human summaries additional threads annotated later tarnpradab et al held threads development set reported performance results remaining threads development set mainly hyperparameter tuning purposes described section reference summaries prepared having human annotators generate summary thread annotators instructed read thread write corresponding summary length limited original thread length annotators encouraged pick sentences directly data reddit reddit forum prepared wubben et al contains threads subreddits size threads ranges sentences words line sentences study utilize threads length sentences threads size smaller necessary summarized training test sets contain threads respectively development set contains threads hyperparameter tuning reference summaries prepared number votes factor select sentences sentences ranked based final votes upvotes downvotes ranked sentences iteratively added output list total words reach compression ratio original total words finally selected sentences ordered according chronological order newsroom summarization dataset contains million articles summaries written authors editors newsrooms major publications training evaluating summarization systems dataset provides training development test sets set comprises summary objects individual includes information article text corresponding summary date density bin density bin denotes summarization strategies reference summary involves extractive abstractive mix study use articles reference summary generated extractive approach similar tripadvisor reddit ruhosting nl wordpress project nlp cornell edu newsroom manuscript submitted acm tarnpradab et al reddit dataset news articles short summary necessary filter articles number sentences lower result training test sets contain total articles respectively development set contains articles exploring best set hyperparameters table data statistics trip advisor reddit newsroom vocabulary threads avg sentences max sentences avg words max words avg words sentence training set creation study sentence requires label train deep neural network create training set sentence marked true indicate summary unit false indicate set s initialized thread news article sentence member set add sentence set measure score set gold summaries sentence removed set sentences score measured candidate sentences increased score permanently added set process repeated following conditions achieved total number words selected sentences hit desired compression ratio rouge score summary improved finally sentences member set labeled true labeled false utilized rouge java evaluate rouge scores presents unigram overlap selected sentences gold summaries model configuration optimum parameters hybrid model explored experimentation fold cross validation tuning training process performed random search sampling replacement possible configurations configuration space large hyperparameters listed table based recommendation found best configuration number bi lstm neurons sentence encoder thread encoder respectively cnn hyperparameters best explored number convolutional layers sentence thread levels best number filters levels filter size stride length best explored dropout rate learning rate batch size lastly rmsprop optimizer shown best optimize binary cross entropy loss function model training validation test split set threads kept split ratio fixed experiments datasets prevent model overfitting applied early stopping training process computing error value model validation dataset epoch terminating training error value monotonically increased obtaining best configuration retrained model ganesan com content manuscript submitted acm improving online forums summarization unifying hierarchical attention networks convolutional neural networks union training development sets evaluated test set training forum threads news articles iterated epoch training process continues loss value converges maximum epoch number met pretrained vectors apply fasttext word level embedding vectors apply sentence level embedding vectors sentence vectors trained bert shown better performances table hyperparameter values evaluated proposed model hyperparameter range number bi lstm hidden layer neurons number convolutional layers number cnn filters cnn receptive field size dropout rate learning rate batch size optimizer baselines baselines follows compare proposed model unsupervised supervised methods detailed descriptions unsupervised learning baselines unsupervised learning baselines comparative study ilp baseline integer linear programming framework implemented sumbasic approach assumes words occurring frequently document cluster higher chance included summary kl sum method adds sentences summary long decreases kl divergence lsa latent semantic analysis technique identify semantically important sentences lexrank graph based summarization approach based eigenvector centrality mead centroid based summarization system scores sentences based sentence length centroid position redundant text opinosis graph based algorithm generating abstractive summaries large amounts highly textrank graph based extractive summarization algorithm computes similarity sentences supervised learning baselines include traditional supervised learning methods support vector machine svm liblinear study employ following features cosine similarity current sentence thread centroid relative sentence position thread number words google com archive p cc docs en english vectors html org elmo com google research bert option svm logreg manuscript submitted acm tarnpradab et al sentence excluding stopwords max avg total tf idf scores consisting words features designed carry similar information proposed model deep learning baseline neural network methods including lstm cnn deep learning baseline study lstm implemented neural network containing single layer lstm classify sentences input thread news article cnn cnn model sentence classification proposed kim applied input layer initialized pre trained static word embeddings network uses features extracted convolutional layer perform classification addition implemented variant han hierarchical convolutional neural network simply replaces lstm cnn allows examine effectiveness individual network versus unified network evaluation methods report rouge l scores sentence level scores evaluation particular quantitative values method computed precision recall measure note refer rouge metrics r l short metrics commonly duc tac competitions precision scores computed number n grams system summary common corresponding human reference summaries divided total n grams system summary set respectively recall scores calculated way number overlapping n grams divided total n grams human reference summary finally score harmonic mean precision recall use means assess informativeness rouge l measures longest common subsequence words sentences system summary reference summary higher r l likely output summary n grams order reference summary better preserve semantic meaning reference summary sentence level score based labels means sentence true false value indicating sentence summary summarizer labels sentences true reference set actual class system set predicted class sentences regarded true positives sentences labelled true reference set false system set false negatives sentences labelled true system set false reference set false positives finally sentences labelled false system set reference set true negatives table presents confusion matrix table confusion matrix predicted class true false actual class true false true positives tp false positives fp false negatives fn true negatives tn sentence level precision number true positives divided sum true positives false positives sentence level recall number true positives divided sum true positives false negatives lastly manuscript submitted acm improving online forums summarization unifying hierarchical attention networks convolutional neural networks sentence level harmonic mean recall precision sentence level scores basically report classification performance model performance evaluation results discussions hybrid network compared set unsupervised supervised approaches variants hierarchical methods section discuss performance different methods involve traditional machine learning baselines non hierarchical hierarchical deep learning methods explain comparisons observations provide detailed analysis extensive ablation studies presented comparison traditional machine learning baselines unsupervised learning baselines table sentence classification results mead demonstrate good performance mead shown perform previous studies study mead lexrank centroid based meaning sentences contain words cluster centroid considered holding key information increasing likelihood included final summary similar pattern results appears kl sum lsa nonetheless terms rouge evaluation shown table outperformed hierarchical based approaches opinosis poor performance relies heavily redundancy data generate graph meaningful paths end hierarchical approaches appear achieve better performance need sophisticated constraint optimization ilp supervised learning baselines according table pattern high precision low recall generally observed svm logreg results reflect sentences classified true unigrams overlapping reference summaries evaluating higher n grams results matches exist system references considering sentence level scores trip advisor dataset example seen logreg failed extract representative sentences evidenced precision recall lowest comparing traditional models hierarchical based models shown hierarchical models shown better potential classifying selecting salient sentences form summary furthermore traditional baselines possess disadvantage reliance set features feature engineering process handcrafted features usually obtained studying signals observable data able capture traits necessary models learn differentiate classes comparison non hierarchical deep learning methods general lstm outperforms cnn terms sentence classification rouge evaluation particularly sentence classification task lstm shown achieve high precision scores datasets indicates importance learning sequential information obtaining effective representation cnn proven efficient previous studies shown table results evidenced omitting sequential information essentially results inferior performance terms rouge evaluation according table lstm cnn baselines competitive compared hierarchical based methods respect r l scores hierarchical based models generally better performance significant margin observe hierarchical models advantage non hierarchical deep learning methods sense explore hierarchical structure sequential information learned lstm feature extraction cnn manuscript submitted acm tarnpradab et al trip advisor reddit newsroom fig comparison scores hierarchical methods based sentence level scores axis denotes types embeddings fasttext elmo bert y axis denotes scores normalized bar color blue presents han orange presents hcnn gray presents hybrid model comparison hierarchical attention based deep network hierarchical based models compare proposed model state art han model examine hybrid architecture contributes performance gain loss hypothesize unifying lstm cnn encourages leverage short term long term dependencies keys learning generating effective representation summarizer comparison hierarchical convolutional neural network hcnn observe effect excluding long term dependencies captured lstm according table sentence level score shows average performance hybrid network comparative hierarchical methods regardless choice embedding hcnn generally inferior hierarchical models demonstrates lstm layers play key role capturing sequential information essential system understand input documents lstm layers system obtains high level representation cnn captures important n grams sentences independent position sentence thread shown insufficient generate effective representation lstm cnn shown promising avenue improving summarization important note contextual representation employed especially reddit newsroom datasets results shown high precision low recall indicates sentences predicted summary sentence predicted labels correct figure illustrates comparison hierarchical methods respect sentence level scores respect rouge evaluation table shows rouge scores hierarchical model promising hierarchical models hybrid methods outperform datasets displayed figure present example summaries generated hierarchical models figure results indicate hybrid model true labeled sentences labeled correctly higher rest datasets hierarchical models observed behavior hierarchical model terms loss minimized figure illustrate training loss hierarchical model fold note fold model objective loss continuously decreases begins converge early average losses epochs han hcnn hybrid model approximately respectively fluctuations appear hcnn curve hybrid model converges faster larger model complexity manuscript submitted acm improving online forums summarization unifying hierarchical attention networks convolutional neural networks table variance scores threads news articles presented sentence level classification results models precision p recall r scores f reported percentage embedding method p r f p f p r f trip advisor newsroom reddit r ilp sum basic kl sum lsa lexrank mead svm logreg lstm cnn han hcnn hybrid han hcnn hybrid han hcnn hybrid han hcnn hybrid han hcnn hybrid han hcnn hybrid han hcnn hybrid han hcnn hybrid hierarchical static embedding hierarchical elmo baselines hierarchical bert fasttext fasttext fasttext manuscript submitted acm tarnpradab et al table summarization results models scores reported percentage rouge l respectively embedding method trip advisor reddit r l newsroom r l ilp sum basic kl sum lsa lexrank mead opinosis textrank svm logreg lstm cnn han hcnn hybrid han hcnn hybrid han hcnn hybrid han hcnn hybrid han hcnn hybrid han hcnn hybrid han hcnn hybrid han hcnn hybrid r l baselines hierarchical static embedding hierarchical elmo hierarchical bert fasttext fasttext fasttext manuscript submitted acm improving online forums summarization unifying hierarchical attention networks convolutional neural networks trip advisor reddit c newsroom trip advisor e reddit newsroom g r l trip advisor h r l reddit fig comparison scores hierarchical methods based rouge scores axis denotes types embeddings fasttext elmo bert y axis denotes scores normalized bar color blue presents han orange presents hcnn gray presents hybrid model fig scores fig scores fig r l scores r l newsroom manuscript submitted acm tarnpradab et al fig accuracy value computed ratio number correctly labelled sentences total sentences selected model example output summaries generated hierarchical model presented bold correctly labelled sentences manuscript submitted acm improving online forums summarization unifying hierarchical attention networks convolutional neural networks fold fold fold fold fold fold fig shown plateau point forward plots showing convergence training loss fold results epochs displayed models manuscript submitted acm tarnpradab et al ablation study model aspects subsection discuss observations extensive ablation experiments conducted better understand model component analysis comprehensive component analysis performed adding different components baseline methods presented table results reveal model equipped cnn lstm performs poorly datasets indicates leveraging hierarchical structure input document generate document representation helped boost performance specifically hierarchical structure captures information word sentence levels word level representation learned subsequently aggregated form sentence likewise sentence level representation learned subsequently aggregated form document representation hierarchical based models rouge l scores han hcnn comparative union illustrated evident performance gain shift improvement noticeable reddit newsroom datasets larger size trip advisor according table comparing results proposed hybrid model baseline cnn baseline lstm overall improvement trip advisor dataset reddit dataset newsroom dataset respectively table ablation study investigate effect component hierarchical based models scores rouge l compared unit percentage component available model overall improvement red blue hybrid model performance gain compared baseline lstm baseline cnn respectivey model component hierarchical attention lstm cnn trip advisor reddit newsroom data baseline lstm baseline cnn han hcnn hybrid overall improvement effect cnn configurations table model performance different receptive field sizes different number convolutional layers parameters remain fixed important note multiple receptive field sizes output obtained local feature needs concatenated yield representation layer following cnn observe receptive field size outperforms datasets sentence classification rouge evaluation reddit dataset particular decrease performance notable number convolutional layers observe increasing convolutional layers leads drop overall performance number layers highest classification performance output summary quality lowest cnn hybrid model applies receptive field size single convolutional layer manuscript submitted acm improving online forums summarization unifying hierarchical attention networks convolutional neural networks table ablation study investigate effect receptive field size overall performance improvement scores reported sentence level classification sl rouge evaluation r l shaded gray best values non shaded values presents loss compared best values size sl trip advisor r l reddit sl r l sl newsroom r l table ablation study investigate effect number convolutional overall performance improvement scores reported sentence level classification sl rouge evaluation r l shaded gray best values non shaded values presents loss compared best values depth sl trip advisor reddit r l sl r l sl newsroom r l representation learning section discuss impact different embeddings hierarchical models report effect static word embeddings versus contextual embeddings concatenate static contextual embeddings examine joint effect performance investigate outputs model initialized static word embeddings results datasets table superior fasttext classifying sentence labels terms rouge evaluation respect contextual representations trip advisor dataset results bert embeddings yield better classification performance terms sentence level scores elmo remaining datasets sentence level results reveal significant drop recall drop scores consequence aforementioned section indicates system determines sentences worthy sentences correctly labelled table rouge evaluation shows general static word embeddings achieves better performance addition inspired study concatenated static contextual representations level encoders terms sentence level scores general results concatenation significantly affect performance reddit dataset significant improvement noticed han hybrid models respect rouge evaluation results obtained concatenated representation reflect significant improvement effect attention mechanism selecting salient units investigate attention layer validate attention mechanism aids selecting representative units table shows respect rouge evaluation attention mechanism incorporated model performance improved datasets particular larger dataset reddit newsroom difference results attention manuscript submitted acm tarnpradab et al performance nontrivial terms sentence level classification incorporating attention mechanism significantly affect important note proposed hybrid model attends important bigrams word level contiguous sentence pairs sentence level word level attention value bigram influences sentence vector bigram belongs attention weight computed according relevance bigram given sentence context sentence contains bigrams high attention values corresponding sentence vector potentially contain information prominent bigrams sentence level likewise attention values sentence pairs influence resulting thread vector high attention value sentence pair indicates importance relevance thread key concept attention weighted sentence pair goes softmax normalization output indicates likely sentence pair key unit summary figure illustrates visualization words example summary bigram high attention weight highlighted darker shade compared bigrams lower attention sentence glad mellow think difficult filling morning married noon contains bigrams glad attention weights respectively sentence encoder outputs weighted sum bigrams normalized attention weight aforementioned bigrams represented encoded sentence later thread encoder sentence shown highest sentence pairs ranked attention weights finally final summary noticed majority sentences italicized belonging example sentence pairs high ranked weights emphasize necessarily case sentence sentence pair high attention selected final summary high attention weights indicate significance constituent unit words sentence chosen summary determined output layer considers sentence thread representations concatenated observed sentences belong sentence pairs high attention weights higher chance selected final thread summary table ablation study investigate effect attention mechanism results obtained hybrid model attention mechanism applied model opposite case data sentence level r l trip advisor reddit newsroom manuscript submitted acm improving online forums summarization unifying hierarchical attention networks convolutional neural networks fig visualization generated summary forum thread relative attention weights bigram hybrid model entire thread bigrams darker highlight present higher importance attention values bigrams obtained word level attention layer row presents list bigrams ranked according attention values formatted bigram attention weights tuple second row presents list sentence pairs ranked attention weights highest weight bigrams bold underlined highest attention weights row presents final summary lists chronologically ordered extracted sentences sentences italic sentence pairs highest attention weights manuscript submitted acm conclusions tarnpradab et al study present framework based hierarchical attention networks extractively summarize online forum threads proposed networks unify deep neural networks bi lstm cnn obtain representations classify sentence summary worthy proposed approach framed multi document summarization evaluate proposed approach news domain dataset addition online forums experimental results real life datasets demonstrated proposed model outperforms majority baseline methods findings confirm initial hypothesis capability encoders enhanced unified architecture essence bi lstm serves role capture contextual information cnn helps signify prominent units keys pertaining summary strength deep neural networks leveraged achieve effective representations finally conducted extensive experiments investigate effect attention mechanism pretrained embeddings results applying attention high level features extracted compressed cnn contextual embeddings provide promising avenue improving extractive summarization performance acknowledgements references work supported crystal photonics inc grant alan akbik duncan blythe roland vollgraf contextual string embeddings sequence labeling proceedings international conference computational linguistics association computational linguistics santa fe new mexico usa aclweb org anthology federico barrios federico lpez luis argerich rosa wachenchauzer variations similarity function textrank automated summarization arxiv preprint taylor berg kirkpatrick dan gillick dan klein jointly learning extract compress proceedings annual meeting association computational linguistics human language technologies association computational linguistics portland oregon usa aclweb org anthology sumit bhatia prakhar biyani prasenjit mitra summarizing online forum discussions dialog acts individual messages help proceedings conference empirical methods natural language processing emnlp association computational linguistics doha qatar piotr bojanowski edouard grave armand joulin tomas mikolov enriching word vectors subword information transactions association computational linguistics florian boudin hugo mougard benoit favre concept based summarization integer linear programming concept pruning multiple optimal solutions proceedings conference empirical methods natural language processing association computational linguistics lisbon portugal jose camacho collados mohammad taher pilehvar word sense embeddings survey vector representations meaning journal artificial intelligence research ziqiang cao wenjie li sujian li furu wei improving multi document summarization text classification proceedings thirty aaai conference artificial intelligence san francisco california usa aaai press ziqiang cao wenjie li sujian li furu wei yanran li attsum joint learning focusing summarization neural attention proceedings coling international conference computational linguistics technical papers coling organizing committee osaka japan aclweb org anthology giuseppe carenini raymond t ng xiaodong zhou summarizing emails conversational cohesion subjectivity proceedings hlt association computational linguistics columbus ohio aclweb org anthology jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings annual meeting association computational linguistics volume long papers association computational linguistics berlin germany manuscript submitted acm improving online forums summarization unifying hierarchical attention networks convolutional neural networks andr ferreira cruz gil rocha henrique lopes cardoso document representations detection biased news articles proceedings annual acm symposium applied computing brno czech republic sac association computing machinery new york ny usa hoa trang dang karolina owczarzak overview tac update summarization task proceedings text analysis conference tac jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language understanding proceedings conference north american chapter association computational linguistics human language technologies volume long short papers association computational linguistics minneapolis minnesota https yijun duan adam jatowt time comparative summarization news articles proceedings twelfth acm international conference web search data mining melbourne vic australia wsdm association computing machinery new york ny usa john duchi elad hazan yoram singer adaptive subgradient methods online learning stochastic optimization journal machine gnes erkan dragomir r radev lexrank graph based lexical centrality salience text summarization j artif int res dec learning research jul rong en fan kai wei chang cho jui hsieh xiang rui wang chih jen lin liblinear library large linear classification journal machine learning research chong feng fei cai honghui chen maarten de rijke attentive encoder based extractive text summarization proceedings acm international conference information knowledge management torino italy cikm association computing machinery new york ny usa kavita ganesan rouge updated improved measures evaluation summarization tasks kavita ganesan chengxiang zhai jiawei han opinosis graph based approach abstractive summarization highly redundant opinions proceedings international conference computational linguistics association computational linguistics max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies arxiv preprint aria haghighi lucy vanderwende exploring content models multi document summarization proceedings human language technologies annual conference north american chapter association computational linguistics association computational linguistics boulder colorado aclweb org anthology udo hahn inderjeet mani challenges automatic summarization computer geoffrey hinton nitish srivastava kevin swersky neural networks machine learning lecture overview mini batch gradient descent d sepp hochreiter jrgen schmidhuber long short term memory neural computation ya han hu yen liang chen hui ling chou opinion mining online hotel reviews text summarization approach information processing management jyun yu jiang mingyang zhang cheng li michael bendersky nadav golbandi marc najork semantic text matching long form documents world wide web conference san francisco usa www association computing machinery new york ny usa yoon kim convolutional neural networks sentence classification proceedings conference empirical methods natural language processing emnlp association computational linguistics doha qatar diederik p kingma jimmy ba adam method stochastic optimization arxiv preprint hyunsoo lee yunseok choi jee hyong lee attention history based attention abstractive text summarization proceedings annual acm symposium applied computing brno czech republic sac association computing machinery new york ny usa chin yew lin rouge package automatic evaluation summaries text summarization branches association computational linguistics barcelona spain aclweb org anthology hui liu xiaojun wan neural review summarization leveraging user product information proceedings acm international conference information knowledge management beijing china cikm association computing machinery new york ny usa shih hung liu kuan yu chen berlin chen enhanced language modeling proximity sentence relatedness information extractive broadcast news summarization acm trans asian low resour lang inf process article feb pages https wencan luo fei liu zitao liu diane litman automatic summarization student course feedback proceedings conference north american chapter association computational linguistics human language technologies association computational linguistics san diego california manuscript submitted acm tarnpradab et al tomas mikolov edouard grave piotr bojanowski christian puhrsch armand joulin advances pre training distributed word representations proceedings international conference language resources evaluation lrec tomas mikolov ilya sutskever kai chen greg corrado jeffrey dean distributed representations words phrases compositionality proceedings international conference neural information processing systems volume lake tahoe nevada curran associates inc red hook ny usa ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents proceedings thirty aaai conference artificial intelligence san francisco california usa aaai press shashi narayan ronald cardenas nikos papasarantopoulos shay b cohen mirella lapata jiangsheng yu yi chang document modeling external attention sentence extraction proceedings annual meeting association computational linguistics volume long papers association computational linguistics melbourne australia shashi narayan shay b cohen mirella lapata ranking sentences extractive summarization reinforcement learning proceedings conference north american chapter association computational linguistics human language technologies volume long papers association computational linguistics new orleans louisiana preksha nema mitesh khapra anirban laha balaraman ravindran diversity driven attention model query based abstractive summarization arxiv preprint minh tien nguyen tran viet cuong nguyen xuan hoai exploiting user comments document summarization matrix factorization proceedings tenth international symposium information communication technology hanoi ha long bay viet nam soict association computing machinery new york ny usa fumio nihei yukiko nakano yutaka takase meeting extracts discussion summarization based multimodal nonverbal information proceedings acm international conference multimodal interaction tokyo japan icmi association computing machinery new york ny usa fumio nihei yukiko nakano yutaka takase fusing verbal nonverbal information extractive meeting summarization proceedings group interaction frontiers technology boulder co usa association computing machinery new york ny usa article pages jeffrey pennington richard socher christopher manning glove global vectors word representation proceedings conference empirical methods natural language processing emnlp association computational linguistics doha qatar matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word representations proceedings conference north american chapter association computational linguistics human language technologies volume long papers association computational linguistics new orleans louisiana https dragomir r radev hongyan jing magorzata sty daniel tam centroid based summarization multiple documents information processing management sebastian ruder overview gradient descent optimization algorithms arxiv preprint koustav rudra niloy ganguly pawan goyal saptarshi ghosh extracting summarizing situational information twitter social media disasters acm trans web article july pages koustav rudra pawan goyal niloy ganguly prasenjit mitra muhammad imran identifying sub events summarizing disaster related information microblogs international acm sigir conference research development information retrieval alexander m rush sumit chopra jason weston neural attention model abstractive sentence summarization proceedings conference empirical methods natural language processing association computational linguistics lisbon portugal ashish sharma koustav rudra niloy ganguly going content richness verified information aware summarization related microblogs proceedings acm international conference information knowledge management beijing china cikm association computing machinery new york ny usa abhishek kumar singh manish gupta vasudeva varma hybrid memnet extractive summarization proceedings acm conference information knowledge management singapore singapore cikm association computing machinery new york ny usa josef steinberger et al latent semantic analysis text summarization summary evaluation d sansiri tarnpradab fei liu kien hua extractive summarization online forum discussions hierarchical attention networks thirtieth international flairs conference naama tepper anat hashavit maya barnea inbal ronen lior leiba collabot personalized group chat summarization proceedings eleventh acm international conference web search data mining marina del rey usa wsdm association computing machinery new york ny usa justine raju thomas santosh kumar bharti korra sathya babu automatic keyword extraction text summarization e newspapers proceedings international conference informatics analytics pondicherry india association computing machinery manuscript submitted acm improving online forums summarization unifying hierarchical attention networks convolutional neural networks new york ny usa article pages lucy vanderwende hisami suzuki chris brockett ani nenkova sumbasic task focused summarization sentence simplification lexical expansion information processing management lu wang wang ling neural network based abstract generation opinions arguments proceedings conference north american chapter association computational linguistics human language technologies association computational linguistics san diego california sander wubben suzan verberne ej krahmer apj van den bosch facilitating online discussions automatic summarization zichao yang diyi yang chris dyer xiaodong alex smola eduard hovy hierarchical attention networks document classification proceedings conference north american chapter association computational linguistics human language technologies association computational linguistics san diego california matthew d zeiler adadelta adaptive learning rate method corr org amy x zhang justin cranshaw making sense group chat collaborative tagging summarization proc acm hum interact cscw article nov pages ye zhang byron wallace sensitivity analysis practitioners guide convolutional neural networks sentence classification proceedings eighth international joint conference natural language processing volume long papers asian federation natural language processing taipei taiwan aclweb org anthology zhou zhao haojie pan changjie fan yan liu linlin li min yang deng cai abstractive meeting summarization hierarchical adaptive segmental network learning world wide web conference san francisco usa www association computing machinery new york ny usa qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural document summarization jointly learning score select sentences proceedings annual meeting association computational linguistics volume long papers association computational linguistics melbourne australia manuscript submitted acm
