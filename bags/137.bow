query output generating words querying distributed word representations paraphrase generation shuming wei sujian wenjie xuancheng key lab computational linguistics school eecs peking university learning lab beijing institute big data research peking university computing hong kong polytechnic university shumingma xusun lisujian edu polyu edu abstract recent approaches use sequence model paraphrase existing sequence sequence tion model tends memorize words patterns training dataset instead ing meaning words generated sentences cally correct semantically improper work introduce novel model based encoder decoder framework called word embedding attention network wean proposed model generates words ing distributed word representations ral word embeddings hoping capturing meaning according words following previous work evaluate model paraphrase oriented tasks text plication short text abstractive rization experimental results model outperforms sequence sequence baseline bleu score english text simplication datasets score nese summarization dataset model achieves state art performances benchmark datasets introduction paraphrase restatement meaning text words natural language generation tasks paraphrase orientated text simplication short text tion text simplication text easier read understand especially poor ers short text summarization generate brief sentence describe short texts posts social media recent approaches use sequence sequence model paraphrase eration prakash cao code available com lancopku wean compresses source text information dense vectors neural encoder neural decoder generates target text pressed vectors neural network models achieve cess paraphrase generation major problems problem isting sequence sequence model tends orize words patterns training dataset instead meaning words main reason word generator output layer decoder model semantic information word generator consists linear transformation softmax operation converts recurrent neural network rnn output small dimension larger dimension words vocabulary dimension sents score word latent tion word generator word dependent score irrelevant scores word synonyms great difference means word generator learns word lationship words problem word generator huge number parameters suppose sequence sequence model den size vocabulary size word generator million ters larger parts encoder decoder model total huge size parameters result slow convergence cause lot parameters learned distributed framework parameters model bandwidth memory consumes tackle problems propose novel model called word embedding attention network wean word generator wean attention based instead simple linear max operation attention based word erator rnn output query candidate words values corresponding word order predict representations keys word attention mechanism lect value matching query means querying keys way model erates words according distributed word representations neural word embeddings retrieval style traditional ative style model able capture mantic meaning word referring bedding attention mechanism smaller number parameters compared linear transformation directly rnn output space vocabulary space reduction parameters increase vergence rate speed training process word embedding updated sources input encoder input decoder query output layer following previous work cao evaluate model paraphrase oriented tasks text simplication short text abstractive summarization experimental results model outperforms sequence sequence baseline bleu score english text simplication datasets score chinese marization dataset model achieves state art performances mark datasets proposed model propose novel model based decoder framework generates words querying distributed word representations attention mechanism section rst present overview model architecture explain details word ation especially way query word dings overview word embedding attention network based encoder decoder framework consists components source text encoder text decoder figure illustration model given source texts encoder presses source texts dense representation vectors decoder generates paraphrased texts predict word decoder uses den output query word embeddings word embeddings assess candidate words return word embedding matches query selected word emitted predicted token embedding input lstm time step propagation word embedding dated sources input encoder input decoder query layer details wean following subsection encoder decoder goal source text encoder vide series dense representation complex source texts decoder model source text encoder long short term memory network lstm produces dense resentation source text goal target text decoder generate series paraphrased words dense resentation source texts fisrt lstm decoder compute dense representation erated words dense representations fed attention layer bahdanau generate context vector tures context information source texts tion vector calculated weighted sum encoder hidden states tihi attentive score decoder hidden state encoder hidden state way respectively represent context information source texts target texts tth time step word generation querying word embedding current sequence sequence model word generator computes distribution output words generative style sof hard admission hard key value easy happy hard competitive query admission extremely competitive figure overview word embedding attention network trainable parameter matrix hidden size number words vocabulary vocabulary large number parameters huge model generates words retrieval style traditional generative style querying word embeddings denote combination source context vector target context vector query implementation select general attention function relevance score function based performance validation sets value pair highest score lected test stage decoder generates key tth predicted word inputs value lstm unit time step training stage scores normalized word probability distribution sof candidate words corresponding embeddings paired key value pairs number candidate words details termine set candidate words section model uses query key value pairs evaluating vance query word tor score function query process regarded attentive selection word embeddings borrow attention energy functions luong vance score function waei weei dot general concat trainable parameter matrices trainable parameter vector selection candidate key value pairs described section model generates words retrieval style selects word according embedding set candidate key value pairs details obtain set candidate key value pairs extract vocabulary source text training set select frequent words candidate words reuse embeddings decoder inputs values date words means decoder input predicted output share lary word embeddings use pretrained word embeddings model parameters learned scratch training generator retrieval style wean differentiable sequence sequence model objective training minimize cross entropy predicted word bility distribution golden hot tion log use adam optimization method train model default hyper parameters learning rate experiments following previous work cao test model following paraphrase orientated tasks text simplication short text abstractive summarization text simplication datasets datasets alignments tween english wikipedia simple glish wikipedia website simple english wikipedia built children adults learning english language cles composed easy words short tences simple english wikipedia natural public simplied text corpus parallel wikipedia simplication corpus pwkp pwkp zhu widely benchmark evaluating text simplication systems consists aligned complex text english wikipedia aug simple text simple wikipedia aug dataset contains tence pairs words average complex sentence words following previous simple sentence work zhang lapata remove duplicate sentence pairs split pus pairs training pairs validation pairs test english wikipedia simple english wikipedia sew sew licly available dataset provided hwang build corpus rst align complex simple sentence pairs score semantic similarity complex tence simple sentence classify wikipedia org wikipedia org sentence pair good good partial partial bad match following previous work nisioi discard classied matches use good matches partial matches scaled threshold greater corpus contains good matches good partial matches use corpus ing set dataset provided validation set test set validation set consists sentence pairs test set contains sentence pairs complex sentence paired reference simplied sentences provided amazon mechanical turk workers evaluation metrics following previous work nisioi evaluate model ferent metrics tasks automatic evaluation use bleu score papineni automatic evaluation metric bleu widely metric machine translation text plication measures agreement model outputs gold erences references single multiple experiments ences single pwkp multiple sew human evaluation human evaluation sential evaluate quality model outputs following nisioi zhang ask human raters rate simplied text dimensions fluency adequacy simplicity fluency assesses outputs cally right formed adequacy resents meaning preservation plied text scores uency adequacy range bad good simplicity shows simpler model outputs source text ranges settings proposed model based decoder framework encoder implemented lstm decoder based lstm luong style attention luong pwkp pbmt wubben hybrid narayan gardent encdeca zhang lapata dress zhang lapata dress zhang lapata implementation wean proposal bleu table automatic evaluation model related systems pwkp datasets results ported test sets sew pbmt wubben hybrid narayan gardent sbmt sari nts nisioi nts nisioi encdeca zhang lapata dress zhang lapata dress zhang lapata implementation wean proposal bleu pwkp nts dress wean reference fluency adequacy simplicity sew fluency adequacy simplicity pbmt sbmt sari nts dress wean reference table human evaluation model lated systems pwkp sew datasets results reported test sets sentence simplication models encdeca model based implemented decoder attention zhang lapata table automatic evaluation model related systems sew datasets results reported test sets pbmt wubben phrase based machine translation model reranks outputs tune hyper parameter development set model lstm layers hidden size lstm embedding size use adam optimizer kingma learn parameters batch size set set dropout rate srivastava gradients clipped norm exceeds baselines compare model neural text simplication systems implementation sequence sequence model attention mechanism popular ral model text generation nts nts nisioi sequence sequence model tra mechanism like prediction ranking nts uses pretrain dress dress zhang lapata deep reinforcement learning hybrid narayan gardent brid approach combines deep tics mono lingual machine translation sbmt sari based machine translation model trained ppdb dataset ganitkevitch tuned sari results compare wean state art els text simplication table table summarize results automatic tion pwkp dataset compare wean pbmt hybrid encdeca dress wean achieves bleu score performing previous systems sew dataset compare wean pbmt hybrid sbmt sari neural models scribed public release code pbmt sbmt sari fortunately provides predictions sbmt sari sew test set compare model systems lcsts rnn rnn cont rnn copynet rnn wean table rouge score lcsts test set denote rouge respectively models sufx table word based rest els character based shows neural models better formance bleu wean achieves best bleu score perform human evaluation wean related systems results shown table dress based forcement learning encourages uency simplicity relevance outputs fore achieves high score human uation wean gains better score dress wean generates equate simpler outputs reference pwkp predictions sbmt sari adequate compared systems sew general wean outperforms systems considering balance ency adequate simplicity conduct nicance tests based test signicance tests suggest wean signicant improvement baseline dress dimension pwkp dress dimension ency nts dimension simplicity dress dimension large scale text summarization dataset large scale chinese social media short text summarization dataset lcsts lcsts constructed dataset sists text summary pairs constructed famous chinese social media website called sina weibo split parts pairs pairs pairs iii text summary pairs iii manually annotated relevant scores ranged reserve pairs scores leaving pairs pairs iii following previous work use training set validation set iii test set metric evaluation metrics rouge evaluation score lin hovy lar summarization evaluation metrics compare automatically produced summary reference summaries computing overlapping lexical units including unigram gram trigram longest common subsequence lcs following previous work rush use igram gram rouge lcs evaluation metrics reported experimental results settings vocabularies extracted training sets source contents summaries share vocabularies tune parameters based rouge scores order alleviate risk validation sets word segmentation mistakes split nese sentences characters prune cabulary size covers common characters set word ding size hidden size ber lstm layers encoder number lstm layers decoder batch size use dropout srivastava dataset following previous work implement beam search optimization set beam size baselines compare model state art baselines rnn rnn cont sequence sequence baseline gru encoder coder provided com param pwkp ewsew lcsts wean table number parameters layer numbers rest parameters wean rnn dist chen based neural model attention mechanism focuses different parts source content copynet incorporates copy mechanism allow ated summary copied source tent srb sequence sequence based neural model improving semantic relevance input text output summary drgd deep recurrent generative decoder model combining coder variational autoencoder sequence sequence model tion mechanism implementation results report rouge score model baseline models test sets ble summarizes comparison model baselines model achieves score rouge outperforming ous models compare model shows sequence sequence model model signicant outperforms sequence baseline large margin compare model related models drgd obtains score rouge model relative gain rouge state art models state art model training curve wean epoch figure training curve wean pwkp validation set analysis discussion reducing parameters wean reduces large number eters output layer analyze ter reduction compare wean model sequence sequence model table lists number parameters output layers models pwkp ewsewhave vocabulary size words hidden size resulting parameters lcsts vocabulary size hidden size parameters layers wean parameter trices parameter vector tion regard vocabulary size parameters pwkp ewsew parameters lcsts wean extra parameters model speeding convergence figure shows training curve wean pwkp validation set wean achieve near optimal score epochs takes epochs achieve optimal score wean faster convergence rate compared faster training speed wean suffer loss bleu improve bleu score yoghurt oryogurt isadairyproduct produced bybacterial fermentation ofmilk yoghurt oryogurt isadairyproduct madebybacterial fermentation ofmilk oryoghurtisadairy product producedbybacterial fermentation ofmilk itismadebybacterial fermentation ofmilk yoghurt oryogurt isadairyproduct producedbybacterial fermentation source reference nts nts pbmt sbmt sari yogurtoryogurt isadairy product drawnupbybacterial fermentation ofmilk wean source yoghurt oryogurt isadairyproduct madebybacterial fermentation ofmilk depending context closely related meaning constituent citizen residing area governed represented served politician thisisrestricted tocitizens whoelected thepolitician thewordconstituentcanalsobeusedtorefertoacitizenwholivesintheareathat governed represented served politician word restricted tocitizens whoelected thepolitician depending context closely related meaning constituent citizen living area governed represented served politician thisisrestricted tocitizens whoelected thepolitician thisisrestricted tocitizens whoelected thepolitician depending context meaning closely related siemens martin citizen living area governed shurba restricted topeople whoelected reference nts nts pbmt sbmt sari terms context closely related sense component citizen living area covered makeup ifnot served byapolicy whoelected thepolicy depending context closely related meaning constituent citizenwholivesintheareagoverned represented orotherwiseservedbyapolitician thewordisrestricted tocitizens whoelected thepolitician wean table examples different text simplication system outputs sew dataset differences source texts shown bold case study close original meaning table shows examples different text plication system outputs sew rst example nts nts pbmt miss essential constituents sentences incomplete uent sbmt sari erates uent sentence output preserve original meaning predicted tence wean uent simple reference second example omits words lacks lot information pbmt generates vant words like siemens martin shurba hurts uency adequacy generated sentence sbmt sari able generate uent sentence meaning ferent source text cult understand compared statistic model wean generates uent sentence wean capture semantic ing word querying word embeddings generated sentence semantically correct related work work related encoder decoder framework cho attention mechanism bahdanau decoder framework like sequence sequence model achieved success machine lation sutskever jean luong lin text marization rush chopra nallapati wang sun natural language processing tasks liu methods improve neural attention model jean luong zhu constructs wikipedia dataset proposes tree based simplication model woodsend lapata introduces data driven model based quasi synchronous grammar captures structural mismatches complex rewrite operations wubben presents method text simplication phrase based machine translation ranking outputs kauchak proposes text simplication corpus evaluates language modeling text simplication proposed corpus narayan gardent propose hybrid approach sentence simplication combines deep semantics monolingual chine translation hwang introduces parallel simplication corpus evaluating similarity source text text based wordnet glavas stajner propose unsupervised approach ical simplication makes use word tors require regular corpora design automatic metrics text plication recently works focus neural sequence sequence model nisioi present sequence sequence model ranks predictions bleu sari zhang lapata propose deep forcement learning model improve ity uency adequacy simplied texts cao introduce novel sequence sequence model join copying restricted eration text simplication rush rst attention based encoder compress texts neural network language decoder generate summaries ing work recurrent encoder introduced text summarization gained better mance lopyrev chopra wards chinese texts built large corpus chinese short text summarization deal unknown word problem nallapati proposed generator pointer model decoder able generate words source texts solved issue incorporating copying mechanism conclusion propose novel model based decoder framework generates words querying distributed word representations imental results model outperforms sequence sequence baseline bleu score english text cation datasets score chinese summarization dataset model achieves state art performances benchmark datasets acknowledgements work supported national ral science foundation china national high technology research opment program china program national thousand young talents program sun sponding author paper references dzmitry bahdanau kyunghyun cho yoshua neural machine translation corr bengio jointly learning align translate ziqiang cao chuwei luo wenjie sujian joint copying restricted generation paraphrase proceedings thirty aaai conference articial intelligence pages qian chen xiaodan zhu zhenhua ling wei hui jiang distraction based neural networks modeling documents proceedings international joint conference articial gence ijcai aaai new york jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings annual meeting sociation computational linguistics acl august berlin germany volume long papers kyunghyun cho bart van merrienboer aglar gulcehre dzmitry bahdanau fethi bougares ger schwenk yoshua bengio learning phrase representations rnn encoder decoder proceedings statistical machine translation conference empirical methods natural language processing emnlp pages sumit chopra michael auli alexander rush abstractive sentence summarization tentive recurrent neural networks naacl hlt conference north american chapter association computational guistics human language technologies pages juri ganitkevitch benjamin van durme chris callison burch paraphrase database human language technologies ference north american chapter ciation computational linguistics proceedings pages ppdb goran glavas sanja stajner simplifying lexical simplication need simplied pora proceedings annual meeting association computational linguistics acl pages jiatao zhengdong hang victor incorporating copying mechanism proceedings sequence sequence learning annual meeting association putational linguistics acl baotian qingcai chen fangze zhu sts large scale chinese short text summarization dataset proceedings conference empirical methods natural language ing emnlp lisbon portugal september pages william hwang hannaneh hajishirzi mari ostendorf wei aligning sentences dard wikipedia simple wikipedia naacl hlt pages sebastien jean kyunghyun cho roland memisevic yoshua bengio large vocabulary neural machine translation proceedings annual meeting sociation computational linguistics acl pages thang luong hieu pham christopher ning effective approaches attention based proceedings neural machine translation conference empirical methods natural language processing emnlp pages shuming sun semantic vance based neural network text summarization text simplication corr shuming sun jingjing houfeng wang wenjie improving semantic relevance sequence sequence learning nese social media text summarization ings annual meeting association computational linguistics acl ver canada july august volume short papers pages ramesh nallapati bowen zhou ccero nogueira dos santos aglar gulcehre bing xiang abstractive text summarization sequence proceedings sequence rnns signll conference computational natural language learning conll berlin germany august pages david kauchak improving text simplication language modeling unsimplied text data proceedings annual meeting ciation computational linguistics acl pages shashi narayan claire gardent hybrid plication deep semantics machine proceedings annual lation ing association computational tics acl pages diederik kingma jimmy adam corr method stochastic optimization piji wai lam lidong bing zihao wang deep recurrent generative decoder stractive text summarization proceedings conference empirical methods natural language processing emnlp copenhagen denmark september pages chin yew lin eduard hovy matic evaluation summaries gram occurrence statistics human language ogy conference north american chapter association computational linguistics naacl junyang lin shuming sun decoding history based adaptive control attention neural machine translation corr tianyu liu kexiang wang lei sha baobao chang zhifang sui table text tion structure aware learning corr sergiu nisioi sanja stajner simone paolo ponzetto liviu dinu exploring neural text plication models proceedings nual meeting association computational linguistics acl pages kishore papineni salim roukos todd ward jing zhu bleu method automatic uation machine translation proceedings annual meeting association tational linguistics pages aaditya prakash sadid hasan kathy lee vivek datla ashequl qadir joey liu oladimeji farri neural paraphrase generation stacked coling residual lstm networks international conference computational guistics proceedings conference cal papers december osaka japan pages alexander rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing emnlp lisbon portugal september pages konstantin lopyrev generating news corr lines recurrent neural networks nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan nov dropout simple way prevent neural wei courtney napoles ellie pavlick quanze chen chris callison burch optimizing statistical machine translation text simplication tacl xingxing zhang mirella lapata tence simplication deep reinforcement ing proceedings conference pirical methods natural language processing emnlp copenhagen denmark september pages zhemin zhu delphine bernhard iryna gurevych monolingual tree based translation model sentence simplication coling pages networks overtting learning research journal machine sun xuancheng ren shuming houfeng wang meprop sparsied tion accelerated deep learning reduced proceedings international tting conference machine learning icml ney nsw australia august pages sun xuancheng ren shuming bingzhen wei wei houfeng wang training cation model simplication deep learning minimal effort propagation method corr sun bingzhen wei xuancheng ren shuming label embedding network learning bel representation soft training deep networks corr ilya sutskever oriol vinyals quoc sequence sequence learning neural works advances neural information ing systems annual conference neural mation processing systems pages sho takase jun suzuki naoaki okazaki tsutomu rao masaaki nagata neural headline generation abstract meaning representation proceedings conference empirical methods natural language processing emnlp austin texas usa november pages kexiang wang tianyu liu zhifang sui baobao chang afnity preserving random walk multi document summarization proceedings conference empirical methods ural language processing emnlp hagen denmark september pages kristian woodsend mirella lapata ing simplify sentences quasi synchronous grammar integer programming proceedings conference empirical methods natural language processing emnlp pages sander wubben antal van den bosch emiel krahmer sentence simplication lingual machine translation nual meeting association computational linguistics proceedings conference pages jingjing sun xuancheng ren junyang lin binzhen wei wei gan promoting generative adversarial network corr erating informative diversied text
