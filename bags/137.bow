r m l c s c v v x r query output generating words querying distributed word representations paraphrase generation shuming xu wei sujian wenjie xuancheng key lab computational linguistics school eecs peking university learning lab beijing institute big data research peking university computing hong kong polytechnic university shumingma xusun lisujian edu cn polyu edu hk abstract recent approaches use sequence model paraphrase existing sequence sequence tion model tends memorize words patterns training dataset instead ing meaning words generated sentences cally correct semantically improper work introduce novel model based encoder decoder framework called word embedding attention network wean proposed model generates words ing distributed word representations e ral word embeddings hoping capturing meaning according words following previous work evaluate model paraphrase oriented tasks text plication short text abstractive rization experimental results model outperforms sequence sequence baseline bleu score english text simplication datasets score nese summarization dataset model achieves state art performances benchmark datasets introduction paraphrase restatement meaning text words natural language generation tasks paraphrase orientated text simplication short text tion text simplication text easier read understand especially poor ers short text summarization generate brief sentence describe short texts e posts social media recent approaches use sequence sequence model paraphrase eration prakash et al cao et al code available com lancopku wean compresses source text information dense vectors neural encoder neural decoder generates target text pressed vectors neural network models achieve cess paraphrase generation major problems problem isting sequence sequence model tends orize words patterns training dataset instead meaning words main reason word generator e output layer decoder model semantic information word generator consists linear transformation softmax operation converts recurrent neural network rnn output small dimension e larger dimension e words vocabulary dimension sents score word latent tion word generator word dependent score irrelevant scores word synonyms great difference means word generator learns word lationship words problem word generator huge number parameters suppose sequence sequence model den size vocabulary size word generator million ters larger parts encoder decoder model total huge size parameters result slow convergence cause lot parameters learned distributed framework parameters model bandwidth memory consumes tackle problems propose novel model called word embedding attention network wean word generator wean attention based instead simple linear max operation attention based word erator rnn output query candidate words values corresponding word order predict representations keys word attention mechanism lect value matching query means querying keys way model erates words according distributed word representations e neural word embeddings retrieval style traditional ative style model able capture mantic meaning word referring bedding attention mechanism smaller number parameters compared linear transformation directly rnn output space vocabulary space reduction parameters increase vergence rate speed training process word embedding updated sources input encoder input decoder query output layer following previous work cao et al evaluate model paraphrase oriented tasks text simplication short text abstractive summarization experimental results model outperforms sequence sequence baseline bleu score english text simplication datasets score chinese marization dataset model achieves state art performances mark datasets proposed model propose novel model based decoder framework generates words querying distributed word representations attention mechanism section rst present overview model architecture explain details word ation especially way query word dings overview word embedding attention network based encoder decoder framework consists components source text encoder text decoder figure illustration model given source texts encoder presses source texts dense representation vectors decoder generates paraphrased texts predict word decoder uses den output query word embeddings word embeddings assess candidate words return word embedding matches query selected word emitted predicted token embedding input lstm time step propagation word embedding dated sources input encoder input decoder query layer details wean following subsection encoder decoder goal source text encoder vide series dense representation complex source texts decoder model source text encoder long short term memory network lstm produces dense resentation hn source text xn goal target text decoder generate series paraphrased words dense resentation source texts fisrt lstm decoder compute dense representation erated words st dense representations fed attention layer bahdanau et al generate context vector ct tures context information source texts tion vector ct calculated weighted sum encoder hidden states ct tihi n x ti hi n hj p hi attentive score decoder hidden state st encoder hidden state hi way ct st respectively represent context information source texts target texts tth time step word generation querying word embedding current sequence sequence model word generator computes distribution output words yt generative style sof st hard admission hard key value easy happy hard competitive query admission extremely competitive figure overview word embedding attention network w trainable parameter matrix k hidden size v number words vocabulary vocabulary large number parameters huge model generates words retrieval style traditional generative style querying word embeddings denote combination source context vector ct target context vector st query qt implementation select general attention function relevance score function based performance validation sets value pair highest score wt et lected test stage decoder generates key wt tth predicted word inputs value et lstm unit t time step training stage scores normalized word probability distribution qt sof ei candidate words wi corresponding embeddings ei paired key value pairs wi n n number candidate words details termine set candidate words section model uses qt query key value pairs wi n evaluating vance query qt word tor ei score function ei query process regarded attentive selection word embeddings borrow attention energy functions luong et al vance score function ei ei t ei qt t waei vt weei dot general concat wq trainable parameter matrices vt trainable parameter vector selection candidate key value pairs described section model generates words retrieval style selects word according embedding set candidate key value pairs details obtain set candidate key value pairs extract vocabulary source text training set select n frequent words candidate words reuse embeddings decoder inputs values date words means decoder input predicted output share lary word embeddings use pretrained word embeddings model parameters learned scratch training generator retrieval style wean differentiable sequence sequence model objective training minimize cross entropy predicted word bility distribution golden hot tion l yi log x use adam optimization method train model default hyper parameters learning rate experiments following previous work cao et al test model following paraphrase orientated tasks text simplication short text abstractive summarization text simplication datasets datasets alignments tween english wikipedia simple glish wikipedia website simple english wikipedia built children adults learning english language cles composed easy words short tences simple english wikipedia natural public simplied text corpus parallel wikipedia simplication corpus pwkp pwkp zhu et al widely benchmark evaluating text simplication systems consists aligned complex text english wikipedia aug simple text simple wikipedia aug dataset contains tence pairs words average complex sentence words following previous simple sentence work zhang lapata remove duplicate sentence pairs split pus pairs training pairs validation pairs test english wikipedia simple english wikipedia ew sew ew sew licly available dataset provided hwang et al build corpus rst align complex simple sentence pairs score semantic similarity complex tence simple sentence classify wikipedia org wikipedia org sentence pair good good partial partial bad match following previous work nisioi et al discard classied matches use good matches partial matches scaled threshold greater corpus contains k good matches k good partial matches use corpus ing set dataset provided xu et al xu et al validation set test set validation set consists sentence pairs test set contains sentence pairs complex sentence paired reference simplied sentences provided amazon mechanical turk workers evaluation metrics following previous work nisioi et al hu et al evaluate model ferent metrics tasks automatic evaluation use bleu score papineni et al automatic evaluation metric bleu widely metric machine translation text plication measures agreement model outputs gold erences references single multiple experiments ences single pwkp multiple ew sew human evaluation human evaluation sential evaluate quality model outputs following nisioi et al zhang et al ask human raters rate simplied text dimensions fluency adequacy simplicity fluency assesses outputs cally right formed adequacy resents meaning preservation plied text scores uency adequacy range bad good simplicity shows simpler model outputs source text ranges settings proposed model based decoder framework encoder implemented lstm decoder based lstm luong style attention luong et al pwkp pbmt wubben et al hybrid narayan gardent encdeca zhang lapata dress zhang lapata dress ls zhang lapata implementation wean proposal bleu table automatic evaluation model related systems pwkp datasets results ported test sets ew sew pbmt r wubben et al hybrid narayan gardent sbmt sari xu et al nts nisioi et al nts nisioi et al encdeca zhang lapata dress zhang lapata dress ls zhang lapata implementation wean proposal bleu pwkp nts dress ls wean reference fluency adequacy simplicity ew sew fluency adequacy simplicity pbmt r sbmt sari nts dress ls wean reference table human evaluation model lated systems pwkp ew sew datasets results reported test sets sentence simplication models encdeca model based implemented decoder attention zhang lapata table automatic evaluation model related systems ew sew datasets results reported test sets pbmt r wubben et al phrase based machine translation model reranks outputs tune hyper parameter development set model lstm layers hidden size lstm embedding size use adam optimizer kingma ba learn parameters batch size set set dropout rate srivastava et al gradients clipped norm exceeds baselines compare model neural text simplication systems implementation sequence sequence model attention mechanism popular ral model text generation nts nts nisioi et al sequence sequence model tra mechanism like prediction ranking nts uses pretrain dress dress ls zhang lapata deep reinforcement learning hybrid narayan gardent brid approach combines deep tics mono lingual machine translation sbmt sari xu et al based machine translation model trained ppdb dataset ganitkevitch et al tuned sari results compare wean state art els text simplication table table summarize results automatic tion pwkp dataset compare wean pbmt hybrid encdeca dress ls wean achieves bleu score performing previous systems sew dataset compare wean pbmt r hybrid sbmt sari neural models scribed nd public release code pbmt r sbmt sari fortunately xu et al provides predictions r sbmt sari ew sew test set compare model systems lcsts r l rnn et al et al rnn cont et al rnn et al et al copynet et al et al rnn et al et al wean table rouge score lcsts test set r l denote rouge l respectively models sufx w table word based rest els character based shows neural models better formance bleu wean achieves best bleu score perform human evaluation wean related systems results shown table dress ls based forcement learning encourages uency simplicity relevance outputs fore achieves high score human uation wean gains better score dress ls wean generates equate simpler outputs reference pwkp predictions sbmt sari adequate compared systems ew sew general wean outperforms systems considering balance ency adequate simplicity conduct nicance tests based t test signicance tests suggest wean signicant improvement baseline p dress ls dimension pwkp p dress ls dimension ency p nts dimension simplicity p dress ls dimension large scale text summarization dataset large scale chinese social media short text summarization dataset lcsts lcsts constructed hu et al dataset sists text summary pairs constructed famous chinese social media website called sina weibo split parts pairs pairs ii pairs iii text summary pairs ii iii manually annotated relevant scores ranged reserve pairs scores leaving pairs ii pairs iii following previous work hu et al use training set ii validation set iii test set metric evaluation metrics rouge evaluation score lin hovy lar summarization evaluation metrics compare automatically produced summary reference summaries computing overlapping lexical units including unigram gram trigram longest common subsequence lcs following previous work rush et al hu et al use igram bi gram rouge l lcs evaluation metrics reported experimental results settings vocabularies extracted training sets source contents summaries share vocabularies tune parameters based rouge scores order alleviate risk validation sets word segmentation mistakes split nese sentences characters prune cabulary size covers common characters set word ding size hidden size ber lstm layers encoder number lstm layers decoder batch size use dropout srivastava et al dataset following previous work li et al implement beam search optimization set beam size baselines compare model state art baselines rnn rnn cont sequence sequence baseline gru encoder coder provided hu et al com param pwkp ewsew lcsts m m m m m m wean table number parameters layer numbers rest parameters wean rnn dist chen et al based neural model attention mechanism focuses different parts source content copynet gu et al incorporates copy mechanism allow ated summary copied source tent srb ma et al sequence sequence based neural model improving semantic relevance input text output summary drgd li et al deep recurrent generative decoder model combining coder variational autoencoder sequence sequence model tion mechanism implementation results report rouge score model baseline models test sets ble summarizes comparison model baselines model achieves score rouge l outperforming ous models compare model shows sequence sequence model model signicant outperforms sequence baseline large margin l compare model related models drgd li et al obtains score rouge l model relative gain rouge l state art models state art model training curve u e l b wean epoch figure training curve wean pwkp validation set analysis discussion reducing parameters wean reduces large number eters output layer analyze ter reduction compare wean model sequence sequence model table lists number parameters output layers models pwkp ewsewhave vocabulary size words hidden size resulting parameters lcsts vocabulary size hidden size parameters layers wean parameter trices parameter vector tion regard vocabulary size parameters pwkp ewsew parameters lcsts wean extra parameters model speeding convergence figure shows training curve wean pwkp validation set wean achieve near optimal score epochs takes epochs achieve optimal score wean faster convergence rate compared faster training speed wean suffer loss bleu improve bleu score yoghurt oryogurt isadairyproduct produced bybacterial fermentation ofmilk yoghurt oryogurt isadairyproduct madebybacterial fermentation ofmilk oryoghurtisadairy product producedbybacterial fermentation ofmilk itismadebybacterial fermentation ofmilk yoghurt oryogurt isadairyproduct producedbybacterial fermentation source reference nts nts pbmt r sbmt sari yogurtoryogurt isadairy product drawnupbybacterial fermentation ofmilk wean source yoghurt oryogurt isadairyproduct madebybacterial fermentation ofmilk depending context closely related meaning constituent citizen residing area governed represented served politician thisisrestricted tocitizens whoelected thepolitician thewordconstituentcanalsobeusedtorefertoacitizenwholivesintheareathat governed represented served politician word restricted tocitizens whoelected thepolitician depending context closely related meaning constituent citizen living area governed represented served politician thisisrestricted tocitizens whoelected thepolitician thisisrestricted tocitizens whoelected thepolitician depending context meaning closely related siemens martin citizen living area governed shurba restricted topeople whoelected reference nts nts pbmt r sbmt sari terms context closely related sense component citizen living area covered makeup ifnot served byapolicy whoelected thepolicy depending context closely related meaning constituent citizenwholivesintheareagoverned represented orotherwiseservedbyapolitician thewordisrestricted tocitizens whoelected thepolitician wean table examples different text simplication system outputs ew sew dataset differences source texts shown bold case study close original meaning table shows examples different text plication system outputs ew sew rst example nts nts pbmt r miss essential constituents sentences incomplete uent sbmt sari erates uent sentence output preserve original meaning predicted tence wean uent simple reference second example omits words lacks lot information pbmt r generates vant words like siemens martin shurba hurts uency adequacy generated sentence sbmt sari able generate uent sentence meaning ferent source text cult understand compared statistic model wean generates uent sentence wean capture semantic ing word querying word embeddings generated sentence semantically correct related work work related encoder decoder framework cho et al attention mechanism bahdanau et al decoder framework like sequence sequence model achieved success machine lation sutskever et al jean et al luong et al lin et al text marization rush et al chopra et al nallapati et al wang et al ma sun natural language processing tasks liu et al methods improve neural attention model jean et al luong et al zhu et al constructs wikipedia dataset proposes tree based simplication model woodsend lapata introduces data driven model based quasi synchronous grammar captures structural mismatches complex rewrite operations wubben et al presents method text simplication phrase based machine translation ranking outputs kauchak proposes text simplication corpus evaluates language modeling text simplication proposed corpus narayan gardent propose hybrid approach sentence simplication combines deep semantics monolingual chine translation hwang et al introduces parallel simplication corpus evaluating similarity source text ed text based wordnet glavas stajner propose unsupervised approach ical simplication makes use word tors require regular corpora xu et al design automatic metrics text plication recently works focus neural sequence sequence model nisioi et al present sequence sequence model ranks predictions bleu sari zhang lapata propose deep forcement learning model improve ity uency adequacy simplied texts cao et al introduce novel sequence sequence model join copying restricted eration text simplication rush et al rst attention based encoder compress texts neural network language decoder generate summaries ing work recurrent encoder introduced text summarization gained better mance lopyrev chopra et al wards chinese texts hu et al built large corpus chinese short text summarization deal unknown word problem nallapati et al proposed generator pointer model decoder able generate words source texts gu et al solved issue incorporating copying mechanism conclusion propose novel model based decoder framework generates words querying distributed word representations imental results model outperforms sequence sequence baseline bleu score english text cation datasets score chinese summarization dataset model achieves state art performances benchmark datasets acknowledgements work supported national ral science foundation china national high technology research opment program china program national thousand young talents program xu sun sponding author paper references dzmitry bahdanau kyunghyun cho yoshua neural machine translation corr bengio jointly learning align translate ziqiang cao chuwei luo wenjie li sujian li joint copying restricted generation paraphrase proceedings thirty aaai conference articial intelligence pages qian chen xiaodan zhu zhenhua ling si wei hui jiang distraction based neural networks modeling documents proceedings international joint conference articial gence ijcai aaai new york ny jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings annual meeting sociation computational linguistics acl august berlin germany volume long papers kyunghyun cho bart van merrienboer c aglar gulcehre dzmitry bahdanau fethi bougares ger schwenk yoshua bengio learning phrase representations rnn encoder decoder proceedings statistical machine translation conference empirical methods natural language processing emnlp pages sumit chopra michael auli alexander m rush abstractive sentence summarization tentive recurrent neural networks naacl hlt conference north american chapter association computational guistics human language technologies pages juri ganitkevitch benjamin van durme chris callison burch paraphrase database human language technologies ference north american chapter ciation computational linguistics proceedings pages ppdb goran glavas sanja stajner simplifying lexical simplication need simplied pora proceedings annual meeting association computational linguistics acl pages jiatao gu zhengdong lu hang li victor o k incorporating copying mechanism li proceedings sequence sequence learning annual meeting association putational linguistics acl baotian hu qingcai chen fangze zhu sts large scale chinese short text summarization dataset proceedings conference empirical methods natural language ing emnlp lisbon portugal september pages william hwang hannaneh hajishirzi mari ostendorf wei wu aligning sentences dard wikipedia simple wikipedia naacl hlt pages sebastien jean kyunghyun cho roland memisevic yoshua bengio large vocabulary neural machine translation proceedings annual meeting sociation computational linguistics acl pages thang luong hieu pham christopher d ning effective approaches attention based proceedings neural machine translation conference empirical methods natural language processing emnlp pages shuming ma xu sun semantic vance based neural network text summarization text simplication corr shuming ma xu sun jingjing xu houfeng wang wenjie li qi su improving semantic relevance sequence sequence learning nese social media text summarization ings annual meeting association computational linguistics acl ver canada july august volume short papers pages ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre bing xiang abstractive text summarization sequence proceedings sequence rnns signll conference computational natural language learning conll berlin germany august pages david kauchak improving text simplication language modeling unsimplied text data proceedings annual meeting ciation computational linguistics acl pages shashi narayan claire gardent hybrid plication deep semantics machine proceedings annual lation ing association computational tics acl pages diederik p kingma jimmy ba adam corr method stochastic optimization piji li wai lam lidong bing zihao wang deep recurrent generative decoder stractive text summarization proceedings conference empirical methods natural language processing emnlp copenhagen denmark september pages chin yew lin eduard h hovy matic evaluation summaries n gram occurrence statistics human language ogy conference north american chapter association computational linguistics naacl junyang lin shuming ma qi su xu sun decoding history based adaptive control attention neural machine translation corr tianyu liu kexiang wang lei sha baobao chang zhifang sui table text tion structure aware learning corr sergiu nisioi sanja stajner simone paolo ponzetto liviu p dinu exploring neural text plication models proceedings nual meeting association computational linguistics acl pages kishore papineni salim roukos todd ward jing zhu bleu method automatic uation machine translation proceedings annual meeting association tational linguistics pages aaditya prakash sadid hasan kathy lee vivek v datla ashequl qadir joey liu oladimeji farri neural paraphrase generation stacked coling residual lstm networks international conference computational guistics proceedings conference cal papers december osaka japan pages alexander m rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing emnlp lisbon portugal september pages konstantin lopyrev generating news corr lines recurrent neural networks nitish srivastava geoffrey e hinton alex krizhevsky ilya sutskever ruslan nov dropout simple way prevent neural wei xu courtney napoles ellie pavlick quanze chen chris callison burch optimizing statistical machine translation text simplication tacl xingxing zhang mirella lapata tence simplication deep reinforcement ing proceedings conference pirical methods natural language processing emnlp copenhagen denmark september pages zhemin zhu delphine bernhard iryna gurevych monolingual tree based translation model sentence simplication coling pages networks overtting learning research journal machine xu sun xuancheng ren shuming ma houfeng wang meprop sparsied tion accelerated deep learning reduced proceedings international tting conference machine learning icml ney nsw australia august pages xu sun xuancheng ren shuming ma bingzhen wei wei li houfeng wang training cation model simplication deep learning minimal effort propagation method corr xu sun bingzhen wei xuancheng ren shuming ma label embedding network learning bel representation soft training deep networks corr ilya sutskever oriol vinyals quoc v le sequence sequence learning neural works advances neural information ing systems annual conference neural mation processing systems pages sho takase jun suzuki naoaki okazaki tsutomu rao masaaki nagata neural headline generation abstract meaning representation proceedings conference empirical methods natural language processing emnlp austin texas usa november pages kexiang wang tianyu liu zhifang sui baobao chang afnity preserving random walk multi document summarization proceedings conference empirical methods ural language processing emnlp hagen denmark september pages kristian woodsend mirella lapata ing simplify sentences quasi synchronous grammar integer programming proceedings conference empirical methods natural language processing emnlp pages sander wubben antal van den bosch emiel krahmer sentence simplication lingual machine translation nual meeting association computational linguistics proceedings conference pages jingjing xu xu sun xuancheng ren junyang lin binzhen wei wei li dp gan promoting generative adversarial network corr erating informative diversied text
