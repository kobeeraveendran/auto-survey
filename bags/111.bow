p e s l c s c v v i x r a deconvolutional paragraph representation learning yizhe zhang dinghan shen guoyin wang zhe gan ricardo henao lawrence carin department of electrical computer engineering duke university abstract learning latent representations from long text sequences is an important rst step in many natural language processing applications recurrent neural networks rnns have become a cornerstone for this challenging task however the ity of sentences during rnn based decoding reconstruction decreases with the length of the text we propose a sequence to sequence purely convolutional and deconvolutional autoencoding framework that is free of the above issue while also being computationally efcient the proposed method is simple easy to implement and can be leveraged as a building block for many applications we show empirically that compared to rnns our framework is better at ing and correcting long paragraphs quantitative evaluation on semi supervised text classication and summarization tasks demonstrate the potential for better utilization of long unlabeled text data introduction a central task in natural language processing is to learn representations features for sentences or multi sentence paragraphs these representations are typically a required rst step toward more applied tasks such as sentiment analysis machine translation dialogue systems and text summarization an approach for learning sentence representations from data is to leverage an encoder decoder framework in a standard autoencoding setup a vector representation is rst encoded from an embedding of an input sequence then decoded to the original domain to reconstruct the input sequence recent advances in recurrent neural networks rnns especially long short term memory lstm and variants have achieved great success in numerous tasks that heavily rely on sentence representation learning rnn based methods typically model sentences recursively as a generative markov process with hidden units where the one step ahead word from an input sentence is generated by conditioning on previous words and hidden units via emission and transition operators modeled as neural networks in principle the neural representations of input sequences aim to encapsulate sufcient information about their structure to subsequently recover the original sentences via decoding however due to the recursive nature of the rnn challenges exist for rnn based strategies to fully encode a sentence into a vector representation typically during training the rnn generates words in sequence conditioning on previous ground truth words i e teacher forcing training rather than decoding the whole sentence solely from the encoded representation vector this teacher forcing strategy has proven important because it forces the output sequence of the rnn to stay close to the ground truth sequence however allowing the decoder to access ground truth information when reconstructing the sequence weakens the encoder s ability to produce self contained representations that carry enough information to steer the decoder through the decoding process without additional guidance aiming to solve this problem proposed a scheduled sampling approach during training which gradually shifts from learning via both latent representation and ground truth signals to solely use the encoded latent representation unfortunately showed that scheduled sampling is a fundamentally inconsistent training strategy in that it produces largely unstable results in practice as a result training may fail to converge on occasion during inference for which ground truth sentences are not available words ahead can only be ated by conditioning on previously generated words through the representation vector consequently decoding error compounds proportional to the length of the sequence this means that generated sentences quickly deviate from the ground truth once an error has been made and as the sentence progresses this phenomenon was coined exposure bias in we propose a simple yet powerful purely convolutional framework for learning sentence tions conveniently without rnns in our framework issues connected to teacher forcing training and exposure bias are not relevant the proposed approach uses a convolutional neural network cnn as encoder and a deconvolutional i e transposed convolutional neural network as decoder to the best of our knowledge the proposed framework is the rst to force the encoded latent representation to capture information from the entire sentence via a multi layer cnn specication to achieve high reconstruction quality without leveraging rnn based decoders our multi layer cnn allows representation vectors to abstract information from the entire sentence irrespective of order or length making it an appealing choice for tasks involving long sentences or paragraphs further since our framework does not involve recursive encoding or decoding it can be very efciently parallelized using convolution specic graphical process unit gpu primitives yielding signicant computational savings compared to rnn based models convolutional auto encoding for text modeling convolutional encoder let wt denote the t th word in a given sentence each word wt is embedded into a k dimensional word vector xt where we is a learned word embedding matrix v is the vocabulary size and denotes the v th column of we all columns of we are normalized to have unit norm i e v by dividing each column with its norm after embedding a sentence of length t padded where necessary is represented as x rkt by concatenating its word embeddings i e is the t th column of x for sentence encoding we use a cnn architecture similar to though originally proposed for image data the cnn consists of l layers l convolutional and the lth fully connected that ultimately summarize an input sentence into a xed length latent representation vector h layer l l consists of pl lters learned from data for the i lter in layer a convolutional operation with stride length applies lter rkh to x where h is the convolution lter size this yields latent feature map c where is a nonlinear activation function and denotes the convolutional operator in our experiments is represented by a rectied linear unit relu note that the original embedding dimension k changes after the rst convolutional layer as for i concatenating the results from lters for layer results in feature map after this rst convolutional layer we apply the convolution operation to the feature map using the same lter size h with this repeated in sequence for l layers each time the length along the spatial coordinate is reduced to t l where is the stride length t l is the spatial length l denotes the l th layer and is the oor function for the nal layer l the feature map is fed into a fully connected layer to produce the latent representation h implementation wise we use a convolutional layer with lter size equals to t regardless of h which is equivalent to a fully connected layer this implementation trick has been also utilized in this last layer summarizes all remaining spatial coordinates t into scalar features that encapsulate sentence sub structures throughout the entire sentence characterized by lters l for i and l l where l denotes lter i for layer l this also implies that the extracted feature is of xed dimensionality independent of the length of the input sentence c figure convolutional auto encoding architecture encoder the input sequence is rst expanded to an embedding matrix x then fully compressed to a representation vector h through a multi layer convolutional encoder with stride in the last layer the spatial dimension is collapsed to remove the spatial dependency decoder the latent vector h is fed through a multi layer deconvolutional decoder with stride to reconstruct x as x via cosine similarity cross entropy loss having pl lters on the last layer results in pl dimensional representation vector h for the input sentence for example in figure the encoder consists of l layers which for a sentence of length t embedding dimension stride lengths lter sizes h and number of lters results in intermediate feature maps and of sizes respectively the last feature map of size corresponds to latent representation vector h conceptually lters from the lower layers capture primitive sentence information h grams gous to edges in images while higher level lters capture more sophisticated linguistic features such as semantic and syntactic structures analogous to image elements such a bottom up architecture models sentences by hierarchically stacking text segments h grams as building blocks for sentation vector h this is similar in spirit to modeling linguistic grammar formalisms via concrete syntax trees however we do not pre specify a tree structure based on some syntactic structure i e english language but rather abstract it from data via a multi layer convolutional network deconvolutional decoder we apply the deconvolution with stride i e convolutional transpose as the conjugate operation of convolution to decode the latent representation h back to the source discrete text domain as the deconvolution operation proceeds the spatial resolution gradually increases by mirroring the convolutional steps described above as illustrated in figure the spatial dimension is rst expanded to match the spatial dimension of the l layer of convolution then progressively expanded as t t l h for l up to l deconvolutional layer which corresponds to the input layer of the convolutional encoder the output of the l layer deconvolution operation aims to reconstruct the word embedding matrix which we denote as x in line with word embedding matrix we columns of x are normalized to have unit norm denoting wt as the t th word in reconstructed sentence s the probability of wt to be word v is specied as p wt v where y is the cosine similarity dened as is the v th column of we xt is the t th column of x is a positive number we denote as temperature parameter this parameter is akin to the concentration parameter of a dirichlet distribution in that it controls the spread of probability vector wt p wt v thus a large encourages uniformly distributed probabilities whereas a small encourages sparse concentrated probability values in the experiments we set note that in our setting the cosine similarity can be obtained as an inner product provided that columns of we and x have unit norm by specication layersconvolution h h model learning the objective of the convolutional autoencoder described above can be written as the word wise log likelihood for all sentences s d i e lae t log p wt dd where d denotes the set of observed sentences the simple maximum likelihood objective in is optimized via stochastic gradient descent details of the implementation are provided in the experiments note that differs from prior related work in two ways use pooling and un pooling operators while we use convolution deconvolution with stride and more importantly do not use a cosine similarity reconstruction as in but a rnn based decoder a further discussion of related work is provided in section we could use pooling and un pooling instead of striding a particular case of deterministic pooling un pooling however in early experiments not shown we did not observe signicant performance gains while convolution deconvolution operations with stride are considerably more efcient in terms of memory footprint compared to a standard lstm based rnn sequence autoencoders with roughly the same number of parameters computations in our case are considerably faster see experiments using single nvidia titan x gpu this is due to the high parallelization efciency of cnns via cudnn primitives comparison between deconvolutional and rnn decoders the proposed framework can be seen as a complementary building block for natural language modeling contrary to the standard based decoder the deconvolutional decoder imposes in general a less strict sequence dependency compared to rnn architectures specically generating a word from an rnn requires a vector of hidden units that recursively accumulate information from the entire sentence in an order preserving manner long term dependencies are heavily down weighted while for a deconvolutional decoder the generation only depends on a representation vector that encapsulates information from throughout the sentence without a pre specied ordering structure as a result for language generation tasks a rnn decoder will usually generate more coherent text when compared to a deconvolutional decoder on the contrary a deconvolutional decoder is better at accounting for distant dependencies in long sentences which can be very benecial in feature extraction for classication and text summarization tasks semi supervised classication and summarization identifying related topics or sentiments and abstracting short summaries from user generated content such as blogs or product reviews has recently received signicant interest in many practical scenarios unlabeled data are abundant however there are not many practical cases where the potential of such unlabeled data is fully realized motivated by this opportunity here we seek to complement scarcer but more valuable labeled data to improve the generalization ability of supervised models by ingesting unlabeled data the model can learn to abstract latent representations that capture the semantic meaning of all available sentences irrespective of whether or not they are labeled this can be done prior to the supervised model training as a two step process recently rnn based methods exploiting this idea have been widely utilized and have achieved state of the art performance in many tasks alternatively one can learn the autoencoder and classier jointly by specifying a classication model whose input is the latent representation see for instance in the case of product reviews for example each review may contain hundreds of words this poses challenges when training rnn based sequence encoders in the sense that the rnn has to abstract information on as it moves through the sentence which often leads to loss of information particularly in long sentences furthermore the decoding process uses ground truth information during training thus the learned representation may not necessarily keep all information from the input text that is necessary for proper reconstruction summarization or classication we consider applying our convolutional autoencoding framework to semi supervised learning from long sentences and paragraphs instead of pre training a fully unsupervised model as in we cast the semi supervised task as a multi task learning problem similar to i e we simultaneously train a sequence autoencoder and a supervised model in principle by using this joint training strategy the learned paragraph embedding vector will preserve both reconstruction and classication ability specically we consider the following objective t log p wt lsemi ddl hd yd where is an annealing parameter balancing the relative importance of supervised and pervised loss dl and du denote the set of labeled and unlabeled data respectively the rst term in is the sequence autoencoder loss in for the th sequence lsup is the supervision loss for the th sequence labeled only the classier function that attempts to reconstruct yd from hd can be either a multi layer perceptron mlp in classication tasks or a cnn rnn in text summarization tasks for the latter we are interested in a purely convolutional specication however we also consider an rnn for comparison for classication we use a standard cross entropy loss and for text summarization we use either for the cnn or the standard lstm loss for the rnn in practice we adopt a scheduled annealing strategy for as in rather than xing it a priori as in during training gradually transits from focusing solely on the unsupervised sequence autoencoder to the supervised task by annealing from to a small positive value min we set min in the experiments the motivation for this annealing strategy is to rst focus on abstracting paragraph features then to selectively rene learned features that are most informative to the supervised task related work previous work has considered leveraging cnns as encoders for various natural language processing tasks typically cnn based encoder architectures apply a single convolution layer followed by a pooling layer which essentially acts as a detector of specic classes of h grams given a convolution lter window of size h the deep architecture in our framework will in principle enable the high level layers to capture more sophisticated language features we use convolutions with stride rather than pooling operators e max pooling for spatial downsampling following where it is argued that fully convolutional architectures are able to learn their own spatial downsampling further uses a layer cnn for text classication our cnn encoder is considerably simpler in structure convolutions with stride and no more than layers while still achieving good performance language decoders other than rnns are less well studied recently proposed a hybrid model by coupling a convolutional deconvolutional network with an rnn where the rnn acts as decoder and the deconvolutional model as a bridge between the encoder convolutional network and decoder additionally considered cnn variants such as pixelcnn for text generation nevertheless to achieve good empirical results these methods still require the sentences to be generated sequentially conditioning on the ground truth historical information akin to rnn based decoders thus still suffering from the exposure bias other efforts have been made to improve embeddings from long paragraphs using unsupervised approaches the paragraph vector learns a xed length vector by concatenating it with a embedding of history sequence to predict future words the hierarchical neural autoencoder builds a hierarchical attentive rnn then it uses paragraph level hidden units of that rnn as embedding our work differs from these approaches in that we force the sequence to be fully restored from the latent representation without aid from any history information previous methods have considered leveraging unlabeled data for semi supervised sequence tion tasks typically rnn based methods consider either i training a sequence to sequence rnn autoencoder or a rnn classier that is robust to adversarial perturbation as initialization for the coder in the supervised model or learning latent representation via a sequence to sequence rnn autoencoder and then using them as inputs to a classier that also takes features extracted from a cnn as inputs for summarization tasks has considered a semi supervised approach based on support vector machines however so far research on semi supervised text summarization using deep models is scarce experiments experimental setup for all the experiments we use a layer convolutional encoder followed by a layer deconvolutional decoder recall implementation details for the top layer filter size stride ground truth hier lstm on every visit to nyc the hotel beacon is the place we love to stay so conveniently located to central park lincoln center and great local restaurants the rooms are lovely beds so comfortable a great little kitchen and new wizz bang coffee maker the staff are so accommodating and just love walking across the street to the fairway supermarket with every imaginable goodies to eat every time in new york lighthouse hotel is our favorite place to stay very convenient central park lincoln center and great restaurants the room is wonderful very comfortable bed a kitchenette and a large explosion of coffee maker the staff is so inclusive just across the street to walk to the supermarket channel love with all kinds of what to eat our lstm lstm on every visit to nyc the hotel beacon is the place to relax and wanting to become conveniently located hotel in the evenings out good budget accommodations the views are great and we were more than two couples manny the doorman has a great big guy come and will denitly want to leave during my stay and enjoy a wonderfully relaxing wind break in having for hour early rick s cafe oh perfect easy easy walking distance to everything imaginable groceries if you may want to watch yours on every visit to nyc the hotel beacon is the place we love to stay so closely located to central park lincoln center and great local restaurants biggest rooms are lovely beds so comfortable a great little kitchen and new unk suggestion coffee maker the staff turned so accommodating and just love walking across the street to former fairway supermarket with every food taxes to eat our cnn dcnn table reconstructed paragraph of the hotel reviews example used in and word embedding are set to h rl for l and k respectively the dimension of the latent representation vector varies for each experiment thus is reported separately for notational convenience we denote our convolutional deconvolutional autoencoder as dcnn in most comparisons we also considered two standard autoencoders as baselines lstm cnn encoder coupled with lstm decoder and lstm lstm lstm encoder with lstm decoder an lstm dcnn conguration is not included because it yields similar performance to cnn dcnn while being more computationally expensive the complete experimental setup and baseline details is provided in the supplementary material sm cnn dcnn has the least number of parameters for example using as the dimension of h results in about million total trainable parameters for cnn dcnn cnn lstm and lstm lstm respectively model lstm lstm hier lstm lstm hier att lstm lstm cnn lstm cnn dcnn bleu table reconstruction evaluation results on the hotel reviews dataset figure bleu score vs sentence length for hotel review data paragraph reconstruction we rst investigate the performance of the proposed autoencoder in terms of learning representations that can preserve paragraph information we adopt evaluation criteria from i e rouge score and bleu score to measure the closeness of the reconstructed paragraph model output to the input paragraph briey rouge and bleu scores measures the n gram recall and precision between the model outputs and the ground truth references we use in our evaluation in alignment with in addition to the lstm and lstm lstm autoencoder we also compared with the hierarchical lstm autoencoder the comparison is performed on the hotel reviews datasets following the experimental setup from i e we only keep reviews with sentence length ranging from to words resulting in training data samples and testing data samples for all comparisons we set the dimension of the latent representation to h from table we see that for long paragraphs the lstm decoder in cnn lstm and lstm lstm suffers from heavy exposure bias issues we further evaluate the performance of each model with different paragraph lengths as shown in figure and table on this task cnn dcnn demonstrates a clear advantage meanwhile as the length of the sentence increases the comparative advantage becomes more substantial for lstm based methods the quality of the reconstruction deteriorates quickly as sequences get longer in constrast the reconstruction quality of cnn dcnn is stable and consistent regardless of sentence length furthermore the computational cost evaluated as wall clock is signicantly lower in cnn dcnn roughly cnn lstm is times slower than cnn dcnn and lstm lstm is times slower on a single gpu details are reported in the sm character level and word level correction this task seeks to evaluate whether the tional decoder can overcome exposure bias which severely limits lstm based decoders we consider scorecnn dcnncnn lstmlstm lstm a denoising autoencoder where the input is tweaked slightly with certain modications while the model attempts to denoise correct the unknown modication thus recover the original sentence for character level correction we consider the yahoo answer dataset the dataset description and setup for word level correction is provided in the sm we follow the experimental setup in for word level and character level spelling correction see details in the sm we considered substituting each word character with a different one at random with probability with for character level analysis we rst map all characters into a dimensional embedding vector with the network structure for and character level models kept the same model actor lstm lstm cnn lstm cnn dcnn model lstm lstm cnn lstm cnn dcnn figure cer comparison black triangles indicate the end of an epoch figure spelling error denoising ison darker colors indicate higher tainty trained on modied sentences table cer and wer parison on yahoo and arxiv data we employ character error rate cer and word error rate wer for evaluation the wer cer measure the ratio of levenshtein distance a a edit distance between model predictions and the ground truth and the total length of sequence conceptually lower wer cer indicates better performance we use lstm lstm and cnn lstm denoising autoencoders for comparison the architecture for the word level baseline models is the same as in the previous experiment for character level correction we set dimension of h to we also compare to actor critic training following their experimental guidelines see details in the sm as shown in figure and table we observed cnn dcnn achieves both lower cer and faster convergence further cnn dcnn delivers stable denoising performance irrespective of the noise location within the sentence as seen in figure for cnn dcnn even when an error is detected but not exactly corrected darker colors in figure indicate higher uncertainty denoising with future words is not effected while for cnn lstm and lstm lstm the error gradually accumulates with longer sequences as expected for word level correction we consider word substitutions only and mixed perturbations from three kinds substitution deletion and insertion generally cnn dcnn outperforms cnn lstm and lstm lstm and is faster we provide experimental details and comparative results in the sm semi supervised sequence classication summarization we investigate whether our dcnn framework can improve upon supervised natural language tasks that leverage features learned from paragraphs in principle a good unsupervised feature extractor will improve the ization ability in a semi supervised learning setting we evaluate our approach on three popular natural language tasks sentiment analysis paragraph topic prediction and text summarization the rst two tasks are essentially sequence classication while summarization involves both language comprehension and language generation we consider three large scale document classication datasets dbpedia yahoo answers and yelp review polarity the partition of training validation and test sets for all datasets follows the settings from the detailed summary statistics of all datasets are shown in the sm to demonstrate the advantage of incorporating the reconstruction objective into the training of text classiers we further evaluate our model with different amounts of labeled data and respectively and the whole training set as unlabeled data for our purely supervised baseline model supervised cnn we use the same convolutional encoder architecture described above with a dimensional latent representation dimension followed by a mlp classier with one hidden layer of hidden units the dropout rate is set to word embeddings are initialized at random as shown in table the joint training strategy consistently and signicantly outperforms the purely supervised strategy across datasets even when all labels are available we hypothesize that during the early phase of training when reconstruction is emphasized features from text fragments can be readily error rate dcnncnn lstmlstm lstmoriginalcoriginalaoriginalnoriginal originalaoriginalnoriginalyoriginalooriginalnoriginaleoriginal originalsoriginaluoriginalgoriginalgoriginaleoriginalsoriginaltoriginal originalsoriginalooriginalmoriginaleoriginal originalgoriginalooriginalooriginaldoriginal originalboriginalooriginalooriginalkoriginalsoriginal modifiedamodifiednmodifiedymodifiedomodifiednmodifiedkmodified modifiedwmodifiedumodifiedgmodifiedgmodifiedemodifiedsmodifiedtmodified modifiedxmodifiedomodifiedhmodifiedemodified modifiedimodifiedomodifiedrmodifieddmodified modifiedymodifiedomodifiedomodifiedkmodifiedumodified actorcriticaactorcriticnactorcriticyactorcriticoactorcriticnactorcriticeactorcritic actorcriticwactorcriticiactorcritictactorcritichactorcriticeactorcriticsactorcritictactorcritic actorcritictactorcriticoactorcritic actorcriticeactorcritic actorcriticfactorcriticoactorcriticractorcriticdactorcritic actorcriticyactorcriticoactorcriticuactorcritic actorcriticuactorcritic lstmclstm lstmalstm lstmnlstm lstm lstm lstmalstm lstmnlstm lstmylstm lstmolstm lstmnlstm lstmelstm lstm lstm lstmslstm lstmulstm lstmglstm lstmglstm lstmelstm lstmslstm lstmtlstm lstm lstm lstmjlstm lstmolstm lstmklstm lstmelstm lstm lstm lstmflstm lstmolstm lstmolstm lstmdlstm lstm lstm lstmylstm lstmolstm lstmulstm lstmnlstm lstmglstm lstm lstm lstmccnn lstmacnn lstmncnn lstm cnn lstmacnn lstmncnn lstmycnn lstmocnn lstmncnn lstmecnn lstm cnn lstmgcnn lstmucnn lstmicnn lstmtcnn lstmecnn lstmscnn lstm cnn lstmscnn lstmocnn lstmmcnn lstmecnn lstm cnn lstmocnn lstmwcnn lstmecnn lstm cnn lstmpcnn lstmocnn lstmocnn lstmkcnn lstmscnn lstm cnn lstm cnn lstm cnn dcnnccnn dcnnacnn dcnnncnn dcnn cnn dcnnacnn dcnnncnn dcnnycnn dcnnocnn dcnnncnn dcnnecnn dcnn cnn dcnnscnn dcnnucnn dcnngcnn dcnngcnn dcnnecnn dcnnscnn dcnntcnn dcnn cnn dcnnscnn dcnnocnn dcnnmcnn dcnnecnn dcnn cnn dcnnwcnn dcnnocnn dcnnocnn dcnndcnn dcnn cnn dcnnbcnn dcnnocnn dcnnocnn dcnnkcnn dcnnscnn dcnn cnn originalsoriginal originalyoriginalooriginaluoriginalroriginal originalioriginaldoriginaleoriginalaoriginal originalooriginalforiginal originalaoriginal originalsoriginaltoriginaleoriginalporiginalporiginalioriginalnoriginalgoriginal originalsoriginaltoriginalooriginalnoriginaleoriginal originaltoriginalooriginal originalboriginaleoriginaltoriginaltoriginaleoriginalroriginal originaltoriginalhoriginalioriginalnoriginalgoriginalsoriginal originaltoriginalooriginal originalcoriginalooriginalmoriginaleoriginal modifiedsmodified modifiedymodifiedomodifiedgmodifiedrmodified modifiedimodifieddmodifiedemodifiedmmodified modifiedomodifiedfmodified modifiedtmodified modifiedsmodifiedtmodifiedemodifiedpmodifiedumodifiedkmodifiednmodifiedgmodified modifiedjmodifiedtmodifiedzmodifiednmodifiedemodified modifiedtmodifiedimodified modifiedbmodifiedemodifiedtmodifiedtmodifiedemodifiedrmodified modifiedtmodifiedhmodifiedimodifiednmodifiedgmodifiedzmodified modifiedtmodifiedtmodified modifiedcmodifiedomodifiedemodifiedemodified actorcriticsactorcritic actorcriticyactorcriticoactorcriticuactorcriticractorcritic actorcriticiactorcriticdactorcriticeactorcriticmactorcritic actorcriticoactorcriticfactorcritic actorcritictactorcritic actorcriticsactorcritictactorcriticeactorcriticpactorcriticuactorcriticaactorcriticnactorcriticgactorcritic actorcriticjactorcriticoactorcritickactorcriticnactorcriticeactorcritic actorcritictactorcriticiactorcritic actorcriticbactorcriticeactorcritictactorcritictactorcriticeactorcriticractorcritic actorcritictactorcritichactorcriticiactorcriticnactorcriticgactorcritic actorcriticiactorcritictactorcritictactorcritic actorcriticcactorcriticoactorcriticmactorcriticeactorcritic lstmwlstm lstmhlstm lstmalstm lstmtlstm lstm lstm lstmslstm lstm lstm lstmylstm lstmolstm lstmulstm lstmrlstm lstm lstm lstmilstm lstmdlstm lstmelstm lstmalstm lstm lstm lstmolstm lstmflstm lstm lstm lstmalstm lstm lstm lstmslstm lstmplstm lstmelstm lstmalstm lstmklstm lstmilstm lstmnlstm lstmglstm lstm lstm lstmslstm lstmtlstm lstmalstm lstmnlstm lstmdlstm lstm lstm lstmtlstm lstmolstm lstm lstm lstmblstm lstmelstm lstmtlstm lstmtlstm lstmelstm lstmrlstm lstm lstm lstmtlstm lstmhlstm lstmilstm lstmnlstm lstmglstm lstmslstm lstm lstm lstmtlstm lstmolstm lstm lstm lstmclstm lstmolstm lstmmlstm lstmelstm lstm lstm lstmwcnn lstmhcnn lstmacnn lstmtcnn lstm cnn lstmscnn lstm cnn lstmycnn lstmocnn lstmucnn lstmrcnn lstm cnn lstmicnn lstmdcnn lstmecnn lstmmcnn lstm cnn lstmocnn lstmfcnn lstm cnn lstmacnn lstm cnn lstmscnn lstmtcnn lstmecnn lstmpcnn lstmpcnn lstmicnn lstmncnn lstmgcnn lstm cnn lstmscnn lstmtcnn lstmacnn lstmrcnn lstmtcnn lstm cnn lstmtcnn lstmocnn lstm cnn lstmbcnn lstmecnn lstmtcnn lstmtcnn lstmecnn lstmrcnn lstm cnn lstmtcnn lstmhcnn lstmicnn lstmncnn lstmgcnn lstm cnn lstmtcnn lstmocnn lstm cnn lstmccnn lstmocnn lstmmcnn lstmecnn lstm cnn lstm cnn dcnnwcnn dcnnhcnn dcnnacnn dcnntcnn dcnn cnn dcnnscnn dcnn cnn dcnnycnn dcnnocnn dcnnucnn dcnnrcnn dcnn cnn dcnnicnn dcnndcnn dcnnecnn dcnnacnn dcnn cnn dcnnocnn dcnnfcnn dcnn cnn dcnnacnn dcnn cnn dcnnscnn dcnntcnn dcnnecnn dcnnpcnn dcnnpcnn dcnnicnn dcnnncnn dcnngcnn dcnn cnn dcnnscnn dcnntcnn dcnnocnn dcnnncnn dcnnecnn dcnn cnn dcnntcnn dcnnocnn dcnn cnn dcnnbcnn dcnnecnn dcnntcnn dcnntcnn dcnnecnn dcnnrcnn dcnn cnn dcnntcnn dcnnhcnn dcnnicnn dcnnncnn dcnngcnn dcnnscnn dcnn cnn dcnntcnn dcnnocnn dcnn cnn dcnnccnn dcnnocnn dcnnmcnn dcnnecnn dcnn cnn dcnn learned as the training proceeds the most discriminative text fragment features are selected further the subset of features that are responsible for both reconstruction and discrimination presumably encapsulate longer dependency structure compared to the features using a purely supervised strategy figure demonstrates the behavior of our model in a semi supervised setting on yelp review dataset the results for yahoo answer and dbpedia are provided in the sm model ngrams tfidf large word convnet small word convnet large char convnet small char convnet sa lstm word level deep convnet ours purely supervised ours joint training with cnn lstm ours joint training with cnn dcnn dbpedia yelp p yahoo table test error rates of document classication results from other methods were obtained from figure semi supervised tion accuracy on yelp review data for summarization we used a dataset composed of abstract title pairs from arxiv title pairs are selected if the length of the title and abstract do not exceed and words respectively we partitioned the training validation and test sets into pairs each we train a sequence to sequence model to generate the title given the abstract using a randomly selected subset of paired data with proportion for every value of we considered both purely supervised summarization using just abstract title pairs and supervised summarization by leveraging additional abstracts without titles we compared lstm and deconvolutional network as the decoder for generating titles for obs proportion supervised semi sup dcnn dec table summarization task on arxiv data using rouge l metric first columns are for the lstm decoder and the last column is for the deconvolutional decoder observed table summarizes quantitative results using rouge l longest common sequence in general the additional abstracts without titles improve the eralization ability on the test set estingly even when all titles are observed the joint training objective still yields a better performance than using lsup alone presumably since the joint training objective requires the latent representation to be capable of reconstructing the input paragraph in addition to generating a title the learned representation may better capture the entire structure meaning of the paragraph we also empirically observed that titles generated under the joint training objective are more likely to use the words appearing in the corresponding paragraph i e more extractive while the the titles generated using the purely supervised objective lsup tend to use wording more freely thus more abstractive one possible explanation is that for the joint training strategy since the reconstructed paragraph and title are all generated from latent representation h the text fragments that are used for reconstructing the input paragraph are more likely to be leveraged when building the title thus the title bears more resemblance to the input paragraph as expected the titles produced by a deconvolutional decoder are less coherent than an lstm decoder presumably since each paragraph can be summarized with multiple plausible titles the deconvolutional decoder may have trouble when positioning text segments we provide discussions and titles generated under different setups in the sm designing a framework which takes the best of these two worlds lstm for generation and cnn for decoding will be an interesting future direction conclusion we proposed a general framework for text modeling using purely convolutional and deconvolutional operations the proposed method is free of sequential conditional generation avoiding issues associated with exposure bias and teacher forcing training our approach enables the model to fully encapsulate a paragraph into a latent representation vector which can be decompressed to reconstruct the original input sequence empirically the proposed approach achieved excellent long paragraph reconstruction quality and outperforms existing algorithms on spelling correction and semi supervised sequence classication and summarization with largely reduced computational cost of labeled supervisedsemi cnn cnn lstm references andrew m dai and quoc v le semi supervised sequence learning in nips quoc le and tomas mikolov distributed representations of sentences and documents in icml rie johnson and tong zhang supervised and semi supervised text categorization using lstm for region embeddings arxiv february takeru miyato andrew m dai and ian goodfellow adversarial training methods for semi supervised dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation by jointly learning text classication in iclr may to align and translate in iclr kyunghyun cho bart van merrinboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio learning phrase representations using rnn encoder decoder for statistical machine translation in emnlp fandong meng zhengdong lu mingxuan wang hang li wenbin jiang and qun liu encoding source language with convolutional neural network for machine translation in acl tsung hsien wen milica gasic nikola mrksic pei hao su david vandyke and steve young mantically conditioned lstm based natural language generation for spoken dialogue systems arxiv jiwei li will monroe alan ritter michel galley jianfeng gao and dan jurafsky deep reinforcement learning for dialogue generation arxiv jiwei li will monroe tianlin shi alan ritter and dan jurafsky adversarial learning for neural dialogue generation ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre and bing xiang abstractive text summarization using sequence to sequence rnns and beyond in conll shashi narayan nikos papasarantopoulos mirella lapata and shay b cohen neural extractive rization with side information arxiv april alexander m rush sumit chopra and jason weston a neural attention model for abstractive sentence ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural networks in summarization in emnlp nips tomas mikolov martin karat lukas burget jan and sanjeev khudanpur recurrent neural network based language model in interspeech sepp hochreiter and jrgen schmidhuber long short term memory in neural computation junyoung chung caglar gulcehre kyunghyun cho and yoshua bengio empirical evaluation of gated recurrent neural networks on sequence modeling arxiv ronald j williams and david zipser a learning algorithm for continually running fully recurrent neural networks neural computation samy bengio oriol vinyals navdeep jaitly and noam shazeer scheduled sampling for sequence prediction with recurrent neural networks in nips ferenc huszr how not to train your generative model scheduled sampling likelihood adversary arxiv sentences in acl nal kalchbrenner edward grefenstette and phil blunsom a convolutional neural network for modelling yoon kim convolutional neural networks for sentence classication in emnlp ishaan gulrajani kundan kumar faruk ahmed adrien ali taiga francesco visin david vazquez and aaron courville pixelvae a latent variable model for natural images arxiv alec radford luke metz and soumith chintala unsupervised representation learning with deep convolutional generative adversarial networks arxiv vinod nair and geoffrey e hinton rectied linear units improve restricted boltzmann machines in icml ian chiswell and wilfrid hodges mathematical logic volume oup oxford emil julius gumbel and julius lieblein statistical theory of extreme values and some practical applications pages a series of lectures ronan collobert jason weston lon bottou michael karlen koray kavukcuoglu and pavel kuksa natural language processing almost from scratch in jmlr sharan chetlur cliff woolley philippe vandermersch jonathan cohen john tran bryan catanzaro and evan shelhamer cudnn efcient primitives for deep learning arxiv zichao yang diyi yang chris dyer xiaodong he alex smola and eduard hovy hierarchical attention networks for document classication in naacl adji b dieng chong wang jianfeng gao and john paisley topicrnn a recurrent neural network with long range semantic dependency in iclr diederik p kingma shakir mohamed danilo jimenez rezende and max welling semi supervised learning with deep generative models in nips yunchen pu zhe gan ricardo henao xin yuan chunyuan li andrew stevens and lawrence carin variational autoencoder for deep learning of images labels and captions in nips sepp hochreiter yoshua bengio paolo frasconi and jrgen schmidhuber gradient ow in recurrent nets the difculty of learning long term dependencies richard socher jeffrey pennington eric h huang andrew y ng and christopher d manning supervised recursive autoencoders for predicting sentiment distributions in emnlp association for computational linguistics samuel r bowman luke vilnis oriol vinyals andrew m dai rafal jozefowicz and samy bengio generating sentences from a continuous space arxiv zichao yang zhiting hu ruslan salakhutdinov and taylor berg kirkpatrick improved variational autoencoders for text modeling using dilated convolutions arxiv february baotian hu zhengdong lu hang li and qingcai chen convolutional neural network architectures for matching natural language sentences in nips rie johnson and tong zhang effective use of word order for text categorization with convolutional neural networks in naacl hlt the all convolutional net arxiv tion in iclr jost tobias springenberg alexey dosovitskiy thomas brox and martin riedmiller striving for simplicity karen simonyan and andrew zisserman very deep convolutional networks for large scale image stanislau semeniuta aliaksei severyn and erhardt barth a hybrid convolutional variational coder for text generation arxiv february nal kalchbrenner lasse espeholt karen simonyan aaron van den oord alex graves and koray kavukcuoglu neural machine translation in linear time arxiv yann n dauphin angela fan michael auli and david grangier language modeling with gated convolutional networks arxiv december j gehring m auli d grangier d yarats and y n dauphin convolutional sequence to sequence learning arxiv may aaron van den oord nal kalchbrenner lasse espeholt oriol vinyals alex graves al conditional image generation with pixelcnn decoders in nips pages jiwei li minh thang luong and dan jurafsky a hierarchical neural autoencoder for paragraphs and documents in acl tomas mikolov ilya sutskever kai chen greg s corrado and jeff dean distributed representations of words and phrases and their compositionality in nips kam fai wong mingli wu and wenjie li extractive summarization using supervised and semi supervised learning in iccl association for computational linguistics chin yew lin rouge a package for automatic evaluation of summaries in acl workshop kishore papineni salim roukos todd ward and wei jing zhu bleu a method for automatic evaluation of machine translation in acl association for computational linguistics xiang zhang junbo zhao and yann lecun character level convolutional networks for text classication in nips pages dzmitry bahdanau philemon brakel kelvin xu anirudh goyal ryan lowe joelle pineau aaron courville and yoshua bengio an actor critic algorithm for sequence prediction arxiv jp woodard and jt nelson an information theoretic measure of speech recognition performance in workshop on standardisation for speech i o diederik kingma and jimmy ba adam a method for stochastic optimization in iclr xavier glorot and yoshua bengio understanding the difculty of training deep feedforward neural networks in aistats
