p e s l c s c v v x r deconvolutional paragraph representation learning yizhe zhang dinghan shen guoyin wang zhe gan ricardo henao lawrence carin department electrical computer engineering duke university abstract learning latent representations long text sequences important rst step natural language processing applications recurrent neural networks rnns cornerstone challenging task ity sentences rnn based decoding reconstruction decreases length text propose sequence sequence purely convolutional deconvolutional autoencoding framework free issue computationally efcient proposed method simple easy implement leveraged building block applications empirically compared rnns framework better ing correcting long paragraphs quantitative evaluation semi supervised text classication summarization tasks demonstrate potential better utilization long unlabeled text data introduction central task natural language processing learn representations features sentences multi sentence paragraphs representations typically required rst step applied tasks sentiment analysis machine translation dialogue systems text summarization approach learning sentence representations data leverage encoder decoder framework standard autoencoding setup vector representation rst encoded embedding input sequence decoded original domain reconstruct input sequence recent advances recurrent neural networks rnns especially long short term memory lstm variants achieved great success numerous tasks heavily rely sentence representation learning rnn based methods typically model sentences recursively generative markov process hidden units step ahead word input sentence generated conditioning previous words hidden units emission transition operators modeled neural networks principle neural representations input sequences aim encapsulate sufcient information structure subsequently recover original sentences decoding recursive nature rnn challenges exist rnn based strategies fully encode sentence vector representation typically training rnn generates words sequence conditioning previous ground truth words e teacher forcing training decoding sentence solely encoded representation vector teacher forcing strategy proven important forces output sequence rnn stay close ground truth sequence allowing decoder access ground truth information reconstructing sequence weakens encoder s ability produce self contained representations carry information steer decoder decoding process additional guidance aiming solve problem proposed scheduled sampling approach training gradually shifts learning latent representation ground truth signals solely use encoded latent representation unfortunately showed scheduled sampling fundamentally inconsistent training strategy produces largely unstable results practice result training fail converge occasion inference ground truth sentences available words ahead ated conditioning previously generated words representation vector consequently decoding error compounds proportional length sequence means generated sentences quickly deviate ground truth error sentence progresses phenomenon coined exposure bias propose simple powerful purely convolutional framework learning sentence tions conveniently rnns framework issues connected teacher forcing training exposure bias relevant proposed approach uses convolutional neural network cnn encoder deconvolutional e transposed convolutional neural network decoder best knowledge proposed framework rst force encoded latent representation capture information entire sentence multi layer cnn specication achieve high reconstruction quality leveraging rnn based decoders multi layer cnn allows representation vectors abstract information entire sentence irrespective order length making appealing choice tasks involving long sentences paragraphs framework involve recursive encoding decoding efciently parallelized convolution specic graphical process unit gpu primitives yielding signicant computational savings compared rnn based models convolutional auto encoding text modeling convolutional encoder let wt denote t th word given sentence word wt embedded k dimensional word vector xt learned word embedding matrix v vocabulary size denotes v th column columns normalized unit norm e v dividing column norm embedding sentence length t padded necessary represented x rkt concatenating word embeddings e t th column x sentence encoding use cnn architecture similar originally proposed image data cnn consists l layers l convolutional lth fully connected ultimately summarize input sentence xed length latent representation vector h layer l l consists pl lters learned data lter layer convolutional operation stride length applies lter rkh x h convolution lter size yields latent feature map c nonlinear activation function denotes convolutional operator experiments represented rectied linear unit relu note original embedding dimension k changes rst convolutional layer concatenating results lters layer results feature map rst convolutional layer apply convolution operation feature map lter size h repeated sequence l layers time length spatial coordinate reduced t l stride length t l spatial length l denotes l th layer oor function nal layer l feature map fed fully connected layer produce latent representation h implementation wise use convolutional layer lter size equals t regardless h equivalent fully connected layer implementation trick utilized layer summarizes remaining spatial coordinates t scalar features encapsulate sentence sub structures entire sentence characterized lters l l l l denotes lter layer l implies extracted feature xed dimensionality independent length input sentence c figure convolutional auto encoding architecture encoder input sequence rst expanded embedding matrix x fully compressed representation vector h multi layer convolutional encoder stride layer spatial dimension collapsed remove spatial dependency decoder latent vector h fed multi layer deconvolutional decoder stride reconstruct x x cosine similarity cross entropy loss having pl lters layer results pl dimensional representation vector h input sentence example figure encoder consists l layers sentence length t embedding dimension stride lengths lter sizes h number lters results intermediate feature maps sizes respectively feature map size corresponds latent representation vector h conceptually lters lower layers capture primitive sentence information h grams gous edges images higher level lters capture sophisticated linguistic features semantic syntactic structures analogous image elements architecture models sentences hierarchically stacking text segments h grams building blocks sentation vector h similar spirit modeling linguistic grammar formalisms concrete syntax trees pre specify tree structure based syntactic structure e english language abstract data multi layer convolutional network deconvolutional decoder apply deconvolution stride e convolutional transpose conjugate operation convolution decode latent representation h source discrete text domain deconvolution operation proceeds spatial resolution gradually increases mirroring convolutional steps described illustrated figure spatial dimension rst expanded match spatial dimension l layer convolution progressively expanded t t l h l l deconvolutional layer corresponds input layer convolutional encoder output l layer deconvolution operation aims reconstruct word embedding matrix denote x line word embedding matrix columns x normalized unit norm denoting wt t th word reconstructed sentence s probability wt word v specied p wt v y cosine similarity dened v th column xt t th column x positive number denote temperature parameter parameter akin concentration parameter dirichlet distribution controls spread probability vector wt p wt v large encourages uniformly distributed probabilities small encourages sparse concentrated probability values experiments set note setting cosine similarity obtained inner product provided columns x unit norm specication layersconvolution h h model learning objective convolutional autoencoder described written word wise log likelihood sentences s d e lae t log p wt dd d denotes set observed sentences simple maximum likelihood objective optimized stochastic gradient descent details implementation provided experiments note differs prior related work ways use pooling un pooling operators use convolution deconvolution stride importantly use cosine similarity reconstruction rnn based decoder discussion related work provided section use pooling un pooling instead striding particular case deterministic pooling un pooling early experiments shown observe signicant performance gains convolution deconvolution operations stride considerably efcient terms memory footprint compared standard lstm based rnn sequence autoencoders roughly number parameters computations case considerably faster experiments single nvidia titan x gpu high parallelization efciency cnns cudnn primitives comparison deconvolutional rnn decoders proposed framework seen complementary building block natural language modeling contrary standard based decoder deconvolutional decoder imposes general strict sequence dependency compared rnn architectures specically generating word rnn requires vector hidden units recursively accumulate information entire sentence order preserving manner long term dependencies heavily weighted deconvolutional decoder generation depends representation vector encapsulates information sentence pre specied ordering structure result language generation tasks rnn decoder usually generate coherent text compared deconvolutional decoder contrary deconvolutional decoder better accounting distant dependencies long sentences benecial feature extraction classication text summarization tasks semi supervised classication summarization identifying related topics sentiments abstracting short summaries user generated content blogs product reviews recently received signicant interest practical scenarios unlabeled data abundant practical cases potential unlabeled data fully realized motivated opportunity seek complement scarcer valuable labeled data improve generalization ability supervised models ingesting unlabeled data model learn abstract latent representations capture semantic meaning available sentences irrespective labeled prior supervised model training step process recently rnn based methods exploiting idea widely utilized achieved state art performance tasks alternatively learn autoencoder classier jointly specifying classication model input latent representation instance case product reviews example review contain hundreds words poses challenges training rnn based sequence encoders sense rnn abstract information moves sentence leads loss information particularly long sentences furthermore decoding process uses ground truth information training learned representation necessarily information input text necessary proper reconstruction summarization classication consider applying convolutional autoencoding framework semi supervised learning long sentences paragraphs instead pre training fully unsupervised model cast semi supervised task multi task learning problem similar e simultaneously train sequence autoencoder supervised model principle joint training strategy learned paragraph embedding vector preserve reconstruction classication ability specically consider following objective t log p wt lsemi ddl hd yd annealing parameter balancing relative importance supervised pervised loss dl du denote set labeled unlabeled data respectively rst term sequence autoencoder loss th sequence lsup supervision loss th sequence labeled classier function attempts reconstruct yd hd multi layer perceptron mlp classication tasks cnn rnn text summarization tasks interested purely convolutional specication consider rnn comparison classication use standard cross entropy loss text summarization use cnn standard lstm loss rnn practice adopt scheduled annealing strategy xing priori training gradually transits focusing solely unsupervised sequence autoencoder supervised task annealing small positive value min set min experiments motivation annealing strategy rst focus abstracting paragraph features selectively rene learned features informative supervised task related work previous work considered leveraging cnns encoders natural language processing tasks typically cnn based encoder architectures apply single convolution layer followed pooling layer essentially acts detector specic classes h grams given convolution lter window size h deep architecture framework principle enable high level layers capture sophisticated language features use convolutions stride pooling operators e max pooling spatial downsampling following argued fully convolutional architectures able learn spatial downsampling uses layer cnn text classication cnn encoder considerably simpler structure convolutions stride layers achieving good performance language decoders rnns studied recently proposed hybrid model coupling convolutional deconvolutional network rnn rnn acts decoder deconvolutional model bridge encoder convolutional network decoder additionally considered cnn variants pixelcnn text generation achieve good empirical results methods require sentences generated sequentially conditioning ground truth historical information akin rnn based decoders suffering exposure bias efforts improve embeddings long paragraphs unsupervised approaches paragraph vector learns xed length vector concatenating embedding history sequence predict future words hierarchical neural autoencoder builds hierarchical attentive rnn uses paragraph level hidden units rnn embedding work differs approaches force sequence fully restored latent representation aid history information previous methods considered leveraging unlabeled data semi supervised sequence tion tasks typically rnn based methods consider training sequence sequence rnn autoencoder rnn classier robust adversarial perturbation initialization coder supervised model learning latent representation sequence sequence rnn autoencoder inputs classier takes features extracted cnn inputs summarization tasks considered semi supervised approach based support vector machines far research semi supervised text summarization deep models scarce experiments experimental setup experiments use layer convolutional encoder followed layer deconvolutional decoder recall implementation details layer filter size stride ground truth hier lstm visit nyc hotel beacon place love stay conveniently located central park lincoln center great local restaurants rooms lovely beds comfortable great little kitchen new wizz bang coffee maker staff accommodating love walking street fairway supermarket imaginable goodies eat time new york lighthouse hotel favorite place stay convenient central park lincoln center great restaurants room wonderful comfortable bed kitchenette large explosion coffee maker staff inclusive street walk supermarket channel love kinds eat lstm lstm visit nyc hotel beacon place relax wanting conveniently located hotel evenings good budget accommodations views great couples manny doorman great big guy come denitly want leave stay enjoy wonderfully relaxing wind break having hour early rick s cafe oh perfect easy easy walking distance imaginable groceries want watch visit nyc hotel beacon place love stay closely located central park lincoln center great local restaurants biggest rooms lovely beds comfortable great little kitchen new unk suggestion coffee maker staff turned accommodating love walking street fairway supermarket food taxes eat cnn dcnn table reconstructed paragraph hotel reviews example word embedding set h rl l k respectively dimension latent representation vector varies experiment reported separately notational convenience denote convolutional deconvolutional autoencoder dcnn comparisons considered standard autoencoders baselines lstm cnn encoder coupled lstm decoder lstm lstm lstm encoder lstm decoder lstm dcnn conguration included yields similar performance cnn dcnn computationally expensive complete experimental setup baseline details provided supplementary material sm cnn dcnn number parameters example dimension h results million total trainable parameters cnn dcnn cnn lstm lstm lstm respectively model lstm lstm hier lstm lstm hier att lstm lstm cnn lstm cnn dcnn bleu table reconstruction evaluation results hotel reviews dataset figure bleu score vs sentence length hotel review data paragraph reconstruction rst investigate performance proposed autoencoder terms learning representations preserve paragraph information adopt evaluation criteria e rouge score bleu score measure closeness reconstructed paragraph model output input paragraph briey rouge bleu scores measures n gram recall precision model outputs ground truth references use evaluation alignment addition lstm lstm lstm autoencoder compared hierarchical lstm autoencoder comparison performed hotel reviews datasets following experimental setup e reviews sentence length ranging words resulting training data samples testing data samples comparisons set dimension latent representation h table long paragraphs lstm decoder cnn lstm lstm lstm suffers heavy exposure bias issues evaluate performance model different paragraph lengths shown figure table task cnn dcnn demonstrates clear advantage length sentence increases comparative advantage substantial lstm based methods quality reconstruction deteriorates quickly sequences longer constrast reconstruction quality cnn dcnn stable consistent regardless sentence length furthermore computational cost evaluated wall clock signicantly lower cnn dcnn roughly cnn lstm times slower cnn dcnn lstm lstm times slower single gpu details reported sm character level word level correction task seeks evaluate tional decoder overcome exposure bias severely limits lstm based decoders consider scorecnn dcnncnn lstmlstm lstm denoising autoencoder input tweaked slightly certain modications model attempts denoise correct unknown modication recover original sentence character level correction consider yahoo answer dataset dataset description setup word level correction provided sm follow experimental setup word level character level spelling correction details sm considered substituting word character different random probability character level analysis rst map characters dimensional embedding vector network structure character level models kept model actor lstm lstm cnn lstm cnn dcnn model lstm lstm cnn lstm cnn dcnn figure cer comparison black triangles indicate end epoch figure spelling error denoising ison darker colors indicate higher tainty trained modied sentences table cer wer parison yahoo arxiv data employ character error rate cer word error rate wer evaluation wer cer measure ratio levenshtein distance edit distance model predictions ground truth total length sequence conceptually lower wer cer indicates better performance use lstm lstm cnn lstm denoising autoencoders comparison architecture word level baseline models previous experiment character level correction set dimension h compare actor critic training following experimental guidelines details sm shown figure table observed cnn dcnn achieves lower cer faster convergence cnn dcnn delivers stable denoising performance irrespective noise location sentence seen figure cnn dcnn error detected exactly corrected darker colors figure indicate higher uncertainty denoising future words effected cnn lstm lstm lstm error gradually accumulates longer sequences expected word level correction consider word substitutions mixed perturbations kinds substitution deletion insertion generally cnn dcnn outperforms cnn lstm lstm lstm faster provide experimental details comparative results sm semi supervised sequence classication summarization investigate dcnn framework improve supervised natural language tasks leverage features learned paragraphs principle good unsupervised feature extractor improve ization ability semi supervised learning setting evaluate approach popular natural language tasks sentiment analysis paragraph topic prediction text summarization rst tasks essentially sequence classication summarization involves language comprehension language generation consider large scale document classication datasets dbpedia yahoo answers yelp review polarity partition training validation test sets datasets follows settings detailed summary statistics datasets shown sm demonstrate advantage incorporating reconstruction objective training text classiers evaluate model different amounts labeled data respectively training set unlabeled data purely supervised baseline model supervised cnn use convolutional encoder architecture described dimensional latent representation dimension followed mlp classier hidden layer hidden units dropout rate set word embeddings initialized random shown table joint training strategy consistently signicantly outperforms purely supervised strategy datasets labels available hypothesize early phase training reconstruction emphasized features text fragments readily error rate dcnncnn lstmlstm lstmoriginalcoriginalaoriginalnoriginal originalaoriginalnoriginalyoriginalooriginalnoriginaleoriginal originalsoriginaluoriginalgoriginalgoriginaleoriginalsoriginaltoriginal originalsoriginalooriginalmoriginaleoriginal originalgoriginalooriginalooriginaldoriginal originalboriginalooriginalooriginalkoriginalsoriginal modifiedamodifiednmodifiedymodifiedomodifiednmodifiedkmodified modifiedwmodifiedumodifiedgmodifiedgmodifiedemodifiedsmodifiedtmodified modifiedxmodifiedomodifiedhmodifiedemodified modifiedimodifiedomodifiedrmodifieddmodified modifiedymodifiedomodifiedomodifiedkmodifiedumodified actorcriticaactorcriticnactorcriticyactorcriticoactorcriticnactorcriticeactorcritic actorcriticwactorcriticiactorcritictactorcritichactorcriticeactorcriticsactorcritictactorcritic actorcritictactorcriticoactorcritic actorcriticeactorcritic actorcriticfactorcriticoactorcriticractorcriticdactorcritic actorcriticyactorcriticoactorcriticuactorcritic actorcriticuactorcritic lstmclstm lstmalstm lstmnlstm lstm lstm lstmalstm lstmnlstm lstmylstm lstmolstm lstmnlstm lstmelstm lstm lstm lstmslstm lstmulstm lstmglstm lstmglstm lstmelstm lstmslstm lstmtlstm lstm lstm lstmjlstm lstmolstm lstmklstm lstmelstm lstm lstm lstmflstm lstmolstm lstmolstm lstmdlstm lstm lstm lstmylstm lstmolstm lstmulstm lstmnlstm lstmglstm lstm lstm lstmccnn lstmacnn lstmncnn lstm cnn lstmacnn lstmncnn lstmycnn lstmocnn lstmncnn lstmecnn lstm cnn lstmgcnn lstmucnn lstmicnn lstmtcnn lstmecnn lstmscnn lstm cnn lstmscnn lstmocnn lstmmcnn lstmecnn lstm cnn lstmocnn lstmwcnn lstmecnn lstm cnn lstmpcnn lstmocnn lstmocnn lstmkcnn lstmscnn lstm cnn lstm cnn lstm cnn dcnnccnn dcnnacnn dcnnncnn dcnn cnn dcnnacnn dcnnncnn dcnnycnn dcnnocnn dcnnncnn dcnnecnn dcnn cnn dcnnscnn dcnnucnn dcnngcnn dcnngcnn dcnnecnn dcnnscnn dcnntcnn dcnn cnn dcnnscnn dcnnocnn dcnnmcnn dcnnecnn dcnn cnn dcnnwcnn dcnnocnn dcnnocnn dcnndcnn dcnn cnn dcnnbcnn dcnnocnn dcnnocnn dcnnkcnn dcnnscnn dcnn cnn originalsoriginal originalyoriginalooriginaluoriginalroriginal originalioriginaldoriginaleoriginalaoriginal originalooriginalforiginal originalaoriginal originalsoriginaltoriginaleoriginalporiginalporiginalioriginalnoriginalgoriginal originalsoriginaltoriginalooriginalnoriginaleoriginal originaltoriginalooriginal originalboriginaleoriginaltoriginaltoriginaleoriginalroriginal originaltoriginalhoriginalioriginalnoriginalgoriginalsoriginal originaltoriginalooriginal originalcoriginalooriginalmoriginaleoriginal modifiedsmodified modifiedymodifiedomodifiedgmodifiedrmodified modifiedimodifieddmodifiedemodifiedmmodified modifiedomodifiedfmodified modifiedtmodified modifiedsmodifiedtmodifiedemodifiedpmodifiedumodifiedkmodifiednmodifiedgmodified modifiedjmodifiedtmodifiedzmodifiednmodifiedemodified modifiedtmodifiedimodified modifiedbmodifiedemodifiedtmodifiedtmodifiedemodifiedrmodified modifiedtmodifiedhmodifiedimodifiednmodifiedgmodifiedzmodified modifiedtmodifiedtmodified modifiedcmodifiedomodifiedemodifiedemodified actorcriticsactorcritic actorcriticyactorcriticoactorcriticuactorcriticractorcritic actorcriticiactorcriticdactorcriticeactorcriticmactorcritic actorcriticoactorcriticfactorcritic actorcritictactorcritic actorcriticsactorcritictactorcriticeactorcriticpactorcriticuactorcriticaactorcriticnactorcriticgactorcritic actorcriticjactorcriticoactorcritickactorcriticnactorcriticeactorcritic actorcritictactorcriticiactorcritic actorcriticbactorcriticeactorcritictactorcritictactorcriticeactorcriticractorcritic actorcritictactorcritichactorcriticiactorcriticnactorcriticgactorcritic actorcriticiactorcritictactorcritictactorcritic actorcriticcactorcriticoactorcriticmactorcriticeactorcritic lstmwlstm lstmhlstm lstmalstm lstmtlstm lstm lstm lstmslstm lstm lstm lstmylstm lstmolstm lstmulstm lstmrlstm lstm lstm lstmilstm lstmdlstm lstmelstm lstmalstm lstm lstm lstmolstm lstmflstm lstm lstm lstmalstm lstm lstm lstmslstm lstmplstm lstmelstm lstmalstm lstmklstm lstmilstm lstmnlstm lstmglstm lstm lstm lstmslstm lstmtlstm lstmalstm lstmnlstm lstmdlstm lstm lstm lstmtlstm lstmolstm lstm lstm lstmblstm lstmelstm lstmtlstm lstmtlstm lstmelstm lstmrlstm lstm lstm lstmtlstm lstmhlstm lstmilstm lstmnlstm lstmglstm lstmslstm lstm lstm lstmtlstm lstmolstm lstm lstm lstmclstm lstmolstm lstmmlstm lstmelstm lstm lstm lstmwcnn lstmhcnn lstmacnn lstmtcnn lstm cnn lstmscnn lstm cnn lstmycnn lstmocnn lstmucnn lstmrcnn lstm cnn lstmicnn lstmdcnn lstmecnn lstmmcnn lstm cnn lstmocnn lstmfcnn lstm cnn lstmacnn lstm cnn lstmscnn lstmtcnn lstmecnn lstmpcnn lstmpcnn lstmicnn lstmncnn lstmgcnn lstm cnn lstmscnn lstmtcnn lstmacnn lstmrcnn lstmtcnn lstm cnn lstmtcnn lstmocnn lstm cnn lstmbcnn lstmecnn lstmtcnn lstmtcnn lstmecnn lstmrcnn lstm cnn lstmtcnn lstmhcnn lstmicnn lstmncnn lstmgcnn lstm cnn lstmtcnn lstmocnn lstm cnn lstmccnn lstmocnn lstmmcnn lstmecnn lstm cnn lstm cnn dcnnwcnn dcnnhcnn dcnnacnn dcnntcnn dcnn cnn dcnnscnn dcnn cnn dcnnycnn dcnnocnn dcnnucnn dcnnrcnn dcnn cnn dcnnicnn dcnndcnn dcnnecnn dcnnacnn dcnn cnn dcnnocnn dcnnfcnn dcnn cnn dcnnacnn dcnn cnn dcnnscnn dcnntcnn dcnnecnn dcnnpcnn dcnnpcnn dcnnicnn dcnnncnn dcnngcnn dcnn cnn dcnnscnn dcnntcnn dcnnocnn dcnnncnn dcnnecnn dcnn cnn dcnntcnn dcnnocnn dcnn cnn dcnnbcnn dcnnecnn dcnntcnn dcnntcnn dcnnecnn dcnnrcnn dcnn cnn dcnntcnn dcnnhcnn dcnnicnn dcnnncnn dcnngcnn dcnnscnn dcnn cnn dcnntcnn dcnnocnn dcnn cnn dcnnccnn dcnnocnn dcnnmcnn dcnnecnn dcnn cnn dcnn learned training proceeds discriminative text fragment features selected subset features responsible reconstruction discrimination presumably encapsulate longer dependency structure compared features purely supervised strategy figure demonstrates behavior model semi supervised setting yelp review dataset results yahoo answer dbpedia provided sm model ngrams tfidf large word convnet small word convnet large char convnet small char convnet sa lstm word level deep convnet purely supervised joint training cnn lstm joint training cnn dcnn dbpedia yelp p yahoo table test error rates document classication results methods obtained figure semi supervised tion accuracy yelp review data summarization dataset composed abstract title pairs arxiv title pairs selected length title abstract exceed words respectively partitioned training validation test sets pairs train sequence sequence model generate title given abstract randomly selected subset paired data proportion value considered purely supervised summarization abstract title pairs supervised summarization leveraging additional abstracts titles compared lstm deconvolutional network decoder generating titles obs proportion supervised semi sup dcnn dec table summarization task arxiv data rouge l metric columns lstm decoder column deconvolutional decoder observed table summarizes quantitative results rouge l longest common sequence general additional abstracts titles improve eralization ability test set estingly titles observed joint training objective yields better performance lsup presumably joint training objective requires latent representation capable reconstructing input paragraph addition generating title learned representation better capture entire structure meaning paragraph empirically observed titles generated joint training objective likely use words appearing corresponding paragraph e extractive titles generated purely supervised objective lsup tend use wording freely abstractive possible explanation joint training strategy reconstructed paragraph title generated latent representation h text fragments reconstructing input paragraph likely leveraged building title title bears resemblance input paragraph expected titles produced deconvolutional decoder coherent lstm decoder presumably paragraph summarized multiple plausible titles deconvolutional decoder trouble positioning text segments provide discussions titles generated different setups sm designing framework takes best worlds lstm generation cnn decoding interesting future direction conclusion proposed general framework text modeling purely convolutional deconvolutional operations proposed method free sequential conditional generation avoiding issues associated exposure bias teacher forcing training approach enables model fully encapsulate paragraph latent representation vector decompressed reconstruct original input sequence empirically proposed approach achieved excellent long paragraph reconstruction quality outperforms existing algorithms spelling correction semi supervised sequence classication summarization largely reduced computational cost labeled supervisedsemi cnn cnn lstm references andrew m dai quoc v le semi supervised sequence learning nips quoc le tomas mikolov distributed representations sentences documents icml rie johnson tong zhang supervised semi supervised text categorization lstm region embeddings arxiv february takeru miyato andrew m dai ian goodfellow adversarial training methods semi supervised dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning text classication iclr align translate iclr kyunghyun cho bart van merrinboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase representations rnn encoder decoder statistical machine translation emnlp fandong meng zhengdong lu mingxuan wang hang li wenbin jiang qun liu encoding source language convolutional neural network machine translation acl tsung hsien wen milica gasic nikola mrksic pei hao su david vandyke steve young mantically conditioned lstm based natural language generation spoken dialogue systems arxiv jiwei li monroe alan ritter michel galley jianfeng gao dan jurafsky deep reinforcement learning dialogue generation arxiv jiwei li monroe tianlin shi alan ritter dan jurafsky adversarial learning neural dialogue generation ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre bing xiang abstractive text summarization sequence sequence rnns conll shashi narayan nikos papasarantopoulos mirella lapata shay b cohen neural extractive rization information arxiv april alexander m rush sumit chopra jason weston neural attention model abstractive sentence ilya sutskever oriol vinyals quoc v le sequence sequence learning neural networks summarization emnlp nips tomas mikolov martin karat lukas burget jan sanjeev khudanpur recurrent neural network based language model interspeech sepp hochreiter jrgen schmidhuber long short term memory neural computation junyoung chung caglar gulcehre kyunghyun cho yoshua bengio empirical evaluation gated recurrent neural networks sequence modeling arxiv ronald j williams david zipser learning algorithm continually running fully recurrent neural networks neural computation samy bengio oriol vinyals navdeep jaitly noam shazeer scheduled sampling sequence prediction recurrent neural networks nips ferenc huszr train generative model scheduled sampling likelihood adversary arxiv sentences acl nal kalchbrenner edward grefenstette phil blunsom convolutional neural network modelling yoon kim convolutional neural networks sentence classication emnlp ishaan gulrajani kundan kumar faruk ahmed adrien ali taiga francesco visin david vazquez aaron courville pixelvae latent variable model natural images arxiv alec radford luke metz soumith chintala unsupervised representation learning deep convolutional generative adversarial networks arxiv vinod nair geoffrey e hinton rectied linear units improve restricted boltzmann machines icml ian chiswell wilfrid hodges mathematical logic volume oup oxford emil julius gumbel julius lieblein statistical theory extreme values practical applications pages series lectures ronan collobert jason weston lon bottou michael karlen koray kavukcuoglu pavel kuksa natural language processing scratch jmlr sharan chetlur cliff woolley philippe vandermersch jonathan cohen john tran bryan catanzaro evan shelhamer cudnn efcient primitives deep learning arxiv zichao yang diyi yang chris dyer xiaodong alex smola eduard hovy hierarchical attention networks document classication naacl adji b dieng chong wang jianfeng gao john paisley topicrnn recurrent neural network long range semantic dependency iclr diederik p kingma shakir mohamed danilo jimenez rezende max welling semi supervised learning deep generative models nips yunchen pu zhe gan ricardo henao xin yuan chunyuan li andrew stevens lawrence carin variational autoencoder deep learning images labels captions nips sepp hochreiter yoshua bengio paolo frasconi jrgen schmidhuber gradient ow recurrent nets difculty learning long term dependencies richard socher jeffrey pennington eric h huang andrew y ng christopher d manning supervised recursive autoencoders predicting sentiment distributions emnlp association computational linguistics samuel r bowman luke vilnis oriol vinyals andrew m dai rafal jozefowicz samy bengio generating sentences continuous space arxiv zichao yang zhiting hu ruslan salakhutdinov taylor berg kirkpatrick improved variational autoencoders text modeling dilated convolutions arxiv february baotian hu zhengdong lu hang li qingcai chen convolutional neural network architectures matching natural language sentences nips rie johnson tong zhang effective use word order text categorization convolutional neural networks naacl hlt convolutional net arxiv tion iclr jost tobias springenberg alexey dosovitskiy thomas brox martin riedmiller striving simplicity karen simonyan andrew zisserman deep convolutional networks large scale image stanislau semeniuta aliaksei severyn erhardt barth hybrid convolutional variational coder text generation arxiv february nal kalchbrenner lasse espeholt karen simonyan aaron van den oord alex graves koray kavukcuoglu neural machine translation linear time arxiv yann n dauphin angela fan michael auli david grangier language modeling gated convolutional networks arxiv december j gehring m auli d grangier d yarats y n dauphin convolutional sequence sequence learning arxiv aaron van den oord nal kalchbrenner lasse espeholt oriol vinyals alex graves al conditional image generation pixelcnn decoders nips pages jiwei li minh thang luong dan jurafsky hierarchical neural autoencoder paragraphs documents acl tomas mikolov ilya sutskever kai chen greg s corrado jeff dean distributed representations words phrases compositionality nips kam fai wong mingli wu wenjie li extractive summarization supervised semi supervised learning iccl association computational linguistics chin yew lin rouge package automatic evaluation summaries acl workshop kishore papineni salim roukos todd ward wei jing zhu bleu method automatic evaluation machine translation acl association computational linguistics xiang zhang junbo zhao yann lecun character level convolutional networks text classication nips pages dzmitry bahdanau philemon brakel kelvin xu anirudh goyal ryan lowe joelle pineau aaron courville yoshua bengio actor critic algorithm sequence prediction arxiv jp woodard jt nelson information theoretic measure speech recognition performance workshop standardisation speech o diederik kingma jimmy ba adam method stochastic optimization iclr xavier glorot yoshua bengio understanding difculty training deep feedforward neural networks aistats
