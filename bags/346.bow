understanding neural abstractive summarization models via uncertainty jiacheng xu shrey desai department of computer science the university of texas at austin greg durrett jcxu utexas edu edu t c o l c s c v v i x r a abstract an advantage of abstractive rization models is that they generate text in a free form manner but this exibility makes it difcult to interpret model behavior in this work we analyze summarization decoders in both blackbox and whitebox ways by studying on the entropy or uncertainty of the model s token level predictions for two strong trained models pegasus zhang et al and bart lewis et al on two summarization datasets we nd a strong relation between low prediction entropy and where the model copies tokens rather than erating novel text the decoder s uncertainty also connects to factors like sentence position and syntactic distance between adjacent pairs of tokens giving a sense of what factors make a context particularly selective for the model s next output token finally we study the tionship of decoder uncertainty and attention behavior to understand how attention gives rise to these observed effects in the model we show that uncertainty is a useful perspective for analyzing summarization and text tion models more broadly introduction recent progress in abstractive summarization has been fueled by the advent of large scale ers pre trained on autoregressive language eling objectives hoang et al khandelwal et al lewis et al zhang et al despite their strong performance on automatic rics like rouge lin abstractive models are not as straightforward and interpretable as their extractive counterparts free form generation in these models also leads to serious downstream rors such as factual inconsistencies with the input document cao et al kryscinski et al is available at jiacheng xu text sum uncertainty wang et al durmus et al goyal and durrett although the interpretability of nlu models has been extensively studied ribeiro et al ghaeini et al jain and wallace desai and durrett summarization models specically have not received similar tion with analysis efforts often focused on datasets and evaluation kryscinski et al in this work we focus on interpreting and standing abstractive summarization models through the lens of decoder uncertainty or the entropy of decisions during generation while uncertainty in generation has been studied from the perspective of data ott et al sampling fan et al holtzman et al and training correia et al kang and hashimoto it is lized as a technique for analysis and inspection of generation systems we study two prominent summarization models pegasus zhang et al and bart lewis et al ne tuned on two english summarization datasets cnn daily mail hermann et al and xsum narayan et al to understand model behavior in each setting first by comparing n grams between the input document and generated summaries we establish two coarse types for decoded tokens copy and erate see et al we nd that the entropy of the generation decision correlates with whether the model is copying or generating as well as where in the sentence the token is this paints a picture of certain contexts being more restrictive from the standpoint of generation particularly early in tences where a model has not decided what to copy yet and illustrates the interaction of content selection and lexical choice second we extend this analysis by looking at how uncertainty relates to the syntax of the generated sentence whether uncertainty connects to syntactic notions of prisal roark et al and how the entropy varies across certain syntactic productions finally we derive a way to quantify decoder attention by aggregating distinct self attention heads ing the correlation between the attention entropy and prediction entropy and investigating the spondence between the prediction entropy and the fraction of the past and future decoded tokens taking this analysis together we nd that the abstractiveness of reference summaries tally changes model behavior the extractive nature of cnn dm makes most of its decisions low tropy and copy oriented while the model maintains higher uncertainty on xsum yielding more stractive summaries more broadly we show that uncertainty is a simple but effective tool to terize decoder behavior in text generation model and experimental setup our experiments use pegasus zhang et al and bart lewis et al two state the art pre trained models we use the large version of these two models which have and transformer layers respectively both models have pre training objectives tailored what to this problem domain modeling for denoising bart or inlling of masked out tences pegasus we directly use the pre trained models from wolf et al as reported in the original papers and sured by l lin pegasus achieves on cnn dm mann et al and on xsum narayan et al and bart achieves and entropy entropy is a standard measure of certainty in a probabilistic distribution given a discrete random variable x with all possible comes xn the entropy of x is dened as p xi log p xi for pre trained transformers the domain of the predictions the vocabulary is large and also fers between models the vocabulary sizes for pegasus and bart are and and the prediction distribution is usually long tailed google pegasus cnn dailymail google pegasus xsm facebook bart large cnn and facebook bart large xsum for pegasus and bart on these two datasets that entropy generally increases as the variable s domain grows a uniform distribution over outcomes has entropy while a uniform distribution over outcomes has entropy to combat this nucleus sampling holtzman et al is used to sample from only the top most probable outcomes the nucleus to avoid erating very unlikely tokens to more fairly pare models with different vocabulary sizes and to better reect the actual sampling distribution we therefore compute all entropy values in this work over the nucleus distribution that is we sort the prediction distribution p xi in ing order and get a minimal set of tokens where v min xiv min p xi then we normalize the distribution as follows p p if xi v min otherwise the probability cumulative where xiv min p xi we use p for all experiments the entropy is computed based on the new distribution p model uncertainty during generation in this section we analyze and compare the tion uncertainty from different models and different datasets by inspecting entropy values during ation allowing us to localize uncertainty to certain positions in a decoded sentence a principle factor that past work has investigated is the amount of copying in abstractive summarization models see et al paulus et al we rst aim to derstand how decisions to copy document content or generate new text are reected in the model s uncertainty one complicating factor is that while bart and pegasus both exhibit a mix of copying and novel generation they do not have an explicit copy ation like in past models and so these behaviors are more difcult to dene we rst separate tion decisions by bigrams that appear in the input document existing bigrams or whether they are free form generations novel bigrams figure shows a histogram of model entropies broken down by these two categories most notably there is a strong correlation between copy like behavior and the entropy of the model s tion distribution on cnn dm we see that low entropy decisions are largely those generating isting bigrams and conversely existing bigrams are usually generated with low entropy new grams are generated with a broad range of high are dened based on tokens rather than pieces and so may consist of more than two generation steps figure next token entropies computed on k eration steps from pegasuscnn dm pegasusxsum bartcnn dm and bartxsum respectively broken into two cases an existing bigram means the bigram just generated occurs in the input document while a novel bigram is an organic model generation these cases are associated with low entropy and high entropy actions respectively the axis shows the entropy truncated at and the y axis shows the count of bigram falling in each bin the dashed lines indicate the median of each distribution entropy values and are much more frequent on xsum these results align with our manual ysis of these summaries pegasuscnn dm and bartcnn dm summaries largely consist of spans from the input document with minor compression while pegasusxsum and bartxsum summaries involve stitching together disparate concepts and paraphrasing key details this reects a sponding divergence in the gold summaries where cnn dm summaries are far more extractive than those in xsum critically though the entropy distributions are dissimilar across the two datasets we see ities among the approximate copy and generate operations on cnn dm and xsum the median entropy values of using existing bigrams are and respectively and for generating new grams and with this connection between entropy and ing behavior we make the following additional observations based on figures and entropy varies across token positions cially on cnn dm in figure we depict a different view of entropy looking at the ing process as it progresses through each sentence across both cnn dm and xsum models are most uncertain at the beginning of the sentence and least uncertain at the end of the sentence however the rate at which entropy drops off is quite ferent on cnn dm the entropy after decoding of tokens falls below while the entropies figure prediction entropy values by relative sentence positions for example indicates the rst of tokens in a sentence and is the last of tokens pegasuscnn dm and bartcnn dm make highly certain decisions to start but then entropy decreases suggesting that these models may be copying based on a sentence prex entropies on xsum are more stant across the sentence on xsum only begin to considerably drop after decoding of tokens our manual analysis gests the following characterization to generate each sentence on cnn dm the model makes some high entropy decisions to identify a tence and begin to copy its prex followed by a series of low entropy decisions to copy that sentence s content on xsum which is highly abstractive and features single sentence summaries content planning and generation are less clearly decoupled pegasus copies and generates more tokens with entropy bart and pegasus port similar rouge results on cnn dm but these models do not place the same distributions over summaries pegasus has more low entropy copying decisions and its start of sentence tropies are also signicantly lower figure this suggests that it is more condent than bart in lecting content to discuss next there are also more low entropy generation decisions particularly on xsum entropies of syntactic productions having observed connections between sentence position and entropy we now esh out this analysis from the lens of syntax focusing in particular on uncertainty at constituent boundaries from our pegasus generations on cnn dm and xsum dmpegasusbartexisting bigramsnovel positionentropy production rule example np np np arsenal vs the game that changed the np np sbar who has not been np cd nn nns np nnp cd table examples of specic np productions with high entropy top and low entropy bottom the tation implies the constituent y is generated with entropy might be straightforward perhaps due to a direct copy from the document but generating a sitional phrase might be more challenging due to the large search space of possible constructions or the higher chance that the model might delete this constituent low entropy spans are often short specic units of information we also investigate the erage entropy of spans within a rule production to uncover what types of spans are likely to elicit certainty or uncertainty during generation in ble we see qualitatively that productions with low average entropy productions are short extracts of document content such as felony counts these are largely factual often containing cardinal ues and more likely to be copied within these constituents the model is very certain about what to generate next supporting the connection with low syntactic distance understanding decoder self attention while we have analyzed the model s predictions we have not yet determined how the different haviors we see emerge from the context our goal is to explore what the encoder attention places its emphasis during generation and how it correlates with the prediction entropy blocking low information tokens analyzing the inner workings of attention in transformers is challenging clark et al kovaleva et al particularly because many heads are useless redundant or noisy and they frequently attend to pegasus and bart models the encoder and decoder attention during decoding are two separate distributions where the encoder attention looks at the encoding context and the decoder attention attends to the previously decoded tokens in this paper we chiey examine the encoder attention to understand how the model references the input document figure correlating syntactic distance between boring tokens with the entropy change in those tokens generation decisions for pegasus summaries the median entropy change is depicted as a dashed black line at points of high syntactic distance the model s behavior is less restricted by the context correlating with higher entropy we obtain constituency parses for each summary sentence using the berkeley neural parser kitaev and klein and explore connections between syntax and uncertainty in more depth low and high entropy decisions can be ized to constituent span boundaries parsing has long been used to explain psycholinguistic tions of surprisal hale roark et al inter alia which are in turn related to uncertainty under a language model in our case uncertainty about generating a text is a different notion than uncertainty when a reader is processing it hence rather than looking at an incremental parser s havior we instead look at a simpler notion of tactic distance shen et al or the number of left and right parentheses between wt and in a linearized constituency tree our hypothesis is that when these words exhibit high syntactic distance this word boundary is a choice point where the model may be less restricted in what it can choose to generate next figure shows the correlation between syntactic distance and the percent change in entropy between the adjacent tokens on both cnn dm and xsum we see two patterns emerge generating a token within the same immediate parent constituent i e zero syntactic distance is typically a certain cision while generating a token belonging to a new constituent is an increasingly uncertain sion from these results we can draw a parallel to the copy vs generate behavior established in section for example generating york after new figure correlation between attention entropy and prediction entropy of and bart on dm and we compute the mean value of the attention entropy within each bucket of tion entropy the uncertainty of attention strongly relates with the entropy of the model s prediction low information tokens such as end of sentence markers or periods inspired by tf idf joachims we propose a method to compute a set of tokens most meaningfully attended to by the model if a token in the encoding document is attended to across many time steps like a word appearing in many documents in tf idf we want to disregard it in our analysis let t denote the number of decoder timesteps and l be the length of the source document we compute an aggregate attention matrix s rt by summing the attentions across all heads and all layers we then compute a count of how often each token is attended to above a threshold q fl q and discard the attention values on tokens with the highest score in practice we discard of tokens from the source document attention entropy one natural question we can ask is whether there is a connection between tropy of the attention distribution and entropy of the decoder s prediction this relationship is shown in figure where each point represents the mean tention entropy within the corresponding prediction entropy bucket the attention entropy is especially low where the prediction entropy ranges from to for cases with prediction entropy greater than the attention entropy saturates and no longer grows with the prediction entropy except the bartcnn dm while attention entropy is ably not causing the low decoder entropy nevertheless decoder entropy provides a lens into the inner workings of the transformer model projecting attention to vocabulary we pothesize that low decoder entropies may arise if the model is heavily attending to certain relevant kens particularly the about to be predicted token yt of time step t and the input token of this time figure vocabulary projected attention attending to the last input current input current output yt and next output when the prediction entropy is low the attention mostly focus a few tokens including the current input and current output yt step xt equivalent to for the predicted token yt we compute the vocabulary projected attention value where we late the attention of all of the occurrences of the specied token yt in the document the higher the value the more attention put to the encoding which are predicted for this time step during decoding we can dene the value for last time step input current time step input and the not yet decoded token for next time step we show the relationship between the lary projected attention and the prediction entropy in figure visualizations for both models and both datasets show that when the prediction tropy is low the attention focuses heavily on a few tokens including the current input token and the current token to predict this suggests a tential mechanism where the model indexes into the source document by attending to then strongly identies and reads off as the next token to generate conclusion this work analyzes pre trained summarization models via uncertainty or the entropy of ing decisions we pursue several lines of inquiry uncertainty can help us understand copying ment spans vs generating novel text the behavior of models in different syntactic environments and coarse properties of the model s attention tion all of these give insight into what conditions most heavily restrict the model s generation ating an observed bigram copying low syntactic distance and attention which can easily identify decoder context in the source document we lieve this approach can power future analyses of pre trained text generation systems entropyvocab projected attention acknowledgments this work was partially supported by nsf grant nsf grant a gift from salesforce inc and an equipment grant from nvidia the authors acknowledge the texas vanced computing center tacc at the sity of texas at austin for providing hpc resources used to conduct this research results presented in this paper were obtained using the chameleon testbed supported by the national science tion thanks as well to the anonymous reviewers for their helpful comments references ziqiang cao furu wei wenjie li and sujian li faithful to the original fact aware neural in proceedings of the aaai tive summarization conference on articial intelligence aaai kevin clark urvashi khandelwal omer levy and christopher d manning what does bert look at an analysis of bert s attention in ceedings of the acl workshop blackboxnlp lyzing and interpreting neural networks for nlp goncalo m correia vlad niculae and andre ft tins adaptively sparse transformers in ceedings of the conference on empirical ods in natural language processing and the ternational joint conference on natural language processing emnlp ijcnlp pages shrey desai and greg durrett calibration of in proceedings of the pre trained transformers conference on empirical methods in natural guage processing emnlp esin durmus he he and mona diab feqa a question answering evaluation framework for faithfulness assessment in abstractive tion in proceedings of the annual conference of the association for computational linguistics acl angela fan mike lewis and yann dauphin hierarchical neural story generation in ings of the annual conference of the association for computational linguistics acl reza ghaeini xioali fern and prasad tadepalli interpreting recurrent and attention based neural models a case study on natural language in proceedings of the conference on ence pirical methods in natural language processing emnlp john hale a probabilistic earley parser as a cholinguistic model in proceedings of the second meeting of the north american chapter of the ciation for computational linguistics karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend in proceedings of the ference on neural information processing systems neurips andrew pau hoang antoine bosselut asli c elikyilmaz and yejin choi efcient tation of pretrained transformers for abstractive summarization arxiv preprint ari holtzman jan buys li du maxwell forbes and yejin choi the curious case of neural text degeneration in proceedings of the conference on international conference on learning tions iclr sarthak jain and byron c wallace attention is in proceedings of the not explanation ference of the north american chapter of the ciation for computational linguistics human guage technologies naacl hlt thorsten joachims a probabilistic analysis of the rocchio algorithm with tfidf for text rization in icml daniel kang and tatsunori hashimoto proved natural language generation via loss tion arxiv preprint urvashi khandelwal k clark daniel jurafsky and lukasz kaiser sample efcient text marization using a single pre trained transformer arxiv preprint nikita kitaev and dan klein constituency ing with a self attentive encoder in proceedings of the annual meeting of the association for putational linguistics acl olga kovaleva alexey romanov anna rogers and anna rumshisky revealing the dark crets of bert in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp wojciech kryscinski nitish shirish keskar bryan cann caiming xiong and richard socher neural text summarization a critical evaluation in proceedings of the conference on cal methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp tanya goyal and greg durrett evaluating tuality in generation with dependency level ment in findings of the conference on empirical methods in natural language processing findings of emnlp wojciech kryscinski bryan mccann caiming xiong and richard socher evaluating the factual consistency of abstractive text summarization in proceedings of the conference on empirical ods in natural language processing emnlp thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi ric cistac tim rault remi louf morgan icz and jamie brew huggingface s formers state of the art natural language ing arxiv preprint jingqing zhang yao zhao mohammad saleh and ter j liu pegasus pre training with tracted gap sentences for abstractive tion proceedings of machine learning research pmlr mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer bart denoising sequence to sequence training for natural language generation translation and comprehension in proceedings of the nual meeting of the association for computational linguistics pages online association for computational linguistics chin yew lin rouge a package for matic evaluation of summaries in proceedings of the annual meeting of the association for tional linguistics acl shashi narayan shay b cohen and mirella lapata do nt give me the details just the mary topic aware convolutional neural networks for extreme summarization in proceedings of the conference on empirical methods in natural guage processing emnlp myle ott michael auli david grangier and marcaurelio ranzato analyzing in tainty in neural machine translation national conference on machine learning pages romain paulus caiming xiong and richard socher a deep reinforced model for abstractive summarization in proceedings of the international conference on learning representations iclr marco tulio ribeiro sameer singh and carlos guestrin why should i trust you plaining the predictions of any classier in proceedings of the acm sigkdd conference on knowledge discovery and data mining sigkdd brian roark asaf bachrach carlos cardenas and christophe pallier deriving lexical and tactic expectation based measures for tic modeling via incremental top down parsing in proceedings of the conference on empirical methods in natural language processing pages singapore association for computational linguistics abigail see peter j liiu and christopher d ning get to the point summarization in proceedings with pointer generator networks of the annual meeting of the association for putational linguistics acl yikang shen zhouhan lin chin wei huang and aaron courville neural language modeling in by jointly learning syntax and lexicon ceedings of the international conference on ing representations iclr alex wang kyunghyun cho and mike lewis asking and answering questions to evaluate the in factual consistency of summaries ings of the annual conference of the association for computational linguistics acl
