understanding neural abstractive summarization models uncertainty jiacheng xu shrey desai department computer science university texas austin greg durrett jcxu utexas edu edu t c o l c s c v v x r abstract advantage abstractive rization models generate text free form manner exibility makes difcult interpret model behavior work analyze summarization decoders blackbox whitebox ways studying entropy uncertainty model s token level predictions strong trained models pegasus zhang et al bart lewis et al summarization datasets nd strong relation low prediction entropy model copies tokens erating novel text decoder s uncertainty connects factors like sentence position syntactic distance adjacent pairs tokens giving sense factors context particularly selective model s output token finally study tionship decoder uncertainty attention behavior understand attention gives rise observed effects model uncertainty useful perspective analyzing summarization text tion models broadly introduction recent progress abstractive summarization fueled advent large scale ers pre trained autoregressive language eling objectives hoang et al khandelwal et al lewis et al zhang et al despite strong performance automatic rics like rouge lin abstractive models straightforward interpretable extractive counterparts free form generation models leads downstream rors factual inconsistencies input document cao et al kryscinski et al available jiacheng xu text sum uncertainty wang et al durmus et al goyal durrett interpretability nlu models extensively studied ribeiro et al ghaeini et al jain wallace desai durrett summarization models specically received similar tion analysis efforts focused datasets evaluation kryscinski et al work focus interpreting standing abstractive summarization models lens decoder uncertainty entropy decisions generation uncertainty generation studied perspective data ott et al sampling fan et al holtzman et al training correia et al kang hashimoto lized technique analysis inspection generation systems study prominent summarization models pegasus zhang et al bart lewis et al ne tuned english summarization datasets cnn daily mail hermann et al xsum narayan et al understand model behavior setting comparing n grams input document generated summaries establish coarse types decoded tokens copy erate et al nd entropy generation decision correlates model copying generating sentence token paints picture certain contexts restrictive standpoint generation particularly early tences model decided copy illustrates interaction content selection lexical choice second extend analysis looking uncertainty relates syntax generated sentence uncertainty connects syntactic notions prisal roark et al entropy varies certain syntactic productions finally derive way quantify decoder attention aggregating distinct self attention heads ing correlation attention entropy prediction entropy investigating spondence prediction entropy fraction past future decoded tokens taking analysis nd abstractiveness reference summaries tally changes model behavior extractive nature cnn dm makes decisions low tropy copy oriented model maintains higher uncertainty xsum yielding stractive summaries broadly uncertainty simple effective tool terize decoder behavior text generation model experimental setup experiments use pegasus zhang et al bart lewis et al state art pre trained models use large version models transformer layers respectively models pre training objectives tailored problem domain modeling denoising bart inlling masked tences pegasus directly use pre trained models wolf et al reported original papers sured l lin pegasus achieves cnn dm mann et al xsum narayan et al bart achieves entropy entropy standard measure certainty probabilistic distribution given discrete random variable x possible comes xn entropy x dened p xi log p xi pre trained transformers domain predictions vocabulary large fers models vocabulary sizes pegasus bart prediction distribution usually long tailed google pegasus cnn dailymail google pegasus xsm facebook bart large cnn facebook bart large xsum pegasus bart datasets entropy generally increases variable s domain grows uniform distribution outcomes entropy uniform distribution outcomes entropy combat nucleus sampling holtzman et al sample probable outcomes nucleus avoid erating unlikely tokens fairly pare models different vocabulary sizes better reect actual sampling distribution compute entropy values work nucleus distribution sort prediction distribution p xi ing order minimal set tokens v min xiv min p xi normalize distribution follows p p xi v min probability cumulative xiv min p xi use p experiments entropy computed based new distribution p model uncertainty generation section analyze compare tion uncertainty different models different datasets inspecting entropy values ation allowing localize uncertainty certain positions decoded sentence principle factor past work investigated copying abstractive summarization models et al paulus et al rst aim derstand decisions copy document content generate new text reected model s uncertainty complicating factor bart pegasus exhibit mix copying novel generation explicit copy ation like past models behaviors difcult dene rst separate tion decisions bigrams appear input document existing bigrams free form generations novel bigrams figure shows histogram model entropies broken categories notably strong correlation copy like behavior entropy model s tion distribution cnn dm low entropy decisions largely generating isting bigrams conversely existing bigrams usually generated low entropy new grams generated broad range high dened based tokens pieces consist generation steps figure token entropies computed k eration steps pegasuscnn dm pegasusxsum bartcnn dm bartxsum respectively broken cases existing bigram means bigram generated occurs input document novel bigram organic model generation cases associated low entropy high entropy actions respectively axis shows entropy truncated y axis shows count bigram falling bin dashed lines indicate median distribution entropy values frequent xsum results align manual ysis summaries pegasuscnn dm bartcnn dm summaries largely consist spans input document minor compression pegasusxsum bartxsum summaries involve stitching disparate concepts paraphrasing key details reects sponding divergence gold summaries cnn dm summaries far extractive xsum critically entropy distributions dissimilar datasets ities approximate copy generate operations cnn dm xsum median entropy values existing bigrams respectively generating new grams connection entropy ing behavior following additional observations based figures entropy varies token positions cially cnn dm figure depict different view entropy looking ing process progresses sentence cnn dm xsum models uncertain beginning sentence uncertain end sentence rate entropy drops ferent cnn dm entropy decoding tokens falls entropies figure prediction entropy values relative sentence positions example indicates rst tokens sentence tokens pegasuscnn dm bartcnn dm highly certain decisions start entropy decreases suggesting models copying based sentence prex entropies xsum stant sentence xsum begin considerably drop decoding tokens manual analysis gests following characterization generate sentence cnn dm model makes high entropy decisions identify tence begin copy prex followed series low entropy decisions copy sentence s content xsum highly abstractive features single sentence summaries content planning generation clearly decoupled pegasus copies generates tokens entropy bart pegasus port similar rouge results cnn dm models place distributions summaries pegasus low entropy copying decisions start sentence tropies signicantly lower figure suggests condent bart lecting content discuss low entropy generation decisions particularly xsum entropies syntactic productions having observed connections sentence position entropy esh analysis lens syntax focusing particular uncertainty constituent boundaries pegasus generations cnn dm xsum dmpegasusbartexisting bigramsnovel positionentropy production rule example np np np arsenal vs game changed np np sbar np cd nn nns np nnp cd table examples specic np productions high entropy low entropy tation implies constituent y generated entropy straightforward direct copy document generating sitional phrase challenging large search space possible constructions higher chance model delete constituent low entropy spans short specic units information investigate erage entropy spans rule production uncover types spans likely elicit certainty uncertainty generation ble qualitatively productions low average entropy productions short extracts document content felony counts largely factual containing cardinal ues likely copied constituents model certain generate supporting connection low syntactic distance understanding decoder self attention analyzed model s predictions determined different haviors emerge context goal explore encoder attention places emphasis generation correlates prediction entropy blocking low information tokens analyzing inner workings attention transformers challenging clark et al kovaleva et al particularly heads useless redundant noisy frequently attend pegasus bart models encoder decoder attention decoding separate distributions encoder attention looks encoding context decoder attention attends previously decoded tokens paper chiey examine encoder attention understand model references input document figure correlating syntactic distance boring tokens entropy change tokens generation decisions pegasus summaries median entropy change depicted dashed black line points high syntactic distance model s behavior restricted context correlating higher entropy obtain constituency parses summary sentence berkeley neural parser kitaev klein explore connections syntax uncertainty depth low high entropy decisions ized constituent span boundaries parsing long explain psycholinguistic tions surprisal hale roark et al inter alia turn related uncertainty language model case uncertainty generating text different notion uncertainty reader processing looking incremental parser s havior instead look simpler notion tactic distance shen et al number left right parentheses wt linearized constituency tree hypothesis words exhibit high syntactic distance word boundary choice point model restricted choose generate figure shows correlation syntactic distance percent change entropy adjacent tokens cnn dm xsum patterns emerge generating token immediate parent constituent e zero syntactic distance typically certain cision generating token belonging new constituent increasingly uncertain sion results draw parallel copy vs generate behavior established section example generating york new figure correlation attention entropy prediction entropy bart dm compute mean value attention entropy bucket tion entropy uncertainty attention strongly relates entropy model s prediction low information tokens end sentence markers periods inspired tf idf joachims propose method compute set tokens meaningfully attended model token encoding document attended time steps like word appearing documents tf idf want disregard analysis let t denote number decoder timesteps l length source document compute aggregate attention matrix s rt summing attentions heads layers compute count token attended threshold q fl q discard attention values tokens highest score practice discard tokens source document attention entropy natural question ask connection tropy attention distribution entropy decoder s prediction relationship shown figure point represents mean tention entropy corresponding prediction entropy bucket attention entropy especially low prediction entropy ranges cases prediction entropy greater attention entropy saturates longer grows prediction entropy bartcnn dm attention entropy ably causing low decoder entropy decoder entropy provides lens inner workings transformer model projecting attention vocabulary pothesize low decoder entropies arise model heavily attending certain relevant kens particularly predicted token yt time step t input token time figure vocabulary projected attention attending input current input current output yt output prediction entropy low attention focus tokens including current input current output yt step xt equivalent predicted token yt compute vocabulary projected attention value late attention occurrences specied token yt document higher value attention encoding predicted time step decoding dene value time step input current time step input decoded token time step relationship lary projected attention prediction entropy figure visualizations models datasets prediction tropy low attention focuses heavily tokens including current input token current token predict suggests tential mechanism model indexes source document attending strongly identies reads token generate conclusion work analyzes pre trained summarization models uncertainty entropy ing decisions pursue lines inquiry uncertainty help understand copying ment spans vs generating novel text behavior models different syntactic environments coarse properties model s attention tion insight conditions heavily restrict model s generation ating observed bigram copying low syntactic distance attention easily identify decoder context source document lieve approach power future analyses pre trained text generation systems entropyvocab projected attention acknowledgments work partially supported nsf grant nsf grant gift salesforce inc equipment grant nvidia authors acknowledge texas vanced computing center tacc sity texas austin providing hpc resources conduct research results presented paper obtained chameleon testbed supported national science tion thanks anonymous reviewers helpful comments references ziqiang cao furu wei wenjie li sujian li faithful original fact aware neural proceedings aaai tive summarization conference articial intelligence aaai kevin clark urvashi khandelwal omer levy christopher d manning bert look analysis bert s attention ceedings acl workshop blackboxnlp lyzing interpreting neural networks nlp goncalo m correia vlad niculae andre ft tins adaptively sparse transformers ceedings conference empirical ods natural language processing ternational joint conference natural language processing emnlp ijcnlp pages shrey desai greg durrett calibration proceedings pre trained transformers conference empirical methods natural guage processing emnlp esin durmus mona diab feqa question answering evaluation framework faithfulness assessment abstractive tion proceedings annual conference association computational linguistics acl angela fan mike lewis yann dauphin hierarchical neural story generation ings annual conference association computational linguistics acl reza ghaeini xioali fern prasad tadepalli interpreting recurrent attention based neural models case study natural language proceedings conference ence pirical methods natural language processing emnlp john hale probabilistic earley parser cholinguistic model proceedings second meeting north american chapter ciation computational linguistics karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend proceedings ference neural information processing systems neurips andrew pau hoang antoine bosselut asli c elikyilmaz yejin choi efcient tation pretrained transformers abstractive summarization arxiv preprint ari holtzman jan buys li du maxwell forbes yejin choi curious case neural text degeneration proceedings conference international conference learning tions iclr sarthak jain byron c wallace attention proceedings explanation ference north american chapter ciation computational linguistics human guage technologies naacl hlt thorsten joachims probabilistic analysis rocchio algorithm tfidf text rization icml daniel kang tatsunori hashimoto proved natural language generation loss tion arxiv preprint urvashi khandelwal k clark daniel jurafsky lukasz kaiser sample efcient text marization single pre trained transformer arxiv preprint nikita kitaev dan klein constituency ing self attentive encoder proceedings annual meeting association putational linguistics acl olga kovaleva alexey romanov anna rogers anna rumshisky revealing dark crets bert proceedings conference empirical methods natural language processing international joint conference natural language processing emnlp ijcnlp wojciech kryscinski nitish shirish keskar bryan cann caiming xiong richard socher neural text summarization critical evaluation proceedings conference cal methods natural language processing international joint conference natural language processing emnlp ijcnlp tanya goyal greg durrett evaluating tuality generation dependency level ment findings conference empirical methods natural language processing findings emnlp wojciech kryscinski bryan mccann caiming xiong richard socher evaluating factual consistency abstractive text summarization proceedings conference empirical ods natural language processing emnlp thomas wolf lysandre debut victor sanh julien chaumond clement delangue anthony moi ric cistac tim rault remi louf morgan icz jamie brew huggingface s formers state art natural language ing arxiv preprint jingqing zhang yao zhao mohammad saleh ter j liu pegasus pre training tracted gap sentences abstractive tion proceedings machine learning research pmlr mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov luke zettlemoyer bart denoising sequence sequence training natural language generation translation comprehension proceedings nual meeting association computational linguistics pages online association computational linguistics chin yew lin rouge package matic evaluation summaries proceedings annual meeting association tional linguistics acl shashi narayan shay b cohen mirella lapata nt details mary topic aware convolutional neural networks extreme summarization proceedings conference empirical methods natural guage processing emnlp myle ott michael auli david grangier marcaurelio ranzato analyzing tainty neural machine translation national conference machine learning pages romain paulus caiming xiong richard socher deep reinforced model abstractive summarization proceedings international conference learning representations iclr marco tulio ribeiro sameer singh carlos guestrin trust plaining predictions classier proceedings acm sigkdd conference knowledge discovery data mining sigkdd brian roark asaf bachrach carlos cardenas christophe pallier deriving lexical tactic expectation based measures tic modeling incremental parsing proceedings conference empirical methods natural language processing pages singapore association computational linguistics abigail peter j liiu christopher d ning point summarization proceedings pointer generator networks annual meeting association putational linguistics acl yikang shen zhouhan lin chin wei huang aaron courville neural language modeling jointly learning syntax lexicon ceedings international conference ing representations iclr alex wang kyunghyun cho mike lewis asking answering questions evaluate factual consistency summaries ings annual conference association computational linguistics acl
