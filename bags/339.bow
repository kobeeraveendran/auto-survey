cdevalsumm empirical study cross dataset evaluation neural summarization systems yiran chen pengfei ming zhong danqing wang xipeng qiu xuanjing huang shanghai key laboratory intelligent information processing fudan university school computer science fudan university songhu road shanghai china mellon university edu zdou cmu edu abstract neural network based models augmented unsupervised pre trained knowledge achieved impressive performance text marization existing evaluation methods limited domain setting summarizers trained evaluated dataset argue proach narrow understanding generalization ability different tion systems paper perform depth analysis characteristics different datasets investigate performance ferent summarization models dataset setting summarizer trained corpus evaluated range domain corpora comprehensive study representative summarization tems datasets different domains veals effect model architectures generation ways abstractive tive model generalization ability experimental results shed light tions existing summarizers brief tion supplementary code found com cdevalsumm introduction neural summarizers achieved impressive formance evaluated rouge lin domain setting recent success trained models drives state art results benchmarks new level liu lapata liu zhong zhang lewis zhong superior performance guarantee fect system exsiting models tend fects evaluated aspects ple zhang observes tive systems tend near extractive practice authors contributed equally corresponding author figure ranking descending order current scoring summarization systems abstractive els red extractive ones blue tem evaluated based diverse evaluation ods averaging system scores datasets evaluating tems designed cross dataset measures stable sec notably bertmatch bart state art models extractive stractive summarization respectively highlighted blue red boxes cao wang kryscinski maynez durmus reveal generated summaries factually incorrect non mainstream tion methods easier identify model weaknesses orthogonal evaluation aspects aim diagnose limitation existing systems cross dataset evaluation marization system trained corpus evaluated range dataset corpora instead evaluating quality summarizers solely based dataset multiple datasets individually cross dataset evaluation enables evaluate model performance different gle example fig shows ranking summarization systems studied paper different evaluation metrics ranking list dataset obtained traditional ranking criteria based designed cross dataset measures intuitively observe different denitions good system evaluation aspects abstractive extractive systems exhibit diverse behaviors evaluated cross dataset setting example recaps general tion work encouraging rethink generalization ability current scoring marization systems perspective dataset evaluation specically ask tions follows different neural architectures summarizers inuence cross dataset tion performances designing tion systems plethora neural components adopted zhou chen bansal gehrmann cheng lapata nallapati example copy coverage mechanisms improve cross dataset eralization ability summarizers risk bert based summarizers perform worse adapted new areas compared ones bert far generalization ability current summarization systems transferring new datasets remains unclear poses signicant challenge design reliable system realistic scenarios work closer look effect model architectures cross dataset generalization setting different generation ways extractive abstractive summarizers inuence dataset generalization ability extractive stractive models typical ways summarize texts usually follow diverse learning frameworks favor different datasets absorbing know discrepancy perspective cross dataset generalization tive summarizers better generating tive faithful summaries new test set answer questions ducted comprehensive experimental analysis involves summarization systems cluding state art models benchmark datasets different domains tion aspects tab illustrates overall sis framework explore effect different architectures generation ways model eralization ability order answer semantic equivalency rouge framework semantic equivalency rouge factuality factcc architecture transformer lstm generation way bert bart sec sec sec sec table overall analysis framework ity adopted characterize different aspects cross dataset generalization ability ally strengthen analysis presenting views evaluation holistic grained views sec contributions summarized cross dataset evaluation orthogonal uation aspects semantic equivalence ity evaluate current marization systems accelerating creation robust summarization systems sign measures stiffness stableness help characterize generalization ity different views encouraging diagnose weaknesses state art systems conduct dataset bias aided analysis sec suggest better understanding datasets helpful interpret systems behaviours representative systems intractable cover neural marization systems try include sentative models comprehensive tion selection strategy follows source codes systems publicly available systems state art performance mace benchmark datasets cnndm pati systems equipped typical neural components transformer lstm mechanism copy extractive summarizers extractive summarizers directly choose output salient sentences phrases original document generally existing tive summarization systems follow framework consisting major modules sentence coder document encoder decoder investigate extractive summarizers different choices encoders decoders lstmnon kedzie summarizer adopts convolutional neural network sentence encoder lstm model cross sentence relation finally sentence selected non autoregressive way transnon liu lapata formerext model liu lapata similar setting document encoder replaced transformer layer transauto zhong decoder replaced pointer network avoid tion autoregressive bertnon liu lapata sumext model liu lapata model extension transnon introducing bert devlin layer bertmatch zhong isting state art extractive summarization system introduce matching layer siamese bert abstractive summarizers abstractive approach involves paraphrasing inputs novel words current tive summarization systems mainly focus encoder decoder paradigm model lstm ptr based sequence sequence summarizer copy coverage mechanism remove coverage module parts unchanged model implemented removing pointer network summarizer liu lapata sequence quence model transformer encoder decoder liu lapata sequence quence model bert encoder decoder bart lewis fully pre trained sequence sequence model existing art abstractive summarization system datasets explore typical summarization datasets cnndm xsum pubmed bigpatent reddit tifu cnndm nallapati xsum narayan news domain summarization datasets publications abstractiveness pubmed cohan scientic paper dataset investigate generalization ity models scientic domain bigpatent sharma category bigpatent dataset consisting patent uments google patents public datasets reddit tifu kim dataset formal posts collected online discussion forum reddit detailed statistics troduction datasets presented appendix section evaluation summarization existing summarization systems usually ated different datasets individually based automatic metric represents dataset cnndm system respectively denotes evaluation metric rouge figure different metrics characterized relation chart generated summaries gsum references ref input documents doc evaluate quality generated summaries metrics designed diverse perspectives abstractly characterized fig specically semantic equivalence tify relation generated summaries gsum references ref factuality aims characterize relation generated maries gsum input documents doc evaluation metrics paper introduce measures quantify relation input documents doc references ref claim better understanding dataset biases help interpret models crepancies semantic equivalence rouge lin classic metric evaluate quality model generated summaries ing number overlapped grams evaluated summaries ideal references factuality apart evaluating semantic equivalence generated summaries references evaluation aspect recent interest ality order analyze generalization mance models different perspectives gsumfactualitysem equa data biasdocref cnn xsum pubmed bigatent reddit figure characteristics test set dataset train set possesses property displayed coverage copy length novelty sentence fusion score repetition choose gram calculate novelty gram repetition work factuality evaluation consideration factcc factcc kryscinski duced measure fact consistency generated summaries source documents model based metric weakly supervised use proportion summary sentences factcc predicts factually consistent ality score paper dataset bias detail measures quantify characteristics datasets helpful understand differences models coverage grusky illustrates lap rate document summary ned proportion copied segments summary copy length measures average length ments summary copied source document novelty dened portion segments summaries appeared source documents segments instantiated grams repetition measures rate repeated segments summaries similar measure choose gram ranges segment unit sentence fusion score calculated sult algorithm proposed lebanoff summary sentence compressed sentence fused sentences sentence fusion score calculated proportion fused sentences tences fused document sentences summary sentences high value coverage copy length gests dataset extractive novelty represents rate novel units summary sentence fusion score represents proportion sentences fused ment sentences zhong explores dataset bias aid analysis model mance focus metrics extractive summarizers dataset bias analysis according coverage copy length sults fig cnndm extractive dataset bigpatent exhibits relatively higher copy rate summary copy ments shorter cnndm hand bigaptent xsum obtain higher sentence sion score suggests proportion fused sentences datasets high xsum reddit obtain gram novel units summary reecting datasets abstractive terms repetition fig pubmed bigpatent contain gram repeated phrases summary rouge models cnn cnn xsum pubm patent red lstmnon transnon transauto bertnon bertmatch ext abs ptr bart table representative summarizers studied paper corresponding performance score different datasets cnndm xsum pubmed bigpatent reddit implement systems datasets implemented results outperform slightly lower performances reported original papers column cnn lengthnoveltysent lengthnoveltysent lengthnoveltysent lengthnoveltysent lengthnoveltysent fusionrepetition measures stiff stable table illustration views stiffness bleness characterize cross dataset generalization based model represent cross dataset matrix models means model gains better cross dataset absolute performance suggests model robust cross dataset evaluation despite recent impressive results diverse marization datasets modern summarization tems mainly focus extensive dataset tecture engineering ignore tion ability indispensable systems required process samples new datasets domains instead evaluating quality summarization system solely based dataset introduce cross dataset evaluation summarizer trained dataset cnndm evaluated range datasets xsum methodologically form cross dataset evaluation views grained holistic detail methodology given summarization system set datasets evaluation metric design different evaluation function quantify system quality ing different forms function eval instantiated scalar vector matrix fine grained measures cross dataset evaluation result stantiated matrix characterize given system grained way specically cell refers metric result rouge summarizer trained dataset tested dataset refers number datasets additionally normalize cell diagonal value uij ujj uij ujj measures close dataset performance trained tested system dataset performance trained tested holistic measures instead matrix holistically tify cross dataset generalization ability summarization system scalar specically propose views characterize dataset generalization stiffness measure reects absolute formance system cross dataset setting given system stiffness calculated uij intuitively higher value stiffness suggests system obtains better performance ferred new datasets stableness characterizes relative mance gap dataset cross dataset test uij ujj generally higher value stableness suggests variance dataset dataset results smaller tab gives example characterize ization ability views shows stiffness stableness unanimous model higher stiffness obtains lower stableness stiffness stableness figure illustration stiffness stableness scores models yellow bars stand extractive models grey bars stand stractive models experiment follows analyze different tion systems terms semantic equivalence factuality results studied tic grained views based measures dened holistic results showed fig analysis aspect model type compare models holistic analysis cnn xsum pubm patent red avg cnn xsum pubm patent red avg architecture ext abs bertmatch bertnon stiff stable bertnon transnon stiff stable stiff stable ptr stiff stable generation way lstm lstmnon stiff stable bertsum bertnon stiff stable grain analysis cnn xsum pubm patent red avg cnn xsum pubm patent red avg cnn xsum pubm patent red avg cnn xsum pubm patent red avg cnn xsum pubm patent red avg cnn xsum pubm patent red avg table difference scores different model pairs column table represents compared results pair models line holistic analysis displays overall stiffness stableness compared models rest table grained results rst line origin compared results model pairs second line normalized compared results model pairs heatmap grey red represent positive negative respectively display compared results limited pairs models results displayed appendix fig hand tab tab display grained observations tab palys dataset results models benchmark datasets semantic equivalence analysis conduct pair wise wilcoxon signed rank nicant test null hypothesis expected performances stiffness stableness pair summarization models identical report observations tically signicant architecture match based reranking improves stiffness nicantly bertmatch semantic match scores rerank candidate summaries hances stiffness model signicantly fig obtaining comparable stableness extractive models fig indicates bertmatch increases absolute mance retaining robustness bertmatch stable transferred datasets bigpatent tab shows compared bertnon bertmatch obtains larger dataset cross dataset mance gap tested bigpatent bigpatent possesses higher tence fusion score higher repetition compared datasets sec demonstrates served test set dataset brings great lenge bertmatch correctly rank didate summaries provides ing signals served training set dataset bigpatent trained model obtain higher score compared cross dataset models trained datasets cause lower stableness non autoregressive decoder robust autoregressive extractive models garding decoder extractive systems shown fig fig non autoregressive tractive decoder transnon stable possesses lower stiffness autoregressive counterpart transauto pointer network coverage mechanism instrumental improving stiffness ness abstractive systems pointer work coverage mechanism enhance solute performance abstractive system fig demonstrates ptr stableness results fig reveals removing pointer mechanism value decreases suggests system stable augmented ability directly extract text spans source document pointer network brings trivial provement tested xsum reddit absolute model performance improvement pointer network trival tested xsum reddit showed tab line expectations datasets abstractive analyzed sec hand coverage ful tested reddit xsum harmful trained xsum heatmap ptr tab shows tested reddit xsum improvement coverage mechanism trivial datasets possess repetition coverage vide help transferred datasets trained xsum ptr gets lower stiffness compared accordance normalized result tab gold summaries xsum exhibit lower repetition score analyzed sec provide learning nals coverage mechanism bert brings unstableness shown fig doubt marizers extractive abstractive equipped pre trained encoder stiffness increase signicantly ing overall cross dataset performance improved surprised fig bert leads bleness result enlightens search tures learning schemas offset unstableness brought bert heatmap bertnon transnon tab shows bert brings unstableness cially tested reddit xsum bert harm absolute cross dataset performance bertnon performs worse transnon cells trained xsum tested cnndm tab bart shows superior performance terms stiffness stableness fig shows bart obtains highest stiffness stractive models comparable bertmatch addition bart outstanding terms stableness compared abstractive models fig performance gap bart proves tive models pre training sequence sequence model works better trained model encoder decoder generation ways extractive models superior abstractive models terms stiffness robustness stiffness stableness figure illustration stiffness stableness tuality scores models yellow bars stand extractive systems grey bars stand abstractive systems extractive models superior advantage solute performance shown fig comparing stableness abstractive tive models fig surprisingly abstractive approaches bart tremely brittle value lower extractive approaches maximum margin gap reduced troducing pointer network observation poses great challenge development tive systems encouraging research pay tention improve generalization ability provided hints solution enabling model extract granular information source document trained sequence sequence model bart tested xsum reddit abstractive systems possess comparable better formance supremacy extractive models retained datasets tab tab extractive models obtain higher stiffness scores tested cnndm pubmed stractive approaches obtained higher comparable stiffness scores tested xsum reddit xsum reddit abstractive analyzed sec factuality analysis extractive models achieve higher ity scores abstractive models obtain lower ones fig interesting observation extractive models factuality scores dataset setting tab diagonal values reveals limitation ext models cnn xsum pubm patent red avg cnn xsum pubm patent red avg transnon bertmatch cnn xsum pubm patent reddit avg cnn xsum pubm patent reddit avg bart abs models cnn xsum pubm patent red avg cnn xsum pubm patent red avg table cross dataset factuality scores extractive abstractive models existing factuality checker bart signicantly improve ability generate factual summaries compared abstractive models showed fig pared equipped pointer network tend copy source document abstractive models obtain higher stableness factuality scores fig surpass tested abstractive datasets xsum sec shows abstractive marizers trained dataset tend stractive obtain lower factuality score gets higher factuality score trained datasets extractive cnndm superiority cross dataset results dataset results leads higher stableness related work work connected following threads topics nlp research cross dataset generalization nlp cently researchers shift focus vidual dataset cross dataset evaluation aiming comprehensive understanding system generalization ability fried explores generalization ability different constituency parsers talmor berant hand shows generalization ability reading comprehension models improved training reading sion datasets studies model generalization eld ner point bottleneck existing ner systems depth analyses provide suggestions ther improvement different works attempt explore generalization ability summarization systems diagnosing limitations existing tion systems rouge recent works try explore weaknesses existing tems divese aspects zhang tries gure extent neural abstractive summarization systems abstractive ers abstractive systems tend perform near extractive hand cao kryscinski study factuality problem modern neural summarization systems puts forward model combining source document preliminary extracted fact scription prove effectiveness model terms factuality correctness ter contributes design model based automatic factuality evaluation metric abstractiveness factuality error works studied onal work easily combined cross dataset evaluation framework paper sec shows wang hua wang attempt investigate domain shift problem text summarization cus single generation way abstractive extractive investigate tion summarizers transferring different datasets include datasets models conclusion performing comprehensive evaluation summarization systems mainstream datasets summarize observations abstractive summarizers extremely tle compared extractive approaches maximum gap reaches terms measure stableness rouge dened paper bart sota system superior abstractive models comparable extractive models terms stiffness rouge hand robust transferring datasets possesses high stableness rouge bertmatch sota system performs excellently terms stiffness lacks bleness transferred bigpatent datasets robustness models improved equipped model ability copy span source document lebanoff use trained sequence sequence pre trained model bart simply adding bert encoder improve stiffness rouge model cause larger cross dataset dataset mance gap better way found merge bert abstractive model better training strategy applied offset negative inuence brings existing factuality checker factcc limited predictive power positive samples sec domain systems surpass domain systems terms ality sec acknowledgements like thank anonymous ers detailed comments constructive suggestions work supported tional natural science foundation china science technology parallel distributed processing laboratory pdl references ziqiang cao furu wei wenjie sujian faithful original fact aware neural abstractive summarization thirty second aaai conference articial intelligence yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proceedings annual ing association computational linguistics volume long papers volume pages jianpeng cheng mirella lapata neural marization extracting sentences words proceedings annual meeting sociation computational linguistics volume long papers volume pages arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang nazli goharian discourse aware attention model abstractive summarization long documents proceedings conference north american chapter association tional linguistics human language technologies volume short papers pages jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language ing arxiv preprint esin durmus mona diab feqa question answering evaluation framework fulness assessment abstractive summarization arxiv pages daniel fried nikita kitaev dan klein cross domain generalization neural constituency proceedings annual parsers ing association computational tics pages florence italy association computational linguistics jinlan pengfei liu zhang xuanjing huang rethinking generalization neural els named entity recognition case study arxiv preprint sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference cal methods natural language processing pages max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies proceedings conference north american chapter association computational linguistics man language technologies volume long pers volume pages jiatao zhengdong hang victor incorporating copying mechanism arxiv preprint sequence sequence learning karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read advances neural comprehend tion processing systems pages xinyu hua wang pilot study main adaptation effect neural abstractive rization arxiv preprint chris kedzie kathleen mckeown hal daume iii content selection deep learning models proceedings summarization ference empirical methods natural language processing pages byeongchang kim hyunwoo kim gunhee kim abstractive summarization reddit posts multi level memory networks ings conference north american chapter association computational guistics human language technologies volume long short papers pages wojciech kryscinski bryan mccann caiming xiong richard socher evaluating factual consistency abstractive text summarization arxiv preprint logan lebanoff kaiqiang song franck dernoncourt doo soon kim seokhwan kim walter chang fei liu scoring sentence singletons pairs abstractive summarization arxiv preprint mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov luke zettlemoyer bart denoising sequence sequence pre training natural translation comprehension arxiv preprint language generation chin yew lin rouge package matic evaluation summaries text summarization branches yang liu fine tune bert extractive marization yang liu mirella lapata text proceedings tion pretrained encoders conference empirical methods ural language processing international joint conference natural language processing emnlp ijcnlp pages joshua maynez shashi narayan bernd bohnet ryan mcdonald faithfulness ality abstractive summarization arxiv preprint ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments thirty aaai conference articial intelligence ramesh nallapati bowen zhou cicero dos santos glar bing xiang tive text summarization sequence sequence rnns conll page shashi narayan shay cohen mirella lapata details summary topic aware convolutional neural networks proceedings treme summarization conference empirical methods natural guage processing pages myle ott sergey edunov alexei baevski angela fan sam gross nathan david grangier fairseq fast extensible michael auli proceedings toolkit sequence modeling naacl hlt demonstrations abigail peter liu christopher ning point summarization proceedings pointer generator networks annual meeting association tational linguistics volume long papers ume pages eva sharma chen wang bigpatent large scale dataset abstractive coherent summarization arxiv preprint alon talmor jonathan berant multiqa empirical investigation generalization fer reading comprehension proceedings annual meeting association putational linguistics pages florence italy association computational linguistics ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need advances neural information cessing systems pages alex wang kyunghyun cho mike lewis asking answering questions evaluate factual consistency summaries arxiv preprint danqing wang pengfei liu ming zhong jie xipeng qiu xuanjing huang exploring domain shift extractive text summarization arxiv preprint fangfang zhang jin yao rui yan abstractiveness neural document proceedings conference tion empirical methods natural language processing pages brussels belgium association computational linguistics jingqing zhang yao zhao mohammad saleh ter liu pegasus pre training extracted gap sentences abstractive summarization arxiv preprint ming zhong pengfei liu yiran chen danqing wang xipeng qiu xuanjing huang extractive summarization text matching ming zhong pengfei liu danqing wang xipeng qiu xuan jing huang searching tive neural extractive summarization works proceedings annual meeting association computational guistics pages ming zhong danqing wang pengfei liu xipeng qiu xuan jing huang closer look data bias neural extractive summarization models proceedings workshop new frontiers summarization pages qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural ment summarization jointly learning score select sentences proceedings annual meeting association computational guistics volume long papers volume pages appendices detailed dataset introduction cnn dailymail cnn dailymail tion answering dataset hermann ied nallapati commonly summarization dataset consists online news articles paired human generated maries data preprocessing use anonymized data replace named entities xsum xsum narayan dataset consists articles single sentence swers question article summary abstractive compared cnn dailymail pubmed pubmed cohan drawn scientic papers specically medical journal articles pubmed open access subset use introduction source document abstract summary bigpatent bigpatent sharma consists million records patent uments corresponding summaries ated human according cooperative patent classication cpc dataset divided categories categories chosen dataset difference domain experiment category performing operations ing reddit tifu reddit tifu kim dataset formal posts compared datasets mentioned use formal documents source collected online discussion forum reddit regard body text source title short summary summary long summary ing sets datasets tifu short tifu long tifu long paper dataset statistics detailed dataset statistics presented tab datasets statistics topics oracle lead news news cnndm xsum pubmed bigpatent patents posts reddit scientic table detailed statistics datasets lead dicates score rst sentences document oracle indicates globally mal combination sentences terms scores ground truth represents upper bound extractive models experimental setup extractive summarizers use training setup zhong use cross entropy loss function train lstmnon transauto hidden state mension lstm lstmnon set hidden state dimension transformer transauto use transformer heads bertnon transnon constructed according liu lapata documents maries truncated tokens training bertnon transnon trained steps gradient accumulated steps use adam optimizer learning rate set bertmatch trained zhong uses base version bert base model use adam optimizer warming learning rate schedule follows vaswani abstractive summarizers ptr trained torch reproduced version code use size hidden state dimension word embedding sion paper models trained maximum training steps use adagrad train models learning rate constructed according liu lapata use separate optimizers decoder encoder set mismatch encoder decoder pre trained ing rates optimizers encoder decoder respectively hand trained gradient lation steps training step bart uses large pre trained sequence quence model lewis total ing step tuning set steps warming use adam optimizer learning rate dataset rouge results models tab displays dataset rouge scores score difference model pairs meaningful compare holistic grained results pair wise comparison displayed tab cnndm xsum pubmed bigpatent reddit models ext lstmnon transnon transauto bertnon bertmatch abs ptr bart table representative summarizers studied paper correspond performance rouge different datasets cross dataset factuality results models cross dataset factcc results abstractive els shown tab factcc results extractive models demonstrated tab code urls training code urls models training code urls listed lstmnon transauto trained code zhong code url com maszhongming effective extractive summarization use code liu lapata bertnon transnon code url com nlpyang presumm bertmatch uses code zhong code url maszhongming matchsum ptr trained code code url com atulkum pointer summarizer use code fairseq ott tune bart code url pytorch fairseq tree master examples bart evaluation code urls evaluation metrics code urls listed bheinzerling pyrouge evaluate rouge performance models pyrouge use url factcc kryscinski com salesforce factcc url metrics dataset bias com cdevalsumm master data bias metrics ptr bart abs models cnn xsum pubm patent red avg table factcc result abstractive models lstmnon transnon transauto bertnon bertmatch ext models cnn xsum pubm patent red avg table factcc result extractive models stiff stable ptr stiff stable stiff stable bart bart bart stiff stable stiff stable stiff stable stiff stable architecture abs architecture ext transnon lstmnon stiff stable transauto transnon stiff stable bertmatch bertnon stiff stable bertnon transnon stiff stable lstm lstmnon stiff stable generation way bertsum transnon stiff stable transformer bertnon stiff stable analysis aspect model type compare models holistic analysis grain analysis cnn xsum pubm patent red avg cnn xsum pubm patent red avg cnn xsum pubm patent red avg cnn xsum pubm patent red avg analysis aspect model type compare models holistic analysis grain analysis table difference scores different models pairs column table resents compared result pair models line holistic analysis displays overall stiffness stableness compared models rest table grained results rst lines origin compared result models pairs second fourth lines normalized compared result models pairs heatmap grey represents positive red represents negative white represents approximately zero
