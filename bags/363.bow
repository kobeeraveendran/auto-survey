c e d l c s c v v x r device tag generation unstructured text manish chugani device ai samsung institute bangalore india m com shubham vatsal device ai samsung institute bangalore india shubham com gopi ramena device ai samsung institute bangalore india gopi com sukumar moharana device ai samsung institute bangalore india com naresh purre device ai samsung institute bangalore india naresh com abstract overwhelming transition smart phones storing important information form unstructured text habitual users mobile devices grocery lists drafts emails important speeches users store lot data form unstructured text eg notes application devices leading cluttering data prevents users efcient navigation applications precludes perceiving relations present data applications paper proposes novel pipeline generate set tags world knowledge based keywords concepts present unstructured textual data tags summarize categorize search desired information enhancing user experience allowing holistic outlook kind information stored form unstructured text proposed system use device mobile phone efcient cnn model pruned conceptnet resource achieve goal architecture presents novel ranking algorithm extract n tags given text index terms abstractive summary keyword extraction text analysis deep tag ranking device concept extraction learning introduction data broadly categorized categories e structured data unstructured data structured data dened text consists certain patterns highly organized structured data dened outline framework machines search navigate ease example data nance account number date formats unstructured data suggests present abundance difcult process conform given set rules examples data product reviews e commerce emails structured data analysis mature industry today analysis unstructured data comprises enterprise data actual challenge lies latest trend concentrates exploiting resource unstructured text contains huge amounts unrelated diverse tion framework outline machines able identify patterns structure order locate said information far unstructured text mobile devices concerned turns users store random information form text e passwords otps blog texts lists emails drafts speeches results data manifold nature varied forms lengths text proposed system draws knowledge concepts encoded hierarchical common sense knowledge database known provide enhanced tag extraction capabilities approach uses deep learning provide abstractive extraction concepts knowledge graph embeddings extract tags keywords ensuring device efciency keeping entire pipeline ally inexpensive knowledge graph cnn use speech pos extract words nouns proper nouns fed input model apart proposed custom ranking algorithm extract n tags generated given data remaining paper organized following manner section ii talks related works work differs section iii describes overall pipeline model techniques employed section iv talks datasets evaluate performance pipeline pipeline section v provides experiments conducted section vi talks methods pipeline compared section vii shows results obtained experimentation section viii talks applications pipeline real world scenarios section ix nally concludes paper lists improvements researched future fig proposed system ii related work keyword extraction important task area text mining extracting small set keywords text document help tasks understanding document prior works best knowledge shared results user notes application prominent sources unstructured text device tionally work targets predicting results entirely device pipeline considered necessary user privacy maintained uploading personal data server previous works approached keyword extraction short text statistical approaches idf bag words features extracted text works focus methods selecting alternative input features approaches rely word frequencies keywords extracted relevant user furnkranz et al uses noun phrases matching number syntactic heuristics features aizawa extracts pos entities matching pre dened patterns representation shows small improvement results works unclear keywords extracted witten et al use key phrase extraction algorithm called kea based naive bayes algorithm algorithm learns model identifying extracted keywords training applied nding keywords new documents tang et al apply bayesian decision theory keyword extraction word linkage information context features methods limit extracting keywords present text extract keywords tags based present text interesting approach depicted sahlgren coster compute concept based sentation word co occurrence data combined text representation combination improved performance task text categorization approaches use text tion methods nd sentences containing relevant keywords fig bi lstm crf character word glove embeddings use scoring mechanism sentences higher weight feature vectors paper propose fast novel system device extraction keywords generation tags unstructured text generates tags entities concepts present text ranks order enhance user experience iii proposed system fig shows pipeline proposed system unstructured text sent input pos tagger set entities extracted depending set entities present knowledge base set similar entities obtained finally set entities passed graph cnn model extract relevant tags form keywords concepts tags extracted passed custom ranking method reorganizes set tags basis priority depth details component pipeline mentioned coming sub sections speech tagging building pos model model similar lample al ma hovy firstly bi lstm layer trained character embeddings train data gives character based representation word concatenated standard glove dimension vectors trained billion corpus wikipedia word vector representation gives contextual representation word bi lstm run sentence represented contextual representation nal output model decoded linear chain crf viterbi algorithm device inference viterbi decode algorithm implemented java run android devices nal output model quantized reduce size feasible device requirements tagged dataset shared task training neural network model uses word embeddings dimension character embeddings dimension lstm units nal pos model device accuracy test dataset input text passed pos model extracted proper nouns added nal set tags verbs lemmatized passed alongside nouns neural network inferencing concepts commonsense based knowledge graph fig shows architecture pos model b cnn based knowledge graph learning approach uses cnn based knowledge graph model explained feipeng zhao et al architecture embedding cnn based score function unknown model develops entity relation embeddings whilst learning knowledge graph structure triplets h r t h t head tail entities r relationship given h r t embedding vectors stacked kernels convolution combined matrix size embedding dimension cnns applied images rows columns image pixels case locally connected structure head relation tail cnn output passed max pooling layer subsamples max pooling lter size set stride dropout added regularization dropout probability set training nal layer network logistic regression layer positive correct triplets score negative incorrect triplets score nal negative score tanh activation regression layer loss function given formula h r h r t r t r h head entity r relation t tail entity h corrupted head entity t corrupted tail entity s set golden triplets s set corrupted triplets margin hyperparameter network r t score golden triplet t score corrupted triplet mini batch stochastic gradient descent mizer loss function require negative sampling order calculate score positive triplet embedding cnn parameters initialized random values training xed certain number epochs based size dataset architecture shown fig training data knowledge graph provided model ltered vast conceptnet dataset explained section iv knowledge graph contains triplets summarizing nature specically ltered task generating concepts unstructured text methods use standard datasets training validation sets task required creation hierarchical knowledge graph split ratio model training phase purpose knowledge graph generate tags ensure approach conned input text knowledge graph facilitates real world knowledge applied extraction process emulate human behaviour trying understand input text reason cnn based learning method pipeline designed device inference models feasible efcient c entity similarity module architecture deployed device constraints model size inference time strict results restrictions deep cnn architecture heavy model sizes prevent device deployment necessitated use glove embeddings nd similar words entities outside knowledge graph order able incorporate entities entity outside vocabulary knowledge graph encountered extract words similar entity question cosine similarity table ii section v c shows device metrics models experimented lishes need alternative approach incorporating large knowledge graphs device d concept selection module word given text passed net gives number concepts corresponding word example pass word car conceptnet concepts artifact tool vehicle item machine concepts generally irrelevant respect general context text order choose appropriate concept calculate context factor ci represents concept set extracted concepts word wj represents output pos represents term frequency word wj n represents length text context factor dened tf wj ci n j cosinesim cosine similarity word wj concept ci calulcated glove embeddings choose concept ci maximum lcontx value appropriate concept given word wi context factor helps analysing general context entire text selecting concept word example consider text typically driver responsible damage car tenure lease fig architecture knowledge graph cnn fault text maximum value word car sentence machine popular term family car rst machine actually caught imagination safely favorite machine till date maximum value extracted concept word car e ranking tags scenario considerably long tured text end extracting large number tags tags turn prove form clutter user order enhance user experience utterly important rank select handful extracted tags given text section present custom ranking algorithm later present evidence form results obtained datasets justication hypothesis algorithm based hypothesis algorithm based tag generating word found vicinity large number tag generating words input text given higher priority ranking tags tag generating word simply word tag extracted basis hypothesis calculate ranking factor rf given equation rftj ti j wj f wi f wj wj co occurrence words tags ti extracted sentence frequencies words wi wj unstructured text average number words occurring unstructured text wi wj plus rftj ti ranking factor tag ti respect tag tj tags ranked descending order rf values table table ranking matrix change vehicle contract responsibility payment change vehicle contract responsibility payment custom ranking method co occurrence value equation determined calculating number sentences words wi wj found frequency words wi wj calculated taking complete unstructured text consideration factor present adds extra weightage extracted tags factor accounts distance words generate tags ti tj knowledge graph embeddings distance measure dened number words words unstructured text tags ti tj generated hypothesis based giving highest priority tag occurs neighborhood tags factor helps achieving small example explaining working gorithm consider note typically driver responsible damage car tenure lease fault insurance apply pay damage credit card pay lease supplemental insurance damage car text passed pipeline tags extracted responsibility contract payment vehicle change visualization construct ranking matrix calculating relatedness extracted tags shown table finally considering values ranking matrix pairs wi ti ranked leasecontract carvehicle damagechange paypayment faultresponsibility wi word input text ti extracted tag iv datasets dataset training convolutional neural knowledge graph learning model conceptnet conceptnet knowledge graph contains r t languages huge variety concepts device constraints entire conceptnet dataset vast inferred result created pruned conceptnet dataset set rules order nally arrive ltered conceptnet rst lter added select smaller set data select triplets english language selection technique select relationships r h r t head entity superset parent tail entity order ensure tags extracted unstructured textual data summarizing nature added constraint relations extract triplets isa derivedfrom instanceof partof relations knowledge graph slightly summarizing nature ambiguous dropped conceptnet knowledge graph incorporated dbpedia relations ltered relevant respect work narrows dataset thousand triplets vast inferred device perspective model mb quantization compression decided manually select smaller dataset commonly relevant concepts knowledge graph results dataset k triplets reduced model size mb quantization compression apart dataset training graph cnn open source datasets amazon enron benchmarking proposed system dataset user notes application evaluate feasibility proposed pipeline amazon review dataset consists short long texts user reviews shopping categories enron email dataset contains emails generated employees enron corporation notes application dataset consists notes variable lengths ranging short lists lengthy email drafts v experiments evaluation metric quality tags tags extracted text contain concepts exact words present text use gold standard datasets compare method comparison method involves annotators judging appropriate tags given piece text ends incorporating bias authors methodology clearly wanted avoid inspired bellaachia et al introduce new way compare quality tags generated methods use volume google search query idea popularity tag extracted rationale approach keyword frequently masses signicance representing piece text average method generated tags test sample datasets mentioned section randomly selected tags extracted method sorted according popularity compared search volumes random tags extracted tags generated given methods comparison purposes sure comparing proper nouns entity names use word volume extracted keywords ahead comparisons let tcorrect number keywords given method popularity keywords extracted methods textracted total number keywords extracted case sample text precision dened p recision tcorrect textracted comparison results pipeline respect methods discussed section shown table iv b evaluation metric quality tags use volume google search query extracted tags measure rank keyword tag widely searched internet s word occurrence factor ranking algorithms based high signicance given piece text discussed bellaachia et al use binary preference calculating rank extracted keywords binary preference measure bpm calculated bp m tt ranked higher t set correct tags set m tags extracted method t correct tag n incorrect tag c model parameters graph cnn model uses adam optimize learn parameters model set width convolutional kernels different size simplicity xed kernel size pairwise ranking loss learn cnn xed margin value learning rate model xed epoch number set conceptnet dataset k triplets use negative sampling method explained section iii b batch size github io amazon index kaggle com wcukierski enron email dataset wordtracker table ii graph cnn model metrics entities knowledge graph triplets model size parameters mb mb mb number nodes final layer number entities knowledge graph size fully connected layer half size pooling layer convolutional layers table iii entity similarity module impact metrics dataset vocabulary entities test sample amazon reviews enron emails notes average length sample words triplets mini batch stochastic gradient descent set embedding dimension set dissimilarity distance measure norm evaluation triplet size set number lters convolution set dropout probability set device metrics different graph cnn models experimenting terms number triplets listed table ii model size vocabulary length essential metrics need taken consideration ing model mobile devices clearly table ii size graph cnn model trained entities mb feasible device perspectives reason went ahead lightweight model entity similarity module mentioned section iii c developing set similar entities order deal entities outside knowledge graph optimally chose similarity score threshold based trial error table iii showcases effectiveness entity similarity module shows average number entities detected outside knowledge graph chosen datasets vi methods comparison following methods comparison proposed system topic modelling latent dirichlet allocation latent dirichlet generative tistical model natural language processing topic modelling explains topics unobserved clusters words explain reasons parts data similar unsupervised learning model clusters similar groups observations posits document mixture small number topics concepts observation s presence attributable topics specic document comparisons set number topics extract relevant keywords representing topic b automatic summarization text rank algorithm automatic summarization process computational reduction shortening data order create synopsis containing highly relevant important information whilst abstracting unnecessary aspects larger data example nding informative sentences news article representative images collection images important frames video fall umbrella automatic summarization text unsupervised approach tomatic summarization text graph based ranking algorithm natural language processing use default parameters candidate parts speech case input text window size c rapid automatic keyword language processing rake popular keyword extraction technique natural involves lists stopwords phrase delimiters extract relevant keywords textual data python implementation rake rake nltk library default parameters comparison experiments common methods tf idf term frequency inverse document frequency bag word models compared length input texts relatively shorter generating appropriate idf score vocabulary comparison require substantial relevant text taking account average length input texts specic case choose compare methods vii results tags extracted model set test samples different datasets evaluation metrics mentioned section iv calculate results precision bpm conducted experiments shown table iv device inference times model sizes shown table v device metrics calculated samsung s galaxy gb ram ghz octa core processor results clearly improvement precision bpm serving data quantitative tive outcomes proposed approach apart results proposed system demonstrates efciency respect device based computational tions entire pipeline s size restricted mb inference time low ms important thing note overall pipeline s size ence timing sum components mentioned table v presence additional resources like glove embeddings multiple components fig application content presentation table iv results datasets enron email precision methods lda tr rake proposed system amazon reviews notes bpm precision bpm precision bpm table v device inference times model sizes component size mb pos graph cnn proposed system inference time sample ms viii applications arbitrary search device note taking applications web list applications providing strong evidence utility signicance notes application modern world smartphones lists email drafts key conversations blog snapshots stored note notes form text expected kind structure soever bound enormous variations depending multiple factors associated user unstructured text notes punctuation marks correct sentence formation correct grammar recently developments eld notes applications ranging automatic detection list type notes notes containing images new features actually address problem cluttering data section ways cluttering data notes application handled proposed work fig shows device screenshots signicantly enhance user experience navigating notes application initially step user uses querying keywords search desired note step notes indexed search user displayed set tags extracted pipeline summarized manner notes indexed step selected match querying keyword step tags displayed note step finally step user select desired note indexed notes step ways content presentation pipeline running background better ways render content ix conclusion future work unstructured text special type text having dened format pattern generating relevant tags form concepts keywords unstructured text involve use contextual semantics usually associated entire text proposed pipeline uses word level dynamics extract concepts keywords tured textual data disorganized nature unstructured data extracted tags prove helpful navigating text popular application targeted device usage proposed pipeline notes application recent developments device based note taking applications proposed pipeline device feasibility play vital role enhancing user experience seen section viii areas signicantly improve analysing multiple input data formats images texts audio time multiple input data formats better context extracting subtle set tags analysing multiple input formats require techniques ocr speech recognition depending input provided user x acknowledgement authors like thank users tributed notes application data collection authors like express gratitude reviewers given constructive feedback improve paper references hulth b b megyesi study automatically extracted keywords text categorization proceedings international conference computational linguistics annual meeting association computational linguistics association computational linguistics pp j furnkranz t mitchell e riloff et al case study linguistic phrases text categorization www working notes aaai icml workshop learning text categorization pp n aizawa linguistic techniques improve performance automatic text categorization nlprs vol citeseer pp h witten g w paynter e frank c gutwin c g manning kea practical automated keyphrase extraction design usability digital libraries case studies asia pacic igi global pp j tang j li k wang y cai loss minimization based keyword distillation asia pacic web conference springer pp m sahlgren r coster bag concepts improve performance support vector machines text categorization y ko j park j seo improving text categorization portance sentences information processing management vol pp g lample m ballesteros s subramanian k kawakami c dyer neural architectures named entity recognition arxiv preprint x ma e hovy end end sequence labeling bi directional lstm cnns crf arxiv preprint f zhao m r min c shen chakraborty convolutional neural knowledge graph learning arxiv preprint bellaachia m al dhelaan ne rank novel graph based keyphrase extraction twitter ieee wic acm international conferences web intelligence intelligent agent technology vol ieee pp d p kingma j ba adam method stochastic optimization arxiv preprint d m blei y ng m jordan latent dirichlet allocation journal machine learning research vol jan pp r mihalcea p tarau textrank bringing order text proceedings conference empirical methods natural language processing pp s rose d engel n cramer w cowley automatic keyword extraction individual documents text mining applications theory vol pp
