ted pretrained unsupervised summarization model theme modeling denoising ziyi chenguang robert michael xuedong eric stanford edu microsoft cognitive services research chezhu rogmyr nzeng com abstract text summarization aims extract essential information piece text form text concise version existing unsupervised abstractive summarization els leverage recurrent neural networks work recently proposed transformer exhibits capability previous summarization models abundant unlabeled corpora resources available pretraining order address issues propose ted based unsupervised abstractive tion system pretraining large scale data rst leverage lead bias news articles pretrain model millions labeled corpora netune ted target domains theme modeling denoising autoencoder enhance ity generated summaries notably ted outperforms unsupervised abstractive lines nyt cnn english word datasets document styles analysis shows summaries erated ted highly abstractive component objective function ted highly effective introduction summarization refers task condensing document shorter version losing key information summarization models categorized types abstractive tive extractive models select sentences input article summary process ensures basic level grammaticality accuracy limits model ability copying trast abstractive models summarize document newly generated tokens phrases found original article involves equal contribution work rst author internship microsoft process requiring advanced ability rene paraphrase organize language information narayan gunel like machine learning algorithms marization models divided vised unsupervised categories supervised proaches require domain parallel data input articles corresponding reference maries present teacher forcing ing hermann liu lapata unfortunately high quality paired data ways available different text domains styles considering fact rization easy task people able human labeled data difcult obtain unsupervised summarization proaches proposed require reference summaries target domain introduce methods follows unsupervised extractive models textrank mihalcea tarau encodes sentences article nodes undirected graph weights edges measured sentences larity centrality node sentence puted pagerank brin page decide sentence included nal summary zheng lapata advances textrank encoding sentences bert resentation devlin compute pairs similarity build graphs directed edges cided relative positions sentences unsupervised abstractive models baziotis leverages differentiable sampling optimizes constructing input article generated summary chu liu poses similar idea multi document marization setting wang lee uses adversarial training reinforcement learning summary human readable fevry phang adopts denoising autoencoders inally sentence compression models tested datasets considerably small article summary length previous models usually utilize recurrent neural networks rnns transformers vaswani devlin shown superior performances rnns ous nlp tasks including machine translation ing comprehension sentiment analysis efforts leverage transformers unsupervised abstractive summarizations pretraining language model recent years pretraining language models proved powerful solving numerous nlp tasks state art pretrained models include cove mccann elmo peters gpt radford bert devlin unilm dong taking advantage corpora billions tokens pretrained language models learn universal bust representations semantic structures linguistic relationships result pretrained models widely considerable success applications question answering zhu sentiment analysis peters passage reranking nogueira cho furthermore unilm dong leverages sequence sequence capability abstractive summarization bert model employed encoder bertsum liu lapata supervised extractive abstractive summarization paper present ted pretrained pervised abstractive summarization model netuned theme modeling denoising domain data ted utilizes transformer based encoder decoder structure pretraining ages large scale corpora containing millions labeled articles primary contributions fold follows leverage lead bias news articles pretrain ted lead bias introduced journalistic convention writing inverted pyramid structure placing important mation beginning article propose use leading sentences target summary train model predict pretraining way pretrain summarization model large scale corpus news articles model yields better performance existing unsupervised methods second netune specic datasets ted trained theme modeling loss denoising autoencoder role theme modeling module generated mary semantically close article module uses semantic classier trained native objective function furthermore optimize generated summary tokens adopt gumbel softmax jang estimator replace non differentiable arg max noising autoencoder previously unsupervised machine translation lample sentence compression fevry phang employ help model extract salient information corrupted text instead classical word tokenization adopt sentencepiece tokenization kudo son alleviates long standing vocabulary oov problem language generation tasks luong sennrich test ted benchmark datasets experimental results ted outperforms unsupervised abstractive baselines datasets example cnn dataset forms state art unsupervised abstractive model points pares favorably unsupervised extractive models ted capable generating novel words phrases summaries highly abstractive system compared supervised systems methodology section model ture ted transformer encoder coder introduce pretraining method domain netuning objectives theme modelling denoising autoencoder overall architecture ted illustrated fig transformer encoder decoder previous unsupervised summarization methods based sequence sequence model sutskever primarily uses rnn model transformer structure vaswani successfully applied large ber nlp tasks ted employs multi layer transformer encoder decoder architecture low standard transformer design ted works refer readers vaswani figure overall structure model ted rst pretrains news articles netunes theme modeling denoising left right technical details transformers denote number layers transformer blocks number self attention heads hidden size explore different urations experiments layers heads layers heads denote input article tokens sequence token rst ferred vector trainable embeddings trix output transformer encoder sequence encoded vectors decoder viewed conditional language model generate mary depending generator outputs given input summary tokens cross attention layer decoder attends encoder outputs decoder outputs probability distribution vocabulary given traditional tokenization algorithms efforts address vocabulary oov issue yang cost ing semantic information mapping oov words special unk token mitigate open vocabulary problem adopt piece kudo richardson data driven method trains tokenization models tences large scale corpora advantage sentencepiece model subwords cover possible word forms subword vocabulary size controllable evaluation experiments train sentencepiece subword vocabulary size note supervised summarization models ing training inputs decoder groundtruths reference summary tokens supervised learning input tokens generated previous pass new token ated pass details available section pretraining unlabeled corpora leveraging large scale unlabeled text corpora pretrain models proven effective method multiple nlp tasks devlin approach utilized text summarization news articles follow inverted pyramid ture loading salient information called lead bias news summarization strong shown rst sentences news article summary score higher sophisticated deep learning models poses great challenge previous research advantage property favor pretraining phase ted news article set target summary rst sentences allows model exploit structural bias news domain infer important information background materials remainder cle collect data pretraining obtain years online news articles industrial search engine search engine indexes major online news domain instance new york times bloomberg lect parsed articles time range raw data note time span overlap test datasets use paper pretraining lead data leakage test worth noting idea utilizing structural bias scale summarization pretraining limited pretrained transformerencoders decoderstheme lossarticlegeneratedsummarydenoised sent noisy sent cross entropylosssent add noisetransformerencoders decoderspretrain likely elaborate beginning articles ratio overlapping words higher pick threshold based observations cnn dataset median overlapping ratio non stopping words golden summary article median ratio tences rest article setting threshold makes nal training set size available computation resources ensures leading sentences contain information finally end articles articles randomly sampled validation set conduct pretraining epochs pick model best rouge score validation set pretraining task predict rst sentences article rest article pretraining teach model simply copy leading sentences removed input transformers note ted start pretrained models like bert pretraining order adapt ted specic target dataset evaluation netune ted target dataset unsupervised ner netuning objective functions includes following theme modeling denoising coder theme modeling theme modeling aims generated mary semantically close input article employ differential sampling enable tion generated summaries train classier improve semantic relatedness output summary article differentiable sampling order optimize transformers summaries need generation summary tokens differentiable recall ditional probability distribution token let note use arg max obtain token forward pass differentiable dient propagation arg max avoided obtaining embedding weighted sum vocabulary embeddings results undesirable gap training weighted sum inference discrete figure example pretraining task predict sentences target summary rest article specic types models applied types text academic papers abstracts novels editor notes books tables contents carefully examine clean source data advantage lead bias sentences form good summary conduct strict data cleaning remove irrelevant distracting content lter articles sentences form good summary news articles begin media names reporter names dates irrelevant information summarization new york cnn adam smith june tomatically clean regular expressions second include articles sentences contain words remaining sentences contain words criterion sentences set lter articles extremely short leading sentences phrases words contain little information reasonable summaries exceedingly long ing sentences reduce pretraining time limit total number words article lter long articles reduce memory sumption purpose remove short articles information condensed suitable summarization pretraining remove articles rst sentences contain major mation article use simple easy compute metric overlapping words compute portion non stopping words sentences appear rest article higher ratio indicates rest article antoniamarshall confirmedthatfergusonhaslaunchedherownlifestylebrand itisstillunclearwhatproductswillbeavailablefromsarahsenses butbasedonmarshallsphoto ithasbeenconfirmedthattherearescentdiffusersandtea itisalsolikelyforpartoftheproceedsfromsarahsensestobedonatedtostreetchilduk acharitythatfergusonsupports antoniamarshall confirmedthatfergusonhaslaunchedherownlifestylebrand itisstillunclearwhatproductswillbeavailablefromsarahsenses butbasedonmarshallsphoto ithasbeenconfirmedthattherearescentdiffusersandtea itisalsolikelyforpartoftheproceedsfromsarahsensestobedonatedtostreetchilduk acharitythatfergusonsupports pretrain transformerencoders decoders sampling forward pass generation solve issue employ straight softmax estimator jang yang baziotis specically forward pass training uses arg max pling gradient computation following gumbel softmax distribution tiable approximation arg max operation samples drawn gumbel distribution denotes softmax temperature shown jang gumbel softmax tion converges categorical hot bution inf gumbel softmax bution converges uniform distribution gradient estimator biased method works practice choose based cnn validation set use value experiments denote input article generated summary generation lows recursive process input transformer decoder obtain input compute rst token special beginning token start encoder transformer semantic figure theme modeling essentially updating ted semantic classier input sentence pair rst processed adding class token ning separation token tences sentence pair fed encoder rst output vector classied similar distinct fig packed sequence fed input ted transformer encoder output vector associated token cls classied similar distinct categories layer fully connected network use following entropy loss optimize encoder semantically similar closed semantically distinct ltheme classier denoising autoencoder generated summary article theme beginning netuning mize ted generated summaries semantically closed input articles frame semantic similarity problem tive setting better adapt target domain data add sentence pairs training articles facilitate similarity computation concretely training pick utive sequences tokens article form positive sequence pair second sequence chosen random cle dataset form negative sequence pair following devlin sequence pair packed single sequence inserting special token sep adding trainable segment embeddings cial classication token cls added beginning packed sequence shown idea denoising autoencoder vincent unsupervised machine lation artetxe lample prevent model learning merely copy input word denoising cess imitates text simplication helps rene essential semantic information detail sequence consecutive tokens input article injected types noise insert noisy tokens sampled articles dataset original sequence random positions obtaining new quence length larger similar lample sequence slightly shufed applying tation permutation distance set length nal corrupted sequence denoted ted model trained self attentionclassifysimilar normalizeadd normalizefeed recover original token sequence given rupted sequence ldenoise denotes mean token level entropy loss denotes sequence probability distribution outputs coder inputting encoder nal objective function mean empirically equal weights terms work practice lted ltheme ldenoise worth pointing conduct pretraining target evaluation datasets target dataset know forehand lead sentences quality summary option datasets lead good maries potentially cherry picking datasets conduct supervised tuning ground truths summaries evaluation datasets want entirely pervised summarization system motivations stated introduction section experiments datasets evaluate model benchmark marization datasets nyt cnn english gigaword containing news articles respectively detailed statistic tion datasets found appendix nyt following liu lapata choose examples validation set lter examples summaries fewer words cnn similar liu lapata input articles truncated tokens english gigaword lter data examples articles containing onlyunk tokens baseline metrics compare ted following baselines unsupervised abstractive systems brief wang lee baziotis radford supervised tuning ground truths summaries pervised extractive systems textrank mihalcea tarau lead supervised tive abstractive models trained truths summaries pacsum zheng ata pgnet refresh narayan sumo liu ted unsupervised abstractive directly comparable supervised baselines purpose supervised systems references describe implementation details model appendix measure quality generated summaries rouge score lin including unigram bigram longest common quence rouge results results english gigaword dataset shown table ted outperforms unsupervised lines table shows experimental results nyt cnn datasets nyt vised tuning ted improves trained model rouge respectively note rouge metric prefers extractive systems serve original phrasing ing factor ted achieves results petitive unsupervised extractive baselines surpasses unsupervised abstractive models cnn ted larger model size outperforms unsupervised abstractive methods compares favorably unsupervised tive baselines note ted outperforms powerful transformer based language generation model pretrained large scale webpage textual data signicant margins ted improves pretrained models congurations table results english gigaword dataset formances baseline models collected original papers best performance metric bold model ted pretrained ted pretrained brief table rouge scores cnn nyt datasets stands respectively best results unsupervised category bold results baseline models obtained original papers running open sourced codes model cnn nyt ted pretrained ted pretrained brief textrank idf textrank skip thought textrank bert pacsum idf pacsum skip thought pacsum bert unsupervised abstractive unsupervised extractive sumo pgnet refresh supervised abstractive extractive article exposing potential security risks airlines entertainment systems experts counter threat intelligence world pulled ight fbi agents chris roberts featured string fox news reports yanked plane landed syracuse new york wednesday night fbi agents uniformed ofcers roberts works security intelligence company world labs questioned hours ted summary chris roberts works security intelligence company world labs pulled plane syracuse new york wednesday night fbi agents uniformed ofcers incident occurred hours report roberts research released government accountability ofce earlier week reference chris roberts world labs grabbed plane landed syracuse fbi agents spent hours questioning cyberhacking agents conscated electronic devices computer les roberts talk aerospace conference plane vulnerabilities roberts featured fox news record greta van susteren regarded world experts counter threat intelligence figure example generated summary ted reference summary parts input article included discussion ablation study ablation studies shown table verify effectiveness component ted ing transformer encoder decoder scratch yields reasonable performance pretraining large scale data results ment metrics training ted scratch pretraining plus theme modeling denoising improves pretrained model ted model pretraining theme modeling denoising produces best result overall table ablation study different components ted nyt dataset test model conguration model train scratch pretrained pretrained theme modeling pretrained denoise loss model hours report roberts research released shows fact cross checking potential future research direction abstractiveness examine abstractive ted compute proportion novel grams summary output fig erence summary output pgnet included comparison ted pervised includes novel grams pervised model pgnet reference summaries highest proportion grams comparison previous unsupervised models ted innovative unsupervised summarization model distinctive features setting apart previous approaches meansum ted leverages structure news articles effective large scale pretraining second meansum loss summary similar input article leverage classical cosine similarity text embeddings contrast ted innovatively encodes similarity transformer encoder modeling capability denoising module ted completely distinct idea reconstruction sum ted denoising module corrupted texts input transformer model trained lter added noises original clean document input unseen ted forward pass tion process meansum employs original document generate summary reconstruct original document figure proportion novel grams summaries erated different models cnn test set conclusion model analysis example showcase sample summary cnn dataset input article reference summary fig shown ted able capture organize essential mation uent highly readable language attribute grammatical correctness pretraining process denoising autoencoder note ted ages recognize temporal information related reported event hours fox news ports makes mistake summarizing paper propose ted unsupervised abstractive summarization model duce effective large scale pretraining approach leveraging lead bias news articles training employs automatic ltering mechanism require human labeled data develop netuning scheme induce tic similarity summaries input articles denoising autoencoder improve quality generated summaries experiments datasets ted signicantly outperforms unsupervised abstractive baselines novel gramspgnettedreference references mikel artetxe gorka labaka eneko agirre kyunghyun cho unsupervised neural chine translation arxiv preprint christos baziotis ion androutsopoulos ioannis stas alexandros potamianos seq differentiable sequence sequence sequence autoencoder unsupervised abstractive sentence compression arxiv preprint sergey brin lawrence page anatomy large scale hypertextual web search engine computer networks isdn systems pages elsevier science publishers eric chu peter liu meansum neural model unsupervised multi document abstractive summarization arxiv preprint jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language ing arxiv preprint dong nan yang wenhui wang furu wei xiaodong liu wang jianfeng gao ming zhou hsiao wuen hon unied language model pre training natural language arxiv preprint understanding generation thibault fevry jason phang vised sentence compression denoising encoders arxiv prints page beliz gunel chenguang zhu michael zeng dong huang mind facts boosted coherent abstractive text summarization arxiv preprint karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend advances neural information processing systems pages eric jang shixiang ben poole ical reparameterization gumbel softmax arxiv preprint taku kudo john richardson sentencepiece simple language independent subword enizer detokenizer neural text processing arxiv preprint guillaume lample alexis conneau ludovic denoyer marcaurelio ranzato unsupervised chine translation monolingual corpora arxiv preprint liyuan liu haoming jiang pengcheng weizhu chen xiaodong liu jianfeng gao jiawei han variance adaptive learning rate arxiv preprint yang liu mirella lapata text tion pretrained encoders arxiv prints page yang liu ivan titov mirella lapata gle document summarization tree induction proceedings conference north american chapter association tional linguistics human language technologies volume long short papers pages minh thang luong ilya sutskever quoc oriol vinyals wojciech zaremba addressing rare word problem neural machine translation arxiv preprint bryan mccann james bradbury caiming xiong richard socher learned translation textualized word vectors advances neural formation processing systems pages rada mihalcea paul tarau textrank ing order text proceedings ference empirical methods natural language processing pages shashi narayan shay cohen mirella lapata ranking sentences extractive arxiv preprint tion reinforcement learning rodrigo nogueira kyunghyun cho arxiv preprint sage ranking bert matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word sentations arxiv preprint alec radford karthik narasimhan tim salimans improving language ilya sutskever standing generative pre training alec radford jeffrey rewon child david luan dario amodei ilya sutskever language models unsupervised multitask learners openai blog abigail peter liu christopher manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers pages chin yew lin rouge package automatic text summarization evaluation summaries branches pages rico sennrich barry haddow alexandra birch neural machine translation rare words subword units arxiv preprint ilya sutskever oriol vinyals quoc sequence sequence learning neural networks advances neural information processing tems pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need guyon luxburg bengio wallach fergus vishwanathan nett editors advances neural information cessing systems pages curran ciates inc pascal vincent hugo larochelle yoshua bengio pierre antoine manzagol extracting composing robust features denoising proceedings international coders conference machine learning pages acm yau shian wang hung lee learning encode text human readable summaries ing generative adversarial networks arxiv preprint zichao yang zhiting chris dyer eric xing taylor berg kirkpatrick unsupervised text style transfer language models tors advances neural information processing systems pages ziyi yang chenguang zhu vin sachidananda embedding imputation arxiv preprint eric darve grounded language information hao zheng mirella lapata sentence ity revisited unsupervised summarization arxiv preprint chenguang zhu michael zeng xuedong huang sdnet contextualized attention based deep network conversational question answering arxiv preprint implementation details pretraining use dropout rate inputs transformer layers use radam liu optimizer learning rate different numerical scales positional embedding initialized sentence piece embeddings divide tional embedding feeding transformer pretrain model epochs epoch model evaluated idation data pick check points highest rouge unsupervised netuning specic datasets learning rate set dropout ratio stays pretraining batch size vocabulary embeddings updated training process test phase generate summarization trained encoder decoder beam search rouge version use evaluation consistent benchmark models version rouge available open sourced codes original papers test time limit longest length erated summaries set based validation dataset instance maximum generation length cnn dataset datasets information better understanding evaluation tocols statistical information evaluation datasets summarized table table average document summary length number words sentences nyt cnn english gigaword datasets test set dataset docs avg document words avg summ sen sen words cnn nyt gigaword
