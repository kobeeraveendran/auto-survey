ted a pretrained unsupervised summarization model with theme modeling and denoising ziyi chenguang robert michael xuedong eric stanford edu microsoft cognitive services research chezhu rogmyr nzeng com t c o l c s c v v i x r a abstract text summarization aims to extract essential information from a piece of text and form the text into a concise version existing unsupervised abstractive summarization els leverage recurrent neural networks work while the recently proposed transformer exhibits much more capability moreover most of previous summarization models abundant unlabeled corpora resources available for pretraining in order to address these issues we propose ted a based unsupervised abstractive tion system with pretraining on large scale data we rst leverage the lead bias in news articles to pretrain the model on millions of labeled corpora next we netune ted on target domains through theme modeling and a denoising autoencoder to enhance the ity of generated summaries notably ted outperforms all unsupervised abstractive lines on nyt cnn dm and english word datasets with various document styles further analysis shows that the summaries erated by ted are highly abstractive and each component in the objective function of ted is highly effective introduction summarization refers to the task of condensing a document into a shorter version without losing the key information summarization models can be categorized into two types abstractive and tive extractive models select sentences from the input article as the summary such process ensures a basic level of grammaticality and accuracy but also limits the model ability to copying in trast abstractive models summarize a document using newly generated tokens and phrases that may not be found in the original article which involves equal contribution work was done during rst author internship at microsoft a process requiring an advanced ability to rene paraphrase and re organize language information see et al narayan et al gunel et al like most machine learning algorithms marization models can also be divided into vised and unsupervised categories supervised proaches require in domain parallel data i e both input articles and corresponding reference maries must be present for the teacher forcing ing hermann et al liu and lapata unfortunately high quality paired data are not ways available across different text domains and styles moreover considering the fact that rization is not an easy task even for people able human labeled data are also difcult to obtain therefore several unsupervised summarization proaches have been proposed which do not require reference summaries for the target domain we introduce these methods as follows unsupervised extractive models textrank mihalcea and tarau encodes sentences in the article as nodes in an undirected graph the weights of edges are measured by sentences larity the centrality of a node sentence is puted by pagerank brin and page to decide whether a sentence should be included in the nal summary zheng and lapata advances upon textrank by encoding sentences with bert resentation devlin et al to compute pairs similarity and build graphs with directed edges cided by the relative positions of sentences unsupervised abstractive models baziotis et al leverages differentiable sampling and optimizes by re constructing the input article from the generated summary chu and liu poses a similar idea in the multi document marization setting wang and lee uses adversarial training and reinforcement learning to make the summary human readable fevry and phang adopts denoising autoencoders inally used in sentence compression however most of these models are only tested on datasets with considerably small article summary length also previous models usually utilize the recurrent neural networks rnns however transformers vaswani et al devlin et al have shown superior performances over rnns on ous nlp tasks including machine translation ing comprehension sentiment analysis few efforts have been made to leverage transformers in unsupervised abstractive summarizations pretraining language model in recent years pretraining language models have proved to be quite powerful in solving numerous nlp tasks the state of the art pretrained models include cove mccann et al elmo peters et al gpt radford et al bert devlin et al and unilm dong et al taking advantage of corpora with billions of tokens the pretrained language models learn universal and bust representations for various semantic structures and linguistic relationships as a result pretrained models have been widely used with considerable success in applications such as question answering zhu et al sentiment analysis peters et al and passage reranking nogueira and cho furthermore unilm dong et al leverages its sequence to sequence capability for abstractive summarization the bert model has been employed as an encoder in bertsum liu and lapata for supervised extractive and abstractive summarization in this paper we present ted a pretrained pervised abstractive summarization model which is netuned with theme modeling and denoising on in domain data ted utilizes a transformer based encoder decoder structure and the pretraining ages large scale corpora containing millions of labeled articles our primary contributions are fold as follows first we leverage the lead bias in news articles to pretrain ted the lead bias is introduced by the journalistic convention of writing using an inverted pyramid structure placing the most important mation in the beginning of an article we propose to use the leading sentences as the target summary and train the model to predict it during pretraining in this way we pretrain a summarization model on a large scale corpus with m news articles the model yields better performance than most existing unsupervised methods second to netune on specic datasets ted is further trained with a theme modeling loss and a denoising autoencoder the role of the theme modeling module is to make the generated mary semantically close to the article the module uses a semantic classier trained using a native objective function furthermore to optimize on the generated summary tokens we adopt the gumbel softmax jang et al estimator to replace the non differentiable arg max the noising autoencoder has been previously used in unsupervised machine translation lample et al and sentence compression fevry and phang and we employ it to help the model extract salient information from corrupted text instead of classical word tokenization we adopt the sentencepiece tokenization kudo and son to alleviates the long standing out vocabulary oov problem in language generation tasks luong et al sennrich et al we test ted on several benchmark datasets the experimental results show that ted outperforms all unsupervised abstractive baselines on all datasets for example on the cnn dm dataset it forms the state of the art unsupervised abstractive model by more than points and pares favorably with most unsupervised extractive models we further show that ted is capable of generating novel words and phrases in summaries and is a highly abstractive system even compared with supervised systems methodology in this section we will go through the model ture of ted i e the transformer encoder and coder then we introduce the pretraining method and two in domain netuning objectives theme modelling and the denoising autoencoder the overall architecture of ted is illustrated in fig transformer encoder and decoder previous unsupervised summarization methods are based on the sequence to sequence model sutskever et al that primarily uses the rnn model as the transformer structure vaswani et al has been successfully applied in a large ber of nlp tasks ted employs the multi layer transformer encoder decoder architecture we low the standard transformer design in ted works and refer readers to vaswani et al figure overall structure of our model ted rst pretrains on news articles and then netunes with theme modeling and denoising from left to right for more technical details on transformers denote the number of layers i e transformer blocks as l the number of self attention heads as h and the hidden size as n we explore two different urations in experiments layers heads with n and layers heads with n ue ue denote the input article tokens sequence as x xn and each token is rst ferred to a vector by a trainable embeddings trix v the output from transformer encoder e is a sequence of encoded vectors ue n the decoder can be viewed as a conditional language model to generate the mary depending on the generator outputs given k input summary tokens w wk the cross attention layer in the decoder d attends with encoder outputs ue the decoder outputs are ud wk ud k the probability distribution over the vocabulary for is given by ud i n p k ud in traditional tokenization algorithms efforts have been made to address the out of vocabulary oov issue yang et al at the cost of ing semantic information such as mapping oov words to a special unk token to mitigate the open vocabulary problem we adopt piece kudo and richardson a data driven method that trains tokenization models from tences in large scale corpora the advantage of the sentencepiece model is that its subwords can cover all possible word forms and the subword vocabulary size is controllable in the evaluation experiments we train a sentencepiece subword vocabulary of size note for supervised summarization models ing training the inputs to the decoder are the groundtruths reference summary tokens for supervised learning input tokens are generated in the previous pass i e one new token is ated in one pass more details are available in section pretraining with unlabeled corpora leveraging large scale unlabeled text corpora to pretrain models has been proven as an effective method in multiple nlp tasks devlin et al however such approach has not yet been utilized in text summarization news articles follow an inverted pyramid ture i e front loading the most salient information this so called lead bias for news summarization is so strong that see et al have shown that using the rst sentences in a news article as a summary can score higher than many sophisticated deep learning models although this poses a great challenge to previous research we take advantage of this property in our favor in the pretraining phase of ted for a news article we set the target summary to be the rst three sentences this allows the model to exploit the structural bias of the news domain and infer the most important information using the background materials in the remainder of the cle to collect data for pretraining we obtain three years of online news articles from to via an industrial search engine the search engine indexes major online news domain for instance new york times and bloomberg then we lect the parsed articles within the time range as the raw data note that this time span does not overlap any of three test datasets we use in this paper therefore the pretraining should not lead to data leakage in test it is also worth noting that this idea of utilizing structural bias for scale summarization pretraining is not limited to pretrained transformerencoders decoderstheme lossarticlegeneratedsummarydenoised sent noisy sent cross entropylosssent add noisetransformerencoders decoderspretrain likely to elaborate on the beginning part we keep those articles with the ratio of overlapping words higher than we pick this threshold based on observations in the cnn dm dataset where the median overlapping ratio of non stopping words between golden summary and the article is and the median ratio between the top three tences and the rest of the article is setting the threshold at makes the nal training set size t with the available computation resources and ensures that the leading sentences contain enough information finally we end up with m articles out of which articles are randomly sampled as the validation set we conduct pretraining for epochs and pick the model with the best rouge l score on the validation set the pretraining task is to predict to the rst three sentences of an article using the rest of the article so pretraining will not teach the model to simply copy the leading three sentences since they are removed from the input to the transformers note that ted does not start off from other pretrained models like bert after pretraining in order to adapt ted to a specic target dataset for evaluation we netune ted on the target dataset in an unsupervised ner the netuning objective functions includes the following theme modeling and denoising coder theme modeling theme modeling aims to make the generated mary semantically close to the input article we employ differential sampling to enable tion on generated summaries and train a classier to improve the semantic relatedness between the output summary and article differentiable sampling in order to optimize the transformers using put summaries we need to make the generation of summary tokens differentiable recall the ditional probability distribution of token is p k ud let note p k one can use arg max on to obtain the token in the forward pass however it is not differentiable in the dient back propagation although arg max can be avoided by obtaining the embedding of as a weighted sum of the vocabulary embeddings v this results in an undesirable gap between the training weighted sum and the inference discrete figure an example of the pretraining task predict the sentences as the target summary using the rest of the article specic types of models and it can be applied to other types of text as well academic papers with abstracts novels with editor s notes books with tables of contents however one should carefully examine and clean the source data to take advantage of lead bias as the top three sentences may not always form a good summary therefore we conduct strict data cleaning to remove irrelevant distracting content and lter out articles whose top three sentences do not form a good summary first many news articles begin with media names reporter names dates or other irrelevant information for summarization e new york cnn adam smith june we tomatically clean these using regular expressions second we only include articles whose top three sentences contain between and words and remaining sentences contain between and words the criterion on top three sentences is set to lter out articles with either extremely short leading sentences e phrases of one or two words which contain too little information to be reasonable summaries or exceedingly long ing sentences to reduce the pretraining time the limit on total number of words in the article is to lter out very long articles to reduce memory sumption another purpose is to remove very short articles of which the information is too condensed and not suitable for summarization pretraining third we also remove articles in which the rst three sentences may not contain the major mation in the article we use a simple and easy compute metric overlapping words we compute the portion of non stopping words in the top three sentences that also appear in the rest of an article a higher ratio indicates that the rest of the article is antoniamarshall confirmedthatfergusonhaslaunchedherownlifestylebrand itisstillunclearwhatproductswillbeavailablefromsarahsenses butbasedonmarshallsphoto ithasbeenconfirmedthattherearescentdiffusersandtea itisalsolikelyforpartoftheproceedsfromsarahsensestobedonatedtostreetchilduk acharitythatfergusonsupports antoniamarshall confirmedthatfergusonhaslaunchedherownlifestylebrand itisstillunclearwhatproductswillbeavailablefromsarahsenses butbasedonmarshallsphoto ithasbeenconfirmedthattherearescentdiffusersandtea itisalsolikelyforpartoftheproceedsfromsarahsensestobedonatedtostreetchilduk acharitythatfergusonsupports pretrain transformerencoders decoders sampling on the forward pass generation to solve this issue we employ the straight through softmax estimator jang et al as in yang et al baziotis et al specically the forward pass in training still uses arg max pling but for gradient computation the following gumbel softmax distribution is used as a tiable approximation for the arg max operation i where gk are i i samples drawn from the gumbel distribution and denotes the softmax temperature as shown in jang et al as the gumbel softmax tion converges to the categorical one hot bution as inf the gumbel softmax bution converges to the uniform distribution though this gradient estimator is biased we nd that this method works well in practice we choose based on the cnn dm validation set and use this value in all the experiments denote the input article as d the generated summary as s wm the generation of s lows the recursive process that input to the transformer decoder to obtain then input to compute and so on the rst put token is always the special beginning token start encoder transformer as a semantic figure theme modeling is essentially updating ted with a semantic classier the input sentence pair is rst processed by adding a class token in the ning and a separation token between the two tences then the sentence pair is fed into the former encoder and the rst output vector is classied to similar or distinct in fig the packed sequence is then fed as input into ted s transformer encoder the output vector associated with the token cls is then classied into similar distinct categories by a two layer fully connected network we use the following entropy loss to optimize the encoder such that the is semantically similar to and s is also closed to d while is semantically distinct from ltheme classier denoising autoencoder as the generated summary may be off the article theme at the beginning of netuning we also mize ted such that the generated summaries are semantically closed to the input articles we frame the semantic similarity problem in a tive setting to better adapt to the target domain data we add sentence pairs from training articles to facilitate similarity computation concretely during training we pick two utive sequences of tokens and from an article to form a positive sequence pair second sequence is chosen from another random cle in the dataset to form the negative sequence pair following devlin et al each sequence pair is packed into one single sequence by inserting a special token sep between them and adding trainable segment embeddings a cial classication token cls is also added to the beginning of the packed sequence as shown the idea of denoising autoencoder vincent et al has been used in unsupervised machine lation artetxe et al lample et al to prevent the model from learning to merely copy every input word one by one this denoising cess imitates text simplication and helps to rene essential semantic information in detail a sequence of n consecutive tokens from the input article is injected with two types of noise first we insert noisy tokens sampled from other articles in the same dataset into the original sequence at random positions obtaining a new quence with length where is larger than n next similar to lample et al the sequence is slightly shufed by applying a tation such that i where the permutation distance k is set to be of the length of the nal corrupted sequence is denoted as ted model is then trained to self attentionclassifysimilar normalizeadd normalizefeed recover the original token sequence given the rupted sequence ldenoise where ce denotes the mean of token level entropy loss denotes the sequence of probability distribution outputs from the coder with inputting to the encoder the nal objective function is the mean of eq and eq we empirically nd that equal weights between the two terms work well enough in practice lted ltheme ldenoise it is worth pointing out that we do not conduct pretraining on target evaluation datasets this is because for a target dataset we do not know forehand whether the lead x sentences will make a quality summary or not we do have the option to do so on datasets where lead x are good maries however it is potentially cherry picking datasets also we do not conduct supervised tuning with ground truths summaries in evaluation datasets because we want to have an entirely pervised summarization system with motivations stated in the introduction section experiments datasets we evaluate our model on three benchmark marization datasets nyt cnn dm and english gigaword containing k k and m news articles respectively the detailed statistic tion on the datasets can be found in the appendix in nyt following liu and lapata we choose examples as the validation set and lter out examples with summaries of fewer than words in cnn dm similar to see et al and liu and lapata input articles are truncated to tokens in english gigaword we lter out data examples with articles containing onlyunk tokens baseline and metrics we compare ted with the following baselines unsupervised abstractive systems brief wang and lee baziotis et al radford et al without supervised tuning with ground truths summaries pervised extractive systems textrank mihalcea and tarau lead x supervised tive and abstractive models trained with truths summaries pacsum zheng and ata pgnet see et al refresh narayan et al and sumo liu et al ted is unsupervised abstractive and therefore not directly comparable with supervised baselines the purpose of supervised systems here is for references we describe the implementation details of our model in appendix we measure the quality of generated summaries by rouge score lin including unigram bigram and longest common quence rouge l results results on english gigaword dataset are shown in table ted outperforms all unsupervised lines table shows the experimental results on nyt and cnn dm datasets in nyt the vised ne tuning of ted improves upon the trained model by on rouge l respectively note that rouge metric prefers extractive systems that serve original phrasing see et al ing this factor ted achieves results that are petitive with unsupervised extractive baselines and surpasses all unsupervised abstractive models in cnn dm ted with a larger model size outperforms all unsupervised abstractive methods and compares favorably with unsupervised tive baselines note that ted outperforms a powerful transformer based language generation model pretrained on large scale webpage textual data by signicant margins again ted further improves upon pretrained models on both and congurations table results on the english gigaword dataset formances of baseline models are collected from their original papers the best performance in each metric is in bold model ted ours pretrained ours ted ours pretrained ours brief rl table rouge scores on cnn dm and nyt datasets rl stands for l respectively best results in each unsupervised category is in bold results of baseline models are obtained from their original papers or running open sourced codes model rl rl cnn dm nyt ted ours pretrained ours ted ours pretrained ours brief textrank tf idf textrank skip thought textrank bert pacsum tf idf pacsum skip thought pacsum bert unsupervised abstractive unsupervised extractive sumo pgnet refresh supervised abstractive extractive article after exposing potential security risks with airlines entertainment systems one of the top experts on counter threat intelligence in the world was pulled off a ight by fbi agents chris roberts who featured in a string of fox news reports was yanked off his plane after it landed in syracuse new york on wednesday night by two fbi agents and two uniformed ofcers roberts who works for security intelligence company one world labs was questioned for the next four hours ted summary chris roberts who works for security intelligence company one world labs was pulled off a plane in syracuse new york on wednesday night by two fbi agents and two uniformed ofcers the incident occurred only a few hours after a report about roberts research was released by the government accountability ofce earlier this week reference chris roberts of one world labs grabbed after plane landed in syracuse two fbi agents spent four hours questioning him about cyberhacking agents conscated electronic devices and computer les from roberts he ew in to give talk at aerospace conference about plane vulnerabilities roberts featured on fox news on the record with greta van susteren regarded as one of the world s top experts on counter threat intelligence figure an example of a generated summary by ted the reference summary and parts of the input article are also included discussion ablation study the ablation studies shown in table verify the effectiveness of each component in ted ing the transformer encoder decoder from scratch yields reasonable performance pretraining on large scale data results in more than ment on all three metrics on training ted from scratch pretraining plus either theme modeling or denoising improves upon the pretrained model by more than the full ted model pretraining with theme modeling and denoising produces the best result overall table ablation study of different components in ted on the nyt dataset we test with the model conguration model rl train from scratch pretrained only pretrained theme modeling pretrained denoise loss full model few hours after a report about roberts research was released it shows that fact cross checking is a potential future research direction abstractiveness to examine how abstractive ted is we compute the proportion of novel grams in the summary output fig the erence summary and the output from pgnet are included for comparison although ted is pervised it includes more novel grams than the pervised model pgnet the reference summaries have the highest proportion of n grams comparison with previous unsupervised models ted is an innovative unsupervised summarization model with several distinctive features setting it apart from previous approaches such as meansum and first ted leverages the structure of news articles for an effective large scale pretraining second although both meansum and have a loss to make the summary similar to the input article they leverage the classical cosine similarity on text embeddings in contrast ted innovatively encodes the similarity by a transformer encoder with much more modeling capability third the denoising module in ted is completely distinct from the idea of reconstruction in and sum in ted s denoising module the corrupted texts are input to the transformer and the model is trained to lter the added noises the original clean document is not used as input and thus unseen by ted in the forward pass however the tion process in meansum and employs the original document to generate a summary which is then used to reconstruct the original document figure proportion of novel grams in summaries erated by different models on the cnn dm test set conclusion model analysis example we showcase a sample summary from cnn dm dataset along with the input article and the reference summary fig as shown ted is able to capture and organize the essential mation into uent and highly readable language we attribute the grammatical correctness to the pretraining process and the denoising autoencoder however we also note that although ted ages to recognize the temporal information related to reported event a few hours after fox news ports it makes a mistake by summarizing as a in this paper we propose ted an unsupervised abstractive summarization model first we duce an effective large scale pretraining approach leveraging the lead bias in news articles the training employs automatic ltering mechanism and does require any human labeled data we then develop a netuning scheme to induce the tic similarity between summaries and input articles together with a denoising autoencoder to improve the quality of generated summaries experiments across three datasets show that ted signicantly outperforms unsupervised abstractive baselines of novel n gramspgnettedreference references mikel artetxe gorka labaka eneko agirre and kyunghyun cho unsupervised neural chine translation arxiv preprint christos baziotis ion androutsopoulos ioannis stas and alexandros potamianos seq differentiable sequence to sequence to sequence autoencoder for unsupervised abstractive sentence compression arxiv preprint sergey brin and lawrence page the anatomy of a large scale hypertextual web search engine in computer networks and isdn systems pages elsevier science publishers b v eric chu and peter j liu meansum a neural model for unsupervised multi document abstractive summarization arxiv preprint jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language ing arxiv preprint li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon unied language model pre training for natural language arxiv preprint understanding and generation thibault fevry and jason phang vised sentence compression using denoising encoders arxiv e prints page beliz gunel chenguang zhu michael zeng and dong huang mind the facts boosted coherent abstractive text summarization arxiv preprint karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend in advances in neural information processing systems pages eric jang shixiang gu and ben poole ical reparameterization with gumbel softmax arxiv preprint taku kudo and john richardson sentencepiece a simple and language independent subword enizer and detokenizer for neural text processing arxiv preprint guillaume lample alexis conneau ludovic denoyer and marcaurelio ranzato unsupervised chine translation using monolingual corpora only arxiv preprint liyuan liu haoming jiang pengcheng he weizhu chen xiaodong liu jianfeng gao and jiawei han on the variance of the adaptive learning rate and beyond arxiv preprint yang liu and mirella lapata text tion with pretrained encoders arxiv e prints page yang liu ivan titov and mirella lapata in gle document summarization as tree induction proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies volume long and short papers pages minh thang luong ilya sutskever quoc v le oriol vinyals and wojciech zaremba addressing the rare word problem in neural machine translation arxiv preprint bryan mccann james bradbury caiming xiong and richard socher learned in translation textualized word vectors in advances in neural formation processing systems pages rada mihalcea and paul tarau textrank ing order into text in proceedings of the ference on empirical methods in natural language processing pages shashi narayan shay b cohen and mirella lapata ranking sentences for extractive arxiv preprint tion with reinforcement learning rodrigo nogueira and kyunghyun cho arxiv preprint sage re ranking with bert matthew e peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer deep contextualized word sentations arxiv preprint alec radford karthik narasimhan tim salimans and improving language ilya sutskever standing by generative pre training alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever language models are unsupervised multitask learners openai blog abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proceedings of the nual meeting of the association for computational linguistics volume long papers pages chin yew lin rouge a package for automatic in text summarization evaluation of summaries branches out pages rico sennrich barry haddow and alexandra birch neural machine translation of rare words with subword units arxiv preprint ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural networks in advances in neural information processing tems pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in i guyon u v luxburg s bengio h wallach r fergus s vishwanathan and r nett editors advances in neural information cessing systems pages curran ciates inc pascal vincent hugo larochelle yoshua bengio and pierre antoine manzagol extracting and composing robust features with denoising in proceedings of the international coders conference on machine learning pages acm yau shian wang and hung yi lee learning to encode text as human readable summaries ing generative adversarial networks arxiv preprint zichao yang zhiting hu chris dyer eric p xing and taylor berg kirkpatrick unsupervised text style transfer using language models as tors in advances in neural information processing systems pages ziyi yang chenguang zhu vin sachidananda and embedding imputation with arxiv preprint eric darve grounded language information hao zheng and mirella lapata sentence ity revisited for unsupervised summarization arxiv preprint chenguang zhu michael zeng and xuedong huang sdnet contextualized attention based deep network for conversational question answering arxiv preprint a implementation details for pretraining we use a dropout rate of for all inputs to transformer layers we use radam liu et al as the optimizer with a learning rate of also due to the different numerical scales of the positional embedding and initialized sentence piece embeddings we divide the tional embedding by before feeding it into the transformer we pretrain one model for epochs after each epoch the model is evaluated on idation data we pick the check points with the highest rouge l for unsupervised netuning on specic datasets the learning rate is set to and dropout ratio stays the same as in pretraining the batch size is and the vocabulary embeddings are also updated in the training process during the test phase we generate the summarization from trained encoder and decoder by beam search the rouge version we use for evaluation is this is consistent with benchmark models whose version of rouge are available in open sourced codes and original papers at test time we limit the longest length of erated summaries which is set based on validation dataset for instance the maximum generation length for cnn dm dataset is b datasets information for a better understanding of the evaluation tocols the statistical information of evaluation datasets is summarized in table table average document and summary length in number of words and sentences on nyt cnn dm and english gigaword datasets test set dataset docs avg document words avg summ sen sen words cnn dm nyt gigaword
