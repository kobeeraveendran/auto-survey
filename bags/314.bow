deep reinforced model zero shot cross lingual summarization bilingual semantic similarity rewards zi yi dou sachin kumar yulia tsvetkov language technologies institute carnegie mellon university zdou sachink cmu edu n u j l c s c v v x r abstract cross lingual text summarization aims erating document summary language given input language tically important explored task marily dearth available data isting methods resort machine translation synthesize training data pipeline proaches suffer error propagation work propose end end cross lingual text summarization model model uses reinforcement learning directly optimize bilingual semantic similarity metric summaries generated target language gold summaries source language introduce techniques pre train model leveraging monolingual summarization machine translation objectives mental results english chinese english german cross lingual summarization settings demonstrate effectiveness methods addition nd ment learning models bilingual semantic similarity rewards generate uent tences strong baselines introduction cross lingual text summarization xls task compressing long article language summary different language dearth training corpora standard sequence sequence approaches summarization applied task traditional approaches xls follow pipeline example summarizing article source language followed ing summary target language versa wan et al wan approaches require separately trained tion translation models suffer error propagation zhu et al com figure minimizing xls entropy loss lxls apply reinforcement ing optimize model directly comparing outputs gold references source language prior studies attempted train xls models end end fashion knowledge lation pre trained machine translation mt monolingual summarization models ayana et al duan et al approaches shown work short outputs tively zhu et al proposed automatically translate source language summaries training set generating pseudo reference summaries target language parallel dataset source documents target summaries end end model trained simultaneously marize translate multi task objective xls model trained end end trained mt generated reference translations prone compounding translation summarization errors work propose train end end xls model directly generate target language summaries given source articles matching semantics predictions semantics source language summaries achieve use reinforcement learning rl gual semantic similarity metric reward eting et al metric computed tween machine generated summary target language gold summary source guage additionally better initialize xls article enmodel summary zhgold reference engeneratedreference zhlclslrl goal train summarization model takes article source language xsrc input generates summary pre specied target language ytgt xsrc learnable parameters f training gold summary tgt available model consists encoder denoted e takes xsrc input generates tor representation h h fed input coders rst decoder predicts summary target language ytgt token time second decoder predicts translation input text vtgt training xls test time intuitively want model select parts input article important summary translate target language bias model encode behavior propose following algorithm pre training use machine translation mt model erate pseudo reference summaries ytgt translating ysrc target language translate ytgt source language ing target source mt model discard examples high reconstruction errors measured rouge lin scores details step found zhu et al pre train model parameters task objective based mt monolingual summarization objectives simple effective techniques described ne tune model ment learning bilingual semantic ity metric wieting et al reward described supervised pre training stage describe second step rithm figure pre training loss use weighted combination objectives ilarly zhu et al use xls training objective mt pre training objective described simple effective improvements introduce additional jective based distilling knowledge lingual summarization model illustration supervised pre training figure stage model trained cross lingual rization machine translation distillation objectives parameters layers decoders shared tasks model rl propose new multi task training objective based machine translation monolingual summarization encode common information available tasks able model differentiate tasks add task specic tags input wu et al evaluate proposed method english chinese english german xls test sets test corpora constructed rst system translate source summaries language post edited man annotators experimental results demonstrate proposed pre training method ne tuning rl improves performing baseline rouge l points applying reinforcement learning yields provements performance rouge l points extensive analyses human evaluation bilingual tic similarity reward model generates summaries accurate longer ent relevant summaries generated baselines model section describe details task proposed approach problem description rst formalize task setup given n articles summaries source language src src training set src src xls pre training objective lxls tive computes cross entropy loss tions considering machine generated summaries target language tgt ences given src inputs sample loss formally written lxls log tgt src m m number tokens summary joint training machine translation zhu et al argue machine translation considered special case xls sion ratio line zhu et al train e encoder decoder translation model mt parallel corpus tgt goal step encoder inductive bias encoding information specic translation similar lxls machine translation objective training ple lmt src lmt log tgt src k k number tokens tgt lxls lmt objectives inspired zhu et al propose following enhancements model leverage better objectives share parameters layers decoders share common high level representations parameters layers specialized decoding separately trained append articial task tag ing xls training mt training beginning input ment model aware kind input dealing simple modications result noticeable performance improvements knowledge distillation monolingual marization bias encoder identify tences relevant summary use extractive monolingual tion method predict probability qi sentence keyword input article evant summary distill knowledge model encoder e making predict probabilities concretely append additional output layer encoder model predicts probability pi including sentence word summary objective minimize difference pi qi use following loss sample model ldis log qj log l l l number sentences keywords article nal pre training objective vised pre training stage lsup lxls lmt ldis hyper parameter set experiments training lmt requires mt parallel corpus objectives lize cross lingual summarization dataset training algorithm alternates parts objective mini batches datasets follows convergence sample minibatch mt tgt train parameters e src lmt sample minibatch xls corpus tgt train parameters e lxls ldis src reinforcement learning stage xls target language reference summaries ytgt pre training automatically generated mt models contain errors section describe ne tune model generated source language summaries ysrc reinforcement learning rl specically rst feed article xsrc input encoder e generate target language summary ytgt ing compute cross lingual similarity metric ytgt ysrc use reward ne tune e experimented common distillation qi log pi tive based minimizing kl divergence n perform following paulus et al adopt different strategies generate ytgt training iteration tgt obtained sampling softmax layer decoding step yg tgt obtained greedy decoding rl objective sample given lrl tgt log tgt m r reward function ne tune model use following hybrid training objective lrl scaling factor train cross lingual similarity model xsim best performing model wieting et al model trained mt lel corpus xsim obtain sentence sentations ytgt ysrc treat cosine similarity representations reward r experimental setup datasets evaluate models english chinese english german article summary datasets english chinese dataset created zhu et al constructed cnn dailymail monolingual summarization corpus hermann et al training validation test sets consist k k k samples tively english german dataset tion constructed gigaword dataset rush et al sample m training k tion k test samples dataset parallel corpora language pairs structed translating summaries target language ltered translation training validation test sets pseudo parallel training sets pre training lxls lated chinese german summaries test articles post edited human annotators construct test set evaluating xls refer readers zhu et al details english chinese dataset use word based segmentation source articles english character based segmentation target summaries chinese zhu et al english german dataset byte pair encoding sennrich et al k merge operations machine translation training xsim model sub sample m tences chinese english german english training dataset bojar et al implementation details use transformer base model vaswani et al underlying architecture model e extractive summarization model distillation baselines refer reader vaswani et al eter details input article special token added beginning sentence mark sentence boundaries cnn dailymail corpus monolingual extractive summarization distillation objective chitecture encoder e trained cnn dailymail corpus constructed liu lapata train encoder ldis nal hidden representation token apply layer feed forward network relu activation middle layer moid nal layer qi sentence gigaword dataset inputs outputs typically short choose keywords sentences prediction unit ically rst use textrank mihalcea tarau extract keywords source document keyword appears target summary gold label qi tion assigned qi assigned keywords appear target share parameters ers decoder multi task setting use trigram model wieting et al measure cross lingual sentence semantic larities pointed pre training stage use xls nal results obtained e use metrics evaluating performance models rouge l lin xsim wieting et al following paulus et al select equation gigaword corpus cnn dailymail dataset baselines compare proposed method ing baselines method english chinese rouge l xsim english german rouge l xsim pipeline based methods tran sum zhu et al sum tran zhu et al end end training methods mle xls mle zhu et al mle reimplemented mle rl rouge rl xsim rl table performance different models highest scores bold statistical signicance compared best baseline indicated p computed compare mt neubig et al xsim computed target language system outputs source language reference summaries pipeline approaches report results summarize translate translate summarize tran sum pipelines results taken zhu et al sum tran mle xls pre train e lxls ne tuning mle pre train e lxls lmt ldis best performing model zhu et al reported results results implementation mle pre train model ing ne tuning rl share decoder layers add task specic tags input described rl rouge rouge score reward function shown improve tion quality monolingual summarization els paulus et al baseline tune pre trained model baseline rouge l reward instead posed xsim rouge l score computed tween output machine generated summary ytgt rl use average rouge score xsim score reward function ne tune pre trained model method mle xls extract dis mle extract dis rouge l table effect hard extract vs soft dis extraction summary sentences input article rouge l points tran sum performs worse sum tran likely translation model trained sentences long articles translating article sentences introduces way errors ing short summary fewer sentences pre training method described mle proposed model performs strongest baseline mle rouge l xsim applying reinforcement learning ne tune model rouge rl rouge xsim xsim mean rl wards results improvements posed method rl xsim performs best overall indicating importance cross lingual similarity reward function rl rouge uses machine generated reference compute wards target language summaries able reason worse mance results analysis main results experiments rized table pipeline approaches expected weakest performance lagging weakest end end approach section conduct experiments cnn dailymail dataset establish importance proposed method gain ther insights model figure reinforcement learning model better generating long summaries use compare mt tool neubig et al statistics method share tag rouge l table effect sharing decoder layers adding task specic tags soft distillation vs hard extraction sults table adding edge distillation objective ldis pre training leads improvement performance tuition ldis bias model softly select sentences input article important summary place soft selection hard selection monolingual extractive tion model described extract sentences input article use input encoder instead compare method ldis shown table mle xls pre training objective extract shows improvement albeit lower overall bers performance leads decrease formance mle hand distillation objective helps cases effect sharing tagging techniques table demonstrate introducing simple enhancements like sharing lower layers decoder share adding task specic tags tags multi task pre training helps ing performance fewer parameters smaller memory footprint effect summary lengths study different baselines model performs respect generating summaries chinese different lengths terms number acters shown figure ne tuning model rl proposed model better generating longer summaries pre training referred gure rl xsim forming best cases posit improvement rl based ne tuning ing problem exposure bias introduced teacher forced pre training especially helps longer generations human evaluation addition automatic evaluation misleading perform human evaluation summaries erated models randomly sample pairs model outputs test set ask human evaluators compare pre trained supervised learning model reinforcement ing models terms relevance uency pair evaluators asked pick rst model mle lose second models win prefer tie results summarized ble observe outputs model trained rouge l rewards favored ones generated pre trained model terms relevance uency likely rl rouge model trained generated summaries references lack uency figure displays example hand cross lingual semantic similarity reward results generations favored terms relevance uency related work previous work cross lingual text rization utilize summarize translate figure example outputs bilingual semantic similarity rewards output uent rouge l rewards sup refers mle baseline metric relevance fluency model v mle win lose tie rl rouge rl xsim rl rouge rl xsim table results showing preferences human tors summaries generated mentioned rl methods vs ones pre trained model referred short mle translate summarize pipeline wan et al wan yao et al ouyang et al methods suffer error tion demonstrated sub optimal performance experiments recently work training models task end end fashion ayana et al duan et al zhu et al els trained cross entropy generated summaries references ready lost information translation step prior work monolingual summarization explored hybrid extractive abstractive rization objectives inspires distillation objective gehrmann et al hsu et al chen bansal line research mainly focus compressing sentences tracted pre trained model biasing diction certain words language generation models trained entropy teacher forcing suffer sure bias mismatch training evaluation objective solve issues reinforcement learning ne tune models explored monolingual summarization rouge rewards typically paulus et al liu et al pasunuru bansal rewards bert score zhang et al explored li et al computing rewards requires access gold summaries typically unavailable cross lingual summarization work rst explore cross lingual similarity reward work issue conclusion work propose use reinforcement ing bilingual semantic similarity metric rewards cross lingual document summarization demonstrate effectiveness proposed approach resource decient setting language gold summaries available propose simple strategies better initialize model reinforcement learning aging machine translation monolingual marization future work plan explore methods stabilizing reinforcement learning extend methods datasets tasks bilingual similarity ric reward improve quality machine translation bill raise legal age buy cigarettes voted law wednesday city council new york largest city raise purchase age federal limit years old law expected effect early year york largest purchase age united states new york rst city raise legal drinking age york largest purchase age united states legal age increased city council approved law wednesday increase age tobacco purchases new york rst city raise legal drinking age york largest city united states buying cigarettes city council approved law wednesday increase age tobacco purchases new york rst city raise legal drinking age acknowledgements grateful junnan zhu john wieting lai vogler graham neubig helpful tions chunting zhou shuyan zhou reading paper thank ruihan zhai zhi hao zhou help human evaluation anurag katakkar post editing english dataset material based work supported nsf grants zon mlra award thank amazon providing gpu credits references ayana shi qi shen yun chen cheng yang zhi yuan liu mao song sun zero shot ieee acm lingual neural headline generation transactions audio speech language cessing ondrej bojar christian buck christian federmann barry haddow philipp koehn johannes leveling christof monz pavel pecina matt post herve saint amand al findings workshop statistical machine translation proc wmt ondrej bojar rajen chatterjee christian federmann yvette graham barry haddow shujian huang matthias huck philipp koehn qun liu varvara logacheva et al findings ference machine translation proc wmt yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proc acl xiangyu duan mingming yin min zhang boxing chen weihua luo zero shot lingual abstractive sentence summarization teaching generation attention proc acl chin yew lin rouge package matic evaluation summaries text tion branches linqing liu yao lu min yang qiang qu jia zhu hongyan li generative adversarial proc work abstractive text summarization aaai yang liu mirella lapata text tion pretrained encoders proc emnlp rada mihalcea paul tarau textrank ing order text proc emnlp graham neubig zi yi dou junjie hu paul michel danish pruthi xinyi wang compare mt tool holistic comparison language tion systems proc naacl demo jessica ouyang boya song kathleen mckeown robust abstractive system cross lingual summarization proc naacl ramakanth pasunuru mohit bansal reward reinforced summarization saliency entailment proc naacl romain paulus caiming xiong richard socher deep reinforced model abstractive marization proc iclr alexander m rush sumit chopra jason weston neural attention model abstractive tence summarization proc emnlp rico sennrich barry haddow alexandra birch neural machine translation rare words subword units proc acl ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention need proc neurips sebastian gehrmann yuntian deng alexander rush abstractive summarization proc emnlp xiaojun wan bilingual information proc cross language document summarization acl karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend proc neurips xiaojun wan huiying li jianguo xiao cross language document summarization based proc machine translation quality prediction acl wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun unied model extractive abstractive summarization inconsistency loss proc acl john wieting taylor berg kirkpatrick kevin gimpel graham neubig bleu training neural machine translation semantic similarity proc acl siyao li deren lei pengda qin william yang wang deep reinforcement learning tributional semantic rewards abstractive rization proc emnlp john wieting kevin gimpel graham neubig lor berg kirkpatrick simple effective paraphrastic similarity parallel translations proc acl yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al google s neural machine translation system bridging gap arxiv preprint man machine translation jin ge yao xiaojun wan jianguo xiao phrase based compressive cross language rization proc emnlp tianyi zhang varsha kishore felix wu kilian q weinberger yoav artzi bertscore arxiv preprint uating text generation bert junnan zhu qian wang yining wang yu zhou jun zhang shaonan chengqing zong ncls neural cross lingual summarization proc emnlp
