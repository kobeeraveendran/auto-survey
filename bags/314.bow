deep reinforced model zero shot cross lingual summarization bilingual semantic similarity rewards dou sachin kumar yulia tsvetkov language technologies institute carnegie mellon university zdou sachink cmu edu abstract cross lingual text summarization aims erating document summary language given input language tically important explored task marily dearth available data isting methods resort machine translation synthesize training data pipeline proaches suffer error propagation work propose end end cross lingual text summarization model model uses reinforcement learning directly optimize bilingual semantic similarity metric summaries generated target language gold summaries source language introduce techniques pre train model leveraging monolingual summarization machine translation objectives mental results english chinese english german cross lingual summarization settings demonstrate effectiveness methods addition ment learning models bilingual semantic similarity rewards generate uent tences strong baselines introduction cross lingual text summarization xls task compressing long article language summary different language dearth training corpora standard sequence sequence approaches summarization applied task traditional approaches xls follow pipeline example summarizing article source language followed ing summary target language versa wan wan approaches require separately trained tion translation models suffer error propagation zhu com figure minimizing xls entropy loss lxls apply reinforcement ing optimize model directly comparing outputs gold references source language prior studies attempted train xls models end end fashion knowledge lation pre trained machine translation monolingual summarization models ayana duan approaches shown work short outputs tively zhu proposed automatically translate source language summaries training set generating pseudo reference summaries target language parallel dataset source documents target summaries end end model trained simultaneously marize translate multi task objective xls model trained end end trained generated reference translations prone compounding translation summarization errors work propose train end end xls model directly generate target language summaries given source articles matching semantics predictions semantics source language summaries achieve use reinforcement learning gual semantic similarity metric reward eting metric computed tween machine generated summary target language gold summary source guage additionally better initialize xls article enmodel summary zhgold reference engeneratedreference zhlclslrl goal train summarization model takes article source language xsrc input generates summary pre specied target language ytgt xsrc learnable parameters training gold summary tgt available model consists encoder denoted takes xsrc input generates tor representation fed input coders rst decoder predicts summary target language ytgt token time second decoder predicts translation input text vtgt training xls test time intuitively want model select parts input article important summary translate target language bias model encode behavior propose following algorithm pre training use machine translation model erate pseudo reference summaries ytgt translating ysrc target language translate ytgt source language ing target source model discard examples high reconstruction errors measured rouge lin scores details step found zhu pre train model parameters task objective based monolingual summarization objectives simple effective techniques described tune model ment learning bilingual semantic ity metric wieting reward described supervised pre training stage describe second step rithm figure pre training loss use weighted combination objectives ilarly zhu use xls training objective pre training objective described simple effective improvements introduce additional jective based distilling knowledge lingual summarization model illustration supervised pre training figure stage model trained cross lingual rization machine translation distillation objectives parameters layers decoders shared tasks model propose new multi task training objective based machine translation monolingual summarization encode common information available tasks able model differentiate tasks add task specic tags input evaluate proposed method english chinese english german xls test sets test corpora constructed rst system translate source summaries language post edited man annotators experimental results demonstrate proposed pre training method tuning improves performing baseline rouge points applying reinforcement learning yields provements performance rouge points extensive analyses human evaluation bilingual tic similarity reward model generates summaries accurate longer ent relevant summaries generated baselines model section describe details task proposed approach problem description rst formalize task setup given articles summaries source language src src training set src src xls pre training objective lxls tive computes cross entropy loss tions considering machine generated summaries target language tgt ences given src inputs sample loss formally written lxls log tgt src number tokens summary joint training machine translation zhu argue machine translation considered special case xls sion ratio line zhu train encoder decoder translation model parallel corpus tgt goal step encoder inductive bias encoding information specic translation similar lxls machine translation objective training ple lmt src lmt log tgt src number tokens tgt lxls lmt objectives inspired zhu propose following enhancements model leverage better objectives share parameters layers decoders share common high level representations parameters layers specialized decoding separately trained append articial task tag ing xls training training beginning input ment model aware kind input dealing simple modications result noticeable performance improvements knowledge distillation monolingual marization bias encoder identify tences relevant summary use extractive monolingual tion method predict probability sentence keyword input article evant summary distill knowledge model encoder making predict probabilities concretely append additional output layer encoder model predicts probability including sentence word summary objective minimize difference use following loss sample model ldis log log number sentences keywords article nal pre training objective vised pre training stage lsup lxls lmt ldis hyper parameter set experiments training lmt requires parallel corpus objectives lize cross lingual summarization dataset training algorithm alternates parts objective mini batches datasets follows convergence sample minibatch tgt train parameters src lmt sample minibatch xls corpus tgt train parameters lxls ldis src reinforcement learning stage xls target language reference summaries ytgt pre training automatically generated models contain errors section describe tune model generated source language summaries ysrc reinforcement learning specically rst feed article xsrc input encoder generate target language summary ytgt ing compute cross lingual similarity metric ytgt ysrc use reward tune experimented common distillation log tive based minimizing divergence perform following paulus adopt different strategies generate ytgt training iteration tgt obtained sampling softmax layer decoding step tgt obtained greedy decoding objective sample given lrl tgt log tgt reward function tune model use following hybrid training objective lrl scaling factor train cross lingual similarity model xsim best performing model wieting model trained lel corpus xsim obtain sentence sentations ytgt ysrc treat cosine similarity representations reward experimental setup datasets evaluate models english chinese english german article summary datasets english chinese dataset created zhu constructed cnn dailymail monolingual summarization corpus hermann training validation test sets consist samples tively english german dataset tion constructed gigaword dataset rush sample training tion test samples dataset parallel corpora language pairs structed translating summaries target language ltered translation training validation test sets pseudo parallel training sets pre training lxls lated chinese german summaries test articles post edited human annotators construct test set evaluating xls refer readers zhu details english chinese dataset use word based segmentation source articles english character based segmentation target summaries chinese zhu english german dataset byte pair encoding sennrich merge operations machine translation training xsim model sub sample tences chinese english german english training dataset bojar implementation details use transformer base model vaswani underlying architecture model extractive summarization model distillation baselines refer reader vaswani eter details input article special token added beginning sentence mark sentence boundaries cnn dailymail corpus monolingual extractive summarization distillation objective chitecture encoder trained cnn dailymail corpus constructed liu lapata train encoder ldis nal hidden representation token apply layer feed forward network relu activation middle layer moid nal layer sentence gigaword dataset inputs outputs typically short choose keywords sentences prediction unit ically rst use textrank mihalcea tarau extract keywords source document keyword appears target summary gold label tion assigned assigned keywords appear target share parameters ers decoder multi task setting use trigram model wieting measure cross lingual sentence semantic larities pointed pre training stage use xls nal results obtained use metrics evaluating performance models rouge lin xsim wieting following paulus select equation gigaword corpus cnn dailymail dataset baselines compare proposed method ing baselines method english chinese rouge xsim english german rouge xsim pipeline based methods tran sum zhu sum tran zhu end end training methods mle xls mle zhu mle reimplemented mle rouge xsim table performance different models highest scores bold statistical signicance compared best baseline indicated computed compare neubig xsim computed target language system outputs source language reference summaries pipeline approaches report results summarize translate translate summarize tran sum pipelines results taken zhu sum tran mle xls pre train lxls tuning mle pre train lxls lmt ldis best performing model zhu reported results results implementation mle pre train model ing tuning share decoder layers add task specic tags input described rouge rouge score reward function shown improve tion quality monolingual summarization els paulus baseline tune pre trained model baseline rouge reward instead posed xsim rouge score computed tween output machine generated summary ytgt use average rouge score xsim score reward function tune pre trained model method mle xls extract dis mle extract dis rouge table effect hard extract soft dis extraction summary sentences input article rouge points tran sum performs worse sum tran likely translation model trained sentences long articles translating article sentences introduces way errors ing short summary fewer sentences pre training method described mle proposed model performs strongest baseline mle rouge xsim applying reinforcement learning tune model rouge rouge xsim xsim mean wards results improvements posed method xsim performs best overall indicating importance cross lingual similarity reward function rouge uses machine generated reference compute wards target language summaries able reason worse mance results analysis main results experiments rized table pipeline approaches expected weakest performance lagging weakest end end approach section conduct experiments cnn dailymail dataset establish importance proposed method gain ther insights model figure reinforcement learning model better generating long summaries use compare tool neubig statistics method share tag rouge table effect sharing decoder layers adding task specic tags soft distillation hard extraction sults table adding edge distillation objective ldis pre training leads improvement performance tuition ldis bias model softly select sentences input article important summary place soft selection hard selection monolingual extractive tion model described extract sentences input article use input encoder instead compare method ldis shown table mle xls pre training objective extract shows improvement albeit lower overall bers performance leads decrease formance mle hand distillation objective helps cases effect sharing tagging techniques table demonstrate introducing simple enhancements like sharing lower layers decoder share adding task specic tags tags multi task pre training helps ing performance fewer parameters smaller memory footprint effect summary lengths study different baselines model performs respect generating summaries chinese different lengths terms number acters shown figure tuning model proposed model better generating longer summaries pre training referred gure xsim forming best cases posit improvement based tuning ing problem exposure bias introduced teacher forced pre training especially helps longer generations human evaluation addition automatic evaluation misleading perform human evaluation summaries erated models randomly sample pairs model outputs test set ask human evaluators compare pre trained supervised learning model reinforcement ing models terms relevance uency pair evaluators asked pick rst model mle lose second models win prefer tie results summarized ble observe outputs model trained rouge rewards favored ones generated pre trained model terms relevance uency likely rouge model trained generated summaries references lack uency figure displays example hand cross lingual semantic similarity reward results generations favored terms relevance uency related work previous work cross lingual text rization utilize summarize translate figure example outputs bilingual semantic similarity rewards output uent rouge rewards sup refers mle baseline metric relevance fluency model mle win lose tie rouge xsim rouge xsim table results showing preferences human tors summaries generated mentioned methods ones pre trained model referred short mle translate summarize pipeline wan wan yao ouyang methods suffer error tion demonstrated sub optimal performance experiments recently work training models task end end fashion ayana duan zhu els trained cross entropy generated summaries references ready lost information translation step prior work monolingual summarization explored hybrid extractive abstractive rization objectives inspires distillation objective gehrmann hsu chen bansal line research mainly focus compressing sentences tracted pre trained model biasing diction certain words language generation models trained entropy teacher forcing suffer sure bias mismatch training evaluation objective solve issues reinforcement learning tune models explored monolingual summarization rouge rewards typically paulus liu pasunuru bansal rewards bert score zhang explored computing rewards requires access gold summaries typically unavailable cross lingual summarization work rst explore cross lingual similarity reward work issue conclusion work propose use reinforcement ing bilingual semantic similarity metric rewards cross lingual document summarization demonstrate effectiveness proposed approach resource decient setting language gold summaries available propose simple strategies better initialize model reinforcement learning aging machine translation monolingual marization future work plan explore methods stabilizing reinforcement learning extend methods datasets tasks bilingual similarity ric reward improve quality machine translation bill raise legal age buy cigarettes voted law wednesday city council new york largest city raise purchase age federal limit years old law expected effect early year york largest purchase age united states new york rst city raise legal drinking age york largest purchase age united states legal age increased city council approved law wednesday increase age tobacco purchases new york rst city raise legal drinking age york largest city united states buying cigarettes city council approved law wednesday increase age tobacco purchases new york rst city raise legal drinking age acknowledgements grateful junnan zhu john wieting lai vogler graham neubig helpful tions chunting zhou shuyan zhou reading paper thank ruihan zhai zhi hao zhou help human evaluation anurag katakkar post editing english dataset material based work supported nsf grants zon mlra award thank amazon providing gpu credits references ayana shi shen yun chen cheng yang zhi yuan liu mao song sun zero shot ieee acm lingual neural headline generation transactions audio speech language cessing ondrej bojar christian buck christian federmann barry haddow philipp koehn johannes leveling christof monz pavel pecina matt post herve saint amand findings workshop statistical machine translation proc wmt ondrej bojar rajen chatterjee christian federmann yvette graham barry haddow shujian huang matthias huck philipp koehn qun liu varvara logacheva findings ference machine translation proc wmt yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proc acl xiangyu duan mingming yin min zhang boxing chen weihua luo zero shot lingual abstractive sentence summarization teaching generation attention proc acl chin yew lin rouge package matic evaluation summaries text tion branches linqing liu yao min yang qiang jia zhu hongyan generative adversarial proc work abstractive text summarization aaai yang liu mirella lapata text tion pretrained encoders proc emnlp rada mihalcea paul tarau textrank ing order text proc emnlp graham neubig dou junjie paul michel danish pruthi xinyi wang compare tool holistic comparison language tion systems proc naacl demo jessica ouyang boya song kathleen mckeown robust abstractive system cross lingual summarization proc naacl ramakanth pasunuru mohit bansal reward reinforced summarization saliency entailment proc naacl romain paulus caiming xiong richard socher deep reinforced model abstractive marization proc iclr alexander rush sumit chopra jason weston neural attention model abstractive tence summarization proc emnlp rico sennrich barry haddow alexandra birch neural machine translation rare words subword units proc acl ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need proc neurips sebastian gehrmann yuntian deng alexander rush abstractive summarization proc emnlp xiaojun wan bilingual information proc cross language document summarization acl karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend proc neurips xiaojun wan huiying jianguo xiao cross language document summarization based proc machine translation quality prediction acl wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun unied model extractive abstractive summarization inconsistency loss proc acl john wieting taylor berg kirkpatrick kevin gimpel graham neubig bleu training neural machine translation semantic similarity proc acl siyao deren lei pengda qin william yang wang deep reinforcement learning tributional semantic rewards abstractive rization proc emnlp john wieting kevin gimpel graham neubig lor berg kirkpatrick simple effective paraphrastic similarity parallel translations proc acl yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey google neural machine translation system bridging gap arxiv preprint man machine translation jin yao xiaojun wan jianguo xiao phrase based compressive cross language rization proc emnlp tianyi zhang varsha kishore felix kilian weinberger yoav artzi bertscore arxiv preprint uating text generation bert junnan zhu qian wang yining wang zhou jun zhang shaonan chengqing zong ncls neural cross lingual summarization proc emnlp
