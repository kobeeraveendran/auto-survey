iterative document representation learning summarization polishing xiuying shen chongyang yan dongyan rui data science peking university beijing china computer science technology peking university beijing china ai lab xy chen chongyangtao zhaody edu cn com abstract paper introduce iterative text marization iteration based model supervised extractive text summarization spired observation essary human read article multiple times order fully understand rize contents current summarization proaches read document generate document representation ing sub optimal representation dress issue introduce model iteratively polishes document tion passes document model introduce tive reading mechanism decides curately extent sentence model updated experimental results cnn dailymail datasets demonstrate model cantly outperforms state art extractive systems evaluated machines humans introduction summary shortened version text ument maintains important ideas original article automatic text rization process machine gleans important concepts article removing secondary redundant concepts nowadays growing need storing digesting large amounts textual data automatic rization systems signicant usage potential society extractive summarization technique generating summaries directly choosing set salient sentences original ment constitute summary efforts extractive summarization rely corresponding author rui yan edu cn human engineered features sentence length word position frequency cohen radev et al woodsend lapata yan et al b use neural networks automatically learn features tence selection cheng lapata ati et al existing extractive summarization methods achieved great success tion share generate summary pass document real world human cognitive processes people read document multiple times order capture main ideas browsing document means model fully document s main ideas leading subpar summarization share examples consider situation nish reading long article forget main points beginning likely review forget write good summary usually rst browse document obtain general understanding article perform intensive reading select salient points include summary terms model design believe letting model read document multiple times polishing updating internal representation document lead better understanding better summarization achieve design model iterative text summarization consisting novel iteration mechanism selective iterative process ing module ing document times encoder decoder iterative unit iteration work polish ument representation nal labeling uses outputs iterations generate summaries selective reading module design m l c s c v v x r ed version gated recurrent unit gru work decide hidden state sentence retained dated based relationship document overall contribution includes propose iterative text summarization iteration based summary generator uses sequence classier extract salient sentences documents introduce novel iterative neural work model repeatedly polishes distributed representation document stead generating propose selective reading mechanism decides information updated sentence based lationship polished document resentation entire architecture trained end end fashion evaluate summarization model representative cnn dailymail corpora benchmark dataset tal results demonstrate model performs state art extractive systems evaluated automatically human related work research builds previous works elds summarization iterative modeling text summarization classied tractive summarization abstractive rization extractive summarization aims ate summary integrating salient tences document abstractive tion aims generate new content concisely paraphrases document scratch emergence powerful neural work models text processing vast majority literature document summarization icated abstractive summarization els typically form convolutional ral networks cnn recurrent neural networks rnn example rush et al propose encoder decoder model uses local tention mechanism generate summaries lapati et al develop work addressing problems adequately solved basic architecture keyword modeling capturing hierarchy word structures follow work nallapati al propose new summarization model generates summaries sampling topic sentence time producing words ing rnn decoder conditioned sentence topic related work et al authors use pointing coverage techniques generate accurate summaries despite focus abstractive summarization extractive summarization remains attractive method capable generating matically semantically correct summaries method follow work tractive summarization cheng lapata propose general framework single document text summarization hierarchical article coder composed attention based extractor following nallapati et al propose simple rnn based sequence classier outperforms matches state art models time approach narayan et al use reinforcement learning method timize rouge evaluation metric text marization recent work topic wu hu authors train reinforced neural extractive summarization model called rnes captures cross sentence ence patterns fact use ferent dataset released code unable compare models theirs idea iteration explored summarization related study xiong et al s work dynamic memory works designs neural networks ory attention mechanisms exhibit certain reasoning capabilities required question swering related work yan generate poetry iterative ing sn chema similiar method applied couplet generation yan et al inspiration work focus document summarization related work singh et al authors present deep network called hybrid memnet gle document summarization task ory network document encoder compared borrow memory network structure propose new iterative architecture methodology problem formulation work propose iterative text marization iteration based supervised model extractive text summarization treat extractive summarization task sequence labeling problem sentence ited sequentially binary label determines included nal mary generated wi takes input list sentences s sns ns number tences document sentence list words wi nw nw word length sentence goal generate score vector y yns sentence score yi denotes sentence s extracting probability probability corresponding sentence extracted included mary train model supervised ner corresponding gold summary written human experts document training set use unsupervised method convert human written summaries gold label tor ns denotes th sentence selected training process cross entropy loss calculated y imized optimize y finally select tences highest score according y extracted summary detail model low model architecture depicted fig consists multiple erations encoder decoder iteration unit iteration combine outputs decoders iterations generate extracting probabilities nal labeling ule encoder illustrated shaded region left half fig takes input sentences document representation previous unit processes eral neural networks outputs nal state iterative unit module updates ment representation decoder takes form bidirectional rnn takes representation sentence erated encoder input initial state polished document representation dk module sentence labeling module catenates hidden states decoders generate integrated score sentence apply supervised training objective maximize likelihood sentence labels ns given input document model parameters log log model encoder subsection describe encoding cess model brevity drop script focusing particular layer w b section different perscripts subscripts parameters learned sentence encoder given discrete set tences s sns use word ding matrix m rv d embed word wi sentence continuous space wi v vocabulary size d dimension word embedding sentence encoder based variety encoding schemes simply taking average embeddings words sentence cause information loss grus long short term memory lstm requires computational resources prone ting considering select positional coding described sukhbaatar et al sentence encoding method sentence resentation si calculated si lj wi j element wise multiplication lj umn vector computed lj j d nw nw lj denotes th dimension lj note study use grus rnn cells alleviate tting problem conrmed experiments selective reading mechanism explained later modied version inal gru cell details gru gru gating mechanism recurrent neural networks introduced cho et al performance found similar lstm cell fewer parameters scribed hochreiter schmidhuber gru cell consists update gate vector figure model structure encoder decoder iterative unit polish document representation iteration nal labeling generating extracting probabilities sentences combining hidden states decoders iterations document consists sentences example ui reset gate vector ri output vector hi time step input xi vious hidden state updated hidden state hi computed ui u ri u hi ri u hi ui hi ui sigmoid w u w r w h activation function rnh ni u u u r u rnh nh nh hidden size ni size input xi study interactions tion exchanges sentences establish bi directional gru bi gru network taking sentence representation input si si si si si sentence representation input time step si hidden state forward gru time step si hidden state backward gru architecture allows formation ow forth generate new sentence representation document encoder initialize ument representation polishing ating document representation sentence representations process similar ing sentence representation word dings time need compress document sentence vector cause information vector contain ited use neural network simply use non linear transformation erage pooling concatenated hidden states bi gru generate document resentation written si si ns concatenation operation selective reading module mally introduce selective reading module fig module bidirectional rnn sisting modied gru cells input sentence representation s sns original version gru update gate ui equation decide den state retained updated way ui lated sensitive position ordering sentences loses information captured polished document representation propose modied gru cell reading iterative unititerative unit replace ui newly computed update gate gi new cell takes inputs tence representation document tion iteration merely sentence representation sentence lective network generates update gate vector gi following way fi si fi w gi si th sentence representation document representation eration equation hi gi hi gi use selective reading module matically decide extent information sentence updated based relationship polished document way modied gru network grasp accurate information document iterative unit sentence passes selective reading module wish update document representation newly constructed sentence representations iterative unit depicted fig designed pose use gruiter cell generate ished document representation input nal state selective reading network previous iteration hns initial state set document representation ous iteration updated document resentation computed dk decoder describe decoders picted shaded right fig ing sequence labeling task xue palmer carreras learn feature vector sentence use bidirectional grudec network iteration output features calculate extracting bilities k th iteration given sentence resentation s input document sentation dk initial state decoder codes features sentences hidden state hk hk ns hk hk hk dk sentence labeling module use feature sentence ate corresponding extracting probability decoder iteration directly transform hidden states iteration tracting probabilities end scores sentence taking age summing specic weights inappropriate inelegant nate hidden states decoders ply multi layer perceptron generate extracting probabilities w yns yi extracting ability setence way let model learn utilize outputs iterations assign hidden state liable weight section labeling method outperforms methods experiment setup section present experimental setup training estimating summarization model rst introduce datasets training evaluation introduce perimental details evaluation protocol datasets order fair comparison lines cnn dailymail constructed hermann et al standard splits training validation testing corpus uments cnn dailymail followed previous studies ing human written story highlight cle gold standard abstractive summary highlights generate gold labels training testing model greedy search method similar nallapati et al tested domain pus consists documents documents corpus belong clusters cluster unique topic document gold summaries written man experts length words implementation details implemented model tensorow abadi et al code models able followed settings nallapati et al trained model ing adam optimizer kingma ba initial learning rate anneals epochs reaching epochs lected sentences highest scores mary preliminary exploration found arranging according scores sistently achieved best performance periments performed batch size documents dimension glove pennington et al embeddings trained wikipedia embedding initialization vocabulary size limited speed purposes initialized vocabulary word embeddings uniform distribution padded cut sentences tain exactly words gru module layer dimensional hidden states initial state set described random initial state prevent overtting dropout gru network ding layer applied loss ased variables iteration number set specied detailed discussion tion number found section baselines datasets method baseline simply chooses rst tences document gold summary dailymail datasets report performance summarunner nallapati et al model cheng lapata logistic regression classier lreg baseline reimplemented brid memnet model singh et al baselines reported formance samples paper narayan et al released refresh model code produce rouge recall scores dailymail dataset reported results cnn dailymail joint dataset baselines cnn dataset similar com yingtaomj iterati ve document representation learning tow ards summarization polishing com edinburghnlp refr esh corpus compare model baselines integer linear ming ilr lreg report mance newest neural networks model cluding nallapati et al cheng ata singh et al evaluation evaluation procedure rouge scores e rouge l responding matches unigram bigrams longest common subsequence lcs tively estimate model obtained rouge scores standard pyrouge compare related works length score cnn corpus ited length bytes bytes recall score dailymail corpus corpus following ofcial guidelines examined rouge recall score length words results experiment statistically nt condence interval estimated rouge script schluter noted rouge metric evaluate summarization quality misleading evaluated model human evaluation highly ucated participants asked rank maries produced models line hybrid memnet human authored highlights chose hybrid memnet human evaluation baselines mance relatively high compared lines judging criteria included informativeness coherence test cases randomly sampled dailymail test set experiment analysis table shows performance comparison model baselines dailymail dataset respect rouge score bytes bytes summary length model performs consistently signicantly better models bytes bytes improvement margin smaller possible terpretation model high precision rank outputs accuracy lower lower rank sentences addition cheng lapata additional supervised training python org pypi pyrouge rouge l rouge l dailymail cheng et summarunner refresh hybrid memnet table comparison baselines dailymail test dataset rouge recall score respect abstractive ground truth bytes bytes cnn cheng et hybrid memnet refresh rouge l table comparison baselines cnn test dataset length variants rouge create sentence level extractive labels train model model uses unsupervised greedy approximation instead examined performance model cnn dataset listed table compare models length rouge metric reported narayan et al results demonstrate model consistently best performance different datasets table present performance domain duc dataset model performs matches basic models including lreg ilr neural network baselines summarunner respect ground truth bytes shows model adapted different copora maintaining high accuracy order explore impact internal ture conducted ablation study table rst variation model selective reading module ond sets iteration number model iteration process variation apply mlp output tion instead concatenating hidden states decoders settings parameters performances models worse metrics demonstrates lreg ilp cheng et summarunner hybrid memnet rouge l table comparison baselines dataset rouge recall score spect abstractive ground truth bytes variations selective reading iteration concatenation l table ablation study dailymail test dataset respect abstractive ground truth bytes preeminence importantly controlled experiment verify tion different module discussion analysis iteration number broad sweep experiments investigate uence iteration process generated mary quality studied inuence iteration number order fair ison models different iteration ber trained models epochs tuning fig illustrates relationship iteration number rouge score bytes summary length dailymail test dataset result shows rouge score increases number iteration begin ing upper limit begins drop note models hybrid memnet gold table system ranking comparison lines dailymail corpus rank best rank worst score represents percentage summary rank sitivity iterations shown fig specic sentences preferred iteration remain low probabilities iterations sentences relatively high scores preferred iteration human evaluation gave human tors system generated summaries generated hybrid memnet human written gold standard summary asked rank summaries based summary informativeness coherence table shows percentages summaries different models der rank scored human experts surprising gold standard maries highest quality model summaries rank sidered best following hybrid memnet ranked case study found number maries generated hybrid memnet sentences distinct sentence model leads better evaluation result considering informativeness coherence readers refer appendix case study conclusion work introduce iteration based extractive summarization model inspired observation necessary man read article multiple times fully derstand summarize experimental results cnn dailymail duc corpora demonstrate effectiveness model acknowledgments like thank anonymous ers constructive comments like thank jin ge yao zhengyuan ma valuable advice project work supported national key figure relationship number iteration rouge score dailymail test dataset respect ground truth bytes figure predicted extracting probabilities sentence calculated output iteration result training model epoch outperforms state art singh et al demonstrates selective ing module effective fact ing process increase performance conrms iteration idea model useful practice based observation set default iteration number analysis polishing process fully investigate iterative process inuences extracting results draw heatmaps tracting probabilities decoder eration pick representative cases fig axis represents sentence index y axis iteration number axis labels omitted darker color higher fig extracting probability seen iteration begins sentences similar probabilities increase number iteration probabilities begin fall saturate means model preferred sentences select interesting feature found search development program china national science dation china nsfc rui yan sponsored tencent open research fund microsoft search asia msra collaborative research gram references martn abadi paul barham jianmin chen zhifeng chen andy davis jeffrey dean matthieu devin sanjay ghemawat geoffrey irving michael isard et al tensorow system large scale machine learning osdi volume pages xavier carreras llus duction shared task semantic role labeling proceedings ninth ence computational natural language ing conll pages stroudsburg pa usa association computational linguistics jianpeng cheng mirella lapata neural summarization extracting sentences words kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase representations rnn encoder decoder statistical machine translation computer ence kevin bretonnel cohen natural language cessing online applications text retrieval language traction categorization review karl moritz hermann edward grefenstette lasse peholt kay mustafa suleyman phil som teaching machines read hend pages sepp hochreiter jurgen schmidhuber neural computation long short term memory diederik kingma jimmy ba adam method stochastic optimization computer ence ramesh nallapati igor melnyk abhishek kumar bowen zhou sengen sentence generating neural variational topic model ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre bing xiang abstractive text summarization sequence sequence rnns shashi narayan shay b cohen mirella lapata ranking sentences extractive tion reinforcement learning jeffrey pennington richard socher christopher manning glove global vectors word conference empirical representation ods natural language processing pages dragomir r radev timothy allison sasha goldensohn john blitzer arda celebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu al mead platform tidocument multilingual text summarization lrec alexander m rush sumit chopra jason weston neural attention model abstractive tence summarization computer science natalie schluter limits automatic proceedings marisation according rouge conference european chapter association computational linguistics volume short papers volume pages abigail peter j liu christopher d manning point summarization generator networks corr abhishek kumar singh manish gupta vasudeva varma hybrid memnet extractive marization pages sainbayar sukhbaatar arthur szlam jason weston rob fergus end end memory works computer science kristian woodsend mirella lapata meeting matic generation story highlights association computational linguistics pages yuxiang wu baotian hu learning extract coherent summary deep reinforcement learning arxiv preprint caiming xiong stephen merity richard socher dynamic memory networks visual textual question answering nianwen xue martha palmer calibrating features semantic role labeling proceedings conference empirical methods natural language processing rui yan poet automatic poetry composition recurrent neural networks iterative ishing schema ijcai pages rui yan cheng te li xiaohua hu ming zhang chinese couplet generation neural work structures meeting association computational linguistics pages rui yan jian yun nie xiaoming li marize interested optimization framework interactive personalized tion conference empirical methods ral language processing emnlp july john mcintyre conference centre edinburgh uk meeting sigdat special interest group acl pages rui yan xiaojun wan mirella lapata wayne xin zhao pu jen cheng xiaoming li sualizing timelines evolutionary summarization iterative reinforcement text image proceedings acm streams national conference information knowledge management pages acm rui yan xiaojun wan jahna otterbacher liang kong xiaoming li yan zhang ary timeline summarization balanced tion framework iterative substitution national acm sigir conference research development information retrieval pages
