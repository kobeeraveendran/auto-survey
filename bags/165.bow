iterative document representation learning summarization polishing xiuying shen chongyang yan dongyan rui data science peking university beijing china computer science technology peking university beijing china lab chen chongyangtao zhaody edu com abstract paper introduce iterative text marization iteration based model supervised extractive text summarization spired observation essary human read article multiple times order fully understand rize contents current summarization proaches read document generate document representation ing sub optimal representation dress issue introduce model iteratively polishes document tion passes document model introduce tive reading mechanism decides curately extent sentence model updated experimental results cnn dailymail datasets demonstrate model cantly outperforms state art extractive systems evaluated machines humans introduction summary shortened version text ument maintains important ideas original article automatic text rization process machine gleans important concepts article removing secondary redundant concepts nowadays growing need storing digesting large amounts textual data automatic rization systems signicant usage potential society extractive summarization technique generating summaries directly choosing set salient sentences original ment constitute summary efforts extractive summarization rely corresponding author rui yan edu human engineered features sentence length word position frequency cohen radev woodsend lapata yan use neural networks automatically learn features tence selection cheng lapata ati existing extractive summarization methods achieved great success tion share generate summary pass document real world human cognitive processes people read document multiple times order capture main ideas browsing document means model fully document main ideas leading subpar summarization share examples consider situation nish reading long article forget main points beginning likely review forget write good summary usually rst browse document obtain general understanding article perform intensive reading select salient points include summary terms model design believe letting model read document multiple times polishing updating internal representation document lead better understanding better summarization achieve design model iterative text summarization consisting novel iteration mechanism selective iterative process ing module ing document times encoder decoder iterative unit iteration work polish ument representation nal labeling uses outputs iterations generate summaries selective reading module design version gated recurrent unit gru work decide hidden state sentence retained dated based relationship document overall contribution includes propose iterative text summarization iteration based summary generator uses sequence classier extract salient sentences documents introduce novel iterative neural work model repeatedly polishes distributed representation document stead generating propose selective reading mechanism decides information updated sentence based lationship polished document resentation entire architecture trained end end fashion evaluate summarization model representative cnn dailymail corpora benchmark dataset tal results demonstrate model performs state art extractive systems evaluated automatically human related work research builds previous works elds summarization iterative modeling text summarization classied tractive summarization abstractive rization extractive summarization aims ate summary integrating salient tences document abstractive tion aims generate new content concisely paraphrases document scratch emergence powerful neural work models text processing vast majority literature document summarization icated abstractive summarization els typically form convolutional ral networks cnn recurrent neural networks rnn example rush propose encoder decoder model uses local tention mechanism generate summaries lapati develop work addressing problems adequately solved basic architecture keyword modeling capturing hierarchy word structures follow work nallapati propose new summarization model generates summaries sampling topic sentence time producing words ing rnn decoder conditioned sentence topic related work authors use pointing coverage techniques generate accurate summaries despite focus abstractive summarization extractive summarization remains attractive method capable generating matically semantically correct summaries method follow work tractive summarization cheng lapata propose general framework single document text summarization hierarchical article coder composed attention based extractor following nallapati propose simple rnn based sequence classier outperforms matches state art models time approach narayan use reinforcement learning method timize rouge evaluation metric text marization recent work topic authors train reinforced neural extractive summarization model called rnes captures cross sentence ence patterns fact use ferent dataset released code unable compare models theirs idea iteration explored summarization related study xiong work dynamic memory works designs neural networks ory attention mechanisms exhibit certain reasoning capabilities required question swering related work yan generate poetry iterative ing chema similiar method applied couplet generation yan inspiration work focus document summarization related work singh authors present deep network called hybrid memnet gle document summarization task ory network document encoder compared borrow memory network structure propose new iterative architecture methodology problem formulation work propose iterative text marization iteration based supervised model extractive text summarization treat extractive summarization task sequence labeling problem sentence ited sequentially binary label determines included nal mary generated takes input list sentences sns number tences document sentence list words word length sentence goal generate score vector yns sentence score denotes sentence extracting probability probability corresponding sentence extracted included mary train model supervised ner corresponding gold summary written human experts document training set use unsupervised method convert human written summaries gold label tor denotes sentence selected training process cross entropy loss calculated imized optimize finally select tences highest score according extracted summary detail model low model architecture depicted fig consists multiple erations encoder decoder iteration unit iteration combine outputs decoders iterations generate extracting probabilities nal labeling ule encoder illustrated shaded region left half fig takes input sentences document representation previous unit processes eral neural networks outputs nal state iterative unit module updates ment representation decoder takes form bidirectional rnn takes representation sentence erated encoder input initial state polished document representation module sentence labeling module catenates hidden states decoders generate integrated score sentence apply supervised training objective maximize likelihood sentence labels given input document model parameters log log model encoder subsection describe encoding cess model brevity drop script focusing particular layer section different perscripts subscripts parameters learned sentence encoder given discrete set tences sns use word ding matrix embed word sentence continuous space vocabulary size dimension word embedding sentence encoder based variety encoding schemes simply taking average embeddings words sentence cause information loss grus long short term memory lstm requires computational resources prone ting considering select positional coding described sukhbaatar sentence encoding method sentence resentation calculated element wise multiplication umn vector computed denotes dimension note study use grus rnn cells alleviate tting problem conrmed experiments selective reading mechanism explained later modied version inal gru cell details gru gru gating mechanism recurrent neural networks introduced cho performance found similar lstm cell fewer parameters scribed hochreiter schmidhuber gru cell consists update gate vector figure model structure encoder decoder iterative unit polish document representation iteration nal labeling generating extracting probabilities sentences combining hidden states decoders iterations document consists sentences example reset gate vector output vector time step input vious hidden state updated hidden state computed sigmoid activation function rnh rnh hidden size size input study interactions tion exchanges sentences establish directional gru gru network taking sentence representation input sentence representation input time step hidden state forward gru time step hidden state backward gru architecture allows formation forth generate new sentence representation document encoder initialize ument representation polishing ating document representation sentence representations process similar ing sentence representation word dings time need compress document sentence vector cause information vector contain ited use neural network simply use non linear transformation erage pooling concatenated hidden states gru generate document resentation written concatenation operation selective reading module mally introduce selective reading module fig module bidirectional rnn sisting modied gru cells input sentence representation sns original version gru update gate equation decide den state retained updated way lated sensitive position ordering sentences loses information captured polished document representation propose modied gru cell reading iterative unititerative unit replace newly computed update gate new cell takes inputs tence representation document tion iteration merely sentence representation sentence lective network generates update gate vector following way sentence representation document representation eration equation use selective reading module matically decide extent information sentence updated based relationship polished document way modied gru network grasp accurate information document iterative unit sentence passes selective reading module wish update document representation newly constructed sentence representations iterative unit depicted fig designed pose use gruiter cell generate ished document representation input nal state selective reading network previous iteration hns initial state set document representation ous iteration updated document resentation computed decoder describe decoders picted shaded right fig ing sequence labeling task xue palmer carreras learn feature vector sentence use bidirectional grudec network iteration output features calculate extracting bilities iteration given sentence resentation input document sentation initial state decoder codes features sentences hidden state sentence labeling module use feature sentence ate corresponding extracting probability decoder iteration directly transform hidden states iteration tracting probabilities end scores sentence taking age summing specic weights inappropriate inelegant nate hidden states decoders ply multi layer perceptron generate extracting probabilities yns extracting ability setence way let model learn utilize outputs iterations assign hidden state liable weight section labeling method outperforms methods experiment setup section present experimental setup training estimating summarization model rst introduce datasets training evaluation introduce perimental details evaluation protocol datasets order fair comparison lines cnn dailymail constructed hermann standard splits training validation testing corpus uments cnn dailymail followed previous studies ing human written story highlight cle gold standard abstractive summary highlights generate gold labels training testing model greedy search method similar nallapati tested domain pus consists documents documents corpus belong clusters cluster unique topic document gold summaries written man experts length words implementation details implemented model tensorow abadi code models able followed settings nallapati trained model ing adam optimizer kingma initial learning rate anneals epochs reaching epochs lected sentences highest scores mary preliminary exploration found arranging according scores sistently achieved best performance periments performed batch size documents dimension glove pennington embeddings trained wikipedia embedding initialization vocabulary size limited speed purposes initialized vocabulary word embeddings uniform distribution padded cut sentences tain exactly words gru module layer dimensional hidden states initial state set described random initial state prevent overtting dropout gru network ding layer applied loss ased variables iteration number set specied detailed discussion tion number found section baselines datasets method baseline simply chooses rst tences document gold summary dailymail datasets report performance summarunner nallapati model cheng lapata logistic regression classier lreg baseline reimplemented brid memnet model singh baselines reported formance samples paper narayan released refresh model code produce rouge recall scores dailymail dataset reported results cnn dailymail joint dataset baselines cnn dataset similar com yingtaomj iterati document representation learning tow ards summarization polishing com edinburghnlp refr esh corpus compare model baselines integer linear ming ilr lreg report mance newest neural networks model cluding nallapati cheng ata singh evaluation evaluation procedure rouge scores rouge responding matches unigram bigrams longest common subsequence lcs tively estimate model obtained rouge scores standard pyrouge compare related works length score cnn corpus ited length bytes bytes recall score dailymail corpus corpus following ofcial guidelines examined rouge recall score length words results experiment statistically condence interval estimated rouge script schluter noted rouge metric evaluate summarization quality misleading evaluated model human evaluation highly ucated participants asked rank maries produced models line hybrid memnet human authored highlights chose hybrid memnet human evaluation baselines mance relatively high compared lines judging criteria included informativeness coherence test cases randomly sampled dailymail test set experiment analysis table shows performance comparison model baselines dailymail dataset respect rouge score bytes bytes summary length model performs consistently signicantly better models bytes bytes improvement margin smaller possible terpretation model high precision rank outputs accuracy lower lower rank sentences addition cheng lapata additional supervised training python org pypi pyrouge rouge rouge dailymail cheng summarunner refresh hybrid memnet table comparison baselines dailymail test dataset rouge recall score respect abstractive ground truth bytes bytes cnn cheng hybrid memnet refresh rouge table comparison baselines cnn test dataset length variants rouge create sentence level extractive labels train model model uses unsupervised greedy approximation instead examined performance model cnn dataset listed table compare models length rouge metric reported narayan results demonstrate model consistently best performance different datasets table present performance domain duc dataset model performs matches basic models including lreg ilr neural network baselines summarunner respect ground truth bytes shows model adapted different copora maintaining high accuracy order explore impact internal ture conducted ablation study table rst variation model selective reading module ond sets iteration number model iteration process variation apply mlp output tion instead concatenating hidden states decoders settings parameters performances models worse metrics demonstrates lreg ilp cheng summarunner hybrid memnet rouge table comparison baselines dataset rouge recall score spect abstractive ground truth bytes variations selective reading iteration concatenation table ablation study dailymail test dataset respect abstractive ground truth bytes preeminence importantly controlled experiment verify tion different module discussion analysis iteration number broad sweep experiments investigate uence iteration process generated mary quality studied inuence iteration number order fair ison models different iteration ber trained models epochs tuning fig illustrates relationship iteration number rouge score bytes summary length dailymail test dataset result shows rouge score increases number iteration begin ing upper limit begins drop note models hybrid memnet gold table system ranking comparison lines dailymail corpus rank best rank worst score represents percentage summary rank sitivity iterations shown fig specic sentences preferred iteration remain low probabilities iterations sentences relatively high scores preferred iteration human evaluation gave human tors system generated summaries generated hybrid memnet human written gold standard summary asked rank summaries based summary informativeness coherence table shows percentages summaries different models der rank scored human experts surprising gold standard maries highest quality model summaries rank sidered best following hybrid memnet ranked case study found number maries generated hybrid memnet sentences distinct sentence model leads better evaluation result considering informativeness coherence readers refer appendix case study conclusion work introduce iteration based extractive summarization model inspired observation necessary man read article multiple times fully derstand summarize experimental results cnn dailymail duc corpora demonstrate effectiveness model acknowledgments like thank anonymous ers constructive comments like thank jin yao zhengyuan valuable advice project work supported national key figure relationship number iteration rouge score dailymail test dataset respect ground truth bytes figure predicted extracting probabilities sentence calculated output iteration result training model epoch outperforms state art singh demonstrates selective ing module effective fact ing process increase performance conrms iteration idea model useful practice based observation set default iteration number analysis polishing process fully investigate iterative process inuences extracting results draw heatmaps tracting probabilities decoder eration pick representative cases fig axis represents sentence index axis iteration number axis labels omitted darker color higher fig extracting probability seen iteration begins sentences similar probabilities increase number iteration probabilities begin fall saturate means model preferred sentences select interesting feature found search development program china national science dation china nsfc rui yan sponsored tencent open research fund microsoft search asia msra collaborative research gram references martn abadi paul barham jianmin chen zhifeng chen andy davis jeffrey dean matthieu devin sanjay ghemawat geoffrey irving michael isard tensorow system large scale machine learning osdi volume pages xavier carreras llus duction shared task semantic role labeling proceedings ninth ence computational natural language ing conll pages stroudsburg usa association computational linguistics jianpeng cheng mirella lapata neural summarization extracting sentences words kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase representations rnn encoder decoder statistical machine translation computer ence kevin bretonnel cohen natural language cessing online applications text retrieval language traction categorization review karl moritz hermann edward grefenstette lasse peholt kay mustafa suleyman phil som teaching machines read hend pages sepp hochreiter jurgen schmidhuber neural computation long short term memory diederik kingma jimmy adam method stochastic optimization computer ence ramesh nallapati igor melnyk abhishek kumar bowen zhou sengen sentence generating neural variational topic model ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre bing xiang abstractive text summarization sequence sequence rnns shashi narayan shay cohen mirella lapata ranking sentences extractive tion reinforcement learning jeffrey pennington richard socher christopher manning glove global vectors word conference empirical representation ods natural language processing pages dragomir radev timothy allison sasha goldensohn john blitzer arda celebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu mead platform tidocument multilingual text summarization lrec alexander rush sumit chopra jason weston neural attention model abstractive tence summarization computer science natalie schluter limits automatic proceedings marisation according rouge conference european chapter association computational linguistics volume short papers volume pages abigail peter liu christopher manning point summarization generator networks corr abhishek kumar singh manish gupta vasudeva varma hybrid memnet extractive marization pages sainbayar sukhbaatar arthur szlam jason weston rob fergus end end memory works computer science kristian woodsend mirella lapata meeting matic generation story highlights association computational linguistics pages yuxiang baotian learning extract coherent summary deep reinforcement learning arxiv preprint caiming xiong stephen merity richard socher dynamic memory networks visual textual question answering nianwen xue martha palmer calibrating features semantic role labeling proceedings conference empirical methods natural language processing rui yan poet automatic poetry composition recurrent neural networks iterative ishing schema ijcai pages rui yan cheng xiaohua ming zhang chinese couplet generation neural work structures meeting association computational linguistics pages rui yan jian yun nie xiaoming marize interested optimization framework interactive personalized tion conference empirical methods ral language processing emnlp july john mcintyre conference centre edinburgh meeting sigdat special interest group acl pages rui yan xiaojun wan mirella lapata wayne xin zhao jen cheng xiaoming sualizing timelines evolutionary summarization iterative reinforcement text image proceedings acm streams national conference information knowledge management pages acm rui yan xiaojun wan jahna otterbacher liang kong xiaoming yan zhang ary timeline summarization balanced tion framework iterative substitution national acm sigir conference research development information retrieval pages
