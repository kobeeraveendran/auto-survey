p e s l c s c v v x r controlling output length neural encoder decoders yuta pi titech ac jp graham cmu edu ryohei titech ac jp hiroya titech ac jp manabu titecjh ac jp institute technology japan mellon university usa abstract neural encoder decoder models shown great success sequence generation tasks previous work vestigated situations like control length encoder decoder puts capability crucial tions text summarization generate concise summaries paper desired length pose methods controlling output quence length neural encoder decoder models decoding based methods learning based methods results learning based methods ity control length degrading mary quality summarization task introduction rst use machine translation brenner blunsom cho et al sutskever et al encoder decoder proach demonstrated great success sequence generation tasks including image caption generation vinyals et al xu et al parsing vinyals et al dialogue response generation li et al serban et al sentence summarization rush et al chopra et al particular focus sentence summarization suggests consists generating shorter sions sentences applications document work author nara stitute science technology com kiyukuta lencon summarization nenkova mckeown headline generation dorr et al recently rush et al automatically constructed large training data sentence summarization led rapid development neural sentence summarization nss neural headline generation nhg models studies address task nallapati et al ayana et al ranzato et al lopyrev gulcehre et al gu et al chopra et al essential properties text rization systems ability erate summary desired length desired lengths summaries strongly depends scene use granularity information user wants understand monitor size device user length depends information contained given source document traditional setting text summarization source document desired length summary given input summarization system methods controlling output sequence length decoder models investigated spite importance settings paper propose investigate methods controlling output sequence length neural encoder decoder models methods decoding based receive sired length decoding process training process standard decoder models methods learning based modify network architecture receive desired length input experiments learning based methods outperform decoding based methods long byte summaries nd despite additional length control capability proposed methods remain tive existing methods standard settings shared background related work text summarization oldest elds study natural language processing summarization methods focused specically sentence compression headline generation traditional approaches task focus word deletion rule based dorr et al zajic et al statistical woodsend et al galanis androutsopoulos filippova strube filippova altun al methods studies abstractive sentence summarization ing syntactic transduction cohn lapata napoles et al taking phrase based tistical machine translation approach banko et al wubben et al cohn lapata recent work adopted techniques encoder decoder kalchbrenner blunsom sutskever et al cho et al tional bahdanau et al luong et al neural network models eld machine translation tailored sentence marization task rush et al rst pose sentence summarization new target task neural sequence sequence learning studies task marks neural sequence transduction ods ranzato et al lopyrev ayana et al studies address portant phenomena frequently occurred written summaries copying source document gu et al gulcehre et al nallapati et al investigate way solve important problems capturing keywords inputting multiple sentences neural encoder decoders viewed statistical language models conditioned sentence context rosenfeld et al proposed sentence language models consider features sentence length described introduction knowledge explicitly controlling length output sequences neural language models encoder decoders investigated finally studies modify sequence according meta information dialogue act wen et al user ality li et al politeness sennrich et al studies focused length topic paper importance controlling output length mentioned section standard setting text summarization input source document desired length summary summarization system rization systems able generate maries lengths obviously property essential summarization methods based neural encoder decoder models encoder decoder model completely data driven approach output sequence length depends training data model trained example use sentence summary pairs extracted annotated english gigaword pus training data rush et al average length human written summary bytes figure shows statistics corpus train standard encoder decoder model perform standard beam search decoding corpus average length output sequence byte situations want summaries lengths ple shared task maximum length summaries set bytes rization systems benet generating tences length limit recent nss models trol output length rush et al following use ad method tem inhibited generating end sentence tag eos tag assigning score rst sentence article headline figure histograms rst sentence length headline length ratio annotated gigaword english c ratio word corpus bracketed values subcaption averages given source sentence summarizer generates shortened version input e n m summary sentence y ym model estimates conditional probability ing parameters trained large training data ing sentence summary pairs typically ditional probability factorized product conditional probabilities word quence y t m t following describe compute y t encoder use bi directional rnn birnn coder shown effective neural chine translation bahdanau et al speech recognition schuster paliwal graves et al birnn processes source sentence forward backward directions separate rnns encoding process birnn computes forward hidden states h h h n backward hidden states h h h n follows h t xt h t xt g kind recurrent unit use long short term memory lstm hochreiter schmidhuber networks memory cells directions c t c t figure encoder decoder architecture base model paper generating xed number nally output summaries truncated bytes ideally models able change output quence depending given output length output eos tag appropriate time point natural manner network architecture encoder decoder attention section describe model ture experiments encoder decoder consisting bi directional rnns attention mechanism figure shows architecture model suppose source sentence represented sequence words xn code com facebook namas ber words set long setting average number words human summaries evaluation set published default encoding set initial hidden states controlling length encoder decoders memory cell decoder follows h c decoder attender decoder based rnn lstm g st xt use attention mechanism developed luong et al uses st compute contextual information dt time step t rst summarize forward backward encoder states taking sum hi h h late context vector dt weighted sum summarized vectors dt atihi weight t th step hi puted softmax operation ati hi context vector dt calculated model updates distribution word lows bhs y t bso note st provided input lstm yt step called input feeding architecture luong et al training decoding training objective models maximize log likelihood sentence summary pairs given training set d log x x y t t models trained use beam search nd output maximizes conditional ity section propose methods control length output rst methods decoder framework decoding process control output length changing model methods model changed trained obtain capability controlling length following evaluation dataset experiments use bytes unit length models use words bytes necessary ixlen beam search eos tags rst method examine decoding approach similar taken recent nss ods slightly ad method inhibit decoder generating eos tag assigning score model stop decoding process simply stop decoding process length output sequence reaches desired length cally beam search length quence generated far exceeds desired length word replaced eos tag score word replaced score eos tag eos replacement ixrng discarding range sequences second decoding method based discarding range sequences inhibited generating eos tag allowing decide stop generation instead dene legitimate range sequence setting minimum imum lengths specically addition normal beam search procedure set rules model generates eos tag output sequence shorter minimum length discard sequence beam generated sequence exceeds mum length discard sequence beam replace word eos tag add sequence beam eos replacement section words sequences contain eos tag dened length range method compromise allows model exibility plan generated quences certain acceptable length range noted method needs larger beam size desired length different average summary length training data need preserve hypotheses sired length lenemb length embedding additional input lstm method learning based method ically trained control length output quence inspired previous work strated additional inputs decoder models effectively control characteristics output wen et al li et al model vides information length form additional input net specically model rd potential uses embedding desired length parameterized length rdl l embedding matrix wle number length types decoding process input embedding remaining length additional input lstm figure lt ized encoding process updated decoding process follows length length output word yt length desired length learn values length embedding matrix wle ing method provides additional information length remaining output sequence allowing decoder plan output based remaining number words erate workaround prevent situation sequences discarded beam figure lenemb remaining length tional input lstm decoder figure leninit initial state decoder s memory cell manages output length leninit length based memory cell initialization lenemb inputs remaining length decoder step decoding process leninit method inputs desired length initial state decoder figure shows chitecture leninit specically model uses memory cell mt control output length initializing states decoder hidden state memory cell follows h bc length bc desired length rh trainable parameter length model lenemb guided appropriate output length inputting maining length step leninit attempts provide model ability manage output length inner state ically memory cell lstm networks able endeavour possible lstms rst sentence c ratio summary figure histograms rst sentence length summary length ratio learn functions example subtract xed particular memory cell time output word ways aging length found approach simple effective experiment dataset trained models annotated english gigaword corpus napoles et al rush et al constructed sentence summarization perform preprocessing standard script dataset sists approximately million pairs rst sentence source document line figure shows length histograms summaries training set vocabulary size source documents target summaries including beginning sentence end sentence unknown word tags lenemb leninit input length headline training note train multiple summarization models line length single model capable trolling length output evaluate methods evaluation set generating short document summaries task summarization systems required create short mary given document summaries length limit bytes truncated bonus creating shorter summary evaluation set consists source documents human written reference summaries example add memory cell managing length com facebook namas source document figure shows length tograms summaries evaluation set note human written summaries long bytes variants rouge lin evaluation metrics gram bigram rouge l longest common subsequence sided permutation test chinchor statistical icance testing p implementation use adam kingma ba optimize eters mini batch size updates rst sampled training examples groups examples source sentence length shufed groups set dimension word embeddings hidden state lstms initialize bias forget gate use gate biases jozefowicz et al use chainer tokui et al plement models lenemb set l larger longest summary lengths dataset figure figure methods f ixrng found beam size sufcient f ixrng beam size aggressively cards candidate sequences beams coding result rouge evaluation table shows rouge scores method length limits byte gardless length limit set model ixlen ixrng byte r l r l byte r l byte table rouge scores length limits scores signicantly worse best score column bolded source reference ixlen ixrng lenemb leninit ve time world champion michelle kwan withdrew gure skating championships wednesday petition skating ofcials chance compete turin olympics injury leaves kwan s olympic hopes limbo kwan withdraws gp kwan withdraws skating championships kwan pulls gure skating championships turin olympics kwan withdraws gp kwan withdraws gure skating championships kwan pulls gure skating championships turin olympics bid kwan withdraws skating kwan withdraws gure skating championships world champion kwan withdraws olympic gure skating championships kwan quits gure skating kwan withdraws gure skating worlds kwan withdraws gure skating championships olympics table examples output method specied lengths tion methods use reference summaries note ixlen ixrng generate maries hard constraint ing process allows follow hard constraint length calculate scores lenemb leninit impose hard constraint length comparison fair e table specically use beam search f ixrng minimum length purpose showing length control capability lenemb leninit lines results standard beam search hard constraints use results discussions sections results learning based ixrng equivalence standard beam search set range ods lenemb leninit tend outperform decoding based methods ixlen f ixrng longer summaries bytes byte setting signicant difference types methods hypothesize average sion rate training data figure byte setting forces model erate summaries average sion rate learning based models training data learn compression steep rate examples generated summaries tables examples validation set annotated gigaword corpus bles models including based methods decoding based methods generate formed sentences paraphrases gure source reference ixlen ixrng lenemb leninit people tested positive bird u virus eastern turkey health minister recep akdag told news conference wednesday test positive bird u virus turkey infected bird infected bird u eastern turkey people tested positive bird u eastern turkey says minister infected bird infected bird u eastern turkey people tested positive bird u eastern turkey says minister bird u cases turkey conrmed positive bird u eastern turkey bird patients test positive bird u eastern turkey cases bird turkey people tested positive bird u turkey people tested positive bird u eastern turkey health conference table examples output method withdrew examples generated single noun phrase suitable short length setting length control capability learning based models figure shows histograms output length standard encoder decoder lenemb leninit output lengths standard model disperse widely lengths learning based models concentrated desired length histograms clearly length controlling bility learning based models table shows nal state beam leninit generates sentence length bytes example standard beam search table sentences beam generated length close desired length shows method obtained ability control output length expected parison table shows nal state beam perform standard beam search dard encoder decoder model ixlen ixrng sentence formed lengths varied comparison existing methods objective paper obtain state art scores evaluation set interest length controllable models competitive task table shows scores methods copied table addition scores existing methods abs rush et al standard model neural sentence summarization similar method baseline setting ixlen table shows score f ixlen parable existing methods table shows lenemb leninit capability controlling length ing rouge score conclusion paper presented rst examination problem controlling length neural decoder models point view marization examined methods controlling length output sequences decoding based methods ixlen f ixrng based methods lenemb leninit sults showed learning based methods generally outperform decoding based methods learning based methods obtained capability controlling output length losing rouge score compared existing summarization methods finally compare methods existing ods standard settings shared acknowledgments normalized number united states work supported jsps kakenhi grant number grateful byte candidate summary cases bird turkey bird u cases turkey people tested bird people tested e turkey bird u cases e turkey bird u cases eastern people tested east turkey bird u cases turkey people fail bird u virus byte candidate summary people tested positive bird u eastern turkey tested positive bird u eastern turkey people tested positive bird people infected bird u eastern turkey tested positive bird infected bird u eastern turkey infected bird u eastern turkey conrmed cases bird u eastern turkey people tested positive bird u turkey beam leninit beam standard encoder decoder table final state beam learning based model instructed output byte summary source document table encoder decoder lenemb leninit figure histograms output lengths generated standard encoder decoder lenemb c leninit lenemb leninit bracketed numbers region desired lengths set model ixlen ixrng lenemb leninit et al et al ras et al ras et al table comparison note reproduced table r l existing studies rows opportunity use kurisu server dwango co ltd experiments references ayana et al ayana s shen z liu m sun neural headline generation minimum risk training corr cho yoshua bengio neural machine translation jointly learning align translate proceedings banko et al michele banko vibhu o mittal headline generation proceedings michael j witbrock based statistical translation pages nancy chinchor cal signicance results proceedings pages cho et al kyunghyun cho bart van merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase representations rnn encoder decoder statistical machine translation proceedings pages chopra et al sumit chopra michael auli alexander m rush abstractive sentence marization attentive recurrent neural networks proceedings naacl pages cohn trevor cohn mirella lapata sentence compression word deletion proceedings pages bahdanau et al dzmitry bahdanau kyunghyun cohn trevor cohn mirella lapata abstractive approach sentence sion acm july models proceedings naacl pages dorr et al bonnie dorr david zajic richard schwartz hedge trimmer parse trim approach headline generation proceedings hlt naacl text summarization workshop pages filippova katja filippova yasemin altun overcoming lack parallel data sentence compression proceedings pages filippova katja filippova michael strube dependency tree based sentence pression proceedings pages filippova et al katja filippova enrique seca carlos colmenares lukasz kaiser oriol vinyals sentence compression deletion lstms proceedings pages galanis dimitrios galanis ion androutsopoulos extractive vised stage method sentence compression proceedings naacl pages graves al graves n jaitly r hamed hybrid speech recognition deep bidirectional lstm proceedings ieee workshop pages gu et al jiatao gu zhengdong lu hang li victor o k li incorporating copying anism sequence sequence learning ings pages gulcehre et al caglar gulcehre sungjin ahn ramesh nallapati bowen zhou yoshua bengio pointing unknown words proceedings pages hochreiter sepp hochreiter jurgen schmidhuber long short term memory neural computation al rafal jozefowicz wojciech zaremba ilya sutskever empirical exploration recurrent network architectures proceedings pages kalchbrenner nal kalchbrenner phil blunsom recurrent continuous translation proceedings pages models seattle washington usa october association computational linguistics kingma diederik p kingma jimmy ba adam method stochastic tion proceedings et al jiwei li michel galley chris brockett jianfeng gao bill dolan promoting objective function neural conversation et al jiwei li michel galley chris brockett georgios spithourakis jianfeng gao bill dolan persona based neural conversation model proceedings pages chin yew lin rouge package automatic evaluation summaries proceedings workshop pages konstantin lopyrev generating news headlines recurrent neural networks corr luong et al thang luong hieu pham christopher d manning effective approaches attention based neural machine translation proceedings pages nallapati et al ramesh nallapati bing xiang bowen zhou sequence sequence rnns text summarization corr napoles et al courtney napoles chris burch juri ganitkevitch benjamin van durme paraphrastic sentence compression character based metric tightening deletion proceedings workshop monolingual text generation pages napoles et al courtney napoles matthew ley benjamin van durme annotated gaword proceedings joint workshop tomatic knowledge base construction web scale knowledge extraction pages nenkova ani nenkova leen mckeown automatic summarization foundations trends r information retrieval volume pages sumit ranzato et al marcaurelio chopra michael auli wojciech zaremba sequence level training recurrent neural networks corr ranzato rosenfeld et al ronald rosenfeld stanley f chen xiaojin zhu sentence exponential language models vehicle statistical integration computer speech language rush et al alexander m rush sumit chopra jason weston neural attention model proceedings abstractive sentence summarization pages schuster m schuster k k bidirectional recurrent neural ieee transactions signal processing wal works sennrich et al rico sennrich barry haddow alexandra birch controlling politeness ral machine translation constraints ceedings naacl pages serban et al iulian vlad serban alessandro doni yoshua bengio aaron c courville joelle pineau building end end dialogue systems generative hierarchical neural network models proceedings pages sutskever et al ilya sutskever oriol vinyals quoc v le sequence sequence learning proceedings pages neural networks tokui et al seiya tokui kenta oono shohei hido justin clayton chainer generation open source framework deep learning proceedings workshop learningsys vinyals et al oriol vinyals lukasz kaiser terry koo slav petrov ilya sutskever geoffrey e ton grammar foreign language ceedings pages vinyals et al oriol vinyals alexander toshev samy bengio dumitru erhan tell neural image caption generator ings ieee conference computer vision pattern recognition pages wen et al tsung hsien wen milica gasic nikola mrksic pei hao su david vandyke steve young semantically conditioned lstm based natural language generation spoken dialogue tems proceedings pages lisbon portugal september association tational linguistics woodsend et al kristian woodsend yansong feng mirella lapata title generation proceedings quasi synchronous grammar pages wubben et al sander wubben antal van den bosch emiel krahmer sentence cation monolingual machine translation ceedings pages xu et al kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan nov rich zemel yoshua bengio attend tell neural image caption generation visual attention david blei francis bach editors proceedings pages jmlr workshop conference proceedings zajic et al david zajic bonnie j dorr r schwartz bbn umd topiary proceedings naacl document standing workshop pages
