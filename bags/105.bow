low resource neural headline generation ottokar tilk and tanel alumae department of software science school of information technologies tallinn university of technology estonia ottokar ee tanel ee l u j l c s c v v i x r a abstract recent neural headline generation models have shown great results but are generally trained on very large datasets we focus our efforts on improving headline quality on smaller datasets by the means of training we propose new methods that enable pre training all the parameters of the model and utilize all available text sulting in improvements by up to relative in perplexity and points in rouge introduction neural headline generation nhg is the process of automatically generating a headline based on the text of the document using articial neural works headline generation is a subtask of text marization while a summary may cover tiple documents generally uses similar style to the summarized document and consists of tiple sentences headline in contrast covers a gle document is often written in a different style headlinese mardh and is much shorter frequently limited to a single sentence due to shortness and specic style ing the the document into a headline often quires the ability to paraphrase which makes this task a good t for abstractive summarization proaches where neural networks based attentive encoder decoder bahdanau et al type of models have recently shown impressive results e rush et al nallapati et al while state of the art results have been obtained by training nhg models on large datasets like gaword access to such resources is often not sible especially when it comes to low resource in this work we focus on languages ing performance on smaller datasets with different pre training methods one of the reasons to expect pre training to be an effective way to improve performance on small datasets is that nhg models are erally trained to generate headlines based on the documents just a few rst sentences of rush et al shen et al chopra et al nallapati et al this leaves the rest of the text unutilized which can be alleviated by pre training subsets of the model on full ments additionally the decoder component of nhg models can be regarded as a language model lm whose predictions are biased by the external information from the encoder as a lm it sees only headlines during training which is a small fraction of text compared to the documents plementing the training data of the decoder with documents via pre training might enable it to learn more about words and language structure although some of the previous work has used pre training before nallapati et al alimoff it is not fully explored how much pre training helps and what is the optimal way to do it another problem is that in previous work only a subset of parameters usually just dings is pre trained leaving the rest of the eters randomly initialized the main contributions of this paper are lm pre training for fully initializing the encoder and decoder sections and combining lm pre training with distant supervision mintz et al pre training using ltered sentences of the documents as noisy targets i e predicting one sentence given the rest to maximally utilize the entire available dataset and pre train all the paramters of the nhg model section and analysis of the effect of pre training different ponents of the nhg model section yt encoder attention decoder enc emb init dec emb xn figure a high level description of the nhg model the model predicts the next headline word yt given the words in the document xn and already generated headline words method the model that we use follows the architecture scribed by bahdanau et al although inally created for neural machine translation this architecture has been successfully used for nhg e by shen et al nallapati et al and in a simplied form by chopra et al the nhg model consists of a bidirectional schuster and paliwal encoder with gated recurrent units gru cho et al a rectional gru decoder and an attention nism and a decoder initialization layer that connect the encoder and decoder bahdanau et al during headline generation the encoder reads and encodes the words of the document ized by the encoder the decoder then starts ating the headline one word at a time attending to relevant parts in the document using the attention mechanism figure during training the eters are optimized to maximize the probabilities of reference headlines while generally at the start of training the parameters of all the components are randomly initialized or only pre trained dings with dashed outline in figure are used nallapati et al paulus et al gulcehre et al we propose pre training methods for more extensive initialization encoder pre training when training a nhg model most approaches generally use a limited number of rst sentences or tokens of the document for example rush et al shen et al chopra et al use only the rst sentence of the document and nallapati et al use up to rst sentences while efcient training is faster and takes less memory as the input sequences are shorter and effective the most informative content tends to be at the beginning of the document nallapati et al this leaves the rest of the sentences in the document unused better understanding of words and their context can be learned if all sentences are used especially on small training sets to utilize the entire training set we pre train the encoder on all the sentences of the training set uments since the encoder consists of two rent components a forward and backward gru we pre train them separately first we add a max output layer to the forward gru and train it on the sentences to predict the next word given the previous ones i e we train it as a lm after convergence on the validation set sentences we take the embedding weights of the forward gru and use them as xed parameters for the backward gru then we train the backwards gru ing the same procedure as with the forward gru with the exception of processing the sentences in a reverse order when both models are fully trained we remove the softmax output layers and ize the encoder of the nhg model with the beddings and gru parameters of the trained lms highlighted with gray background in figure decoder pre training pre training the decoder as a lm seems natural since it is essentially a conditional lm during nhg model training the decoder is fed only line words which is relatively little data compared to the document contents to improve the quality of the headlines it is essential to have high ity embeddings that are a good semantic sentation of the input words and to have a well trained recurrent and output layer to predict ble words that make up coherent sentences when it comes to statistical models the simplest way to improve the quality of the parameters is to train the model on more data but it also has to be the right kind of data moore and lewis to increase the amount of suitable training data for the decoder we use lm pre training on tered sentences of the training set documents for ltering we use the xenc tool by rousseau with the cross entropy difference ltering moore and lewis in our case the domain data is training set headlines out domain data is the sentences from training set documents and the best cut off point is evaluated on validation set headlines the careful selection of sentences is mostly motivated by preventing the pre trained coder from deviating too much from headlinese but it also reduces training time before pre training we initialize the input and output embeddings of the lm for words that are common in both encoder and decoder vocabulary with the corresponding pre trained encoder beddings we train the lm on the selected tences until perplexity on the validation set lines stops improving and then use it to initialize the decoder parameters of the nhg model lighted with dotted background in figure a similar approach without data selection and embedding initialization has also been used by alimoff distant supervision pre training approaches described in sections and able full pre training of the encoder and decoder but leaves the connecting parameters with white background in figure untrained this still as results in language modelling suggest surrounding sentences contain useful tion to predict words in the current sentence wang and cho this implies that other sentences contain informative sections that the tention mechanism can learn to attend to and eral context that the initialization component can learn to extract to utilize this phenomenon we propose using carefully picked sentences from the documents as pseudo headlines and pre train the nhg model to generate these given the rest of sentences in the document our pseudo headline picking strategy consists of choosing sentences that occur within rst tokens of the document and were retained during cross entropy ltering in section ing sentences from the beginning of the document should give us the most informative sentences and cross entropy ltering keeps sentences that most closely resemble headlines the pre training procedure starts with ing the encoder and decoder with lm pre trained parameters sections and after that we continue training the attention and initialization parameters until perplexity on validation set lines converges we then use the trained ters to initialize all parameters of the nhg model has been also used by supervision multi document summarization distant for no pre training embeddings encoder decoder enc dec distant all enc dec dist y t i e l p r e p epoch figure validation set en perplexities of the nhg model with different pre training methods model no pre training embeddings encoder decoder enc dec distant all enc dec dist ppl en ppl et table perplexities on the test set with a klakow and peters condence interval all pre trained models are signicantly better than the no pre training baseline bravo marquez and manriquez experiments we evaluate the proposed pre training methods in terms of rouge and perplexity on two relatively small datasets english and estonian training details all our models use hidden layer sizes of and the weights are initialized according to glorot and bengio the vocabularies sist of up to most frequent training set words that occur at least times the model is implemented in theano bergstra et al bastien et al and trained on gpus using mini batches of size during training the weights are updated with adam kingma and ba parameters and and norm of is kept within a threshold of the gradient en et model no pre training embeddings encoder decoder enc dec distant all enc dec dist rlr rlp rlr rlp table recall and precision of and rouge l on the test sets best scores in bold results with statistically signicant differences condence compared to no pre training underlined pascanu et al during headline generation we use beam search with beam size datasets the use cnn daily mail for dataset we hermann et al experiments on english en the number of headline document pairs is and in training validation and test set correspondingly the processing consists of tokenization lowercasing replacing numeric characters with and ing irrelevant parts editor notes timestamps from the beginning of the document with heuristic rules for estonian et experiments we use a ilarly sized and training validation and test split dataset that also consist of news from two sources during ing compound words are split words are cased and numbers are written out as words we used estnltk orasmaa et al stemmer for rouge evaluations results and analysis models are evaluated in terms of perplexity ppl and full length rouge lin in tion to pre training methods described in sections we also test initializing only the dings using parameters from the lm pre trained encoder and decoder embeddings initializing the encoder and decoder but leaving connecting parameters randomized enc dec pre training the whole model from random initialization with distant supervision only distant all and a line that is not pre trained at all no pre training all pre training methods gave signicant provements in ppl table the best method nyu enc dec dist improved the test set ppl by relative pre trained nhg models also converged faster during training figure and most of them beat the nal ppl of the baseline already after the rst epoch general trend is that pre training a larger amount of parameters and the parameters closer to the outputs of the nhg model improves the ppl more distant all is an tion to that observation as it used much less ing data same as baseline than other methods for rouge evaluations we report and rouge l table in contrast with ppl evaluations some pre training methods ther do nt improve signicantly or even worsen rouge measures another difference pared to ppl evaluations is that for rouge training parameters that reside further from puts embeddings and encoder seems more ecial this might imply that a better ment representation is more important to stay on topic during beam search while it is less tant during ppl evaluation where predicting next target headline word with high condence is warded and the process is aided by previous get headline words that are fed to the decoder as inputs it is also possible that a well trained decoder becomes too reliant on expecting rect words as inputs making it sensitive to errors during generation which would somewhat explain why enc dec performs worse than encoder alone this hypothesis can be checked in further work by experimenting with methods like uled sampling bengio et al that should crease the robustness to mistakes during tion pre training all parameters on all available text enc dec dist still gives the best result on english and quite decent results on estonian best models improve rouge by points document reference headline no pre training embeddings encoder decoder enc dec distant all enc dec dist document reference headline no pre training embeddings encoder decoder enc dec distant all enc dec dist a democratic congressman is at the head of a group of representatives trying to help undocumented immigrants avoid deportations with what they have called the family defender toolkit the informational pamphlet includes a bilingual card that some are calling a get out of deportation free card that lists reasons a person should not be deported under expanded congressman is developing a get out of deportation toolkit to help mented immigrants if they are detained congressman calls for undocumented immigrants congressman calls for help from immigrants trying to help immigrants ing deportation republican congressman calls for immigrants trying to avoid deportation congressman who tried to stop deportations of immigrants immigration congressman at the head of the head of the group who tries to avoid deportation congressman calls for deportation to immigrants who stay in the country congressman tries to help undocumented immigrants avoid deportation a chihuahua and a bearded dragon showed off their interspecies friendship when they embarked upon a game of tag together videoed in their front room the dog named foxxy cleopatra and the reptile called ryuu can be seen chasing after one another around a coffee table standing perfectly still while looking in the other direction the bearded dragon initially appears ested as the chihuahua jumps around excitedly you re it is this the creepiest crawly meet the poodle it s a knockout the bearded dragon lizard the bearded dragon lizard spotted in the middle of the street oh this is a lion meet the dragon dragon meet the dragon dragon is this the world s youngest lion table examples of generated headlines on cnn daily mail dataset some examples of the generated headlines on the cnn daily mail dataset are shown in table conclusions we proposed three new nhg model pre training methods that in combination enable utilizing the entire dataset and initializing all parameters of the nhg model we also evaluated and analyzed training methods and their combinations in terms of perplexity ppl and rouge the results vealed that better ppl does nt necessarily late to better rouge ppl tends to benet from pre training parameters that are closer to outputs but for rouge it is generally the opposite also ppl beneted from pre training more parameters while for rouge it was not always the case training in general proved to be useful our best results improved ppl by relative and rouge measures by points compared to a nhg model without pre training current work focused on maximally ing available headlined corpora one ing future direction would be to additionally lize potentially much more abundant corpora of documents without headlines also proposed by shen et al for pre training another open question is the relationship between the dataset size and the effect of pre training acknowledgments we would like to thank nvidia for the donated gpu the anonymous reviewers for their valuable comments and kyunghyun cho for the help with the cnn daily mail dataset references alex alimoff abstractive sentence rization with attentive deep recurrent neural works dzmitry bahdanau kyunghyun cho and yoshua neural machine translation by bengio jointly learning to align and translate bastien pascal lamblin razvan pascanu james bergstra ian j goodfellow arnaud eron nicolas bouchard and yoshua bengio theano new features and speed improvements in deep learning and unsupervised feature learning nips workshop samy bengio oriol vinyals navdeep jaitly and noam shazeer scheduled sampling for quence prediction with recurrent neural networks in advances in neural information processing tems pages james bergstra olivier breuleux bastien pascal lamblin razvan pascanu guillaume jardins joseph turian david warde farley and a cpu and yoshua bengio in proceedings gpu math expression compiler of the python for scientic computing conference scipy oral presentation theano felipe bravo marquez and manuel manriquez a zipf like distant supervision approach for document summarization using wikinews articles in international symposium on string processing and information retrieval pages springer association for computational tics karl moritz hermann tomas kocisky ward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend in vances in neural information processing systems nips diederik kingma and jimmy ba adam a method for stochastic optimization arxiv preprint dietrich klakow and jochen peters testing the correlation of word error rate and perplexity speech communication chin yew text summarization branches out chapter rouge a package for automatic evaluation of summaries lin ingrid mardh headlinese on the mar of english front page headlines volume liberlaromedel gleerup mike mintz and steven bills jurafsky rion daniel snow distant supervision for relation extraction without labeled data in proceedings of the joint conference of the annual meeting of the acl and the national joint conference on natural language processing of the afnlp pages association for computational linguistics kyunghyun cho bart van merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio learning phrase representations using rnn encoder decoder for statistical machine translation in proceedings of the conference on ical methods in natural language processing emnlp pages association for computational linguistics c robert moore and william lewis intelligent selection of language model training data in proceedings of the acl conference short papers pages association for tional linguistics ramesh nallapati bowen zhou cicero dos tos caglar gulcehre and bing xiang abstractive text summarization using sequence to sequence rnns and beyond in proceedings of the signll conference on computational natural language learning pages association for computational linguistics m chopra michael alexander auli sumit rush and abstractive sentence summarization with attentive recurrent neural networks in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies pages association for computational linguistics siim orasmaa xavier glorot and yoshua bengio ing the difculty of training deep feedforward neural networks in international conference on articial intelligence and statistics pages caglar gulcehre sungjin ahn ramesh ati bowen zhou and yoshua bengio pointing the unknown words in proceedings of the annual meeting of the association for tational linguistics volume long papers pages timo petmanson alexander tkachenko sven laur and heiki jaan kaalep in estnltk nlp toolkit for estonian the tenth international conference ceedings of on language resources and evaluation lrec paris france european language resources association elra razvan pascanu tomas mikolov and yoshua bengio on the difculty of training recurrent ral networks proceedings of the international conference on machine learning icml romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization arxiv preprint anthony rousseau xenc an open source tool for data selection in natural language processing the prague bulletin of mathematical linguistics m alexander rush sumit jason weston and a neural attention model for abstractive sentence summarization in proceedings of the conference on ical methods in natural language processing pages association for computational linguistics chopra mike schuster and kuldip k paliwal tional recurrent neural networks ieee transactions on signal processing shiqi shen yu zhao zhiyuan liu maosong neural headline generation arxiv preprint sun et al with sentence wise optimization tian wang and kyunghyun cho larger context language modelling with recurrent neural network in proceedings of the annual meeting of the association for computational linguistics volume long papers pages association for computational linguistics
