low resource neural headline generation ottokar tilk tanel alumae department software science school information technologies tallinn university technology estonia ottokar ee tanel ee l u j l c s c v v x r abstract recent neural headline generation models shown great results generally trained large datasets focus efforts improving headline quality smaller datasets means training propose new methods enable pre training parameters model utilize available text sulting improvements relative perplexity points rouge introduction neural headline generation nhg process automatically generating headline based text document articial neural works headline generation subtask text marization summary cover tiple documents generally uses similar style summarized document consists tiple sentences headline contrast covers gle document written different style headlinese mardh shorter frequently limited single sentence shortness specic style ing document headline quires ability paraphrase makes task good t abstractive summarization proaches neural networks based attentive encoder decoder bahdanau et al type models recently shown impressive results e rush et al nallapati et al state art results obtained training nhg models large datasets like gaword access resources sible especially comes low resource work focus languages ing performance smaller datasets different pre training methods reasons expect pre training effective way improve performance small datasets nhg models erally trained generate headlines based documents rst sentences rush et al shen et al chopra et al nallapati et al leaves rest text unutilized alleviated pre training subsets model ments additionally decoder component nhg models regarded language model lm predictions biased external information encoder lm sees headlines training small fraction text compared documents plementing training data decoder documents pre training enable learn words language structure previous work pre training nallapati et al alimoff fully explored pre training helps optimal way problem previous work subset parameters usually dings pre trained leaving rest eters randomly initialized main contributions paper lm pre training fully initializing encoder decoder sections combining lm pre training distant supervision mintz et al pre training ltered sentences documents noisy targets e predicting sentence given rest maximally utilize entire available dataset pre train paramters nhg model section analysis effect pre training different ponents nhg model section yt encoder attention decoder enc emb init dec emb xn figure high level description nhg model model predicts headline word yt given words document xn generated headline words method model use follows architecture scribed bahdanau et al inally created neural machine translation architecture successfully nhg e shen et al nallapati et al simplied form chopra et al nhg model consists bidirectional schuster paliwal encoder gated recurrent units gru cho et al rectional gru decoder attention nism decoder initialization layer connect encoder decoder bahdanau et al headline generation encoder reads encodes words document ized encoder decoder starts ating headline word time attending relevant parts document attention mechanism figure training eters optimized maximize probabilities reference headlines generally start training parameters components randomly initialized pre trained dings dashed outline figure nallapati et al paulus et al gulcehre et al propose pre training methods extensive initialization encoder pre training training nhg model approaches generally use limited number rst sentences tokens document example rush et al shen et al chopra et al use rst sentence document nallapati et al use rst sentences efcient training faster takes memory input sequences shorter effective informative content tends beginning document nallapati et al leaves rest sentences document unused better understanding words context learned sentences especially small training sets utilize entire training set pre train encoder sentences training set uments encoder consists rent components forward backward gru pre train separately add max output layer forward gru train sentences predict word given previous ones e train lm convergence validation set sentences embedding weights forward gru use xed parameters backward gru train backwards gru ing procedure forward gru exception processing sentences reverse order models fully trained remove softmax output layers ize encoder nhg model beddings gru parameters trained lms highlighted gray background figure decoder pre training pre training decoder lm natural essentially conditional lm nhg model training decoder fed line words relatively little data compared document contents improve quality headlines essential high ity embeddings good semantic sentation input words trained recurrent output layer predict ble words coherent sentences comes statistical models simplest way improve quality parameters train model data right kind data moore lewis increase suitable training data decoder use lm pre training tered sentences training set documents ltering use xenc tool rousseau cross entropy difference ltering moore lewis case domain data training set headlines domain data sentences training set documents best cut point evaluated validation set headlines careful selection sentences motivated preventing pre trained coder deviating headlinese reduces training time pre training initialize input output embeddings lm words common encoder decoder vocabulary corresponding pre trained encoder beddings train lm selected tences perplexity validation set lines stops improving use initialize decoder parameters nhg model lighted dotted background figure similar approach data selection embedding initialization alimoff distant supervision pre training approaches described sections able pre training encoder decoder leaves connecting parameters white background figure untrained results language modelling suggest surrounding sentences contain useful tion predict words current sentence wang cho implies sentences contain informative sections tention mechanism learn attend eral context initialization component learn extract utilize phenomenon propose carefully picked sentences documents pseudo headlines pre train nhg model generate given rest sentences document pseudo headline picking strategy consists choosing sentences occur rst tokens document retained cross entropy ltering section ing sentences beginning document informative sentences cross entropy ltering keeps sentences closely resemble headlines pre training procedure starts ing encoder decoder lm pre trained parameters sections continue training attention initialization parameters perplexity validation set lines converges use trained ters initialize parameters nhg model supervision multi document summarization distant pre training embeddings encoder decoder enc dec distant enc dec dist y t e l p r e p epoch figure validation set en perplexities nhg model different pre training methods model pre training embeddings encoder decoder enc dec distant enc dec dist ppl en ppl et table perplexities test set klakow peters condence interval pre trained models signicantly better pre training baseline bravo marquez manriquez experiments evaluate proposed pre training methods terms rouge perplexity relatively small datasets english estonian training details models use hidden layer sizes weights initialized according glorot bengio vocabularies sist frequent training set words occur times model implemented theano bergstra et al bastien et al trained gpus mini batches size training weights updated adam kingma ba parameters norm kept threshold gradient en et model pre training embeddings encoder decoder enc dec distant enc dec dist rlr rlp rlr rlp table recall precision rouge l test sets best scores bold results statistically signicant differences condence compared pre training underlined pascanu et al headline generation use beam search beam size datasets use cnn daily mail dataset hermann et al experiments english en number headline document pairs training validation test set correspondingly processing consists tokenization lowercasing replacing numeric characters ing irrelevant parts editor notes timestamps beginning document heuristic rules estonian et experiments use ilarly sized training validation test split dataset consist news sources ing compound words split words cased numbers written words estnltk orasmaa et al stemmer rouge evaluations results analysis models evaluated terms perplexity ppl length rouge lin tion pre training methods described sections test initializing dings parameters lm pre trained encoder decoder embeddings initializing encoder decoder leaving connecting parameters randomized enc dec pre training model random initialization distant supervision distant line pre trained pre training pre training methods gave signicant provements ppl table best method nyu enc dec dist improved test set ppl relative pre trained nhg models converged faster training figure beat nal ppl baseline rst epoch general trend pre training larger parameters parameters closer outputs nhg model improves ppl distant tion observation ing data baseline methods rouge evaluations report rouge l table contrast ppl evaluations pre training methods ther nt improve signicantly worsen rouge measures difference pared ppl evaluations rouge training parameters reside puts embeddings encoder ecial imply better ment representation important stay topic beam search tant ppl evaluation predicting target headline word high condence warded process aided previous headline words fed decoder inputs possible trained decoder reliant expecting rect words inputs making sensitive errors generation somewhat explain enc dec performs worse encoder hypothesis checked work experimenting methods like uled sampling bengio et al crease robustness mistakes tion pre training parameters available text enc dec dist gives best result english decent results estonian best models improve rouge points document reference headline pre training embeddings encoder decoder enc dec distant enc dec dist document reference headline pre training embeddings encoder decoder enc dec distant enc dec dist democratic congressman head group representatives trying help undocumented immigrants avoid deportations called family defender toolkit informational pamphlet includes bilingual card calling deportation free card lists reasons person deported expanded congressman developing deportation toolkit help mented immigrants detained congressman calls undocumented immigrants congressman calls help immigrants trying help immigrants ing deportation republican congressman calls immigrants trying avoid deportation congressman tried stop deportations immigrants immigration congressman head head group tries avoid deportation congressman calls deportation immigrants stay country congressman tries help undocumented immigrants avoid deportation chihuahua bearded dragon showed interspecies friendship embarked game tag videoed room dog named foxxy cleopatra reptile called ryuu seen chasing coffee table standing perfectly looking direction bearded dragon initially appears ested chihuahua jumps excitedly creepiest crawly meet poodle s knockout bearded dragon lizard bearded dragon lizard spotted middle street oh lion meet dragon dragon meet dragon dragon world s youngest lion table examples generated headlines cnn daily mail dataset examples generated headlines cnn daily mail dataset shown table conclusions proposed new nhg model pre training methods combination enable utilizing entire dataset initializing parameters nhg model evaluated analyzed training methods combinations terms perplexity ppl rouge results vealed better ppl nt necessarily late better rouge ppl tends benet pre training parameters closer outputs rouge generally opposite ppl beneted pre training parameters rouge case training general proved useful best results improved ppl relative rouge measures points compared nhg model pre training current work focused maximally ing available headlined corpora ing future direction additionally lize potentially abundant corpora documents headlines proposed shen et al pre training open question relationship dataset size effect pre training acknowledgments like thank nvidia donated gpu anonymous reviewers valuable comments kyunghyun cho help cnn daily mail dataset references alex alimoff abstractive sentence rization attentive deep recurrent neural works dzmitry bahdanau kyunghyun cho yoshua neural machine translation bengio jointly learning align translate bastien pascal lamblin razvan pascanu james bergstra ian j goodfellow arnaud eron nicolas bouchard yoshua bengio theano new features speed improvements deep learning unsupervised feature learning nips workshop samy bengio oriol vinyals navdeep jaitly noam shazeer scheduled sampling quence prediction recurrent neural networks advances neural information processing tems pages james bergstra olivier breuleux bastien pascal lamblin razvan pascanu guillaume jardins joseph turian david warde farley cpu yoshua bengio proceedings gpu math expression compiler python scientic computing conference scipy oral presentation theano felipe bravo marquez manuel manriquez zipf like distant supervision approach document summarization wikinews articles international symposium string processing information retrieval pages springer association computational tics karl moritz hermann tomas kocisky ward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend vances neural information processing systems nips diederik kingma jimmy ba adam method stochastic optimization arxiv preprint dietrich klakow jochen peters testing correlation word error rate perplexity speech communication chin yew text summarization branches chapter rouge package automatic evaluation summaries lin ingrid mardh headlinese mar english page headlines volume liberlaromedel gleerup mike mintz steven bills jurafsky rion daniel snow distant supervision relation extraction labeled data proceedings joint conference annual meeting acl national joint conference natural language processing afnlp pages association computational linguistics kyunghyun cho bart van merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase representations rnn encoder decoder statistical machine translation proceedings conference ical methods natural language processing emnlp pages association computational linguistics c robert moore william lewis intelligent selection language model training data proceedings acl conference short papers pages association tional linguistics ramesh nallapati bowen zhou cicero dos tos caglar gulcehre bing xiang abstractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning pages association computational linguistics m chopra michael alexander auli sumit rush abstractive sentence summarization attentive recurrent neural networks proceedings conference north american chapter association tional linguistics human language technologies pages association computational linguistics siim orasmaa xavier glorot yoshua bengio ing difculty training deep feedforward neural networks international conference articial intelligence statistics pages caglar gulcehre sungjin ahn ramesh ati bowen zhou yoshua bengio pointing unknown words proceedings annual meeting association tational linguistics volume long papers pages timo petmanson alexander tkachenko sven laur heiki jaan kaalep estnltk nlp toolkit estonian tenth international conference ceedings language resources evaluation lrec paris france european language resources association elra razvan pascanu tomas mikolov yoshua bengio difculty training recurrent ral networks proceedings international conference machine learning icml romain paulus caiming xiong richard socher deep reinforced model abstractive marization arxiv preprint anthony rousseau xenc open source tool data selection natural language processing prague bulletin mathematical linguistics m alexander rush sumit jason weston neural attention model abstractive sentence summarization proceedings conference ical methods natural language processing pages association computational linguistics chopra mike schuster kuldip k paliwal tional recurrent neural networks ieee transactions signal processing shiqi shen yu zhao zhiyuan liu maosong neural headline generation arxiv preprint sun et al sentence wise optimization tian wang kyunghyun cho larger context language modelling recurrent neural network proceedings annual meeting association computational linguistics volume long papers pages association computational linguistics
