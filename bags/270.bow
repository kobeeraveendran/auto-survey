felix flexible text editing tagging insertion jonathan mallinson university edinburgh j ac aliaksei severyn google research eric malmi google research severyn emalmi com guillermo garrido google research r m l c s c v v x r abstract present felix exible text editing approach generation designed derive maximum benet ideas coding bi directional contexts supervised pre training contrast tional sequence sequence models felix efcient low resource settings fast inference time capable modeling exible input output tions achieve decomposing text editing task sub tasks tagging decide subset input tokens order output text insertion missing tokens output present input tagging model employs novel pointer mechanism insertion model based masked language model models chosen autoregressive guarantee faster inference felix performs favourably compared recent text editing methods strong baselines evaluated nlg tasks sentence fusion machine translation matic post editing summarization text simplication introduction idea text coupled self supervised pre training deep transformer networks large text corpora dramatically changed landscape natural language derstanding bert devlin et al successive renements roberta liu et al albert lan et al implement recipe signicantly pushed state art multiple nlu benchmarks glue wang et al squad rajpurkar et al recently idea masked lling style objective model pretraining equal contribution work completed internship google research figure felix transforms source big loud cat target text big old cat applied sequence sequence tasks signicantly pushed state art number text generation tasks g mit chan et al mass song et al rothe et al bart lewis et al raffel et al sequence sequence frameworks offer generic tool modeling kind text text transduction world tasks generating target texts pletely scratch approaches unnecessary especially true monolingual settings input texts relatively high degrees overlap cases natural approach cast task conditional text generation text editing task model learns reconstruct target texts applying set edit operations inputs typically set edit operations xed dened ahead time hand limits exibility model reconstruct arbitrary output texts inputs leads taggingkeep keepins del keepthe big repl loud mask cattaggerthe big loud catpointerinsertionmasked language modelthe big repl loud old cat higher sample efciency limited set lowed operations signicantly reduces search space based observation text editing proaches recently gained signicant est gu et al dong et al awasthi et al malmi et al paper present novel text editing framework felix heavily inspired ideas bi directional decoding slot self supervised pre training particular designed felix following requirements mind sample efciency training high precision text generation model typically requires large amounts high quality supervised data supervised techniques based text shown provide crucial advantage resource settings focus approaches able benet existing pre trained language models bert nal model directly ne tuned stream task fast inference time achieving low latencies serving text generation models typically quires specialized hardware nding trade model size accuracy jor reasons slow inference times text generation models typically employ sive decoder e output texts generated sequential non parallel fashion ensure faster inference times opt keeping felix fully autoregressive known autoregressive decoding leads higher accuracy scores fast inference priority features felix flexible text editing simplifying learning task text editing models erful general purpose sequence sequence proaches comes modeling arbitrary output text transductions strive strike balance complexity learned edit operations percentage input output formations model capture ordering target words present source represented generic slot predictions insertion model benet self supervised pre training chose insertion model fully compatible bert tecture easily use publicly available pre trained checkpoints decomposing text editing tasks way redistribute complexity load generating output text models source text provides building blocks required reconstruct target dled tagging model missing pieces insertion model job easier output text place step proach key able use completely non autoregressive decoding models achieve competitive results compared fully autoregressive approaches evaluate felix distinct text eration tasks sentence fusion text tion summarization automatic post editing machine translation compare recent text editing approaches task unique editing operations required training data available helps better quantify value solutions integrated model description felix decomposes conditional probability generating output sequence y input follows terms correspond tagging insertion model term ym denotes intermediate sequence masked spans ym fed insertion model constructed yt sequence tags assigned input token permutation reorders input tokens given factorization models trained independently felix meet aforementioned tum propose tackle text editing ing sub problems tagging insertion fig tagger transformer based network implements novel pointing nism vinyals et al decides source tokens preserve order appear output allowing arbitrary word tagging tag sequence yt constructed follows source tokens copied assigned tag tokens present output marked delete tag token spans present output missing input modeled code publicly available url added src ym pred ym pred cat big del loud del delins repl big loud mask mask noisy large del delins mask large repl big loud mask noisy cat cat del mask inll mask mask cat pad pad cat figure example ways model inputs insertion model token masking mask inlling inll case tagging model predicts number masked tokens ins delegated insertion model replaces generic ins tag xed length span length note insertion model predicts special pad symbol mark end predicted span replacements modeled keeping deleted spans repl tags transforms source text big loud cat target noisy large cat note simplicity example include reordering root big loud cat figure pointing mechanism transform big loud cat big cat insert ins tag converted masked token spans insertion model word reordering handled specialized pointing mechanism pointing felix explicitly models word reordering allow larger global edits smaller local changes swapping nearby words john mary mary john word ordering step vanilla editing model based tagging malmi et al dong et al rst need delete span mary insert mary john felix able model need deletions insertions given sequence predicted tags ordering model generates permutation yt reconstruct tion model input ym highlight predicted independently non regressive fashion output model ries predicted pointers source token token ym easily constructed daisy chaining pointers seen fig highlighted gure felix s reordering process similar non projective dependency parsing dozat manning head relationships non autoregressively predicted form tree similarly felix predicts word relationship instead forms sequence implementation based pointer work vinyals et al attention mechanism points token unlike vious approaches decoder state attends encoder sequence setup applies attention source tokens attend source tokens constructing training data possible combinations produce ym trivially source tokens deleted target tokens inserted construct dataset greedy method maximize number kept tokens minimize number inserted token minimize reordering keeping source tokens continuous sequences possible token point token loops formed token pointed multiple times constructing dataset ensure token pointed inference time constrained beam search ensure loops created insertion input insertion model ym contains set input tokens order determined tagging model masked token spans needs represent masked token spans consider options masking inlling fig case tagging model predicts tokens need inserted specializing insert tag ins k k translates span k mask tokens inlling case tagging model predicts generic ins tag signals insertion model inll span tokens arbitrary length use autoregressive tion model natural way model run decoder decides stop producing special stop symbol e eos design opted non autoregressive model represent variable length insertions use pad symbol pad insertions xed length quence mask tokens note preserve deleted span input insertion model enclosing tags introduces undesired discrepancy pretraining ne tuning data insertion model observes found making model aware text needs replace signicantly boosts accuracy insertion model felix insertion transformer intuitive way picture felix works draw connection insertion stern et al decoder starts blank output text canvas atively inlls deciding token position appear output tiple tokens inserted time ing sub linear decoding times contrast felix trains separate tagger model output canvas input tokens single step second nal step felix insertion slots predicted tagger equivalent single decoding step insertion felix requires signicantly fewer decoding steps insertion tagging insertion position task straightforward directly advantage existing pre trained masked guage models experiments maximum lengths sufcient represent insertion spans training set text edit tasks reported paper sponds output tokens model implementation tagging model tagger tagger layer bert base model tags predicted applying single feed forward layer output encoder hl t argmaxf hl pointer input pointer layer position combination encoder hidden state hl embedding predicted tag positional embedding follows hl token prediction uses pointer network attending hidden states attention hidden states calculated ing query key network scaled dot product k softmax qkt dk k q linear projections dk hidden dimension found optional inclusion additional transformer layer prior query projection increased performance movement heavy datasets realizing pointers use strained beam search ensure loops created note loops form insertion model similar tagger insertion model based layer bert base initialized public pretrained checkpoint masking approach insertion model essentially solving masked language modeling task directly vantage bert style pretrained checkpoints considerable advantage especially low resource settings waste training data learning language model component text editing model task tion tagging insertion trained et al shown models trained masked language modeling objectives lose positional tion property consider important reordering x beam size batch size mum sequence length beam search incurs additional penalty run xeon disjointly essentially comes switching masking approach inlling shifts complexity modeling length inserted token spans tagging model insertion model depending training data available provides interesting trade offs accuracy tagging insertion models explore detail sec experiments evaluate felix distinct text editing tasks sentence fusion text simplication marization automatic post editing chine translation addition reporting ously published results task pare recent text editing approach ger malmi et al follow setup set phrase vocabulary size run experiments accurate gressive model tasks run ablation study examining effect open vocabulary reordering felixinsert xed reordering model felixpoint task analysis chosen tasks cover diverse set edit operations wide range dataset sizes varying data points million table provides dataset statistics including size sentence length tion error rate ter snover et al source target sentences use ter highlight unique properties task marization dataset deletion heavy dataset highest number deletion edits largest reduction sentence length contains moderate amounts substitutions large number shift edits caused sentence ordering simplication post editing datasets contain large number insertions substitutions simplication contains greater number tion edits post editing larger dataset covering multiple languages sentence sion lowest ter indicating obtaining fused targets requires limited number local edits edits require modeling ne tune insertion model accommodate additional token spans repl learns condition prediction masked tokens spans simplicity use lasertagger phrase lary discourse relation input tences common edit type predicting correct discourse connective geva et al additionally provide coverage statistics percentage training instances editing model fully reconstruct output input proposed model table trasting lasertagger felix felixinsert use open vocabulary cover test data felixpoint lasertagger cover half dataset felixpoint covers signicantly higher percentage lasertagger noticeable case summarization increase coverage explained high number shift edits tion table felixpoint explicitly designed model found difference coverage felixpoint ger correlates strongly correlation number shift edits comparing erage percentage masks inserted felix inserts masks felixinsert word reordering requires deletions insertions sentence fusion sentence fusion problem fusing dent sentences coherent output data use balanced wikipedia portion discofuse dataset geva et al study effect training data size ing increasingly smaller subsets discofuse data points metrics following geva et al report metrics exact score percentage exactly correctly predicted fusions sari xu et al computes average scores added kept deleted n grams results table includes additional bert based baselines rothe et al malmi et al felix variants break scores based insertion modelled token masking mask inlling inll ditionally better understand contribution tagging insertion models nal accuracy report scores assuming oracle insertion ging predictions respectively highlighted rows dataset size lsrc ltgt ter ins del sub shft post editing simplication summarization sentence fusion m k k m table statistics tasks size dataset size source length tokens lsrc target length tokens ltgt ter score snover et al components including number insertions ins deletions del substitutions sub shifts shft dataset coverage mask lasertagger felixpoint felixinsert felix postediting simplication summarization sentence fusion table coverage mask statistics coverage percentage training examples models able generate felixinsert felix coverage test sets mask ratio masked tokens target tokens results felix variants nicantly outperform baselines lasertagger data conditions der condition achieves highest sari exact score data conditions felix outperforms results highlights models form poorly datapoints editing models achieve relatively good performance comparing felix variants case felixinsert outperforms felix note felixinsert lowed malmi et al additional sentence ordering tag hand crafted feature lored discofuse swaps sentence order included malmi et al resulted signicant exact increase low resource setting felix outperforms lixinsert suggesting felix data efcient felixinsert ablation rst contrast impact sertion model tagging model noticing models inll achieves better tagging scores worse insertion scores mask secondly felix achieves worse tagging scores better insertion scores felixinsert lights pressure model ing making tagging task harder inclusion reordering insertion task comes easier finally insertion models low data conditions achieve impressive performance suggests low data conditions pressure applied insertion model simplication sentence simplication problem fying sentences easier stand simplication lexical replacing deleting complex words syntactic replacing complex syntactic constructions data training performed wikilarge zhang lapata large tion corpus consists mixture wikipedia simplication datasets collected kauchak woodsend lapata zhu et al test set created xu et al consists source sentences taken wikipedia simplied amazon mechanical turkers create references source sentence metrics report sari breaking component delete add found scores uneven metrics include readability metrics fkgl percentage unchanged source sentences copy results table compare state art smt based simplication systems pbmt r wubben et al phrase based machine translation model hybrid narayan gardent model performs tence spiting deletions simplies pbmt r sbmt sari xu et al syntax based translation model trained ppdb tuned sari neural approaches dress zhang model insertion mask inll oracle tag ins sari exact lasertagger felixpoint felixinsert felix table sentence fusion results discofuse subsets training set report model variants felixpoint felixinsert felix mask inll insertion modes rows gray background report scores assuming oracle tagging tag insertion ins predictions wikilarge sari add del fkgl copy summarization sbmt sari pbmt r hybrid nts dress dress ls editnts lasertagger felixpoint felixinsert felix table sentence simplication results wikilarge ata lstm based trained reinforcement learning dress ls variant dress additional lexical tion component nts nisioi et al model dmass zhao et al transformer based model enhanced cation rules ppdb neural editing models lasertagger editnts felix achieves highest overall sari score highest sari score addition ablated models achieve higher sari scores lasertagger felixinsert achieves higher sari score editnts felixpoint explained large number substitutions insertions dataset felixpoint achieving low add score data use dataset toutanova et al contains short input texts sentences human written summaries resulting total training pairs human experts restricted ing words generating summary allowed insert new words reorder parts sentence makes dataset larly suited abstractive summarization models metrics addition sari include rouge l metrics monly summarization literature results results table felix achieves highest sari rouge bleu score ablated models achieve higher sari scores models interestingly ference felixpoint lasertagger modest felixpoint covers twice data lasertagger ger trained data points point trained table lasertagger felixpoint perform larly low data conditions post editing automatic post editing ape task matically correcting common repetitive errors found machine translation mt outputs sari add del rouge bleu lasertagger felixpoint felixinsert felix table summarization copy included models copied time copy transformer lasertagger levt sota lee et al felixpoint felixinsert felix ter bleu data ape approaches trained triples source sentence machine translation output target translation experiment en de post editing goal improve output mt system translates english german applied documents domain follow procedures introduced junczys dowmunt grundkiewicz train models synthetic corpora m k examples merged corpus k real examples sampled times models study pect single input string obtain models possibility attend english source text append source text man translation separated special token model input consists different languages use multilingual bert proposed methods lasertagger metrics follow evaluation procedure ape task report translation error rate ter snover et al primary metric bleu secondary metric results consider following baselines copy competitive baseline given required edits typically limited lasertagger malmi et al shtein transformer levt gu et al partially autoregressive model employs deletion insertion mechanisms standard transformer evaluated gu et al state art method lee et al unlike methods baseline tailored specically ape task ing source separately conditioning mt output encoding source encoding lee et al org ape task html googleapis com bert zip table ende post editing results results shown table custom method lee et al brings signicant improvements generic text transduction methods second felix performs competitively yielding comparative results levenshtein transformer gu et al partially autoregressive model performing generic models terms ter felixinsert performs considerably worse felix felixpoint suggesting pointing mechanism important ape task observation backed ble shows pointing anism average proportion masked tokens target pointing removing pointing nism shifts responsibility heavily tagging model insertion model related work models sutskever et al applied text generation tasks cast monolingual translation fer known drawbacks wiseman et al require large amounts training data outputs difcult control input output sequences large lap reasonable cast problem text editing task sequence sequence generation ribeiro et al argued general problem string transduction reduced sequence labeling approach applied character deletion insertion based simple patterns lasertagger malmi et al general approach shown perform number text editing tasks limitations allow arbitrary reordering input tokens sertions restricted xed phrase vocabulary derived training data similarly itnts dong et al pie awasthi et al text editing models predict tokens delete add oped specically tasks text simplication grammatical error correction respectively contrast aforementioned models felix lows exible rewriting pointer work points source decide tokens preserved output order pointer networks previously proposed way copy parts input hybrid sequence sequence models gulcehre et al nallapati et al trained pointer network specically deal vocabulary words named entities et al hybrid proach learns use pointer copy parts input chen bansal proposed summarization model rst selects salient tences rewrites abstractively pointer mechanism directly copy vocabulary words methods typically require large amounts training data inherently slow inference time sive decoding previous approaches proposed alternatives autoregressive decoding gu et al lee et al chan et al wang cho instead left right autoregressive decoding insertion transformer stern et al blm shen et al generate output quence insertion operations enshtein transformer levt gu et al ditionally incorporates deletion operation methods produce output iteratively felix requires steps tagging insertion differences proposed model felix ablated variants selection lated works summarized table conclusions future work introduced felix novel approach text editing decomposing task tagging insertion trained independently separation allows maximal benet existing pretrained masked lm models felix works extremely low resource tings fully non autoregressive favors faster inference empirical results demonstrate delivers highly competitive performance type non gressive pretrained reordering open vocab text edit transformer copying levt pie editnts lasertagger felixinsert felixpoint felix table model comparison ve dimensions model type decoder non autoregressive levt partially autoregressive model uses pretrained checkpoint word reordering anism uses reordering pretraining task dedicated copying mechanism ing reordering model generate possible output open vocab compared strong baselines recent text editing approaches future work plan investigate following ideas effectively share senations tagging insertion models single shared encoder perform joint training insertion tagging models stead training separately iii strategies unsupervised pre training tagging model appears bottleneck highly resource settings iv distillations recipes acknowledgments thank aleksandr chuklin daniil mirylenka ryan mcdonald sebastian krause useful discussions running early experiments paper suggestions references abhijeet awasthi sunita sarawagi rasna goyal sabyasachi ghosh vihari piratla allel iterative edit models local sequence duction proceedings conference empirical methods natural language processing international joint conference ral language processing emnlp ijcnlp pages william chan nikita kitaev kelvin guu mitchell stern jakob uszkoreit kermit ative insertion based modeling sequences yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proceedings annual ing association computational linguistics volume long papers pages jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language ing arxiv preprint yue dong zichao li mehdi rezagholizadeh jackie chi kit cheung editnts ral programmer interpreter model sentence plication explicit editing arxiv preprint timothy dozat christopher d manning deep biafne attention neural dependency international conference learning ing representations iclr toulon france april conference track proceedings review net mor geva eric malmi idan szpektor jonathan berant discofuse large scale dataset discourse based sentence fusion arxiv preprint jiatao gu james bradbury caiming xiong tor o k li richard socher autoregressive neural machine translation national conference learning representations jiatao gu changhan wang junbo zhao h wallach levenshtein transformer h larochelle beygelzimer f buc e fox r garnett editors advances neural information processing systems pages curran associates inc caglar gulcehre sungjin ahn ramesh nallapati bowen zhou yoshua bengio annual meeting ing unknown words association computational linguistics acl pages association tational linguistics acl marcin junczys dowmunt roman grundkiewicz log linear combinations monolingual bilingual neural machine translation models proceedings tomatic post editing conference machine translation pages berlin germany association computational linguistics david kauchak improving text simplication language modeling unsimplied text data proceedings annual meeting ation computational linguistics volume long papers pages zhenzhong lan mingda chen sebastian goodman kevin gimpel piyush sharma radu soricut albert lite bert self supervised ing language representations jason lee elman mansimov kyunghyun cho deterministic non autoregressive neural quence modeling iterative renement ceedings conference empirical ods natural language processing pages wonkee lee junsu park byung hyun jong hyeok lee transformer based matic post editing context aware encoding approach multi source inputs arxiv preprint mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov luke zettlemoyer bart denoising sequence sequence pre training natural language generation translation comprehension yinhan liu myle ott naman goyal jingfei du dar joshi danqi chen omer levy mike lewis luke zettlemoyer veselin stoyanov roberta robustly optimized bert pretraining proach eric malmi sebastian krause sascha rothe daniil mirylenka aliaksei severyn encode tag realize high precision text editing ings conference empirical methods natural language processing national joint conference natural language cessing emnlp ijcnlp pages ramesh nallapati bowen zhou cicero dos santos caglar gulcehre bing xiang tive text summarization sequence sequence proceedings rnns signll conference computational natural guage learning pages shashi narayan claire gardent hybrid plication deep semantics machine lation sergiu nisioi sanja stajner simone paolo ponzetto liviu p dinu exploring neural text proceedings plication models nual meeting association computational linguistics volume short papers pages colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li peter j liu exploring limits transfer learning unied text text pranav rajpurkar jian zhang konstantin lopyrev percy liang squad questions machine comprehension text joana ribeiro shashi narayan shay b cohen xavier carreras local string transduction sequence labeling proceedings national conference computational linguistics pages kristian woodsend mirella lapata ing simplify sentences quasi synchronous grammar integer programming proceedings conference empirical methods natural language processing pages association computational linguistics sander wubben antal van den bosch emiel mer sentence simplication monolingual machine translation proceedings nual meeting association computational linguistics volume long papers pages jeju island korea association tional linguistics wei xu courtney napoles ellie pavlick quanze chen chris callison burch optimizing statistical machine translation text simplication transactions association computational linguistics xingxing zhang mirella lapata tence simplication deep reinforcement ing arxiv preprint xingxing zhang mirella lapata tence simplication deep reinforcement ing proceedings conference pirical methods natural language processing pages association computational guistics sanqiang zhao rui meng daqing saptono andi parmanto bambang integrating paraphrase rules sentence tion arxiv preprint zhemin zhu delphine bernhard iryna gurevych monolingual tree based translation model proceedings sentence simplication international conference computational guistics pages association tational linguistics sascha rothe shashi narayan aliaksei leveraging pre trained checkpoints arxiv preprint eryn sequence generation tasks abigail peter j liu christopher d manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers pages tianxiao shen victor quach regina barzilay tommi jaakkola blank language models matthew snover bonnie dorr richard schwartz nea micciulla john makhoul study translation edit rate targeted human annotation proceedings association machine tion americas volume kaitao song xu tan tao qin jianfeng lu yan liu mass masked sequence sequence pre training language generation mitchell stern william chan jamie kiros jakob uszkoreit insertion transformer flexible quence generation insertion operations arxiv preprint sutskever o vinyals qv le sequence sequence learning neural networks advances nips kristina toutanova chris brockett ke m tran saleema amershi dataset evaluation metrics abstractive compression sentences short paragraphs oriol vinyals meire fortunato navdeep jaitly pointer networks advances neural formation processing systems pages elena voita rico sennrich ivan titov evolution representations transformer study machine translation language modeling objectives arxiv preprint alex wang kyunghyun cho bert mouth speak bert markov arxiv preprint random field language model alex wang amanpreet singh julian michael felix hill omer levy samuel r bowman glue multi task benchmark analysis platform natural language understanding sam wiseman stuart shieber alexander rush learning neural templates text generation proceedings conference cal methods natural language processing pages
