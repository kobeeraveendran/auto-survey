neural text generation practical guide ziang xie department computer science stanford university stanford edu november recent revision stanford textgen pdf abstract deep learning methods recently achieved great empirical success machine tion dialogue response generation summarization text generation tasks high level technique train end end neural network models consisting encoder model produce hidden representation source text followed decoder model generate target models signicantly fewer pieces earlier systems signicant tuning required achieve good performance text generation models particular decoder behave undesired ways generating truncated tive outputs outputting bland generic responses cases producing ungrammatical gibberish paper intended practical guide resolving undesired behavior text generation models aim helping enable real world applications v o n l c s c v v x r language makes innite use nite means language wilhelm von humboldt noodynaady s actual ingrate tootle come garner mauve thy nice stores morning buy bunch iodines finnegans wake james joyce focus guide limitations covered setting encoder decoder models training overview decoding overview attention evaluation contents introduction background preprocessing training decoding diagnostics common issues rare vocabulary oov words decoded output short truncated ignore portions input decoded output repeats lack diversity deployment conclusion acknowledgements introduction neural networks recently attained state art results tasks machine learning including natural language processing tasks sentiment understanding machine lation nlp number core tasks involve generating text conditioned input information prior years predominant techniques text generation based template rule based systems understood probabilistic models n gram log linear models chen goodman koehn et al rule based statistical models despite fairly interpretable behaved require infeasible amounts hand engineering scale case rule template based models tend saturate performance increasing training data jozefowicz et al hand neural work models text despite sweeping empirical success poorly understood poorly behaved figure illustrates trade os types systems figure figure illustrating tradeos rule based vs neural text generation systems help adoption usage neural text generation systems detail practical suggestions developing ntg systems include brief overview training decoding procedures suggestions training ntg models primary focus advice diagnosing resolving pathological behavior decoding long time retrain models comparatively cheap tune decoding procedure s worth understanding quickly deciding retrain figure illustrates feedback loops improving dierent components model training decoding procedures despite growing body research information best practices tends scattered depends specic model architectures starting hyperparameters suggested advice guide intended architecture agnostic possible error analysis emphasized instead helpful rst read background section remaining sections read independently focus guide guide focuses advice training decoding neural encoder decoder models attention mechanism text generation tasks roughly speaking source target assumed order dozens tokens primary focus guide decoding templaterule basedhybridcombinationneuralend endflexibilityexpressivitycontrollabilitypredictability figure development cycle ntg systems procedure suggestions improving model training decoding algorithms touch briey preprocessing section deployment section limitations covered continuing describe guide cover current tions neural text generation models guide consider natural language understanding semantics impressive work learning word embeddings mikolov et al pennington et al goal learning thought vectors sentences remained elusive kiros et al previously mentioned consider sequence labeling classication tasks capture long term dependencies brief discussion attention maintain global coherence remains challenge curse dimensionality neural networks failing learn abstract concepts predominant step prediction training objective interface models knowledge base structured data supplied short piece text recent work pointer mechanisms end vinyals et al consequently focus natural language precise guide cover natural language generation nlg entails generating documents longer descriptions structured data primary focus tasks target single sentence term text generation opposed language generation eld evolving quickly tasks older rule based systems reasonable option consider example seminal work eliza baum computer program intended emulate psychotherapist based tern matching rules generating responses general neural based systems unable perform dialogue state management required systems consider task ating summary large collection documents soft attention mechanisms neural systems currently direct way condition text trainingruntime daystoweeksinitializationoptimizationregularizationrapidlyevolvingarchitecturesdecodingruntime minutestohoursscoringfunctionlanguagemodelspeedrelativelyunchangingprocedureiterativeimprovement background summary notation symbol shape description v x y e d h scalar s t s h t h t s varies scalar size output vocabulary input source sequence xs length s output target sequence yt length t encoder hidden states ej denotes representation timestep j decoder hidden states di denotes representation timestep attention matrix aij attention weight ith decoder timestep jth encoder state representation hypothesis hypothesis set score hypothesis h beam search decoding h minibatch size dimension omitted shape column setting consider modeling discrete sequences text tokens given sequence u vocabulary v seek model u t denotes equality follows chain rule probability depending choose tokenize text vocabulary contain set characters pieces byte pairs words unit tasks consider paper divide sequence u input source sequence x provided output target sequence y example machine translation tasks x sentence english y translated sentence chinese case model u t s x x y t t note generalization consider x machine translation encompasses tasks natural language processing table summary tasks described rst half table techniques described paper extend tasks intersection text modalities example speech recognition x sequence features computed short snippets audio y corresponding text transcript image captioning x image straightforward express sequence y corresponding text description include sequence labeling example speech tagging task instead task x example language modeling machine translation grammar correction summarization dialogue sequence source sequence english noisy ungrammatical sentence body news article conversation history y example tokens news corpus target sequence french corrected sentence headline article response turn related tasks outside scope guide speech transcription image captioning question answering audio speech features image supporting text knowledge base question text transcript caption describing image answer table example tasks consider figure figure illustrating generic encoder decoder model architecture assume guide choices possible encoder decoder architectures attention mechanism outputs single timestep consider tasks clear correspondence source target lack correspondence leads issues decoding focus section reasoning applies sequence classication tasks sentiment analysis encoder decoder models encoder decoder models referred sequence sequence models developed chine translation rapidly exceeded performance prior systems depite having paratively simple architectures trained end end map source directly target neural network based approaches count based methods chen goodman methods involving learning phrase pair probabilities language modeling translation prior recent encoder decoder models feed forward fully connected neural networks shown work language modeling models simply stack ane matrix transforms followed nonlinearities input following hidden layer bengio et al networks fallen favor modeling sequence data require dening xed x y t use parameter sharing timesteps context length modeling decodestepencoderxattentiondecodery y surpassed performance subsequent architectures time writing dierent architectures demonstrated strong results recurrent neural networks rnns use shared parameter matrices dierent time steps combine input current time step previous hidden state ing previous time steps mikolov et al sutskever et al cho et al dierent gating mechanisms developed architectures try ease optimization hochreiter schmidhuber cho et al convolutional neural networks cnns convolutions kernels reused timesteps masking avoid peeking ahead future inputs training section overview training procedure kalchbrenner et al convolutions benet training parallelizing time dimension instead computing hidden state step time recurrent convolutional networks modeling sequences typically rely timestep attention mechanism bahdanau et al acts shortcut connection target output prediction relevant source input hidden states level decoder timestep decoder representation di compute weight ij encoder representation ej example dot product ej logits applying softmax function ij ek s weighted representation ijej fed decoder x recent models rely purely attention mechanisms masking shown obtain good better results rnn cnn based models vaswani et al describe attention mechanism detail section indicated advice guide intended agnostic model tecture long following conditions hold model performs step prediction target conditioned source previous predicted targets e models x y t model uses attention mechanism resulting attention matrix eases training simple implement cheap compute cases standard component encoder decoder models figure illustrates backbone architecture use guide recent architectures use self attention mechanism decoder outputs conditioned previous decoder hidden states simplicity discuss extension training overview training optimize model parameters sequence cross entropy loss t log x y t maximizing log likelihood training data previous ground truth inputs given model predicting index sequence training method referred unfortunately teacher forcing inability t current datasets memory faster convergence gradient updates computed minibatches training sentences stochastic gradient descent sgd optimizers adam kingma ba shown work empirically recent reserarch explored methods training sequence models reinforcement learning separate adversarial loss goodfellow et al li et al bahdanau et al arjovsky et al writing aforementioned training method primary workhorse training models decoding overview decoding given source sequence x seek generate target y maximizes scoring function s y greedy decoding simply argmax softmax output distribution timestep feed input timestep timestep single hypothesis greedy decoding work surprisingly note result probable output hypothesis path probable overall despite including output argmax holds true scoring functions choose s usually intractable consider possible y branching factor number timesteps instead perform beam search iteratively expand hypotheses token time end search iteration k best terms s hypotheses k beam width beam size s beam search procedure detail begin beam procedure start sequence token esis h token sos consisting single hypothesis h sos sos set list start repeat t tmax repeat h h x h repeat u vocabulary v probability add hypothesis hnew sos u h b compute cache example log probability hypothesis log simply computes cumulative scoring function x tensors input simplicity consider figure toy example beam search procedure beam width search run edges annotated probabilities steps hypothesis terminated tokens tokens pruning k hypotheses shown eos hypotheses end end sequence token hypothesis list terminated hypotheses nal k best remaining hypotheses according s h h finally return h arg maxhhfinal considering completed potheses wish use modied scoring function snal eos surprising result neural models relatively small beam sizes yield good results rapidly diminishing returns larger beam sizes yield slightly worse results example beam size work marginally better beam size beam size work worse koehn knowles finally oftentimes incorporating language model lm scoring function help improve performance lms need trained target corpus train language models larger corpuses parallel data objective decoding maximize joint probability arg max y arg max y p y y y given x y s intractable maximize instead maximize pseudo objective y practice original score assume involves x term lm augmented score store information hypothesis arg max p y y y log hsositime itis hyperparameter balance lm decoder scores despite issues simple procedure works fairly arise cases beam search language model result far optimal outputs inherent biases decoding training procedure describe diagnose tackle problems arise section attention figure expected attention matrix source target monotonically aligned sized illustrative example basic attention mechanism attend portions encoder hidden states decoder timestep extensions applications attention previous decoder hidden states called self attention vaswani et al components separate encoder decoder model instead encoder hidden states grave et al monitor training progress inspecting clear alignment develops encoder decoder hidden states inspect correspondences input output sequence network learns bit attention matrix typically follows correspondences input output useful discuss methods guiding decoding procedure section detail attention matrix attention matrix t columns s rows t number output timesteps s number input timesteps row ai discrete probability distribution encoder hidden states guide assume equal number input timesteps s case encoder column entry threshold suggests corresponding encoder input ignored decoder likewise j multiple values threshold suggests encoder hidden states repeatedly multiple decoder timesteps figure shows attention matrix expect trained network source target aligned e english french translation great overview visualizations attention rnn models olah carter evaluation key challenges developing text generation systems satisfying mated metric evaluating nal output system unlike classication sequence labeling tasks precisely measure output quality text generation systems barring human evaluation perplexity correlate downstream metrics chen et al automated common metrics based n gram overlap rouge lin bleu papineni et al rough approximations capture linguistic uency cohererence conroy dang liu et al metrics especially problematic open ended generation tasks summarization dialogue recent results shown automated metrics great distinguishing systems performance passes baseline nonetheless useful nding examples performance poor consistent evaluating similar systems novikova et al despite issues current automated evaluation metrics assume use model development manual human evaluation interspersed preprocessing increasingly advanced libraries building computation graphs performing automatic dierentiation signicant portion software development process devoted data preparation broadly speaking raw data collected remains cleaning tokenization splitting training test data important consideration cleaning setting character encoding example ascii libraries python s unidecode save lot time cleaning comes easily specied tasks splitting text sentences tokenization present recommend stanford extensive options better handling sentence word boundaries available libraries alternative performing tokenization later detokenization avoid altogether instead working word level instead operate character level use intermediate subword units sennrich et al models result longer sequences overall empirically subword models tend provide good trade o sequence length speed handling rare words wu et al section discusses benets subword models detail ultimately word tokens s important use consistent tokenization scheme inputs system includes handling contractions punctuation marks quotes hyphens periods denoting abbreviations nonbreaking prexes vs sentence boundaries character escaping multilingual preprocessing welcome com stanfordnlp corenlp stanford tokenizer page stanford edu software tokenizer html detailed list options training heuristics sucient handling issues training models start getting model overt tiny subset data quick sanity check loss explodes reducing learning rate nt model overts apply dropout srivastava et al zaremba et al weight decay nt gradient clipping crucial avoid exploding gradient problem reasonably large learning rate sgd variants periodically annealing learning rate validation loss fails decrease typically helps signicantly useful heuristics robust hyperparameter settings optimization settings use sort dozen batches sentences length batch examples roughly length saving computation sutskever et al training set small tuning regularization key performance melis et al noising token dropout worth trying et al touch issue briey training data cases primary bottleneck performance ntg model measure validation loss epoch anneal learning rate validation loss stops decreasing depending validation loss uctates based o validation set size optimizer settings wish anneal patience wait epochs non decreasing learning rate reducing learning rate periodically checkpoint model parameters measure downstream performance bleu model checkpoints validation cross entropy loss nal performance correlate signicant dierences nal performance checkpoints similar validation losses ensembling improves performance averaging checkpoints cheap way approximate ensembling eect huang et al survey model parameters consider suggested settings hyperparameters britz et al melis et al decoding suppose ve trained neural network encoder decoder model achieves reasonable perplexity validation set try running decoding generation model simplest way run greedy decoding described section beam search decoding yield additional performance improvements s rare things simply work section intended use quick reference encountering common issues decoding examples purely illustrative excerpts alice s adventures wonderland carroll diagnostics manual inspection s helpful create diagnostic metrics debugging dierent components text generation system despite training encoder decoder network map source target decoding procedure introduce additional components scoring function tells good hypothesis h beam optionally language model trained large corpus similar target corpus clear components prioritize trying improve performance combined system helpful run ablative analysis ng language model suggestions measuring performance reasonably spaced values plotting performance trend measuring perplexity language model trained varying amounts training data data helpful yields diminishing returns measuring performance training language model dierent domains news data wikipedia cases s dicult obtain data close target domain measuring scoring function computing metrics inspecting decoded outputs vs gold sentences immediately yields insights useful metrics include average length decoded outputs y vs average length reference targets y s y vs inspecting ratio s y average ratio especially low bug beam search beam size need increased average ratio high scoring function appropriate applications computing edit distance insertions substitutions deletions y y useful example looking frequent edits examining cases length normalized distances highest common issues rare vocabulary oov words decoded thought stood expected ush thought stood jabberwock eyes ame eos eyes ame unk unk eos languages large vocabularies especially languages rich morphologies rare words problematic choosing tokenization scheme results token labels feasible model output softmax ad approach rst deal issue simply truncate softmax output size k assign remaining class luong et al box illustrates resulting output token labels detokenization rare words replaced s elegant approach use character subword preprocessing sennrich et al wu et al avoid oovs entirely slow runtime training decoding unk unk figure example attention matrix decoding terminates early having covered input x eos token decoded output short truncated ignore portions input decoded s use going yesterday expected s use going yesterday dierent person eos token decoder decoding search procedure hypotheses terminate token target fully network learn place low probability suciently low probability erated length hypothesis grows total log probability decreases normalize log probability length hypothesis shorter hypotheses favored box illustrates example hypothesis terminates early issue exacerbated incorporating language model term simple ways resolving issue normalizing log probability score adding length bonus eos eos eos length normalization replace score s y score normalized hypothesis length s y t length bonus replace score s y s y t hyperparameter note normalizing total log probability length equivalent maximizing t th root probability adding length bonus equivalent multiplying probability timestep baseline e method avoiding issue coverage penalty attention matrix tu et al wu et al formulated coverage penalty applied hypothesis corresponding attention matrix terminated figure example attention matrix decoding exhibits repeating behavior incorporated snal attention matrix shape t s coverage penalty computed perform nal ranking hypotheses given hypothesis h log min aij s t intuitively source timestep attention matrix places probability source timestep aggregated decoding timesteps coverage penalty zero penalty incurred attending source timestep finally source target expected roughly equal lengths trick simply constrain target length t delta source length s e s case s small hyperparameters s include s s t decoded output repeats decoded m expected m eos repeating outputs common issue expose neural versus template based systems simple measures include adding penalty model reattends previous timesteps attention shifted away easily detected attention matrix manually selected threshold finally fundamental issue consider repeating outputs poor training model parameters passing attention vector decoder input predicting yi training time method et al lack diversity nt know li et al dialogue qa common responses dierent conversation turns generic responses nt know common problem similarly problems possible source inputs map smaller set possible target outputs diversity outputs issue increasing temperature softmax j simple method trying encourage diversity decoded outputs practice method penalizing low ranked siblings step beam search decoding procedure shown work li et al sophisticated method maximize mutual information source target signicantly dicult implement requires generating n best lists li et al deployment speed decoding huge concern trying achieve state art results concern deploying models production real time decoding requirement gains highly parallelized hardware gpus libraries optimized matrix vector operations discuss techniques improving runtime decoding consider factors determine runtime decoding algorithm beam search algorithms consider runtime scale linearly beam size practice batching hypotheses lead sublinear scaling runtime scale approximately quadratically hidden size network n nally linearly number timesteps t decoding complexity t jumbled collection possible methods speeding decoding include developing heuristics prune beam nding best trade o size vocabulary softmax decoder timesteps batching multiple examples caching previous computations case cnn models performing computation possible compiled computation graph conclusion describe techniques training dealing undesired behavior natural language eration models neural network decoders training models tends far consuming decoding worth making sure decoder fully debugged committing training additional models encoder decoder models evolving rapidly hope techniques useful diagnosing variety issues developing ntg system acknowledgements thank arun chaganty yingtao tian helpful discussions dan jurafsky helpful pointers references m arjovsky s chintala l bottou wasserstein gan arxiv preprint d bahdanau k cho y bengio neural machine translation jointly learning align translate arxiv preprint d bahdanau p brakel k xu goyal r lowe j pineau courville y bengio actor critic algorithm sequence prediction arxiv preprint y bengio r ducharme p vincent c jauvin neural probabilistic language model journal machine learning research d britz goldie t luong q le massive exploration neural machine translation architectures arxiv preprint l carroll alice s adventures wonderland url gutenberg org h htm s f chen j goodman empirical study smoothing techniques language modeling association computational linguistics acl s f chen d beeferman r rosenfeld evaluation metrics language models k cho b van merrienboer c gulcehre d bahdanau f bougares h schwenk y bengio learning phrase representations rnn encoder decoder statistical machine translation arxiv preprint j m conroy h t dang mind gap dangers divorcing evaluations summary content linguistic quality proceedings international conference computational linguistics volume goodfellow j pouget abadie m mirza b xu d warde farley s ozair courville y bengio generative adversarial nets advances neural information processing systems pages e grave joulin n usunier improving neural language models continuous cache arxiv preprint s hochreiter j schmidhuber long short term memory neural computation g huang y li g pleiss z liu j e hopcroft k q weinberger snapshot ensembles train m free arxiv preprint r jozefowicz o vinyals m schuster n shazeer y wu exploring limits language modeling arxiv preprint n kalchbrenner l espeholt k simonyan v d oord graves k kavukcuoglu neural machine translation linear time arxiv preprint d kingma j ba adam method stochastic optimization arxiv preprint r kiros y zhu r r salakhutdinov r zemel r urtasun torralba s fidler thought vectors advances neural information processing systems p koehn r knowles challenges neural machine translation arxiv preprint p koehn f j och d marcu statistical phrase based translation north american chapter association computational linguistics naacl j li m galley c brockett j gao b dolan diversity promoting objective function neural conversation models arxiv preprint j li w monroe d jurafsky simple fast diverse decoding algorithm neural generation arxiv preprint j li w monroe ritter m galley j gao d jurafsky deep reinforcement learning dialogue generation arxiv preprint j li w monroe t shi ritter d jurafsky adversarial learning neural dialogue generation arxiv preprint c lin rouge package automatic evaluation summaries text summarization branches proceedings workshop volume c liu r lowe v serban m noseworthy l charlin j pineau evaluate dialogue system empirical study unsupervised evaluation metrics dialogue response generation arxiv preprint m luong sutskever q v le o vinyals w zaremba addressing rare word problem neural machine translation arxiv preprint g melis c dyer p blunsom state art evaluation neural language models arxiv preprint t mikolov m karaat l burget j s khudanpur recurrent neural network based language model interspeech pages t mikolov sutskever k chen g s corrado j dean distributed representations words phrases compositionality advances neural information processing systems ng advice applying machine learning lecture notes j novikova o dusek c curry v rieser need new evaluation metrics nlg arxiv preprint c olah s carter attention augmented recurrent neural networks distill doi distill url augmented rnns k papineni s roukos t ward w zhu bleu method automatic evaluation machine translation proceedings annual meeting association computational linguistics pages association computational linguistics j pennington r socher c d manning glove global vectors word representation empirical methods natural language processing emnlp p j liu c d manning point summarization pointer generator networks arxiv preprint r sennrich b haddow birch neural machine translation rare words subword units arxiv preprint n srivastava g hinton krizhevsky sutskever r salakhutdinov dropout simple way prevent neural networks overtting journal machine learning research sutskever o vinyals q v le sequence sequence learning neural networks advances neural information processing systems pages z tu z lu y liu x liu h li modeling coverage neural machine translation arxiv preprint vaswani n shazeer n parmar j uszkoreit l jones n gomez l kaiser sukhin attention need arxiv preprint o vinyals m fortunato n jaitly pointer networks advances neural information processing systems j weizenbaum eliza computer program study natural language communication man machine communications acm y wu m schuster z chen q v le m norouzi w macherey m krikun y cao q gao k macherey et al google s neural machine translation system bridging gap human machine translation arxiv preprint z xie s wang j li d levy nie d jurafsky y ng data noising smoothing neural network language models arxiv preprint w zaremba sutskever o vinyals recurrent neural network regularization arxiv preprint
