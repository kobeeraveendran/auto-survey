q learning language model edit based unsupervised summarization ryosuke kohita ibm research akifumi wachi ibm research kohi akifumi wachi com yang zhao ibm research ryuki tachibana ibm research ibm com t c o l c s c v v x r abstract far unsupervised methods promising stractive textsummarization lel corpora required performance research promising ed tions going paper pose new approach based q learning edit based summarization method combines key modules form torial agent language model converter ealm agent predicts edit actions e t delete replace lm converter deterministically generates mary basis action signals learning leveraged train agent duce proper edit actions experimental results ealm delivered competitive mance compared previous decoder based methods truly zero paired data e validation set dening task q learning enables develop competitive method latest techniques reinforcement learning available unsupervised summarization conduct qualitative analysis providing sights future study unsupervised marizers introduction automatic text attractive nique helping humans grasp content documents effortlessly supervised neural methods shown good performances et al zhang et al unsupervised proach starting attract interest tage requiring costly parallel corpora empirical performance unsupervised methods currently state art supervised models zhao et al baziotis codes available kohilin ealm figure overview previous left proposed right approaches cr learning paradigm et al unsupervised text summarization developing stage solutions actively explored previous unsupervised approach extends neural encoder decoder modeling zero paired data scenario model trained paradigm called compression reconstruction cr learning miao blunsom fevry phang zhao et al mechanism similar translation sennrich et al model consists compressor e summarizer reconstructor co trained reconstructor recover original sentence summary generated compressor miao blunsom left figure experimental results showed unsupervised encoder decoder based summarizer able learn mapping tence summary paired data baziotis et al yang et al refer abstractive summarization paper reinforcement learning rl potential reconstruction evaluationcompression evaluationcompression encoder decodereditorial agentreconstructionencoder decodereditorial actions replace language model converterq learning approach language modelconventional approach generatorsmachine learning perfectai imperfectmachine learning perfectai imperfectmachine learning perfectcompressionreconstructionmachine learning perfect solution paired data situation related elds example unsupervised ods text simplication text compression policy gradient learning zhang lapata zhao et al recent rl techniques value based approach e q learning dqn mnih et al tion policy value based approaches asynchronous advantage actor critic mnih et al critical requirement leverage based method value function represents goodness action given state sutton et al naturally dene value tion utilizing cr learning paradigm makes latest value based approaches available unsupervised text summarization paper propose new method based q learning edit based summarization gu et al malmi et al right ure edit based summarization generates summary operating edit action e remove replace word input tence method implements editing cess modules editorial agent predicts edit actions language mmodel lm converter deterministically decodes sentence basis action signals ealm cr learning dened learning framework train agent predict edit actions instruct lm converter produce good summary vast action space causing sparsity reward word eration encoder decoder model generally difcult learned rl method mitigates issue thanks fewer edit actions deterministic decoding language model formulation q learning enables incorporate latest techniques rl main contribution paper provide new solution form pervised edit based summarization leveraging learning language model experimental sults method achieved competitive performance encoder decoder based methods truly paired data e tion set qualitative analysis brings insights current unsupervised models ing problem formulation q learning enables import latest techniques rl leads potential improvements future research task denition begin formally dening problem supervised summarization cr learning goal task produce informative summary y consisting m words ym given input sentence consisting n words xn m challenge task learn transformation y input sentence tackle cr learning introduces ditional transformation called reconstruction reconstruction requests reproduce input tence x generated summary y x reproduced sentence consisting n words xn terms generated sentences x let c compression function r reconstruction function y c x r c r respective parameters task written following mization problem c r arg max y x c r y x functions return higher value favorable y x regard input sentence according cr ing s hypothesis summary contain information guess original contents y favorable difference x smaller y maintains essential aspects summary e shortness uency previous method previous approaches use generative decoder model sutskever et al pression reconstruction functions miao blunsom fevry phang wang lee baziotis et al jective functions implementation details differ depending study underlying motivation entails hypothesis cr learning example baziotis et al introduced objective functions discrepancy y trained language model topical distance length y probability difference xi xi threes regarded f y nal x encoder decoder model formed generation tasks suffers inherent difculties related repetition et al length control kikuchi et al exposure bias ranzato et al runs convergence problems co training multiple generators salimans al proposed method proposed method ealm sists essential modules editorial agent lm converter agent sends action nals remove replace word tence conveter deterministically transforms input sentence according nals train agent nd action signals lm converter produces sentences demanded cr learning following sections rst share background q learning present task approach q learning framework explain core algorithmic details nish explanations training inference preliminaries q learning popular approach rl sented deep q networks dqn mnih et al q learning leverages action value tion estimate value pair state action respect policy action value function e q function represented pected reward state action pair e s s state action r reward tion state action pair discount factor solve text summarization task q learning rst need appropriately dene state action reward function unsupervised edit based summarization q learning approach given input sentence dene state si regard word xi action ai state chosen options remove replace goal editorial agent provide timal action sequence iteratively making action decisions word obtain y x propose ministic transformation algorithm based lm converter finally dene reward function r evaluate action action sequence terms produced sentences y x reward function designed align cr learning paradigm leads agent bringing action sequence generates appropriate y x algorithms section describe principle rithms create predict ai generate x means lm converter compute reward iterative action prediction overall ow iteratively predicting action word shown figure agent predicts action state e word prediction step express subscription t example respectively denote state action xi t th step note predicted action agent prediction xi step prepare boolean vector length n representing prediction statuses t th step prediction th word nished order predict action determined q values let s action pair operated comes maximum q values unoperated states s dened s agent reiterates predictions nishes determining action words dening state regard single word instead sentence asking agent prediction order handle variable sentence lengths natural language note left right process agent conducts prediction order condence explain encode send agent contextual information previous decisions prediction statuses sentence dynamically create state concatenation encodings local encoding global encoding create encodings rst map xi xed sized vector ei arbitrary encoder s arg max figure algorithmic visualization iterative action prediction use bert devlin et al ei repeatedly process regardless steps dene local encoding ei learnable bias vectors action prediction status th word respectively create global encoding self attention fashion j computed k figure deterministic compression tion masked language model thanks bias terms attention aware previous cisions word interactions decisions addition bert encoding ei enables sentence account deterministic decoding language model action signals section explain compress reconstruct sentences deterministic manner relu instead conventional exp cause exp caused exploding gradient case lm converter lm converter use bert devlin et al masked language model mlm trained dict masked portions sentence mlm estimate probability distribution th word xi sentence mask tion mask wang cho denotes function return word highest probability th position procedure obtain y x encodingglobal context machine learning perfectmachinelearningis context ai imperfectabpreprocess conversionc mlm shown figure vert skeleton sequence consisting n tokens zn zi xi ai null token dene pression reconstruction functions c r ai l zi xi ai l zi ai replace remove replace word predicted given replace compression reconstruction set original tence prexed context comes compression reconstruction mlm aware meaning example shown figure mlm receives machine learning perfect mask compression input predicts words multiple masks conduct prediction autoregressive fashion pendix note language model lm converter mlm geous utilizes contexts restriction looking ahead coming words stepwise reward computation section explain reward computation chosen action referring y x stated action sequence step t apply c r obtain list tuples s r y tuple let ence enables evaluate state action pair respect single transition section propose techniques step reward olation penalty summary assessment evaluate agent s behavior stepwise periences refer table work reward computation actual example moving details let dene important notions section sion rate cr reconstruction rate rr xi cr learning assumes higher values cr rr better use calculating rewards pruning experiences step reward task agent duce action sequence lm verter generates appropriately compressed tence keeping reconstruction successful dene reward function r y x rsr rsa rsr step reward designed encourage agent improve compression reconstruction rate respectively rsa additional score qualitative assessment y explain later returning step reward rsr multiplication rc rr dened rsr rc rr rc rr minimum requirement construction rate t th step dened t n hyperparameter set requests perfect reconstruction regardless t need forgive reconstruction failure extent cause information loss compression adjusts allowed number failures ample requests model recover half original sentence correctly let describe behavior step reward rsr reward agent chooses replace rc change length y second ward gets positive value agent chooses remove satises requirement construction rate ward gets negative value agent chooses remove reconstruction rate requirement short step reward recommends remove long agent recover nal word replace violation penalty sequential modeling ing performed agent essentially suffers error propagation caused incorrect tions earlier stage collins roark violation penalty mitigates issue giving negative reward latest problematic action excluding experiences mistake experiences beginning t steps dened step reward paragraph let explain terms inside square ets rst rst term multiplication aims shortness mativeness gets higher value agent achieves right balance compression construction second term sim aims uate informativeness brought replace concretely sim returns semantic similarity score range sentence vectors checking exact matches words term llh represents uency log likelihood given pre trained language model zhao et al use bert computation sim llh devlin et al wang cho appendix nally t ratio number operated words closer agent reaching termination e nishing tion words avoiding violation penalty makes rsa larger contrast agent fails earlier stage gets small value rsa training inference replay buffer training leveraging experiences s r y x lin agent learns policy summarizing sentence q learning framework specically utilize dqn mnih et al learn q function q corresponding optimal policy minimizing loss es r r q q function parameters periodically updated accordance latest network rameters collection experiences rl requires agent explore action given state nding better policy unique point work agent explore action order predict rations use algorithm watkins stochastically forces agent ignore q values behave randomly appendix inference modeling provides y x step advantage terms inference nal output use y step achieves best balance compression reconstruction ratios t figure violation penalty compression left reconstruction right axis step y axis ratio horizontal lines middle dashed lines represent circles represent step agent breaks constraints addition introduce parameter represents minimum ment compression rate denotes threshold t step dened t n agent satisfy condition penalty forcibly assign reward state action pair t step agent breaks constraint tion ignore experiences step t onward agent keeps predicting end dene t n figure shows constraints work experience sequence summary assessment step ward considers compression reconstruction ratios ignores critical aspects generated summary replacement shorter onym uency sentence explain rsa mentioned previous paragraph describe reect qualitative assessments reward given agent essential properties y perspectives account informativeness ness uency informativeness refers y retains original meaning x shortness uency self explanatory reect perspectives agent s decision dene rsa rsa t n sim computes similarity score llh computes log likelihood y hyperparameters adjust importance sim llh addition rsr rsa trainingagent breaks constraint action remove remove remove force type force force force force cr rr t n sim llh rsr rsa r experiences violation occurred step t table example stepwise reward computation breaks reconstruction constraint step removing force rsr rsa computed step step settings hyperparameters arg based relationship compression reconstruction seen precision recall curve experiment baselines compare proposed approach baselines lead n simply takes beginning n words summary cent encoder decoder model baziotis et al cmatch new approach explicit reconstruction learning zhou rush conduct qualitative analysis generated maries ran baselines cated model provided model cmatch test types els tuned validation set parameters iteration training ealm cmatch need paired data tion proposed method implemented ealm follows q network agent consists layered mlp units layer relu adam optimizer ing rate apply gradient clipping epsilon greedy strategy rst set exploring probability decay plying updates reaches minimum exploration rate set discount factor size replay buffer sample experiences com cbaziotis ran training script conguration inal paper decreasing batch size gpu limitation trained models obtained slightly lower scores ones reported original paper report averaged score models com unsupervised sentence summarization batch update nal model use parameters time averaged score reward replay buffer maximum e model need validation set parameters step reward set respectively hyperparameters summary assessment set train models tion report averaged score q learning inherently contains randomness training dataset baziotis et al train model gigaword corpus giga rush et al k sentences randomly picked sentences words training ealm data m sentences large expose agent different ences sentence note entirety sentences training models followed baziotis et al uation test set consisting giga sentences duc datasets sentences tences et al models follow tokenization policy default tokenization giga bert ealm uses vocabulary based subwords apply subwording single tokenization policy words bert ulary interpreted unknown words ratio unknown words giga trained large dataset takes long time exploitation exploration learning strategy q learning k better balance required time model performance data model giga r l len nw cm el cm el cm el table rouge scores averaged lengths len averaged occurrences new words nw lead n represent models cm cmatch el ealm rouge scores puted summaries capped rst bytes evaluation quantitative analysis amine rouge scores mitigate bias longer sentences rouge calculation capped summaries rst bytes note averaged sentence length gold summaries capping giga respectively examine sentence length len count new words nw number words ated summary appear input tence additionally qualitative isons manual check generated summaries questionnaire survey conducted assess deeper quality summaries informativeness readability hides exact points model s strengths weaknesses consider specic indications provide sights future work current unsupervised summarizers manually checked summaries model dataset include samples appendix results table lists results rouge scores averaged lengths averaged counts new words ealm showed better performance respect r l giga performed competitively lines original length generated summaries tended longer occurrence pltrdy following baziotis et al cm el copy words grammatical informative meaningless rephrasing grammatical fluent successful cases giga select words input contain keywords grammatical lack rephrasing lack information short table pros cons found generated summaries cmatch ealm new words lowest cmatch achieved highest scores rouge meaningful length giga scores r l superior points means cmatch captured salient words word co occurrences generating summaries cmatch uses language model trained gold summaries giga words internally store probable word distributions summary tences giga actually results better giga cmatch require paired data practical collect summaries train language model domain showed competitive performance models scores dropped idation set available requirement validation set keen disadvantage ating input summary pairs comes signicant human labor cost best scores given statistical models performed petitively result indicates unsupervised summarization methods overcome trivial baseline signicant barrier preventing progress unsupervised methods ably difculty rephrasing writing good summary condensing longer expression shorter form essential seen nw column table number new words cmatch ealm current models tend operate paste consistent report baziotis et al finally manually assess summaries duced model sum pros cons table actual examples shown input japan s nec corp unk computer corp united states said wednesday agreed join forces puter sales mechanical lems threaten shut astronomical vations hubble space telescope prompt repair mission months earlier planned billion spacecraft nasa ofcials told congress wednesday s endeavour nauts connected building rst blocks tional space station sunday creating seven story tower shuttle cargo bay human nec unk computer sales tie japan s nec corp computer corp united states said cmatch nec agrees join forces puter sales ealm nec computer united states said agreed join forces sales problems stop hubble astronomical observations nasa accelerate pair mission mechanical problems threaten shut vations ble space telescope threaten nasa observes threaten shut astronomical servations space telescope prompt repair mission lier planned lion spacecraft nasa ofcials told congress building blocks tional space station successfully joined s endeavour nauts connected building rst blocks ternational space station endeavour tronauts shuttle s create connected rst ing blocks tional space ating tower shuttle bay table summaries human gold reference cmatch ealm giga center buttom table found summary likely exact copy input sentence kept sentences grammatical informative rephrasing meet expectation cases changing week day e wednesday thursday common adjective pronoun adjective e astronomical cmatch stably generated uent summaries giga seen rouge scores generated grammatically correct tences number agreement e nec agrees duc datasets meaningless summaries increased containing tant information e nasa observes relatedly cmatch s summaries short found half maries consisted qeual words finally ealm s outputs tended longer containing non informative portions e nasa ofcials told likely matical leaving functional word e mechanical problems threaten deleting required prepositions e agreed join failures resulted lower readability ealm tried keywords input exist positions sentence supported tively higher score r l challenge caused low readable ungrammatical summaries interesting research direction sophisticate ealm s behavior conclusion brought q learning framework pervised text summarization proposed new method ealm edit based unsupervised summarizer leveraging q learning agent language model experments showed ealm performed competitively previous encoder decoder based methods itative analysis found quality erated summaries unsupervised model sufcient individual limitations model issue overcome step forward generating practically available summaries paired data particular ealm room improvement ing latest techniques rl work paves way research bridging q learning unsupervised text summarization references christos baziotis ion androutsopoulos ioannis konstas alexandros potamianos differentiable sequence sequence sequence autoencoder unsupervised abstractive sentence proceedings compression ference association computational linguistics human language technologies pages north american chapter michael collins brian roark incremental parsing perceptron algorithm ings annual meeting association computational linguistics pages jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language standing proceedings conference north american chapter association computational linguistics human language nologies pages thibault fevry jason phang vised sentence compression denoising encoders proceedings conference computational natural language learning pages jiatao gu changhan wang junbo zhao levenshtein transformer proceedings ternational conference neural information cessing systems pages yuta kikuchi graham neubig ryohei sasano hiroya takamura manabu okumura ling output length neural encoder decoders proceedings conference empirical methods natural language processing pages long ji lin self improving reactive agents based reinforcement learning planning ing machine learning eric malmi sebastian krause sascha rothe daniil mirylenka aliaksei severyn encode tag realize high precision text editing ings conference empirical methods natural language processing national joint conference natural language cessing emnlp ijcnlp pages hong kong china association computational guistics yishu miao phil blunsom language latent variable discrete generative models tence compression proceedings ference empirical methods natural language processing pages volodymyr mnih badia mehdi mirza alex graves tim harley timothy p lillicrap david silver koray kavukcuoglu asynchronous methods deep ment learning proceedings tional conference international conference machine learning pages volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc g bellemare alex graves martin riedmiller andreas k jeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis human level trol deep reinforcement learning nature paul hoa dang donna harman duc context information processing management marcaurelio ranzato sumit chopra michael auli wojciech zaremba sequence level ing recurrent neural networks proceedings international conference learning sentations alexander m rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages tim salimans ian goodfellow wojciech zaremba vicki cheung alec radford xi chen xi chen proceedings neural information ing systems pages improved techniques training gans abigail peter j liu christopher d manning point summarization generator networks proceedings nual meeting association computational linguistics pages rico sennrich barry haddow alexandra birch improving neural machine translation proceedings els monolingual data annual meeting association tational linguistics volume long papers pages berlin germany association tional linguistics ilya sutskever oriol vinyals quoc v le sequence sequence learning neural networks proceedings international conference neural information processing systems pages richard s sutton andrew g barto al troduction reinforcement learning volume mit press cambridge alex wang kyunghyun cho bert mouth speak bert markov proceedings dom eld language model workshop methods optimizing ing neural language generation pages yaushian wang hung yi lee learning encode text human readable summaries generative adversarial networks proceedings conference empirical methods ral language processing pages christopher john cornish hellaby watkins thesis learning delayed rewards king s college cambridge uk ph d ziyi yang chenguang zhu robert gmyr michael zeng xuedong huang eric darve ted pretrained unsupervised summarization model theme modeling denoising haoyu zhang jingjing cai jianjun xu ji wang pretraining based natural language tion text summarization proceedings conference computational natural guage learning conll pages hong kong china association computational guistics xingxing zhang mirella lapata sentence simplication deep reinforcement learning proceedings conference empirical methods natural language processing pages copenhagen denmark association computational linguistics yang zhao zhiyuan luo akiko aizawa language model based evaluator sentence pression proceedings annual ing association computational linguistics pages jiawei zhou alexander rush simple supervised summarization contextual matching proceedings annual meeting association computational linguistics pages florence italy association tational linguistics appendices autoregressive prediction mlm algorithm describes autoregressive prediction mlm input contains multiple masks algorithm autoregressive prediction mlm sentence replacing input sentence includes outpit predicted words xi j wj end j arg max ji xj wj end p exploration prediction order explained section main paper editorial agent explores order predict agent basically chooses state maximum q value state pick uncertain state instead dene uncertainty state entropy action probabilities aa log s selected s arg max arg max sst aa semantic similarity log likelihood computation summary assessment semantic similarity use pre trained model predict semantic similarity sentences bert encodings model trained supervised manner pair sentences similarity score original library outputs real valued score range normalize log likelihood compute log likelihood compressed sentence bert lows wang cho m im yi com semantic text similarity llh function performs thresholding returns score old raw log likelihood score scaled rewards empirically set threshold relaxations calculation calculation reconstruction rate duced section based exact match word x given ambiguity natural language strict agent rarely acquires rewards relax situation excluding stop words calculation comparing k candidates equation rr formally written xi xi w xi w returns k probable words th position w pre dened set words set k common stopwords e infrequent words giga w experimental details computing infrastructure run models machine specications ubuntu ram gb nvidia tesla model size ealm number trainable parameters experimental setting editorial agent trainable parameters language model hypperparameter search conduct hyperparameter search empirically mined values described main paper proposed method paragraph runtime speed ealm processes sentence seconds pm average gpu generated summaries samples summaries generated model listed tables following pages examples taken rst sentences giga randomly picked include human generated maries e gold reference input japan s nec corp unk computer corp united states said wednesday agreed join forces puter sales sri lankan ernment day announced sure government schools ate effect tary campaign tamil separatists lated north country police arrested ve anti nuclear protesters thursday sought disrupt ing french tic research supply vessel spokesman protesters said factory orders ufactured goods rose percent ber commerce department said thursday bank japan appealed nancial markets remain calm friday following decision bank order daiwa close ltd operations croatian president franjo tudjman said friday croatian negotiators serb meet saturday thrash agreement serb held area croatia deal reached brokered talks japan s toyota team europe banned world rally championship year friday crushing ruling world council international bile federation human nec unk computer sales tie japan s nec corp computer corp united states said cmatch nec agrees join forces puter sales ealm nec computer united states said agreed join forces sales lanka sri schools escalates closes war sri lankan ment thursday nounced closure government schools immediate tary country sri lankan government announces military campaign tamil separatists sri lankan government closure announced schools government military effect campaign escalated north country protesters french research ship target police arrest ve nuclear protesters police arrested ve anti nuclear protesters tuesday sought disrupt antarctic protesters factory orders ufactured goods rose percent ber bank japan appealed nancial markets remain calm thursday ing decision september factory orders percent factory orders rise percent september bank unk unk calm nancial markets bank daiwa ltd close tions police arrested sought disrupt loading french antarctic search supply vessel spokesman said factory orders factured goods rose september commerce said bank japan appealed nancial markets main calm following decision order bank close operations rebel serb talks sume saturday man peter unk croatian president franjo tudjman said thursday croatian serb negotiators meet saturday agreement talks croatian president franjo tudjman says serb negotiators meet croatian said ian serb meet thrash ment area tia deal reached talks toyota banned year japan s toyota team europe banned world rally championship europe banned world pionship year japan team toyota europe banned world rally onship year ruling council international automobile crushing table summaries human gold reference cmatch ealm giga space input mechanical problems threaten shut astronomical observations hubble scope prompt repair mission months earlier planned billion craft nasa ofcials told congress wednesday city offers public ple problems homelessness san francisco biggest complaint itors lodge city concerns gressive panhandling tions homelessness experience city tourist cials atlanta maybe maybe customers pay use bank atm machines ginning ght maybe getting smarter head turkey s pro islamic party said thursday insist rightful chance lead turkey s government heading frontation itary deepen nation s political crisis bombers suicide targeted crowded market open air setting friday blasts killed assailants injured shoppers passersby prompted israeli cabinet action new peace accord president nelson dela acknowledged saturday african congress national violated human rights apartheid ting odds deputy president report divided south africa human problems stop hubble astronomical observations nasa accelerate repair mission mechanical problems threaten shut vations ble space telescope threaten cmatch nasa observes lack affordable housing basic san francisco s homeless crisis city offers public example problems san offers public tourist francisco city san lidge bank customers ning resist double charges atm use atlanta maybe maybe customers pay use bank machines getting atlanta gets smarter broad based ist coalition likely turkey head turkey s party said tuesday insist rightful turkey s crisis head turkey s pro islamic party possible early tion car bomb injures bombers killed suicide bombers geted crowded air market tuesday setting blasts killed assailants cord israeli cabinet puts accord nelson president mandela edges anc rights violations leaders disagree president clinton dela acknowledged saturday african congress national violated human rights apartheid setting africa nelson mandela knowledges human rights ealm threaten shut astronomical servations space telescope prompt repair mission lier planned lion spacecraft nasa cials told congress city fers public ample problems san francisco complaint lodge city concerns sive experience city tourist ofcials atlanta maybe maybe customers pay use bank atm machines beginning ght maybe getting smarter head turkey party said insist rightful chance lead turkey ment heading frontation tary nation political crisis suicide bombers geted crowded market setting blasts killed injured prompted israeli cabinet action new peace accord mandela national edged congress violated human setting odds deputy president report divided south table summaries human gold reference cmatch ealm input endeavour s nauts connected building blocks tional space station sunday creating seven story tower shuttle cargo bay cocoon loyal wealthy ers president clinton said friday live consequences mistakes contended democrats pride achievements presidency heart possibilities eve holiday linked antiabortion violence authorities day investigating picture aborted fetus sent canadian newspaper connected month s fatal shooting buffalo n tor provided tions similar tacks western new york canada famine threatened north korea s harvest better year worse senior u n aid ofcial said saturday matthew wayne ard gay student beaten dead night tied fence left die mourned funeral friday people cluding met delegation chilean lobbying legislators possible extradition augusto pinochet spain warned face trial thursday chile brink political turmoil human building blocks international space station successfully joined clinton supports candidates speaks fundraisers edges mistakes endeavour s nauts connected building rst blocks space ternational station loyal wealthy supporters president clinton said tuesday loyal wealthy porters clinton cmatch s create endeavour tronauts shuttle ealm connected rst ing blocks tional space ing tower shuttle bay democrats pride presidency anti abortion yer canada related buffalo clinic ing eve holiday linked violence ties holiday linked lence eve holiday loyal supporters wealthy clinton president live said consequences mistakes pride achievements presidency heart possibilities eve holiday linked olence ing picture sent canadian newspaper connected month fatal shooting buffalo doctor provided similar tacks western new york canada world food program reports famine killed million north koreans north korea s vest ter year worse south korea s zhan north korea harvest better worse senior aid ofcial said matthew shepard logized wanted ple s lives better matthew wayne gay student beaten dead night gay student beaten ceos gay beaten dead night tied fence left die funeral including met legislators chilean protest madrid extradition pinochet delegation chilean legislators lobbying possible extradition augusto face pinochet turmoil delegation chilean legislators faces trial delegation chilean legislators lobbying possible augusto spain trial warned chile brink political turmoil table summaries human gold reference cmatch ealm
