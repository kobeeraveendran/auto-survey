c e d l c s c v v i x r a neural abstractive text summarization and fake news detection soheil esmaeilzadeh edu stanford university ca gao xian peh edu stanford university ca angela xu edu stanford university ca abstract in this work we study abstractive text summarization by exploring different models such as lstm encoder decoder with attention pointer generator networks age mechanisms and transformers upon extensive and careful hyperparameter tuning we compare the proposed architectures against each other for the abstractive text summarization task finally as an extension of our work we apply our text summarization model as a feature extractor for a fake news detection task where the news articles prior to classication will be summarized and the results are compared against the classication using only the original news text keywords lstm encoder deconder abstractive text summarization generator coverage mechanism transformers fake news detection introduction pattern recognition and data understanding has been the topic of research in multiple deep learning tasks such computer vision and natural language processing in the natural language processing area understanding the content and main idea of a text and summarizing a corpus is of great importance in simple words text summarization is the task of creating a summary for a large piece of text generating meaningful summaries of long texts is of great importance in many different areas such as medical educational media social and where the summary needs to contain the main contextual aspects of the text while reducing the amount of unnecessary information in general text summarization can be classied into two main groups extractive summarization and abstractive summarization extractive summarization creates summaries by synthesizing salient phrases from the full text verbatim however abstractive summarization creates an internal semantic representation of the text unlike extractive summarization which concatenates sentences taken explicitly from the source text abstractive text summarization paraphrases the text in a way that it is closer to the human s style of summarization and this makes abstractive text summarization a challenging yet preferable approach decent quality summaries using abstractive approaches were only obtained in the past few years by applying the sequence to sequence endoder decoder architectures with attention mechanisms common in machine translation tasks to summarization however only focused on short input texts subsequent works attempted to perform the abstractive summarization task on longer input texts however appearance of unknown words and repetitions adversely affected the outcome of the summarization tasks in this work we focus on abstractive text summarization as a more robust approach compared to its counterpart i e extractive summarization and explore recent advancements in the state of the art natural language models for abstractive text summarization the input of our natural language model is a single document or article and the output of it is a combination of a few sentences that summarize the content of the input document in a meaningful manner in addition to the main goal of this work after exploring the natural language models for abstractive text summarization we use the summarization model as a feature building module for fake news detection and news headline generation and show the effect of summarization on fake news detection preprint work in progress approaches figure baseline sequence to sequence model s architecture with attention baseline model in this work as the baseline model we consider an lstm encoder decoder architecture with attention as shown in figure sequence to sequence encoder decoder the sequence to sequence framework consists of a recurrent neural network rnn encoder and an rnn decoder the rnn encoder as a single layer bidirectional long short term memory lstm unit reads in the input sequence token by token and produces a sequence of encoder s hidden states hi that encode or represent the input the rnn decoder as a single layer unidirectional lstm generates the decoder s hidden states st one by one which produces the output sequence as the summary attention mechanism in the attention mechanism an attention distribution at is calculated as a probability distribution over the words in the source text that helps the decoder decide which source words to concentrate on when it generates the next word the attention distribution at is calculated for each decoder timestep t as et i vt w st battn at where v wh ws battn are learnable parameters on each decoder s step attention weights at i which are part of the at distribution for the source words are computed an attention weight represents the amount of attention that should be paid to a certain source word in order to generate an output word decoder state in the decoder the attention distribution is used to compute a weighted sum of the encoder hidden states known as the context vector h t which represents what has been read from the source for this step and can be calculated as h t at ihi i the context vector along with the decoder s state are then used to calculate the vocabulary distribution pvocab which provides a nal distribution for predicting words w as pvocab st h where v v b and are learnable parameters subsequently we calculate the loss for the timestep t as the negative log likelihood of the target word w as losst log the overall loss for the whole sequence is the average of the loss at each time step i e losst as t loss losst t t baseline model s problems some problems are associated with the baseline model proposed in section one problem is the model s tendency to reproduce factual details inaccurately this happens specially when an uncommon word that exists in the vocabulary is replaced with a more common word another problem with the baseline model is that during summary generation it repeats the already generated parts of the summary lastly the baseline is unable to handle out of vocabulary words oov in general it is hard for the sequence to sequence with attention model to copy source words as well as to retain longer term information in the decoder state which leads to the aforementioned issues see proposed a so called pointer generator network that also includes a coverage mechanism in order to address these problems by combining both context extraction pointing and context abstraction generating we revisit the model proposed by see in the following and as well compare it with a transformer based model proposed by for machine translation tasks and nally use it as a feature generation mechanism for fake news classication pointer generator network figure pointer generator model s architecture pointer generator mechanism pointer generator is a hybrid network that chooses during training and test whether to copy words from the source via pointing or to generate words from a xed vocabulary set figure shows the architecture for the pointer generator mechanism where the decoder part is modied compared to figure in figure the baseline model only an attention distribution and a vocabulary distribution are calculated however here in the pointer generator network a generation probability pgen which is a scalar value between and is also calculated which represents the probability of generating a word from the vocabulary versus copying a word from the source text the generation probability pgen weights and combines the vocabulary distribution pvocab used for generating and the attention distribution a used for pointing to source words into the nal distribution pnal as pgen ai i wi w based on equation the probability of producing word is equal to the probability of generating it from the vocabulary multiplied by the generation probability plus the probability of pointing to it anywhere it appears in the source text multiplied by the copying probability compared to the lstm encoder decoder model with attention as baseline in section the pointer generator network makes it easy to copy words from the source text by putting sufciently large attention on the relevant word it also is able to copy out of vocabulary words from the source text enabling the model to handle unseen words while allowing to use a smaller vocabulary leading to less computation and storage space the pointer generator model is also faster to train as it requires fewer training iterations to achieve the same performance as the baseline model in section figure the transformer model architecture left scaled dot product attention right multi head attention consists of several attention layers running in parallel coverage mechanism to reduce the repetition during summarization as a common issue with sequence to sequence models mentioned in section we apply the coverage mechanism rst proposed by and adapted by the coverage mechanism keeps track of a coverage vector computed as the sum of attention distributions over previous decoder time steps this coverage vector is incorporated into the attention mechanism and represents the degree of coverage that words in the source text have received from the attention mechanism so far thus by maintaining this coverage vector which represents a cumulative attention the model avoids attending to any word that has already been covered and used for summarization on each timestep t of the decoder the coverage vector ct is the sum of all the attention distributions so far as ct this coverage vector also contributes to computing the attention mechanism described in the previous section so that equation becomes vt w st wcct et i battn intuitively this informs the attention mechanism s current timestep about the previous attention information which is captured in ct thus preventing repeated attention to the same source words to further discourage repetition see penalizes repeatedly attending to the same parts of the source text by dening a coverage loss and adding it to the primary loss function in equation this extra coverage loss term penalizes any overlap between the coverage vector ct and the new attention distribution at as covlosst i ct i finally the total loss becomes loss t we have consulted the gihub repositories referenced at the end of this report covlosst for the aforementioned models transformers in this part we revisit the transformers network proposed by vaswani for machine translation and investigate its performance on abstractive text summarization on our dataset in the transformer model the encoder maps an input sequence of symbol representations as xn to a sequence of continuous representations as z zn given z the decoder then generates an output sequence as y of symbols one element at a time at each step the model is auto regressive consuming the previously generated symbols as additional input when generating the next the transformer follows this overall architecture using stacked self attention and point wise fully connected layers for both the encoder and decoder shown in the left and right halves of figure respectively the encoder part of this architecture is mainly a stack of some identical layers where each one has two sublayers the rst is a multi head self attention mechanism and the second is a simple position wise fully connected feed forward network the decoder is also composed of a stack of identical layers in addition to the two sub layers in each encoder layer the decoder inserts a third sub layer which performs multi head attention over the output of the encoder stack in the transformer architecture a variation of attention mechanism called scaled dot product attention is used where the input consists of queries and keys of dimension and values of dimension dv the dk the result goes through a dot products of the query with all keys is calculated then divided by softmax function to obtain the weights on the values in practice the attention function is computed on a set of queries simultaneously packed together into a matrix q the keys and values are also packed together into matrices k and v where the matrix of output can be calculated as k v softmax v qk t in the proposed transformer model by instead of performing a single attention function they linearly project the queries keys and values different times with different learned linear projections and that way they build a multi head attention on each of the projected versions of queries keys and values they then perform the attention function in parallel yielding multi dimensional output values which are concatenated and once again projected figure for the transformer model we have consulted the gihub repositories referenced at the end of this report experiments dataset overview preprocessing to train our summarization models we use the cnn dailymail dataset a collection of news articles and interviews that have been published on the two popular news websites cnn com and mail com like the common styles on newspapers and journals each article contains highlighted sections that together form the summary of the whole article the raw dataset includes the text contents of web pages saved in separate html les we use the cnn and dailymail dataset provided by deepmind our dataset is split in between training dev and test set respectively leading to training pairs validation pairs and test pairs there is an average of tokens per news article each reference summary contains sentences and tokens on average we preprocess the dataset and convert the characters all to lower case we use the stanford corenlp library to tokenize the input articles and their corresponding reference summaries and to add graph and sentence start and end markers as and s respectively in addition we have tried limiting our vocabulary size to and evaluation metric we evaluate our models with the standard rouge recall oriented understudy for gisting ation score a measure of the amount of overlap between the system generated and the reference summaries we report the precision and recall scores for and rouge l which measure respectively the word overlap bigram overlap and longest common sequence between the system generated and reference summaries the rouge recall and precision for summarization task can be calculated as rouge recall number of overlapping words total words in reference summary rouge precision number of overlapping words total words in system summary where the system summary refers to the summary generated by a summarization model using precision it s possible to measure essentially how much of the system summary was in fact relevant or needed and using recall rouge it s possible to measure how much of the reference summary is the summarization model generating in terms of measuring the overlapping words in equations and considering the overlap of unigrams or bigrams or longest common sequence leads to and rouge l scores respectively for precision and recall experimental details results and analysis text summarization in this work we investigate the performance of the summarization models presented in section namely lstm encoder decoder with only attention mechanism baseline lstm encoder decoder with attention and pointer generator mechanisms lstm encoder decoder with attention pointer generator and coverage mechanisms and transformers table shows the and rouge l scores for the four different models that have been trained on the summarization dataset we have trained the models upon hyperparameter tuning using adagrad optimizer for iterations epochs our training results outperform the similar ones presented by for cases and and are very close in case model rouge l precision recall precision recall precision recall table lstm encoder decoder with only attention mechanism baseline lstm decoder with attention and pointer generator mechanisms lstm encoder decoder with attention pointer generator and coverage mechanisms and transformers figure validation and training loss values v s the number of iterations for summarization models figure shows the loss on the training set and validation set for as a function of number of iterations for the summarization models for iterations epochs the results of summarization are compared for one case v s its ground truth for the three summarization models in table as it can be seen the summary generated by model contains unk instead of the word mysak in the original summary however due to having attention and the pointer generator mechanism model has replaced the unk with the proper word from the source text however summary of model has repeated a sentence twice the summary generated by the pointer generator together with the coverage mechanism not only could have overcome the unk problem but also does not have repetition in the generated summary and gives a nice summary pretty close to the reference summary reference model model model model once a super typhoon maysak is now a tropical storm with mph winds it could still cause ooding landslides and other problems in the philippines gained super typhoon status thanks to its sustained mph winds it s now classied as a tropical storm it s expected to make landfall sunday on the southeastern coast of province tropical storm maysak approached the asian island nation saturday it s now classied as a tropical storm according to the philippine national weather service it s now classied as a tropical storm according to the philippine weather service just a few days ago maysak gained super typhoon status thanks to its sustained mph winds it s now classied as a tropical storm according to the philippine national weather service super typhoon could weaken new jersey but it will philippine ocean strength at least people are injured including table comparison of the generated summary using the summarization models v s the ground truth the summary generated by the transformer model can only capture some keywords but does not convey the grasp of summary very well fake news detection subsequent to summarization in this part we use the best summarization model that we have trained on the summarization dataset in order to create summaries of a fake news detection dataset we will build a fake news detection model and we investigate its performance when the input is the original news text the news headline and the summarized news text generated by our summarization model basically we use our text summarizing model as a feature generator for a fake news classication model in fake news classication the article content contains much more information than the article headline and due to this a fake news classier performs better on article contents than on article headlines figure fake news classication architecture full body text headline text figure confusion matrix for test set of fake news detection task using three different input features summary text input input features exp train acc valid loss full body text headline text summary text lstm lstm lstm lstm cells lstm lstm lstm lstm size drop out bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm lstm lstm lstm lstm train loss valid acc table experiments on the fake news detection input features accuracy average length in words full body text headline text summary text table fake news classier results for fake news classication we use a fake news dataset with headlines and article content provided by george mcintire the dataset contains fake news articles and real articles i e a balanced dataset on politics from a wide range of news sources we shufe the data and use of it for training of it for validation and for testing and also do fold cross validation we build a long short term memory lstm network together with an embedding layer as shown in figure table shows our hyperparameter studies for fake news classication and table shows the nal test accuracies using the three input features of full body text headline text and generated summary texts by our summarization models as it can be seen in this table the best model using the body text as input features perform better than headline text as input furthermore it s worth noting that the summary text as input feature leads to an even higher accuracy compared to the full body text as input feature this nding shows that summarization model serves as a feature generator for fake news detection task which actually increases its accuracy also this summarization model can also serve as a headline generator for the news articles as an automatic approach conclusion as we showed in section the pointer generator architecture with attention and coverage nisms led to the highest accuracies and could overcome the problems common in abstractive text summarization such as out of vocabulary words and repetition furthermore as shown in section a text summarizing model can successfully be applied as a feature generator prior to classication tasks such as fake news classication and increase the accuracy of those tasks datasciencecentral com proles blogs on building a fake news classication model references soheil esmaeilzadeh ouassim khebzegga and mehrad moradshahi clinical parameters prediction for gait disorder recognition soheil esmaeilzadeh dimitrios ioannis belivanis kilian m pohl and ehsan adeli to end alzheimer s disease diagnosis and biomarker identication machine learning in medical imaging mlmi pp vol soheil esmaeilzadeh yao yang and ehsan adeli end to end parkinson disease diagnosis using brain mr images by cnn org pengxiang cheng and katrin erk attending to entities for better text understanding org hui liu qingyu yin and william yang wang towards explainable nlp a generative nation framework for text classication org mehdi allahyari seyedamin pouriyeh mehdi asse saeid safaei elizabeth d trippe juan b gutierrez and krys kochut text summarization techniques a brief survey org bonnie dorr david zajic and richard schwartz hedge trimmer a parse and trim approach to headline generation proceedings of the hlt naacl text summarization workshop pp ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based sequence model for extractive summarization of documents org chandra khatri gyanit singh and nish parikh abstractive and extractive text summarization using document context vector and recurrent neural networks https org shen gao xiuying chen piji li zhaochun ren lidong bing dongyan zhao and rui yan abstractive text summarization by incorporating reader comments org dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation by jointly learning to align and translate org ramesh nallapati bing xiang and bowen zhou sequence to sequence rnns for text rization iclr karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend advances in neural information processing systems nips abigail see peter j liu and christopher d manning get to the point summarization with pointer generator networks org ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser and illia polosukhin attention is all you need https org zhaopeng tu zhengdong lu yang liu xiaohua liu and hang li modeling coverage for neural machine translation org danqi chen jason bolton and christopher d manning a thorough examination of the cnn daily mail reading comprehension task proceedings of the annual meeting of the association for computational linguistics acl mahnaz koupaee and william yang wang wikihow a large scale text summarization dataset org chin yew lin and marina rey rouge a package for automatic evaluation of summaries text summarization branches out acl
