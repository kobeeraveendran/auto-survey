c e d l c s c v v x r neural abstractive text summarization fake news detection soheil esmaeilzadeh edu stanford university gao xian peh edu stanford university angela xu edu stanford university abstract work study abstractive text summarization exploring different models lstm encoder decoder attention pointer generator networks age mechanisms transformers extensive careful hyperparameter tuning compare proposed architectures abstractive text summarization task finally extension work apply text summarization model feature extractor fake news detection task news articles prior classication summarized results compared classication original news text keywords lstm encoder deconder abstractive text summarization generator coverage mechanism transformers fake news detection introduction pattern recognition data understanding topic research multiple deep learning tasks computer vision natural language processing natural language processing area understanding content main idea text summarizing corpus great importance simple words text summarization task creating summary large piece text generating meaningful summaries long texts great importance different areas medical educational media social summary needs contain main contextual aspects text reducing unnecessary information general text summarization classied main groups extractive summarization abstractive summarization extractive summarization creates summaries synthesizing salient phrases text verbatim abstractive summarization creates internal semantic representation text unlike extractive summarization concatenates sentences taken explicitly source text abstractive text summarization paraphrases text way closer human s style summarization makes abstractive text summarization challenging preferable approach decent quality summaries abstractive approaches obtained past years applying sequence sequence endoder decoder architectures attention mechanisms common machine translation tasks summarization focused short input texts subsequent works attempted perform abstractive summarization task longer input texts appearance unknown words repetitions adversely affected outcome summarization tasks work focus abstractive text summarization robust approach compared counterpart e extractive summarization explore recent advancements state art natural language models abstractive text summarization input natural language model single document article output combination sentences summarize content input document meaningful manner addition main goal work exploring natural language models abstractive text summarization use summarization model feature building module fake news detection news headline generation effect summarization fake news detection preprint work progress approaches figure baseline sequence sequence model s architecture attention baseline model work baseline model consider lstm encoder decoder architecture attention shown figure sequence sequence encoder decoder sequence sequence framework consists recurrent neural network rnn encoder rnn decoder rnn encoder single layer bidirectional long short term memory lstm unit reads input sequence token token produces sequence encoder s hidden states hi encode represent input rnn decoder single layer unidirectional lstm generates decoder s hidden states st produces output sequence summary attention mechanism attention mechanism attention distribution calculated probability distribution words source text helps decoder decide source words concentrate generates word attention distribution calculated decoder timestep t et vt w st battn v wh ws battn learnable parameters decoder s step attention weights distribution source words computed attention weight represents attention paid certain source word order generate output word decoder state decoder attention distribution compute weighted sum encoder hidden states known context vector h t represents read source step calculated h t ihi context vector decoder s state calculate vocabulary distribution pvocab provides nal distribution predicting words w pvocab st h v v b learnable parameters subsequently calculate loss timestep t negative log likelihood target word w losst log overall loss sequence average loss time step e losst t loss losst t t baseline model s problems problems associated baseline model proposed section problem model s tendency reproduce factual details inaccurately happens specially uncommon word exists vocabulary replaced common word problem baseline model summary generation repeats generated parts summary lastly baseline unable handle vocabulary words oov general hard sequence sequence attention model copy source words retain longer term information decoder state leads aforementioned issues proposed called pointer generator network includes coverage mechanism order address problems combining context extraction pointing context abstraction generating revisit model proposed following compare transformer based model proposed machine translation tasks nally use feature generation mechanism fake news classication pointer generator network figure pointer generator model s architecture pointer generator mechanism pointer generator hybrid network chooses training test copy words source pointing generate words xed vocabulary set figure shows architecture pointer generator mechanism decoder modied compared figure figure baseline model attention distribution vocabulary distribution calculated pointer generator network generation probability pgen scalar value calculated represents probability generating word vocabulary versus copying word source text generation probability pgen weights combines vocabulary distribution pvocab generating attention distribution pointing source words nal distribution pnal pgen ai wi w based equation probability producing word equal probability generating vocabulary multiplied generation probability plus probability pointing appears source text multiplied copying probability compared lstm encoder decoder model attention baseline section pointer generator network makes easy copy words source text putting sufciently large attention relevant word able copy vocabulary words source text enabling model handle unseen words allowing use smaller vocabulary leading computation storage space pointer generator model faster train requires fewer training iterations achieve performance baseline model section figure transformer model architecture left scaled dot product attention right multi head attention consists attention layers running parallel coverage mechanism reduce repetition summarization common issue sequence sequence models mentioned section apply coverage mechanism rst proposed adapted coverage mechanism keeps track coverage vector computed sum attention distributions previous decoder time steps coverage vector incorporated attention mechanism represents degree coverage words source text received attention mechanism far maintaining coverage vector represents cumulative attention model avoids attending word covered summarization timestep t decoder coverage vector ct sum attention distributions far ct coverage vector contributes computing attention mechanism described previous section equation vt w st wcct et battn intuitively informs attention mechanism s current timestep previous attention information captured ct preventing repeated attention source words discourage repetition penalizes repeatedly attending parts source text dening coverage loss adding primary loss function equation extra coverage loss term penalizes overlap coverage vector ct new attention distribution covlosst ct finally total loss loss t consulted gihub repositories referenced end report covlosst aforementioned models transformers revisit transformers network proposed vaswani machine translation investigate performance abstractive text summarization dataset transformer model encoder maps input sequence symbol representations xn sequence continuous representations z zn given z decoder generates output sequence y symbols element time step model auto regressive consuming previously generated symbols additional input generating transformer follows overall architecture stacked self attention point wise fully connected layers encoder decoder shown left right halves figure respectively encoder architecture mainly stack identical layers sublayers rst multi head self attention mechanism second simple position wise fully connected feed forward network decoder composed stack identical layers addition sub layers encoder layer decoder inserts sub layer performs multi head attention output encoder stack transformer architecture variation attention mechanism called scaled dot product attention input consists queries keys dimension values dimension dv dk result goes dot products query keys calculated divided softmax function obtain weights values practice attention function computed set queries simultaneously packed matrix q keys values packed matrices k v matrix output calculated k v softmax v qk t proposed transformer model instead performing single attention function linearly project queries keys values different times different learned linear projections way build multi head attention projected versions queries keys values perform attention function parallel yielding multi dimensional output values concatenated projected figure transformer model consulted gihub repositories referenced end report experiments dataset overview preprocessing train summarization models use cnn dailymail dataset collection news articles interviews published popular news websites cnn com mail com like common styles newspapers journals article contains highlighted sections form summary article raw dataset includes text contents web pages saved separate html les use cnn dailymail dataset provided deepmind dataset split training dev test set respectively leading training pairs validation pairs test pairs average tokens news article reference summary contains sentences tokens average preprocess dataset convert characters lower case use stanford corenlp library tokenize input articles corresponding reference summaries add graph sentence start end markers s respectively addition tried limiting vocabulary size evaluation metric evaluate models standard rouge recall oriented understudy gisting ation score measure overlap system generated reference summaries report precision recall scores rouge l measure respectively word overlap bigram overlap longest common sequence system generated reference summaries rouge recall precision summarization task calculated rouge recall number overlapping words total words reference summary rouge precision number overlapping words total words system summary system summary refers summary generated summarization model precision s possible measure essentially system summary fact relevant needed recall rouge s possible measure reference summary summarization model generating terms measuring overlapping words equations considering overlap unigrams bigrams longest common sequence leads rouge l scores respectively precision recall experimental details results analysis text summarization work investigate performance summarization models presented section lstm encoder decoder attention mechanism baseline lstm encoder decoder attention pointer generator mechanisms lstm encoder decoder attention pointer generator coverage mechanisms transformers table shows rouge l scores different models trained summarization dataset trained models hyperparameter tuning adagrad optimizer iterations epochs training results outperform similar ones presented cases close case model rouge l precision recall precision recall precision recall table lstm encoder decoder attention mechanism baseline lstm decoder attention pointer generator mechanisms lstm encoder decoder attention pointer generator coverage mechanisms transformers figure validation training loss values v s number iterations summarization models figure shows loss training set validation set function number iterations summarization models iterations epochs results summarization compared case v s ground truth summarization models table seen summary generated model contains unk instead word mysak original summary having attention pointer generator mechanism model replaced unk proper word source text summary model repeated sentence twice summary generated pointer generator coverage mechanism overcome unk problem repetition generated summary gives nice summary pretty close reference summary reference model model model model super typhoon maysak tropical storm mph winds cause ooding landslides problems philippines gained super typhoon status thanks sustained mph winds s classied tropical storm s expected landfall sunday southeastern coast province tropical storm maysak approached asian island nation saturday s classied tropical storm according philippine national weather service s classied tropical storm according philippine weather service days ago maysak gained super typhoon status thanks sustained mph winds s classied tropical storm according philippine national weather service super typhoon weaken new jersey philippine ocean strength people injured including table comparison generated summary summarization models v s ground truth summary generated transformer model capture keywords convey grasp summary fake news detection subsequent summarization use best summarization model trained summarization dataset order create summaries fake news detection dataset build fake news detection model investigate performance input original news text news headline summarized news text generated summarization model basically use text summarizing model feature generator fake news classication model fake news classication article content contains information article headline fake news classier performs better article contents article headlines figure fake news classication architecture body text headline text figure confusion matrix test set fake news detection task different input features summary text input input features exp train acc valid loss body text headline text summary text lstm lstm lstm lstm cells lstm lstm lstm lstm size drop bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm bi lstm lstm lstm lstm lstm train loss valid acc table experiments fake news detection input features accuracy average length words body text headline text summary text table fake news classier results fake news classication use fake news dataset headlines article content provided george mcintire dataset contains fake news articles real articles e balanced dataset politics wide range news sources shufe data use training validation testing fold cross validation build long short term memory lstm network embedding layer shown figure table shows hyperparameter studies fake news classication table shows nal test accuracies input features body text headline text generated summary texts summarization models seen table best model body text input features perform better headline text input furthermore s worth noting summary text input feature leads higher accuracy compared body text input feature nding shows summarization model serves feature generator fake news detection task actually increases accuracy summarization model serve headline generator news articles automatic approach conclusion showed section pointer generator architecture attention coverage nisms led highest accuracies overcome problems common abstractive text summarization vocabulary words repetition furthermore shown section text summarizing model successfully applied feature generator prior classication tasks fake news classication increase accuracy tasks datasciencecentral com proles blogs building fake news classication model references soheil esmaeilzadeh ouassim khebzegga mehrad moradshahi clinical parameters prediction gait disorder recognition soheil esmaeilzadeh dimitrios ioannis belivanis kilian m pohl ehsan adeli end alzheimer s disease diagnosis biomarker identication machine learning medical imaging mlmi pp vol soheil esmaeilzadeh yao yang ehsan adeli end end parkinson disease diagnosis brain mr images cnn org pengxiang cheng katrin erk attending entities better text understanding org hui liu qingyu yin william yang wang explainable nlp generative nation framework text classication org mehdi allahyari seyedamin pouriyeh mehdi asse saeid safaei elizabeth d trippe juan b gutierrez krys kochut text summarization techniques brief survey org bonnie dorr david zajic richard schwartz hedge trimmer parse trim approach headline generation proceedings hlt naacl text summarization workshop pp ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents org chandra khatri gyanit singh nish parikh abstractive extractive text summarization document context vector recurrent neural networks https org shen gao xiuying chen piji li zhaochun ren lidong bing dongyan zhao rui yan abstractive text summarization incorporating reader comments org dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate org ramesh nallapati bing xiang bowen zhou sequence sequence rnns text rization iclr karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend advances neural information processing systems nips abigail peter j liu christopher d manning point summarization pointer generator networks org ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser illia polosukhin attention need https org zhaopeng tu zhengdong lu yang liu xiaohua liu hang li modeling coverage neural machine translation org danqi chen jason bolton christopher d manning thorough examination cnn daily mail reading comprehension task proceedings annual meeting association computational linguistics acl mahnaz koupaee william yang wang wikihow large scale text summarization dataset org chin yew lin marina rey rouge package automatic evaluation summaries text summarization branches acl
