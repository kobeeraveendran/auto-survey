what makes a good summary reconsidering the focus of automatic summarization maartje ter hoeve university of amsterdam m a nl julia kiseleva microsoft research julia com maarten rijke university of amsterdam ahold delhaize m nl c e d l c s c v v i x r a abstract automatic text summarization has enjoyed great progress over the last years now is the time to re assess its focus and objectives does the current focus fully adhere to users desires or should we expand or change our focus we investigate this question ically by conducting a survey amongst heavy users of pre made summaries we find that the current focus of the field does not fully align with participants wishes in response we identify three groups of implications first we argue that it is important to adopt a broader perspective on automatic summarization based on our findings we illustrate how we can expand our view when it comes to the types of input material that is to be summarized the purpose of the summaries and their potential formats second we define requirements for datasets that can facilitate these research tions third usefulness is an important aspect of summarization that should be included in our evaluation methodology we propose a methodology to evaluate the usefulness of a summary with this work we unlock important research directions for future work on automatic summarization and we hope to initiate the development of methods in these directions introduction automatic text summarization has been an important research rection since the early days of the ir and nlp community the often implicit goal of the work on automatic text summarization is to generate a condensed textual version of the original input while preserving the main message of the original this notion is embedded in today s most common ation metrics for the summarization task these metrics computed either automatically or by performing a human evaluation focus on characteristics such as informativeness fluency succinctness and especially recently factuality e in recent years the quality of automatically generated textual summaries has increased tremendously with the rise of neural quence to sequence models e the introduction of formers and self supervised language representation models like bert have given the summarization quality an additional boost e given these positive developments it is important to ask selves what the future directions of automatic summarization should be and whether the current form of automatic summarization aligns with users wishes an important aspect in explicit definitions of the goal of automatic summarization e for ple mani defines this goal as to take an information source extract content from it and present the most important content to the user in a condensed form and in a manner sensitive to the user s or plication s needs in this paper we empirically explore users needs most current automatic text summarization techniques example of automatic text summarization with users desires figure summarization methods that are currently the standard vs example of summarizing while taking users wishes and desires into account and compare them with current efforts for automatic text rization we also examine how we can evaluate the usefulness of a summary in a feasible and comprehensive manner we start our investigation by conducting a survey amongst heavy users of pre made summaries we use the word pre made to tiate these summaries from summaries that people write themselves for example to help them to understand a text e instead pre made summaries are made by someone else e a teacher who writes a summary to help students studying for their exams as automatically generated summaries also fall in this category of pre made summaries they should have the same characteristics as users desire for this category in this research we identify and investigate these characteristics during our investigation we focus on the three classes of context factors defined by jones input factors purpose factors and output factors which describe the input material the purpose of the summary and what the summary should look like respectively figure gives an example figure shows the textual unstructured input and output of most current textual summarization techniques purpose factors are often ignored figure shows an example of how the interpretation of these factors could differ keeping the purpose of the summary for the user in mind output factorspurpose factorsinput factorsthis figure shows the classical approach for textual summarization unstructured textual input is transformed into shorter textual output the usual approach for textual summarization uses unstructured textual input and output output factorspurpose factorsinput factorsother ways of summarizationwe take users wishes into account and might have a more structured input and output more aware of users desires more useful summary we conclude our investigation by carefully examining the cations of our findings by doing so we contribute the following we unveil important and currently underexposed research rections for research on automatic text summarization cerning the input purpose and output factors we define priorities for efforts on dataset collection to be able to study these directions and we propose a new feasible and comprehensive evaluation methodology to evaluate the usefulness of a generated mary the rest of this paper is structured as follows we discuss related work in section in section we describe our survey we present our results in section in section we discuss the implications of our research for future research on automatic text summarization and we conclude in section related work below we introduce the context factors in greater detail and use them to give an overview of methods for automatic text tion we then give an overview of evaluation schemes for automatic summarization we conclude by framing our own position requirements of an automated summary jones argues that one should take the context of a summary into account in order to generate useful summaries a statement that has been repeated by others e to do this in a structured manner jones defines three classes of context factors input factors purpose factors and output factors each of these classes is concerned with a step in the summarization process input factors describe the input material that is to be summarized output factors describe what the generated summary looks like purpose factors are the most important context factors according to jones they describe the purpose of the generated summary jones argues that the purpose factors are often not fully recognized a statement that is still timely at present as we will show in section each context factor class can be divided into more fine grained classes we refer to table in appendix a for an extensive overview automatic text summarization now we discuss recent work on automatic summarization tured around the three context factor classes specifically we nect this work to the fine grained factors that each of the context factor classes can be divided into input factors we start with the factor unit which describes how many sources are to be summarized at once and the factor scale which describes the length of the input data that we are ing these factors are related to the difference between single and multi document summarization e scale plays an important role when material shorter than a single ment is summarized such as in sentence summarization e regarding the genre of the input material we see that most current work on automatic text summarization focuses on the news domain or wikipedia e a smaller body of work addresses different input genres such as scientific articles e forum data e or opinions e the aforementioned differences are closely related to the input factor subject type which describes the difficulty level of the input material the factor medium refers to the input language most research on automatic text summarization is concerned with english as language input although there are ceptions such as chinese e or multi lingual input the last input factor is structure especially in recent neural approaches explicit structure of the input text is often ignored exceptions clude graph based approaches where implicit structure is used to summarize a document e and summarization of tabular data e or screenplays e purpose factors although identified as the most important text factor class by jones and followed by for example mani purpose factors do not receive a substantial amount of attention in work on automatic text summarization there are some exceptions such as query based summarization e question driven summarization e and personalized rization e they take the situation and the audience into account the use cases of the generated summaries are also clearer in these approaches than in typical work on automatic text marization output factors we start with the output factors style and terial the latter is concerned with the degree of coverage of the summary most generated summaries have an informative style and cover most of the input material there are exceptions for example the xsum dataset constructs summaries of a single sentence and is therefore more indicative in terms of style and inevitably less of the input material is covered not many summaries have a critical or aggregative style aggregative summaries put different source texts in relation to one another to give an overview of a topic currently most popular summarization techniques focus on a running format work on template based summarization follows a more headed structured format e falke and gurevych introduce a more structured format in the form of concept maps and wu et al make knowledge graphs there is also a small body of work on multi modal summarization which has a more structured output e the difference between abstractive and extractive summarization is likely the best known distinction in output type e although it is not entirely clear which output factor best describes the difference evaluation evaluation methods for automatic text summarization can be ed in different ways one way is in intrinsic vs extrinsic evaluation methods intrinsic methods evaluate the model itself for ple on informativeness or fluency e extrinsic methods target how well the summary performs when used for a certain task e extrinsic methods require a lot of resources which explains the popularity of intrinsic methods another popular way to distinguish different types of tion metrics is between automatic and human evaluation over the years different automatic metrics have been proposed rouge which is most popular evaluates on lexical similarity the recently proposed bertscore evaluates on semantic similarity wang et al introduce an automatic extrinsic way of evaluating erated summaries by automatically generating questions about an input document and answering these questions based on the summary a similar approach is proposed by durmus et al most human evaluation approaches evaluate intrinsic factors such as informativeness readability and conciseness factors that are difficult to evaluate automatically there are some examples of extrinsic human evaluation methods where judges are asked to perform a certain task based on the summary examples are relevance assessment where the relevance of a document for a certain topic is judged based on its summary e and reading comprehension such as question answering e our position we conclude this related work section by stating how our work relates to the context factors and evaluation metrics context factors in this work we empirically investigate the needs and desires of heavy users of pre made summaries when it comes to the input purpose and output factors we do this by conducting a survey amongst this group of people the outcomes of this survey allow us to identify underexposed context factors and by doing so to reveal important and exciting research directions for future work on automatic summarization evaluation we return the observation by jones that the purpose factors are not explicitly addressed in most work on matic summarization in some cases one can justify this when the usefulness of a generated summary is less important for example when the objective is to test a certain model architecture or when it is not precisely known who will use the summary defined by the purpose factor situation however we agree with jones that the purpose factors deserve more attention as a next step we should also explicitly evaluate generated summaries on their usefulness for the intended use cases so far usefulness is not evaluated in a ble and comprehensive manner the few existing metrics are often either very resource demanding and too task specific e or too little specific and hence ignoring the purpose factors e moreover these metrics are ignored by most current work on automatic summarization in this paper we aim to bridge the gap by introducing a feasible and comprehensive evaluation methodology to evaluate usefulness method here we describe the participants of our survey section and our survey procedure section participants we recruited our participants among university students this group is particularly well suited for our investigation as sity students are heavy users of pre made summaries for example during exam preparations their extensive experience with made summaries makes them experts in this area and therefore they can be expected to have strong and grounded opinions on this topic because of their expertise we can use their identified requirements as a reliable starting point to broaden our focus on automatic summarization we also considered two different strategies for choosing the participant pool no restrictions on background summarization usage and investigating a number of different target groups study levels study backgrounds figure participant details figure overview of survey procedure the first would not work as it would be unclear how to compare experiences from different potentially unexperienced participants and hence it would not lead to reliable or actionable conclusions although the second setup would give us more data points it comes at the cost of making our study unnecessarily cluttered whereas not adding much generalizability we recruited participants by contacting ongoing courses and dent associations and through advertisements on internal student websites as an incentive we offered a ten euro shopping voucher to ten randomly selected participants a total of participants started the survey and completed the full survey resulting in a completion rate we only include participants who completed the study in our analysis participants spent minutes on average on the survey in the final part of our survey we ask participants to indicate their current level of education and main field of study the details are given in figure survey procedure figure shows a brief schematic overview of our survey procedure a detailed account is given in appendix b figure we arrived at this version of the survey after a number of initial pilot runs where we ensured participants understood their task and all questions we ran the survey with surveymonkey com the entire survey with the exact formulation of the instructions questions and answer options is attached in appendix c to ensure reproducibility our survey can also be re used to inquire different target groups with slight modifications to match the target group for example the current framing around study activities can easily be adapted to activities representative for another target group introduction the survey starts with an introduction in which we explain to participants what to expect how we process the data and that participation is voluntarily after participants agree with this an explanation of the term pre made summary follows as we do not want to bias participants by stating that the summary was automatically generated we explain that this summary can be made by anyone e a teacher a good performing fellow student the authors of the original material or a computer recall from tion that an automatically generated summary is also a pre made summary and for this reason our survey identifies the istics a good automatically generated summary should have we also give some examples of types of pre made summaries as based busin introductioncontext factorsrememberedfuture featuresclosingimagined on the feedback from our initial pilot experiments we noticed that participants were missing this information we explicitly state that these are just examples and that participants can come up with any type of summary themselves context factors in the main part of our survey we focus on the context factors first we ask participants whether they have made use of a pre made summary in one of their recent study activities if so we ask them to choose the study activity where a summary was most useful we call this group of participants the remembered group as they describe an existing summary from memory if participants indicate that they have not used a made summary in one of their recent study activities we ask them whether they can imagine a situation where a pre made summary would have been helpful if not we ask them to explain their answer and lead them to the final background questions and closing page if yes we ask them to keep this imaginary situation in mind for the rest of the survey we call this group the imagined group now we ask the remembered and imagined groups about the input purpose and output factors of the summary they have in mind we ask questions for each of the subcategories of the context factors that we discussed in section and that can be found in the overview in appendix a as well at this point the two groups are in different branches of the survey the difference between the branches is mainly linguistically motivated in the imagined group we use verbs of probability instead of asking them to describe an existing situation a number of questions can only be asked in the remembered group e how helpful the summary was for the first question of the context factors part we ask ticipants what the study material consisted of we give them a number of options as well as an other checkbox to avoid sition bias all answers options for multiple choice and multiple response questions in the survey are randomized with the other checkbox always as the last option if participants do not choose the mainly text option for this first question we tell them that we cus on textual input in the current and we ask them whether they can think of a situation where the input material consisted of text if not we lead participants to the background questions and closing page if yes they proceed to the remaining questions that give us a full overview of the input purpose and output factors of the situation that participants have in mind finally we ask the remembered group to suggest how their described summary could be turned into their ideal summary we then ask both groups for any final remarks about the summary or input material trustworthiness and future features questions both groups are then led to some exploratory questions we add these questions to get some initial understanding of the trust users would have in machine generated summaries and to get some preliminary ideas for the interpretation of the context factors in a less standard setting but these questions are not the main focus of this research for the first set of questions we tell participants to imagine that the summary was made by a computer but contained all the needs that were identified in the previous part of the survey we then ask them questions about trust in computer versus human generated acknowledge that other modalities as well as a mixture of modalities are important to investigate but leave this for future work to ensure clarity in our results table different levels of investigation we did not find nificant differences for but add it here for completeness remembered branch vs imagined branch all respondents together different study fields different study levels different levels of how helpful the summary was according to participants rated on a point likert scale note that only the remembered group answered this question summaries as a next step we ask participants to imagine that they could interact with the computer program that made the summary in the form of a digital assistant we tell them not to feel restricted by the capabilities of today s digital assistants the full scenario sketch can be found in appendix c we then ask participants to select the three most useful and the three least useful features for the digital assistant to have in this scenario in a similar fashion as in ter hoeve et al results here we present the outcomes for each survey question and ine them at different levels summarized in table for space and clarity reasons we present the results on a per group level when interesting differences are found otherwise we present the results of all respondents together we use the question formulation as used for the remembered group and abbreviate the answer options identifying branches of our participants indicated that they had used a pre made summary before and hence they were led to the remembered branch of the remaining responded that they could think of a situation where a pre made summary would be useful for them they were led to the imagined branch we asked the few remaining participants why they could not think of a situation people answered that they would not trust a pre made summary and that making a summary themselves helped them with their study activities previous work has indeed found that writing summaries can help with tasks such as reading comprehension e input factors figure shows the results for the input factor questions here we highlight some particularly noteworthy results first we see that textual input is significantly more popular than the other input types figure this result is based on participants initial sponses and not on the follow up question if they selected another option than text this stresses the relevance of automatic text summarization furthermore participants described a very diverse input for the factors scale and unit figure much more diverse than the classical focus of automatic text summarization figure shows that most input material had a considerable amount of ture typically this structure is discarded in work on automatic summarization even though it can be an important source of mation we discuss the implications of these findings in section medium the study material consisted of mc scale unit what was the length of the study material mc genre what was the genre of the study material mc subject type how would you classify the difficulty level of the study material mc e structure how was the study material structured mr figure results for the input factor questions specific input factor in italics answer type in brackets mc multiple choice mr multiple response indicates significance after bonferroni correction with if two options are flagged with these options are not significantly different from each other yet both have been chosen significantly more often than the other options purpose factors figure shows the results for the purpose factor questions again we highlight a number of particularly interesting results first for the purpose factor audience figure we asked how much main knowledge was or should be expected from the readers of the summary we find that people selected the targeted level and targeted level option i e a lot or full domain knowledge significantly more often than the other options this corresponds to the result we found in the previous section for the difficulty level of the input most participants described specialized input material figure as our participant pool consists of students this is rather unsurprising however for other target groups the objective could be to rather make a summary for people without a lot of domain knowledge based on very specialized input material the main takeaway here is that one should make sure that the expected difficulty level of the summary is aligned with the users expectations in our example we can see this was the case in ure we show the results split based on how helpful participants indicated the summary was we can see that targeted level i e almost no domain knowledge was mostly perceived as not helpful although this result is not significant with a fisher s exact it denotes a trend that is worth paying attention to when designing future summarization models is more suitable in this case than a standard test due to the small sample size for some options in figure we find how the summary helped participants with their task we show the results for the remembered and the imagined group a first interesting observation from our data i e not noted in figure is that participants in the imagined group ticked more boxes than participants in the remembered group vs per participant on average this is a first observation that shows the wide variety of potential use cases of pre made summaries secondly we find that the imagined group chose the option fresh memory and overview more often than the remembered group fisher s exact test although this result is not significant after a bonferroni correction to correct for the number of tests we think this can inspire interesting future research directions when it comes to defining the purpose factors for a generated summary in general we can see that many different use cases were very popular whereas current research on automatic summarization is mostly concerned with simply giving an overview of the input material this is an important observation that we should use to broaden our vision of the automatic summarization research participants reported that the summary was helpful or very ful figure which allows us to draw valid conclusions from the results of this survey in section we discuss the implications of we do not find any significant differences in the overall results when we exclude these few participants who did not find their summary helpful and we do not find many correlations w t how helpful a summary was and a particular context factor we choose to include all participants in the analysis regardless of how helpful they found their summary of respondentsmainly textmainly figuresmainly videomainly resps of respondentssingle articlemult articlessingle book chapt mult chapt same bookmult chapt various knowotherscale resps of resps of respondentsordinaryspecializedgeographicallybasedsubject resps of respondentsnonetitle titlessubheadingschapterssections and resps situation what was the goal of this study activity mc situation who made this pre made summary mc only if remembered situation the summary was made specifically to help me and potentially my fellow students with my study activity ls only if remembered audience for what type of people was the summary intended ls e use how did this summary help you with your task mr use overall how helpful was the made summary for you ls only if bered figure results for the purpose factor questions specific purpose factor in italics answer type in brackets mc multiple choice mr multiple response ls likert scale indicates significance after bonferroni correction with with indicates noteworthy results where significance was lost after correction for the number of tests if two options are flagged these options are not significantly different from each other yet both were chosen significantly more often than the other options our findings for the research on automatic summarization regarding the purpose factors in more detail output factors figure shows the results for the output factor questions textual summaries were significantly more popular than the other summary types figure this again stresses the importance of automatic text summarization when we investigate the results for the described coverage of the summary figure we find that most participants indicated that the summary covered or should cover most of the input material we split these results based on how helpful the summary was for participants and find that summaries that only covered some of the input material were significantly less helpful for participants than summaries with more coverage this is in line with the findings that most participants used the summary to study for an exam section purpose factors figure studying for an exam likely requires an overview of the full study material this result shows that in agreement with jones the purpose factors are extremely important in order to define the output factors for the output factor style we find a fascinating difference tween the remembered and imagined group figure whereas the remembered group described significantly more often an formative summary the imagined group opted significantly more often for a critical or aggregative summary most research on tomatic summarization focusses on informative summaries only this result opens up very exciting directions for future research the results for the described output structure of the summary figure are also very important and insightful participants scribed a substantially richer format of the pre made summaries than is adopted in most research on automatic summarization stead of consisting of just a running text the vast majority of ipants indicated that the summary contained or should contain all kinds of structural elements such as special formatting diagrams headings moreover we find that participants in the imagined group ticked more boxes on average than participants in the membered group vs per participant indicating a desire for structure in the generated summaries this is supported by the answers to the open ended question where we asked participants in the remembered group what would be needed to optimize the described summary we discuss these results in the next paragraph of respondentsstudying for resps of respondentsteacher tafellow studentofficial orga the authors oforig materialcomputer knowothersummary of respondentsstrongly disagreedisagreeneither agreenor disagreeagreestrongly agreei do know made to help of respondentsuntargeted of beredimagi of a format what was the type of the mary mc format how was the summary tured mr material how much of the study rial was covered by the summary ls style what was the style of this mary mc figure results for the output factor questions specific output factor in italics answer type in brackets mc multiple choice mr multiple response ls likert scale indicates significance or fisher s exact test after bonferroni tion with with the results we found for the output factors unlock many future research directions which again indicates that we should widen our focus on the automatic summarization research we discuss this in more detail in section open answer questions we asked the participants who scribed an existing summary how this summary could be formed into their ideal summary of the participants who filled out this question made suggestions many of these suggestions are centered around adding additional structural elements to the summary like figures diagrams or tables one of the participants wrote an ideal summary is good enough to fully replace the original often longer texts contained in articles that need to be read for exams the main purpose behind this is speed of learning from my experience more tables graphs and visual representations of the study material and key concepts links would improve the summary as i would faster comprehend the study material another participant wrote more images graphs to have some changes from just studying from text perhaps some online video material about the most difficult parts participants also indicated a desire for more structure in the mary text itself for example by adding headings or color codings one participant wrote colors and a key for color coding different sections such as definitions on the left maybe and then the rest of the page reflects the structure of the course material with notes on the readings that have many headings and subheadings another theme that can be distilled from participants answers is the desire to have more examples in the summary one participant wrote more examples i think for me personally i need examples to understand the material now i needed to imagine them myself some participants wrote that they would like to have a more personalized summary i d highlight some things i find difficult so i d personalise the summary more another participant wrote make it more personalized may be these notes were by another student i might have focussed more on some parts and less on others trustworthiness and future features in this section we report the results for the exploratory questions that we asked about the trustworthiness of a summary generated by a machine versus a human as well as the results for the questions about features for summarization with a digital voice assistant we find that participants are divided on the question whether it would make a difference to them whether the summary was generated by a machine or a computer if we look at all participants together we find that of the participants answered that it would make a difference whereas answered that it would not however if we split the participants based on study background an interesting difference emerges figure participants with a background in stem indicated significantly more often that it would not make a difference to them whereas the other groups of students indicated the opposite almost all participants who of respondentslecture notesblog posthighlightsabstractive textshort videoslide showothersummary resps of respondentsrunning texthighlightsspecialformattingdiagramstablesgraphsfiguresheadingssections of respondentsnonealmost nonesomemostallsummary of respondentsinformativeindicativecriticalaggregativeothersummary would it make a difference to you whether the summary was generated by a computer program or by a human mc which type of summary would you trust more mc please choose the three most useful tures for a digital assistant to have in this scenario mr please choose the three least useful tures for a digital assistant to have in this scenario mr figure results for the future feature questions answer type in brackets mc multiple choice mr multiple response indicates significance or fisher s exact test after bonferroni correction with answered that it would make a difference said that they would not trust a computer on being able to find the relevant information i e all seemed to favor the human generated summary only one participant advocated for the computer generated summary as a computer is more objective almost all participants who said it would not matter to them did add the condition that the quality of the generated summary should be as good as if a human had generated it similar to the observation reported in vtyurina et al for automatic systems for conversational search one person wrote if the summary captures all previously discussed elements it is effectively good for the same purpose so then it does not matter who generated it this comment exactly captures the motivation of the setup of our survey this caution regarding automatically generated summaries is confirmed by the question where we asked which type of summary participants would trust more a human generated one or a chine generated one people chose the human generated summary significantly more often figure this also holds for the pants with a stem background which aligns with the responses to the open questions we reported earlier apparently participants do not fully trust that the condition they raised earlier would be satisfied namely that only if the machine was just as good as the human it would not matter for them whether the summary was generated by a machine or a computer the results for the most and least useful features for a digital assistant in a summarization scenario are given in figure and adding more details to the summary and answering specific tions based on the content of the summary are very popular features whereas summarizing parts of the input material with less detail is not very popular lastly we asked participants whether they could think of any other features that they would like their digital assistant to have in the outlined scenario a number of participants answered that they would like the digital assistant to generate questions based on the summary so that they could test their own understanding e one participant said make questions for me to test me and another participant had a related comment maybe the the digital assistant could find old exam questions to link to parts of the summary where the question is related to so that there is a function to test if you ve understood the summary another line of answers pointed towards giving explicit relations between the input material and summary for example show links between subject materials and what their relation is and another person wrote dynamic linking from summary to original source is a great added value of generating a summary of respondentsyesnodifference computer soc of respondentshuman genmachine genno differencetrust soc of respondentssummarizemore detailedsummarizeless detailedswitch summarystylesexplainsummaryprovide sourcesearch relatedsourcesanswer questionsmost useful resps of respondentssummarizemore detailedsummarizeless detailedswitch summarystylesexplainsummaryprovide sourcesearch relatedsourcesanswer questionsleast useful resps implications in the previous section we have presented the results of our survey and we have discussed our interpretation of these findings on a question basis here we want to take the opportunity to summarize our findings as general implications for future research efforts we explicitly do not argue that current research efforts on matic summarization are invalid on the contrary we are excited to see the great progress of the last years instead we argue that our results help to spark the development of methods and datasets that facilitate a more encompassing range of summaries here we first discuss the implications of our empirical findings for the context factors of summaries and automatic summarization methods this will then lead us to answer what this means for dataset collection efforts finally we move our focus to evaluation and propose a new evaluation methodology to evaluate usefulness context factors input factors as noted in section the input factors are best represented from all context factors in current research efforts on automatic summarization despite this the vast majority of search focuses on english news or wikipedia summarization while stripping away all cues other than raw text in order to serve a wider audience and to test the generalization capabilities of current proaches we argue that a wider range of input factors is necessary our survey results lead to concrete suggestions different styles difficulty levels and including structure the feasibility of this is strongly dependent on available datasets therefore we focus on the implications for dataset collection efforts later in this section purpose factors automatically generated summaries can serve a wide variety of use cases current research efforts often serve the purpose of improving model performance on evaluation metrics such as rouge leaving the intended use cases implicit we see great value in this type of research as the precise users are not always known and believe that our research can inspire new tions for summarization that can contribute to the challenging goal of making strong performing models with a good understanding of the input that generalize to a wide variety of situations nevertheless we also echo jones the purpose factors serve more attention as the ideal format of the summary highly depends on them who will use the summary why and in what scenario our results inspire many ways of taking the purpose factors into account we need to evaluate whether our generated summaries are indeed useful for the intended users as there is no comprehensive way of evaluating this yet we propose an evaluation methodology to evaluate usefulness at the end of this section output factors the results of our survey inspire the generation of different summary types that are different than the current dard we are particularly excited about the implications for the summary format and style including more structure in the maries for example by means of diagrams or figures as well as by explicitly adding relations between input text and summary or between parts of the summary as requested by users requires a thorough understanding of the input text the many recent cations on model hallucinations e show that there are still many challenging and exciting research questions in this area dataset requirements one important requirement for the execution of the defined search directions is the availability of appropriate datasets as tioned in section there are some datasets available that are unlike most of the existing datasets especially in terms of input factors these datasets deserve a stronger recognition and research focus on top of that we hope to inspire dataset collection efforts that clude a more encompassing range of context factors and especially output factors more datasets with an explicit use of structural elements such as diagrams headers and explicit formatting we strongly encourage keeping the purpose factors closely in mind during these collection efforts usefulness as evaluation methodology following jones and mani we argue that only a correct choice of context factors will result in a useful summary for users it is important to explicitly evaluate this usefulness in our survey we found that participants mostly found their described summaries very helpful yet it was hard to define a single factor that makes a summary particularly helpful instead it is the combination that counts therefore usefulness can best be evaluated with a human evaluation as existing metrics are very resource demanding e or not comprehensive enough e here we propose a feasible and comprehensive method to evaluate usefulness first from the purpose factors the intended use factors of the summary need to be identified next the output factors need to be evaluated on these use factors for this we take inspiration from research on simulated work tasks evaluators should be given a specific task to imagine e writing a news article or studying for an exam this task should be relatable to the evaluators so that reliable answers can be obtained with this task in mind evaluators should be asked to judge two summaries in a pairwise manner on their usefulness in the following format the output factor of which of these two summaries is most useful to you to use factor an example of such a question would be the style of which of these two summaries is most useful to you to substitute a chapter that you need to learn for your exam preparation as with all human evaluations it is important to ensure that judges understand the meaning of each of the evaluation criteria e style and substitute in the example we give example questions for each of the remaining output and use factors in appendix d conclusion in this paper we have empirically investigated the desiderata of users of automatic summaries by means of a survey amongst heavy users of pre made summaries we focused on three classes of tial context factors input purpose and output factors we identified that purpose factors are often under addressed and found that users desiderata deviate especially from the current focus of automatic summarization research when it comes to the output factors based on these findings we identified important future research directions and requirements for efforts on dataset collection we also proposed a new methodology to evaluate the usefulness of automatically generated summaries our study opens new tant future directions to enhance further research on automatic summarization acknowledgments we thank ana lucic for her helpful feedback this research was supported by the nationale politie all content represents the ion of the authors which is not necessarily shared or endorsed by their respective employers sponsors references reinald kim amplayo and mirella lapata unsupervised opinion rization with noising and denoising arxiv preprint abdelkrime aries walid khaled hidouci al automatic text tion what has been done and what has to be done arxiv preprint pia borlund the iir evaluation model a framework for evaluation of interactive information retrieval systems information research pia borlund a study of the use of simulated work task situations in interactive information retrieval evaluations journal of documentation ziqiang cao wenjie li sujian li and furu wei retrieve rerank and rewrite soft template based neural summarization in proceedings of the annual meeting of the association for computational linguistics volume long papers jianpeng cheng and mirella lapata neural summarization by extracting sentences and words arxiv preprint sumit chopra michael auli and alexander m rush abstractive sentence summarization with attentive recurrent neural networks in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian a discourse aware attention model for abstractive summarization of long documents arxiv preprint yang deng wenxuan zhang and wai lam multi hop inference for question driven summarization arxiv preprint jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language understanding arxiv preprint bonnie dorr christof monz stacy president richard schwartz and david zajic a methodology for extrinsic evaluation of text summarization does rouge correlate in proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation summarization duc duc documents tasks and measures esin durmus he he and mona diab feqa a question answering uation framework for faithfulness assessment in abstractive summarization arxiv preprint tobias falke and iryna gurevych bringing structure into maries crowdsourcing a benchmark corpus of concept maps arxiv preprint sebastian gehrmann yuntian deng and alexander rush bottom up abstractive summarization in proceedings of the conference on empirical methods in natural language processing ben goodrich vinay rao peter j liu and mohammad saleh assessing the factual accuracy of generated text in proceedings of the acm sigkdd international conference on knowledge discovery data mining karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend in advances in neural information processing systems baotian hu qingcai chen and fangze zhu lcsts a large scale chinese short text summarization dataset arxiv preprint k sparck jones automatic summarizing factors and directions in advances in automatic text summarization number mit press cambridge mass usa mahnaz koupaee and william yang wang wikihow a large scale text summarization dataset arxiv preprint faisal ladhak esin durmus claire cardie and kathleen mckeown ilingua a new benchmark dataset for cross lingual abstractive summarization arxiv preprint chin yew lin rouge a package for automatic evaluation of summaries in text summarization branches out marina litvak and natalia vanetik query based summarization using mdl principle in proceedings of the multiling workshop on summarization and summary evaluation across source types and genres peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer generating wikipedia by summarizing long sequences arxiv preprint yang liu and mirella lapata text summarization with pretrained encoders arxiv preprint hans peter luhn the automatic creation of literature abstracts ibm journal of research and development inderjeet mani automatic summarization vol john benjamins ing inderjeet mani summarization evaluation an overview rbert mro et al personalized text summarization based on important terms identification in international workshop on database and expert systems applications ieee ramesh nallapati feifei zhai and bowen zhou summarunner a current neural network based sequence model for extractive summarization of documents in aaai shashi narayan shay b cohen and mirella lapata do nt give me the details just the summary topic aware convolutional neural networks for extreme summarization arxiv preprint shashi narayan shay b cohen and mirella lapata ranking sentences for extractive summarization with reinforcement learning arxiv preprint preksha nema mitesh m khapra anirban laha and balaraman ravindran diversity driven attention model for query based abstractive summarization in proceedings of the annual meeting of the association for computational linguistics volume long papers ani nenkova and rebecca j passonneau evaluating content selection in summarization the pyramid method in proceedings of the human language nology conference of the north american chapter of the association for computational linguistics hlt naacl pinelopi papalampidi frank keller lea frermann and mirella lapata screenplay summarization using latent narrative structure arxiv preprint romain paulus caiming xiong and richard socher a deep reinforced model for abstractive summarization arxiv preprint p david pearson linda fielding al comprehension instruction book of reading research giuseppe riccardi frederic bechet morena danieli benoit favre robert gaizauskas udo kruschwitz and massimo poesio the sensei project making sense of human conversations in international workshop on future and emerging trends in language technology springer alexander m rush sumit chopra and jason weston a neural attention model for abstractive sentence summarization arxiv preprint evan sandhaus the new york times annotated corpus linguistic data consortium philadelphia abigail see peter j liu and christopher d manning get to the point summarization with pointer generator networks arxiv preprint jiwei tan xiaojun wan and jianguo xiao abstractive document rization with a graph based attentional neural model in proceedings of the annual meeting of the association for computational linguistics volume long papers maartje ter hoeve robert sim elnaz nouri adam fourney maarten rijke and ryen w white conversations with documents an exploration of document centered assistance in proceedings of the conference on human information interaction and retrieval naushad uzzaman jeffrey p bigham and james f allen multimodal summarization of complex sentences in proceedings of the international conference on intelligent user interfaces ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in advances in neural information processing systems michael vlske martin potthast shahbaz syed and benno stein tl dr mining reddit to learn automatic summarization in proceedings of the workshop on new frontiers in summarization alexandra vtyurina denis savenkov eugene agichtein and charles la clarke exploring conversational search with humans assistants and wizards in proceedings of the chi conference extended abstracts on human factors in computing systems alex wang kyunghyun cho and mike lewis asking and answering questions to evaluate the factual consistency of summaries arxiv preprint lu wang hema raghavan vittorio castelli radu florian and claire cardie a sentence compression based framework to query focused multi document summarization arxiv preprint zeqiu wu rik koncel kedziorski mari ostendorf and hannaneh hajishirzi extracting summary knowledge graphs from long documents arxiv preprint jiacheng xu zhe gan yu cheng and jingjing liu discourse aware neural extractive text summarization in proceedings of the annual meeting of the association for computational linguistics association for computational linguistics michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan vasan and dragomir radev graph based neural multi document rization in proceedings of the conference on computational natural language learning conll shuo zhang zhuyun dai krisztian balog and jamie callan rizing and exploring tabular data in conversational search arxiv preprint tianyi zhang varsha kishore felix wu kilian q weinberger and yoav artzi bertscore evaluating text generation with bert arxiv preprint junnan zhu haoran li tianshang liu yu zhou jiajun zhang chengqing zong et al msmo multimodal summarization with multimodal output a overview context factors table overview of different context factors classes defined by jones with descriptions of the factors within these classes input factors form purpose factors situation output factors material structure how is the input text structured e subheadings rhetorical patterns tied it is known who will use the summary for what purpose and when covering the summary covers all of the important information in the source text scale how large is the input data that we are summarizing e a book a chapter a single article floating it is not exactly known who will use the summary for what purpose or when partial the summary intentionally covers only parts of the important information in the source text medium what is the input language type e full text telegraphese style this also refers to which natural language is used audience genre what type of literacy does the input text have e description narrative targetted a lot of domain knowledge is pected from the readers of the summary running the summary is formatted as an abstract like text subject type untargetted no domain knowledge is pected from the readers of the summary headed the summary is structured ing a certain standardised format with ings and other explicit structure format style specialized you need to speak the jargon to understand this input type retrieving use the summary to retrieve source text informative the summary conveys the raw information that is in the source text previewing use the summary preview a text that one is about to read indicative the summary just states the topic of the source text nothing more ordinary everyone could understand this input type use restricted the input type text is only standable for people familiar with a certain area for example because it contains local names unit single only one input source is given substitutes use the summary as a substitute for the source text critical the summary gives a critical review of the merits of the source text refreshing use the summary to refresh ones memory of the source text aggregative different source texts are put in relation to one another to give an overview of a certain topic multi multiple input sources are given prompts use the summary as action prompt to read the source text b survey overview figure overview survey design used pre made you think of of of human vs assistance a pre made summaryimagine a pre made summaryfuture feature questionsclosing questions c verbatim survey overview table a complete overview of the survey this table includes the explanation that participants received as well as all the questions and the answer options if a question was the start of a branch the direction of the branch has been written behind the answer options in italic this was never shown to the participants note that the survey was performed in surveymonkey the survey had a lay out as provided by surveymonkey i e it consisted of different pages and colors were used to highlight certain important parts in texts question nr question and answer options introduction and instructions thank you for taking the time to fill out this survey before you start please take the time to read these instructions carefully if you still have any questions after reading the instructions please send them to anonymized we will give away anonymized vouchers of anonymized currency each among the participants if you would like to take part in the raffle you can leave your email address at the end of this survey goal of the study what the survey will look like the goal of this survey is to get insight in how summaries help or can help you when studying in what follows you will get questions that aim to develop an understanding for for which types of study material it is useful to have summaries how these summaries can help you with your task what these summaries should look like we expect this survey to take approximately minutes of your time use the next button to go to the next page once you have filled out all the questions on the page use the prev button to go back one page about your privacy we value your privacy and will process your answers anonymously the answers of all participants in this survey will be used to gain insight in how pre made summaries can be helpful for different types of studying activities the answers will be presented in a research paper about this topic this will be done either in an aggregated manner or by citing verbatim examples of the answers again this will all be done anonymously i agree that i have read and understood the instructions i also understand that my participation in this survey is voluntarily i agree important some background knowledge you need to know throughout this survey we make use of the term pre made summary it is very important that you understand what this means on this page we explain this term so please make sure to read this carefully com question nr question and answer options definition pre made summary one type of summary is one that you make yourself another type of summary is one that has been made for you in this survey we focus on this latter type and we call them pre made summaries who makes these pre made summaries these pre made summaries can be made by a person for example your teacher your friend a fellow student or someone at some official organisation the pre made summaries can also be made by a computer what kinds of summaries are we talking about there are no restrictions on what these pre made summaries can look like on the contrary that is one of the things we aim to find out with this survey but to give some examples you could think of a written overview of a text book highlights in text to draw your attention to important bits blog posts these are really just examples and do nt let them limit your creativity you can come up with any example of a pre made summary that is helpful for you yes i understand what a pre made summary is yes please think back to your recent study activities examples of study activities can be studying for an exam writing a paper doing homework exercises note that these are just examples any other study activity is fine too did you use a pre made summary in any of these study activities yes participants are led to no participants are led to can you think of one of your recent study activities where a pre made summary would have been useful for you yes participants are led to no participants are led to why do you think a pre made summary would not have helped you with any of your recent study activities open response participants are led to start branch of participants who described an existing summary if you have multiple study activities where you used a pre made summary please take the one where you found the pre made summary most useful the original study material consisted of mainly text participants are led to mainly figures participants are led to mainly video participants are led to mainly audio participants are led to a combination of some or all of the above participants are led to i do not know because i have not seen the study material participants are led to other please specify participants are led to question nr question and answer options for now we narrow down our survey to study material that is mostly textual do you recall any other recent study activity where you made use of a pre made summary and where the original study material mainly consisted of text yes participants are led to no participants are led to what was the goal of this study activity studying for an exam writing a paper essay report doing homework exercises other please specify who made this pre made summary a teacher or teaching assistant a fellow student an official organisation the authors of the original study material a computer program i am not sure i found it online other please specify now some questions will follow about what the study material that was summarized looked like what was the length of the study material a single article multiple articles a single book chapter multiple book chapters from the same book multiple book chapters from various books a combination of the above i do not know because i have not seen the study material only the summary other please specify how was the study material structured multiple answers possible there was no particular structure e just one large text the text contained a title or titles the text contained subheadings the text consisted of different chapters the text consisted of different sections and or paragraphs i do not know because i have not seen the study material only the summary other please specify question nr question and answer options what was the genre of the study material mainly educational such as a text book chapter mainly scientific such as an academic article publication report mainly nonfiction writing such as history books mainly fiction writing such as novels short fictional stories other please specify how would you classify the difficulty level of the study material ordinary most people would be able to understand it specialized you need to know the jargon of the field to be able to understand it geographically based you can only understand it if you are familiar with a certain area for example because it contains local names now we will ask some questions about the purpose of the pre made summary that you used the summary was made specifically to help me and potentially fellow students with my study activity strongly disagree disagree neither agree nor disagree agree strongly agree i do nt know for what type of people was the summary intended your score can range from untargetted to targetted untargetted no domain knowledge is expected from the users of the summmary targetted full domain knowledge is expected from the users of the summmary question nr question and answer options how did this summary help you with your task multiple answers possible the summary helped to retrieve parts of the original study material i used the summary to preview the text that i was about to read i used the summary as a substitute for the original study material i used the summary to refresh my memory of the original study material i used the summary as a reminder that i had to read the original study material the summary helped to get an overview of the original study material the summary helped to understand the original study material other please specify what was the type of the summary lecture notes blog post highlights of some kind in the original study material abstractive piece of text such as a written overview of a text book an abstract of a paper short video a slide show other please specify how was the summary structured multiple answers possible the summary was a running text without particular structure the summary consisted of highlights in the original study material without particular structure the summary itself contained special formatting such as bold or cursive text highlights the summary contained diagrams the summary contained tables the summary contained graphs the summary contained figures the summary contained headings the summary consisted of different sections paragraphs other please specify how much of the study material was covered by the summary none of the study material was covered almost none of the study material was covered some of the study material was covered most of the study material was covered all of the study material was covered question nr question and answer options what was the style of this summary informative the summary simply conveyed the information that was in the original study material indicative the summary gave an idea of the topic of the study material but not more critical the summary gave a critical review of the study material aggregative the summary put different source texts in relation to one another and by doing this gave an overview of a certain topic other please specify overall how helpful was the pre made summary for you your score can range from not helpful at all to very helpful not helpful at all very helpful imagine you could turn this summary into your ideal summary what would you change is there anything else you want us to know about the summary that we have not covered yet open response open response is there anything else you want us to know about the original study material that we have not covered yet open response participants are led to start branch of participants who described an imagined summary please take one of these study activities in mind and imagine you would have had a pre made summary the original study material consisted of mainly text participants are led to mainly figures participants are led to mainly video participants are led to mainly audio participants are led to a combination of some or all of the above participants are led to other please specify participants are led to question nr question and answer options for now we narrow down our survey to study material that is mostly textual do you recall any other recent study activity where you could have used a pre made summary and where the original study material mainly consisted of text yes participants are led to no participants are led to now some questions will follow about what the study material that could be summarized looked like what was the goal of this study activity studying for an exam writing a paper essay report doing homework exercises other please specify what was the length of the study material a single article multiple articles a single book chapter multiple book chapters from the same book multiple book chapters from various books a combination of the above other please specify how was the study material structured multiple answers possible there was no particular structure e just one large text the text contained a title or titles the text contained subheadings the text consisted of different chapters the text consisted of different sections and or paragraphs other please specify what was the genre of the study material mainly educational such as a text book chapter mainly scientific such as an academic article publication report mainly nonfiction writing such as history books mainly fiction writing such as novels short fictional stories other please specify question nr question and answer options how would you classify the difficulty level of the study material ordinary most people would be able to understand it specialized you need to know the jargon of the field to be able to understand it geographically based you can only understand it if you are familiar with a certain area for example because it contains local names now we will ask some questions about the purpose of the pre made summary that would have been helpful for what type of people should the summary ideally be intended your score can range from untargetted to targetted untargetted no domain knowledge is expected from the users of the summmary targetted full domain knowledge is expected from the users of the summmary how would this summary help you with your task multiple answers possible the summary would help to retrieve parts of the original study material i would use the summary to preview the text that i was about to read i would use the summary as a substitute for the original study material i would use the summary to refresh my memory of the original study material i would use the summary as a reminder that i had to read the original study material the summary would help to get an overview of the original study material the summary would help to understand the original study material other please specify now we will ask some questions about what the summary should look like and cover what would be the ideal type of the summary lecture notes blog post highlights of some kind in the original study material abstractive piece of text such as a written overview of a text book an abstract of a paper short video a slide show other please specify question nr question and answer options what is the ideal structure of the summary multiple answers possible the summary should be a running text without particular structure the summary should consist of highlights in the original study material without particular structure the summary itself should contain special formatting such as bold or cursive text highlights the summary should contain diagrams the summary should contain tables the summary should contain graphs the summary should contain figures the summary should contain headings the summary should consist of different sections paragraphs other please specify how much of the study material should be covered by the summary none of the study material should be covered almost none of the study material should be covered some of the study material should be covered most of the study material should be covered all of the study material should be covered what should the style of this summary be informative the summary should simply convey the information that was in the original study material indicative the summary should give an idea of the topic of the study material but not more critical the summary should give a critical review of the study material aggregative the summary should put different source texts in relation to one another and by doing this give an overview of a certain topic other please specify is there anything else you would want us to know about your ideal summary that we have not covered yet is there anything else you would want us to know about the original study material that we have not covered yet open response open response look out questions now let s assume the pre made summary was generated by a computer you can assume that this machine generated summary captures all the needs you have identified in the previous questions question nr question and answer options would it make a difference to you whether the summary was generated by a computer program or by a human yes participants are led to no participants are led to please explain the difference open response please explain your answer open response which type of summary would you trust more a summary generated by a human for example a teacher or a good performing fellow student a summary generated by a computer no difference which type of summary would you trust more a summary generated by a human for example a teacher or a good performing fellow student a summary generated by a computer no difference now imagine that you can interact with the computer program that made the summary in the form of a digital assistant imagine that your digital assistant made an initial summary for you and you can ask questions about it to your digital assistant and the assistant can answer them answers can be voice output but also screen output e a written summary on the screen in the next part we would like to investigate how you would interact with the assistant please do not feel restricted by the capabilities of today s digital assistants please choose the three most useful features for a digital assistant to have in this scenario summarize particular parts of the study material with more detail summarize particular parts of the study material with less detail switch between different summary styles for example highlighting vs a generated small piece of text explain why particular pieces ended up in the summary provide the source of certain parts of the summary on request search for different related sources based on the content of the summary answer specific questions based on the content of the summary question nr question and answer options please choose the three least useful features for a digital assistant to have in this scenario summarize particular parts of the study material with more detail summarize particular parts of the study material with less detail switch between different summary styles for example highlighting vs a generated small piece of text explain why particular pieces ended up in the summary provide the source of certain parts of the summary on request search for different related sources based on the content of the summary answer specific questions based on the content of the summary can you think of any other features that you would like your digital assistant to have to help you in this scenario thank you for filling out this survey so far we would still like to ask you two final background questions open response background questions what is the current level of education you are pursuing bachelor s degree master s degree mba other please specify what is your main field of study open response thank you you have come to the end of our survey thanks a lot for helping out we very much appreciate your time if you would like to participate in the raffle to win a voucher please fill out your e mail address below we will only use this e mail address to blindly draw names who win a voucher and to contact you if your name has been drawn open response d examples evaluation questions here we give additional examples for the evaluation questions that can be used for our proposed evaluation methodology the phrase a document that is important for your task should be substituted to match the task at hand for example in the case of exam tions this could be replaced with a chapter that you need to learn for your exam preparation only the questions with the intended purpose factors should be used in the evaluation purpose factor use output factor style the style of which of these two summaries is most useful to you to retrieve a document that is important for your task the style of which of these two summaries is most useful to you to preview a document that is important for your task the style of which of these two summaries is most useful to you to substitute a document that is important for your task the style of which of these two summaries is most useful to you to refresh your memory about a document that is important for your task the style of which of these two summaries is most useful to you to prompt you to read a source text that is important for your task purpose factor use output factor format the format of which of these two summaries is most useful to you to retrieve a document that is important for your task the format of which of these two summaries is most useful to you to preview a document that is important for your task the format of which of these two summaries is most useful to you to substitute a document that is important for your task the format of which of these two summaries is most useful to you to refresh your memory about a document that is important for your task the format of which of these two summaries is most useful to you to prompt you to read a source text that is important for your task purpose factor use output factor material the coverage of which of these two summaries is most useful to you to retrieve a document that is important for your task the coverage of which of these two summaries is most useful to you to preview a document that is important for your task the coverage of which of these two summaries is most useful to you to substitute a document that is important for your task the coverage of which of these two summaries is most useful to you to refresh your memory about a document that is important for your task the coverage of which of these two summaries is most useful to you to prompt you to read a source text that is important for your task
