noname manuscript inserted editor machine reading comprehension literature review xin zhang yang sujian yizhong wang received date accepted date abstract machine reading comprehension aims teach machines understand text like human new challenging direction articial intelligence article summarizes recent advances mrc mainly focusing aspects corpus techniques specic characteristics mrc corpus listed compared main idea typical mrc techniques described keywords machine reading comprehension natural language processing introduction past decades growing interest making machine understand human languages recently great progress machine reading comprehension mrc view recent tasks titled mrc seen extended tasks question answering early simmons summarized dozen systems proposed preceding years review survey hirschman gaizauskas classies model categories natural language ends database dialogue interactive advisory systems question answering story comprehension systems rst category like baseball lunar system usually transform natural language questions query structured database based linguistic xin zhang peking university tel mail edu yang peking university sujian peking university yizhong wang peking university xin zhang knowledge performing fairly certain tasks suered constraints narrow domain database dialogue interactive advisory systems including shrdlu gus early models database knowledge source problems like ellipsis anaphora conservation systems struggled dealing remain challenge nowadays models category seen origin modern mrc tasks wendy lehnert rst proposed systems consider story question answer question necessary interpretation inference lehnert designed system called qualm according theory past decade witnessed huge development mrc eld including soar numbers corpus great progress techniques mrc corpus plenty datasets dierent domains styles released recent years mctest released choice reading comprehension dataset high quality small train neural models cnn daily mail cbt released datasets generated automatically dierentdomains larger previous datasets squad shown rst scale dataset questions answers written human techniques proposed competition dataset year marco released emphasis narrative answers quently newsqa narrativeqa constructed similar paradigm squad marco respectively datasets crowdsourced expectation high quality datasets sourced dierent domains sprung following years including race cloth arc collected exams based trivias mcscript primarily focused scripts released wikihop aimed examing systems ability multi hop reasoning coqa proposed test conversation ability models appearance large scale datasets makes training end end neural mrc model possible competing leaderboard models niques developed attempt conquer certain dataset word resentations attention mechanisms high level architectures neural models evolve rapidly surpass human performance tasks article aim extensive review recent datasets niques mrc section categorize mrc datasets types describe briey section introduce traditional non neural methods neural network based models attention mechanism mrc tasks finally section concludes review mrc corpus fast development mrc eld driven large realistic datasets released recent years dataset usually composed documents questions testing document understanding ability answers raised questions obtained seeking documents selecting preseted options according formats answers classify datasets types datasets extractive answers descriptive answers machine reading comprehension literature review multiple choice answers introduce respectively following subsections parallel survey new datasets steadily coming diverse task formulations testing complicated derstanding reasoning abilities datasets extractive answers test system ability reading comprehension kind datasets originates cloze style questions rstly provide system large documents passages feed questions answers segments corresponding passages good system select correct text span given context comprehension tests appealing objectively gradable measure range important abilities basic understanding complex inference sourced crowdworkers generated automatically dierent corpus datasets use text span document answer proposed question released recent years large training strong neural models datasets include squad cnn daily mail cbt newsqa triviaqa wikihop described briey squad famous dataset kind stanford question ing dataset squad stanford question answering dataset squad consists questions posed crowdworkers set wikipedia articles answer question segment text span sponding reading passage squad contains question answer pairs articles larger previous manually labeled datasets quote example question answer pairs fig answer span document meteorology precipitation product condensation atmospheric water vapor falls gravity main forms precipitation include drizzle rain sleet snow graupel hail precipitation forms smaller droplets coalesce collision rain drops ice crystals cloud short intense periods rain scattered locations called showers causes precipitation fall gravity graupel cloud main form precipitation drizzle rain snow sleet hail water droplets collide ice crystals form precipitation fig question answer pairs sample passage squad com xin zhang answer type percentage example date numeric person location entity common noun phrase adjective phrase verb phrase clause october thomas coke germany abc sports property damage second largest returned earth avoid trivialization quietly table answer type distribution squad article endangered species act paragraph legislation followed including migratory bird conservation act treaty prohibiting hunting right gray whales bald eagle protection act later laws low cost society species relatively rare little opposition raised question laws faced signicant opposition plausible answer later laws question treaty plausible answer bald eagle protection act fig unanswerable question examples plausible incorrect answers squad answers belong dierent categories shown table common noun phrases data proper noun phrases data rest consists date numbers adjective phrase verb phrase clauses indicates answers squad displays reasonable diversity reasoning skills squad answer questions authors examples lexical syntactic divergence question answer passage manually annotating examples later squad released emphasis unanswerable questions new version squad adds unanswerable questions created adversarially crowdworkers according original ones order challenge existing models tend unreliable guesses questions answers stated context newly added questions highly similar corresponding context plausible incorrect answers context quote examples shown fig unanswerable questions squad posed humans exhibit diversity delity automatic constructed datasets cases simple heuristics based overlapping entity type recognition able distinguish answerable unanswerable questions cnn daily mail cnn daily mail dataset released google deepmind university oxford rst large scale reading consisting person location entities machine reading comprehension literature review original version context bbc producer allegedly struck jeremy clarkson press charges gear host lawyer said friday clarkson hosted watched television shows world dropped bbc day internal investigation british broadcaster found subjected producer oisin tymon unprovoked physical verbal attack query producer press charges jeremy clarkson lawyer says anonymised version producer allegedly struck press charges host lawyer said friday hosted watched television shows world dropped wednesday ter internal investigation broadcaster found subjected ducer unprovoked physical verbal attack producer press charges lawyer says answer oisin tymon table example data point quoted hension dataset constructed natural language materials unlike relevant work uses templates syntactic semantic rules extract document answer triples work collects articles articles daily source text article comes number bullet points summarize article work converts bullet points document query answer triples cloze style questions exclusively examine system ability reading comprehension world knowledge occurrence modications implemented triples construct anonymized version entity anonymized abstract entity marker easily predicted world knowledge gram language model example data point anonymized version shown table basic statistics cnn daily mail shown table quote percentages right answers appearing frequent entities given document table illustrating diculty degree questions extent cbt children book test babi project facebook aims researching automatic text understanding reasoning dren books chosen ensure clear narrative structure aids task children stories cbt come books freely available project questions formed enumerating consecutive sentences chapters books rst sentences serve context query removing word candidates selected words appearing context query example question given fig dataset size shown table www cnn com www dailymail com downloads gutenberg org xin zhang cnn daily mail cumulative train valid test train valid test cnn daily mail months documents queries max entities avg entities avg tokens vocab size table corpus statistics cnn daily mail table percentage rect answers contained frequent entities given document quoted fig cbt example quoted cbt distinct types word named entities common nouns verbs removed respectively form classes questions class questions wrong candidates selected randomly words type answer options corresponding context query compared human performance dataset state art models like recurrent neural networks rnns long short term memory lstm performed worse predicting nouns named entities great job predicting prepostions verbs probably explained fact models based exclusively local contexts contrast memory networks exploit wider context outperform conventional models predicting nouns named entities corpus encourages use world knowledge comparison cnn daily mail focuses paraphrasing parts context newsqa based news articles news newsqa dataset contains question answer pairs generated crowdworkers similar squad based output pos tagger named entity recognizer stanford core nlp toolkit www cnn com machine reading comprehension literature review training validation test number books number questions average words contexts average words queries distinct candidates vocabulary size table corpus statistics cbt answer type example proportion date time numeric person location entity common noun phr adjective phr verb phr clause phr prepositional phr march million ludwig van beethoven torrance california pew hispanic center federal prosecutors hour suered minor damage trampling human rights attack nearly half table answer types distribution newsqa answer question text span arbitrary length ing article null span included cnn articles chosen source materials authors view machine comprehension systems particularly suited high volume rapidly changing information sources like news major ferences cnn daily mail newsqa answers newsqa necessarily entities anonymization procedure considered generation newsqa statistics answer types newsqa shown table seen table variety answer types ensured furthermore authors sampled examples newsqa squad respectively analyzed possible reasoning skills answer questions results indicate compared squad larger proportion questions newsqa require high level reasoning skills including inference synthesis simple skills like word ing paraphrasing solve questions datasets newsqa tends require complex reasoning skills squad detailed comparison result given table triviaqa instead relying crowdworkers create question answer pairs selected passages like newsqa squad triviaqa answer evidence triples generated automatic procedures firstly huge question answer pairs trivia quiz league websites ered ltered evidence documents question answer pair collected web search results wikipedia articles finally clean free human annotated subset triples triviaqa given triple example shown fig basic statistics triviaqa given table sampling examples dataset annotating manually turns wikipedia titles including person organization location miscellaneous consists reasoning example word matching paraphrasing inference synthesis ndings published sets research ndings lished thursday struggle rwanda struggle pits ethnic tutsis ported rwanda ethnic hutu backed congo drew inspiration presidents rudy ruiz says lives presidents positive role models dents brittanee drexel mother year old rochester new york high school student says daughter permission trip brittanee marie drexel mom says house barack obama mother law marian robinson join obamas family private quarters vania avenue michelle mentioned xin zhang proportion newsqa squad ambiguous insucient mother moving white table reasoning skills newsqa squad corresponding proportions question dodecanese campaign wwii attempt allied forces capture islands aegean sea inspiration acclaimed commando answer guns navarone excerpt dodecanese campaign world war attempt allied forces capture italian held dodecanese islands aegean sea following der italy september use bases german controlled balkans failed campaign particular battle leros inspired novel guns navarone successful movie question american callan pinckney eponymously named system selling book video franchise genre answer fitness excerpt callan pinckney american tness professional achieved precedented success callanetics exercises books tional best sellers video series followed went sell million copies pinckney rst video release callanetics years younger hours outsold tness video fig example question answer evidence triples triviaqa quoted answer rest small percentage answers mainly belong numerical free text type average number entities question percentages certain types questions shown table wikihop wikihop released purpose evaluating system ability multi hop reasoning multiple documents existing datasets information needed answer question usually contained machine reading comprehension literature review total number pairs number unique answers number evidence documents avg question length word avg document length word table corpus statistics triviaqa property example annotation statistics avg entities question fine grained answer type fragrant essential oil obtained damask questions coarse grained answer typewho won nobel peace prize questions time frame comparisons photographed rst time october questions appropriate largest type frog politician won nobel peace prize question questions table properties questions sampled examples boldfaced words mean presence corresponding properties wikihop medhop train dev test total table dataset sizes wikihop medhop sentence makes current mrc models pay attention simple reasoning skills like locating matching aligning information query support text example squad sentence highest lexical similarity question contains answer time simple binary word query indicator feature boosted relative accuracy baseline model authors dene novel mrc task model needs combine evidences dierent documents answer questions sample wikihop displays characteristics shown fig construct wikihop authors collect triples subject entity relation object entity wikidata wikipedia articles associated entities added candidate evidence documents triple query removing answer reach goal multi hop reasoning bipartite graphs constructed help construction shown fig vertices sides respectively correspond entities documents knowledge base edges denote entities appear corresponding documents given pair answer candidates support documents identied traversing bipartite graph search documents visited support documents dataset medhop constructed way wikihop focus medicine area basic statistics wikihop medhop shown table table table lists proportions dierent types answer samples indicates perform wikihop system needs good multi step reasoning xin zhang hanging gardens known pherozeshah mehta gardens terraced gardens provide sunset views arabian sea known bombay ocial capital city indian state maharashtra populous city india arabian sea region northern indian ocean bounded north pakistan iran west northeastern somalia arabian peninsula east india question hanging gardens mumbai country options iran india pakistan somalia fig sample wikihop quoted displays necessity multi hop reasoning documents fig bipartite graph given paper connecting entities documents mentioning bold edges traversed rst fact small right yellow highlighting indicates documents candidates check cross indicate correct false candidates descriptive answer datasets instead text spans entities obtained candidate documents descriptive answers stand sentences exhibit uency integrity addition real world questions answered simply text span entity presenting answers supporting evidence examples preferred human light reasons descriptive answer datasets released recent years mainly introduce detail marco narrativeqa marco marco microsoft machine reading comprehension large dataset released microsoft dataset aims address questions documents real world sourced real anonymized queries issued machine reading comprehension literature review cand docs tok cand docs tok min max avg median table corpus statistics wikihop medhop wikihop medhop unique multi step answer likely multi step unique answer multiple plausible answers ambiguity hypernymy single document required answer follow wikidata wikipedia discrepancy table qualitiative analysis sampled answers wikihop corresponding searching results bing search engine marco reproduce situations real world question dataset crowdworker asked answer form complete sentence passages provided bing unanswerable questions kept dataset purpose encouraging system judge question answerable scanty conicting materials rst version marco released questions latest version released questions available msmarco org dataset compositions marco shown table tribution dierent types questions shown table table contain interrogatives queries come real users interrogative contained queries description questions account major question type generally interrogative distribution questions shows reasonable diversity dataset descriptive answers released deepmind university oxford narrativeqa specically designed examine system capture underlying narrative elements answer questions answered simple pattern recognition global salience example question answer pair shown fig relatively high level abstraction reasoning required answer question stories narrativeqa consist books project movie scripts relative story plot summary nally provided crowdworkers create question answer pairs www bing com microsoft com cortana gutenberg mainly imsdb dailyscript awesomelm xin zhang field description query question query issued bing passages passages web documents retrieved bing passages presented ranked order human editors passage editor uses compose answer annotated selected document urls urls ranked documents question bing passages extracted documents answers composed human editors question matically extracted passages corresponding ments formed formed answer rewritten human editors inal answer segment classication tallest mountain south america longs entity segment answer entity aconcagua table marco dataset composition percentage question question segment question types yesno question classication description numeric entity location person table distribution dierent question types marco crowdworkers text likely create questions answers solely based localized context answers sentences exhibit articial intelligence asked factual information basic statistics shown table distribution dierent types questions answers shown table table according original paper answers appear text segments stories decreases possibility answering questions simple skills system machine reading comprehension literature review title ghostbusters question oscar related dana answer son summary snippet peter girlfriend dana barrett son oscar story snippet dana setting wheel brakes buggy thank frank hang eventually continues digging purse frank leans buggy makes funny faces baby oscar cute month old boy frank baby hiya oscar slugger good looking kid got barrett frank dana fig example question answer pair narrativeqa given paper documents books movie scripts question answer pairs avg tok summaries max tok summaries avg tok stories max tok stories avg tok questions avg tok answers train valid test table narrativeqa dataset statistics token frequency category frequency person description location reason method event entity object numeric duration relation table frequency rst token question training set narrativeqa table question categories sample questions validation set xin zhang machine reading comprehension literature review xin zhang machine reading comprehension literature review james turtle getting trouble reach freezer food times sled deck splinter aunt jane tried hard trouble sneaky got lots trouble day james thought town kind trouble went grocery store pulled pudding shelves ate jars walked fast food restaurant ordered bags fries pay instead headed home aunt waiting room told james loved start acting like behaved turtle month getting lots trouble james nally mind better turtle trouble making turtle fries pudding james jane james pull shelves grocery store pudding fries food splinters james went grocery store deck freezer fast food restaurant room james ordered fries went grocery store went home paying ate mind better turtle fig sample mctest given paper multiple choice datasets descriptive answers relatively dicult evaluate system performance precisely objectively multiple choice question long testing students reading comprehension ability jectively gradable generally kind questions extensively examine reasoning skills including simple pattern recognition clausal inference sentence reasoning given passage light datasets format released listed follows mctest mctest high quality dataset consisting stories tions ction stories released microsoft format race targeting year old children passages questions mctest easy understandable reduces world knowledge requisite mctest answers found story stories ctional main drawback mctest size small train performed model sample mctest shown fig race race contains passages questions collected english exams middle high school chinese students considering passages questions specically designed english teachers experts evaluate reading comprehension ability students dataset promising developing testing mrc systems questions created high quality human experts noises race passages race cover wide range topics xin zhang release date type race cloth mctest mcscript arc coqa multiple choice multiple choice domain exam exam question source natural natural multiple choice fiction stories crowd multiple choice script scenarios crowd multiple choice multiple choice science widea natural crowd human performance sota contain unanswerable question test common sense specically raw document document number average length document query number average length query average length answer children stories literature mid high school exams news wikipedia science reddit race race total middle high domain domain race race scenarios science related sentences stories texts passages table basic information statistics multiple choice datasets machine reading comprehension literature review wanted plant tree went home garden store picked nice oak planted garden dig hole shovel bare hands plant tree watering taking home fig example questions mcscript overcoming topic bias problem commonly exists datasets like news articles cnn daily mail wikipedia articles squad sample race shown table dataset rstly provides dents systems passage read presents questions didate answers words questions candidate answers appear passage simple context matching techniques aid datasets analysis paper shows reasoning skill indispensable answering questions race correctly race divided subsets race race middle school high school respectively basic statistics race given table table distributions dierent reasoning types required answer certain questions illustrated table denoting half questions race requires reasoning skill cloth cloth cloze test teachers constructed format cloze questions composed english tests chinese middle school high school example shown table cloth missing blanks questions carefully designed teachers test dierent aspects language knowledge candidate answers usually subtle dierences making tions dicult answer human similar race cloth divided parts cloth middle school cloth high school ones basic statistics corpus shown table experiments cloth authors came conclusion performance gap human system mainly results ability long term context multiple sentence reasoning mcscript mcscript focuses questions need reasoning sense knowledge released march new dataset provides stories describing people daily activities ambiguity implicitness resolved easily commonsense crowdworkers generate questions correct answers questions appear given text shown examples fig consists texts questions according statistical analysis questions mcscript require commonsense knowledge answer dataset literally examine systems commonsense inference ability questions dataset answerable distribution questions types mcscript shown fig xin zhang passage small village england years ago mail coach standing street come village people pay lot letter person sent letter pay postage receiver letter miss alice brown said mailman alice brown girl said low voice alice looked envelope minute handed mailman sorry money pay said gentleman standing sorry came paid postage gentleman gave letter said smile thank letter tom going marry went london look work waited long time letter need know gentleman said surprise told signs envelope look sir cross corner means circle means found work good news gentleman sir rowland hill forgot alice letter postage paid receiver changed said good plan postage lower penny person sends letter pays postage buy stamp envelope said government accepted plan rst stamp called penny black picture queen questions rst postage stamp england america alice girl handed letter mailman know letter money pay postage received letter want open known written letter know alice words tom told signs meant leaving alice clever guess meaning signs alice signs lope tom signs alice told table sample race quoted idea stamps thought government sir rowland hill alice brown tom passage know high postage people send letters lovers lose touch people try best avoid paying receivers refuse pay coming letters answer adabc machine reading comprehension literature review race dataset subset passages questions race train dev test train dev test train dev test race table basic statistics training development test sets race race race dataset passage len question len option len vocab size race race race table statistics race len denotes length vocab denotes vocabulary dataset word matching paraphrasing single sentence reasoning multi sentence reasoning ambiguous insucient race race race cnn squad newsqa table distribution reasoning type race datasets denotes quoting based samples dataset quoting fig distribution question types mcscript arc reasoning challenge makes use standardized tests questions objectively gradable exhibit variety diculty grand challenge arc consists questions authors arc designe baselines retrieval based gorithm word occurrence algorithm challenge set subset arc long xin zhang passage nancy got job secretary company monday rst day went work arrived early door open found arrive thought came desk surprised bunch fresh sweet looked somebody sent owers rst day thought began day passed quickly nancy interest following days rst thing nancy change water followers set work came monday came near desk overjoyed bunch owers quickly vase old ones thing happened monday nancy began think ways tuesday afternoon sent hand plan waited directives secretary happened desk half opened notebook order secretaries high spirits company decided monday morning bunch fresh owers secretarys desk later told general manager business management psychologist questions pushed second grapes forced rst bananas held bottle depressed encouraged excited surprised turned keys smelled ate vase angrily seek low month old covering demanding replacing forbidding sender receiver assistant colleague employee manager notebook desk said knocked owers took glass strangely happily work great year blue room quietly wonder little period red ask general week new secretary waiter oce printed house signed written table sample passage cloth bold faces highlight correct answers best answer candidates candidates correct dataset cloth cloth cloth train dev test train dev test train dev test passages questions vocab size avg sentence avg words table statistics training development test sets cloth subsets paper containing questions created gathering questions answered incorrectly baselines easy set composed ing questions state art models tested challenge set able signicantly outperform random baseline reects diculty challenge set example questions challenge set questions follows machine reading comprehension literature review train dev test total challenge easy total table number questions arc grade challenge qns easy qns qns table grade level distribution arc questions qns property question words question sentences answer option words answer options min average max challenge easy table properties arc dataset property mineral determined looking ter correct mass weight hardness student riding bicycle observes moves faster smooth road rough road happens smooth road gravity gravity friction correct friction example rst question dicult ground truth luster determined looking appears stand sentence web text incorrect candidate hardness strong correlation mineral text arc corpus scientic text contains science related sentences mentions knowledge related challenge set questions according sample analysis released arc questions set use corpus optional statistics arc shown table table table coqa question answering systems conversational style datasets consists questions sourced conversations dierent domains answers questions free form motivation coqa daily life human usually information asking questions conversations desirable machine capable answering questions coqa xin zhang jessica went sit rocking chair today birthday turning granddaughter annie coming afternoon jessica excited daughter melanie melanie husband josh coming jessica birthday jessica jessica went sit rocking chair today birthday turning old turning plan visitors yes granddaughter annie coming granddaughter annie coming afternoon jessica excited daughter melanie melanie husband josh coming annie melanie josh granddaughter annie coming afternoon jessica excited daughter melanie melanie husband josh coming fig conversation example coqa turn contains question answer rationale supports answer rstly provides models text passage understand presents series questions appear conversation example given fig key challenge coqa system handle conversation history properly tackle problems like resolving coreference domains passages questions collected cross domain uation domain evaluation distribution domains shown table linguistic phenomena statistics given table coreference pragmatics unique challenging linguistic phenomena appeare datasets section introduce dierent techniques employed mrc mrc techniques non neural method neural networks came fashion mrc systems oped based dierent non neural techniques serve baselines comparison introduce techniques including idf sliding window logistic regression boosted method machine reading comprehension literature review domain passages passage length turns passage pairs domain children sto literature mid high sch news wikipedia science reddit total lexical match paraphrasing pragmatics coref explicit coref implicit coref table distribution domains coqa phenomenon example percentage relationship question passage rescue coast guard outen rescued coast guard wild dog approach yes drew cautiously closer joey male female male looked like stick man kept named new noodle friend joey ifl bashti forgotten puppy sirisena sworn local time relationship question conversation history table linguistic phenomena coqa questions given paper idf idf term frequency inverse document frequency technique widely information retrieval area nds place mrc tasks later validated candidate answers presented retrieval based models serve strong baseline kind baseline widely document datasets wikihop solely exploiting lexical correlation tween concatenation candidate answer query given document kind algorithm predict candidate highest similarity score documents inter document information usually ignored idf baseline detect question rely cross document reasoning xin zhang sliding window sliding window algorithm constructed baseline dataset mctest predicts answer based simple lexical information sliding window inspired idf algorithm uses inverse word count weight word maximize bag word similarity answer sliding window given passage logistic regression baseline method proposed squad extracts large mount features candidates including lengths bigram frequencies word frequencies span pos tags lexical features dependency tree path features predicts text span nal answer based information boosting method model proposed conventional feature based baseline cnn daily mail dataset task seen ranking problem making score predicted answer rank candidates authors turn implementation lambdamart ranklib highly successful ranking algorithm forests boosting decision trees feature engineering features chosen form feature vector represents candidate weight vector learnt correct answer ranked highest neural based method popularity neural networks end end models produced ing results mrc tasks models need design complex manually devised features traditional approaches relied perform better introduce end end models mainly chronological order match network rst end end neural architecture posed squad model combines match lstm query aware representation passage pointer network aims construct answer token comes input text overall picture model architecture given fig match lstm originally designed predicting textual entailment task premise hypothesis given match lstm encodes pothesis premise aware way token hypothesis model uses soft attention mechanism discussed later sect weighted vector representation premise weighted vector concatenated vector representation according token fed lstm match lstm paper authors replace premise hypothesis query passage query aware representation given passage preprocessing lstms employed respectively encode query passage bidirectional match lstm employed obtain query aware representation passage net lemur wiki details found paper machine reading comprehension literature review fig overview models getting query aware representation passage pointer net employed generate answers selecting tokens input passage inference step ptr net uses soft attention mechanism probability tribution input sequence selects token largest possibility output symbol dierent strategies proposed constructing answer sequence model assumes word answer appear position passage length answer xed order tell model stop generating tokens getting answer special symbol placed end passage prediction symbol indicates termination answer generating boundary model works dierently sequence model predicts start indice end indice word based assumption answer appears continuous segment passage test result shows advantage boundary model directional attention flow proposed directional attention flow key features context encoding stage model takes dierent levels granularity input including character level word level contextualized embeddings second uses directional attention passage query attention query passage attention query aware passage tation detailed description given follows shown fig bidaf model layers character ding layer word embedding layer map word vector space based respectively character level cnns pre trained glove embedding concatenation word embeddings passed layer highway networks output provided directional lstm contextual embedding layer rene word embedding xin zhang fig overview bidaf architecture given context information rst layers applied query passage attention flow layer information query passage mixed interacted instead summarizing passage query xed vector like attention mechanisms layer grants raw information including attention vectors embeddings previous layers owing subsequent layer reduces information loss attentions computed directions passage query query passage detailed information attention flow layer given sect modeling layer takes query aware representation context words directional lstm capture interactions passage words according query output layer task specic gives prediction answer gated attention gated attention reader targets realizing multi hop ing answering cloze style questions documents multiplicative interaction query hidden state document employed tion mechanism multi hop architecture model imitates multi step reasoning human reading comprehension overview model given fig model reads document query iteratively row layers kth layer model uses bidirectional gated recurrent gru transform dings document passed layer layer specic query representation transformed gru gru gru modeling layeroutput layerattention flow layercontextual embed layerword embed softmaxdense cnncharacter embed machine reading comprehension literature review fig gated attention architecture given fed gated attention module result passed layer token gated attention module uses soft attention token specied representation query finally new embeddings token applying element wise multiplication softmax stage decoder employs softmax layer inner product outputs layer possibility distribution predict answers dcn dynamic coattention introduces coattention mechanism combine dependent representations query document dynamic iteration avoid trapped local maxima corresponding incorrect answers like previous single pass models dynamic pointer decoder takes output coattention encoder generates nal predictions detailed procedures given follows let document question encoder vector representations ment query fed lstm respectively hidden states step combined form encoding matrix dmd qnq sentinel vector appended encoding matrix enable model map unrelated words exclusively appear query document void vector allow variation document encoding space query encoding space document details dcn follows denote sequence embeddings words query xin zhang non linear projection tanh applied nal representations document query coattention encoder takes outputs coattention encoding matrix input dynamic pointing decoder details coattention encoder discussed sec overview dynamic pointing decoder given fig enable model recover local maxima highway maxout network hmn posed predict start point end point iteratively hmn composed highway networks characterized skip connect passes gradient eectively deep networks maxout networks learnable activation function strong empirical performance iteration hidden state decoder updated according lst dec coattention representations according start end words predicted iteration given possibility tth word start end point calculated hmn word maximum possibility selected prediction current step architecture hmn given fig mathematical description hmn given follows hmn max tanh max max non linear projection current state fastqa fastqa achieved competitive performance simple architecture questions necessity improving complexity systems unlike systems employed complex interaction layer catch interaction query context fastqa makes use computable features word levels overview fastqa architecture given fig binary word feature indicates token passages appears corresponding query wiqb weighted dened takes term frequency similarity query context account simi vwiq wiqw softmax vwiq machine reading comprehension literature review fig architecture dynamic decoder paper blue denotes variables functions related estimating start position red denotes variables tions related estimating end position fig architecture highway maxout network given concatenation features original representation words fed lstm nal hidden state answer layer composed simple layer feed forward network beam search net net proposed msra achieved state art results squad marco overview architecture shown fig given word level character level embeddings net rstly employs directional gru encode questions passages uses gated attention based recurrent network fuse information question hmnargmax xin zhang fig overview fastqa architecture passage later self matching layer tune nal tation passage output layer based pointer networks similar match lstm predict boundary answer initial hidden vectors pointer network computed attention pooling nal passage representations gated attention based recurrent network adds gate normal based recurrent networks gate gives weight certain passage information according question inspired sentence pair representations obtained follows rnn tanh exp exp sigmoid original representations passage question added gate exploit information passage token self matching attention applied nal representation passage details self matching attention given sec output layer uses pointer networks predict start end position answer initial hidden vector pointer network pooling question representation objective function sum negative log probabilities ground truth start end position predicted distributions reasonet unlike previous models xed number turns reading reasoning regardless complexity queries passages reasonet makes use reinforcement learning dynamically determine reading machine reading comprehension literature review fig overview net architecture paper reasoning depth intuition work comes diculty dierent questions vary lot dataset fact human usually revisit important passage question answer question better overview reasonet structure given fig external memory usually word embeddings encoded rnn internal state updated according rnn attention vector fatt termination gate determines stop updating states predict answers according binary variable way reasonet mimic inference process human exploit passages answer questions better qanet models primarily based rnns attention slow training inference sequential nature rnns machine comprehension fast qanet proposed rnns architecture overview qanet structure given fig key dierence qanet previous models qanet use convolutional self attention mechanism embedding modeling encoders discarding commonly rnns depthwise separable tions capture local structure text multi head attention mechanism model global interactions passages query context attention similar dcn applied qanet achieved state art accuracy achieving speedup training training iteration compared rnn counterparts xin zhang fig overview reasonet structure attention attention mechanisms shown great power selecting important mation aligning capturing similarity dierent input introduce representative attention mechanism primarily based time order hard attention proposed image caption task stochastic hard attention let denote feature vectors captured cnn corresponding image deciding features feed decoder lstm generate caption hot variable dened indicator set vector extract visual features current step denote input decoder lstm paper assigns multinoulli distribution parametrized view random variable iai eti fatt truememory machine reading comprehension literature review fig overview qanet architecture left encoder blocks coder blocks number convolutional layers varies fatt multilayer perceptron dening objective function log log log approximate gradient monte carlo method nal learning rule model log log hyperparameters set crossvalidation hard attention tricky troublesome training trained perform better soft attention sharp focus memory provided xin zhang soft attention rst introduce basic form soft attention neural machine translation task talk variants tasks like natural language mrc dierent hard attention soft attention calculates weight distribution input representations use weighted sum input decoder example let htx denote encoder output sequence denote weight indicates extent related current output token input decoder ijhj txx weights calculated learn feedforward neural network exp eij exp eik ptx eij nli task input components premise hypothesis attention exploit interaction relation parts match lstm example denote resulting hidden states encoder lstm separately premise hypothesis predicting label hypothesis attention weighted combinations hidden states premise computed match lstm kjhs ekj tanh wtht wmhm attention vector stated parameters learned hidden state match lstm position finally concatenated predicting result mrc task regard question premise passage hypothesis likes model match network applying attention mechanism additional query information token passage improve model performance compared hard attention soft attention advantage dierentiabile easy train fast training inference directional attention proposed bidaf compared attention scribed considers attention directions query attention context attention bidaf example given concatenation outputs lstms contextual embedding layer similarity matrix computed stj machine reading comprehension literature review compute attention weights attended query vectors trainable parameters elementwise multiplication similarily attention weights attended context vectors softmax atju softmax finally attention vectors combined original tual embeddings vector fusing function result serve base future modeling prediction directional attention adds information attention compared normal attention mechanism shown ablation study attention direction useful standard squad dev set reason query usually short added information relatively small coattention proposed architecture coattention encoder dcn shown fig coattention encoder anity matrix calculated normalized row wise column wise obtain attention weights matrix document word query attention weights matrix query word document attention contexts question computed daq concatenated obtain nal document representation step fed bidirectional lstm lstm result serves foundation predicting answer hidden states form coattention encoding matrix similarly directional attention coattention mechanism utilizes tion information directions dierent way successively computes attention contexts question document fuses dependent representation document self matching attention proposed net introduced useful information exist passage context captured traditional mainly exploits information words surrounding window self matching attention mechanism proposed address problem collects evidence token passage according question information result nal passage representation birnn xin zhang fig architecture attention encoder refers attention pooling vector passage tanh exp exp ivp gate dene sec uniquely self matching attention captures long distance information passage helps net dealing problems like coreference pre trained word representations eciently represent words vectors serve base modern mrc systems problem concerns researchers previously hot representation gram model popular simple techniques met limits tasks address problem technologies proposed according time occurrence introduce follows moving feedforward neural net language recurrent neural net language paper proposed novel models learn distributed representations words tinuous bag words continuous skip gram model architectures models given fig cbow model uses history words future words input maximizes probability correctly predicting current word contrast skip gram model uses current word input tries predict words certain range current word result word vectors models achieved state art performance tests aqaddocumentproductconcatproductbi lstmbi lstmbi lstmbi lstmbi cqcdutu machine reading comprehension literature review fig architectures cbow model skip gram model probability ratio solid gas water fashion fig ratio greater means word correlate ice ratio greater means word correlate stream glove method belongs local context window methods ods capture grained semantic syntactic regularities words eciently exploit global statistical information like latent semantic belongs global matrix factorization methods glove combines advantages family methods glove takes occurrence probabilities words consideration use ratio probabilities reect relations dierent words example denote probability word appear context word pij ratio pik pjk tell correlation certain words example given fig glove model takes form according phenomenon pik pjk word vectors varies according dierent constrains elmo disadvantages word vectors generated methods static independent application linguistic contexts lead input projection input projection cbow skip gram xin zhang poor performance comes polysemy light elmo proposed addresses problem elmo model employs lstm character convolutions input jointly maximizes log likelihood forward backward directions record internal states log lst log lst finally task specic linear combination internal states obtain elmo representation way elmo capture context dependent aspects word meaning syntax information token tuned domain specic data model usually performs better gpt compared elmo gpt uses variant transformer instead lstm better capture long term linguistic structure overview work given fig given corpus standard language model multi layer transformer decoder log transformer block softmax context window size context vectors tokens number layers token embedding matrix position embedding matrix parameters trained stochastic gradient descent nal transformer blocks activation denoted supervised tuning applied dierent stream tasks tasks like text classication linear output layer parameters needed predict softmax recently successor released scale gpt larger volume billion parameters claimed achieve state art performance language modeling code released time paper written machine reading comprehension literature review fig graph comes paper left transformer architecture training tives work right input transformations tuning dierent tasks structured inputs converted token sequences processed gpt followed layer fig model architectures bert gpt elmo quoted bert shown fig elmo gpt models use unidirectional language models learn representation tokens bert points restriction severely limited eciency pre trained representation address problem new prediction tasks proposed pre train bert direction masked language model sentence prediction inspired cloze task masked language model predict randomly masked tokens based context input words left right context taken consideration computing representations capture sentence level information relationship rized sentence prediction task predict sentence sentence wordpiece embeddings input layer ment embeddings position embeddings input embeddings sum embeddings shown fig main architecture bert model multi layer bidirectional transformer encoder identical inal similar gpt tuned steam tasks additional output layer minimal number parameters needed shown fig bert advanced state art results nlp tasks comparison size bert gpt given table bert openai gptlstmelmolstmlstmlstmlstmlstmlstmlstmlstmlstmlstmlstm xin zhang fig bert input representation fig task specic models overview paper sep tok ntok tnsingle sentence berttok tok tok tnsingle sentence peroo label tmstart end spanclass labelberttok tok tok sep tok ntok tokmsentence sentence machine reading comprehension literature review model bertbase bertlarge parameters layers hidden size table hyperparameter comparison similar models layers means number transformer blocks conclusion paper summarized advances mrc eld recent years briey introduced history mrc tasks early mrc systems section introduced recent datasets categories squad cnn daily mail cbt newsqa triviaqa cloth extractive format marco narrative narrative format wikihop mctest race mcscript arc multiple choice format coqa novel dataset focuses conversational questions included section rst non neural methods including sliding window logistic regression idf boosted method importantly neural based models like dcn bidaf fastqa rnet reasonet qanet discussed compared important positions models pre training technology attention anism detail covered glove elmo bert section hard attention soft attention directional attention coattention self attention mechanisms section reviewed major progress recent years mrc eld mrc direction developing fast dicult include newly proposed mrc work survey hope review ease reference recent mrc advences encourage researchers work mrc eld references bahdanau cho bengio neural machine translation jointly learning align translate corr bengio ducharme vincent jauvin neural probabilistic language model journal machine learning research bobrow kaplan kay norman thompson winograd gus frame driven dialog system articial intelligence chen bolton manning thorough examination cnn daily mail ing comprehension task arxiv preprint cho van merrienboer gulcehre bahdanau bougares schwenk bengio learning phrase representations rnn encoder decoder statistical chine translation arxiv preprint chollet xception deep learning depthwise separable convolutions arxiv preprint clark cowhey etzioni khot sabharwal schoenick tafjord think solved question answering try arc reasoning challenge arxiv preprint xin zhang clark cowhey etzioni khot sabharwal schoenick tafjord think solved question answering try arc reasoning challenge arxiv preprint clark etzioni computer honor studentbut intelligent standardized tests measure magazine clark etzioni khot sabharwal tafjord turney khashabi combining retrieval statistics inference answer elementary science questions aaai deerwester dumais furnas landauer harshman indexing latent semantic analysis journal american society information science devlin chang lee toutanova bert pre training deep bidirectional transformers language understanding arxiv preprint dhingra liu yang cohen salakhutdinov gated attention readers text comprehension arxiv preprint goodfellow warde farley mirza courville bengio maxout networks arxiv preprint green wolf chomsky laughery baseball automatic papers presented western joint ire aiee acm answerer computer conference acm hermann kocisky grefenstette espeholt kay suleyman som teaching machines read comprehend advances neural information processing systems hill bordes chopra weston goldilocks principle reading children books explicit memory representations arxiv preprint hirschman gaizauskas natural language question answering view natural language engineering hochreiter schmidhuber long short term memory neural computation jia liang adversarial examples evaluating reading comprehension systems arxiv preprint joshi choi weld zettlemoyer triviaqa large scale distantly pervised challenge dataset reading comprehension arxiv preprint kaiser gomez chollet depthwise separable convolutions neural machine translation arxiv preprint kim convolutional neural networks sentence classication arxiv preprint schwarz blunsom dyer hermann melis grefenstette narrativeqa reading comprehension challenge transactions association computational linguistics lai xie liu yang hovy race large scale reading comprehension dataset examinations arxiv preprint lehnert conceptual theory question answering proceedings international joint conference articial intelligence volume morgan kaufmann publishers inc levy seo choi zettlemoyer zero shot relation extraction reading comprehension arxiv preprint liu saleh pot goodrich sepassi kaiser shazeer generating wikipedia summarizing long sequences arxiv preprint manning surdeanu bauer finkel bethard mcclosky stanford corenlp natural language processing toolkit proceedings annual meeting association computational linguistics system demonstrations merity xiong bradbury socher pointer sentinel mixture models arxiv preprint mikolov chen corrado dean ecient estimation word representations vector space arxiv preprint nguyen rosenberg song gao tiwary majumder deng marco human generated machine reading comprehension dataset arxiv preprint machine reading comprehension literature review ostermann modi roth thater pinkal mcscript novel dataset assessing machine comprehension script knowledge arxiv preprint pennington socher manning glove global vectors word representation proceedings conference empirical methods natural language processing emnlp peters neumann iyyer gardner clark lee zettlemoyer deep contextualized word representations arxiv preprint radford narasimhan salimans sutskever improving language ing unsupervised learning tech rep technical report openai rajpurkar jia liang know know unanswerable questions squad arxiv preprint rajpurkar zhang lopyrev liang squad questions machine comprehension text arxiv preprint reddy chen manning coqa conversational question answering challenge arxiv preprint richardson burges renshaw mctest challenge dataset domain machine comprehension text proceedings conference pirical methods natural language processing richardson burges renshaw mctest challenge dataset domain machine comprehension text proceedings conference pirical methods natural language processing robbins monro stochastic approximation method herbert robbins selected papers springer rocktaschel grefenstette hermann blunsom reasoning entailment neural attention arxiv preprint seo kembhavi farhadi hajishirzi bidirectional attention machine comprehension arxiv preprint shankar garg sarawagi surprisingly easy hard attention sequence shankar sarawagi label organized memory augmented neural network corr quence learning emnlp shen huang gao chen reasonet learning stop reading machine comprehension proceedings acm sigkdd international conference knowledge discovery data mining acm simmons answering english questions computer survey tech rep system development corp santa monica calif srivastava gre schmidhuber highway networks arxiv preprint bulletin taylor new tool measuring readability journalism trischler wang yuan harris sordoni bachman suleman newsqa machine comprehension dataset arxiv preprint vaswani shazeer parmar uszkoreit jones gomez kaiser polosukhin attention need advances neural information processing systems vinyals fortunato jaitly pointer networks arxiv prints vinyals fortunato jaitly pointer networks advances neural tion processing systems vrandecic wikidata new platform collaborative data collection proceedings international conference world wide web acm wadhwa embar grabmair nyberg inference oriented reading comprehension parallelqa arxiv preprint wang jiang learning natural language inference lstm arxiv preprint wang jiang machine comprehension match lstm answer pointer arxiv preprint wang yang wei chang zhou gated self matching networks reading comprehension question answering proceedings annual ing association computational linguistics volume long papers vol xin zhang weissenborn wiese seie fastqa simple ecient neural architecture question answering corr weissenborn wiese seie making neural simple possible simpler arxiv preprint weissenborn wiese seie making neural simple possible simpler arxiv preprint welbl stenetorp riedel constructing datasets multi hop reading hension documents transactions association computational linguistics weston chopra bordes memory networks corr winograd understanding natural language cognitive psychology woods progress natural language understanding application lunar geology proceedings june national computer conference exposition acm burges svore gao adapting boosting information retrieval measures information retrieval schuster chen norouzi macherey krikun cao gao macherey google neural machine translation system bridging gap human machine translation arxiv preprint xie lai dai hovy large scale cloze test dataset designed teachers xiong zhong socher dynamic coattention networks question answering arxiv preprint arxiv preprint kiros cho courville salakhudinov zemel bengio attend tell neural image caption generation visual attention international conference machine learning yih chang meek pastusiak question answering enhanced lexical semantic models proceedings annual meeting association computational linguistics volume long papers vol dohan luong zhao chen norouzi qanet combining local convolution global self attention reading comprehension arxiv preprint
