noname manuscript no will be inserted by the editor machine reading comprehension a literature review xin zhang an yang sujian li yizhong wang n u j l c s c v v i x r a received date accepted date abstract machine reading comprehension aims to teach machines to understand a text like a human and is a new challenging direction in articial intelligence this article summarizes recent advances in mrc mainly focusing on two aspects i e corpus and techniques the specic characteristics of various mrc corpus are listed and compared the main idea of some typical mrc techniques are also described keywords machine reading comprehension natural language processing more introduction over past decades there has been a growing interest in making the machine understand human languages and recently great progress has been made in machine reading comprehension mrc in one view the recent tasks titled mrc can also be seen as the extended tasks of question answering qa as early as simmons had summarized a dozen of qa systems proposed over the preceding years in his review the survey by hirschman and gaizauskas classies those qa model into three categories namely the natural language front ends to the database the dialogue interactive advisory systems and the question answering and story comprehension for qa systems in the rst category like the baseball and the lunar system they usually transform the natural language questions into a query against a structured database based on linguistic xin zhang peking university tel e mail edu an yang peking university sujian li peking university yizhong wang peking university xin zhang et al knowledge although performing fairly well on certain tasks they suered from the constraints of the narrow domain of the database as about the dialogue interactive advisory systems including the shrdlu and the gus early models also used the database as their knowledge source problems like ellipsis and anaphora in the conservation which those systems struggled in dealing with still remain as a challenge even for nowadays models the last category can be seen as the origin of modern mrc tasks wendy lehnert rst proposed that the qa systems should consider both the story and the question and answer the question after necessary interpretation and inference lehnert also designed a system called qualm according to her theory the past decade has witnessed a huge development in the mrc eld including the soar of numbers of corpus and great progress in techniques as about mrc corpus plenty of datasets in dierent domains and styles have been released in recent years in mctest was released as a choice reading comprehension dataset which was of high quality whereas too small to train neural models in cnn daily mail and cbt were released these two datasets were generated automatically from dierentdomains and much larger than previous datasets in squad was shown up as the rst scale dataset with questions and answers written by the human many techniques have been proposed along with the competition on this dataset in the same year the ms marco was released with the emphasis on narrative answers quently newsqa and narrativeqa were constructed in similar paradigm with squad and ms marco respectively and both datasets were crowdsourced with the expectation for high quality next various datasets sourced from dierent domains sprung up in the following two years including race cloth and arc that were collected from exams that were based on trivias and mcscript primarily focused on scripts released in wikihop aimed at examing systems ability of multi hop reasoning and coqa were proposed to test conversation ability of models the appearance of large scale datasets above makes training an end to end neural mrc model possible when competing on the leaderboard many models and niques were developed in an attempt to conquer a certain dataset from word resentations attention mechanisms to high level architectures neural models evolve rapidly and even surpass human performance in some tasks in this article we aim to make an extensive review on recent datasets and niques for mrc in section we categorize the mrc datasets into three types and describe them briey in section we introduce the traditional non neural methods neural network based models and attention mechanism which have been used in the mrc tasks finally section concludes our review mrc corpus the fast development of the mrc eld is driven by various large and realistic datasets released in recent years each dataset is usually composed of documents and questions for testing the document understanding ability the answers for the raised questions can be obtained through seeking from the documents or selecting the preseted options here according to the formats of answers we classify the datasets into three types namely datasets with extractive answers with descriptive answers machine reading comprehension a literature review and with multiple choice answers and introduce them respectively in the following subsections in parallel to this survey there are also new datasets steadily coming out with more diverse task formulations and testing more complicated derstanding and reasoning abilities datasets with extractive answers to test a system s ability of reading comprehension this kind of datasets which originates from cloze style questions rstly provide the system with a large amount of documents or passages and then feed it with questions whose answers are segments of corresponding passages a good system should select a correct text span from a given context such comprehension tests are appealing because they are objectively gradable and may measure a range of important abilities from basic understanding to complex inference either sourced by crowdworkers or generated automatically from dierent corpus these datasets all use a text span in the document as the answer to the proposed question many of them released in recent years are large enough for training strong neural models these datasets include squad cnn daily mail cbt newsqa triviaqa wikihop which are described briey below squad one of the most famous dataset of this kind is stanford question ing dataset squad the stanford question answering dataset squad consists of questions posed by crowdworkers on a set of wikipedia articles where the answer to each question is a segment of text or span from the sponding reading passage squad contains question answer pairs from articles which is much larger than previous manually labeled rc datasets we quote some example question answer pairs as in fig where each answer is a span of the document in meteorology precipitation is any product of the condensation of atmospheric water vapor that falls under gravity the main forms of precipitation include drizzle rain sleet snow graupel and hail precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud short intense periods of rain in scattered locations are called showers what causes precipitation to fall gravity graupel within a cloud what is another main form of precipitation besides drizzle rain snow sleet and hail where do water droplets collide with ice crystals to form precipitation fig question answer pairs for a sample passage in the squad qa com xin zhang et al answer type percentage example date other numeric person location other entity common noun phrase adjective phrase verb phrase clause other october thomas coke germany abc sports property damage second largest returned to earth to avoid trivialization quietly table answer type distribution in squad article endangered species act paragraph other legislation followed including the migratory bird conservation act of a treaty prohibiting the hunting of right and gray whales and the bald eagle protection act of these later laws had a low cost to society the species were relatively rare and little opposition was raised question which laws faced signicant opposition plausible answer later laws question what was the name of the treaty plausible answer bald eagle protection act fig unanswerable question examples with plausible but incorrect answers in squad the answers belong to dierent categories as shown in table as we can see common noun phrases make up of the whole data proper noun phrases make up of the data and the rest one third consists of date numbers adjective phrase verb phrase clauses and so on this indicates that the answers of squad displays reasonable diversity as about the reasoning skills of squad to answer the questions the authors show that all examples at least have some lexical or syntactic divergence between the question and the answer in the passage through manually annotating some examples later squad was released with emphasis on unanswerable questions this new version of squad adds over unanswerable questions which were created adversarially by crowdworkers according to the original ones in order to challenge the existing models which tend to make unreliable guesses on questions whose answers are not stated in context newly added questions are highly similar to corresponding context and have plausible but incorrect answers in context we also quote some examples as shown in fig the unanswerable questions in squad are posed by humans and exhibit much more diversity and delity than those in other automatic constructed datasets in such cases simple heuristics which are based on overlapping or entity type recognition are not able to distinguish answerable from unanswerable questions cnn daily mail cnn and daily mail dataset which was released by google deepmind and university of oxford in is the rst large scale reading consisting of person location and other entities machine reading comprehension a literature review original version context the bbc producer allegedly struck by jeremy clarkson will not press charges against the top gear host his lawyer said friday clarkson who hosted one of the most watched television shows in the world was dropped by the bbc day after an internal investigation by the british broadcaster found he had subjected producer oisin tymon to an unprovoked physical and verbal attack query producer x will not press charges against jeremy clarkson his lawyer says anonymised version the producer allegedly struck by will not press charges against the host his lawyer said friday who hosted one of the most watched television shows in the world was dropped by the wednesday ter an internal investigation by the broadcaster found he had subjected ducer to an unprovoked physical and verbal attack producer x will not press charges against his lawyer says answer oisin tymon table an example data point quoted from hension dataset constructed from natural language materials unlike most relevant work which uses templates or syntactic semantic rules to extract document answer triples this work collects articles from the and articles from the daily as the source text since each article comes along with a number of bullet points to summarize the article this work converts these bullet points into document query answer triples with the cloze style questions to exclusively examine a system s ability of reading comprehension rather than using world knowledge or co occurrence further modications are implemented on those triples to construct an anonymized version that is each entity is anonymized by using an abstract entity marker which is not easily predicted by using world knowledge or n gram language model an example data point and its anonymized version is shown in table some basic statistics of cnn and daily mail are shown in table we also quote the percentages of the right answers appearing in the top n most frequent entities in an given document as in table illustrating the diculty degree of the questions to some extent cbt the children s book test is a part of babi project of facebook ai which aims at researching automatic text understanding and reasoning dren books are chosen because they ensure a clear narrative structure which aids this task the children stories used in cbt come from books freely available from project questions are formed by enumerating consecutive sentences from chapters in books of which the rst sentences serve as context and the last one as query after removing one word candidates are selected from words appearing in either context or query an example question is given in fig and the dataset size is shown in table www cnn com www dailymail co fb com downloads gutenberg org xin zhang et al cnn daily mail top n cumulative train valid test train valid test cnn daily mail months documents queries max entities avg entities avg tokens vocab size table corpus statistics of cnn and daily mail table percentage of rect answers contained in the top n most frequent entities in a given document quoted from fig an cbt example quoted from in cbt four distinct types of word named entities common nouns verbs and are removed respectively to form classes of questions for each class of questions the nine wrong candidates are selected randomly from words which have the same type as the answer options in the corresponding context and query compared to human performance on this dataset the state of art models like recurrent neural networks rnns with long short term memory lstm performed much worse when predicting nouns or named entities whereas they did great job in predicting prepostions and verbs this may probably be explained by the fact that these models are almost based exclusively on local contexts in contrast memory networks can exploit a wider context and outperform the conventional models when predicting nouns or named entities thus this corpus encourages the use of world knowledge in comparison with cnn daily mail and therefore focuses less on paraphrasing parts of a context newsqa based on news articles from news the newsqa dataset contains question answer pairs generated by crowdworkers similar to squad based on output from the pos tagger and named entity recognizer in the stanford core nlp toolkit www cnn com machine reading comprehension a literature review training validation test number of books number of questions average words in contexts average words in queries distinct candidates vocabulary size table corpus statistics of cbt answer type example proportion date time numeric person location other entity common noun phr adjective phr verb phr clause phr prepositional phr other march million ludwig van beethoven torrance california pew hispanic center federal prosecutors hour suered minor damage trampling on human rights in the attack nearly half table answer types distribution of newsqa the answer to each question is a text span of arbitrary length in the ing article a null span is also included cnn articles are chosen as source materials because in the authors view machine comprehension systems are particularly suited to high volume rapidly changing information sources like news the major ferences between cnn daily mail and newsqa are that the answers of newsqa are not necessarily entities and therefore no anonymization procedure is considered in the generation of newsqa the statistics of answer types in newsqa is shown in table as can be seen in the table the variety of answer types is ensured furthermore the authors sampled examples from newsqa and squad respectively and analyzed the possible reasoning skills to answer the questions the results indicate that compared to squad a larger proportion of questions in newsqa require high level reasoning skills including inference and synthesis besides while simple skills like word ing and paraphrasing can solve most questions in both datasets newsqa tends to require more complex reasoning skills than squad the detailed comparison result is given in table triviaqa instead of relying on crowdworkers to create question answer pairs from selected passages like newsqa and squad over k triviaqa answer evidence triples are generated through automatic procedures firstly a huge amount of question answer pairs from trivia and quiz league websites are ered and ltered then the evidence documents for each question answer pair are collected from either web search results or wikipedia articles finally a clean free and human annotated subset of triples from triviaqa is given and an triple example is shown in fig the basic statistics of triviaqa is given in table by sampling examples from the dataset and annotating them manually it turns out that the wikipedia titles including person organization location and miscellaneous consists of over of reasoning example word matching paraphrasing inference synthesis q when were the ndings published s both sets of research ndings were lished thursday the struggle between in q who is rwanda s the struggle pits ethnic tutsis ported by rwanda against ethnic hutu backed by congo q who drew inspiration from presidents s rudy ruiz says the lives of us presidents can make them positive role models for dents q where is brittanee drexel from s the mother of a year old rochester new york high school student says she did not give her daughter permission to go on the trip brittanee marie drexel s mom says house s barack obama s mother in law marian robinson will join the obamas at the family s private quarters at vania avenue michelle is never mentioned xin zhang et al proportion newsqa squad ambiguous insucient q whose mother is moving to the white table reasoning skills used in newsqa and squad and their corresponding proportions question the dodecanese campaign of wwii that was an attempt by the allied forces to capture islands in the aegean sea was the inspiration for which acclaimed commando lm answer the guns of navarone excerpt the dodecanese campaign of world war ii was an attempt by allied forces to capture the italian held dodecanese islands in the aegean sea following the der of italy in september and use them as bases against the german controlled balkans the failed campaign and in particular the battle of leros inspired the novel the guns of navarone and the successful movie of the same name question american callan pinckney s eponymously named system became a selling book video franchise in what genre answer fitness excerpt callan pinckney was an american tness professional she achieved precedented success with her callanetics exercises her books all became tional best sellers and the video series that followed went on to sell over million copies pinckney s rst video release callanetics years younger in hours outsold every other tness video in the us fig example question answer evidence triples in triviaqa quoted from all answer and the rest small percentage of answers mainly belong to numerical and free text type the average number of entities per question and the percentages of certain types of questions are also shown in table wikihop wikihop was released for the purpose of evaluating a system s ability of multi hop reasoning across multiple documents in in most existing datasets the information needed to answer a question is usually contained in only one machine reading comprehension a literature review total number of qa pairs number of unique answers number of evidence documents avg question length word avg document length word table corpus statistics of triviaqa property example annotation statistics avg entities question fine grained answer type what fragrant essential oil is obtained from damask of questions coarse grained answer typewho won the nobel peace prize in of questions time frame comparisons what was photographed for the rst time in october of questions what is the appropriate name of the largest type of frog which politician won the nobel peace prize in per question of questions table properties of questions on sampled examples the boldfaced words mean the presence of the corresponding properties wikihop medhop train dev test total table dataset sizes of wikihop and medhop sentence which makes current mrc models pay much attention on simple reasoning skills like locating matching or aligning information between query and support text for example in squad the sentence which has the highest lexical similarity with the question contains the answer about of the time and a simple binary word in query indicator feature boosted the relative accuracy of a baseline model by to move beyond this the authors dene a novel mrc task in which a model needs to combine evidences in dierent documents to answer the questions a sample in wikihop which displays such characteristics is shown in fig to construct wikihop the authors collect s r o triples with subject entity s relation r and object entity o from wikidata then wikipedia articles associated with the entities are added as candidate evidence documents d the triple becomes a query after removing answer from it that is q s r and a o to reach the goal of multi hop reasoning bipartite graphs are constructed for the help of construction as shown in fig vertices on two sides respectively correspond to the entities and the documents from the knowledge base and edges denote the entities appear in the corresponding documents for a given q a pair the answer candidates cq and support documents sq d are identied by traversing the bipartite graph using search the documents visited will become the support documents sq another dataset medhop is constructed in the same way as wikihop with the focus on the medicine area some basic statistics of wikihop and medhop are shown in table and table table lists the proportions of dierent types of answer samples which indicates that to perform well on wikihop one system needs to be good at multi step reasoning xin zhang et al the hanging gardens in also known as pherozeshah mehta gardens are terraced gardens they provide sunset views over the arabian sea also known as bombay the ocial name until is the capital city of the indian state of maharashtra it is the most populous city in india the arabian sea is a region of the northern indian ocean bounded on the north by pakistan and iran on the west by northeastern somalia and the arabian peninsula and on the east by india question hanging gardens of mumbai country options iran india pakistan somalia fig a sample of wikihop quoted from which displays the necessity of multi hop reasoning across several documents fig a bipartite graph given in paper connecting entities and documents mentioning them bold edges are those traversed for the rst fact in the small kb on the right yellow highlighting indicates documents in sq and candidates in cq check and cross indicate correct and false candidates descriptive answer datasets instead of text spans or entities obtained from candidate documents descriptive answers are whole stand alone sentences which exhibit more uency and integrity in addition in real world many questions may not be answered simply by a text span or an entity what s more presenting answers with their supporting evidence and examples is preferred by human so in light of these reasons some descriptive answer datasets are released in recent years next we mainly introduce two of them in detail namely ms marco and narrativeqa ms marco ms marco microsoft machine reading comprehension is a large dataset released by microsoft in this dataset aims to address questions and documents in the real world sourced from real anonymized queries issued through r r machine reading comprehension a literature review cand wh docs wh tok wh cand mh docs mh tok mh min max avg median table corpus statistics of wikihop and medhop wh wikihop mh medhop unique multi step answer likely multi step unique answer multiple plausible answers ambiguity due to hypernymy only single document required answer does not follow wikidata wikipedia discrepancy table qualitiative analysis of sampled answers of wikihop or and the corresponding searching results from bing search engine ms marco can well reproduce qa situations in real world for each question in the dataset a crowdworker is asked to answer it in the form of a complete sentence using passages provided by bing the unanswerable questions are also kept in the dataset for the purpose of encouraging one system to judge whether a question is answerable due to scanty or conicting materials the rst version of ms marco released in has about questions and the latest version released in has over questions both are now available at msmarco org the dataset compositions of ms marco are shown in table and the tribution of dierent types of questions are shown in table from this table we can see that not all of them contain interrogatives because the queries come from real users we can also see that the interrogative what is contained in of the queries and description questions account for the major question type generally interrogative distribution in questions shows reasonable diversity is another dataset with descriptive answers released by deepmind and university of oxford in narrativeqa is specically designed to examine how well a system can capture the underlying narrative elements to answer those questions which can not be answered by simple pattern recognition or global salience from an example of question answer pair shown in fig we can see that relatively high level abstraction or reasoning is required to answer the question the stories used in narrativeqa consist of books from project and movie scripts from relative each story as well as its plot summary is nally provided to crowdworkers to create question answer pairs because the www bing com microsoft com en us cortana gutenberg mainly from imsdb and also from dailyscript and awesomelm xin zhang et al field description query a question query issued to bing passages top passages from web documents as retrieved by bing the passages are presented in ranked order to human editors the passage that the editor uses to compose the answer is annotated as is selected document urls urls of the top ranked documents for the question from bing the passages are extracted from these documents answers composed by human editors for the question matically extracted passages and their corresponding ments well formed well formed answer rewritten by human editors and the inal answer segment qa classication e tallest mountain in south america longs to the entity segment because the answer is an entity aconcagua table the ms marco dataset composition percentage of question question segment question types yesno what how where when why who which other question classication description numeric entity location person table distribution of dierent question types in ms marco crowdworkers never see the full text it s less likely for them to create questions and answers solely based on localized context the answers can be full sentences which exhibit more articial intelligence when asked about factual information some basic statistics are shown in table and the distribution of dierent types of questions and answers are shown in table and table according to the original paper less than of answers appear as text segments of the stories which decreases the possibility of answering questions with simple skills for a system as before machine reading comprehension a literature review title ghostbusters ii question how is oscar related to dana answer her son summary snippet peter s former girlfriend dana barrett has had a son oscar story snippet dana setting the wheel brakes on the buggy thank you frank i ll get the hang of this eventually she continues digging in her purse while frank leans over the buggy and makes funny faces at the baby oscar a very cute nine month old boy frank to the baby hiya oscar what do you say slugger that s a good looking kid you got there ms barrett frank to dana fig an example question answer pair of narrativeqa given in paper documents books movie scripts question answer pairs avg tok in summaries max tok in summaries avg tok in stories max tok in stories avg tok in questions avg tok in answers train valid test table narrativeqa dataset statistics first token frequency category frequency what who why how where which how many much when in other person description location why reason how method event entity object numeric duration relation table frequency of rst token of the question in the training set of narrativeqa table question categories on a sample of questions from the validation set of xin zhang et al a q e v i a r r a n o c r a m s m p o h k w i i a q a i v i r t a q s w e n t b c n n c l i a m y l i a d d a u q s v d a u q s v v e v i a r r a n s t p i r c s w o r c e n i g n e h c r a e s l a r u t a n y r e u q e c r u o s i a m o t u a r e w s n a e v i a r r a n e v i t c a r t e e v i t c a r t e e v i t c a r t e e v i t c a r t e e v i t c a r t e e v i t c a r t e e v i t c a r t e a i e p i i w a i v i r t s o o b s w e n a i e p i i w a i e p i i w i a m o t u a l a r u t a n i a m o t u a i a m o t u a w o r c w o r c e c r u o s e c r u o s s w e n w o r c e c r u o s e s a e l e r e t a e y t n i a m o d n o i t s e u q e c r u o s a m e f m e f n a m u h e c n a m r o r e m e f e n n c m e f m e f m e f a t o s s t e s a t a e v i a r r a n d n a e v i t c a r t e l l a o o n i i s a b e l b a t n i a t n o c e l b a r e w s n a n u n o i t s e u q k s a t n o i a r e n e g e g u a g n a l l a r u t a n a q n o u e l b l e g u o r l e g u o r r o e t e m u e l b u e l b a m o i i w m o b e w m o i i w m o b e w f m e e n n c b v r p machine reading comprehension a literature review a q e v i a r r a n o c r a m s m p o h k w i i a q a i v i r t a q s w e n t b c l i a m y l i a d v v n n c d a u q s d a u q s e h g g a a w a r t n e m u c o g g t n e m u c o d e b m u n e g a r e v a h t g n e l t n e m u c o d r e u q e b m u n g a r e v a h t g n e l r e u q c g i xin zhang et al s t e s a t a e v i a r r a n d n a e v i t c a r t e l l a o n o i a m o n i s c i t s i a t s e l b a t j g a r e v a h t g n e l e w s n a s r e p a p l a n i g i r o g n i n o p s e r r o c m o r e m o c s c i t s i a t s r e h t o e i c e s s s e l n u s e v l e s r u o y b e t n u o c e r a h t i w s c i t s i a t s e i c e s s s e l n u h t g n e l g n i a l u c l a c n e h w r e b m u n r o w e s u r e r a m y t i n e n a s i e w s n a e h t n o i s r e v e s i m y n o n a t s e t v e n i a r t s e l c i t r a a i e p i i w t s e t v e n i a r t s w e n s h t n o m e l b a l i a v a n u s i a t a g n i n o p s e r r o c t s e t v e n i a r t r e b m u n s e l c i t r a s o o b c s w e n t s e t v e n i a r t s h p a r g a r a p g t s e t v e n i a r t s e i r o t s s t n e m u c o d b e w l l u f m o r t l u s e r s e g a s s a p h a e i j t l u a e d e h t machine reading comprehension a literature review james the turtle was always getting in trouble sometimes he d reach into the freezer and empty out all the food other times he sled on the deck and get a splinter his aunt jane tried as hard as she could to keep him out of trouble but he was sneaky and got into lots of trouble behind her back one day james thought he would go into town and see what kind of trouble he could get into he went to the grocery store and pulled all the pudding o the shelves and ate two jars then he walked to the fast food restaurant and ordered bags of fries he nt pay and instead headed home his aunt was waiting for him in his room she told james that she loved him but he would have to start acting like a well behaved turtle after about a month and after getting into lots of trouble james nally made up his mind to be a better turtle what is the name of the trouble making turtle a fries b pudding c james d jane what did james pull o of the shelves in the grocery store a pudding b fries c food d splinters where did james go after he went to the grocery store a his deck b his freezer c a fast food restaurant d his room what did james do after he ordered the fries a went to the grocery store b went home without paying c ate them d made up his mind to be a better turtle fig a sample of mctest given in paper multiple choice datasets with descriptive answers are relatively dicult to evaluate the system performance precisely and objectively nevertheless multiple choice question which has long been used for testing students reading comprehension ability can be jectively gradable generally this kind of questions can extensively examine one s reasoning skills including simple pattern recognition clausal inference and sentence reasoning of a given passage in light of this many datasets in this format are released and listed as follows mctest mctest a high quality dataset consisting of stories and tions about ction stories was released in by microsoft with the same format as race targeting at year old children passages and questions used in mctest are quite easy and understandable which reduces the world knowledge requisite for mctest many answers can only be found in the story since the stories are ctional the main drawback of mctest is that its size is too small to train a well performed model a sample of mctest is shown in fig race race contains passages and questions that are collected from english exams for middle and high school chinese students considering that those passages and questions are specically designed by english teachers and experts to evaluate reading comprehension ability of students this dataset is promising in developing and testing mrc systems because the questions are created with high quality by human experts there are few noises in race what s more passages in race cover a wide range of topics xin zhang et al release date type race cloth mctest mcscript arc coqa multiple choice multiple choice domain exam exam question source natural natural multiple choice fiction stories crowd multiple choice script scenarios crowd multiple choice multiple choice science widea natural crowd human performance b sota c d contain unanswerable question test common sense specically raw document document number average length of document query number average length of query average length of answer g e k a children s stories literature mid high school exams news wikipedia science reddit b race m race h c total middle high e in domain out of domain f race m race h g scenarios h science related sentences i stories j texts k passages table basic information and statistics of all multiple choice datasets machine reading comprehension a literature review t i wanted to plant a tree i went to the home and garden store and picked a nice oak afterwards i planted it in my garden what was used to dig the hole a a shovel b his bare hands when did he plant the tree a after watering it after taking it home fig example questions of mcscript overcoming the topic bias problem that commonly exists in other datasets like news articles for cnn daily mail and wikipedia articles for squad a sample of race is shown in table the dataset rstly provides dents systems with a passage to read then presents several questions with didate answers words in the questions and candidate answers may not appear in the passage so simple context matching techniques will not aid as much as in other datasets analysis in the paper shows that reasoning skill is indispensable to answering most questions of race correctly race is divided into two subsets namely race m and race h for middle school and high school respectively some basic statistics of race is given in table and table distributions of dierent reasoning types required to answer certain questions are illustrated in table denoting that over half of the questions in race requires reasoning skill cloth cloth cloze test by teachers was constructed with the format of cloze questions it is also composed of english tests for chinese middle school and high school one example is shown in table in cloth the missing blanks in the questions were carefully designed by teachers to test dierent aspects of language knowledge the candidate answers usually have subtle dierences making the tions dicult to answer even for human similar to race cloth is also divided into two parts cloth m for middle school and cloth h for high school ones some basic statistics of this corpus are shown in table through experiments on cloth the authors came to the conclusion that the performance gap between human and a system mainly results from the ability of using a long term context or multiple sentence reasoning mcscript mcscript focuses on questions that need reasoning using sense knowledge released in march this new dataset provides stories describing people s daily activities in which ambiguity and implicitness can be resolved easily by commonsense with crowdworkers to generate questions the correct answers to the questions may not appear in the given text as is shown in the examples in fig it consists of about k texts and k questions according to statistical analysis of all the questions in mcscript require commonsense knowledge to answer thus this dataset can literally examine systems commonsense inference ability all questions in the dataset are answerable the distribution of the questions types in mcscript is shown in fig xin zhang et al passage in a small village in england about years ago a mail coach was standing on the street it did nt come to that village often people had to pay a lot to get a letter the person who sent the letter did nt have to pay the postage while the receiver had to here s a letter for miss alice brown said the mailman i m alice brown a girl of about said in a low voice alice looked at the envelope for a minute and then handed it back to the mailman i m sorry i ca nt take it i do nt have enough money to pay it she said a gentleman standing around were very sorry for her then he came up and paid the postage for her when the gentleman gave the letter to her she said with a smile thank you very much this letter is from tom i m going to marry him he went to london to look for work i ve waited a long time for this letter but now i do nt need it there is nothing in it really how do you know that the gentleman said in surprise he told me that he would put some signs on the envelope look sir this cross in the corner means that he is well and this circle means he has found work that s good news the gentleman was sir rowland hill he did nt forgot alice and her letter the postage to be paid by the receiver has to be changed he said to himself and had a good plan the postage has to be much lower what about a penny and the person who sends the letter pays the postage he has to buy a stamp and put it on the envelope he said the government accepted his plan then the rst stamp was put out in it was called the penny black it had a picture of the queen on it questions the rst postage stamp was made a in england b in america c by alice d in the girl handed the letter back to the mailman because a she did nt know whose letter it was b she had no money to pay the postage c she received the letter but she did nt want to open it d she had already known what was written in the letter we can know from alice s words that a tom had told her what the signs meant before leaving b alice was clever and could guess the meaning of the signs c alice had put the signs on the lope herself d tom had put the signs as alice had told him to table a sample of race quoted from the idea of using stamps was thought of by a the government b sir rowland hill c alice brown d tom from the passage we know the high postage made a people never send each other letters b lovers almost lose every touch with each other c people try their best to avoid paying it d receivers refuse to pay the coming letters answer adabc machine reading comprehension a literature review race m dataset subset passages questions race h train dev test train dev test train dev test race all table the basic statistics of the training development and test sets of race m race h and race dataset passage len question len option len vocab size race m race h race table statistics of race where len denotes length and vocab denotes vocabulary dataset word matching paraphrasing single sentence reasoning multi sentence reasoning ambiguous insucient race m race h race cnn squad newsqa table distribution of reasoning type in race and other datasets denotes quoting based on samples per dataset and quoting fig distribution of question types in mcscript arc reasoning challenge makes use of standardized tests whose questions are objectively gradable and exhibit the variety in diculty which can be a grand challenge for ai arc consists about k questions the authors of arc also designe two baselines namely a retrieval based gorithm and a word co occurrence algorithm the challenge set a subset of arc how many how long when where why who what y n xin zhang et al passage nancy had just got a job as a secretary in a company monday was the rst day she went to work so she was very and arrived early she the door open and found nobody there i am the to arrive she thought and came to her desk she was surprised to nd a bunch of on it they were fresh she them and they were sweet she looked around for a to put them in somebody has sent me owers the very rst day she thought but who could it be she began to the day passed quickly and nancy did everything with interest for the following days of the the rst thing nancy did was to change water for the followers and then set about her work then came another monday she came near her desk she was overjoyed to see bunch of owers there she quickly put them in the vase the old ones the same thing happened again the next monday nancy began to think of ways to nd out the on tuesday afternoon she was sent to hand in a plan to the she waited for his directives at his secretary s she happened to see on the desk a half opened notebook which in order to keep the secretaries in high spirits the company has decided that every monday morning a bunch of fresh owers should be put on each secretarys desk later she was told that their general manager was a business management psychologist questions b pushed b second b grapes d forced d rst d bananas d held d bottle a depressed b encouraged c excited d surprised a turned a last a keys a smelled b ate a vase a angrily a seek a low a month a unless a old a covering b demanding c replacing d forbidding a sender b receiver a assistant b colleague c employee d manager a notebook b desk a said c knocked c third c owers c took c glass c strangely d happily c work c great c year c since c blue b room b quietly b wonder b little b period b when b red d ask d general d week d before d new c secretary d waiter c oce c printed d house d signed b written table a sample passage of cloth bold faces highlight the correct answers there is only one best answer among four candidates although several candidates may seem correct dataset cloth m cloth h cloth train dev test train dev test train dev test passages questions vocab size avg sentence avg words table the statistics of the training development and test sets of cloth and two subsets from paper containing about k questions is created by gathering questions that are answered incorrectly by both of these two baselines the easy set is composed of the ing k questions several state of the art models are tested on the challenge set but none of them are able to signicantly outperform a random baseline which reects the diculty of the challenge set two example questions of the challenge set questions are as follows machine reading comprehension a literature review train dev test total challenge easy total table number of questions in arc grade challenge qns easy qns qns table grade level distribution of arc questions qns property question words question sentences answer option words answer options min average max challenge easy table properties of the arc dataset in which property of a mineral can be determined just by looking at it a ter correct b mass c weight d hardness a student riding a bicycle observes that it moves faster on a smooth road than on a rough road this happens because the smooth road has a less gravity b more gravity c less friction correct d more friction for example the rst question is dicult in that the ground truth luster can be determined by looking at something only appears as a stand alone sentence in the web text however the incorrect candidate hardness has a strong correlation with mineral in the text the arc corpus a scientic text which contains m science related sentences and mentions of the knowledge related to the challenge set questions according to a sample analysis is released along with the arc questions set the use of the corpus is optional some statistics of arc is shown in table table and table coqa question answering systems is a conversational style datasets which consists of questions sourced from conversations in dierent domains answers of questions are in free form the motivation of coqa is that in daily life human usually get information by asking questions in conversations and so it is desirable for a machine to be capable of answering such questions coqa xin zhang et al jessica went to sit in her rocking chair today was her birthday and she was turning her granddaughter annie was coming over in the afternoon and jessica was very excited to see her her daughter melanie and melanie s husband josh were coming as well jessica had who had a birthday jessica jessica went to sit in her rocking chair today was her birthday and she was turning how old would she be she was turning did she plan to have any visitors yes her granddaughter annie was coming over how many three her granddaughter annie was coming over in the afternoon and jessica was very excited to see her her daughter melanie and melanie s husband josh were coming as well who annie melanie and josh her granddaughter annie was coming over in the afternoon and jessica was very excited to see her her daughter melanie and melanie s husband josh were coming as well fig a conversation example from the coqa each turn contains a question qi an answer ai and a rationale ri that supports the answer rstly provides models with a text passage to understand and then presents a series of questions that appear in a conversation one example is given in fig the key challenge of coqa is that a system must handle conversation history properly to tackle problems like resolving the coreference among domains of the passages from which the questions are collected are used for cross domain uation and are used for in domain evaluation the distribution of domains are shown in table some linguistic phenomena statistics are given in table the coreference and pragmatics are unique and challenging linguistic phenomena that do not appeare in other datasets in this section we will introduce dierent techniques employed in mrc mrc techniques non neural method before the neural networks came into fashion many mrc systems were oped based on dierent non neural techniques which now mostly serve as baselines for comparison next we will introduce the techniques including tf idf sliding window logistic regression and boosted method machine reading comprehension a literature review domain passages passage length turns per passage q a pairs out of domain children s sto literature mid high sch news wikipedia science reddit total lexical match paraphrasing pragmatics no coref explicit coref implicit coref table distribution of domains in coqa in phenomenon example percentage relationship between a question and its passage q who had to rescue her a the coast guard r outen was rescued by the coast guard q did the wild dog approach a yes r he drew cautiously closer q is joey a male or female a male r it looked like a stick man so she kept him she named her new noodle friend joey q what is ifl q who had bashti forgotten a the puppy q what was his name q when will sirisena be sworn in a p m local time q where relationship between a question and its conversation history table linguistic phenomena in coqa questions given by paper tf idf the tf idf term frequency inverse document frequency technique is widely used in the information retrieval area and nds a place in the mrc tasks later as validated before if candidate answers are presented retrieval based models can serve as a strong baseline this kind of baseline is widely used in document datasets such as wikihop by solely exploiting lexical correlation tween the concatenation of a candidate answer and the query and a given document this kind of algorithm can predict the candidate with the highest similarity score among all documents because the inter document information is usually ignored by tf idf this baseline can not detect how much a question rely on cross document reasoning xin zhang et al sliding window the sliding window algorithm is constructed as a baseline in the dataset mctest it predicts an answer based on simple lexical information in a sliding window inspired by tf idf this algorithm uses inverse word count as weight of each word and maximize the bag of word similarity between the answer and the sliding window in the given passage logistic regression this baseline method is proposed in squad it extracts a large mount of features from the candidates including lengths bigram frequencies word frequencies span pos tags lexical features dependency tree path features and predicts whether a text span is the nal answer based on all those information boosting method this model is proposed as a conventional feature based baseline for cnn daily mail dataset since the task can be seen as a ranking problem making the score of the predicted answer rank top among all the candidates the authors turn to the implementation of lambdamart in ranklib a highly successful ranking algorithm using forests of boosting decision trees through feature engineering features are chosen to form a feature vector which represents a candidate and the weight vector will be learnt so that the correct answer will be ranked the highest neural based method with the popularity of neural networks end to end models have produced ing results on some mrc tasks these models do not need to design complex manually devised features that traditional approaches relied on and perform much better than them next we will introduce several end to end models mainly in chronological order match network as the rst end to end neural architecture posed for squad this model combines the match lstm which is used to get a query aware representation of passage and the pointer network which aims to construct an answer so that every token within it comes from the input text an overall picture of the model architecture is given in fig match lstm is originally designed for predicting textual entailment in that task a premise and a hypothesis are given and the match lstm encodes the pothesis in a premise aware way for every token in hypothesis this model uses soft attention mechanism which will be discussed later in sect to get a weighted vector representation of premise this weighted vector is concatenated with a vector representation of the according token and both are fed into an lstm namely the match lstm in this paper the authors replace the premise and hypothesis with the query and passage to get a query aware representation of the given passage two preprocessing lstms are employed respectively to encode the query and the passage and a bidirectional match lstm is employed to obtain the query aware representation of the passage net p lemur wiki the details can be found in the paper machine reading comprehension a literature review fig the overview of two models in after getting the query aware representation of the passage a pointer net is employed to generate answers by selecting tokens from the input passage at each inference step ptr net uses soft attention mechanism to get a probability tribution of the input sequence and selects the token with the largest possibility as the output symbol moreover two dierent strategies are proposed for constructing the answer the sequence model assumes that every word in the answer can appear in any position in the passage and the length of the answer is not xed in order to tell the model to stop generating tokens after getting the whole answer a special symbol is placed at the end of the passage the prediction of this symbol indicates the termination of the answer generating the boundary model works dierently from the sequence model in that it only predicts the start indice as and the end indice ae in other word it s based on the assumption that the answer appears as a continuous segment of the passage the test result shows an advantage of the boundary model over the other one bi directional attention flow proposed by the bi directional attention flow has two key features at the context encoding stage first this model takes dierent levels of granularity as input including character level word level and contextualized embeddings second it uses bi directional attention ow namely a passage to query attention and a query to passage attention to get a query aware passage tation the detailed description is given as follows as is shown in fig the bidaf model has six layers the character ding layer and the word embedding layer map each each word into the vector space based respectively on character level cnns and the pre trained glove embedding the concatenation of these two word embeddings is passed to a two layer highway networks the output of which is provided to a bi directional lstm in the contextual embedding layer to rene the word embedding using xin zhang et al fig overview of bidaf architecture given in the context information these rst three layers are applied to both the query and the passage the attention flow layer is where the information from the query and the passage mixed and interacted instead of summarizing the passage and the query into a xed vector like most attention mechanisms do this layer grants raw information including attention vectors and the embeddings from previous layers owing to the subsequent layer which reduces the information loss the attentions are computed in two directions from passage to query and from query to passage the detailed information of the attention flow layer will be given in sect the modeling layer takes in the query aware representation of context words and used two bi directional lstm to capture the interactions among the passage words according to the query the last output layer is task specic which gives the prediction of the answer gated attention gated attention reader targets at realizing multi hop ing in answering cloze style questions over documents a multiplicative interaction between the query and the hidden state of the document is employed in its tion mechanism the multi hop architecture of the model imitates the multi step reasoning of human in reading comprehension the overview of the model is given in fig the model reads the document and the query iteratively in a row of k layers in kth layer the model uses bidirectional gated recurrent gru to transform the x dings of document passed from the last layer to get then a layer specic query representation is transformed by another bi gru to get gru k d gru k q y modeling layeroutput layerattention flow layercontextual embed layerword embed softmaxdense and cnncharacter embed machine reading comprehension a literature review fig gated attention architecture given in then both and are fed to a gated attention module the result of which x k will be passed to the next layer for each token in the gated attention module uses soft attention to get a token specied representation of query finally we get the new embeddings of this token xi by applying a element wise multiplication for qi and i softmax qi qi qi at the last stage the decoder employs a softmax layer to the inner product between outputs of last layer to get the possibility distribution of the predict answers xq n dcn dynamic coattention introduces coattention mechanism to combine co dependent representations of query and the document and dynamic iteration to avoid been trapped in local maxima corresponding to incorrect answers like previous single pass models the dynamic pointer decoder takes in the output of coattention encoder and generates the nal predictions detailed procedures is given as follows xq xq m let in the document and question encoder the vector representations of the ment and the query are fed into lstm respectively and the hidden states at each step are combined to form the encoding matrix d dmd and qnq sentinel vector d and q is appended to the encoding matrix to enable the model to map some unrelated words that exclusively appear in either the query or the document to this void vector to allow for some variation between the document encoding space and the query encoding space a for those in document the the details of dcn are as follows denote the sequence of embeddings of words in query and xin zhang et al non linear projection q tanh applied to the nal representations of the document and the query are d and q the coattention encoder takes in d and q and outputs coattention encoding matrix u um which is the input to the dynamic pointing decoder the details of coattention encoder will be discussed in sec the overview of dynamic pointing decoder is given in fig to enable the model to recover from local maxima the highway maxout network hmn is posed to predict the start point and the end point iteratively hmn is composed of highway networks which is characterized by the skip connect that passes gradient eectively through deep networks and maxout networks a learnable activation function that has strong empirical performance during the iteration the hidden state of the decoder is updated according to eq hi lst m dec where and are the coattention representations of according start and end words predicted by iteration given hi and the possibility of the tth word to be the start or the end point is calculated by eq t hmn hi the word with the maximum possibility is selected as the prediction at current step the architecture of hmn is given in fig the mathematical description of hmn is given as follows hmn hi i w h t t max r tanh d t max ut t w t max where r is a non linear projection of the current state fastqa fastqa achieved competitive performance with simple architecture which questions the necessity of improving complexity of qa systems unlike many systems that employed a complex interaction layer to catch the interaction between the query and the context fastqa only makes use of computable features on word levels the overview of fastqa architecture is given in fig the binary word in feature indicates whether a token in passages appears in the corresponding query wiqb j i i the weighted which is dened as below takes the term frequency and the similarity between query and context into account simi j vwiq xj qi x wiqw j softmax vwiq rn machine reading comprehension a literature review fig architecture of dynamic decoder from paper blue denotes the variables and functions related to estimating the start position whereas red denotes the variables and tions related to estimating the end position fig architecture of highway maxout network given in the concatenation of these two features and the original representation of each words is fed into a bi lstm to get the nal hidden state the answer layer is composed of a simple layer feed forward network along with a beam search r net the r net was proposed in by msra and achieved state of art results on squad and ms marco an overview of its architecture is shown in fig given the word level and character level embeddings r net rstly employs a bi directional gru to encode the questions and passages then it uses a gated attention based recurrent network to fuse the information from the question and hmnargmax u xin zhang et al fig overview of fastqa architecture from passage later a self matching layer is used to ne tune and get the nal tation of the passage the output layer is based on pointer networks similar to that in match lstm to predict the boundary of the answer the initial hidden vectors of the pointer network are computed by an attention pooling over the nal passage representations the gated attention based recurrent network adds another gate to normal based recurrent networks this gate gives the weight of certain passage information according to the question inspired by the sentence pair representations are obtained as follows t t ct t ct vp t rnn vp gt t ct j vt tanh st i exp at i ct m u uq u up j w p w q exp j i t w p v vp where gt sigmoid original representations of the passage and the question is the added gate t ct t and om n uq t are to exploit information from the whole passage for each token a self matching attention is applied to get the nal representation of the passage hp the details of self matching attention is given in sec the output layer uses pointer networks to predict the start and end position of the answer the initial hidden vector for the pointer network is an pooling over the question representation hp the objective function is the sum of the negative log probabilities of the ground truth start and end position by the predicted distributions reasonet unlike previous models which have xed number of turns during reading or reasoning regardless of the complexity of queries and passages the reasonet makes use of reinforcement learning to dynamically determine the reading and x machine reading comprehension a literature review fig overview of the r net architecture from paper reasoning depth the intuition of this work comes from that the diculty of dierent questions can vary a lot in the same dataset and the fact that human usually revisit important part of passage and question to answer the question better an overview of reasonet structure is given in fig the external memory m is usually the word embeddings encoded by a bi rnn the internal state s is updated according to rnn st xt s where xt is the attention vector fatt st m x the termination gate determines when to stop updating states above and predict the answers according to the binary variable tt tt st tg in this way the reasonet can mimic the inference process of human exploit the passages and answer the questions better qanet most of the models above are primarily based on rnns with attention therefore are often slow for both training and inference due to the sequential nature of rnns to make machine comprehension fast the qanet are proposed without rnns in its architecture an overview of qanet structure is given in fig the key dierence between qanet and the previous models is that qanet only use convolutional and self attention mechanism in its embedding and modeling encoders discarding the commonly used rnns the depthwise separable tions can capture the local structure of the text and the multi head attention mechanism will model global interactions within the whole passages a query to context attention similar to that in dcn is applied afterwards the qanet achieved state of the art accuracy while achieving up to speedup in training and per training iteration compared to the rnn counterparts xin zhang et al fig overview of reasonet structure from attention the attention mechanisms have shown great power in selecting important mation aligning and capturing similarity between dierent part of input next we will introduce several representative attention mechanism primarily based on time order hard attention was proposed in image caption task in as the stochastic hard attention let a al ai rd denote the feature vectors captured by cnn each corresponding to a part of the image when deciding which one of all features is to feed to the decoder lstm to generate caption a one hot variable st i is dened the indicator st i is set to if the i th vector of a is the one used to extract visual features at current step t if we denote the input of decoder lstm as zt the paper assigns a multinoulli distribution parametrized by t i and view zt as a random variable x zt st iai i p st i t a t i eti fatt ai ti pl truememory m machine reading comprehension a literature review fig overview of the qanet architecture left which has several encoder blocks all coder blocks are the same except that the number of convolutional layers for each varies from where fatt is a multilayer perceptron after dening the objective function ls as below ls log a log a x s x s log and approximate its gradient by a monte carlo method the nal learning rule for the model is then ls w n n x log a w a b log p w e h w where the r and e are two hyperparameters set by crossvalidation although hard attention is tricky and troublesome in training once trained well it can perform better than soft attention for the sharp focus on memory provided xin zhang et al soft attention here we will rst introduce the basic form of soft attention in neural machine translation task then we will talk about its variants in other tasks like natural language and mrc dierent to hard attention soft attention calculates a weight distribution among all the input representations and use the weighted sum of them as the input to the decoder for example in let htx denote the encoder s output sequence and ij denote the weight of each which indicates to what extent is hj related to the current output token ti then the input to the decoder ci is ci ijhj txx the weights are calculated and learn through a feedforward neural network a ij exp eij exp eik ptx eij a hj in nli task input has two components namely a premise and a hypothesis and attention is used to exploit the interaction relation between these two parts take the match lstm as example we denote hs k as the resulting hidden states of the encoder lstm separately for premise and hypothesis when predicting the label of the hypothesis an attention weighted combinations of the hidden states of the premise is computed through a match lstm j and ht ak pm kjhs j kj p ekj we tanh j wtht k wmhm where ak is the attention vector stated above we ws wt wm is the parameters to be learned and hm is the hidden state of match lstm at position k finally ak is concatenated with ht k for predicting the result in mrc task we can regard the question as a premise and the passage as a hypothesis as it likes in the model match network by applying the attention mechanism we can get additional query information for each token in the passage which will improve the model performance compared to hard attention soft attention s advantage is that it is dierentiabile thus easy to train and fast in training and inference bi directional attention was proposed in bidaf compared to the attention scribed above it considers attention in two directions or query to attention and context to attention take bidaf as example given h and u the concatenation of the outputs of the lstms in contextual embedding layer the similarity matrix s is computed stj h u j u w u h u machine reading comprehension a literature review where w compute the attention weights and the attended query vectors by s is trainable parameters is elementwise multiplication then we can similarily the attention weights and attended context vectors are at softmax st u p j atju j softmax h p t t finally two attention vectors above are combined together with the original tual embeddings h through a vector fusing function the result of which serve as the base for future modeling or prediction the bi directional attention adds more information in the attention part compared to normal attention mechanism however as shown in the ablation study of the attention in this direction is less useful than the standard squad dev set the reason is that the query is usually short and the added information is relatively small than that of the other one coattention is proposed in the architecture of the coattention encoder in dcn is shown in fig in the coattention encoder the anity matrix l d q is calculated and normalized row wise and column wise to obtain aq the attention weights matrix across the document for each word of query and ad the attention weights matrix across the query for each word of document then the attention contexts for question are computed c q daq and concatenated with q to obtain the nal document representation c d c ad at the last step d c d is fed to a bidirectional lstm ut bi lstm cd t the result serves as the foundation for predicting the answer the hidden states form coattention encoding matrix u um similarly to bi directional attention the coattention mechanism utilizes tion information in two directions while in a dierent way it successively computes the attention contexts for the question and the document and fuses them to get a co dependent representation of document self matching attention is proposed in r net introduced before because many useful information exist in the passage context while they can not be captured by the traditional mainly exploits information in words surrounding window so the self matching attention mechanism is proposed to address this problem it collects evidence for each token vt from the whole passage and its according question information and the result hp is the nal passage representation hp t birnn t ct gt hp t ct t ct xin zhang et al fig architecture of co attention encoder from ct here refers to an attention pooling vector of the whole passage st j vt tanh i exp st at ct n j w p v vp w p v vp t exp i ivp i j and gt is the gate dene in sec uniquely self matching attention captures long distance information from the passage itself this helps r net in dealing with problems like coreference pre trained word representations how to eciently represent words as vectors which serve as the base of most of the modern mrc systems is a problem that concerns researchers very much previously one hot representation and n gram model were popular however those simple techniques met their limits in many tasks to address this problem many technologies have been proposed according to the time of occurrence we introduce them as follows moving further from feedforward neural net language and recurrent neural net language this paper proposed two novel models to learn the distributed representations of words namely the tinuous bag of words and the continuous skip gram model the architectures of these two models are given in fig the cbow model uses several history words and future words as input and maximizes the probability of correctly predicting the current word by contrast the skip gram model uses current word as input and tries to predict words within a certain range before and after the current word the result word vectors of both models achieved state of the art performance on several tests aqaddocumentproductconcatproductbi lstmbi lstmbi lstmbi lstmbi q cqcdutu machine reading comprehension a literature review fig architectures of cbow model and skip gram model from probability and ratio k solid k gas k water fashion fig from a ratio much greater than means word k correlate well with ice and a ratio much greater than means word k correlate well with stream glove the method belongs to local context window methods those ods can capture ne grained semantic and syntactic regularities of words eciently however they can not exploit global statistical information like latent semantic which belongs to global matrix factorization methods glove combines the advantages of these two family of methods glove takes the co occurrence probabilities of words into consideration and use the ratio of probabilities to reect the relations of dierent words for example if we denote the probability that word j appear in the context of word j as pij then the ratio pik pjk can tell the correlation between certain words an example is given in fig the glove model f takes the below form according to above phenomenon f wi wj wk pik pjk where w rd are word vectors f varies according to dierent constrains elmo one disadvantages of word vectors generated by above methods is that they are static thus are independent of application linguistic contexts this may lead input projection input projection cbow skip gram xin zhang et al to poor performance when it comes to polysemy in light of this elmo was proposed to addresses this problem elmo s model employs a bi lstm with character convolutions on the input then it jointly maximizes the log likelihood of the forward and backward directions and record the internal states p tn p p tn p tn n y n y log x lst m s n x log tn x lst m s finally a task specic linear combination of those internal states are used to obtain the elmo representation in this way elmo can capture context dependent aspects of word meaning as well as syntax information for each token if ne tuned on domain specic data the model usually performs better gpt compared to elmo gpt uses a variant of transformer instead of lstm to better capture the long term linguistic structure the overview of this work is given in fig given a corpus u un a standard language model with a multi layer transformer decoder is used log p x i u we wp hl transformer block i n p u softmax t e where k is the context window size u uk is the context vectors of tokens n is the number of layers we is the token embedding matrix and wp is the position embedding matrix all the parameters are trained using stochastic gradient descent the nal transformer blocks activation is denoted as hm l a supervised ne tuning can be applied in dierent down stream tasks as for some tasks like text classication only a linear output layer with parameters wy is needed to predict p xm softmax hm wy more recently its successor is released which is a scale up of gpt while with much larger volume has billion parameters and claimed to achieve state of the art performance on many language modeling however its code have not been released by the time this paper is written machine reading comprehension a literature review fig graph comes from paper left is transformer architecture and training tives used in this work right is input transformations for ne tuning on dierent tasks all structured inputs are converted into token sequences to be processed by gpt followed by a layer fig model architectures of bert gpt and elmo quoted from bert as shown in fig both elmo and gpt models only use unidirectional language models to learn the representation of tokens bert points out that this restriction has severely limited the eciency of the pre trained representation to address this problem two new prediction tasks are proposed to pre train bert in two direction namely the masked language model and the next sentence prediction inspired by the cloze task the masked language model is to predict the randomly masked tokens i d based on their context in the input in other words both the left and the right context will be taken into consideration when computing representations and to capture sentence level information and relationship a rized next sentence prediction task is to predict whether a sentence a is the next sentence of b the wordpiece embeddings are used in the input layer along with the ment embeddings and the position embeddings the input embeddings is the sum of above three embeddings as shown in fig the main architecture of bert s model is a multi layer bidirectional transformer encoder almost identical to the inal one similar to gpt when ne tuned on down steam tasks only an additional output layer with a minimal number of parameters is needed as shown in fig bert advanced state of the art results on nlp tasks a comparison of size of bert and gpt is given in table bert openai gptlstmelmolstmlstmlstmlstmlstmlstmlstmlstmlstmlstmlstm tn en en tn en xin zhang et al fig bert input representation fig task specic models overview from paper sep tok ntok tnsingle sentence berttok tok tok n tnsingle sentence b peroo label tmstart end spanclass labelberttok tok tok n sep tok ntok tokmsentence sentence machine reading comprehension a literature review model gp t bertbase bertlarge gp t parameters layers hidden size m m m m table hyperparameter comparison among similar models layers means the number of the transformer blocks conclusion in this paper we summarized advances in mrc eld in recent years in we briey introduced the history of mrc tasks and some early mrc systems in section we introduced recent datasets in three categories i e squad cnn daily mail cbt newsqa triviaqa and cloth in extractive format ms marco and narrative qa in narrative format and wikihop mctest race mcscript and arc in multiple choice format the coqa a novel dataset focuses on conversational questions is also included in section we rst go through several non neural methods including sliding window logistic regression tf idf and boosted method then more importantly the neural based models like dcn ga bidaf fastqa rnet reasonet and qanet afterwards we discussed and compared two important positions of these models namely the pre training technology and attention anism in detail we covered glove elmo and bert in section and hard attention soft attention bi directional attention coattention and self attention mechanisms in section all together we reviewed the major progress that has been made in recent years in mrc eld however the mrc direction is developing very fast and it is dicult to include all the newly proposed mrc work in this survey we hope this review will ease the reference to recent mrc advences and encourage more researchers to work on mrc eld references bahdanau d cho k bengio y neural machine translation by jointly learning to align and translate corr bengio y ducharme r vincent p jauvin c a neural probabilistic language model journal of machine learning research bobrow d g kaplan r m kay m norman d a thompson h winograd t gus a frame driven dialog system articial intelligence chen d bolton j manning c d a thorough examination of the cnn daily mail ing comprehension task arxiv preprint cho k van merrienboer b gulcehre c bahdanau d bougares f schwenk h bengio y learning phrase representations using rnn encoder decoder for statistical chine translation arxiv preprint chollet f xception deep learning with depthwise separable convolutions arxiv preprint pp clark p cowhey i etzioni o khot t sabharwal a schoenick c tafjord o think you have solved question answering try arc the reasoning challenge arxiv preprint xin zhang et al clark p cowhey i etzioni o khot t sabharwal a schoenick c tafjord o think you have solved question answering try arc the reasoning challenge arxiv preprint clark p etzioni o my computer is an honor studentbut how intelligent is it standardized tests as a measure of ai ai magazine clark p etzioni o khot t sabharwal a tafjord o turney p d khashabi d combining retrieval statistics and inference to answer elementary science questions in aaai pp deerwester s dumais s t furnas g w landauer t k harshman r indexing by latent semantic analysis journal of the american society for information science devlin j chang m w lee k toutanova k bert pre training of deep bidirectional transformers for language understanding arxiv preprint dhingra b liu h yang z cohen w w salakhutdinov r gated attention readers for text comprehension arxiv preprint goodfellow i j warde farley d mirza m courville a bengio y maxout networks arxiv preprint green jr b f wolf a k chomsky c laughery k baseball an automatic in papers presented at the may western joint ire aiee acm answerer computer conference pp acm hermann k m kocisky t grefenstette e espeholt l kay w suleyman m som p teaching machines to read and comprehend in advances in neural information processing systems pp hill f bordes a chopra s weston j the goldilocks principle reading children s books with explicit memory representations arxiv preprint hirschman l gaizauskas r natural language question answering the view from here natural language engineering hochreiter s schmidhuber j long short term memory neural computation jia r liang p adversarial examples for evaluating reading comprehension systems arxiv preprint joshi m choi e weld d s zettlemoyer l triviaqa a large scale distantly pervised challenge dataset for reading comprehension arxiv preprint kaiser l gomez a n chollet f depthwise separable convolutions for neural machine translation arxiv preprint kim y convolutional neural networks for sentence classication arxiv preprint t schwarz j blunsom p dyer c hermann k m melis g grefenstette e the narrativeqa reading comprehension challenge transactions of the association of computational linguistics lai g xie q liu h yang y hovy e race large scale reading comprehension dataset from examinations arxiv preprint lehnert w g a conceptual theory of question answering in proceedings of the international joint conference on articial intelligence volume pp morgan kaufmann publishers inc levy o seo m choi e zettlemoyer l zero shot relation extraction via reading comprehension arxiv preprint liu p j saleh m pot e goodrich b sepassi r kaiser l shazeer n generating wikipedia by summarizing long sequences arxiv preprint manning c surdeanu m bauer j finkel j bethard s mcclosky d the stanford corenlp natural language processing toolkit in proceedings of annual meeting of the association for computational linguistics system demonstrations pp merity s xiong c bradbury j socher r pointer sentinel mixture models arxiv preprint mikolov t chen k corrado g dean j ecient estimation of word representations in vector space arxiv preprint nguyen t rosenberg m song x gao j tiwary s majumder r deng l ms marco a human generated machine reading comprehension dataset arxiv preprint machine reading comprehension a literature review ostermann s modi a roth m thater s pinkal m mcscript a novel dataset for assessing machine comprehension using script knowledge arxiv preprint pennington j socher r manning c glove global vectors for word representation in proceedings of the conference on empirical methods in natural language processing emnlp pp peters m e neumann m iyyer m gardner m clark c lee k zettlemoyer l deep contextualized word representations arxiv preprint radford a narasimhan k salimans t sutskever i improving language ing with unsupervised learning tech rep technical report openai rajpurkar p jia r liang p know what you do nt know unanswerable questions for squad arxiv preprint rajpurkar p zhang j lopyrev k liang p squad questions for machine comprehension of text arxiv preprint reddy s chen d manning c d coqa a conversational question answering challenge arxiv preprint richardson m burges c j renshaw e mctest a challenge dataset for the domain machine comprehension of text in proceedings of the conference on pirical methods in natural language processing pp richardson m burges c j renshaw e mctest a challenge dataset for the domain machine comprehension of text in proceedings of the conference on pirical methods in natural language processing pp robbins h monro s a stochastic approximation method in herbert robbins selected papers pp springer rocktaschel t grefenstette e hermann k m t blunsom p reasoning about entailment with neural attention arxiv preprint seo m kembhavi a farhadi a hajishirzi h bidirectional attention ow for machine comprehension arxiv preprint shankar s garg s sarawagi s surprisingly easy hard attention for sequence to shankar s sarawagi s label organized memory augmented neural network corr quence learning in emnlp shen y huang p s gao j chen w reasonet learning to stop reading in machine comprehension in proceedings of the acm sigkdd international conference on knowledge discovery and data mining pp acm simmons r f answering english questions by computer a survey tech rep system development corp santa monica calif srivastava r k gre k schmidhuber j highway networks arxiv preprint bulletin taylor w l a new tool for measuring readability journalism trischler a wang t yuan x harris j sordoni a bachman p suleman k newsqa a machine comprehension dataset arxiv preprint vaswani a shazeer n parmar n uszkoreit j jones l gomez a n kaiser l polosukhin i attention is all you need in advances in neural information processing systems pp vinyals o fortunato m jaitly n pointer networks arxiv e prints vinyals o fortunato m jaitly n pointer networks in advances in neural tion processing systems pp vrandecic d wikidata a new platform for collaborative data collection in proceedings of the international conference on world wide web pp acm wadhwa s embar v grabmair m nyberg e towards inference oriented reading comprehension parallelqa arxiv preprint wang s jiang j learning natural language inference with lstm arxiv preprint wang s jiang j machine comprehension using match lstm and answer pointer arxiv preprint wang w yang n wei f chang b zhou m gated self matching networks for reading comprehension and question answering in proceedings of the annual ing of the association for computational linguistics volume long papers vol pp xin zhang et al weissenborn d wiese g seie l fastqa a simple and ecient neural architecture for question answering corr weissenborn d wiese g seie l making neural qa as simple as possible but not simpler arxiv preprint weissenborn d wiese g seie l making neural qa as simple as possible but not simpler arxiv preprint welbl j stenetorp p riedel s constructing datasets for multi hop reading hension across documents transactions of the association of computational linguistics weston j chopra s bordes a memory networks corr winograd t understanding natural language cognitive psychology woods w a progress in natural language understanding an application to lunar geology in proceedings of the june national computer conference and exposition pp acm wu q burges c j svore k m gao j adapting boosting for information retrieval measures information retrieval wu y schuster m chen z le q v norouzi m macherey w krikun m cao y gao q macherey k al google s neural machine translation system bridging the gap between human and machine translation arxiv preprint xie q lai g dai z hovy e large scale cloze test dataset designed by teachers xiong c zhong v socher r dynamic coattention networks for question answering arxiv preprint arxiv preprint xu k ba j kiros r cho k courville a salakhudinov r zemel r bengio y show attend and tell neural image caption generation with visual attention in international conference on machine learning pp yih w t chang m w meek c pastusiak a question answering using enhanced lexical semantic models in proceedings of the annual meeting of the association for computational linguistics volume long papers vol pp yu a w dohan d luong m t zhao r chen k norouzi m le q v qanet combining local convolution with global self attention for reading comprehension arxiv preprint
