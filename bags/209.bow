noname manuscript inserted editor machine reading comprehension literature review xin zhang yang sujian li yizhong wang n u j l c s c v v x r received date accepted date abstract machine reading comprehension aims teach machines understand text like human new challenging direction articial intelligence article summarizes recent advances mrc mainly focusing aspects e corpus techniques specic characteristics mrc corpus listed compared main idea typical mrc techniques described keywords machine reading comprehension natural language processing introduction past decades growing interest making machine understand human languages recently great progress machine reading comprehension mrc view recent tasks titled mrc seen extended tasks question answering qa early simmons summarized dozen qa systems proposed preceding years review survey hirschman gaizauskas classies qa model categories natural language ends database dialogue interactive advisory systems question answering story comprehension qa systems rst category like baseball lunar system usually transform natural language questions query structured database based linguistic xin zhang peking university tel e mail edu yang peking university sujian li peking university yizhong wang peking university xin zhang et al knowledge performing fairly certain tasks suered constraints narrow domain database dialogue interactive advisory systems including shrdlu gus early models database knowledge source problems like ellipsis anaphora conservation systems struggled dealing remain challenge nowadays models category seen origin modern mrc tasks wendy lehnert rst proposed qa systems consider story question answer question necessary interpretation inference lehnert designed system called qualm according theory past decade witnessed huge development mrc eld including soar numbers corpus great progress techniques mrc corpus plenty datasets dierent domains styles released recent years mctest released choice reading comprehension dataset high quality small train neural models cnn daily mail cbt released datasets generated automatically dierentdomains larger previous datasets squad shown rst scale dataset questions answers written human techniques proposed competition dataset year ms marco released emphasis narrative answers quently newsqa narrativeqa constructed similar paradigm squad ms marco respectively datasets crowdsourced expectation high quality datasets sourced dierent domains sprung following years including race cloth arc collected exams based trivias mcscript primarily focused scripts released wikihop aimed examing systems ability multi hop reasoning coqa proposed test conversation ability models appearance large scale datasets makes training end end neural mrc model possible competing leaderboard models niques developed attempt conquer certain dataset word resentations attention mechanisms high level architectures neural models evolve rapidly surpass human performance tasks article aim extensive review recent datasets niques mrc section categorize mrc datasets types describe briey section introduce traditional non neural methods neural network based models attention mechanism mrc tasks finally section concludes review mrc corpus fast development mrc eld driven large realistic datasets released recent years dataset usually composed documents questions testing document understanding ability answers raised questions obtained seeking documents selecting preseted options according formats answers classify datasets types datasets extractive answers descriptive answers machine reading comprehension literature review multiple choice answers introduce respectively following subsections parallel survey new datasets steadily coming diverse task formulations testing complicated derstanding reasoning abilities datasets extractive answers test system s ability reading comprehension kind datasets originates cloze style questions rstly provide system large documents passages feed questions answers segments corresponding passages good system select correct text span given context comprehension tests appealing objectively gradable measure range important abilities basic understanding complex inference sourced crowdworkers generated automatically dierent corpus datasets use text span document answer proposed question released recent years large training strong neural models datasets include squad cnn daily mail cbt newsqa triviaqa wikihop described briey squad famous dataset kind stanford question ing dataset squad stanford question answering dataset squad consists questions posed crowdworkers set wikipedia articles answer question segment text span sponding reading passage squad contains question answer pairs articles larger previous manually labeled rc datasets quote example question answer pairs fig answer span document meteorology precipitation product condensation atmospheric water vapor falls gravity main forms precipitation include drizzle rain sleet snow graupel hail precipitation forms smaller droplets coalesce collision rain drops ice crystals cloud short intense periods rain scattered locations called showers causes precipitation fall gravity graupel cloud main form precipitation drizzle rain snow sleet hail water droplets collide ice crystals form precipitation fig question answer pairs sample passage squad qa com xin zhang et al answer type percentage example date numeric person location entity common noun phrase adjective phrase verb phrase clause october thomas coke germany abc sports property damage second largest returned earth avoid trivialization quietly table answer type distribution squad article endangered species act paragraph legislation followed including migratory bird conservation act treaty prohibiting hunting right gray whales bald eagle protection act later laws low cost society species relatively rare little opposition raised question laws faced signicant opposition plausible answer later laws question treaty plausible answer bald eagle protection act fig unanswerable question examples plausible incorrect answers squad answers belong dierent categories shown table common noun phrases data proper noun phrases data rest consists date numbers adjective phrase verb phrase clauses indicates answers squad displays reasonable diversity reasoning skills squad answer questions authors examples lexical syntactic divergence question answer passage manually annotating examples later squad released emphasis unanswerable questions new version squad adds unanswerable questions created adversarially crowdworkers according original ones order challenge existing models tend unreliable guesses questions answers stated context newly added questions highly similar corresponding context plausible incorrect answers context quote examples shown fig unanswerable questions squad posed humans exhibit diversity delity automatic constructed datasets cases simple heuristics based overlapping entity type recognition able distinguish answerable unanswerable questions cnn daily mail cnn daily mail dataset released google deepmind university oxford rst large scale reading consisting person location entities machine reading comprehension literature review original version context bbc producer allegedly struck jeremy clarkson press charges gear host lawyer said friday clarkson hosted watched television shows world dropped bbc day internal investigation british broadcaster found subjected producer oisin tymon unprovoked physical verbal attack query producer x press charges jeremy clarkson lawyer says anonymised version producer allegedly struck press charges host lawyer said friday hosted watched television shows world dropped wednesday ter internal investigation broadcaster found subjected ducer unprovoked physical verbal attack producer x press charges lawyer says answer oisin tymon table example data point quoted hension dataset constructed natural language materials unlike relevant work uses templates syntactic semantic rules extract document answer triples work collects articles articles daily source text article comes number bullet points summarize article work converts bullet points document query answer triples cloze style questions exclusively examine system s ability reading comprehension world knowledge co occurrence modications implemented triples construct anonymized version entity anonymized abstract entity marker easily predicted world knowledge n gram language model example data point anonymized version shown table basic statistics cnn daily mail shown table quote percentages right answers appearing n frequent entities given document table illustrating diculty degree questions extent cbt children s book test babi project facebook ai aims researching automatic text understanding reasoning dren books chosen ensure clear narrative structure aids task children stories cbt come books freely available project questions formed enumerating consecutive sentences chapters books rst sentences serve context query removing word candidates selected words appearing context query example question given fig dataset size shown table www cnn com www dailymail co fb com downloads gutenberg org xin zhang et al cnn daily mail n cumulative train valid test train valid test cnn daily mail months documents queries max entities avg entities avg tokens vocab size table corpus statistics cnn daily mail table percentage rect answers contained n frequent entities given document quoted fig cbt example quoted cbt distinct types word named entities common nouns verbs removed respectively form classes questions class questions wrong candidates selected randomly words type answer options corresponding context query compared human performance dataset state art models like recurrent neural networks rnns long short term memory lstm performed worse predicting nouns named entities great job predicting prepostions verbs probably explained fact models based exclusively local contexts contrast memory networks exploit wider context outperform conventional models predicting nouns named entities corpus encourages use world knowledge comparison cnn daily mail focuses paraphrasing parts context newsqa based news articles news newsqa dataset contains question answer pairs generated crowdworkers similar squad based output pos tagger named entity recognizer stanford core nlp toolkit www cnn com machine reading comprehension literature review training validation test number books number questions average words contexts average words queries distinct candidates vocabulary size table corpus statistics cbt answer type example proportion date time numeric person location entity common noun phr adjective phr verb phr clause phr prepositional phr march million ludwig van beethoven torrance california pew hispanic center federal prosecutors hour suered minor damage trampling human rights attack nearly half table answer types distribution newsqa answer question text span arbitrary length ing article null span included cnn articles chosen source materials authors view machine comprehension systems particularly suited high volume rapidly changing information sources like news major ferences cnn daily mail newsqa answers newsqa necessarily entities anonymization procedure considered generation newsqa statistics answer types newsqa shown table seen table variety answer types ensured furthermore authors sampled examples newsqa squad respectively analyzed possible reasoning skills answer questions results indicate compared squad larger proportion questions newsqa require high level reasoning skills including inference synthesis simple skills like word ing paraphrasing solve questions datasets newsqa tends require complex reasoning skills squad detailed comparison result given table triviaqa instead relying crowdworkers create question answer pairs selected passages like newsqa squad k triviaqa answer evidence triples generated automatic procedures firstly huge question answer pairs trivia quiz league websites ered ltered evidence documents question answer pair collected web search results wikipedia articles finally clean free human annotated subset triples triviaqa given triple example shown fig basic statistics triviaqa given table sampling examples dataset annotating manually turns wikipedia titles including person organization location miscellaneous consists reasoning example word matching paraphrasing inference synthesis q ndings published s sets research ndings lished thursday struggle q rwanda s struggle pits ethnic tutsis ported rwanda ethnic hutu backed congo q drew inspiration presidents s rudy ruiz says lives presidents positive role models dents q brittanee drexel s mother year old rochester new york high school student says daughter permission trip brittanee marie drexel s mom says house s barack obama s mother law marian robinson join obamas family s private quarters vania avenue michelle mentioned xin zhang et al proportion newsqa squad ambiguous insucient q mother moving white table reasoning skills newsqa squad corresponding proportions question dodecanese campaign wwii attempt allied forces capture islands aegean sea inspiration acclaimed commando lm answer guns navarone excerpt dodecanese campaign world war ii attempt allied forces capture italian held dodecanese islands aegean sea following der italy september use bases german controlled balkans failed campaign particular battle leros inspired novel guns navarone successful movie question american callan pinckney s eponymously named system selling book video franchise genre answer fitness excerpt callan pinckney american tness professional achieved precedented success callanetics exercises books tional best sellers video series followed went sell million copies pinckney s rst video release callanetics years younger hours outsold tness video fig example question answer evidence triples triviaqa quoted answer rest small percentage answers mainly belong numerical free text type average number entities question percentages certain types questions shown table wikihop wikihop released purpose evaluating system s ability multi hop reasoning multiple documents existing datasets information needed answer question usually contained machine reading comprehension literature review total number qa pairs number unique answers number evidence documents avg question length word avg document length word table corpus statistics triviaqa property example annotation statistics avg entities question fine grained answer type fragrant essential oil obtained damask questions coarse grained answer typewho won nobel peace prize questions time frame comparisons photographed rst time october questions appropriate largest type frog politician won nobel peace prize question questions table properties questions sampled examples boldfaced words mean presence corresponding properties wikihop medhop train dev test total table dataset sizes wikihop medhop sentence makes current mrc models pay attention simple reasoning skills like locating matching aligning information query support text example squad sentence highest lexical similarity question contains answer time simple binary word query indicator feature boosted relative accuracy baseline model authors dene novel mrc task model needs combine evidences dierent documents answer questions sample wikihop displays characteristics shown fig construct wikihop authors collect s r o triples subject entity s relation r object entity o wikidata wikipedia articles associated entities added candidate evidence documents d triple query removing answer q s r o reach goal multi hop reasoning bipartite graphs constructed help construction shown fig vertices sides respectively correspond entities documents knowledge base edges denote entities appear corresponding documents given q pair answer candidates cq support documents sq d identied traversing bipartite graph search documents visited support documents sq dataset medhop constructed way wikihop focus medicine area basic statistics wikihop medhop shown table table table lists proportions dierent types answer samples indicates perform wikihop system needs good multi step reasoning xin zhang et al hanging gardens known pherozeshah mehta gardens terraced gardens provide sunset views arabian sea known bombay ocial capital city indian state maharashtra populous city india arabian sea region northern indian ocean bounded north pakistan iran west northeastern somalia arabian peninsula east india question hanging gardens mumbai country options iran india pakistan somalia fig sample wikihop quoted displays necessity multi hop reasoning documents fig bipartite graph given paper connecting entities documents mentioning bold edges traversed rst fact small kb right yellow highlighting indicates documents sq candidates cq check cross indicate correct false candidates descriptive answer datasets instead text spans entities obtained candidate documents descriptive answers stand sentences exhibit uency integrity addition real world questions answered simply text span entity s presenting answers supporting evidence examples preferred human light reasons descriptive answer datasets released recent years mainly introduce detail ms marco narrativeqa ms marco ms marco microsoft machine reading comprehension large dataset released microsoft dataset aims address questions documents real world sourced real anonymized queries issued r r machine reading comprehension literature review cand wh docs wh tok wh cand mh docs mh tok mh min max avg median table corpus statistics wikihop medhop wh wikihop mh medhop unique multi step answer likely multi step unique answer multiple plausible answers ambiguity hypernymy single document required answer follow wikidata wikipedia discrepancy table qualitiative analysis sampled answers wikihop corresponding searching results bing search engine ms marco reproduce qa situations real world question dataset crowdworker asked answer form complete sentence passages provided bing unanswerable questions kept dataset purpose encouraging system judge question answerable scanty conicting materials rst version ms marco released questions latest version released questions available msmarco org dataset compositions ms marco shown table tribution dierent types questions shown table table contain interrogatives queries come real users interrogative contained queries description questions account major question type generally interrogative distribution questions shows reasonable diversity dataset descriptive answers released deepmind university oxford narrativeqa specically designed examine system capture underlying narrative elements answer questions answered simple pattern recognition global salience example question answer pair shown fig relatively high level abstraction reasoning required answer question stories narrativeqa consist books project movie scripts relative story plot summary nally provided crowdworkers create question answer pairs www bing com microsoft com en cortana gutenberg mainly imsdb dailyscript awesomelm xin zhang et al field description query question query issued bing passages passages web documents retrieved bing passages presented ranked order human editors passage editor uses compose answer annotated selected document urls urls ranked documents question bing passages extracted documents answers composed human editors question matically extracted passages corresponding ments formed formed answer rewritten human editors inal answer segment qa classication e tallest mountain south america longs entity segment answer entity aconcagua table ms marco dataset composition percentage question question segment question types yesno question classication description numeric entity location person table distribution dierent question types ms marco crowdworkers text s likely create questions answers solely based localized context answers sentences exhibit articial intelligence asked factual information basic statistics shown table distribution dierent types questions answers shown table table according original paper answers appear text segments stories decreases possibility answering questions simple skills system machine reading comprehension literature review title ghostbusters ii question oscar related dana answer son summary snippet peter s girlfriend dana barrett son oscar story snippet dana setting wheel brakes buggy thank frank ll hang eventually continues digging purse frank leans buggy makes funny faces baby oscar cute month old boy frank baby hiya oscar slugger s good looking kid got ms barrett frank dana fig example question answer pair narrativeqa given paper documents books movie scripts question answer pairs avg tok summaries max tok summaries avg tok stories max tok stories avg tok questions avg tok answers train valid test table narrativeqa dataset statistics token frequency category frequency person description location reason method event entity object numeric duration relation table frequency rst token question training set narrativeqa table question categories sample questions validation set xin zhang et al q e v r r n o c r m s m p o h k w q v r t q s w e n t b c n n c l m y l d d u q s v d u q s v v e v r r n s t p r c s w o r c e n g n e h c r e s l r u t n y r e u q e c r u o s m o t u r e w s n e v r r n e v t c r t e e v t c r t e e v t c r t e e v t c r t e e v t c r t e e v t c r t e e v t c r t e e p w v r t s o o b s w e n e p w e p w m o t u l r u t n m o t u m o t u w o r c w o r c e c r u o s e c r u o s s w e n w o r c e c r u o s e s e l e r e t e y t n m o d n o t s e u q e c r u o s m e f m e f n m u h e c n m r o r e m e f e n n c m e f m e f m e f t o s s t e s t e v r r n d n e v t c r t e l l o o n s b e l b t n t n o c e l b r e w s n n u n o t s e u q k s t n o r e n e g e g u g n l l r u t n q n o u e l b l e g u o r l e g u o r r o e t e m u e l b u e l b m o w m o b e w m o w m o b e w f m e e n n c b v r p machine reading comprehension literature review q e v r r n o c r m s m p o h k w q v r t q s w e n t b c l m y l d v v n n c d u q s d u q s e h g g w r t n e m u c o g g t n e m u c o d e b m u n e g r e v h t g n e l t n e m u c o d r e u q e b m u n g r e v h t g n e l r e u q c g xin zhang et al s t e s t e v r r n d n e v t c r t e l l o n o m o n s c t s t s e l b t j g r e v h t g n e l e w s n s r e p p l n g r o g n n o p s e r r o c m o r e m o c s c t s t s r e h t o e c e s s s e l n u s e v l e s r u o y b e t n u o c e r h t w s c t s t s e c e s s s e l n u h t g n e l g n l u c l c n e h w r e b m u n r o w e s u r e r m y t n e n s e w s n e h t n o s r e v e s m y n o n t s e t v e n r t s e l c t r e p w t s e t v e n r t s w e n s h t n o m e l b l v n u s t g n n o p s e r r o c t s e t v e n r t r e b m u n s e l c t r s o o b c s w e n t s e t v e n r t s h p r g r p g t s e t v e n r t s e r o t s s t n e m u c o d b e w l l u f m o r t l u s e r s e g s s p h e j t l u e d e h t machine reading comprehension literature review james turtle getting trouble d reach freezer food times sled deck splinter aunt jane tried hard trouble sneaky got lots trouble day james thought town kind trouble went grocery store pulled pudding o shelves ate jars walked fast food restaurant ordered bags fries nt pay instead headed home aunt waiting room told james loved start acting like behaved turtle month getting lots trouble james nally mind better turtle trouble making turtle fries b pudding c james d jane james pull o shelves grocery store pudding b fries c food d splinters james went grocery store deck b freezer c fast food restaurant d room james ordered fries went grocery store b went home paying c ate d mind better turtle fig sample mctest given paper multiple choice datasets descriptive answers relatively dicult evaluate system performance precisely objectively multiple choice question long testing students reading comprehension ability jectively gradable generally kind questions extensively examine s reasoning skills including simple pattern recognition clausal inference sentence reasoning given passage light datasets format released listed follows mctest mctest high quality dataset consisting stories tions ction stories released microsoft format race targeting year old children passages questions mctest easy understandable reduces world knowledge requisite mctest answers found story stories ctional main drawback mctest size small train performed model sample mctest shown fig race race contains passages questions collected english exams middle high school chinese students considering passages questions specically designed english teachers experts evaluate reading comprehension ability students dataset promising developing testing mrc systems questions created high quality human experts noises race s passages race cover wide range topics xin zhang et al release date type race cloth mctest mcscript arc coqa multiple choice multiple choice domain exam exam question source natural natural multiple choice fiction stories crowd multiple choice script scenarios crowd multiple choice multiple choice science widea natural crowd human performance b sota c d contain unanswerable question test common sense specically raw document document number average length document query number average length query average length answer g e k children s stories literature mid high school exams news wikipedia science reddit b race m race h c total middle high e domain domain f race m race h g scenarios h science related sentences stories j texts k passages table basic information statistics multiple choice datasets machine reading comprehension literature review t wanted plant tree went home garden store picked nice oak planted garden dig hole shovel b bare hands plant tree watering taking home fig example questions mcscript overcoming topic bias problem commonly exists datasets like news articles cnn daily mail wikipedia articles squad sample race shown table dataset rstly provides dents systems passage read presents questions didate answers words questions candidate answers appear passage simple context matching techniques aid datasets analysis paper shows reasoning skill indispensable answering questions race correctly race divided subsets race m race h middle school high school respectively basic statistics race given table table distributions dierent reasoning types required answer certain questions illustrated table denoting half questions race requires reasoning skill cloth cloth cloze test teachers constructed format cloze questions composed english tests chinese middle school high school example shown table cloth missing blanks questions carefully designed teachers test dierent aspects language knowledge candidate answers usually subtle dierences making tions dicult answer human similar race cloth divided parts cloth m middle school cloth h high school ones basic statistics corpus shown table experiments cloth authors came conclusion performance gap human system mainly results ability long term context multiple sentence reasoning mcscript mcscript focuses questions need reasoning sense knowledge released march new dataset provides stories describing people s daily activities ambiguity implicitness resolved easily commonsense crowdworkers generate questions correct answers questions appear given text shown examples fig consists k texts k questions according statistical analysis questions mcscript require commonsense knowledge answer dataset literally examine systems commonsense inference ability questions dataset answerable distribution questions types mcscript shown fig xin zhang et al passage small village england years ago mail coach standing street nt come village people pay lot letter person sent letter nt pay postage receiver s letter miss alice brown said mailman m alice brown girl said low voice alice looked envelope minute handed mailman m sorry nt nt money pay said gentleman standing sorry came paid postage gentleman gave letter said smile thank letter tom m going marry went london look work ve waited long time letter nt need know gentleman said surprise told signs envelope look sir cross corner means circle means found work s good news gentleman sir rowland hill nt forgot alice letter postage paid receiver changed said good plan postage lower penny person sends letter pays postage buy stamp envelope said government accepted plan rst stamp called penny black picture queen questions rst postage stamp england b america c alice d girl handed letter mailman nt know letter b money pay postage c received letter nt want open d known written letter know alice s words tom told signs meant leaving b alice clever guess meaning signs c alice signs lope d tom signs alice told table sample race quoted idea stamps thought government b sir rowland hill c alice brown d tom passage know high postage people send letters b lovers lose touch c people try best avoid paying d receivers refuse pay coming letters answer adabc machine reading comprehension literature review race m dataset subset passages questions race h train dev test train dev test train dev test race table basic statistics training development test sets race m race h race dataset passage len question len option len vocab size race m race h race table statistics race len denotes length vocab denotes vocabulary dataset word matching paraphrasing single sentence reasoning multi sentence reasoning ambiguous insucient race m race h race cnn squad newsqa table distribution reasoning type race datasets denotes quoting based samples dataset quoting fig distribution question types mcscript arc reasoning challenge makes use standardized tests questions objectively gradable exhibit variety diculty grand challenge ai arc consists k questions authors arc designe baselines retrieval based gorithm word co occurrence algorithm challenge set subset arc long y n xin zhang et al passage nancy got job secretary company monday rst day went work arrived early door open found arrive thought came desk surprised nd bunch fresh sweet looked somebody sent owers rst day thought began day passed quickly nancy interest following days rst thing nancy change water followers set work came monday came near desk overjoyed bunch owers quickly vase old ones thing happened monday nancy began think ways nd tuesday afternoon sent hand plan waited directives secretary s happened desk half opened notebook order secretaries high spirits company decided monday morning bunch fresh owers secretarys desk later told general manager business management psychologist questions b pushed b second b grapes d forced d rst d bananas d held d bottle depressed b encouraged c excited d surprised turned keys smelled b ate vase angrily seek low month old covering b demanding c replacing d forbidding sender b receiver assistant b colleague c employee d manager notebook b desk said c knocked c c owers c took c glass c strangely d happily c work c great c year c c blue b room b quietly b wonder b little b period b b red d ask d general d week d d new c secretary d waiter c oce c printed d house d signed b written table sample passage cloth bold faces highlight correct answers best answer candidates candidates correct dataset cloth m cloth h cloth train dev test train dev test train dev test passages questions vocab size avg sentence avg words table statistics training development test sets cloth subsets paper containing k questions created gathering questions answered incorrectly baselines easy set composed ing k questions state art models tested challenge set able signicantly outperform random baseline reects diculty challenge set example questions challenge set questions follows machine reading comprehension literature review train dev test total challenge easy total table number questions arc grade challenge qns easy qns qns table grade level distribution arc questions qns property question words question sentences answer option words answer options min average max challenge easy table properties arc dataset property mineral determined looking ter correct b mass c weight d hardness student riding bicycle observes moves faster smooth road rough road happens smooth road gravity b gravity c friction correct d friction example rst question dicult ground truth luster determined looking appears stand sentence web text incorrect candidate hardness strong correlation mineral text arc corpus scientic text contains m science related sentences mentions knowledge related challenge set questions according sample analysis released arc questions set use corpus optional statistics arc shown table table table coqa question answering systems conversational style datasets consists questions sourced conversations dierent domains answers questions free form motivation coqa daily life human usually information asking questions conversations desirable machine capable answering questions coqa xin zhang et al jessica went sit rocking chair today birthday turning granddaughter annie coming afternoon jessica excited daughter melanie melanie s husband josh coming jessica birthday jessica jessica went sit rocking chair today birthday turning old turning plan visitors yes granddaughter annie coming granddaughter annie coming afternoon jessica excited daughter melanie melanie s husband josh coming annie melanie josh granddaughter annie coming afternoon jessica excited daughter melanie melanie s husband josh coming fig conversation example coqa turn contains question qi answer ai rationale ri supports answer rstly provides models text passage understand presents series questions appear conversation example given fig key challenge coqa system handle conversation history properly tackle problems like resolving coreference domains passages questions collected cross domain uation domain evaluation distribution domains shown table linguistic phenomena statistics given table coreference pragmatics unique challenging linguistic phenomena appeare datasets section introduce dierent techniques employed mrc mrc techniques non neural method neural networks came fashion mrc systems oped based dierent non neural techniques serve baselines comparison introduce techniques including tf idf sliding window logistic regression boosted method machine reading comprehension literature review domain passages passage length turns passage q pairs domain children s sto literature mid high sch news wikipedia science reddit total lexical match paraphrasing pragmatics coref explicit coref implicit coref table distribution domains coqa phenomenon example percentage relationship question passage q rescue coast guard r outen rescued coast guard q wild dog approach yes r drew cautiously closer q joey male female male r looked like stick man kept named new noodle friend joey q ifl q bashti forgotten puppy q q sirisena sworn p m local time q relationship question conversation history table linguistic phenomena coqa questions given paper tf idf tf idf term frequency inverse document frequency technique widely information retrieval area nds place mrc tasks later validated candidate answers presented retrieval based models serve strong baseline kind baseline widely document datasets wikihop solely exploiting lexical correlation tween concatenation candidate answer query given document kind algorithm predict candidate highest similarity score documents inter document information usually ignored tf idf baseline detect question rely cross document reasoning xin zhang et al sliding window sliding window algorithm constructed baseline dataset mctest predicts answer based simple lexical information sliding window inspired tf idf algorithm uses inverse word count weight word maximize bag word similarity answer sliding window given passage logistic regression baseline method proposed squad extracts large mount features candidates including lengths bigram frequencies word frequencies span pos tags lexical features dependency tree path features predicts text span nal answer based information boosting method model proposed conventional feature based baseline cnn daily mail dataset task seen ranking problem making score predicted answer rank candidates authors turn implementation lambdamart ranklib highly successful ranking algorithm forests boosting decision trees feature engineering features chosen form feature vector represents candidate weight vector learnt correct answer ranked highest neural based method popularity neural networks end end models produced ing results mrc tasks models need design complex manually devised features traditional approaches relied perform better introduce end end models mainly chronological order match network rst end end neural architecture posed squad model combines match lstm query aware representation passage pointer network aims construct answer token comes input text overall picture model architecture given fig match lstm originally designed predicting textual entailment task premise hypothesis given match lstm encodes pothesis premise aware way token hypothesis model uses soft attention mechanism discussed later sect weighted vector representation premise weighted vector concatenated vector representation according token fed lstm match lstm paper authors replace premise hypothesis query passage query aware representation given passage preprocessing lstms employed respectively encode query passage bidirectional match lstm employed obtain query aware representation passage net p lemur wiki details found paper machine reading comprehension literature review fig overview models getting query aware representation passage pointer net employed generate answers selecting tokens input passage inference step ptr net uses soft attention mechanism probability tribution input sequence selects token largest possibility output symbol dierent strategies proposed constructing answer sequence model assumes word answer appear position passage length answer xed order tell model stop generating tokens getting answer special symbol placed end passage prediction symbol indicates termination answer generating boundary model works dierently sequence model predicts start indice end indice ae word s based assumption answer appears continuous segment passage test result shows advantage boundary model bi directional attention flow proposed bi directional attention flow key features context encoding stage model takes dierent levels granularity input including character level word level contextualized embeddings second uses bi directional attention ow passage query attention query passage attention query aware passage tation detailed description given follows shown fig bidaf model layers character ding layer word embedding layer map word vector space based respectively character level cnns pre trained glove embedding concatenation word embeddings passed layer highway networks output provided bi directional lstm contextual embedding layer rene word embedding xin zhang et al fig overview bidaf architecture given context information rst layers applied query passage attention flow layer information query passage mixed interacted instead summarizing passage query xed vector like attention mechanisms layer grants raw information including attention vectors embeddings previous layers owing subsequent layer reduces information loss attentions computed directions passage query query passage detailed information attention flow layer given sect modeling layer takes query aware representation context words bi directional lstm capture interactions passage words according query output layer task specic gives prediction answer gated attention gated attention reader targets realizing multi hop ing answering cloze style questions documents multiplicative interaction query hidden state document employed tion mechanism multi hop architecture model imitates multi step reasoning human reading comprehension overview model given fig model reads document query iteratively row k layers kth layer model uses bidirectional gated recurrent gru transform x dings document passed layer layer specic query representation transformed bi gru gru k d gru k q y modeling layeroutput layerattention flow layercontextual embed layerword embed softmaxdense cnncharacter embed machine reading comprehension literature review fig gated attention architecture given fed gated attention module result x k passed layer token gated attention module uses soft attention token specied representation query finally new embeddings token xi applying element wise multiplication qi softmax qi qi qi stage decoder employs softmax layer inner product outputs layer possibility distribution predict answers xq n dcn dynamic coattention introduces coattention mechanism combine co dependent representations query document dynamic iteration avoid trapped local maxima corresponding incorrect answers like previous single pass models dynamic pointer decoder takes output coattention encoder generates nal predictions detailed procedures given follows xq xq m let document question encoder vector representations ment query fed lstm respectively hidden states step combined form encoding matrix d dmd qnq sentinel vector d q appended encoding matrix enable model map unrelated words exclusively appear query document void vector allow variation document encoding space query encoding space document details dcn follows denote sequence embeddings words query xin zhang et al non linear projection q tanh applied nal representations document query d q coattention encoder takes d q outputs coattention encoding matrix u um input dynamic pointing decoder details coattention encoder discussed sec overview dynamic pointing decoder given fig enable model recover local maxima highway maxout network hmn posed predict start point end point iteratively hmn composed highway networks characterized skip connect passes gradient eectively deep networks maxout networks learnable activation function strong empirical performance iteration hidden state decoder updated according eq hi lst m dec coattention representations according start end words predicted iteration given hi possibility tth word start end point calculated eq t hmn hi word maximum possibility selected prediction current step architecture hmn given fig mathematical description hmn given follows hmn hi w h t t max r tanh d t max ut t w t max r non linear projection current state fastqa fastqa achieved competitive performance simple architecture questions necessity improving complexity qa systems unlike systems employed complex interaction layer catch interaction query context fastqa makes use computable features word levels overview fastqa architecture given fig binary word feature indicates token passages appears corresponding query wiqb j weighted dened takes term frequency similarity query context account simi j vwiq xj qi x wiqw j softmax vwiq rn machine reading comprehension literature review fig architecture dynamic decoder paper blue denotes variables functions related estimating start position red denotes variables tions related estimating end position fig architecture highway maxout network given concatenation features original representation words fed bi lstm nal hidden state answer layer composed simple layer feed forward network beam search r net r net proposed msra achieved state art results squad ms marco overview architecture shown fig given word level character level embeddings r net rstly employs bi directional gru encode questions passages uses gated attention based recurrent network fuse information question hmnargmax u xin zhang et al fig overview fastqa architecture passage later self matching layer ne tune nal tation passage output layer based pointer networks similar match lstm predict boundary answer initial hidden vectors pointer network computed attention pooling nal passage representations gated attention based recurrent network adds gate normal based recurrent networks gate gives weight certain passage information according question inspired sentence pair representations obtained follows t t ct t ct vp t rnn vp gt t ct j vt tanh st exp ct m u uq u j w p w q exp j t w p v vp gt sigmoid original representations passage question added gate t ct t om n uq t exploit information passage token self matching attention applied nal representation passage hp details self matching attention given sec output layer uses pointer networks predict start end position answer initial hidden vector pointer network pooling question representation hp objective function sum negative log probabilities ground truth start end position predicted distributions reasonet unlike previous models xed number turns reading reasoning regardless complexity queries passages reasonet makes use reinforcement learning dynamically determine reading x machine reading comprehension literature review fig overview r net architecture paper reasoning depth intuition work comes diculty dierent questions vary lot dataset fact human usually revisit important passage question answer question better overview reasonet structure given fig external memory m usually word embeddings encoded bi rnn internal state s updated according rnn st xt s xt attention vector fatt st m x termination gate determines stop updating states predict answers according binary variable tt tt st tg way reasonet mimic inference process human exploit passages answer questions better qanet models primarily based rnns attention slow training inference sequential nature rnns machine comprehension fast qanet proposed rnns architecture overview qanet structure given fig key dierence qanet previous models qanet use convolutional self attention mechanism embedding modeling encoders discarding commonly rnns depthwise separable tions capture local structure text multi head attention mechanism model global interactions passages query context attention similar dcn applied qanet achieved state art accuracy achieving speedup training training iteration compared rnn counterparts xin zhang et al fig overview reasonet structure attention attention mechanisms shown great power selecting important mation aligning capturing similarity dierent input introduce representative attention mechanism primarily based time order hard attention proposed image caption task stochastic hard attention let al ai rd denote feature vectors captured cnn corresponding image deciding features feed decoder lstm generate caption hot variable st dened indicator st set th vector extract visual features current step t denote input decoder lstm zt paper assigns multinoulli distribution parametrized t view zt random variable x zt st iai p st t t eti fatt ai ti pl truememory m machine reading comprehension literature review fig overview qanet architecture left encoder blocks coder blocks number convolutional layers varies fatt multilayer perceptron dening objective function ls ls log log x s x s log approximate gradient monte carlo method nal learning rule model ls w n n x log w b log p w e h w r e hyperparameters set crossvalidation hard attention tricky troublesome training trained perform better soft attention sharp focus memory provided xin zhang et al soft attention rst introduce basic form soft attention neural machine translation task talk variants tasks like natural language mrc dierent hard attention soft attention calculates weight distribution input representations use weighted sum input decoder example let htx denote encoder s output sequence ij denote weight indicates extent hj related current output token ti input decoder ci ci ijhj txx weights calculated learn feedforward neural network ij exp eij exp eik ptx eij hj nli task input components premise hypothesis attention exploit interaction relation parts match lstm example denote hs k resulting hidden states encoder lstm separately premise hypothesis predicting label hypothesis attention weighted combinations hidden states premise computed match lstm j ht ak pm kjhs j kj p ekj tanh j wtht k wmhm ak attention vector stated ws wt wm parameters learned hm hidden state match lstm position k finally ak concatenated ht k predicting result mrc task regard question premise passage hypothesis likes model match network applying attention mechanism additional query information token passage improve model performance compared hard attention soft attention s advantage dierentiabile easy train fast training inference bi directional attention proposed bidaf compared attention scribed considers attention directions query attention context attention bidaf example given h u concatenation outputs lstms contextual embedding layer similarity matrix s computed stj h u j u w u h u machine reading comprehension literature review w compute attention weights attended query vectors s trainable parameters elementwise multiplication similarily attention weights attended context vectors softmax st u p j atju j softmax h p t t finally attention vectors combined original tual embeddings h vector fusing function result serve base future modeling prediction bi directional attention adds information attention compared normal attention mechanism shown ablation study attention direction useful standard squad dev set reason query usually short added information relatively small coattention proposed architecture coattention encoder dcn shown fig coattention encoder anity matrix l d q calculated normalized row wise column wise obtain aq attention weights matrix document word query ad attention weights matrix query word document attention contexts question computed c q daq concatenated q obtain nal document representation c d c ad step d c d fed bidirectional lstm ut bi lstm cd t result serves foundation predicting answer hidden states form coattention encoding matrix u um similarly bi directional attention coattention mechanism utilizes tion information directions dierent way successively computes attention contexts question document fuses co dependent representation document self matching attention proposed r net introduced useful information exist passage context captured traditional mainly exploits information words surrounding window self matching attention mechanism proposed address problem collects evidence token vt passage according question information result hp nal passage representation hp t birnn t ct gt hp t ct t ct xin zhang et al fig architecture co attention encoder ct refers attention pooling vector passage st j vt tanh exp st ct n j w p v vp w p v vp t exp ivp j gt gate dene sec uniquely self matching attention captures long distance information passage helps r net dealing problems like coreference pre trained word representations eciently represent words vectors serve base modern mrc systems problem concerns researchers previously hot representation n gram model popular simple techniques met limits tasks address problem technologies proposed according time occurrence introduce follows moving feedforward neural net language recurrent neural net language paper proposed novel models learn distributed representations words tinuous bag words continuous skip gram model architectures models given fig cbow model uses history words future words input maximizes probability correctly predicting current word contrast skip gram model uses current word input tries predict words certain range current word result word vectors models achieved state art performance tests aqaddocumentproductconcatproductbi lstmbi lstmbi lstmbi lstmbi q cqcdutu machine reading comprehension literature review fig architectures cbow model skip gram model probability ratio k solid k gas k water fashion fig ratio greater means word k correlate ice ratio greater means word k correlate stream glove method belongs local context window methods ods capture ne grained semantic syntactic regularities words eciently exploit global statistical information like latent semantic belongs global matrix factorization methods glove combines advantages family methods glove takes co occurrence probabilities words consideration use ratio probabilities reect relations dierent words example denote probability word j appear context word j pij ratio pik pjk tell correlation certain words example given fig glove model f takes form according phenomenon f wi wj wk pik pjk w rd word vectors f varies according dierent constrains elmo disadvantages word vectors generated methods static independent application linguistic contexts lead input projection input projection cbow skip gram xin zhang et al poor performance comes polysemy light elmo proposed addresses problem elmo s model employs bi lstm character convolutions input jointly maximizes log likelihood forward backward directions record internal states p tn p p tn p tn n y n y log x lst m s n x log tn x lst m s finally task specic linear combination internal states obtain elmo representation way elmo capture context dependent aspects word meaning syntax information token ne tuned domain specic data model usually performs better gpt compared elmo gpt uses variant transformer instead lstm better capture long term linguistic structure overview work given fig given corpus u un standard language model multi layer transformer decoder log p x u wp hl transformer block n p u softmax t e k context window size u uk context vectors tokens n number layers token embedding matrix wp position embedding matrix parameters trained stochastic gradient descent nal transformer blocks activation denoted hm l supervised ne tuning applied dierent stream tasks tasks like text classication linear output layer parameters wy needed predict p xm softmax hm wy recently successor released scale gpt larger volume billion parameters claimed achieve state art performance language modeling code released time paper written machine reading comprehension literature review fig graph comes paper left transformer architecture training tives work right input transformations ne tuning dierent tasks structured inputs converted token sequences processed gpt followed layer fig model architectures bert gpt elmo quoted bert shown fig elmo gpt models use unidirectional language models learn representation tokens bert points restriction severely limited eciency pre trained representation address problem new prediction tasks proposed pre train bert direction masked language model sentence prediction inspired cloze task masked language model predict randomly masked tokens d based context input words left right context taken consideration computing representations capture sentence level information relationship rized sentence prediction task predict sentence sentence b wordpiece embeddings input layer ment embeddings position embeddings input embeddings sum embeddings shown fig main architecture bert s model multi layer bidirectional transformer encoder identical inal similar gpt ne tuned steam tasks additional output layer minimal number parameters needed shown fig bert advanced state art results nlp tasks comparison size bert gpt given table bert openai gptlstmelmolstmlstmlstmlstmlstmlstmlstmlstmlstmlstmlstm tn en en tn en xin zhang et al fig bert input representation fig task specic models overview paper sep tok ntok tnsingle sentence berttok tok tok n tnsingle sentence b peroo label tmstart end spanclass labelberttok tok tok n sep tok ntok tokmsentence sentence machine reading comprehension literature review model gp t bertbase bertlarge gp t parameters layers hidden size m m m m table hyperparameter comparison similar models layers means number transformer blocks conclusion paper summarized advances mrc eld recent years briey introduced history mrc tasks early mrc systems section introduced recent datasets categories e squad cnn daily mail cbt newsqa triviaqa cloth extractive format ms marco narrative qa narrative format wikihop mctest race mcscript arc multiple choice format coqa novel dataset focuses conversational questions included section rst non neural methods including sliding window logistic regression tf idf boosted method importantly neural based models like dcn ga bidaf fastqa rnet reasonet qanet discussed compared important positions models pre training technology attention anism detail covered glove elmo bert section hard attention soft attention bi directional attention coattention self attention mechanisms section reviewed major progress recent years mrc eld mrc direction developing fast dicult include newly proposed mrc work survey hope review ease reference recent mrc advences encourage researchers work mrc eld references bahdanau d cho k bengio y neural machine translation jointly learning align translate corr bengio y ducharme r vincent p jauvin c neural probabilistic language model journal machine learning research bobrow d g kaplan r m kay m norman d thompson h winograd t gus frame driven dialog system articial intelligence chen d bolton j manning c d thorough examination cnn daily mail ing comprehension task arxiv preprint cho k van merrienboer b gulcehre c bahdanau d bougares f schwenk h bengio y learning phrase representations rnn encoder decoder statistical chine translation arxiv preprint chollet f xception deep learning depthwise separable convolutions arxiv preprint pp clark p cowhey etzioni o khot t sabharwal schoenick c tafjord o think solved question answering try arc reasoning challenge arxiv preprint xin zhang et al clark p cowhey etzioni o khot t sabharwal schoenick c tafjord o think solved question answering try arc reasoning challenge arxiv preprint clark p etzioni o computer honor studentbut intelligent standardized tests measure ai ai magazine clark p etzioni o khot t sabharwal tafjord o turney p d khashabi d combining retrieval statistics inference answer elementary science questions aaai pp deerwester s dumais s t furnas g w landauer t k harshman r indexing latent semantic analysis journal american society information science devlin j chang m w lee k toutanova k bert pre training deep bidirectional transformers language understanding arxiv preprint dhingra b liu h yang z cohen w w salakhutdinov r gated attention readers text comprehension arxiv preprint goodfellow j warde farley d mirza m courville bengio y maxout networks arxiv preprint green jr b f wolf k chomsky c laughery k baseball automatic papers presented western joint ire aiee acm answerer computer conference pp acm hermann k m kocisky t grefenstette e espeholt l kay w suleyman m som p teaching machines read comprehend advances neural information processing systems pp hill f bordes chopra s weston j goldilocks principle reading children s books explicit memory representations arxiv preprint hirschman l gaizauskas r natural language question answering view natural language engineering hochreiter s schmidhuber j long short term memory neural computation jia r liang p adversarial examples evaluating reading comprehension systems arxiv preprint joshi m choi e weld d s zettlemoyer l triviaqa large scale distantly pervised challenge dataset reading comprehension arxiv preprint kaiser l gomez n chollet f depthwise separable convolutions neural machine translation arxiv preprint kim y convolutional neural networks sentence classication arxiv preprint t schwarz j blunsom p dyer c hermann k m melis g grefenstette e narrativeqa reading comprehension challenge transactions association computational linguistics lai g xie q liu h yang y hovy e race large scale reading comprehension dataset examinations arxiv preprint lehnert w g conceptual theory question answering proceedings international joint conference articial intelligence volume pp morgan kaufmann publishers inc levy o seo m choi e zettlemoyer l zero shot relation extraction reading comprehension arxiv preprint liu p j saleh m pot e goodrich b sepassi r kaiser l shazeer n generating wikipedia summarizing long sequences arxiv preprint manning c surdeanu m bauer j finkel j bethard s mcclosky d stanford corenlp natural language processing toolkit proceedings annual meeting association computational linguistics system demonstrations pp merity s xiong c bradbury j socher r pointer sentinel mixture models arxiv preprint mikolov t chen k corrado g dean j ecient estimation word representations vector space arxiv preprint nguyen t rosenberg m song x gao j tiwary s majumder r deng l ms marco human generated machine reading comprehension dataset arxiv preprint machine reading comprehension literature review ostermann s modi roth m thater s pinkal m mcscript novel dataset assessing machine comprehension script knowledge arxiv preprint pennington j socher r manning c glove global vectors word representation proceedings conference empirical methods natural language processing emnlp pp peters m e neumann m iyyer m gardner m clark c lee k zettlemoyer l deep contextualized word representations arxiv preprint radford narasimhan k salimans t sutskever improving language ing unsupervised learning tech rep technical report openai rajpurkar p jia r liang p know nt know unanswerable questions squad arxiv preprint rajpurkar p zhang j lopyrev k liang p squad questions machine comprehension text arxiv preprint reddy s chen d manning c d coqa conversational question answering challenge arxiv preprint richardson m burges c j renshaw e mctest challenge dataset domain machine comprehension text proceedings conference pirical methods natural language processing pp richardson m burges c j renshaw e mctest challenge dataset domain machine comprehension text proceedings conference pirical methods natural language processing pp robbins h monro s stochastic approximation method herbert robbins selected papers pp springer rocktaschel t grefenstette e hermann k m t blunsom p reasoning entailment neural attention arxiv preprint seo m kembhavi farhadi hajishirzi h bidirectional attention ow machine comprehension arxiv preprint shankar s garg s sarawagi s surprisingly easy hard attention sequence shankar s sarawagi s label organized memory augmented neural network corr quence learning emnlp shen y huang p s gao j chen w reasonet learning stop reading machine comprehension proceedings acm sigkdd international conference knowledge discovery data mining pp acm simmons r f answering english questions computer survey tech rep system development corp santa monica calif srivastava r k gre k schmidhuber j highway networks arxiv preprint bulletin taylor w l new tool measuring readability journalism trischler wang t yuan x harris j sordoni bachman p suleman k newsqa machine comprehension dataset arxiv preprint vaswani shazeer n parmar n uszkoreit j jones l gomez n kaiser l polosukhin attention need advances neural information processing systems pp vinyals o fortunato m jaitly n pointer networks arxiv e prints vinyals o fortunato m jaitly n pointer networks advances neural tion processing systems pp vrandecic d wikidata new platform collaborative data collection proceedings international conference world wide web pp acm wadhwa s embar v grabmair m nyberg e inference oriented reading comprehension parallelqa arxiv preprint wang s jiang j learning natural language inference lstm arxiv preprint wang s jiang j machine comprehension match lstm answer pointer arxiv preprint wang w yang n wei f chang b zhou m gated self matching networks reading comprehension question answering proceedings annual ing association computational linguistics volume long papers vol pp xin zhang et al weissenborn d wiese g seie l fastqa simple ecient neural architecture question answering corr weissenborn d wiese g seie l making neural qa simple possible simpler arxiv preprint weissenborn d wiese g seie l making neural qa simple possible simpler arxiv preprint welbl j stenetorp p riedel s constructing datasets multi hop reading hension documents transactions association computational linguistics weston j chopra s bordes memory networks corr winograd t understanding natural language cognitive psychology woods w progress natural language understanding application lunar geology proceedings june national computer conference exposition pp acm wu q burges c j svore k m gao j adapting boosting information retrieval measures information retrieval wu y schuster m chen z le q v norouzi m macherey w krikun m cao y gao q macherey k al google s neural machine translation system bridging gap human machine translation arxiv preprint xie q lai g dai z hovy e large scale cloze test dataset designed teachers xiong c zhong v socher r dynamic coattention networks question answering arxiv preprint arxiv preprint xu k ba j kiros r cho k courville salakhudinov r zemel r bengio y attend tell neural image caption generation visual attention international conference machine learning pp yih w t chang m w meek c pastusiak question answering enhanced lexical semantic models proceedings annual meeting association computational linguistics volume long papers vol pp yu w dohan d luong m t zhao r chen k norouzi m le q v qanet combining local convolution global self attention reading comprehension arxiv preprint
