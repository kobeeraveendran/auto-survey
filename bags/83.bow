distraction based neural networks document summarization qian chen xiaodan zhu zhenhua ling wei hui jiang university science technology china hefei china national research council canada ottawa canada iflytek research hefei china york university toronto canada ustc edu com edu com yorku abstract distributed representation learned neural works recently shown effective eling natural languages granularities words phrases sentences approach extended help model larger spans text documents triguing investigation sirable paper aims enhance neural network models purpose typical problem document level modeling automatic tion aims model documents order generate summaries paper propose ral models train computers pay tion specic regions content input ments attention models distract traverse different content document better grasp overall meaning marization engineering features train models large datasets models achieve state art performance signicantly benet distraction modeling particularly input documents long introduction modeling meaning text lies center natural guage understanding distributed representation learned neural networks recently shown effective eling granularities text including words collobert mikolov chen phrases yin schutze zhu guably sentences socher irsoy cardie kalchbrenner tai zhu zhu chen zhu approach extended help model larger spans text documents intriguing investigation desirable interesting research conducted recently line wang cho published international joint ence articial intelligence hermann typical problem level modeling automatic summarization mani das martins nenkova mckeown computers generate summaries documents based shallow deep understanding documents regards process representing input documents generating summaries interaction complicated function tting function expect large scale annotated dataset estimate large set parameters hand coding summarization knowledge different forms iting number model parameters tractive summarization models adopted training data work explores direction utilizes relatively large datasets hermann train neural summarization models general neural networks universal approximators complicated functions shown fective problems recently understanding input documents generating maries challenging understanding recent work suggested distributed representations vectors adequate representing sentences let longer documents additional modeling soft hard tention applied retrospect subsequences words input text remedy limits shown improve performances different tasks discussed bahdanau luong rush regard mechanism provides connection input ment modeling encoding summary generating ing model level cognitive controls human summarizers input ments summaries summarize document consider control layer important paper focus better designing control layer marization propose neural models train computers pay attention specic regions content input documents attention models distract traverse different content document ter grasp overall meaning summarization engineering features train models large datasets models achieve state art performance signicantly benet tion modeling particularly input documents long explore technologies applied sentence level tasks extend document marization present paper technologies showed help improve summarization performance applied models leveraged technologies distraction models prove performance signicantly general models aim perform abstractive summarization related work distributed representation distributed representation shown effective modeling granularities text discussed recent work attempted model longer spans text neural networks lin wang cho hermann includes research corporates document level information language ing wang cho lin answers questions hermann comprehending input documents attention based models relevant learned distributed work resentation short documents averaged length word tokens objective summarization summarization typically faces documents longer summarization sary documents long paper propose neural models summarizing typical news articles thousands word tokens necessary enable computers pay attention specic content documents attention models distract traverse different content better grasp overall meaning summarization particularly ments long neural summarization models automatic summarization mani intensively studied text das martins nenkova mckeown speech zhu penn zhu art summarization models focused extractive summarization efforts exerted abstractive summarization recent neural tion models include recent efforts rush lopyrev research performed rush focuses neural models sentence compression rewriting document tion work lopyrev leverages neural networks generate news headline input documents ited word tokens work deals short texts dozens word tokens summarization problems content redundancy prominent attention based models sufcient summarization typically faces documents longer summarization needed uments long work attempt explore ral summarization technologies news articles thousands word tokens distraction based summarization models help improve performance note improvement achieved model outperformed attention based model reported short documents approach overview base model general encoder decoder work sutskever sutskever cho shown effective recently ferent tasks general sequence sequence ing framework encoding devoted model input documents decoder generate believe control layer helps navigate input documents optimize generation objectives importance focus control layer paper enrich expressiveness specically rization unlike recent work focuses tion order grasp local context correspondence machine translation sentence compression force models traverse different content document avoid focusing region content better grasp overall meaning summarization objective explore popular technologies applied sentence level tasks extend ment summarization present help improve summarization performance gru based encoding decoding encoding general document modeling ing framework takes input document xtx write summary document output yty summarization process modeled ing output text maximizes conditional bility arg maxy given gold summary sequences discussed model found fective modeling sentences sub sentential text spans address challenges faced document level encoding restrict encoders architectures recurrent neural network rnn recent ture shows long short term memory lstm hochreiter schmidhuber sutskever gated rent units gru bahdanau good chitectures developing systems empirically found gru achieved similar performance lstm fast train describe gru implement neural summarization models simplest uni directional setting reading input symbols left right gru learns hidden tions time encodes content seen far time computed dimensional embedding current word forward propagation gru computed follows rnm rnn weight matrices number hidden units element wise multiplication work actually applied directional grus grus found achieving better results single directional grus consistently suggests gru unit annotation vector encodes sequence directions modeling left right context figure shows encoder intuitively details readers refer bahdanau discussion figure high level view summarization model generation generating summaries decoder dicts word given annotations obtained coding htx previously predicted words objective probability summary decomposition ordered als argmax argmax equation depicts high level abstraction erating summary word previous output input annotations htx legal output word time optimal summary found model conditional probability rewritten tion factorize model according structures neural networks function nonlinear function computes probability vector legal words output time takes ement resulting vector corresponding word predicated probability word vector control layers connect input discuss details tion completeness function computed uost coct softmax function rkn rnn rnm weight matrices vocabulary size dimensional ding previously predicted word control layers document modeling summary generation scribed components input document ing summary generation core problem components associated sentence level modeling machine translation speech recognition attention model applied grasp local context dence input output texts example lation attention shown useful aligning words target language language translated corresponding source words context attention regarded type cognitive controls modeling documents general viewpoint control layer propose distraction modeling enable model traverse different content long document improves summarization performance signicantly general control layer allows cated examination input section describe controls consider attention distraction navigate input documents generate summaries layer hidden output rst extended recent level hidden output model luong marization models presented later experiments level hidden output model consistently improves summarization performance different datasets specically updating follows layer gru chitecture shown figure forward propagation computed similar equation use untied parameter matrices layer model allows ing direct interaction encoding current previous output information encoding current input content primed distraction attention discuss vectors computed distraction training propose enforce distraction perspectives adding distraction constraints training decoding rst discuss tion training distraction input content vectors training force model pay attention content input documents accumulate previously viewed content vector history content vector incorporate currently computed refer model diagonal ces input content vectors rectly penalized history directly computed conventional equation follows found achieving better performance natives distances held data min max max distraction score added output probability beam search order encourage model avoid redundant content scoret ihi annotation vectors encode current input word context input gru described equation attention weight current output time distraction based computed equation incorporated equation distraction attention weight vectors propose add distraction directly attention weight vectors ilarly accumulate past attention weights history attention weight use prime current attention weights model uses history attention weights use history force distraction order avoid redundancy concern machine translation task refer model scoret follows beam search distraction algorithm parameter determined development set refer model algorithm beam search distraction require vocabulary size beam size max output length computed probabilities words vocabulary choose likely words initialize potheses hypothesis compute conditional probabilities candidates sponding probabilities use distraction primed value score choose uahi likely candidates end rln weight matrices number hidden units note uahi equation computes ventional attention penalizing attention history normalized softmax generate attention weights turn equation distraction decoding decoding process enforced different types distraction computing difference distribution current tion weight previous attention weights seen proper probabilistic distribution normalized equation leibler divergence measure difference equation found consistently better distance metrics tried held data enforced distraction similar way attention primed input content vector hidden output vector normalized regular content vectors cosine similarity unknown word replacement summarization rowed unknown word replacement jean machine translation summarization models found improved performance summarizing long documents specically time complexity dling larger vocabulary softmax layer summary generation infrequent words removed lary replaced symbol old vocabulary size data dependent detailed later experiment set section rst round summary generated document token labeled replaced word input documents specically obtained ment equation largest element source location current experiment set data experiment summarization models licly available corpora different document lengths different languages cnn news collection hermann chinese corpus available cently large datasets appropriate training neural models discussed ploy large number parameters potentially plicated summarization process involving representing input documents generating summaries interacting hermann cnn data cnn data human generated real life summary news cle dataset collected available github data preprocessed stanford corenlp tools manning tokenization boundary detection capital information kept speed training removed documents long word tokens training validation set kept documents test set change difculty task lcsts data second corpus lcsts nese corpus available recently data constructed chinese microblogging site sina weibo original training testing split mentioned additionally randomly sampled small training data validation set table gives details datasets table averaged document length cnn corpus seven time long lcsts corpus summary times longer lcsts cnn train valid test doc sum doc train valid test table cnn lcsts dataset rst rows table averaged document length doc summary length sum terms numbers word kens row lists number documents datasets training details mini batch stochastic gradient descent sgd timize log likelihood adadelta zeiler matically adapt learning rate parameters cnn dataset training performed shufed mini batches size sorting length limit vocabulary include frequent words words replaced token cussed earlier paper based validation data set embedding dimension vector length den layers uni gru gru end sentence token inserted sentence end document token added end beam size decoder set lcsts data larger mini batch size found better based observation validation set com deepmind data characters words tokens vocabulary size ding dimension vector size hidden layer nodes beam search size cnn dataset code publicly tion uses python based theano library bergstra experimental results results cnn dataset overall performance results cnn dataset presented table rouge scores lin measure performance summary lengths preset report rouge upper table includes baseline results number ical summarization algorithms listed ble luhn luhn edmundson edmundson lsa steinberger jezek lex rank erkan radev text rank mihalcea tarau basic vanderwende sum haghighi vanderwende baseline results implemented open source tool results lower half table gru encoder achieves better performance gru encoder consistent results sts dataset reported later table level output model discussed method section cial consistent results sts dataset addition unknown replacement technique yields additional improvement strong model technologies row marked unk replace model row incorporates distraction modeling nally achieves score score rouge score signicantly improving rouge scores respectively largest improvement presented table pared techniques listed table lists details model improve formance additively neural models neer features use content additional formality features locations input sentences bring additional improvement performance different lengths documents observe effectiveness distraction model different ument lengths selected short documents cnn training dataset subset age length word tokens subset data number documents averaged document length word tokens shown table data distraction model improves results signicantly relative improvement compared respectively general best formance dataset lower training code available com nats python org pypi sumy system luhn edmundson lsa lex rank text rank sum basic sum uni gru gru level unk replace distraction distraction distraction rouge table results cnn dataset data suggesting training data improve marization performance rouge distraction distraction relative impr distraction distraction relative impr table results subsets cnn datasets different document lengths results lcsts dataset experiment proposed model public lcsts corpus baseline best result reported modied uni gru achieves slight ment reported results gru attention based model achieves better performance conrming ness directional models summarization implementation state art serves strong baseline cnn dataset discussed note input text length lcsts far shorter cnn documents containing words roughly sentences distraction improve performance contrast documents longer benets signicant achieving biggest improvement discussed earlier suggests ness distraction modeling helping summarize thank authors generously sharing latest output models achieves better mance results reported reported updated scores higher performance baseline challenging longer documents summarization necessary short texts system uni gru gru level att unk replace distraction rouge table results lcsts dataset compare models simple baseline selects rst numbers word tokens input documents reaches maximal rouge scores rst tokens taken achieves rouge models signicantly better cnn data set ing rst sentences achieves best results reach rouge respectively cnn data news data baseline selecting rst sentences known strong baseline models explore forming abstractive summarization conclusions future work propose train neural document summarization models pay attention specic regions input documents attention models distract models different content order better grasp overall meaning input documents engineering features train models large datasets models achieve art performance signicantly benet distraction modeling particularly input documents long explore recent technologies summarization help improve tion performance applied models leveraged technologies distraction models improve performance signicantly general viewpoint enriching ness control layers link input encoding layer output decoding layer importance edy shortcomings current models plan form work direction acknowledgments rst author paper supported science technology development hui province china grants mental research funds central universities grant strategic priority research program chinese academy sciences grant references bahdanau dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate corr bergstra bergstra breuleux bastien blin pascanu desjardins turian warde farley bengio theano cpu gpu math expression compiler scipy volume page austin chen zhigang chen wei lin qian chen wei hui jiang xiaodan zhu revisiting word embedding ing meaning proceedings acl chen qian chen xiaodan zhu zhenhua ling wei hui jiang enhancing combining sequential tree lstm natural language inference cho cho van merrienboer gulcehre bahdanau bougares schwenk bengio learning phrase representations rnn encoder decoder statistical machine translation emnlp collobert collobert weston bottou karlen kavukcuoglu kuksa natural language processing scratch jmlr das martins dipanjan das andre martins vey automatic text summarization edmundson harold edmundson new methods matic extracting jacm erkan radev gunes erkan dragomir radev lexrank graph based lexical centrality salience text marization jair pages haghighi vanderwende aria haghighi lucy derwende exploring content models multi document marization naacl hermann hermann kocisky grefenstette espeholt kay suleyman blunsom teaching machines read comprehend nips hochreiter schmidhuber sepp hochreiter jurgen schmidhuber long short term memory neural computation baotian qingcai chen fangze zhu sts large scale chinese short text summarization dataset emnlp irsoy cardie ozan irsoy claire cardie deep recursive neural networks compositionality language nips pages jean sebastien jean kyunghyun cho roland sevic yoshua bengio large target vocabulary neural machine translation acl nal kalchbrenner edward stette phil blunsom convolutional neural network modelling sentences acl june jiwei minh thang luong dan jurafsky hierarchical neural autoencoder paragraphs documents acl lin lin liu yang zhou hierarchical recurrent neural network document eling emnlp lin chin yew lin rouge package automatic ation summaries lopyrev konstantin lopyrev generating news headlines recurrent neural networks corr luhn hans peter luhn automatic creation literature abstracts ibm journal research development luong thang luong hieu pham christopher manning effective approaches attention based neural chine translation emnlp mani inderjeet mani automatic summarization jamins pub amsterdam manning manning surdeanu bauer finkel bethard mcclosky stanford corenlp natural guage processing toolkit acl mihalcea tarau rada mihalcea paul tarau trank bringing order text emnlp mikolov mikolov sutskever chen rado dean distributed representations words phrases compositionality nips nenkova mckeown ani nenkova kathleen eown survey text summarization techniques springer rush alexander rush sumit chopra jason weston neural attention model abstractive sentence marization emnlp socher socher huval manning semantic compositionality recursive vector spaces emnlp steinberger jezek steinberger jezek latent semantic analysis text summarization summary uation isim pages sutskever ilya sutskever james martens frey hinton generating text recurrent neural networks icml pages sutskever ilya sutskever oriol vinyals sequence sequence learning neural quoc networks nips pages tai kai sheng tai richard socher pher manning improved semantic representations structured long short term memory networks acl zhaopeng zhengdong yang liu xiaohua liu hang coverage based neural machine translation corr vanderwende suzuki ett nenkova sumbasic task focused marization sentence simplication lexical expansion wang cho tian wang kyunghyun cho context language modelling corr yin schutze yin schutze exploration acl student embeddings generalized phrases research workshop pages june zeiler matthew zeiler adadelta adaptive learning rate method corr zhu penn xiaodan zhu gerald penn comparing roles textual acoustic spoken language features spontaneous conversation summarization naacl zhu xiaodan zhu gerald penn frank icz summarizing multiple spoken documents finding evidence untranscribed audio acl zhu xiaodan zhu hongyu guo saif mohammad svetlana kiritchenko empirical study effect negation words sentiment proceedings annual meeting association computational linguistics zhu xiaodan zhu hongyu guo parinaz hani neural networks integrating compositional compositional sentiment sentiment composition ings joint conference lexical computational tics june zhu xiaodan zhu parinaz sobhani hongyu guo long short term memory recursive structures proceedings international conference machine learning zhu xiaodan zhu parinaz sobhani hongyu guo dag structured long short term memory semantic positionality proceedings meeting north ican chapter association computational linguistics naacl
