distraction based neural networks document summarization qian chen xiaodan zhu zhenhua ling si wei hui jiang university science technology china hefei china national research council canada ottawa canada iflytek research hefei china york university toronto canada ustc edu cn com edu cn com yorku t c o l c s c v v x r abstract distributed representation learned neural works recently shown effective eling natural languages ne granularities words phrases sentences approach extended help model larger spans text e documents triguing investigation sirable paper aims enhance neural network models purpose typical problem document level modeling automatic tion aims model documents order generate summaries paper propose ral models train computers pay tion specic regions content input ments attention models distract traverse different content document better grasp overall meaning marization engineering features train models large datasets models achieve state art performance signicantly benet distraction modeling particularly input documents long introduction modeling meaning text lies center natural guage understanding distributed representation learned neural networks recently shown effective eling ne granularities text including words collobert et al mikolov et al chen et al phrases yin schutze zhu et al guably sentences socher et al irsoy cardie kalchbrenner et al tai et al zhu et al zhu et al chen et al zhu et al approach extended help model larger spans text e documents intriguing investigation desirable interesting research conducted recently line li et al hu et al wang cho published international joint ence articial intelligence hermann et al typical problem level modeling automatic summarization mani das martins nenkova mckeown computers generate summaries documents based shallow deep understanding documents regards process representing input documents generating summaries interaction complicated function tting function expect large scale annotated dataset estimate large set parameters hand coding summarization knowledge different forms iting number model parameters e tractive summarization models adopted training data work explores direction utilizes relatively large datasets et al hermann et al train neural summarization models general neural networks universal approximators complicated functions shown fective problems recently understanding input documents generating maries challenging understanding recent work suggested distributed representations vectors adequate representing sentences let longer documents additional modeling soft hard tention applied retrospect subsequences words input text remedy limits shown improve performances different tasks discussed bahdanau et al luong et al rush et al regard mechanism provides connection input ment modeling encoding summary generating ing model level cognitive controls human summarizers input ments summaries summarize document consider control layer important paper focus better designing control layer marization propose neural models train computers pay attention specic regions content input documents attention models distract traverse different content document ter grasp overall meaning summarization engineering features train models large datasets models achieve state art performance signicantly benet tion modeling particularly input documents long explore technologies applied sentence level tasks extend document marization present paper technologies showed help improve summarization performance applied models leveraged technologies distraction models prove performance signicantly general models aim perform abstractive summarization related work distributed representation distributed representation shown effective modeling ne granularities text discussed recent work attempted model longer spans text neural networks li et al hu et al lin et al wang cho hermann et al includes research corporates document level information language ing wang cho lin et al answers questions hermann et al comprehending input documents attention based models relevant li et al learned distributed work resentation short documents averaged length word tokens objective summarization summarization typically faces documents longer summarization sary documents long paper propose neural models summarizing typical news articles thousands word tokens nd necessary enable computers pay attention specic content documents attention models distract traverse different content better grasp overall meaning summarization particularly ments long neural summarization models automatic summarization mani intensively studied text das martins nenkova mckeown speech zhu penn zhu et al art summarization models focused extractive summarization efforts exerted abstractive summarization recent neural tion models include recent efforts rush et al lopyrev hu et al research performed rush et al focuses neural models sentence compression rewriting document tion work lopyrev leverages neural networks generate news headline input documents ited word tokens work et al deals short texts dozens word tokens summarization problems content redundancy prominent attention based models sufcient summarization typically faces documents longer summarization needed uments long work attempt explore ral summarization technologies news articles thousands word tokens nd distraction based summarization models help improve performance note improvement achieved model outperformed attention based model reported et al short documents approach overview base model general encoder decoder work sutskever et al sutskever et al cho et al shown effective recently ferent tasks general sequence sequence ing framework encoding devoted model input documents decoder generate believe control layer helps navigate input documents optimize generation objectives importance focus control layer paper enrich expressiveness specically rization unlike recent work focuses tion order grasp local context correspondence e machine translation sentence compression force models traverse different content document avoid focusing region content better grasp overall meaning summarization objective explore popular technologies applied sentence level tasks extend ment summarization present help improve summarization performance gru based encoding decoding encoding general document modeling ing framework takes input document xtx write summary document output y yty summarization process modeled ing output text y maximizes conditional bility arg maxy given gold summary sequences discussed model found fective modeling sentences sub sentential text spans address challenges faced document level encoding restrict encoders architectures recurrent neural network rnn recent ture shows long short term memory lstm hochreiter schmidhuber sutskever et al gated rent units gru bahdanau et al good chitectures developing systems empirically found gru achieved similar performance lstm fast train describe gru implement neural summarization models simplest uni directional setting reading input symbols left right gru learns hidden tions hi time hi hi rn encodes content seen far time computed rm m dimensional embedding current word xi forward propagation gru computed follows hi ui ui hi hi u ri ui wu wr w rnm uu ur u rnn weight matrices n number hidden units element wise multiplication work actually applied bi directional grus grus found achieving better results single directional grus consistently suggests bi gru unit annotation vector ht encodes sequence directions modeling left right context figure shows encoder intuitively details readers refer bahdanau et al discussion figure high level view summarization model generation generating summaries decoder dicts word yt given annotations obtained coding h htx previously predicted words objective probability summary y decomposition ordered als y argmax h argmax st ty ty y y equation depicts high level abstraction erating summary word yt previous output input annotations h htx yt legal output word time t y optimal summary found model conditional probability rewritten tion factorize model according structures neural networks function st ct nonlinear function computes probability vector legal words output time t st takes ement resulting vector corresponding word yt e predicated probability word yt vector st ct control layers connect y input h discuss details tion completeness function g computed st ct uost coct softmax function wo rkn uo rnn vo rnm co weight matrices k vocabulary size rm m dimensional ding previously predicted word control layers document modeling summary generation scribed components input document ing summary generation core problem components associated sentence level modeling machine translation speech recognition attention model applied grasp local context dence input output texts example lation attention shown useful aligning words target language language translated corresponding source words context attention regarded type cognitive controls modeling documents general viewpoint control layer propose distraction modeling enable model traverse different content long document improves summarization performance signicantly general control layer allows cated examination input section describe controls consider attention distraction navigate input documents generate summaries layer hidden output rst extended recent level hidden output model luong et al marization models presented later experiments level hidden output model consistently improves summarization performance different datasets specically updating st follows layer gru chitecture shown figure st t t ct forward propagation computed similar equation use untied parameter matrices layer model allows ing direct interaction t ct encoding current previous output information encoding current input content primed distraction attention discuss vectors computed distraction training propose enforce distraction perspectives adding distraction constraints training decoding rst discuss tion training distraction input content vectors training force model pay attention content input documents accumulate previously viewed content vector history content vector cj incorporate currently computed t refer model ct t uc cj wc ua diagonal ces t input content vectors rectly penalized history t directly computed conventional equation follows found achieving better performance natives e distances held data d t min dc t max ci ds t max si distraction score added output probability beam search order encourage model avoid redundant content ty scoret t t t t t ihi hi annotation vectors encode current input word context input gru described equation t attention weight hi current output time t distraction based ct computed equation incorporated equation distraction attention weight vectors propose add distraction directly attention weight vectors ilarly accumulate past attention weights history attention weight j use prime current attention weights model tu et al uses history attention weights use history force distraction order avoid redundancy concern machine translation task refer model scoret follows beam search distraction algorithm parameter determined development set refer model algorithm beam search distraction require vocabulary size k beam size b max output length n computed probabilities words vocabulary choose b likely words initialize b potheses n hypothesis compute conditional probabilities b k candidates sponding probabilities use distraction primed value score choose b t vt t uahi ba j likely candidates end wa rln ua ba rl rl weight matrices l number hidden units note t uahi equation computes ventional attention penalizing attention history normalized softmax generate attention weights t turn equation t t t j distraction decoding decoding process enforced different types distraction computing difference distribution current tion weight t previous attention weights seen proper probabilistic distribution normalized equation leibler kl divergence measure difference equation found consistently better distance metrics tried held data enforced distraction similar way attention primed input content vector ct hidden output vector st ct st normalized regular content vectors cosine similarity unknown word replacement summarization rowed unknown word replacement jean et al machine translation summarization models found improved performance summarizing long documents specically time complexity dling larger vocabulary softmax layer summary generation infrequent words removed lary replaced symbol old vocabulary size data dependent detailed later experiment set section rst round summary generated document token labeled replaced word input documents specically obtained ment equation e largest element t nd source location current experiment set data experiment summarization models licly available corpora different document lengths different languages cnn news collection hermann et al chinese corpus available cently et al large datasets appropriate training neural models discussed ploy large number parameters t potentially plicated summarization process involving representing input documents generating summaries interacting hermann et al cnn data cnn data human generated real life summary news cle dataset collected available github data preprocessed stanford corenlp tools manning et al tokenization boundary detection capital information kept speed training removed documents long word tokens training validation set kept documents test set change difculty task lcsts data second corpus lcsts nese corpus available recently et al data constructed chinese microblogging site sina weibo original training testing split mentioned et al additionally randomly sampled small training data validation set table gives details datasets table averaged document length cnn corpus seven time long lcsts corpus summary times longer lcsts cnn train valid test doc l sum l doc train valid test table cnn lcsts dataset rst rows table averaged document length doc l summary length sum l terms numbers word kens row lists number documents datasets training details mini batch stochastic gradient descent sgd timize log likelihood adadelta zeiler matically adapt learning rate parameters cnn dataset training performed shufed mini batches size sorting length limit vocabulary include frequent words words replaced token cussed earlier paper based validation data set embedding dimension vector length den layers uni gru bi gru end sentence token inserted sentence end document token added end beam size decoder set lcsts data larger mini batch size found better based observation validation set com deepmind rc data et al characters words tokens vocabulary size ding dimension vector size hidden layer nodes beam search size cnn dataset code publicly tion uses python based theano library bergstra et al experimental results results cnn dataset overall performance results cnn dataset presented table rouge scores lin measure performance summary lengths preset report rouge upper table includes baseline results number ical summarization algorithms listed ble luhn luhn edmundson edmundson lsa steinberger jezek lex rank erkan radev text rank mihalcea tarau basic vanderwende et al kl sum haghighi vanderwende baseline results implemented open source tool results lower half table bi gru encoder achieves better performance gru encoder consistent results sts dataset reported later table level output model discussed method section cial consistent results sts dataset addition unknown replacement technique yields additional improvement strong model technologies row marked unk replace model row incorporates distraction modeling nally achieves score score rouge l score signicantly improving rouge scores respectively largest improvement presented table pared techniques listed table lists details model improve formance additively neural models neer features use content additional formality features locations input sentences bring additional improvement performance different lengths documents observe effectiveness distraction model different ument lengths selected short documents cnn training dataset subset age length word tokens subset data number documents averaged document length word tokens shown table data distraction model improves results signicantly relative improvement compared respectively general best formance dataset lower training code available com nats python org pypi sumy system luhn edmundson lsa lex rank text rank sum basic kl sum uni gru bi gru level unk replace distraction distraction distraction rouge l table results cnn dataset data suggesting training data improve marization performance rouge l w distraction distraction relative impr w distraction distraction relative impr table results subsets cnn datasets different document lengths results lcsts dataset experiment proposed model public lcsts corpus baseline best result reported et al modied uni gru achieves slight ment reported results bi gru attention based model achieves better performance conrming ness bi directional models summarization implementation state art serves strong baseline cnn dataset discussed note input text length lcsts far shorter cnn documents containing words roughly sentences distraction improve performance contrast documents longer benets signicant achieving biggest improvement discussed earlier suggests ness distraction modeling helping summarize thank authors et al generously sharing latest output models achieves better mance results reported et al reported updated scores higher performance baseline challenging longer documents summarization necessary short texts system et al uni gru bi gru level att unk replace distraction rouge l table results lcsts dataset compare models simple baseline selects rst n numbers word tokens input documents reaches maximal rouge scores rst tokens taken achieves rouge l models signicantly better cnn data set ing rst sentences achieves best results reach rouge l respectively cnn data news data baseline selecting rst sentences known strong baseline models explore forming abstractive summarization conclusions future work propose train neural document summarization models pay attention specic regions input documents attention models distract models different content order better grasp overall meaning input documents engineering features train models large datasets models achieve art performance signicantly benet distraction modeling particularly input documents long explore recent technologies summarization help improve tion performance applied models leveraged technologies distraction models improve performance signicantly general viewpoint enriching ness control layers link input encoding layer output decoding layer importance edy shortcomings current models plan form work direction acknowledgments rst author paper supported science technology development hui province china grants mental research funds central universities grant strategic priority research program chinese academy sciences grant references bahdanau et al dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate corr bergstra et al j bergstra o breuleux f bastien p blin r pascanu g desjardins j turian d warde farley y bengio theano cpu gpu math expression compiler scipy volume page austin tx chen et al zhigang chen wei lin qian chen si wei hui jiang xiaodan zhu revisiting word embedding ing meaning proceedings acl chen et al qian chen xiaodan zhu zhenhua ling si wei hui jiang enhancing combining sequential tree lstm natural language inference cho et al k cho b van merrienboer c gulcehre d bahdanau f bougares h schwenk y bengio learning phrase representations rnn encoder decoder statistical machine translation emnlp collobert et al r collobert j weston l bottou m karlen k kavukcuoglu p kuksa natural language processing scratch jmlr das martins dipanjan das andre martins vey automatic text summarization edmundson harold p edmundson new methods matic extracting jacm erkan radev gunes erkan dragomir r radev lexrank graph based lexical centrality salience text marization jair pages haghighi vanderwende aria haghighi lucy derwende exploring content models multi document marization naacl hermann et al k hermann t kocisky e grefenstette l espeholt w kay m suleyman p blunsom teaching machines read comprehend nips hochreiter schmidhuber sepp hochreiter jurgen schmidhuber long short term memory neural computation et al baotian hu qingcai chen fangze zhu sts large scale chinese short text summarization dataset emnlp irsoy cardie ozan irsoy claire cardie deep recursive neural networks compositionality language nips pages jean et al sebastien jean kyunghyun cho roland sevic yoshua bengio large target vocabulary neural machine translation acl et al nal kalchbrenner edward stette phil blunsom convolutional neural network modelling sentences acl june et al jiwei li minh thang luong dan jurafsky hierarchical neural autoencoder paragraphs documents acl lin et al r lin s liu m yang m li m zhou s li hierarchical recurrent neural network document eling emnlp lin chin yew lin rouge package automatic ation summaries lopyrev konstantin lopyrev generating news headlines recurrent neural networks corr luhn hans peter luhn automatic creation literature abstracts ibm journal research development luong et al thang luong hieu pham christopher d manning effective approaches attention based neural chine translation emnlp mani inderjeet mani automatic summarization j jamins pub co amsterdam manning et al c manning m surdeanu j bauer j finkel s bethard d mcclosky stanford corenlp natural guage processing toolkit acl mihalcea tarau rada mihalcea paul tarau trank bringing order text emnlp mikolov et al t mikolov sutskever k chen g rado j dean distributed representations words phrases compositionality nips nenkova mckeown ani nenkova kathleen eown survey text summarization techniques springer rush et al alexander m rush sumit chopra jason weston neural attention model abstractive sentence marization emnlp socher et al r socher b huval c manning ng semantic compositionality recursive vector spaces emnlp steinberger jezek j steinberger k jezek latent semantic analysis text summarization summary uation isim pages sutskever et al ilya sutskever james martens frey hinton generating text recurrent neural networks icml pages sutskever et al ilya sutskever oriol vinyals sequence sequence learning neural quoc vv le networks nips pages tai et al kai sheng tai richard socher pher manning improved semantic representations structured long short term memory networks acl tu et al zhaopeng tu zhengdong lu yang liu xiaohua liu hang li coverage based neural machine translation corr et al l vanderwende h suzuki c ett nenkova sumbasic task focused marization sentence simplication lexical expansion wang cho tian wang kyunghyun cho context language modelling corr yin schutze w yin h schutze exploration acl student embeddings generalized phrases research workshop pages june zeiler matthew d zeiler adadelta adaptive learning rate method corr zhu penn xiaodan zhu gerald penn comparing roles textual acoustic spoken language features spontaneous conversation summarization naacl zhu et al xiaodan zhu gerald penn frank icz summarizing multiple spoken documents finding evidence untranscribed audio acl zhu et al xiaodan zhu hongyu guo saif mohammad svetlana kiritchenko empirical study effect negation words sentiment proceedings annual meeting association computational linguistics zhu et al xiaodan zhu hongyu guo parinaz hani neural networks integrating compositional compositional sentiment sentiment composition ings joint conference lexical computational tics june zhu et al xiaodan zhu parinaz sobhani hongyu guo long short term memory recursive structures proceedings international conference machine learning zhu et al xiaodan zhu parinaz sobhani hongyu guo dag structured long short term memory semantic positionality proceedings meeting north ican chapter association computational linguistics naacl
