distraction based neural networks for document summarization qian chen xiaodan zhu zhenhua ling si wei hui jiang university of science and technology of china hefei china national research council canada ottawa canada iflytek research hefei china york university toronto canada t c o l c s c v v i x r a abstract distributed representation learned with neural works has recently shown to be effective in eling natural languages at ne granularities such as words phrases and even sentences whether and how such an approach can be extended to help model larger spans of text documents is triguing and further investigation would still be sirable this paper aims to enhance neural network models for such a purpose a typical problem of document level modeling is automatic tion which aims to model documents in order to generate summaries in this paper we propose ral models to train computers not just to pay tion to specic regions and content of input ments with attention models but also distract them to traverse between different content of a document so as to better grasp the overall meaning for marization without engineering any features we train the models on two large datasets the models achieve the state of the art performance and they signicantly benet from the distraction modeling particularly when input documents are long introduction modeling the meaning of text lies in center of natural guage understanding distributed representation learned with neural networks has recently shown to be effective in eling ne granularities of text including words collobert et al mikolov et al chen et al phrases yin and schutze zhu et al and guably sentences socher et al irsoy and cardie kalchbrenner al tai et al zhu et al zhu et al chen et al zhu et al whether and how such an approach can be extended to help model larger spans of text documents is intriguing and further investigation would still be desirable although there has been interesting research conducted recently along this line li et al hu et al wang and cho published in the international joint ence on articial intelligence hermann et al a typical problem of level modeling is automatic summarization mani das and martins nenkova and mckeown in which computers generate summaries for documents based on their shallow or deep understanding of the documents if one regards the process of representing input documents generating summaries and the interaction between them to be a complicated function tting such a function could expect to have a large scale annotated dataset to estimate a large set of parameters while on the other hand coding summarization knowledge in different forms or iting the number of model parameters as in many tractive summarization models are often adopted when there are no enough training data this work explores the former direction and utilizes relatively large datasets hu et al hermann et al to train neural summarization models in general neural networks as universal approximators can very complicated functions and have shown to be very fective on many problems recently understanding the input documents and generating maries are both challenging on the understanding side much recent work seems to have suggested that distributed representations often vectors by themselves may not be adequate for representing sentences let along with longer documents additional modeling such as soft or hard tention has been applied to retrospect subsequences or even words in input text to remedy the limits which has shown to improve performances of different tasks such as those discussed in bahdanau al luong et al rush et al among others we regard this to be a mechanism that provides a connection between input ment modeling encoding and summary generating ing which could model a level of cognitive controls human summarizers themselves often move between the input ments and summaries when they summarize a document we consider this control layer to be important and in this paper we focus on better designing this control layer for marization we propose neural models to train computers not just to pay attention to specic regions and content of input documents with attention models but also distract them to traverse between different content of a document so as to ter grasp the overall meaning for summarization without engineering any features we train the models with two large datasets the models achieve the state of the art performance and they signicantly benet from the tion modeling particularly when input documents are long we also explore several technologies that have been applied to sentence level tasks and extend them to document marization and we present in this paper the technologies that showed to help improve the summarization performance even when it is applied onto the models that have leveraged these technologies the distraction models can further prove the performance signicantly in general our models here aim to perform abstractive summarization related work distributed representation distributed representation has shown to be effective in modeling ne granularities of text as discussed above much recent work has also attempted to model longer spans of text with neural networks li et al hu et al lin et al wang and cho hermann et al this includes research that corporates document level information for language ing wang and cho lin et al and that answers questions hermann et al by comprehending input documents with attention based models more relevant to li et al learned distributed ours the work of resentation for short documents with the averaged length of about a hundred word tokens although the objective is not summarization summarization typically faces documents longer than those and summarization may be more sary when documents are long in this paper we propose neural models for summarizing typical news articles with up to thousands of word tokens we nd it is necessary to enable computers not just to pay attention to specic content of put documents with attention models but also distract them to traverse between different content so as to better grasp the overall meaning for summarization particularly when ments are long neural summarization models automatic summarization mani has been intensively studied for both text das and martins nenkova and mckeown and speech zhu and penn zhu et al most of the art summarization models have focused on extractive summarization although some efforts have also been exerted on abstractive summarization recent neural tion models include the recent efforts of rush et al lopyrev hu et al the research performed in rush et al focuses on neural models for sentence compression and rewriting but not full document tion the work of lopyrev leverages neural networks to generate news headline where input documents are ited to word tokens and the work of et al also deals with short texts up to dozens of word tokens in which summarization problems such as content redundancy is less prominent and attention based models seem to be sufcient however summarization typically faces documents longer than that and summarization is often more needed when uments are long in this work we attempt to explore ral summarization technologies for news articles with up to thousands of word tokens in which we nd distraction based summarization models help improve performance note that our improvement is achieved over the model that has already outperformed the attention based model reported in et al on short documents our approach overview we base our model on the general encoder decoder work sutskever et al sutskever et al cho et al that has shown to be effective recently on ferent tasks this is a general sequence to sequence ing framework in which the encoding part can be devoted to model the input documents and the decoder to generate put we believe the control layer that helps navigate the input documents to optimize the generation objectives would be of importance and we will focus on the control layer in this paper and enrich its expressiveness specically for rization unlike much recent work that focuses more on tion in order to grasp local context or correspondence in machine translation and sentence compression we force our models to traverse between different content of a document to avoid focusing on a region or same content to better grasp the overall meaning for the summarization objective we also explore several popular technologies that have been applied to sentence level tasks and extend them to ment summarization and we present those that help improve the summarization performance gru based encoding and decoding encoding the general document modeling and ing framework takes in an input document xtx and write the summary of the document as the output y yty the summarization process is modeled as ing the output text y that maximizes the conditional bility arg maxy given gold summary sequences as discussed above such a model has been found to be very fective in modeling sentences or sub sentential text spans we will address the challenges faced at the document level on encoding we do not restrict the encoders architectures as if it is a recurrent neural network rnn the recent ture shows long short term memory lstm hochreiter and schmidhuber sutskever et al and gated rent units gru bahdanau et al are both good chitectures in developing our systems we empirically found gru achieved similar performance as lstm but it is fast to train we will therefore describe the gru implement of our neural summarization models in the simplest uni directional setting when reading input symbols from left to right a gru learns the hidden tions hi at time i with hi where the hi rn encodes all content seen so far at time i which is computed from and where rm is the m dimensional embedding of the current word xi the forward propagation of gru is computed as follows hi ui ui hi hi u ri ui where wu wr w rnm and uu ur u rnn are weight matrices n is the number of hidden units and is element wise multiplication in our work we actually applied bi directional grus grus which we found achieving better results than single directional grus consistently as its name suggests in a bi gru unit the annotation vector ht encodes the sequence from two directions modeling both the left and right context the bottom part of figure shows the encoder intuitively while for more details readers can refer to bahdanau al for further discussion figure a high level view of the summarization model generation when generating summaries the decoder dicts the next word yt given all annotations obtained in coding h htx as well as all previously predicted words the objective is a probability over the summary y with decomposition into the ordered als y argmax h argmax st ty ty y y where equation depicts a high level abstraction of erating a summary word yt over previous output as well as input annotations h htx and yt is a legal output word at time t while y is the optimal summary found by the model the conditional probability is further rewritten as tion to factorize the model according to the structures of neural networks the function st ct is a nonlinear function that computes the probability vector for all legal put words at output time t and st takes the ement of the resulting vector corresponding to word yt the predicated probability for word yt the vector st and ct are the control layers that connect put y and input h which we will discuss in details in tion for completeness function g is computed with st ct uost coct where is a softmax function wo rkn uo rnn vo rnm and co are weight matrices k is the vocabulary size rm is the m dimensional ding of the previously predicted word the control layers the document modeling and summary generation are scribed above as two components input document ing and summary generation a core problem is how these two components are associated in sentence level modeling such as machine translation and speech recognition attention model is often applied to grasp local context and dence between input and output texts for example in lation attention is shown to be very useful for aligning the words of the target language the language being translated to to the corresponding source words and their context attention can be regarded as a type of cognitive controls in modeling documents we take a general viewpoint on this control layer and propose distraction modeling to enable the model to traverse over different content of a long document and we will show it improves the summarization performance signicantly in general the control layer allows a cated examination over the input in this section we describe the controls that consider both attention and distraction to navigate input documents and to generate summaries two layer hidden output we rst extended the recent level hidden output model luong et al to our marization models as presented later in the experiments the two level hidden output model consistently improves the summarization performance on different datasets more specically the updating of st follows a two layer gru chitecture shown in the top part of figure st t t ct the forward propagation of and are computed similar to equation above and use untied parameter matrices the two layer model allows for ing a direct interaction between t and ct with the former encoding the current and previous output information and the latter encoding the current input content that is primed with distraction and attention we will discuss how these vectors are computed below distraction in training we propose to enforce distraction from two perspectives adding the distraction constraints in training as well as in decoding we rst discuss the tion in training distraction over input content vectors in training we force the model not to pay attention to the same content or same part of the input documents too much we accumulate the previously viewed content vector as a history content vector cj and incorporate it into the currently computed we refer to this model as ct t uc cj where wc and ua are diagonal ces and t is input content vectors that have not been rectly penalized with history yet t is directly computed with conventional equation as follows was found achieving a better performance than several natives and distances on the held out data d t min i dc t max ci ds t max si the distraction score was then added into the output probability and the beam search in order to encourage the model to avoid redundant content ty scoret t t t t t ihi where hi are annotation vectors that encode the current input word and its context with the input gru described above in equation and t i is the attention weight put on hi at the current output time the distraction based ct computed in equation can then be incorporated in equation distraction over attention weight vectors we also propose to add distraction directly on the attention weight vectors ilarly as above we accumulate the past attention weights as a history attention weight j i and use it to prime the current attention weights the model in tu al also uses history attention weights but we use history here to force distraction in order to avoid redundancy which is not a concern in the machine translation task we refer to the model as where scoret was used as follows in the beam search with distraction algorithm and parameter and were determined on the development set we refer to this model as algorithm beam search with distraction require vocabulary size k beam size b max output length n computed probabilities of all the words in vocabulary choose the b most likely words and initialize the b potheses for i n do for each hypothesis compute the next conditional probabilities then have b k candidates with the sponding probabilities use the distraction primed value score to choose b t i vt a t uahi ba j i most likely candidates end for where wa rln ua ba rl and rl are the weight matrices and l is the number of hidden units note that t uahi in the equation computes the ventional attention without penalizing attention history is often normalized with a softmax to generate attention weights t i below which is in turn used in equation t t i t j distraction in decoding in the decoding process we also enforced different types of distraction one by computing the difference between the distribution of the current tion weight t and that of all previous attention weights since can be seen as a proper probabilistic distribution normalized in equation we used leibler kl divergence to measure their difference with equation which was found to be consistently better than several other distance metrics we tried on the held out data we also enforced distraction in a similar way on the attention primed input content vector ct as well as on the hidden output vector st both ct and st are not normalized but are regular content vectors where the cosine similarity unknown word replacement for summarization we rowed the unknown word replacement jean et al from machine translation to our summarization models and found it improved the performance when summarizing long documents specically due to the time complexity in dling a larger vocabulary in the softmax layer in summary generation infrequent words were removed from the lary and were replaced with the symbol the old of vocabulary size is data dependent and will be detailed later in the experiment set up section after the rst round summary generated for a document a token labeled as will be replaced with a word in the input documents more specically we obtained the ment using equation we used the largest element in t to nd the source location for the current experiment set up data we experiment with our summarization models on two licly available corpora with different document lengths and in different languages a cnn news collection hermann et al and a chinese corpus made available more cently in et al both are large datasets appropriate for training neural models which as discussed above ploy a large number of parameters to t the potentially plicated summarization process involving representing input documents generating summaries and interacting between them hermann et al have cnn data the cnn data a human generated real life summary for each news cle the dataset collected in was made available at the data was preprocessed with the stanford corenlp tools manning al for tokenization and boundary detection all capital information is kept to speed up training we removed the documents that are too long over word tokens from the training and validation set but kept all documents in the test set which does not change the difculty of the task lcsts data the second corpus is lcsts which is a nese corpus made available more recently in et al the data is constructed from the chinese microblogging site sina weibo we used the original training testing split mentioned in hu et al but additionally randomly sampled a small part of the training data as our validation set table gives more details about the two datasets we can see from the table that averaged document length of the cnn corpus is about seven time as long as the lcsts corpus and the summary is about times longer lcsts cnn train valid test doc sum doc train valid test table the cnn and lcsts dataset the rst two rows of the table are the averaged document length doc and summary length sum in terms of numbers of word kens the bottom row lists the number of documents in the datasets training details we used mini batch stochastic gradient descent sgd to timize log likelihood and adadelta zeiler to matically adapt the learning rate of parameters and for the cnn dataset training was performed with shufed mini batches of size after sorting by length we limit our vocabulary to include the top most frequent words other words were replaced with the token as cussed earlier in the paper based on the validation data we set embedding dimension to be the vector length in den layers to be for uni gru and for bi gru an end of sentence token was inserted between every sentence and an end of document token was added at the end the beam size of decoder was set to be for the lcsts data a larger mini batch size was found to be better based the observation on the validation set same as in et al we used characters rather than words as our tokens the vocabulary size is ding dimension is and the vector size of the hidden layer nodes is beam search size is same as in the cnn dataset we make our code publicly our tion uses python and is based on the theano library bergstra et al experimental results results on the cnn dataset overall performance our results on the cnn dataset are presented in table we used rouge scores lin to measure performance since the summary lengths are not preset to be the same we report rouge the upper part of the table includes the baseline results of a number of ical summarization algorithms which we listed in the ble as luhn luhn edmundson edmundson lsa steinberger and jezek lex rank erkan and radev text rank mihalcea and tarau basic vanderwende al and kl sum haghighi and vanderwende these baseline results are implemented in the open source tool the results at the lower half of the table show that the bi gru encoder achieves a better performance than the gru encoder this is consistent with the results on the sts dataset reported later in table we show that two level output model we discussed in the method section is cial which is also consistent with the results on the sts dataset in addition the unknown replacement technique yields an additional improvement over the strong model that has used these technologies the row marked as unk replace the model in the last row that incorporates all distraction modeling and nally achieves a score of a score of and a rouge l score of signicantly improving the three rouge scores by and respectively these are also the largest improvement presented in the table pared with the other techniques listed the table also lists the details of how the model and improve the formance additively again the neural models do not neer any features and use only content but not any additional formality features such as locations of input sentences which may bring additional improvement performance on different lengths of documents to observe the effectiveness of the distraction model over different ument lengths we further selected all short documents from the cnn training dataset into a subset with age length at word tokens and a subset of data that have the same number of documents as the with averaged document length at word tokens as shown in table on the data the distraction model improves the results more signicantly the relative improvement is and compared with and on respectively in general the best formance on both dataset is lower than that using all training code is available at system luhn edmundson lsa lex rank text rank sum basic kl sum uni gru bi gru two level out unk replace distraction distraction distraction rouge l table results on the cnn dataset data suggesting using more training data can improve marization performance rouge l distraction distraction relative impr distraction distraction relative impr table results on two subsets of the cnn datasets with different document lengths results on the lcsts dataset we experiment with the proposed model on public lcsts corpus the baseline is the best result reported in et al our modied uni gru achieves a slight ment over the reported results the bi gru attention based model achieves a better performance conrming the ness of bi directional models for summarization as well as that our implementation is the state of the art and serves as a very strong baseline in the cnn dataset discussed above note that since the input text length of lcsts is far shorter than the cnn documents each containing about words and roughly sentences we show that distraction does not improve the performance but in contrast when documents are longer its benets are signicant achieving the biggest improvement as discussed earlier this suggests the ness of distraction modeling in helping summarize the more thank the authors of et al for generously sharing us the latest output of their models which achieves a better mance than the results reported in et al we reported here the updated scores higher performance as our baseline challenging longer documents where summarization is often more necessary than for short texts system et al uni gru bi gru two level att unk replace distraction rouge l table results on the lcsts dataset we also compare our models with the simple baseline that selects the rst n numbers of word tokens from the input documents which reaches its maximal rouge scores when the rst tokens were taken and achieves and rouge l at and and our models are signicantly better than that for the cnn data set ing the rst three sentences achieves the best results which reach and rouge l at and respectively since the cnn data is news data the baseline of selecting rst several sentences has known to be a very strong baseline again the models we explore here are towards forming abstractive summarization conclusions and future work we propose to train neural document summarization models not just to pay attention to specic regions of input documents with attention models but also distract the models to different content in order to better grasp the overall meaning of input documents without engineering any features we train the models on two large datasets the models achieve the of the art performance and they signicantly benet from the distraction modeling particularly when the input documents are long we also explore several recent technologies for summarization and show that they help improve tion performance as well even if applied onto the models that have already leveraged these technologies the distraction models can further improve the performance signicantly from a more general viewpoint enriching the ness of the control layers that link the input encoding layer and the output decoding layer could be of importance to edy the shortcomings of the current models we plan to form more work along this direction acknowledgments the rst and the third author of this paper were supported in part by the science and technology development of hui province china grants no the mental research funds for the central universities grant no and the strategic priority research program of the chinese academy of sciences grant no references bahdanau et al dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation by jointly learning to align and translate corr bergstra et al j bergstra o breuleux f bastien p blin r pascanu g desjardins j turian d warde farley and y bengio theano a cpu and gpu math expression compiler in scipy volume page austin tx chen et al zhigang chen wei lin qian chen si wei hui jiang and xiaodan zhu revisiting word embedding for ing meaning in proceedings of acl chen et al qian chen xiaodan zhu zhenhua ling si wei and hui jiang enhancing and combining sequential and tree lstm for natural language inference in cho et al cho van merrienboer gulcehre bahdanau bougares schwenk and bengio learning phrase representations using rnn encoder decoder for statistical machine translation in emnlp collobert et al collobert weston bottou karlen kavukcuoglu and kuksa natural language processing almost from scratch jmlr das and martins dipanjan das and andre martins a vey on automatic text summarization edmundson harold p edmundson new methods in matic extracting jacm erkan and radev gunes erkan and dragomir r radev lexrank graph based lexical centrality as salience in text marization jair pages haghighi and vanderwende aria haghighi and lucy derwende exploring content models for multi document marization in naacl hermann et al hermann kocisky grefenstette espeholt kay suleyman and blunsom teaching machines to read and comprehend in nips hochreiter and schmidhuber sepp hochreiter and jurgen schmidhuber long short term memory neural computation et al baotian hu qingcai chen and fangze zhu sts a large scale chinese short text summarization dataset in emnlp irsoy and cardie ozan irsoy and claire cardie deep recursive neural networks for compositionality in language in nips pages jean et al sebastien jean kyunghyun cho roland sevic and yoshua bengio on using very large target vocabulary for neural machine translation in acl kalchbrenner al nal kalchbrenner edward stette and phil blunsom a convolutional neural network for modelling sentences acl june li et al jiwei li minh thang luong and dan jurafsky a hierarchical neural autoencoder for paragraphs and documents in acl lin et al lin liu yang li zhou and li hierarchical recurrent neural network for document eling in emnlp lin chin yew lin rouge a package for automatic ation of summaries lopyrev konstantin lopyrev generating news headlines with recurrent neural networks corr luhn hans peter luhn the automatic creation of literature abstracts ibm journal of research and development luong et al thang luong hieu pham and christopher manning effective approaches to attention based neural chine translation in emnlp mani inderjeet mani automatic summarization jamins pub amsterdam manning al c manning m surdeanu j bauer j finkel s bethard and d mcclosky the stanford corenlp natural guage processing toolkit in acl mihalcea and tarau rada mihalcea and paul tarau trank bringing order into text in emnlp mikolov et al mikolov sutskever chen rado and dean distributed representations of words and phrases and their compositionality in nips nenkova and mckeown ani nenkova and kathleen eown a survey of text summarization techniques springer rush et al alexander rush sumit chopra and jason weston a neural attention model for abstractive sentence marization in emnlp socher et al socher huval manning and ng semantic compositionality through recursive vector spaces in emnlp steinberger and jezek steinberger and jezek using latent semantic analysis in text summarization and summary uation in isim pages sutskever et al ilya sutskever james martens and frey hinton generating text with recurrent neural networks in icml pages sutskever et al ilya sutskever oriol vinyals and sequence to sequence learning with neural quoc vv le networks in nips pages tai et al kai sheng tai richard socher and pher manning improved semantic representations from structured long short term memory networks in acl tu et al zhaopeng tu zhengdong lu yang liu xiaohua liu and hang li coverage based neural machine translation corr et al vanderwende suzuki ett and nenkova beyond sumbasic task focused marization with sentence simplication and lexical expansion wang and cho tian wang and kyunghyun cho context language modelling corr yin and schutze yin and schutze an exploration in acl student of embeddings for generalized phrases research workshop pages june zeiler matthew zeiler adadelta an adaptive learning rate method corr zhu and penn xiaodan zhu and gerald penn comparing the roles of textual acoustic and spoken language features on spontaneous conversation summarization in naacl zhu et al xiaodan zhu gerald penn and frank icz summarizing multiple spoken documents finding evidence from untranscribed audio in acl zhu et al xiaodan zhu hongyu guo saif mohammad and svetlana kiritchenko an empirical study on the effect of negation words on sentiment in proceedings of the annual meeting of the association for computational linguistics zhu et al xiaodan zhu hongyu guo and parinaz hani neural networks for integrating compositional and compositional sentiment in sentiment composition in ings of joint conference on lexical and computational tics june zhu al xiaodan zhu parinaz sobhani and hongyu guo long short term memory over recursive structures in proceedings of international conference on machine learning zhu al xiaodan zhu parinaz sobhani and hongyu guo dag structured long short term memory for semantic positionality in proceedings of the meeting of the north ican chapter of the association for computational linguistics naacl
