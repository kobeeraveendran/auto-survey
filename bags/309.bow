n u j l c s c v v i x r a seal segment wise extractive abstractive long form text summarization yao zhao google research brain team com mohammad saleh google research brain team com peter j liu google research brain team com abstract most prior work in the sequence to sequence paradigm focused on datasets with put sequence lengths in the hundreds of tokens due to the computational constraints of common rnn and transformer architectures in this paper we study long form abstractive text summarization a sequence to sequence setting with input sequence lengths up to tokens and output sequence lengths up to tokens we propose seal a transformer based model featuring a new encoder decoder tention that dynamically extracts selects input snippets to sparsely attend to for each output segment using only the original documents and summaries we derive proxy labels that provide weak supervision for extractive layers simultaneously with regular supervision from abstractive summaries the seal model achieves state of the art results on existing long form summarization tasks and outperforms strong baseline models on a new dataset task we introduce with much longer input text since content selection is explicit in the seal model a desirable side effect is that the selection can be inspected for enhanced interpretability introduction text summarization is a language generation task that seeks to output concise and informative content given possibly multiple input documents abstractive summarization aims to summarize text beyond solely copying text segments usually whole sentences using novel words and phrases to be more concise comprehensive or achieve a certain style neural abstractive summarization is a data driven summarization approach that trains sequence to sequence models on large numbers of document summary pairs and have demonstrated promising results on summarizing relatively short texts where the input documents typically contain up to document tokens and up to summary tokens a few sentences or a paragraph despite progress in relatively short form settings the development of long form neural abstractive summarization lf nas was constrained due to the lack of large scale long form datasets recently there has been increasing interest in collecting such datasets for single document sds including for scientic articles from arxiv pubmed and multi document summarization mds although the challenge remains of developing model architectures that can cope with that increased data scale in particular sequence length prior work on lf nas typically divided the task into two separate stages mainly the result of memory and computation constraints associated with processing long input sequences with rnn or transformer models the rst extractive stage selects a small fraction of the input text for the second abstractive stage which typically relies on some avor of sequence to sequence model to process extracted inputs into summaries in this work we systematically study single models for lf nas trained jointly with weak supervision for extractive layers and with regular supervision for abstractive layers to use a single approach for preprint under review both sds and mds we break input documents into sequences of snippets section and consider three general categories of modeling approach the rst truncates inputs to leading snippets section the second compressive abstractive compresses all snippets into a xed length representation that is used as input to the abstractive attention layers section the third extractive abstractive encodes each snippet independently then sparsely selects snippets when applying abstractive attention layers section the sparse selection is performed by a scorer sub network and gating function the scorer is weakly supervised by proxy extractive labels automatically calculated from input snippets and gold summaries similar to nallapati et al the abstractive attention layers employ full encoder decoder attention on the resulting input representations along with self attention which is supervised using the abstractive summaries during training the extractive and abstractive losses are added together and optimized we propose the segment wise extractive abstractive long form model section or seal that generalizes and improves the extractive abstractive approach by augmenting the scorer inputs with previously decoded tokens for each segment allowing the model to dynamically select input snippets during decoding although interpretability was not our goal as a nice side effect the model shows the link between selected input snippets and decoded segments resulting in better interpretability than soft attention models we are able to apply our models to very long input documents and achieve state of the art results with the seal model on the arxiv pubmed datasets to further demonstrate the advantage of the seal model over other approaches we collected a massive new dataset which we use to generate full wikipedia pages from top search results with up to input tokens related work sequence to sequence has become a dominant paradigm in abstractive summarization using decoder architectures based on rnns and more recently transformer transformers have shown to be more effective at incorporating long term dependencies than rnns but have issues scaling running into computation and memory limitations quickly beyond sequence tokens in sequence length n one option is to simply truncate sequences but depending on the task may drop important information another option is to factor the summarization problem into extractive and abstractive stages the extract abstract framework has received increased attention recently as a natural way to reduce the input size for abstractive sequence to sequence models in past work the abstractive model was trained separately from the extractive stage chen and bansal trained extractor and abstractor sub modules separately then used reinforce to ne tune the non differentiable extractive selection end to end however the reinforce slows training down signicantly and is not scalable for our setting amplayo and lapata proposed encoding documents using an encoder derived from a pre trained autoencoder followed by a pooling layer before processing with a separately trained decoder which they call condense abstract our proposed seal model and baseline models loosely t in the extract abstract or abstract frameworks except they are trained jointly in a single model end to end and without pre training for unconditional language modeling child et al investigated scaling transformer by using sparse attention reducing complexity to p n kitaev et al replaced dot product attention by one that uses locality sensitive hashing changing its complexity to roy et al endowed self attention with a sparse routing module based on online k means while reducing its complexity to while dai et al grave et al cache state across attention frames to incorporate longer context pointer networks are supervised to copy input tokens to the output whereas in our tor scorer it is supervised to provide input to the subsequent abstractor layers shazeer et al gross et al used a gating network to save computation in very large networks our scorer gate mechanism also saves computation and memory by restricting attention to small subsets of the input data in our model the extraction can be interpreted similarly to hard attention which restricts what the downstream model layers decoder see hard attention has been designed into sequence models previously to increase the interpretability of predictions lei et al pre training sequence to sequence models using massive external data using a self supervised tive as in raffel et al song et al dong et al zhang et al has lead to improvements in downstream summarization tasks although this line of work is orthogonal to our focus on scaling the transformer to long inputs thus we train from randomly initialized weights sauper and barzilay generated wikipedia extractively from search results for a few categories liu et al augmented search results with cited references and generated articles focusing on lead section abstractively with sequence to sequence architectures our work differs from the latter by focusing on generating full articles and using only search results although many more vs models figure model architectures e d s s are encoder decoder and scorers that contains trainable parameters g is the gating function that selects and concatenates top scored snippet representations up to a certain maximum length xi are inputs snippet ids each xi is a sequence of ids i are encoded compressed representations of input snippets y ysegj are current decode ids all previous decode ids and previous decode ids in segment j in this gure there are in total inputs snippets and decoders always attend up to input representations the seal model is decoding the third segment figure losses and how gradients ow the left side are trunc and ca model the right side are ea model and seal model and are abstractive and extractive loss red arrows are gradients in this section we discuss architectures fig losses fig and attention maps fig of three general approaches and our proposed method to deal with lf nas truncated input section compressive abstractive section extractive abstractive section and seal section our proposed more general form of extractive abstractive model we encode the text using sub word tokenization similar to with a vocabulary size of each model generates an output sequence ids y from a sequence of input snippets ids section a snippet is dened as a continuous text span such as one or few sentences or a paragraph all our models components are based on transformer a transformer encoder maps an input sequence of tokens to a sequence of high dimensional vectors in ird using self attention and feed forward layers a ea seal trunc ca modeltransformer encodera input snippet encoded snippet decodertransformer scorergate next decode decode segment of previous decode trunc ea transformer decoder auto regressively by self attention generates an output sequence of tokens while attending to this input representation by encoder decoder cross attention unied approach for single and multi document summarization to unify our approach to sds and mds we break a input document or a list of documents into snippets with the following criteria concatenate tokens in continuous sentences until reaching a maximum snippet length lsnpt this helps to reduce the percentage of paddings empty tokens in each snippet for better compute efciency in the unlikely case of a single sentence exceeding the maximum length truncate that sentence ensure that snippets do not span across document boundaries order by their natural order of appearances within each document in mds they are ordered by the order in the data section stop adding snippets when their number reaches maximum snippets nsnpt figure illustration of encoder self attention and encoder decoder attention maps for four models considered are inputs snippets encoders inputs i are encoded compressed representations encoders outputs decoders inputs that correspond to input snippets ysj are decode segments decoder s outputs representing parts of the long decode sequence encoder self attentions from xi to i to ysj are colored in red note each square represents a sequence of tokens in a input snippet or a decode segment not a single token i are colored in blue and encoder decoder attentions from truncated input model trunc one limitation of a standard transformer model is it does not scale well to longer inputs due to the complexity of encoder self attention to input length we truncate the input sequences as in liu et al to a maximum input length linput only including the leading few snippets fig we refer this model as trunc encoder e and decoder d parameterized by and respectively as shown in fig the trunc model is trained using the standard teacher forcing and cross entropy loss over all generated tokens y y y is the target decode sequence and t is the length of decode the few snippets have full self attention among themselves and the decoder has full attention over the snippets fig compressive abstractive ca model the second approach compressive abstractive encodes and compresses continuous snippets into shorter representations and concatenates all representations as the decoder input fig where c is the transformer encoder that also compresses the input snippets the ca model is trained with a similar loss la as the trunc model fig amplayo and lapata pooled each input representation xld into a single vector xd l is the sequence length and d is the representation dimension whereas we compress a snippet group into a self dec short sequence of vectors xcd for richer representation c is the compressed size a snippet group is a block of continuous k snippets for sds or all snippets within the same document for mds the compression is implemented by concatenating learnable vectors to transformer encoders inputs and retrieving the processed vectors as compressed representations as shown in fig the compressed representation is derived from full self attention to snippets within the compression group and the decoder has full attention to all compressed representations extractive abstractive ea model the third approach extractive abstractive rst encodes each input snippet separately with e assigns scores to encoded snippets with scorer s a transformer encoder then selects encoded snippets by scores through gating function g the decoder attends to sequences selected by g fig each input snippet only has encoder self attention to itself the decoder has attention to selected snippets through the gating function fig the scorer s utilizes a transformer encoder to map a list of input snippets representations xld n to a list of scores l is the sequence length is the representation dimension and n is the number of snippets it consists of a attention pooling layer transformer encoder and a feed forward layer the attention pooling layer wang et al reduces snippets representations from xld n to xd i n the transformer encoder process the concatenation of pooled snippets xnd for contextual representation across snippets the feed forward layer assigns a scores for each contextual snippets representation xd i in mds we assign a document i d to each snippet and add learnable document embedding to the pooled snippet representation i i after each snippet is assigned a score by the scorer we apply a gating function g to select snippets based on their scores it is implemented in the following way sort the snippets by their predicted scores concatenate each snippet representation until their total length reaches a limit we refer to this length limit as maximum extractive length lext note concatenation is done by matrix multiplication of the sorting mask an one hot matrix mapping inputs positions to sorted concatenated positions and encoded snippets thus gradients can back propagate through the gating function to the encoder the encoder e scorer s and decoder d are jointly trained with two losses and the abstractive loss la is the same as trunc and ca model section the extractive loss provides supervision for the scorer and encoder to assign higher scores to better snippets for the decoder to attend to during training we calculate text similarities between gold summary and input snippets as weakly supervised proxy labels and minimize the distance between model predicted scores and proxy labels we minimize the sum of the two losses la and during training as shown in fig the la loss back propagates to d and e through g while the loss back propagates to s and e this differs from two stage models where the extractive and abstractive stages use different encoders and trained separately i n segment wise extractive abstractive long form seal model the seal model encodes snippets in the same way as the ea model on the decoder side the model divides the process into non overlapping segments each with size segment length lseg fig different snippets are selected for the decoder to attend to at each decode segment s where s is current decoding step s segment with size s starting index the inputs to the scorer s are all encoded snippets unchanged and prior decode segments changed at the start of each decode segment the gating function g selects subsets of snippets based on s assigned scores at each segment this model has the same self attention mask as the ea model and an encoder decoder attention that dynamically changes between decode segments as shown in fig the dynamic selection of snippets allows more efcient usage of attention memory more targeted proxy labels section and improved interpretability section segj k into representations ysd segj at the start of each decode segment the encoder e encodes the tokens of each k previous segment ys k is the number of previous decode segments the wise scorer consists of an attention pooling layer and a transformer encoder the attention pooling layer pools ysd k and concatenates them to ydk the transformer encoder s not only processes segj the pooled input snippets xnd by self attention same as in the ea model but also attends to ydk by encoder decoder cross attention such that the scorer is aware of what has been decoded refer to in bert tensorow the is pooled snippet representations the is pooled previous decode segments training the seal model is similar to the ea model fig except the number of supervised labels increases from n to n m where n is the number of input snippets and m is the number of decode segments the proxy labels are calculated as similarities between each gold summary segment and input snippet thus the extractive loss is j sij mn decoding segments are trained in parallel while attending to different inputs snippets i the seal model is a more general version of the ea and trunc models when lseg ldec the seal model reduces to an ea model when lext linput the ea model reduces to a trunc model datasets table statistics of the long text summarization tasks the lengths are calculated in the number of subword tokens word on average equals to subword input length mean target length mean arxiv pubmed m examples dataset several existing datasets are suitable as benchmarks for lf nas statistics in table cohan et al collected scientic publications from arxiv and pubmed and used articles bodies to generate abstracts to visualize selections by the seal model we use a popular relatively short summarization dataset cnn non anonymized version as in see et al for ease of presentation it contains newspaper articles paired with bullet point summaries liu et al approached generating the lead section of english wikipedia article as multi document summarization from reference documents wikipedia references and web search results and troduced the wikisum dataset to demonstrate the performance of our models on much longer input and target sequences we created a similar lf nas dataset named the dataset consists of full english wikipedia articles as the target summaries and a collection of search result documents about the topic as input documents the main differences are the input uses the top search result documents compared to the top in wikusum this is important to demonstrate the effectiveness of our proposed models on longer input sequences we drop wikipedia references which could vary a lot per page it allows generating pages for entities not currently in wikipedia to make the dataset more abstractive we apply a stronger ltering of wiki clone documents retrieved from search please refer to appendix b for more details the order of documents presented to model is by their search relevance in descending order experiments and results experimental details our transformer block has hidden size dmodel feed forward size df and attention heads we set number of transformer layers to in encoder in scorer and in decoder making the total number of parameters m for the seal model trunc ca and ea model have similar number of parameters all models are trained with adafactor com google research bert blob master modeling py optimizer batch size of dropout rate of learning rate of with square root decay we train the models until perplexity stopped decreasing on dev set steps for arxiv pubmed and steps for in order to have a clear comparison between models all our models are decoded with greedy decoding figure on the arxiv dataset trunc models trained on different maximum input length linput ea models trained on different maximum extractive length lext effect of segment length lseg and maximum extractive length lext for seal model on the arxiv dataset amount of input context we show that the amount of input context the decoder attends to is important for lf nas we limited the amount of input context to the trunc model to linput to and tokens on the arxiv dataset the performance of the model increases signicantly as the length grows fig suggesting longer input context is crucial for lf nas we observe similar trends on other lf nas datasets which is consistent with input snippets selection we show that the required amount of context the decoder attends to greatly reduces when better snippets are selected we trained ea models with maximum extractive length lext section of and tokens on the arxiv dataset with same number of tokens the ea model achieves much better results compared to the trunc model and plateaus at tokens fig methods of creating proxy extractive labels we investigated methods to automatically create proxy labels based on multiple text similarity measures including or precision recall scores and whether to create the labels sequentially as in nallapati et al appendix a shows that specic choice of labeling method does nt make large differences for simplicity we chose non sequential f in all of our models segment length and extractive length for the seal model when the segment length lseg is large the decoder effectively attends to less context on the other hand when lseg is as small as a few words the similarity labels become very spurious and are not meaningful to guide extraction fig shows that larger maximum extractive lengths lext are always better and is the optimal segment length for the arxiv datasets therefore in all other experiments we set the segment length to one eighth of maximum decode length lseg values of ldec can be found in appendix c on different dataset table comparison of our models with state of the art baselines the metrics are rl rouge l best numbers for each dataset are bolded there are two types of rouge l scores sentence level and summary level denotes the summary level denotes pretrained model model trucated input pegasus trunc ca ea sea y n n y y n n n arxiv rl pubmed rl rl input frougel extractive f comparison between our models and other works we compare trunc ca ea and seal models on arxiv pubmed and datasets in table the inputs to trunc models are leading snippets on arxiv pubmed datasets and top snippets ranked by tf idf on dataset similar to liu et al the inputs are concatenated and truncated to the rst tokens between our models the ca and ea models have similar performance and consistently outperform the trunc model on all datasets the seal model performs better than trunc ca and ea on long output datasets on which has larger number of training examples and longer input sequences the advantage of the seal model is more obvious prior work on lf nas typically truncates the input or target in some way truncating the changes the nature of the problem usually making it easier and leading to higher evaluation metrics in this work we ensure that the model s maximum input linput and decode ldec lengths appendix c exceed the percentile of the corresponding data distribution table so that we closely tackle the intended summarization problem dened by the dataset the seal model has better performances comparing to previous state of the art extractive and abstractive approaches on lf nas interpretability decode amir khan wants to ght kell brook in the next year brook has previously refused to ght brook but now promises it will take place within months khan has previously promised to ght brook in the ring input khan wants to ght kell brook within the next year paving the way for a spectacular return to wembley stadium for british boxing as recently as monday khan was talking rematch with george groves at wembley said i love to go in the ring and ght kell brook after watching his brook last performance i know i can do a deal winner takes all pictured celebrating his win over dan on saturday appears to have got his wish to ght khan has previously refused a ght with brook but now promises it will take place within months he added it s all about trust me i do plans promoter eddie hearn had of staging the ght on june he has a provisional booking with wembley are scuppered by khan s claim to have promised a ght to a different opponent sportsmail understands khan figure visualization of the seal model on an cnn dailymail example best viewed in color segments of decodes are colored differently input snippets each segment attends to are are colored accordingly and the segment ids are inserted to the front when multiple segment attend to the same input snippet it is colored as the rst segment the seal model provides a natural way to inspect what input sequence the decoder is likely using when generating a particular segment of the summary in this section we show how this enhances interpretability on an example from the cnn dailymail dataset for ease of presentation fig in the appendix we show more examples on long sds and mds datasets on cnn dailymail dataset we nd a seal model with lseg lsnpt lext nsnpt achieves of which is on par with a trunc model of input length when generating the rst decode segment the model attended to two snippets and copied from the summary like lead sentence the second segment nishes the rst sentence while continuously attending to the lead snippet and begins a second sentence rephrasing three other snippets the nal segment writes a new sentence combining multiple snippets conclusion in this work we studied and compared different lf nas models we showed that models formance heavily depends on seeing longer sequences from the input and sparsely selecting better content relieves this requirement we proposed the seal model which encodes input snippets separately and dynamically selects sparse input snippets to attend to when generating different segments of the summary the seal model achieves state of the art results on existing lf nas datasets including arxiv pubmed and outperform baseline models on our new much longer dataset broader impact this work may bring more attention to long document summarization research and spur new plications if successful producers of summaries from long multiple documents may benet from higher productivity due to less manual work while consumers may benet from reduced information overload a failure case of such a system is that it may generate text that is unfaithful to the source material i e factually inaccurate a risk that must be taken into account when deploying the models biases present in the training data may also be reected in the model output acknowledgments and disclosure of funding we thank david grangier for feedback and reviewing the manuscript and ben goodrich for helping with earlier iterations of the models references alexander m rush sumit chopra and jason weston a neural attention model for abstractive sentence summarization in emnlp ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang stractive text summarization using sequence to sequence rnns and beyond proceedings of the signll conference on computational natural language learning doi url ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural networks abigail see peter j liu and christopher d manning get to the point summarization with pointer generator networks proceedings of the annual meeting of the association for computational linguistics volume long papers url yang liu and mirella lapata text summarization with pretrained encoders proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing emnlp ijcnlp doi url sebastian gehrmann yuntian deng and alexander m rush bottom up abstractive rization in emnlp li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon unied language model pre training for natural language derstanding and generation in conference on neural information processing systems neurips arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian a discourse aware attention model for abstractive summarization of long documents in naacl hlt peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer generating wikipedia by summarizing long sequences sepp hochreiter and jrgen schmidhuber long short term memory neural comput november issn neco url neco junyoung chung caglar gulcehre kyunghyun cho and yoshua bengio empirical evaluation of gated recurrent neural networks on sequence modeling ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser and illia polosukhin attention is all you need ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network in proceedings of the based sequence model for extractive summarization of documents thirty first aaai conference on articial intelligence pages aaai press url acm org citation alec radford jeff wu rewon child david luan dario amodei and ilya sutskever language models are unsupervised multitask learners chenliang li weiran xu si li and sheng gao guiding generation for abstractive text summarization based on key information guide network in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume short papers pages new orleans louisiana june association for computational linguistics url aclweb org anthology sebastian gehrmann yuntian deng and alexander rush bottom up abstractive summarization in proceedings of the conference on empirical methods in natural language processing pages brussels belgium october november association for computational linguistics url aclweb org sandeep subramanian raymond li jonathan pilault and christopher pal on extractive and abstractive neural document summarization with transformer language models yang liu and mirella lapata hierarchical transformers for multi document summarization proceedings of the annual meeting of the association for computational linguistics url yen chun chen and mohit bansal fast abstractive summarization with reinforce selected sentence rewriting in proceedings of the annual meeting of the association for putational linguistics volume long papers pages melbourne australia july association for computational linguistics url aclweb org anthology ronald j williams simple statistical gradient following algorithms for connectionist forcement learning machine learning may issn doi url reinald kim amplayo and mirella lapata informative and controllable opinion summarization rewon child scott gray alec radford and ilya sutskever generating long sequences with sparse transformers url com blog sparse transformers nikita kitaev ukasz kaiser and anselm levskaya reformer the efcient transformer aurko roy mohammad saffar ashish vaswani and david grangier efcient content based sparse attention with routing transformers zihang dai zhilin yang yiming yang jaime carbonell quoc le and ruslan salakhutdinov transformer xl attentive language models beyond a xed length context proceedings of the annual meeting of the association for computational linguistics url edouard grave armand joulin and nicolas usunier improving neural language models with a continuous cache oriol vinyals meire fortunato and navdeep jaitly pointer networks in c cortes n d lawrence d d lee m sugiyama and r garnett editors advances in neural information processing systems pages curran associates inc url papers nips cc pointer networks pdf noam shazeer azalia mirhoseini krzysztof maziarz andy davis quoc le geoffrey hinton and jeff dean outrageously large neural networks the sparsely gated mixture of experts layer sam gross marcaurelio ranzato and arthur szlam hard mixtures of experts for large scale weakly supervised vision ieee conference on computer vision and pattern recognition cvpr jul cvpr url cvpr tao lei regina barzilay and tommi jaakkola rationalizing neural predictions in proceedings of the conference on empirical methods in natural language processing pages austin texas november association for computational linguistics url aclweb org anthology colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu exploring the limits of transfer learning with a unied text to text transformer arxiv e prints kaitao song xu tan tao qin jianfeng lu and tie yan liu mass masked sequence to sequence pre training for language generation in international conference on machine learning pages jingqing zhang yao zhao mohammad saleh and peter j liu pegasus pre training with extracted gap sentences for abstractive summarization christina sauper and regina barzilay automatically generating wikipedia articles a aware approach in proceedings of the joint conference of the annual meeting of the acl and the international joint conference on natural language processing of the afnlp pages suntec singapore august association for computational linguistics url aclweb org anthology yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin johnson xiaobing liu ukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes and jeffrey dean google s neural machine translation system bridging the gap between human and machine translation aurko roy and david grangier unsupervised paraphrasing without translation proceedings of the annual meeting of the association for computational linguistics doi url wenhui wang nan yang furu wei baobao chang and ming zhou gated self matching in proceedings of the networks for reading comprehension and question answering annual meeting of the association for computational linguistics volume long papers pages vancouver canada july association for computational linguistics doi url aclweb org anthology karl moritz hermann tom kocisk edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend noam shazeer and mitchell stern adafactor adaptive learning rates with sublinear memory cost chin yew lin rouge a package for automatic evaluation of summaries in text tion branches out pages barcelona spain july association for computational linguistics url aclweb org anthology ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based sequence model for extractive summarization of documents wen xiao and giuseppe carenini extractive summarization of long documents by combining global and local context a choice of text similarity measure self supervision labeling method ngram sequential false true type precision recall precision recall precision recall precision recall metric rl table comparison of ea models trained with different self supervised labeling method on arxiv dataset using the extractive abstractive model b clone detection in wikisum to detect whether a source document d is a clone of a wikipedia article a the maximum recall of unigrams between each section of a and d is computed as follows a max a clone is detected if a while this approach detects and lters most of the clones we observed many near clone documents left undetected in the wikisum dataset most of these near clones are documents that copy small parts of the wikipedia article rather than the whole article or a whole section to lter these near clones more effectively and make the dataset more abstractive we extended the equation above to a maximum recall of n grams between each section of a and as follows a n max n we experimented with different values of n a near clone is detected in if a c model dimensions dataset arxiv pubmed linput lsnpt nsnpt lext ldec lseg nseg table dimensions of the models inputs and outputs for ca ea and seal length are all in unit of subword tokens linput is the maximum input length lsnpt is the snippet length nsnpt is the maximum number of snippets linput lsnpt nsnpt lext is the maximum extractive length ldec is the maximum decode length for the seal models lseg is the decode segment length and nseg is the maximum number of segments ldec lseg nseg
