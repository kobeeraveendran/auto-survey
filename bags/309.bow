n u j l c s c v v x r seal segment wise extractive abstractive long form text summarization yao zhao google research brain team com mohammad saleh google research brain team com peter j liu google research brain team com abstract prior work sequence sequence paradigm focused datasets sequence lengths hundreds tokens computational constraints common rnn transformer architectures paper study long form abstractive text summarization sequence sequence setting input sequence lengths tokens output sequence lengths tokens propose seal transformer based model featuring new encoder decoder tention dynamically extracts selects input snippets sparsely attend output segment original documents summaries derive proxy labels provide weak supervision extractive layers simultaneously regular supervision abstractive summaries seal model achieves state art results existing long form summarization tasks outperforms strong baseline models new dataset task introduce longer input text content selection explicit seal model desirable effect selection inspected enhanced interpretability introduction text summarization language generation task seeks output concise informative content given possibly multiple input documents abstractive summarization aims summarize text solely copying text segments usually sentences novel words phrases concise comprehensive achieve certain style neural abstractive summarization data driven summarization approach trains sequence sequence models large numbers document summary pairs demonstrated promising results summarizing relatively short texts input documents typically contain document tokens summary tokens sentences paragraph despite progress relatively short form settings development long form neural abstractive summarization lf nas constrained lack large scale long form datasets recently increasing interest collecting datasets single document sds including scientic articles arxiv pubmed multi document summarization mds challenge remains developing model architectures cope increased data scale particular sequence length prior work lf nas typically divided task separate stages mainly result memory computation constraints associated processing long input sequences rnn transformer models rst extractive stage selects small fraction input text second abstractive stage typically relies avor sequence sequence model process extracted inputs summaries work systematically study single models lf nas trained jointly weak supervision extractive layers regular supervision abstractive layers use single approach preprint review sds mds break input documents sequences snippets section consider general categories modeling approach rst truncates inputs leading snippets section second compressive abstractive compresses snippets xed length representation input abstractive attention layers section extractive abstractive encodes snippet independently sparsely selects snippets applying abstractive attention layers section sparse selection performed scorer sub network gating function scorer weakly supervised proxy extractive labels automatically calculated input snippets gold summaries similar nallapati et al abstractive attention layers employ encoder decoder attention resulting input representations self attention supervised abstractive summaries training extractive abstractive losses added optimized propose segment wise extractive abstractive long form model section seal generalizes improves extractive abstractive approach augmenting scorer inputs previously decoded tokens segment allowing model dynamically select input snippets decoding interpretability goal nice effect model shows link selected input snippets decoded segments resulting better interpretability soft attention models able apply models long input documents achieve state art results seal model arxiv pubmed datasets demonstrate advantage seal model approaches collected massive new dataset use generate wikipedia pages search results input tokens related work sequence sequence dominant paradigm abstractive summarization decoder architectures based rnns recently transformer transformers shown effective incorporating long term dependencies rnns issues scaling running computation memory limitations quickly sequence tokens sequence length n option simply truncate sequences depending task drop important information option factor summarization problem extractive abstractive stages extract abstract framework received increased attention recently natural way reduce input size abstractive sequence sequence models past work abstractive model trained separately extractive stage chen bansal trained extractor abstractor sub modules separately reinforce ne tune non differentiable extractive selection end end reinforce slows training signicantly scalable setting amplayo lapata proposed encoding documents encoder derived pre trained autoencoder followed pooling layer processing separately trained decoder condense abstract proposed seal model baseline models loosely t extract abstract abstract frameworks trained jointly single model end end pre training unconditional language modeling child et al investigated scaling transformer sparse attention reducing complexity p n kitaev et al replaced dot product attention uses locality sensitive hashing changing complexity roy et al endowed self attention sparse routing module based online k means reducing complexity dai et al grave et al cache state attention frames incorporate longer context pointer networks supervised copy input tokens output tor scorer supervised provide input subsequent abstractor layers shazeer et al gross et al gating network save computation large networks scorer gate mechanism saves computation memory restricting attention small subsets input data model extraction interpreted similarly hard attention restricts downstream model layers decoder hard attention designed sequence models previously increase interpretability predictions lei et al pre training sequence sequence models massive external data self supervised tive raffel et al song et al dong et al zhang et al lead improvements downstream summarization tasks line work orthogonal focus scaling transformer long inputs train randomly initialized weights sauper barzilay generated wikipedia extractively search results categories liu et al augmented search results cited references generated articles focusing lead section abstractively sequence sequence architectures work differs focusing generating articles search results vs models figure model architectures e d s s encoder decoder scorers contains trainable parameters g gating function selects concatenates scored snippet representations certain maximum length xi inputs snippet ids xi sequence ids encoded compressed representations input snippets y ysegj current decode ids previous decode ids previous decode ids segment j gure total inputs snippets decoders attend input representations seal model decoding segment figure losses gradients ow left trunc model right ea model seal model abstractive extractive loss red arrows gradients section discuss architectures fig losses fig attention maps fig general approaches proposed method deal lf nas truncated input section compressive abstractive section extractive abstractive section seal section proposed general form extractive abstractive model encode text sub word tokenization similar vocabulary size model generates output sequence ids y sequence input snippets ids section snippet dened continuous text span sentences paragraph models components based transformer transformer encoder maps input sequence tokens sequence high dimensional vectors ird self attention feed forward layers ea seal trunc modeltransformer encodera input snippet encoded snippet decodertransformer scorergate decode decode segment previous decode trunc ea transformer decoder auto regressively self attention generates output sequence tokens attending input representation encoder decoder cross attention unied approach single multi document summarization unify approach sds mds break input document list documents snippets following criteria concatenate tokens continuous sentences reaching maximum snippet length lsnpt helps reduce percentage paddings tokens snippet better compute efciency unlikely case single sentence exceeding maximum length truncate sentence ensure snippets span document boundaries order natural order appearances document mds ordered order data section stop adding snippets number reaches maximum snippets nsnpt figure illustration encoder self attention encoder decoder attention maps models considered inputs snippets encoders inputs encoded compressed representations encoders outputs decoders inputs correspond input snippets ysj decode segments decoder s outputs representing parts long decode sequence encoder self attentions xi ysj colored red note square represents sequence tokens input snippet decode segment single token colored blue encoder decoder attentions truncated input model trunc limitation standard transformer model scale longer inputs complexity encoder self attention input length truncate input sequences liu et al maximum input length linput including leading snippets fig refer model trunc encoder e decoder d parameterized respectively shown fig trunc model trained standard teacher forcing cross entropy loss generated tokens y y y target decode sequence t length decode snippets self attention decoder attention snippets fig compressive abstractive model second approach compressive abstractive encodes compresses continuous snippets shorter representations concatenates representations decoder input fig c transformer encoder compresses input snippets model trained similar loss la trunc model fig amplayo lapata pooled input representation xld single vector xd l sequence length d representation dimension compress snippet group self dec short sequence vectors xcd richer representation c compressed size snippet group block continuous k snippets sds snippets document mds compression implemented concatenating learnable vectors transformer encoders inputs retrieving processed vectors compressed representations shown fig compressed representation derived self attention snippets compression group decoder attention compressed representations extractive abstractive ea model approach extractive abstractive rst encodes input snippet separately e assigns scores encoded snippets scorer s transformer encoder selects encoded snippets scores gating function g decoder attends sequences selected g fig input snippet encoder self attention decoder attention selected snippets gating function fig scorer s utilizes transformer encoder map list input snippets representations xld n list scores l sequence length representation dimension n number snippets consists attention pooling layer transformer encoder feed forward layer attention pooling layer wang et al reduces snippets representations xld n xd n transformer encoder process concatenation pooled snippets xnd contextual representation snippets feed forward layer assigns scores contextual snippets representation xd mds assign document d snippet add learnable document embedding pooled snippet representation snippet assigned score scorer apply gating function g select snippets based scores implemented following way sort snippets predicted scores concatenate snippet representation total length reaches limit refer length limit maximum extractive length lext note concatenation matrix multiplication sorting mask hot matrix mapping inputs positions sorted concatenated positions encoded snippets gradients propagate gating function encoder encoder e scorer s decoder d jointly trained losses abstractive loss la trunc model section extractive loss provides supervision scorer encoder assign higher scores better snippets decoder attend training calculate text similarities gold summary input snippets weakly supervised proxy labels minimize distance model predicted scores proxy labels minimize sum losses la training shown fig la loss propagates d e g loss propagates s e differs stage models extractive abstractive stages use different encoders trained separately n segment wise extractive abstractive long form seal model seal model encodes snippets way ea model decoder model divides process non overlapping segments size segment length lseg fig different snippets selected decoder attend decode segment s s current decoding step s segment size s starting index inputs scorer s encoded snippets unchanged prior decode segments changed start decode segment gating function g selects subsets snippets based s assigned scores segment model self attention mask ea model encoder decoder attention dynamically changes decode segments shown fig dynamic selection snippets allows efcient usage attention memory targeted proxy labels section improved interpretability section segj k representations ysd segj start decode segment encoder e encodes tokens k previous segment ys k number previous decode segments wise scorer consists attention pooling layer transformer encoder attention pooling layer pools ysd k concatenates ydk transformer encoder s processes segj pooled input snippets xnd self attention ea model attends ydk encoder decoder cross attention scorer aware decoded refer bert tensorow pooled snippet representations pooled previous decode segments training seal model similar ea model fig number supervised labels increases n n m n number input snippets m number decode segments proxy labels calculated similarities gold summary segment input snippet extractive loss j sij mn decoding segments trained parallel attending different inputs snippets seal model general version ea trunc models lseg ldec seal model reduces ea model lext linput ea model reduces trunc model datasets table statistics long text summarization tasks lengths calculated number subword tokens word average equals subword input length mean target length mean arxiv pubmed m examples dataset existing datasets suitable benchmarks lf nas statistics table cohan et al collected scientic publications arxiv pubmed articles bodies generate abstracts visualize selections seal model use popular relatively short summarization dataset cnn non anonymized version et al ease presentation contains newspaper articles paired bullet point summaries liu et al approached generating lead section english wikipedia article multi document summarization reference documents wikipedia references web search results troduced wikisum dataset demonstrate performance models longer input target sequences created similar lf nas dataset named dataset consists english wikipedia articles target summaries collection search result documents topic input documents main differences input uses search result documents compared wikusum important demonstrate effectiveness proposed models longer input sequences drop wikipedia references vary lot page allows generating pages entities currently wikipedia dataset abstractive apply stronger ltering wiki clone documents retrieved search refer appendix b details order documents presented model search relevance descending order experiments results experimental details transformer block hidden size dmodel feed forward size df attention heads set number transformer layers encoder scorer decoder making total number parameters m seal model trunc ea model similar number parameters models trained adafactor com google research bert blob master modeling py optimizer batch size dropout rate learning rate square root decay train models perplexity stopped decreasing dev set steps arxiv pubmed steps order clear comparison models models decoded greedy decoding figure arxiv dataset trunc models trained different maximum input length linput ea models trained different maximum extractive length lext effect segment length lseg maximum extractive length lext seal model arxiv dataset input context input context decoder attends important lf nas limited input context trunc model linput tokens arxiv dataset performance model increases signicantly length grows fig suggesting longer input context crucial lf nas observe similar trends lf nas datasets consistent input snippets selection required context decoder attends greatly reduces better snippets selected trained ea models maximum extractive length lext section tokens arxiv dataset number tokens ea model achieves better results compared trunc model plateaus tokens fig methods creating proxy extractive labels investigated methods automatically create proxy labels based multiple text similarity measures including precision recall scores create labels sequentially nallapati et al appendix shows specic choice labeling method nt large differences simplicity chose non sequential f models segment length extractive length seal model segment length lseg large decoder effectively attends context hand lseg small words similarity labels spurious meaningful guide extraction fig shows larger maximum extractive lengths lext better optimal segment length arxiv datasets experiments set segment length eighth maximum decode length lseg values ldec found appendix c different dataset table comparison models state art baselines metrics rl rouge l best numbers dataset bolded types rouge l scores sentence level summary level denotes summary level denotes pretrained model model trucated input pegasus trunc ea sea y n n y y n n n arxiv rl pubmed rl rl input frougel extractive f comparison models works compare trunc ea seal models arxiv pubmed datasets table inputs trunc models leading snippets arxiv pubmed datasets snippets ranked tf idf dataset similar liu et al inputs concatenated truncated rst tokens models ea models similar performance consistently outperform trunc model datasets seal model performs better trunc ea long output datasets larger number training examples longer input sequences advantage seal model obvious prior work lf nas typically truncates input target way truncating changes nature problem usually making easier leading higher evaluation metrics work ensure model s maximum input linput decode ldec lengths appendix c exceed percentile corresponding data distribution table closely tackle intended summarization problem dened dataset seal model better performances comparing previous state art extractive abstractive approaches lf nas interpretability decode amir khan wants ght kell brook year brook previously refused ght brook promises place months khan previously promised ght brook ring input khan wants ght kell brook year paving way spectacular return wembley stadium british boxing recently monday khan talking rematch george groves wembley said love ring ght kell brook watching brook performance know deal winner takes pictured celebrating win dan saturday appears got wish ght khan previously refused ght brook promises place months added s trust plans promoter eddie hearn staging ght june provisional booking wembley scuppered khan s claim promised ght different opponent sportsmail understands khan figure visualization seal model cnn dailymail example best viewed color segments decodes colored differently input snippets segment attends colored accordingly segment ids inserted multiple segment attend input snippet colored rst segment seal model provides natural way inspect input sequence decoder likely generating particular segment summary section enhances interpretability example cnn dailymail dataset ease presentation fig appendix examples long sds mds datasets cnn dailymail dataset nd seal model lseg lsnpt lext nsnpt achieves par trunc model input length generating rst decode segment model attended snippets copied summary like lead sentence second segment nishes rst sentence continuously attending lead snippet begins second sentence rephrasing snippets nal segment writes new sentence combining multiple snippets conclusion work studied compared different lf nas models showed models formance heavily depends seeing longer sequences input sparsely selecting better content relieves requirement proposed seal model encodes input snippets separately dynamically selects sparse input snippets attend generating different segments summary seal model achieves state art results existing lf nas datasets including arxiv pubmed outperform baseline models new longer dataset broader impact work bring attention long document summarization research spur new plications successful producers summaries long multiple documents benet higher productivity manual work consumers benet reduced information overload failure case system generate text unfaithful source material e factually inaccurate risk taken account deploying models biases present training data reected model output acknowledgments disclosure funding thank david grangier feedback reviewing manuscript ben goodrich helping earlier iterations models references alexander m rush sumit chopra jason weston neural attention model abstractive sentence summarization emnlp ramesh nallapati bowen zhou cicero dos santos caglar gulcehre bing xiang stractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning doi url ilya sutskever oriol vinyals quoc v le sequence sequence learning neural networks abigail peter j liu christopher d manning point summarization pointer generator networks proceedings annual meeting association computational linguistics volume long papers url yang liu mirella lapata text summarization pretrained encoders proceedings conference empirical methods natural language processing international joint conference natural language processing emnlp ijcnlp doi url sebastian gehrmann yuntian deng alexander m rush abstractive rization emnlp li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou hsiao wuen hon unied language model pre training natural language derstanding generation conference neural information processing systems neurips arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang nazli goharian discourse aware attention model abstractive summarization long documents naacl hlt peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser noam shazeer generating wikipedia summarizing long sequences sepp hochreiter jrgen schmidhuber long short term memory neural comput november issn neco url neco junyoung chung caglar gulcehre kyunghyun cho yoshua bengio empirical evaluation gated recurrent neural networks sequence modeling ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser illia polosukhin attention need ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network proceedings based sequence model extractive summarization documents thirty aaai conference articial intelligence pages aaai press url acm org citation alec radford jeff wu rewon child david luan dario amodei ilya sutskever language models unsupervised multitask learners chenliang li weiran xu si li sheng gao guiding generation abstractive text summarization based key information guide network proceedings conference north american chapter association computational linguistics human language technologies volume short papers pages new orleans louisiana june association computational linguistics url aclweb org anthology sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference empirical methods natural language processing pages brussels belgium october november association computational linguistics url aclweb org sandeep subramanian raymond li jonathan pilault christopher pal extractive abstractive neural document summarization transformer language models yang liu mirella lapata hierarchical transformers multi document summarization proceedings annual meeting association computational linguistics url yen chun chen mohit bansal fast abstractive summarization reinforce selected sentence rewriting proceedings annual meeting association putational linguistics volume long papers pages melbourne australia july association computational linguistics url aclweb org anthology ronald j williams simple statistical gradient following algorithms connectionist forcement learning machine learning issn doi url reinald kim amplayo mirella lapata informative controllable opinion summarization rewon child scott gray alec radford ilya sutskever generating long sequences sparse transformers url com blog sparse transformers nikita kitaev ukasz kaiser anselm levskaya reformer efcient transformer aurko roy mohammad saffar ashish vaswani david grangier efcient content based sparse attention routing transformers zihang dai zhilin yang yiming yang jaime carbonell quoc le ruslan salakhutdinov transformer xl attentive language models xed length context proceedings annual meeting association computational linguistics url edouard grave armand joulin nicolas usunier improving neural language models continuous cache oriol vinyals meire fortunato navdeep jaitly pointer networks c cortes n d lawrence d d lee m sugiyama r garnett editors advances neural information processing systems pages curran associates inc url papers nips cc pointer networks pdf noam shazeer azalia mirhoseini krzysztof maziarz andy davis quoc le geoffrey hinton jeff dean outrageously large neural networks sparsely gated mixture experts layer sam gross marcaurelio ranzato arthur szlam hard mixtures experts large scale weakly supervised vision ieee conference computer vision pattern recognition cvpr jul cvpr url cvpr tao lei regina barzilay tommi jaakkola rationalizing neural predictions proceedings conference empirical methods natural language processing pages austin texas november association computational linguistics url aclweb org anthology colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li peter j liu exploring limits transfer learning unied text text transformer arxiv e prints kaitao song xu tan tao qin jianfeng lu tie yan liu mass masked sequence sequence pre training language generation international conference machine learning pages jingqing zhang yao zhao mohammad saleh peter j liu pegasus pre training extracted gap sentences abstractive summarization christina sauper regina barzilay automatically generating wikipedia articles aware approach proceedings joint conference annual meeting acl international joint conference natural language processing afnlp pages suntec singapore august association computational linguistics url aclweb org anthology yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin johnson xiaobing liu ukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes jeffrey dean google s neural machine translation system bridging gap human machine translation aurko roy david grangier unsupervised paraphrasing translation proceedings annual meeting association computational linguistics doi url wenhui wang nan yang furu wei baobao chang ming zhou gated self matching proceedings networks reading comprehension question answering annual meeting association computational linguistics volume long papers pages vancouver canada july association computational linguistics doi url aclweb org anthology karl moritz hermann tom kocisk edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend noam shazeer mitchell stern adafactor adaptive learning rates sublinear memory cost chin yew lin rouge package automatic evaluation summaries text tion branches pages barcelona spain july association computational linguistics url aclweb org anthology ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents wen xiao giuseppe carenini extractive summarization long documents combining global local context choice text similarity measure self supervision labeling method ngram sequential false true type precision recall precision recall precision recall precision recall metric rl table comparison ea models trained different self supervised labeling method arxiv dataset extractive abstractive model b clone detection wikisum detect source document d clone wikipedia article maximum recall unigrams section d computed follows max clone detected approach detects lters clones observed near clone documents left undetected wikisum dataset near clones documents copy small parts wikipedia article article section lter near clones effectively dataset abstractive extended equation maximum recall n grams section follows n max n experimented different values n near clone detected c model dimensions dataset arxiv pubmed linput lsnpt nsnpt lext ldec lseg nseg table dimensions models inputs outputs ea seal length unit subword tokens linput maximum input length lsnpt snippet length nsnpt maximum number snippets linput lsnpt nsnpt lext maximum extractive length ldec maximum decode length seal models lseg decode segment length nseg maximum number segments ldec lseg nseg
