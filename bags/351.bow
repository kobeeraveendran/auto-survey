t c o l c s c v v i x r a learning to summarize long texts with ory compression and transfer jaehong park jonathan and christopher ai montreal of montreal cifar ai chair firstname com abstract we introduce a memory to memory mechanism for hierarchical current neural network based encoder decoder architectures and we explore its use for abstractive document summarization transfers memories via readable writable external memory modules that augment both the encoder and decoder our memory regularization compresses an encoded input article into a more compact set of sentence representations most importantly the ory compression step performs implicit extraction without labels sidestepping issues with suboptimal ground truth data and exposure bias of hybrid abstractive summarization techniques by allowing the decoder to read write over the encoded input memory the model learns to read salient information about the input article while keeping track of what has been generated our approach yields results that are competitive with state of the art transformer based summarization methods but with times fewer parameters introduction automatic summarization is the automated process of reducing the size of an input text while serving its most relevant information content and its core semantics techniques for summarization are often characterized as being either extractive or abstractive extractive methods construct summaries by combining the most salient passages usually whole sentences of a source text a process similar to human way of identifying the right information one way to achieve tive summarization is to dene the problem as a sentence classication task using some form of representation of the sentences in a document nallapati et al cheng lapata to avoid content overlap issues previous work has used sentence reranking narayan et al or sentence ordering by extracting sentences recurrently chen bansal abstractive methods generate summaries by generating new sentence constructs from scratch or from representation of document content a process that is conceptually more similar to the notion of paraphrasing stractive text summarization has attracted interest since it is capable of generating novel formulations of summaries using language generation models conditioned on the source text several based recurrent neural network rnn encoder decoders have been introduced to tackle varying text generation issues of standalone abstractive sequence to sequence models copy and pointer mechanisms gu et al tu et al vinyals et al see et al for ample have enabled decoders to better generate unseen words out of vocabulary words and named entities most recently hybrid extractive and abstractive architectures have been proposed and have shown promising results in both quantitative performance measures and human evaluations in such ups the extractive model rst selects salient sentences from a source article and the abstractive model paraphrases the extracted sentences into a nal summary the majority of current state the art abstractive summarization are based on the hybrid approach chen bansal gehrmann et al hsu et al bae et al subramanian et al nonetheless authors contributed equally to this work summarization models using large scale pre trained language models such as bert devlin et al hybrid models can be limited by three disadvantages first since ground truth labels for extractive summarization are usually not provided extractive labels must be generated by a potentially mal algorithm hsu et al subramanian et al the performance of models trained with such labels is therefore bounded by the quality of the performance of the extractive heuristics ond since ground truth binary labels for recurrently extracted sentences are typically teacher forced as in chen bansal exposure bias ranzato et al may negatively affect content selection performance at inference finally given that the hard extraction step is not differentiable existing hybrid models typically require multi step training not end to end gehrmann et al subramanian et al or reinforcement learning chen bansal bae et al to train the whole model in this paper we introduce a novel abstractive summarization model that incorporates an ate extractive step but does not require labels for this type of extractive content selection and it is fully end to end trainable to achieve this we propose a new memory augmented encoder decoder maed architecture wang et al yogatama et al ma principe le et al called has memorization modes absorb key information of the encoded source sequence via a compression mechanism and sequentially update the ternal memory during target summary generation without using extractive ground truth labels we nd in our analysis that s compression mechanism behaves as an implicit sentence extractor that stores sentence representations of the salient content the choice of sentence tations is only guided by the memory regularization and conditional language modeling loss of the decoder thus avoiding exposure bias from maximizing the likelihood of sequential binary tion labels finally the encoded memory is transferred to the decoder memory which is iteratively rened during the decoding process to our knowledge is the rst abstractive rization model that uses memory compression for sentence extraction and that directly employs the memorized representations during summary generation we empirically demonstrate the merits of this approach by setting a new state of the art on long text abstractive summarization tasks on the pubmed arxiv and newsroom datasets cohan et al grusky et al our contributions are three fold we introduce the approach that i stores salient sentence level representations via memory compression ii transfers memory from encoder to decoder and iii updates the memory as the summary generation proceeds our method yields results that are competitive with state of the art unlike previous works our model combines the best of extractive and abstractive rization in a fully end to end trainable manner without supervision on the extraction step transformer vaswani et al language model based techniques but with substantially fewer rameters only of the parameters used by a recent state of the art transformer based method background for our baseline we use a hierarchical recurrent encoder decoder structure hred based model from nallapati et al and cohan et al the hred has two encoder grus a sentence encoder and a document encoder given an input sentence of length n the sentence encoder takes a sequence of token embeddings and transforms it into a sequence of hidden states n xn the last hidden state of the sentence encoder is used as a corresponding sentence embedding s the sequence of sentence embeddings s are then processed by the document encoder l sl where sj is the j th sentence embedding j l is the total number of sentences in the document is the associated document encoder hidden state and the decoder gru generates a target summary one token at a time at each decoding step t the decoder creates a decoder hidden state the decoder then obtains a context vector ct via ti t figure architecture sentence level representations from the document encoder are reduced to a sized encoder memory me the encoder memory is transferred to the decoder memory md and is read using a ram like mechanism the resulting memory readout vector is used to condition sentence and word level attention the decoder hidden states and the memory readout vector are then used to update the memory state md via a gated write operation zi and an alignment between t sentence level attention such that i ti is computed by combining token level attention and ti t i tj t j ti nd p where denotes the index of the sentence corresponding to the l th word and nd is the total number of tokens in the input document attn in equation is dened as in luong et al the probability distribution of the next target word yt is estimated using the decoder hidden state and the context vector the training objective l is the average negative log likelihood of the t target word yt over the whole ground truth summary of length t finally the pointer generator and decoder coverage method in see et al are used for the baseline hred more details of the baseline architecture can be found in appendix memory to memory mechanism has three main features an encoder memory bank that compresses a large set of encoder hidden representations into a smaller subset of salient representations read write operations that allow the decoder to read and update the encoder memory bank with new information during summary generation token generation that is conditioned on the extracted sentence representations accessed from the dynamic memory the components can be seamlessly integrated with existing hred architectures in essence the whole process can be seen as extraction followed by generation the architecture is depicted in figure memory compression on encoder the aim of having an external memory on the encoder is to create a xed size representation that reduces the set of l hidden representations from grudoc to a subset of r representations from a quence of sentence level document encoder hidden representations l we construct an intermediate d matrix h rld where is the document encoder hidden size h t l i a smaller sized memory bank is generated by taking a linear combination of each row in h the weight vector for the linear combination a is computed with self attention mechanism a where rda rdad and da is the size of the hidden layer which is a hyperparameter to capture various aspects of the input document a is extended to a multi head memory write attention matrix a with r heads a where rrda and r is a hyperparameter this results in r different convex combinations of h which gives us the nal multi head encoder memory matrix me rrd me ah to ensure the attention weights in various heads focus on a diverse set of salient input sentences we propose a novel regularization loss for memory compression the following regularization term encourages the diversity of compressed encoded states where is the frobenius norm the regularization loss achieves two goals neously it promotes diversity over the r sentence representations stored in the memory thereby reducing the risk of redundant information it hardens the attention probabilities of each head assuring that each memory slot is approximately associated to a single sentence representation as a result the encoder memory me essentially performs implicit extraction over the encoder hidden states in a fully differentiable manner figure in appendix shows the effect of regularization on the encoder memory compression note that no supervision exists on this extractive step and the ory compression is only guided by the memory regularization and back propagated error signals from the target summary generation read write operations on decoder once the encoder memory is constructed the context read from the memory is used to augment both attention mechanism and the target token generation as a rst step the encoder memory me is transferred to the decoder and used as an initial state of the decoder memory md at every time step t the decoder reads from the memory and generates a memory read vector mt specically the decoder takes the weighted sum over r memory slots via a ram like attention mechanism tk t mt r x where is the vector representation of the head or slot of md is then combined with mt and generate a memory augmented decoder hidden state and the t next token estimation of the baseline system is replaced with as a consequence the attention over the source text and the prediction of the target token are conditioned on the memory read mt thus there is a direct link between the contents of the memory and the text generation t t during the summary generation process the semantics of the source sequence that is kept in the decoder memory needs to be modied the memory write operation of enables the memory to log the history of what has been attended and generated the decoder memory write operation outlined below removes and adds information using a gated mechanism to forget used memories and update each memory slot the gating mechanism is conditioned on the the memory content md the memory read vector mt and the decoder hidden state t zk t mt zk t zk t t although text generation is directly conditioned on the memory context mt the benet of ing memory was limited in our preliminary experiments we observed only a few memory slots table results on the pubmed arxiv and newsroom dataset each type corresponds to purely abstractive a extractive e or extractive abstractive hybrid h approaches tlm uses the model radford et al that has times larger parameters than the highest rouge scores for abstractive methods are boldfaced all rouge scores have a condence interval of at most as reported by the ofcial rouge results taken from et al and et al newsroom abstractive summarization test set results model type sent sent attn ptr gen discourse ours e e e a a a a h a of params pubmed m m m m l arxiv newsroom were repetitively attended during decoding to ensure that the decoder fully utilizes its memory to improve generation we propose another regularization term t t j t t ls x t t x r x where is the vector representation of the kth memory head of me and is a sentence level context vector the regularization assumes that if the memory context mt which is a level representation is combined properly within the decoding the sentence level context vector t would correlate with it the initial state of the decoder memory me is used to compute since t the representation of md deviates from the original representation space of due to the write j operation the nal training objective of becomes as follows t l where the weights for regularization and are hyperparameters related work recent works in abstractive summarization have leveraged intermediate content selection in these approaches writing a summary is factorized into two steps extraction and generation an extractor is used to prioritize and select the most important part of the input text the extractor is normally trained on a sequence of binary labels where each label indicates whether the corresponding text unit should be selected or not the level of extraction can be word level gehrmann et al cho et al or sentence level chen bansal hsu et al bae et al subramanian et al as the ground truth for extraction is typically missing heuristics that measure n gram overlap with the target summary are used to build extractive oracles similar to other approaches performs sentence level extraction to deal with long source determines the alignment between source and target sentences in a latent space without relying on possibly suboptimal extractive heuristics in addition sentence extraction is not sequentially done in which addresses the exposure bias issue ranzato et al memory augmented encoder decoder maed architectures wang et al yogatama et al ma principe le et al have been proposed for conditional natural language preliminary experiments we applied word level selection on the pubmed and arxiv datasets which led to poor results table model ablation study on the pubmed dataset model baseline hred encoder mem decoder mem mem transfer reg reg rouge l generation tasks such as machine translation kaiser et al and image captioning park et al using differentiable read and write operations to an external module maed can represent non local context of rnns with enhanced memory capacity such models are able to store rally distant information of large input sequences a feature that is particularly useful for long text summarization in the context of short text abstractive summarization kim et al proposed a memory architecture named multi level memory networks mmn mmn can exibly reduce sentations at different levels of document hierarchy into a xed size external memory authors used multi layer dilated convolutional neural networks cnn yu koltun van den oord et al to build a hierarchical representation of the document also constructs memory from the hierarchical representation of the document but by compressing it into a sparse set of sentence representations further mmn s memory representations remain static throughout the decoding process while dynamically updates its memory which is more effective in learning long term dependency lastly but not least our work proposes novel regularization for memory read and compression results and discussion experiment setup we evaluate on the pubmed arxiv cohan et al and newsroom abstractive datasets grusky et al which are large scale summarization datasets the average lengths of source articles and target summaries are pubmed arxiv and room respectively they are up to times longer than the widely used cnn dailymail dataset hermann et al see et al our pre processing and training setups are tical to cohan et al and subramanian et al more details on training and evaluation can be found in appendix for quantitative evaluation we use the rouge metric lin and report rouge scores results table shows the rouge scores on three summarization datasets the hybrid type h refers to models that use two step extractive abstractive summarization on the pubmed dataset the tlm model shows the highest scores in and r l is close to those scores and shows higher scores with times less parameters m vs m it achieves such performance over the tlm model without ground truth labels for sentence extraction we also reiterate that is trained completely end to end whereas the hybrid tlm requires separate training for the extractor and the conditional transformer language model we nd similar results on the arxiv and newsroom datasets on the arxiv dataset even surpasses the transformer based tlm model in and scores also shows competitive results on the newsroom abstractive dataset ablation study to assess the importance of components we conduct an ablation study on the pubmed dataset table shows the effects of adding different add ons on rouge scores mem is the baseline hred augmented with the encoder memory described in section table rouge scores of unsupervised extractive methods on the pubmed and the arxiv dataset the result of baseline extractive methods is from subramanian et al data pubmed arxiv model lexrank gold ext lexrank gold ext rouge l s s s s s s s s s s s s s s s s t t t t t t t t t t t t t t t t t t t t decoding timstep a multi head memory write attention matrix a memory read attention matrix figure multi head memory write attention matrix a and memory read attention matrix in a rows denote memory heads or slots and columns indicate input sentence indices in b columns indicate decoding time steps the result demonstrates that memory context indeed enhances the performance measures of the generated summaries decoder mem adds the write operation to the memory but without memory transfer in this case the decoder memory md is initialized with zeros not with the encoder memory me compared to the baseline hred the result shows that the write mechanism on the decoder memory helps the generation even without the memory transfer this indicates that the summary writing process largely benets from accessing long term contextual information of the output text transferring memory mem transfer brings substantial improvements in rouge scores it seems to be crucial to initiate the summary generation from the selected memory representations showing the importance of the memory compression and transfer steps furthermore it can be observed that discouraging redundancies over the encoder memory head via leads to additional ments on all rouge scores finally adding another regularization on the decoder memory read operation completes the architecture and achieves the best rouge scores implicit extraction via memory compression our initial hypothesis was that the encoder memory me would pick a set of the most salient input sentences for summarization to conrm we analyze the quality of the extractive summarization by memory compression concretely we concatenate the sentences with the highest attention weight of each memory head to generate a summary table shows the rouge scores of different pervised extractive summarization methods on the pubmed and arxiv datasets the extractive marization performed by the s memory compression outperforms existing unsupervised extractive summarization baselines the result indicates that s memory compression is able to prioritize amongst a large set of input sentences without ground truth sentence labels table percentage ratios of output summary n grams found in the pubmed original input article table results of human evaluation maximum score for each criterion is models reference baseline hred tlm hybrid ours n grams models baseline hred tlm hybrid ours human evaluation scores coh inf rel flu dynamic memory read by decoder the benet of implicit extraction is maximized when the extracted representations are properly sumed in the text generation to understand the link between the representations stored in the ory and the summary generation we analyze the memory read attention weights tk in equation throughout the decoding figure shows that the decoder fully utilizes all memory representations we also nd a pattern that memory read attention weights are mostly concentrated on the front part of the source text in the beginning of the decoding mem slots and gradually moves to the latter part mem slots this demonstrates s ability to update the read operation to dynamically capture relevant input contexts during the summary generation abstractiveness of the summary to analyze the abstractiveness of generated summaries we present the ratio of output summary grams present in the original input article table shows that copies less n grams than the baseline hred compared to the baseline hred generates approximately more novel grams and more grams respectively the result along with higher rouge scores highlights the s ability to generate novel words for abstractive summarization while staying focused on important parts of the article although tlm subramanian et al shows the highest abstractiveness achieves its result with a signicanly smaller model human evaluation we also perform a human evaluation to assess the quality of generated summaries for the human evaluation random arxiv testset article summary pairs are presented to three amazon cal turk workers workers judge generated summaries on four different aspects coherence coh does the summary make sense informativeness inf are the most important points of the article captured redundancy red does the summary repeat itself and fluency flu how uent is the summary table shows that obtains the highest scores in informativeness and dundancy its coherence score is also close to the best score by tlm fluency is an advantage for the transformer based tlm as expected but is close and still greatly superior to vanilla hierarchical encoder decoders while the rouge difference between the baseline and are substantial in table the results of the human evaluation show more pronounced differences in the quality of generated text for output summary examples please refer to table in appendix authors provided example summaries from their model conclusion this work proposes a novel maed based mechanism for very long text abstractive summarization involves two memory types a static encoder memory for compressing input texts and a dynamic decoder memory which renes the generation process memory fer between them links two memories and maximizes the benet of content extraction aimed for summarization different from existing hybrid extractive and abstractive approaches incorporates an extraction step without ground truth sentence labels and multi step training we demonstrate the effectiveness of by showing promising results on the pubmed arxiv and newsroom summarization datasets with an order of magnitude less parameters than competing transformer based models the s memory compression can be generalized to other mains that require text generation guided by content selection in future work we will extend and validate the strength of our approach on a variety of language learning tasks references sanghwan bae taeuk kim jihoon kim and sang goo lee training the of sentence rewriting for abstractive summarization shop on new frontiers in summarization pp hong kong china november association for computational linguistics url doi aclweb org anthology in proceedings of summary level yen chun chen and mohit bansal fast abstractive summarization with reinforce selected the association for sentence rewriting computational linguistics volume long papers pp melbourne australia july association for computational linguistics url aclweb org anthology the annual meeting of in proceedings of in proceedings of jianpeng cheng and mirella lapata neural summarization by extracting sentences and the association for the annual meeting of words tational linguistics volume long papers pp berlin germany august association for computational linguistics url doi aclweb org anthology jaemin cho minjoon seo and hannaneh hajishirzi mixture content selection for in proceedings of the conference on empirical verse sequence generation ods in natural language processing and the international joint conference on ral language processing emnlp ijcnlp pp hong kong china ber association for computational linguistics url aclweb org anthology kyunghyun cho bart van merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio learning phrase representations using rnn encoder in proceedings of the conference on decoder for statistical machine translation pirical methods in natural language processing emnlp pp doha qatar tober association for computational linguistics url aclweb org anthology arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian a discourse aware attention model for abstractive summarization of long ments in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume short papers pp new orleans louisiana june association for computational linguistics doi url aclweb org anthology jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language understanding in proceedings of the ference of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pp minneapolis nesota june association for computational linguistics url aclweb org anthology sebastian gehrmann yuntian deng and alexander rush bottom up abstractive the conference on empirical methods in marization ral language processing pp brussels belgium october november association for computational linguistics url aclweb org anthology in proceedings of doi max grusky mor naaman and yoav artzi newsroom a dataset of million summaries with diverse extractive strategies in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long papers pp jiatao gu zhengdong lu hang li and victor o k li incorporating copying mechanism in in proceedings of the annual meeting of the sequence to sequence learning tion for computational linguistics volume long papers pp berlin germany august association for computational linguistics url aclweb org anthology karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay teaching machines to read and comprehend mustafa suleyman and phil blunsom in advances in neural information processing systems pp url nips cc teaching machines to read and comprehend pdf wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun a unied model for extractive and abstractive summarization using inconsistency loss in proceedings of the annual meeting of the association for computational linguistics volume long papers pp melbourne australia july association for computational linguistics doi url aclweb org anthology lukasz kaiser or nachum aurko roy and samy bengio learning to remember rare in international conference on learning representations iclr toulon events france april conference track proceedings openreview net url net sjtqldqlg byeongchang kim hyunwoo kim and gunhee kim abstractive summarization of reddit in proceedings of the conference of the posts with multi level memory networks north american chapter of the association for computational linguistics human language technologies volume long and short papers pp minneapolis minnesota june association for computational linguistics url aclweb org anthology diederik p kingma and jimmy ba adam a method for stochastic optimization in yoshua bengio and yann lecun eds international conference on learning representations iclr san diego ca usa may conference track proceedings url org hung le truyen tran thin nguyen and svetha venkatesh variational memory decoder in advances in neural information processing systems pp url nips cc variational memory encoder decoder pdf c y lin looking for a few good metrics automatic summarization evaluation how many samples are enough in proceedings of the ntcir workshop thang luong hieu pham and christopher d manning effective approaches to the conference on based neural machine translation cal methods in natural language processing pp lisbon portugal ber association for computational linguistics url aclweb org anthology in proceedings of y ma and j c principe a taxonomy for neural memory networks ieee transactions on neural networks and learning systems pp ramesh nallapati bowen zhou cicero dos santos c aglar and bing xiang abstractive text summarization using sequence to sequence rnns and beyond in proceedings of the signll conference on computational natural language learning pp berlin many august association for computational linguistics url aclweb org anthology ramesh nallapati bowen zhou and mingbo ma classify or select neural architectures for tractive document summarization arxiv preprint shashi narayan shay b cohen and mirella lapata ranking sentences for extractive the marization with reinforcement learning north american chapter of the association for computational linguistics human guage technologies volume long papers pp new orleans louisiana june association for computational linguistics url aclweb org anthology the conference of in proceedings of cesc chunseong park byeongchang kim and gunhee kim attend to you personalized image captioning with context sequence memory networks ieee conference on computer vision and pattern recognition cvpr pp alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever language models are unsupervised multitask learners openai blog marcaurelio ranzato sumit chopra michael auli and wojciech zaremba sequence level ing with recurrent neural networks in yoshua bengio and yann lecun eds international conference on learning representations iclr san juan puerto rico may conference track proceedings url org abigail see peter j liu and christopher d manning get to the point summarization with in proceedings of the annual meeting of the association pointer generator networks for computational linguistics volume long papers pp vancouver canada july association for computational linguistics url aclweb org anthology sandeep subramanian raymond li jonathan pilault and christopher pal on extractive and abstractive neural document summarization with transformer language models arxiv preprint zhaopeng tu zhengdong lu yang liu xiaohua liu and hang li coverage based neural machine translation corr url org aaron van den oord sander dieleman heiga zen karen simonyan oriol vinyals alexander graves nal kalchbrenner andrew senior and koray kavukcuoglu wavenet a generative model for raw audio in arxiv url org ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in i guyon u v luxburg s bengio h wallach r fergus s vishwanathan and r garnett eds advances in ral information processing systems pp curran associates inc url nips cc attention is all you need pdf oriol vinyals meire fortunato and navdeep jaitly pointer networks vances in neural nips cc pointer networks pdf information processing systems pp in url mingxuan wang zhengdong lu hang li and qun liu memory enhanced decoder the conference on in proceedings of for neural machine translation cal methods in natural language processing pp austin texas november association for computational linguistics url doi aclweb org anthology dani yogatama yishu miao gabor melis wang ling adhiguna kuncoro chris dyer and in phil blunsom memory architectures in recurrent neural network language models international conference on learning representations iclr vancouver bc canada april may conference track proceedings openreview net url net fisher yu and vladlen koltun multi scale context aggregation by dilated convolutions in yoshua bengio and yann lecun eds international conference on learning representations iclr san juan puerto rico may conference track proceedings url org a appendix a baseline architecture in addition to the hierarchical recurrent encoder decoder hred architecture described in section the following features are used for both baseline and architectures a pointer generator network in order to handle out of vocabulary oov token predictions pointer generator in see et al is used to copy words directly from the input document at each step t the decoder decides whether to predict the next target word from the input text or the generation mechanism the pointer ator computes zt which denotes the probability of choosing pg for sampling the next target token zt c ct t wt x t where x t is the embedding of the previous target token the probability zt is used as a variable for soft switch between generating a word from the vocabulary pg or directly copying from the source document pc the probability of copying a word w from the source text is calculated based on the attention weights ti x i xi w note that if w does not exist in the source document likewise if w is an out of vocabulary word combining two probability distributions the nal probability of the next word yt being w is as follows p yt a decoder coverage it is well known that rnn sequence to sequence models tend to suffer from repeated phrases when generating long target sequences see et al tackled this issue by keeping track of the attention coverage more concretely the coverage vector covt at the decoding step t is computed by taking the summation of the token level attention weights until the last step t covt t x to inform the decoder of the history of attention weights the coverage vector is fed into the level attention mechanism which modies the equation to the following equation ti i t wc covt t a training details we generally follow the pre processing steps in cohan et al for the pubmed and arxiv datasets the maximum number of sections is set to and the maximum number of tokens for each section is the length of the target summary is limited to tokens single layer bidirectional grus cho et al are used for the sentence and the document coders the decoder is also a single layer gru all grus have the hidden size of the sionality of token embeddings is and embeddings are trained from scratch the vocabulary size is limited to batch size is and adam kingma ba with learning rate is used for training maximum gradient norm is set to we train all models for epochs at the test time beam search with the beam size is used for decoding for hyperparameters the number of heads for the memory compression is and the self attention hidden size is the weights and for the regularization and are and respectively a the effect of regularization the following gures show the effect of regularization in s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s with regularization without regularization figure the effect of regularization on memory compression examples of the multi head encoder memory write attention matrix a are illustrated rows denote memory heads or slots and columns indicate input sentence indices note that the regularization loss removes the redundancy over different memory heads and guides each slot to focus on a single sentence t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t t decoding timstep with regularization decoding timestep without regularization figure the effect of regularization on memory read examples of the decoder memory read attention matrix are illustrated rows denote memory heads or slots and columns indicate ing time steps note that the regularization loss encourages the model to fully utilize the compressed memory representations a qualitative results reference and system generated summaries from the test set of the arxiv dataset are shown in the following table compared to the baseline shows much less repetition and produces more concise summaries even the tlm model repeats several phrases or sentences table reference and system generated summaries from the test set of the arxiv dataset in the number of variables and constraints reference we study the behavior of simple principal pivoting methods for the p matrix linear mentarity problem p lcp we solve an open problem of morris by showing that murty s least index pivot rule under any xed index order leads to a quadratic number of iterations on morris s highly cyclic p lcp examples we then show that on k matrix lcp instances all pivot rules require only a linear number of iterations as the main tool we employ unique sink orientations of cubes a useful combinatorial abstraction of the lcp baseline hred the third author of this paper still vividly recollects his visit to victor klee at the versity of washington seattle in august the third author of this paper still vividly recollects his visit to victor klee at the university of washington seattle in august the third author of this paper still vividly recollects his visit to victor klee at the university of washington seattle in august in this paper we introduce the digraph model behind p lcps and show that the simplex method is polynomial time for which the number of iterations is exponential tlm hybrid we study linear complementarity problems with sufcient matrices and the criss cross linear complementarity to derive linear complementarity problems method we use a novel notion of with sufcient matrices and the linear version of the quadratic version of the linear version of the linear version this yields a new family of np complete problems for which both linear complementarity and the quadratic version of the linear version of the quadratic version are possible in this paper we introduce a digraph model behind p lcps and show that the simplex exponential method can be used to solve a linear program lp for which the number of iterations is in the number of variables and constraints on the number of iterations we show that the number of arithmetic operations does not depend on the bit lengths of the input numbers we also show that the number of iterations required to solve the linear complementarity problem can be used to determine the expected performance of the lp reference we have developed a procedure for the classication of eclipsing binaries from their light the procedure was tested on more than systems with known curve parameters and spectral type classication and its efciency was estimated for every evolutionary status we use the procedure was applied to about binaries with no classication and the vast majority of them was classied fully systems of relatively rare evolutionary classes were detected in that process as well as systems with unusual contradictory parameters also for previously unclassied cluster binaries evolutionary classes were identied these stars can serve as tracers for age and distance estimation of their parent stellar systems the procedure proved itself as fast exible and effective enough to be applied to large ground based and space born surveys containing tens of thousands of eclipsing binaries baseline hred in this paper we present a novel procedure for determination of the evolutionary status of eclipsing binaries the procedure is based on the most comprehensive set of rules for the classication of eclipsing binaries which is the world s principal database of eclipsing binary systems with available classication the main goal of this paper is to develop a fast and effective procedure for determination of the binary s spectral type or color index the procedure is based on the most comprehensive set of rules for the classication of eclipsing binaries which is the world s principal database of eclipsing binary systems with available classication the main goal of this paper is to develop a fast and effective procedure for determination of the evolutionary status of eclipsing binaries the method is based on a catalogue which includes a number and the eclipsing variables and a classication procedure we nd a good t of all known parameters which can not have any value in a large range we nd a signicant fraction mp depth for each of these systems and that a given number is mag tlm hybrid a fast and effective procedure for determination of the evolutionary status of eclipsing binaries is presented a complete set of rules is developed that utilizes all available data for a specic class of eclipsing binaries while requiring only light curve parameters and an estimate of the binary s spectral type or color index the procedure is tested with the catalogue of eclipsing binaries cev which is the world s principal database of eclipsing binaries with available classication the main goal of our work is to develop a fast and effective procedure for determination of the evolutionary status of eclipsing binaries we apply it to cev binaries with already available classication the main objective of this work is to develop a fast and effective procedure for determination of the evolutionary status of eclipsing binaries the main objective of our work is to develop a fast and effective procedure for determination of the evolutionary status of eclipsing binaries we apply it to cev binaries with already available classication the main objective of this work is to demonstrate the usefulness of our method and to propose a possible application of it to cev binaries with already known classication in this paper we present a novel procedure for determination of the evolutionary status of eclipsing binaries while requiring only light curve parameters and an estimate of the binary s spectral type or color index the procedure is based on the most comprehensive set of rules for the classication of ing binaries which can be used to quickly characterize large numbers of eclipsing binaries the procedure is tested with the catalogue of eclipsing variables dr sh which is the world s principal database of ing binary systems with available classication we nd that the number of fully characterized eclipsing binaries can be used to quickly characterize large numbers of eclipsing binaries
