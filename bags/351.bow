learning summarize long texts ory compression transfer jaehong park jonathan christopher montreal montreal cifar chair firstname com abstract introduce memory memory mechanism hierarchical current neural network based encoder decoder architectures explore use abstractive document summarization transfers memories readable writable external memory modules augment encoder decoder memory regularization compresses encoded input article compact set sentence representations importantly ory compression step performs implicit extraction labels sidestepping issues suboptimal ground truth data exposure bias hybrid abstractive summarization techniques allowing decoder read write encoded input memory model learns read salient information input article keeping track generated approach yields results competitive state art transformer based summarization methods times fewer parameters introduction automatic summarization automated process reducing size input text serving relevant information content core semantics techniques summarization characterized extractive abstractive extractive methods construct summaries combining salient passages usually sentences source text process similar human way identifying right information way achieve tive summarization dene problem sentence classication task form representation sentences document nallapati cheng lapata avoid content overlap issues previous work sentence reranking narayan sentence ordering extracting sentences recurrently chen bansal abstractive methods generate summaries generating new sentence constructs scratch representation document content process conceptually similar notion paraphrasing stractive text summarization attracted interest capable generating novel formulations summaries language generation models conditioned source text based recurrent neural network rnn encoder decoders introduced tackle varying text generation issues standalone abstractive sequence sequence models copy pointer mechanisms vinyals ample enabled decoders better generate unseen words vocabulary words named entities recently hybrid extractive abstractive architectures proposed shown promising results quantitative performance measures human evaluations ups extractive model rst selects salient sentences source article abstractive model paraphrases extracted sentences nal summary majority current state art abstractive summarization based hybrid approach chen bansal gehrmann hsu bae subramanian nonetheless authors contributed equally work summarization models large scale pre trained language models bert devlin hybrid models limited disadvantages ground truth labels extractive summarization usually provided extractive labels generated potentially mal algorithm hsu subramanian performance models trained labels bounded quality performance extractive heuristics ond ground truth binary labels recurrently extracted sentences typically teacher forced chen bansal exposure bias ranzato negatively affect content selection performance inference finally given hard extraction step differentiable existing hybrid models typically require multi step training end end gehrmann subramanian reinforcement learning chen bansal bae train model paper introduce novel abstractive summarization model incorporates ate extractive step require labels type extractive content selection fully end end trainable achieve propose new memory augmented encoder decoder maed architecture wang yogatama principe called memorization modes absorb key information encoded source sequence compression mechanism sequentially update ternal memory target summary generation extractive ground truth labels analysis compression mechanism behaves implicit sentence extractor stores sentence representations salient content choice sentence tations guided memory regularization conditional language modeling loss decoder avoiding exposure bias maximizing likelihood sequential binary tion labels finally encoded memory transferred decoder memory iteratively rened decoding process knowledge rst abstractive rization model uses memory compression sentence extraction directly employs memorized representations summary generation empirically demonstrate merits approach setting new state art long text abstractive summarization tasks pubmed arxiv newsroom datasets cohan grusky contributions fold introduce approach stores salient sentence level representations memory compression transfers memory encoder decoder iii updates memory summary generation proceeds method yields results competitive state art unlike previous works model combines best extractive abstractive rization fully end end trainable manner supervision extraction step transformer vaswani language model based techniques substantially fewer rameters parameters recent state art transformer based method background baseline use hierarchical recurrent encoder decoder structure hred based model nallapati cohan hred encoder grus sentence encoder document encoder given input sentence length sentence encoder takes sequence token embeddings transforms sequence hidden states hidden state sentence encoder corresponding sentence embedding sequence sentence embeddings processed document encoder sentence embedding total number sentences document associated document encoder hidden state decoder gru generates target summary token time decoding step decoder creates decoder hidden state decoder obtains context vector figure architecture sentence level representations document encoder reduced sized encoder memory encoder memory transferred decoder memory read ram like mechanism resulting memory readout vector condition sentence word level attention decoder hidden states memory readout vector update memory state gated write operation alignment sentence level attention computed combining token level attention denotes index sentence corresponding word total number tokens input document attn equation dened luong probability distribution target word estimated decoder hidden state context vector training objective average negative log likelihood target word ground truth summary length finally pointer generator decoder coverage method baseline hred details baseline architecture found appendix memory memory mechanism main features encoder memory bank compresses large set encoder hidden representations smaller subset salient representations read write operations allow decoder read update encoder memory bank new information summary generation token generation conditioned extracted sentence representations accessed dynamic memory components seamlessly integrated existing hred architectures essence process seen extraction followed generation architecture depicted figure memory compression encoder aim having external memory encoder create xed size representation reduces set hidden representations grudoc subset representations quence sentence level document encoder hidden representations construct intermediate matrix rld document encoder hidden size smaller sized memory bank generated taking linear combination row weight vector linear combination computed self attention mechanism rda rdad size hidden layer hyperparameter capture aspects input document extended multi head memory write attention matrix heads rrda hyperparameter results different convex combinations gives nal multi head encoder memory matrix rrd ensure attention weights heads focus diverse set salient input sentences propose novel regularization loss memory compression following regularization term encourages diversity compressed encoded states frobenius norm regularization loss achieves goals neously promotes diversity sentence representations stored memory reducing risk redundant information hardens attention probabilities head assuring memory slot approximately associated single sentence representation result encoder memory essentially performs implicit extraction encoder hidden states fully differentiable manner figure appendix shows effect regularization encoder memory compression note supervision exists extractive step ory compression guided memory regularization propagated error signals target summary generation read write operations decoder encoder memory constructed context read memory augment attention mechanism target token generation rst step encoder memory transferred decoder initial state decoder memory time step decoder reads memory generates memory read vector specically decoder takes weighted sum memory slots ram like attention mechanism vector representation head slot combined generate memory augmented decoder hidden state token estimation baseline system replaced consequence attention source text prediction target token conditioned memory read direct link contents memory text generation summary generation process semantics source sequence kept decoder memory needs modied memory write operation enables memory log history attended generated decoder memory write operation outlined removes adds information gated mechanism forget memories update memory slot gating mechanism conditioned memory content memory read vector decoder hidden state text generation directly conditioned memory context benet ing memory limited preliminary experiments observed memory slots table results pubmed arxiv newsroom dataset type corresponds purely abstractive extractive extractive abstractive hybrid approaches tlm uses model radford times larger parameters highest rouge scores abstractive methods boldfaced rouge scores condence interval reported ofcial rouge results taken newsroom abstractive summarization test set results model type sent sent attn ptr gen discourse params pubmed arxiv newsroom repetitively attended decoding ensure decoder fully utilizes memory improve generation propose regularization term vector representation kth memory head sentence level context vector regularization assumes memory context level representation combined properly decoding sentence level context vector correlate initial state decoder memory compute representation deviates original representation space write operation nal training objective follows weights regularization hyperparameters related work recent works abstractive summarization leveraged intermediate content selection approaches writing summary factorized steps extraction generation extractor prioritize select important input text extractor normally trained sequence binary labels label indicates corresponding text unit selected level extraction word level gehrmann cho sentence level chen bansal hsu bae subramanian ground truth extraction typically missing heuristics measure gram overlap target summary build extractive oracles similar approaches performs sentence level extraction deal long source determines alignment source target sentences latent space relying possibly suboptimal extractive heuristics addition sentence extraction sequentially addresses exposure bias issue ranzato memory augmented encoder decoder maed architectures wang yogatama principe proposed conditional natural language preliminary experiments applied word level selection pubmed arxiv datasets led poor results table model ablation study pubmed dataset model baseline hred encoder mem decoder mem mem transfer reg reg rouge generation tasks machine translation kaiser image captioning park differentiable read write operations external module maed represent non local context rnns enhanced memory capacity models able store rally distant information large input sequences feature particularly useful long text summarization context short text abstractive summarization kim proposed memory architecture named multi level memory networks mmn mmn exibly reduce sentations different levels document hierarchy xed size external memory authors multi layer dilated convolutional neural networks cnn koltun van den oord build hierarchical representation document constructs memory hierarchical representation document compressing sparse set sentence representations mmn memory representations remain static decoding process dynamically updates memory effective learning long term dependency lastly work proposes novel regularization memory read compression results discussion experiment setup evaluate pubmed arxiv cohan newsroom abstractive datasets grusky large scale summarization datasets average lengths source articles target summaries pubmed arxiv room respectively times longer widely cnn dailymail dataset hermann pre processing training setups tical cohan subramanian details training evaluation found appendix quantitative evaluation use rouge metric lin report rouge scores results table shows rouge scores summarization datasets hybrid type refers models use step extractive abstractive summarization pubmed dataset tlm model shows highest scores close scores shows higher scores times parameters achieves performance tlm model ground truth labels sentence extraction reiterate trained completely end end hybrid tlm requires separate training extractor conditional transformer language model similar results arxiv newsroom datasets arxiv dataset surpasses transformer based tlm model scores shows competitive results newsroom abstractive dataset ablation study assess importance components conduct ablation study pubmed dataset table shows effects adding different add ons rouge scores mem baseline hred augmented encoder memory described section table rouge scores unsupervised extractive methods pubmed arxiv dataset result baseline extractive methods subramanian data pubmed arxiv model lexrank gold ext lexrank gold ext rouge decoding timstep multi head memory write attention matrix memory read attention matrix figure multi head memory write attention matrix memory read attention matrix rows denote memory heads slots columns indicate input sentence indices columns indicate decoding time steps result demonstrates memory context enhances performance measures generated summaries decoder mem adds write operation memory memory transfer case decoder memory initialized zeros encoder memory compared baseline hred result shows write mechanism decoder memory helps generation memory transfer indicates summary writing process largely benets accessing long term contextual information output text transferring memory mem transfer brings substantial improvements rouge scores crucial initiate summary generation selected memory representations showing importance memory compression transfer steps furthermore observed discouraging redundancies encoder memory head leads additional ments rouge scores finally adding regularization decoder memory read operation completes architecture achieves best rouge scores implicit extraction memory compression initial hypothesis encoder memory pick set salient input sentences summarization conrm analyze quality extractive summarization memory compression concretely concatenate sentences highest attention weight memory head generate summary table shows rouge scores different pervised extractive summarization methods pubmed arxiv datasets extractive marization performed memory compression outperforms existing unsupervised extractive summarization baselines result indicates memory compression able prioritize large set input sentences ground truth sentence labels table percentage ratios output summary grams found pubmed original input article table results human evaluation maximum score criterion models reference baseline hred tlm hybrid grams models baseline hred tlm hybrid human evaluation scores coh inf rel flu dynamic memory read decoder benet implicit extraction maximized extracted representations properly sumed text generation understand link representations stored ory summary generation analyze memory read attention weights equation decoding figure shows decoder fully utilizes memory representations pattern memory read attention weights concentrated source text beginning decoding mem slots gradually moves mem slots demonstrates ability update read operation dynamically capture relevant input contexts summary generation abstractiveness summary analyze abstractiveness generated summaries present ratio output summary grams present original input article table shows copies grams baseline hred compared baseline hred generates approximately novel grams grams respectively result higher rouge scores highlights ability generate novel words abstractive summarization staying focused important parts article tlm subramanian shows highest abstractiveness achieves result signicanly smaller model human evaluation perform human evaluation assess quality generated summaries human evaluation random arxiv testset article summary pairs presented amazon cal turk workers workers judge generated summaries different aspects coherence coh summary sense informativeness inf important points article captured redundancy red summary repeat fluency flu uent summary table shows obtains highest scores informativeness dundancy coherence score close best score tlm fluency advantage transformer based tlm expected close greatly superior vanilla hierarchical encoder decoders rouge difference baseline substantial table results human evaluation pronounced differences quality generated text output summary examples refer table appendix authors provided example summaries model conclusion work proposes novel maed based mechanism long text abstractive summarization involves memory types static encoder memory compressing input texts dynamic decoder memory renes generation process memory fer links memories maximizes benet content extraction aimed summarization different existing hybrid extractive abstractive approaches incorporates extraction step ground truth sentence labels multi step training demonstrate effectiveness showing promising results pubmed arxiv newsroom summarization datasets order magnitude parameters competing transformer based models memory compression generalized mains require text generation guided content selection future work extend validate strength approach variety language learning tasks references sanghwan bae taeuk kim jihoon kim sang goo lee training sentence rewriting abstractive summarization shop new frontiers summarization hong kong china november association computational linguistics url doi aclweb org anthology proceedings summary level yen chun chen mohit bansal fast abstractive summarization reinforce selected association sentence rewriting computational linguistics volume long papers melbourne australia july association computational linguistics url aclweb org anthology annual meeting proceedings proceedings jianpeng cheng mirella lapata neural summarization extracting sentences association annual meeting words tational linguistics volume long papers berlin germany august association computational linguistics url doi aclweb org anthology jaemin cho minjoon seo hannaneh hajishirzi mixture content selection proceedings conference empirical verse sequence generation ods natural language processing international joint conference ral language processing emnlp ijcnlp hong kong china ber association computational linguistics url aclweb org anthology kyunghyun cho bart van merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase representations rnn encoder proceedings conference decoder statistical machine translation pirical methods natural language processing emnlp doha qatar tober association computational linguistics url aclweb org anthology arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang nazli goharian discourse aware attention model abstractive summarization long ments proceedings conference north american chapter association computational linguistics human language technologies volume short papers new orleans louisiana june association computational linguistics doi url aclweb org anthology jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language understanding proceedings ference north american chapter association computational linguistics human language technologies volume long short papers minneapolis nesota june association computational linguistics url aclweb org anthology sebastian gehrmann yuntian deng alexander rush abstractive conference empirical methods marization ral language processing brussels belgium october november association computational linguistics url aclweb org anthology proceedings doi max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies proceedings conference north american ter association computational linguistics human language technologies volume long papers jiatao zhengdong hang victor incorporating copying mechanism proceedings annual meeting sequence sequence learning tion computational linguistics volume long papers berlin germany august association computational linguistics url aclweb org anthology karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay teaching machines read comprehend mustafa suleyman phil blunsom advances neural information processing systems url nips teaching machines read comprehend pdf wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun unied model extractive abstractive summarization inconsistency loss proceedings annual meeting association computational linguistics volume long papers melbourne australia july association computational linguistics doi url aclweb org anthology lukasz kaiser nachum aurko roy samy bengio learning remember rare international conference learning representations iclr toulon events france april conference track proceedings openreview net url net sjtqldqlg byeongchang kim hyunwoo kim gunhee kim abstractive summarization reddit proceedings conference posts multi level memory networks north american chapter association computational linguistics human language technologies volume long short papers minneapolis minnesota june association computational linguistics url aclweb org anthology diederik kingma jimmy adam method stochastic optimization yoshua bengio yann lecun eds international conference learning representations iclr san diego usa conference track proceedings url org hung truyen tran thin nguyen svetha venkatesh variational memory decoder advances neural information processing systems url nips variational memory encoder decoder pdf lin looking good metrics automatic summarization evaluation samples proceedings ntcir workshop thang luong hieu pham christopher manning effective approaches conference based neural machine translation cal methods natural language processing lisbon portugal ber association computational linguistics url aclweb org anthology proceedings principe taxonomy neural memory networks ieee transactions neural networks learning systems ramesh nallapati bowen zhou cicero dos santos aglar bing xiang abstractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning berlin august association computational linguistics url aclweb org anthology ramesh nallapati bowen zhou mingbo classify select neural architectures tractive document summarization arxiv preprint shashi narayan shay cohen mirella lapata ranking sentences extractive marization reinforcement learning north american chapter association computational linguistics human guage technologies volume long papers new orleans louisiana june association computational linguistics url aclweb org anthology conference proceedings cesc chunseong park byeongchang kim gunhee kim attend personalized image captioning context sequence memory networks ieee conference computer vision pattern recognition cvpr alec radford jeffrey rewon child david luan dario amodei ilya sutskever language models unsupervised multitask learners openai blog marcaurelio ranzato sumit chopra michael auli wojciech zaremba sequence level ing recurrent neural networks yoshua bengio yann lecun eds international conference learning representations iclr san juan puerto rico conference track proceedings url org abigail peter liu christopher manning point summarization proceedings annual meeting association pointer generator networks computational linguistics volume long papers vancouver canada july association computational linguistics url aclweb org anthology sandeep subramanian raymond jonathan pilault christopher pal extractive abstractive neural document summarization transformer language models arxiv preprint zhaopeng zhengdong yang liu xiaohua liu hang coverage based neural machine translation corr url org aaron van den oord sander dieleman heiga zen karen simonyan oriol vinyals alexander graves nal kalchbrenner andrew senior koray kavukcuoglu wavenet generative model raw audio arxiv url org ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need guyon luxburg bengio wallach fergus vishwanathan garnett eds advances ral information processing systems curran associates inc url nips attention need pdf oriol vinyals meire fortunato navdeep jaitly pointer networks vances neural nips pointer networks pdf information processing systems url mingxuan wang zhengdong hang qun liu memory enhanced decoder conference proceedings neural machine translation cal methods natural language processing austin texas november association computational linguistics url doi aclweb org anthology dani yogatama yishu miao gabor melis wang ling adhiguna kuncoro chris dyer phil blunsom memory architectures recurrent neural network language models international conference learning representations iclr vancouver canada april conference track proceedings openreview net url net fisher vladlen koltun multi scale context aggregation dilated convolutions yoshua bengio yann lecun eds international conference learning representations iclr san juan puerto rico conference track proceedings url org appendix baseline architecture addition hierarchical recurrent encoder decoder hred architecture described section following features baseline architectures pointer generator network order handle vocabulary oov token predictions pointer generator copy words directly input document step decoder decides predict target word input text generation mechanism pointer ator computes denotes probability choosing sampling target token embedding previous target token probability variable soft switch generating word vocabulary directly copying source document probability copying word source text calculated based attention weights note exist source document likewise vocabulary word combining probability distributions nal probability word follows decoder coverage known rnn sequence sequence models tend suffer repeated phrases generating long target sequences tackled issue keeping track attention coverage concretely coverage vector covt decoding step computed taking summation token level attention weights step covt inform decoder history attention weights coverage vector fed level attention mechanism modies equation following equation covt training details generally follow pre processing steps cohan pubmed arxiv datasets maximum number sections set maximum number tokens section length target summary limited tokens single layer bidirectional grus cho sentence document coders decoder single layer gru grus hidden size sionality token embeddings embeddings trained scratch vocabulary size limited batch size adam kingma learning rate training maximum gradient norm set train models epochs test time beam search beam size decoding hyperparameters number heads memory compression self attention hidden size weights regularization respectively effect regularization following gures effect regularization regularization regularization figure effect regularization memory compression examples multi head encoder memory write attention matrix illustrated rows denote memory heads slots columns indicate input sentence indices note regularization loss removes redundancy different memory heads guides slot focus single sentence decoding timstep regularization decoding timestep regularization figure effect regularization memory read examples decoder memory read attention matrix illustrated rows denote memory heads slots columns indicate ing time steps note regularization loss encourages model fully utilize compressed memory representations qualitative results reference system generated summaries test set arxiv dataset shown following table compared baseline shows repetition produces concise summaries tlm model repeats phrases sentences table reference system generated summaries test set arxiv dataset number variables constraints reference study behavior simple principal pivoting methods matrix linear mentarity problem lcp solve open problem morris showing murty index pivot rule xed index order leads quadratic number iterations morris highly cyclic lcp examples matrix lcp instances pivot rules require linear number iterations main tool employ unique sink orientations cubes useful combinatorial abstraction lcp baseline hred author paper vividly recollects visit victor klee versity washington seattle august author paper vividly recollects visit victor klee university washington seattle august author paper vividly recollects visit victor klee university washington seattle august paper introduce digraph model lcps simplex method polynomial time number iterations exponential tlm hybrid study linear complementarity problems sufcient matrices criss cross linear complementarity derive linear complementarity problems method use novel notion sufcient matrices linear version quadratic version linear version linear version yields new family complete problems linear complementarity quadratic version linear version quadratic version possible paper introduce digraph model lcps simplex exponential method solve linear program number iterations number variables constraints number iterations number arithmetic operations depend bit lengths input numbers number iterations required solve linear complementarity problem determine expected performance reference developed procedure classication eclipsing binaries light procedure tested systems known curve parameters spectral type classication efciency estimated evolutionary status use procedure applied binaries classication vast majority classied fully systems relatively rare evolutionary classes detected process systems unusual contradictory parameters previously unclassied cluster binaries evolutionary classes identied stars serve tracers age distance estimation parent stellar systems procedure proved fast exible effective applied large ground based space born surveys containing tens thousands eclipsing binaries baseline hred paper present novel procedure determination evolutionary status eclipsing binaries procedure based comprehensive set rules classication eclipsing binaries world principal database eclipsing binary systems available classication main goal paper develop fast effective procedure determination binary spectral type color index procedure based comprehensive set rules classication eclipsing binaries world principal database eclipsing binary systems available classication main goal paper develop fast effective procedure determination evolutionary status eclipsing binaries method based catalogue includes number eclipsing variables classication procedure good known parameters value large range signicant fraction depth systems given number mag tlm hybrid fast effective procedure determination evolutionary status eclipsing binaries presented complete set rules developed utilizes available data specic class eclipsing binaries requiring light curve parameters estimate binary spectral type color index procedure tested catalogue eclipsing binaries cev world principal database eclipsing binaries available classication main goal work develop fast effective procedure determination evolutionary status eclipsing binaries apply cev binaries available classication main objective work develop fast effective procedure determination evolutionary status eclipsing binaries main objective work develop fast effective procedure determination evolutionary status eclipsing binaries apply cev binaries available classication main objective work demonstrate usefulness method propose possible application cev binaries known classication paper present novel procedure determination evolutionary status eclipsing binaries requiring light curve parameters estimate binary spectral type color index procedure based comprehensive set rules classication ing binaries quickly characterize large numbers eclipsing binaries procedure tested catalogue eclipsing variables world principal database ing binary systems available classication number fully characterized eclipsing binaries quickly characterize large numbers eclipsing binaries
