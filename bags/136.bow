extractive text summarization neural networks aakash sinha department computer science engineering indian institute technology delhi new delhi india abhishek yadav department computer science engineering indian institute technology delhi new delhi india akshay gahlot department computer science engineering indian institute technology delhi new delhi india abstract text summarization extensively studied problem traditional approaches text summarization rely heavily feature engineering contrast propose fully data driven approach feedforward neural networks single document summarization train evaluate model standard duc dataset shows results comparable state art models proposed model scalable able produce summary arbitrarily sized documents breaking original document fixed sized parts feeding recursively network keywords neural networks recursive extractive summarization introduction text summarization known task natural language understanding summarization general refers task presenting information concise manner focusing important parts data whilst preserving meaning main idea summarization find subset data contains information entire set today world data generation consumption exploding exponential rate text summarization necessity applications search engine business analysis market review automatic document summarization involves producing summary given text document human help broadly divided classes extractive summarization abstractive summarization extractive summarization picks sentences directly document based scoring function form coherent summary hand abstractive summarization produce tries summary parts appear original document summary include verbal innovations cases vocabulary summary original document general building abstract summaries difficult task involves complex language modeling text summarization finds applications nlp related tasks question answering text classification related fields generation summaries integrated systems intermediate stage helps reduce length document turn leads faster access information searching news summarization headline generation important application search engines use machine generated headlines displaying news articles feeds objects paper focus extractive summarization focuses extracting objects directly entire collection extractive modifying summarizers sentences input produce probability vector output entries vector represent probability sentence included summary produce final summary best sentences chosen according required summary length models based graphs linguistic scoring machine learning proposed task till date approaches model problem classification problem outputs include sentence summary achieved standard naive bayes classifier support vector machines supervised learning based models rely human engineered features word position sentence position word frequency based features sentence assigned score scoring functions including idf centroid based metrics date sentences ranked according importance similarity ranking algorithm similarity sentences calculated cosine similarity prevent occurrence repetitive information trained identify specific feature engineering based models proved successful domain genre specific summarization medical reports specific news articles classifiers types information techniques poor results general text summarization work propose fully driven approach neural networks gives reliable results irrespective document type require predecided features classifying sentences proposed model capable producing summaries corresponding documents varying lengths recursive approach produce summaries variable length documents trained model duc datasets evaluated proposed model rouge automatic evaluator duc dataset compare variants rouge scores existing models experimental results proposed model achieves performance comparable state art systems access linguistic information rest paper presented follows section formulate problem section conceptualizes proposed model describes neural network detail presented information datasets experimental details section comparison existing models provided results experiments shown section paper concluded section problem formulation section describe summarization task formal manner given document sequence sentences want generate summary sentence level extractive methods yield naturally grammatical summaries require relatively little linguistic analysis create extractive summary document selecting set sentences document number sentences summary original document assume output length fixed summarizer knows length summary generation selection process involves scoring sentence document predicting label indicates sentence included summary use supervised learning technique objective maximize likelihood sentence labels wlx given input document model parameters log log purpose scoring function assigns value sentence denoting probability picked summary summary length fixed known according summary length sentences chosen included summary obtain optimal sentence subset document represents summary quality summary depends choice sentences iii proposed model proposed model based neural network consists input layer hidden layer output layer document fed input layer computations carried hidden layer output generated final layer section talk input vector generation processing taking place network summary generation output neural network sentences document fed input network input neural networks numbers sentences converted represented numerical form purpose model model provides vector representation words english language language model trained large datasets words vocabulary assigned vector fixed dimension based context appears note dimension fixed word model basically tries predict word given context words vectors important properties example closely related words similar representations representative language details vectors generated reader advised refer fig proposed neural network obtaining word vectors vector representation sentences created representation able reflect sentence best possible manner intuitive approaches averaging word vectors turn useful leads poor results lack consideration order relationship words generating meaningful representation kind contextual relation words taken care approach based grams model fasttext library provided facebook convert sentences vectors model takes input sentences english language vector representation words converts sentences fixed dimension vectors case size input layer fixed varied different documents sentences converted fixed dimensional vectors need worry variation length sentences problem remains variation length documents document different length terms number sentences summarizer work sizes approaches recurrent neural networks end end learning proposed proven work lot computation needed models fairly difficult implement instead propose simpler approach summarizing text recursively proposed model performance comparable complex systems pages let number sentences document divide document segments having fixed number sentences segment called page let fixed number way obtain run network sentences page converted corresponding vectors having entries vectors concatenated order form dimension vector fed input layer network pages number sentences input vector padded zeros note fixed model later test model values parameter report results equals softmax activation function applied output layer entry obtained vector denotes weight associated corresponding sentence represents measure belief sentence included summary fig shows schematic representation model supervised learning model correct labels sentences document error loss correct prediction calculated cross entropy predicted output correct hot vector error fed network training weights bias matrices adjusted iteration propagating error optimal value learning rate rate parameters updated repeated experiments obtained fig flow diagram proposed model generation summary given document entire text broken pages summary length terms number sentences fixed known summary generation let number pages fed network input network outputs probability vector sentences chosen summary length generated summary segments concatenated order produce document recursively fed summarizer till number sentences document reduces fig recursive approach able generate summary corresponding original document consists best sentences good representative entire text final output vector corresponding sentences picked document concatenated order produce final summary single document summarization model assumed exceed discussion value available paper experimental setup section explain measured performance proposed network set network parameters optimal performance briefly discuss dataset training evaluation existing state art systems comparison dataset trained proposed model datasets duc datasets dataset raw form consisted xml pages pre processed preprocessing involved converting dataset text documents dataset consisted document summary pairs divided clusters document summaries word summary word summary extractive word summaries purposes training documents rest evaluating model performance evaluated proposed model document dataset dataset consisted duc documents extractive summaries documents train network rest evaluation purposes based evaluation proposed model training sets variants rouge evaluator details evaluation comparison existing models found upcoming sections implementation details tensorflow org uses data implemented proposed model tensorflow library flow graphs tensorflow allowed available hardware flexible portable library correct differentiation capabilities error loss prediction calculated cross entropy function tested values learning rate hidden layer size best combination performance computation time allowed saturating accuracy approximately epochs fixed number sentences parameter denotes number sentences fed network time apart standard implementation mentioned needed set appropriate value parameter tested performance proposed network values parameter entire dataset performance comparison shown tables table dataset varying parameter value performance proposed network score fig performance network dataset parameter table dataset varying parameter value performance proposed network score numsen score numsen score results proposed network achieves maximum performance considering results value set taking document input sentences time gives best results details rouge evaluator refer upcoming sections performance evaluation compared proposed model previously published models known good performance dataset model ilp model operates phrase based representation source document obtained merging information pcfg parse trees dependency graphs integer linear programming formulation model learns select combine phrases subject length coverage grammar constraints model approach based recurrent neural networks shows promising results comparison system tgraph approach based weighted graphical representation documents obtained topic modeling system urank model proposes novel unified approach simultaneous multi document summarizations mutual influences tasks incorporated graph model ranking scores sentence tasks obtained unified ranking process comparison systems tgraph urank produce typical extractive summaries considered state art finally compared proposed model system gene proposed approach presents extraction based single document text summarization technique genetic algorithms single document results section shown proposed model faired existing systems known good performance rouge evaluation purposes rouge stands recall oriented understudy gisting evaluation measure determines quality summary automatically comparing human ideal generated summaries scores allotted counting number overlapping units computer generated ideal summaries variants rouge compared proposed model basis variants table iii higher better rouge score comparison dataset score score model ilp tgraph urank proposed model fig performance network dataset parameter table iii shows proposed model faired compared models mentioned models known good performance dataset based complex approaches use recurrent neural networks sophisticated constraint optimization ilp sentence ranking mechanisms urank approaches hard implement require lot computation hand data driven approach uses simple feedforward neural network implementationally computationally light obtains performance par art systems evident table table gene model precision comparison proposed model document number gene precision proposed model table shows comparison proposed model performance performance genetic algorithm based system gene document dataset performance measure custom precision function demonstrate systems performance table shows performance proposed models precision function results proposed model easily outperforms complex genetic algorithm approach conclusion future work work presented fully data driven approach automatic text summarization proposed evaluated model standard datasets results comparable state art models access linguistic information demonstrated straightforward relatively simpler approach terms implementation memory complexity produce results equivalent complex deep networks sequence based models assumed summary length generated try improve aspect future references cheng jianpeng mirella lapata neural summarization extracting sentences words arxiv preprint nallapati ramesh abstractive text summarization sequence rnns arxiv preprint mihalcea rada paul tarau textrank bringing order text proceedings conference empirical methods natural language processing erkan gnes dragomir radev lexrank graph based lexical centrality salience text summarization journal artificial intelligence research lin chin yew rouge package automatic evaluation summaries text summarization branches proceedings workshop vol wong kam fai mingli wenjie extractive summarization supervised semi supervised learning proceedings international conference computational linguistics volume association computational linguistics bazrfkan mehrnoosh muosa radmanesh machine learning methods summarize persian texts indian sci res freitas kaestner automatic text summarization learning approach brazilian symposium artificial machine intelligence sbia brazil ferreira rafael assessing sentence scoring techniques extractive text summarization expert systems applications gaikwad deepali namrata mahender review paper text summarization international journal advanced research computer communication engineering fachrurrozi novi yusliani rizky utami yoanita frequent term based text summarization bahasa indonesia radev dragomir centroid based summarization multiple documents information processing bojanowski grave joulin mikolov enriching word vectors subword information mikolov tomas distributed representations words phrases compositionality advances neural information processing systems chatterjee niladri amol mittal shubham goyal single document extractive text summarization genetic algorithms emerging applications eait international conference ieee information technology woodsend kristian mirella lapata automatic generation story highlights proceedings annual meeting association computational linguistics association computational linguistics parveen daraksha hans martin ramsl michael strube topical coherence graph based extractive summarization wan xiaojun unified approach simultaneous document multi document summarizations proceedings international conference computational linguistics association computational linguistics joulin armand classification arxiv preprint bag tricks efficient text sarkar kamal mita nasipuri suranjan ghose machine learning medical document summarization international journal database theory application fachrurrozi novi yusliani rizky utami yoanita frequent term based text summarization bahasa indonesia subramaniam manjula vipul dalal test model rich semantic graph representation hindi text abstractive method kaikhah khosrow text summarization neural networks mrs kulkarni text summarization neural networks rhetorical structure theory
