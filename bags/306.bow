taming language gans cautious sampling strategies thomas paul alexis sylvain lamprier benjamin jacopo cnrs france sorbonne universit cnrs paris france recital paris france thomas jacopo paul sylvain lamprier benjamin abstract training regimes based maximum likelihood estimation mle suffer known limitations leading poorly generated text sequences root limitations mismatch training inference called exposure bias exacerbated considering reference texts correct practice alternative formulations good generative ial networks gans mitigate limitations discrete nature text hindered application language generation approaches proposed far based reinforcement learning shown underperform mle departing previous works analyze exploration step gans applied text generation classical sampling results unstable training propose consider alternative exploration strategies gan framework coldgan force sampling close bution modes smoother learning dynamics rst time best knowledge proposed language gans compare favorably mle obtain improvements state art generative tasks unconditional text generation question generation abstractive summarization introduction deep learning approaches paved way signicant achievements natural language generation nlg popular paradigm sequence sequence models trained maximum likelihood estimation mle teacher forcing training neural networks mle succeed modeling sequence probabilities inference model conditioned sequences observed training time generated texts approach degenerate prone repetition nonetheless architectures discriminators able distinguish human machine generated text disconcerting efciency reported values long article generation abstractive summarization generative architectures encoder reach performances supporting hypothesis generation failures decoding step mle training regimes decoding suffers exposure bias lacks sequence level loss optimize mitigate mle limitations reinforcement learning applied text generation tasks considering sequence level metrics bleu rouge reward metrics based grams similarity known poorly correlate human judgments preprint review preserve meaning reinforced models yield poorer tions higher degradation compared mle counterparts overcome drawbacks better rewards necessary end ziegler proposed directly reward systems human judgment approach performs approximates best possible reward obviously viable solution practice attests perfect rewards achieve excellent levels performance natural alternative requiring human judgments frame problem generative adversarial network gan paradigm successfully image generation text modeled sequence discrete symbols naive computation gradients intractable language gans based gradient estimation based techniques reward case extremely sparse discussed section yielding high variance gradient estimation known challenging optimization previous works focused aspect proposed denser rewards unfortunately attempts apply gans text generation obtained limited success found underperform mle known crucial exploration surprisingly understudied applied text generation work propose new exploration method aims sampling structured rewards better suits gans training dynamics allowing rst time successfully train language gans main contributions summarized study discriminators behavior degree specialization important implications exploration stabilize training process particular reducing exploration space essential successfully train discrete gans based observations propose coldgan gan architecture alternative sampling strategies force sampling remain closer distribution modes finally apply proposed methods tasks report positive results compared previous works including gans mle based models related work text generation metrics interest nlp non differentiable approaches text generation knowledge works based text generation use standard sampling policy gradient estimation following current policy generator dene apart text gans suffer aforementioned limitations ill dened reward metrics bleu rouge text gans tackling problem implicitly learning metric discriminator adversarial approaches proposed text generation given high dimension generative action space sparsity associated rewards provided discriminator section large body works focused dening denser rewards ranking comparative discriminators sequential discriminators rewards provided time step generation masked language modeling policy usually learned vanilla policy gradient reinforce exception maligan difculty gans discrete sequential data discriminators inaccurate samples close generator distribution modes training usually scattered space enable specialization useful difcult areas section preliminary experiments cautious standard works proposed ways avoid catastrophic moves policy parameters enforcing new policy stay importance sampling reinforcement learning generally sample ciency purposes policy policy gradient methods allows use previously sampled sequences conversely work employed improve stability text gans closer work maligan proposes rely consider estimation data distribution target objective theoretically appealing stability relies strong assumptions discriminator guarantees rarely hold practice instead propose rely stabilize generator discriminator min max game alternative careful sampling strategies note approach easily included maligan framework discriminators generators interaction generating discriminating text text tasks generator text generation naturally lends autoregressive modeling probability generate sequence composed tokens given learnable parameters generator input sequence neural networks typically produce class probabilities softmax output layer converts logit computed token vocabulary probability exp exp temperature hyper parameter set specied higher temperature uniform probability distribution vocabulary resulting diversity mistakes following note distribution dened generator temperature discriminator generated texts input logistic regression problem following consider discriminator learned sets human set pairs input associated human written text data distribution set pairs generated outputs text text tasks casting nlp task text text problem demonstrated state art results established glue benchmark challenging successor accordingly employ architecture discrimination generation allows fairer comparisons generator discriminator architecture pre training capacity discriminator generator equilibrium exposure bias mentioned discriminator easily predict human machine nature text reason lies exposure bias quantify statement compare results discriminator trained following generation strategies standard generation suffering exposure bias teacher forcing generation truth tokens fed generator expose model prediction generated machine results fig expected discriminators score observe perform standard generation discriminator obtains consistently larger improvements teacher forcing generation discriminator length sequence increases indicate presence exposure bias errors accumulate time relatively high accuracy observed teacher forcing generation suggests additional factors exposure bias involved following extreme specialization discriminators figure accuracy discriminator model trained different generation modes standard subject exposure bias teacher forcing axis corresponds partial length sequence discriminate table probability text human according discriminators dperf ect corresponds theoretical perfect discriminator innite capacity training data corresponds discriminator trained samples generated temperature past past correspond results samples obtained generator weights resumed previous stage training checkpoint epoch nal state section memory replay human past past evaluated dperf ect discriminator free lunch dened temperature generator parameter allows control randomness predictions sampling scaling logits applying softmax dene sampling strategies generator low close temperatures provide samples close sequence sgreedy greedy procedure takes token max generator probability step output beam search beam size high temperatures distribution sequences tends uniform distribution experiment different temperature settings generator trained mle use obtained samples train test discriminator allows evaluate impact differences sampling temperatures training inference discriminator performance words discriminator trained samples obtained specic temperature performs faced samples generated different sampling setups train evaluate discriminators samples generated temperatures conditional generation task summarization section allows consider sequence samples low temperatures report results table expected case discriminators perform better trained evaluated sequences generated temperature mismatch training evaluation samples generated different temperatures observe discriminator fails distinguish human generated ones precisely considers sentences human generated conversely trained different temperatures results balanced robust temperatures yielding drop accuracy consistently known accuracy robustness trade highlights individual discriminators specialized specic generation pairs machine human knowing crucial orient specialization useful areas interestingly trained samples issued discriminator inaccurate identifying samples close sgreedy generated ones equals average samples particularly bad discriminator reward signal process samples lie useful area output distribution correspond samples close modes distribution text generation applications generation accuracystandard generationteacher forcing generation strategies beam search target sequences prediction outputs bad reward function locations likely lead bad generation performance discriminator trained samples close mode bad samples indicating simply use samples train discriminator considering standard sampling generator training rewards inaccurate implications discrete gans holtzman report sampling tail distribution expected happen rst steps decoding probability superior steps unstructured exploration causes large variance grows number time steps perturbs actions frequently random exploration yield better structured sequences lower variance closer distribution learned discriminator likely enable better training dynamics discriminator generator models based ndings seek sampling strategies allow discriminator train useful samples generator trained reliable rewards discriminator policy gradient scheme interested maximizing according generator parameters discriminator updated end training epoch gradient ascent human machine pairs new articial sequences resulting generator distribution order introduce cautious sampling focuses modes distributions note useless consider policy gradient generator distribution modied temperature compared imply rescaling network outputs altering learning process log instead propose employ importance sampling dening cautious sampling strategies text gans based fact distribution function exp case yields following unbiased policy gradient log token sequence subsequence rst tokens generator probability modied sampling distribution enables generation possible sequence tokens given vocabulary work focus exploration stage conversely previous works choose sober form reward predicted human sparse reward limitation sampling strategy close modes distribution provided initial solution good bootstrap according experiments case note trained samples avoid mismatch generator training samples problematic pointed section coldgans exploration temperature plays major role moderating exploration scaling factor applied generator outputs directly denes degree diversity generated sequences default exploration obtained recursively sampling sequence tokens model distribution higher random sampled sequences regardless model policy conversely lower temperatures reduce exploration ultimately equivalent argmax function consider distribution lower colder temperatures allows explore sequences composed tokens likely sampled tail note addition consider sophisticated technique nucleus sampling coldgansnucleus decoding method shown produce higher quality texts previous sampling strategies including temperature based sampling nucleus tokens containing vast majority probability mass approach dynamically truncates unreliable tail probability distribution instance cautious generative process nucleus sampling sequences invalidating avoid propose use mixture combining low temperatures nucleus policies hyper parameter nucleus rescaled temperature described previous paragraph probability nucleus probability importance weight clipping importance weights large causing instability adapting paragraph paper details truncate importance weights add correction term computation log max log rst term clipping importance weight variance gradient estimate bounded second term equation ensures estimate unbiased sampling sequence true policy experiments set note contrary policy clipping proposed case clipping rare occurs sequences probability generator higher sampling distribution designed sampling close mode happens clipping ensures corresponding gradient explode memory replay table observed performance discriminators lower evaluated samples generated previous checkpoint model evaluated past connect failure mode gans observed metz generator discriminator oscillate training converging xed point lifelong learning literature shown experience replay sufcient avoid catastrophic forgetting inspired work construct memory buffer contains samples generated training steps replace discriminator training examples samples buffer allows discriminator remain accurate samples previous state generator preventing failure loop training experiments computational cost large parameters small parameters experiments validation sets hyperparameter selection detail evaluated approach learning reporting results value best performing coldgan conguration perform ablations assess impact memory replay importance weight clipping finally experimented bart instead unconditional language generation previous works language gans evaluated unconditional language generation benchmarks task input provided goal generate meaningful diverse texts consistently measure aspects respectively bleu bleu metrics obtain ner comparison models caccia proposed draw curve negative bleu self bleu sampling temperatures inference allows measure trade quality diversity following news dataset report coldgan results figure left notice comparable performance large fewer parameters com deepmind deepmind research tree master scratchgan statmt org figure results emnlp news dataset metrics lower better scores previous works taken figure relative gains obtained coldgan mle grouped ground truth sequence length table results question generation abstractive summarization summ tasks squad summ cnn params rouge semqg bertsumabs unilm pegasus large mle small mle gan coldgan coldgannucleus coldgannucleus coldgannucleus memory replay weight clipping bart mle coldgannucleus previous works use self supervised pretrained models explains improvement mle baseline theirs mle scratchgan directly compare performances reported previous works study performance variations corresponding mle baseline consistently previous works observe model default exploration gant performs strictly worse mle baseline experimented coldgant training temperature randomly sampled sequence performs better gant compare favorably mle finally coldgant coldgannucleus obtain better results mle entire curve knowledge rst time mle falls short gan based approaches task conditional language generation evaluate coldgan popular tasks text inputs given conditioning generation question generation text summarization highly competitive benchmarks recent state art results achieved mle based pre trained ers answer aware question generation task given text target answer goal generate relevant question following previous works squad dataset automatic summarization aims produce concise uent summaries given longer text popular cnn dataset corpus containing news articles corresponding abstractive summaries conditional text generation tasks output sequences commonly evaluated bleu machine translation question generation rouge summarization metrics contrast unconditioned scenario diversity linked variety inputs common practice decode beam search inference work small previous work mle truth gain coldgannucleus table human evaluation coldgan corresponds bart trained coldgannucleus tailed test results reported model compared human fluency relevance answerability human bart mle coldgan figure probability generated text human according cnn results tasks data evaluation metrics released dong results shown table consistent tasks observe exploring default temperature yields poor performances coldgan compare favorably mle best performance achieved experiment emphasizing coldgannucleus exploration independent training runs observed stable results model standard deviation average lower test set finally applied coldgan setup bart achieving new state art summarization rouge mitigating exposure bias figure report relative gain obtained terms small best conguration coldgannucleus corresponding mle baseline axis gives length considered ground truth target sequences observe longer target sequence coldgan outperforms mle indicate coldgan successfully mitigate exposure bias human evaluation discussed section automatic metrics known suffer key limitations additionally conducted human evaluation task professional english speakers asked judge likert scale extent generated questions posed natural fluency relevant context relevance answerable looking context answer answerability results table surprisingly mle bart coldgan outperform ground truth fluency similar result reported yoon refer table paper plausible explanation humans inclined use informal language grammar mistakes instance human question yellow cabs operate new york sounds slightly formal generated coldgan yellow taxicabs manhattan compared mle coldgan enables signicantly improve term uency remaining competitive metrics consistently experiments exposure bias adversarial training curves figure shows evolution training different setups probability generated text human according discriminator consistently table coldgannucleus appears adverse discriminator conversely regular gan adversarial comparatively perturbed conclusion proposed coldgan novel approach able tame exploration language gans allowing obtain performance improvements conditional unconditional text generation mle based training proposed method makes compatible advanced sampling methods nucleus future decoding methods future plan combine coldgan orthogonal approaches proposed previous works denser rewards com microsoft unilm tree master unilm probability broader impact fluent reliable natural language generation signicant societal impacts hand envision applications benecial business research education automatic summarization news papers books efcient information access automatic alized student evaluation tests trough question generation responsive conversational interfaces hand malicious actors use technology build tools detrimental society creation propagation misleading fake news discussed impersonation deceit nonetheless keeping research open public scrutiny arguably best ways defend actors implementation details models implemented pytext single rtx gpu experiments conducted small million parameters generator discriminator rst trained corresponding task mle small underperforms larger version billion parameters bart performs parameters task chose train bart following dure best set hyper parameters found small coldgannucleus bart conditional text generation applied inference beam search bart recommended epoch train coldgan takes hours hours bart references ahmed aly kushal lakhotia shicong zhao mrinal mohit barlas oguz abhinav arora sonal gupta christopher dewan stef nelson lindall rushin shah pytext seamless path nlp research production arxiv preprint samy bengio oriol vinyals navdeep jaitly noam shazeer scheduled sampling sequence prediction recurrent neural networks advances neural information processing systems pages andrew brock jeff donahue karen simonyan large scale gan training high delity natural image synthesis arxiv preprint sbastien bubeck eric price ilya razenshteyn adversarial examples tational constraints arxiv preprint massimo caccia lucas caccia william fedus hugo larochelle joelle pineau rent charlin language gans falling short international conference learning representations asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents abstractive summarization proceedings conference north ican chapter association computational linguistics human language technologies volume long papers pages tong che yanran ruixiang zhang devon hjelm wenjie yangqiu song yoshua bengio maximum likelihood augmented discrete generative adversarial networks arxiv preprint nina dethlefs heriberto cuayhuitl hierarchical reinforcement learning tive text generation proceedings international natural language generation conference pages association computational linguistics dong nan yang wenhui wang furu wei xiaodong liu wang jianfeng gao ming zhou hsiao wuen hon unied language model pre training natural language understanding generation advances neural information processing systems pages com google research text text transfer transformer xinya junru shao claire cardie learning ask neural question generation reading comprehension proceedings annual meeting association computational linguistics volume long papers pages william fedus ian goodfellow andrew dai maskgan better text generation filling international conference learning representations jakob foerster nantas nardelli gregory farquhar triantafyllos afouras philip torr pushmeet kohli shimon whiteson stabilising experience replay deep agent reinforcement learning proceedings international conference machine learning volume pages jmlr org justin gilmer luke metz fartash faghri samuel schoenholz maithra raghu martin wattenberg ian goodfellow adversarial spheres ian goodfellow jean pouget abadie mehdi mirza bing david warde farley sherjil ozair aaron courville yoshua bengio generative adversarial nets advances neural information processing systems pages jiaxian guo sidi han cai weinan zhang yong jun wang long text generation adversarial training leaked information thirty second aaai conference articial intelligence geoffrey hinton oriol vinyals jeff dean distilling knowledge neural network arxiv preprint ari holtzman jan buys maxwell forbes yejin choi curious case neural text degeneration arxiv preprint jens kober jan peters policy search motor primitives robotics advances neural information processing systems pages mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy ves stoyanov luke zettlemoyer bart denoising sequence sequence pre training natural language generation translation comprehension arxiv preprint jiwei monroe tianlin shi sbastien jean alan ritter dan jurafsky adversarial learning neural dialogue generation proceedings conference empirical methods natural language processing pages kevin lin dianqi xiaodong zhengyou zhang ming ting sun adversarial ranking language generation advances neural information processing systems pages yang liu mirella lapata text summarization pretrained encoders ings conference empirical methods natural language processing international joint conference natural language processing emnlp ijcnlp pages cyprien masson dautume shakir mohamed mihaela rosca jack rae training language gans scratch advances neural information processing systems pages cyprien masson dautume sebastian ruder lingpeng kong dani yogatama episodic memory lifelong language learning advances neural information processing systems pages luke metz ben poole david pfau jascha sohl dickstein unrolled generative adversarial networks arxiv preprint ramesh nallapati bowen zhou caglar gulcehre bing xiang abstractive text summarization sequence sequence rnns arxiv preprint renato negrinho matthew gormley geoffrey gordon learning beam search policies imitation learning advances neural information processing systems pages jekaterina novikova ondrej duek amanda cercas curry verena rieser need new evaluation metrics nlg proceedings conference pirical methods natural language processing pages copenhagen denmark association computational linguistics kishore papineni salim roukos todd ward wei jing zhu bleu method automatic evaluation machine translation proceedings annual meeting association computational linguistics pages association computational linguistics romain paulus caiming xiong richard socher deep reinforced model abstractive summarization arxiv preprint alec radford jeffrey rewon child david luan dario amodei ilya sutskever language models unsupervised multitask learners openai blog colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei peter liu exploring limits transfer learning unied text text transformer arxiv preprint pranav rajpurkar jian zhang konstantin lopyrev percy liang squad proceedings conference questions machine comprehension text empirical methods natural language processing pages marcaurelio ranzato sumit chopra michael auli wojciech zaremba sequence level training recurrent neural networks arxiv preprint thomas rckstie martin felder jrgen schmidhuber state dependent ration policy gradient methods joint european conference machine learning knowledge discovery databases pages springer john schulman sergey levine pieter abbeel michael jordan philipp moritz trust region policy optimization international conference machine learning pages john schulman filip wolski prafulla dhariwal alec radford oleg klimov mal policy optimization algorithms arxiv preprint thomas scialom paul alexis dray sylvain lamprier benjamin piwowarski jacopo staiano discriminative adversarial search abstractive summarization arxiv preprint stanislau semeniuta aliaksei severyn sylvain gelly accurate evaluation gans language generation arxiv preprint elior sulem omri abend ari rappoport bleu suitable evaluation text simplication proceedings conference empirical methods natural language processing pages ilya sutskever oriol vinyals quoc sequence sequence learning neural networks advances neural information processing systems pages richard sutton andrew barto reinforcement learning introduction mit press guy tevet gavriel habib vered shwartz jonathan berant evaluating text gans language models arxiv preprint philip thomas emma brunskill data efcient policy policy evaluation reinforcement learning international conference machine learning pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need advances neural information processing systems pages alex wang yada pruksachatkun nikita nangia amanpreet singh julian michael felix hill omer levy samuel bowman superglue stickier benchmark general purpose language understanding systems advances neural information processing systems pages alex wang amanpreet singh julian michael felix hill omer levy samuel bowman glue multi task benchmark analysis platform natural language understanding proceedings emnlp workshop blackboxnlp analyzing interpreting neural networks nlp pages brussels belgium association computational linguistics ziyu wang victor bapst nicolas heess volodymyr mnih remi munos koray kavukcuoglu nando freitas sample efcient actor critic experience replay sean welleck ilia kulikov stephen roller emily dinan kyunghyun cho jason weston neural text generation unlikelihood training arxiv preprint ronald williams simple statistical gradient following algorithms connectionist reinforcement learning machine learning ronald williams david zipser learning algorithm continually running fully recurrent neural networks neural computation wonjin yoon yoon sun yeo minbyul jeong bong jun jaewoo kang learning semantic similarity makes abstractive summarization better arxiv preprint lantao weinan zhang jun wang yong seqgan sequence generative adversarial nets policy gradient arxiv prints page arxiv preprint rowan zellers ari holtzman hannah rashkin yonatan bisk ali farhadi franziska roesner yejin choi defending neural fake news advances neural information processing systems pages jingqing zhang yao zhao mohammad saleh peter liu pegasus pre training extracted gap sentences abstractive summarization arxiv preprint shiyue zhang mohit bansal addressing semantic drift question generation semi supervised question answering arxiv preprint yizhe zhang zhe gan kai fan zhi chen ricardo henao dinghan shen lawrence carin adversarial feature matching text generation proceedings international conference machine learning volume pages jmlr org qingyu zhou nan yang furu wei chuanqi tan hangbo bao ming zhou neural question generation text preliminary study national ccf conference natural language processing chinese computing pages springer wangchunshu zhou tao furu wei ming zhou self adversarial learning comparative discrimination text generation international conference learning representations yaoming zhu sidi lei zheng jiaxian guo weinan zhang jun wang yong texygen benchmarking platform text generation models international acm sigir conference research development information retrieval pages daniel ziegler nisan stiennon jeffrey tom brown alec radford dario amodei paul christiano geoffrey irving fine tuning language models human preferences arxiv preprint
