pragmatically informative text generation sheng shen daniel fried jacob andreas dan klein computer science division uc berkeley computer science articial intelligence laboratory mit sheng dfried edu edu r l c s c v v x r abstract improve informativeness models conditional text generation techniques computational pragmatics niques formulate language production game speakers listeners speaker generate output text tener use correctly identify original input text describes proaches widely cognitive science grounded language learning ceived attention standard guage generation tasks consider matic modeling methods text generation pragmatics imposed tion preservation ics imposed explicit modeling tors nd methods improve performance strong existing systems stractive summarization generation structured meaning representations introduction computational approaches pragmatics cast guage generation interpretation theoretic bayesian inference procedures land et al frank goodman approaches capable modeling variety pragmatic phenomena main plication natural language processing improve informativeness generated text grounded language learning problems monroe et al paper matic reasoning similarly improve performance traditional language tion tasks like generation structured meaning representations figure summarization work builds line learned rational speech acts rsa models monroe potts andreas klein erated strings selected optimize input meaning representation shop human written cheap coffee shop riverside customer rating fitzbillies fitzbillies family friendly serves english food base sequence sequence model fitzbillies family friendly coffee shop located near river distractor based pragmatic system sd fitzbillies family friendly coffee shop serves english food located riverside area customer rating cheap reconstructor based pragmatic system sr fitzbillies family friendly coffee shop serves cheap english food riverside area customer rating figure example outputs systems generation task base sequence sequence model sec fails describe attributes input meaning representation matic systems sr sec sd sec human written reference ior embedded listener model cal presentation rsa framework frank goodman grounded reference tion models speakers attempt describe erents presence distractors models listeners attempt resolve descriptors erents recent work extended models complex groundings including images mao et al trajectories fried et al techniques settings similar primary intuition rsa framework preserved speaker s perspective good description picks tively possible content speaker intends listener identify outside grounding cognitive modeling frank et al targeted analysis guistic phenomena orita et al rational speech acts models seen limited application natural language processing literature work extended distinct class language generation lems use referents structured descriptions lingustic content natural language texts accordance maxim tity grice q principle horn pragmatic approaches naturally correct formativeness problems observed state art language generation systems figure present experiments language ation tasks generation meaning tions novikova et al summarization task evaluate models ics reconstructor based model fried et al distractor based model gordon et al models improve formance tasks increasing rouge scores points cnn daily mail stractive summarization dataset bleu scores points end end generation dataset obtaining new state art results tasks formulate conditional generation task ing input space possible inputs e input sentences abstractive tion meaning representations structured eration producing output o sequence tokens ot build pragmatic approaches learned base speaker els produce probability distribution output text given input cus conditional generation tasks information input context largely preserved output text apply matic procedures outlined sec task models use systems past work strong mative relative human reference outputs e figure meaning representations rst task eration structured meaning representations mrs containing attribute value pairs novikova et al example shown figure systems generate description restaurant specied attributes ply pragmatics encourage output strings input mr identied model use publicly released neural ation system puzikov gurevych achieves comparable performance best lished results dusek et al abstractive summarization second task multi sentence document summarization vast past work tion nenkova mckeown recent ral models large datasets e hermann et al train models extractive cheng lapata nallapati et al abstractive rush et al et al settings works build cent abstractive neural summarization system chen bansal system uses sentence level extractive model rnn ext tify sequence salient sentences source document second system uses abstractive model abs rewrite output concatenated duce nal summary rely xed ext model extract sentences inputs pragmatic procedure abs model applying pragmatics stractive step pragmatic models produce informative outputs consider matic methods extend base speaker els listener models l produce distribution o possible inputs given output listener models derive matic speakers produce output high probability making listener model l identify correct input large space possible choices designing l deriving follow lines past work categorize reconstructor based distractor based tailor matic methods tasks ing reconstructor models methods choosing distractors reconstructor based pragmatics pragmatic approaches category dusek jurccek fried et al rely structor listener model lr dened independently speaker listener model produces tribution o possible input contexts given output description use sequence sequence structured classication models lr described train models data supervise models listener model base speaker model dene pragmatic speaker output score given o sr rationality parameter controls model optimizes discriminative outputs monroe et al fried et al discussion select output text sequence o given input choosing highest scoring output eq set candidates obtained beam search meaning representations construct lr meaning representation generation task multi task multi class classier dening tribution possible values attribute mr attribute prediction layer attention based aggregation layer ditions basic encoding o shared attributes appendix architecture tails dene o joint ability predicting input mr attributes o summarization construct lr rization train abs model type use chen bansal reverse e taking input sentence summary producing sentence source document train lr heuristically extracted aligned source document sentences train chen bansal distractor based pragmatics pragmatic approaches category frank goodman andreas klein tam et al cohn gordon et al derive pragmatic behavior producing outputs tinguish input alternate distractor inputs construct distractor given input task dependent way follow approach cohn gordon et al outlined briey base speakers build produce outputs incrementally probability ot word output time t conditioned input previously erated words t output generated incrementally separate tasks contrastive captioning referring pression generation distractors given tional generation task pragmatic behavior obtained constructing selecting single tor contrasts input listener model needs condition entire output decisions distractor based approach able pragmatic decisions word choosing entire output dates reconstructor approaches listener ld pragmatic speaker sd derived base speaker belief tribution pt maintained timestep t possible inputs d t t ot t o t sd t t rationality parameter initial belief distribution uniform e eqs normalized true input distractor eq malized output vocabulary construct output text sequence pragmatic speaker sd incrementally beam search mately maximize eq meaning representations distractor mr automatically constructed input distinctive possible input construct distractor masking present input attribute replacing value present attribute value quent attribute training data ample input mr figure distractor king summarization extracted input tence use previous extracted sentence document distractor rst sentence use tor intended encourage outputs contain distinctive information maries produced document experiments conditional generation tasks evaluate standard benchmark dataset following past work automatic tion human produced reference text choose hyperparameters models beam size parameters maximize task metrics dataset s development set pendix settings code publicly available com sincerass system bleu nist meteor r l cider system r l meteor t gen best prev sr sd table test results generation task comparison t gen baseline dusek jurccek best results challenge ported dusek et al juraska et al gurevych et al gong bold highest performing model metric previous work outperforms models meaning representations evaluate task generation meaning representations containing restaurant tributes novikova et al report task s ve automatic metrics bleu papineni et al nist doddington meteor lavie agarwal rouge l lin cider vedantam et al table compares performance base pragmatic models baseline t gen system dusek jurccek best previous result primary systems uated challenge dusek et al systems obtaining results encompass range approaches template system puzikov gurevych neural model zhang et al models trained reinforcement learning gong systems ensembling reranking juraska et al ensure benet reconstructor based pragmatic proach uses models solely model combination effect compare ensemble base models ensemble uses weighted combination scores independently trained models ing eq weights tuned development data pragmatic systems improve strong baseline system ve metrics largest improvements bleu nist meteor rouge l cider sr model sr model outperforms ous best results obtained system challenge bleu nist cider rable performance meteor rouge l extractive inputs abstractive sr sd best previous test results table non anonymized cnn daily mail summarization task compare extractive baselines best previous abstractive results celikyilmaz et al paulus et al bansal bold highest performing model metric previous work outperforms models abstractive summarization evaluate cnn daily mail tion dataset hermann et al nallapati et al et al s non anonymized preprocessing previous work chen bansal evaluate rouge teor table compares pragmatic systems base model scores taken chen bansal obtained comparable mance ensemble base models best previous stractive summarization result metric dataset celikyilmaz et al paulus et al chen bansal report extractive baselines uses rst sentences document summary et al inputs concatenation extracted sentences inputs els e pragmatic methods obtain improvements rouge scores meteor base model distractor based approach sd outperforming based approach sr strong rics obtaining results competitive best vious abstractive systems sd use retrained versions chen bansal s sentence extractor abstractive models ments n gram reranking based inference cedure replacing scores base model scores sr respective pragmatic procedures sd coverage ratio attribute area et food pr ff cr sd sd sd sd sd sd r t t r o t c r t s d coverage ratios attribute type base model pragmatic models sr pragmatic models typically improve coverage ratios attribute types compared base model sd coverage ratios attribute type columns base model pragmatic system sd constructing distractor masking specied attribute rows cell colors degree coverage ratio increases green decreases red relative figure coverage ratios task attribute type estimating frequently values attribute input meaning representations mentioned output text analysis base speaker model mative e task failing mention certain attributes mr training examples incorporate better understand performance improvements pragmatic models compute coverage ratio proxy measure content input preserved generated outputs coverage ratio attribute fraction times exact match text generated output attribute s value source mr instances attribute specied figure shows coverage ratio attribute category models sr model increases coverage ratio compared attributes showing reconstruction model score select outputs lead crease mentions attribute coverage ratios increase sd categories increase typically produced sr sd optimizes explicitly attribute mentions sr provides potential method control generated outputs choosing alternate distractors figure shows age ratios sd masking single tribute distractor highest coverage ratio attribute usually obtained ing attribute distractor mr entries main diagonal underlined particular familyfriendly ff food pricerange measure roughly provides lower bound model s actual informativeness attribute measure assign credit paraphrases pr area masking single tribute results decreasing erage ratio observe substantial creases masking attributes ing familyfriendly ing cr produces equal increase coverage ratio customerrating attribute reect underlying correlations training data attributes small number possible values respectively conclusion results models previous work strong imperfectly capture behavior people exhibit generating text explicit pragmatic modeling procedure improve results pragmatic methods uated paper encourage prediction puts identify inputs reconstructing inputs entirety guishing true inputs distractors haps unsurprising methods produce ilar improvements performance future work allow ner grained modeling tradeoff informativity sequence generation pipeline e learned communication cost model explore tions pragmatics content selection earlier generation pipeline acknowledgments thanks reuben cohn gordon helpful discussions suggestions work ported darpa xai program df supported tencent ai lab fellowship ffetfoodprareacrmeaning representation references jacob andreas dan klein reasoning pragmatics neural listeners speakers proceedings conference empirical ods natural language processing dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly arxiv preprint learning align translate asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents proceedings abstractive summarization annual meeting north american ter association computational linguistics pages new orleans louisiana ation computational linguistics yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proceedings annual meeting association computational linguistics jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings annual meeting ciation computational linguistics pages berlin germany association tional linguistics kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase representations rnn encoder decoder proceedings statistical machine translation conference empirical methods ral language processing pages doha qatar association computational linguistics reuben cohn gordon noah goodman chris potts pragmatically informative image tioning character level reference ings annual meeting north american chapter association computational guistics george doddington automatic evaluation machine translation quality n gram occurrence statistics proceedings second international conference human language nology research pages morgan mann publishers inc ondrej dusek filip jurccek sequence sequence generation spoken dialogue deep syntax trees strings acl ondrej dusek jekaterina novikova verena rieser findings nlg challenge ceedings international conference natural language generation michael c frank noah d goodman dicting pragmatic reasoning language games ence michael c frank noah d goodman peter lai informative joshua b tenenbaum nication word production word learning proceedings annual conference cognitive science society pages daniel fried jacob andreas dan klein unied pragmatic models generating ing instructions proceedings annual ing north american chapter tion computational linguistics dave golland percy liang dan klein game theoretic approach generating spatial scriptions proceedings conference empirical methods natural language ing pages association computational linguistics heng gong technical report nlg lenge nlg challenge system descriptions herbert p grice logic conversation karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend advances ral information processing systems laurence horn new taxonomy pragmatic inference q based based ture meaning form use context linguistic applications juraj juraska panagiotis karagiannis kevin bowden marilyn walker deep ensemble model slot alignment sequence sequence ral language generation proceedings nual meeting north american chapter association computational linguistics pages new orleans louisiana association computational linguistics alon lavie abhaya agarwal meteor automatic metric mt evaluation high levels correlation human judgments ings second workshop statistical machine translation pages prague czech lic association computational linguistics chin yew lin rouge package automatic text summarization evaluation summaries branches proceedings shop junhua mao jonathan huang alexander toshev oana camburu alan yuille kevin murphy generation comprehension unambiguous ject descriptions arxiv preprint monroe robert xd hawkins noah d goodman christopher potts colors context pragmatic neural model grounded language derstanding transactions association computational linguistics abigail peter j liu christopher d manning point summarization proceedings annual generator networks meeting association computational guistics pages vancouver canada sociation computational linguistics ramakrishna vedantam samy bengio kevin murphy devi parikh gal chechik context aware captions context agnostic supervision ramakrishna vedantam c lawrence zitnick devi parikh cider consensus based proceedings age description evaluation conference computer vision pattern nition biao zhang jing yang qian lin jinsong su attention regularized sequence sequence learning nlg challenge nlg lenge system descriptions monroe jennifer hu andrew jong pher potts generating bilingual pragmatic proceedings annual color references meeting north american chapter sociation computational linguistics monroe christopher potts learning rational speech acts model proceedings amsterdam colloquium amsterdam illc ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments proceedings meeting ation advancement articial intelligence ramesh nallapati bowen zhou cicero dos santos caglar gulcehre bing xiang tive text summarization sequence sequence rnns signll conference putational natural language learning pages berlin germany association tional linguistics ani nenkova kathleen mckeown matic summarization foundations trends information retrieval jekaterina novikova ondrej dusek verena rieser dataset new challenges end end generation proceedings annual dial meeting discourse dialogue naho orita eliana vornov naomi feldman hal daume iii discourse affects ers choice referring expressions proceedings annual meeting association computational linguistics international joint conference natural language processing volume long papers volume pages kishore papineni salim roukos todd ward jing zhu bleu method automatic proceedings uation machine translation annual meeting association tational linguistics pages association computational linguistics romain paulus caiming xiong richard socher deep reinforced model abstractive proceedings international marization conference learning representations volume yevgeniy puzikov iryna gurevych nlg challenge neural models vs templates proceedings international conference ural language generation alexander m rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages lisbon portugal association computational linguistics supplemental material reconstructor model details reconstructor based speaker task rst follow data preprocessing steps puzikov gurevych cludes delexicalization module deals sparsely occurring mr attributes near mapping values placeholder tokens mrs possible values attributes attributes fewer seven unique values remaining attributes near handled sd delexicalized placeholders ing puzikov gurevych way reconstructor needs predict presence attributes boolean variable attributes corresponding categorical variable use layer bi directional gru cho et al shared sentence encoder concatenate latent vectors tions construct bi directional encoded vector hi single word vector hi gru hi gru hi hi hi l words contribute equally ing mr attribute use attention mechanism bahdanau et al determine importance single word gated sentence vector task k calculated hi k hj hi l task specic sentence representation input k layers softmax outputs turning probability vector y k k mr attributes hyperparameters structured generation use beam size tuned maximize normalized average ve metrics velopment set abstractive summarization use beam size tuned maximize rouge l development set
