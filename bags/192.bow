pragmatically informative text generation sheng shen daniel fried jacob andreas dan klein computer science division berkeley computer science articial intelligence laboratory mit sheng dfried edu edu abstract improve informativeness models conditional text generation techniques computational pragmatics niques formulate language production game speakers listeners speaker generate output text tener use correctly identify original input text describes proaches widely cognitive science grounded language learning ceived attention standard guage generation tasks consider matic modeling methods text generation pragmatics imposed tion preservation ics imposed explicit modeling tors methods improve performance strong existing systems stractive summarization generation structured meaning representations introduction computational approaches pragmatics cast guage generation interpretation theoretic bayesian inference procedures land frank goodman approaches capable modeling variety pragmatic phenomena main plication natural language processing improve informativeness generated text grounded language learning problems monroe paper matic reasoning similarly improve performance traditional language tion tasks like generation structured meaning representations figure summarization work builds line learned rational speech acts rsa models monroe potts andreas klein erated strings selected optimize input meaning representation shop human written cheap coffee shop riverside customer rating fitzbillies fitzbillies family friendly serves english food base sequence sequence model fitzbillies family friendly coffee shop located near river distractor based pragmatic system fitzbillies family friendly coffee shop serves english food located riverside area customer rating cheap reconstructor based pragmatic system fitzbillies family friendly coffee shop serves cheap english food riverside area customer rating figure example outputs systems generation task base sequence sequence model sec fails describe attributes input meaning representation matic systems sec sec human written reference ior embedded listener model cal presentation rsa framework frank goodman grounded reference tion models speakers attempt describe erents presence distractors models listeners attempt resolve descriptors erents recent work extended models complex groundings including images mao trajectories fried techniques settings similar primary intuition rsa framework preserved speaker perspective good description picks tively possible content speaker intends listener identify outside grounding cognitive modeling frank targeted analysis guistic phenomena orita rational speech acts models seen limited application natural language processing literature work extended distinct class language generation lems use referents structured descriptions lingustic content natural language texts accordance maxim tity grice principle horn pragmatic approaches naturally correct formativeness problems observed state art language generation systems figure present experiments language ation tasks generation meaning tions novikova summarization task evaluate models ics reconstructor based model fried distractor based model gordon models improve formance tasks increasing rouge scores points cnn daily mail stractive summarization dataset bleu scores points end end generation dataset obtaining new state art results tasks formulate conditional generation task ing input space possible inputs input sentences abstractive tion meaning representations structured eration producing output sequence tokens build pragmatic approaches learned base speaker els produce probability distribution output text given input cus conditional generation tasks information input context largely preserved output text apply matic procedures outlined sec task models use systems past work strong mative relative human reference outputs figure meaning representations rst task eration structured meaning representations mrs containing attribute value pairs novikova example shown figure systems generate description restaurant specied attributes ply pragmatics encourage output strings input identied model use publicly released neural ation system puzikov gurevych achieves comparable performance best lished results dusek abstractive summarization second task multi sentence document summarization vast past work tion nenkova mckeown recent ral models large datasets hermann train models extractive cheng lapata nallapati abstractive rush settings works build cent abstractive neural summarization system chen bansal system uses sentence level extractive model rnn ext tify sequence salient sentences source document second system uses abstractive model abs rewrite output concatenated duce nal summary rely xed ext model extract sentences inputs pragmatic procedure abs model applying pragmatics stractive step pragmatic models produce informative outputs consider matic methods extend base speaker els listener models produce distribution possible inputs given output listener models derive matic speakers produce output high probability making listener model identify correct input large space possible choices designing deriving follow lines past work categorize reconstructor based distractor based tailor matic methods tasks ing reconstructor models methods choosing distractors reconstructor based pragmatics pragmatic approaches category dusek jurccek fried rely structor listener model dened independently speaker listener model produces tribution possible input contexts given output description use sequence sequence structured classication models described train models data supervise models listener model base speaker model dene pragmatic speaker output score given rationality parameter controls model optimizes discriminative outputs monroe fried discussion select output text sequence given input choosing highest scoring output set candidates obtained beam search meaning representations construct meaning representation generation task multi task multi class classier dening tribution possible values attribute attribute prediction layer attention based aggregation layer ditions basic encoding shared attributes appendix architecture tails dene joint ability predicting input attributes summarization construct rization train abs model type use chen bansal reverse taking input sentence summary producing sentence source document train heuristically extracted aligned source document sentences train chen bansal distractor based pragmatics pragmatic approaches category frank goodman andreas klein tam cohn gordon derive pragmatic behavior producing outputs tinguish input alternate distractor inputs construct distractor given input task dependent way follow approach cohn gordon outlined briey base speakers build produce outputs incrementally probability word output time conditioned input previously erated words output generated incrementally separate tasks contrastive captioning referring pression generation distractors given tional generation task pragmatic behavior obtained constructing selecting single tor contrasts input listener model needs condition entire output decisions distractor based approach able pragmatic decisions word choosing entire output dates reconstructor approaches listener pragmatic speaker derived base speaker belief tribution maintained timestep possible inputs rationality parameter initial belief distribution uniform eqs normalized true input distractor malized output vocabulary construct output text sequence pragmatic speaker incrementally beam search mately maximize meaning representations distractor automatically constructed input distinctive possible input construct distractor masking present input attribute replacing value present attribute value quent attribute training data ample input figure distractor king summarization extracted input tence use previous extracted sentence document distractor rst sentence use tor intended encourage outputs contain distinctive information maries produced document experiments conditional generation tasks evaluate standard benchmark dataset following past work automatic tion human produced reference text choose hyperparameters models beam size parameters maximize task metrics dataset development set pendix settings code publicly available com sincerass system bleu nist meteor cider system meteor gen best prev table test results generation task comparison gen baseline dusek jurccek best results challenge ported dusek juraska gurevych gong bold highest performing model metric previous work outperforms models meaning representations evaluate task generation meaning representations containing restaurant tributes novikova report task automatic metrics bleu papineni nist doddington meteor lavie agarwal rouge lin cider vedantam table compares performance base pragmatic models baseline gen system dusek jurccek best previous result primary systems uated challenge dusek systems obtaining results encompass range approaches template system puzikov gurevych neural model zhang models trained reinforcement learning gong systems ensembling reranking juraska ensure benet reconstructor based pragmatic proach uses models solely model combination effect compare ensemble base models ensemble uses weighted combination scores independently trained models ing weights tuned development data pragmatic systems improve strong baseline system metrics largest improvements bleu nist meteor rouge cider model model outperforms ous best results obtained system challenge bleu nist cider rable performance meteor rouge extractive inputs abstractive best previous test results table non anonymized cnn daily mail summarization task compare extractive baselines best previous abstractive results celikyilmaz paulus bansal bold highest performing model metric previous work outperforms models abstractive summarization evaluate cnn daily mail tion dataset hermann nallapati non anonymized preprocessing previous work chen bansal evaluate rouge teor table compares pragmatic systems base model scores taken chen bansal obtained comparable mance ensemble base models best previous stractive summarization result metric dataset celikyilmaz paulus chen bansal report extractive baselines uses rst sentences document summary inputs concatenation extracted sentences inputs els pragmatic methods obtain improvements rouge scores meteor base model distractor based approach outperforming based approach strong rics obtaining results competitive best vious abstractive systems use retrained versions chen bansal sentence extractor abstractive models ments gram reranking based inference cedure replacing scores base model scores respective pragmatic procedures coverage ratio attribute area food coverage ratios attribute type base model pragmatic models pragmatic models typically improve coverage ratios attribute types compared base model coverage ratios attribute type columns base model pragmatic system constructing distractor masking specied attribute rows cell colors degree coverage ratio increases green decreases red relative figure coverage ratios task attribute type estimating frequently values attribute input meaning representations mentioned output text analysis base speaker model mative task failing mention certain attributes training examples incorporate better understand performance improvements pragmatic models compute coverage ratio proxy measure content input preserved generated outputs coverage ratio attribute fraction times exact match text generated output attribute value source instances attribute specied figure shows coverage ratio attribute category models model increases coverage ratio compared attributes showing reconstruction model score select outputs lead crease mentions attribute coverage ratios increase categories increase typically produced optimizes explicitly attribute mentions provides potential method control generated outputs choosing alternate distractors figure shows age ratios masking single tribute distractor highest coverage ratio attribute usually obtained ing attribute distractor entries main diagonal underlined particular familyfriendly food pricerange measure roughly provides lower bound model actual informativeness attribute measure assign credit paraphrases area masking single tribute results decreasing erage ratio observe substantial creases masking attributes ing familyfriendly ing produces equal increase coverage ratio customerrating attribute reect underlying correlations training data attributes small number possible values respectively conclusion results models previous work strong imperfectly capture behavior people exhibit generating text explicit pragmatic modeling procedure improve results pragmatic methods uated paper encourage prediction puts identify inputs reconstructing inputs entirety guishing true inputs distractors haps unsurprising methods produce ilar improvements performance future work allow ner grained modeling tradeoff informativity sequence generation pipeline learned communication cost model explore tions pragmatics content selection earlier generation pipeline acknowledgments thanks reuben cohn gordon helpful discussions suggestions work ported darpa xai program supported tencent lab fellowship ffetfoodprareacrmeaning representation references jacob andreas dan klein reasoning pragmatics neural listeners speakers proceedings conference empirical ods natural language processing dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly arxiv preprint learning align translate asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents proceedings abstractive summarization annual meeting north american ter association computational linguistics pages new orleans louisiana ation computational linguistics yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proceedings annual meeting association computational linguistics jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings annual meeting ciation computational linguistics pages berlin germany association tional linguistics kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase representations rnn encoder decoder proceedings statistical machine translation conference empirical methods ral language processing pages doha qatar association computational linguistics reuben cohn gordon noah goodman chris potts pragmatically informative image tioning character level reference ings annual meeting north american chapter association computational guistics george doddington automatic evaluation machine translation quality gram occurrence statistics proceedings second international conference human language nology research pages morgan mann publishers inc ondrej dusek filip jurccek sequence sequence generation spoken dialogue deep syntax trees strings acl ondrej dusek jekaterina novikova verena rieser findings nlg challenge ceedings international conference natural language generation michael frank noah goodman dicting pragmatic reasoning language games ence michael frank noah goodman peter lai informative joshua tenenbaum nication word production word learning proceedings annual conference cognitive science society pages daniel fried jacob andreas dan klein unied pragmatic models generating ing instructions proceedings annual ing north american chapter tion computational linguistics dave golland percy liang dan klein game theoretic approach generating spatial scriptions proceedings conference empirical methods natural language ing pages association computational linguistics heng gong technical report nlg lenge nlg challenge system descriptions herbert grice logic conversation karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend advances ral information processing systems laurence horn new taxonomy pragmatic inference based based ture meaning form use context linguistic applications juraj juraska panagiotis karagiannis kevin bowden marilyn walker deep ensemble model slot alignment sequence sequence ral language generation proceedings nual meeting north american chapter association computational linguistics pages new orleans louisiana association computational linguistics alon lavie abhaya agarwal meteor automatic metric evaluation high levels correlation human judgments ings second workshop statistical machine translation pages prague czech lic association computational linguistics chin yew lin rouge package automatic text summarization evaluation summaries branches proceedings shop junhua mao jonathan huang alexander toshev oana camburu alan yuille kevin murphy generation comprehension unambiguous ject descriptions arxiv preprint monroe robert hawkins noah goodman christopher potts colors context pragmatic neural model grounded language derstanding transactions association computational linguistics abigail peter liu christopher manning point summarization proceedings annual generator networks meeting association computational guistics pages vancouver canada sociation computational linguistics ramakrishna vedantam samy bengio kevin murphy devi parikh gal chechik context aware captions context agnostic supervision ramakrishna vedantam lawrence zitnick devi parikh cider consensus based proceedings age description evaluation conference computer vision pattern nition biao zhang jing yang qian lin jinsong attention regularized sequence sequence learning nlg challenge nlg lenge system descriptions monroe jennifer andrew jong pher potts generating bilingual pragmatic proceedings annual color references meeting north american chapter sociation computational linguistics monroe christopher potts learning rational speech acts model proceedings amsterdam colloquium amsterdam illc ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments proceedings meeting ation advancement articial intelligence ramesh nallapati bowen zhou cicero dos santos caglar gulcehre bing xiang tive text summarization sequence sequence rnns signll conference putational natural language learning pages berlin germany association tional linguistics ani nenkova kathleen mckeown matic summarization foundations trends information retrieval jekaterina novikova ondrej dusek verena rieser dataset new challenges end end generation proceedings annual dial meeting discourse dialogue naho orita eliana vornov naomi feldman hal daume iii discourse affects ers choice referring expressions proceedings annual meeting association computational linguistics international joint conference natural language processing volume long papers volume pages kishore papineni salim roukos todd ward jing zhu bleu method automatic proceedings uation machine translation annual meeting association tational linguistics pages association computational linguistics romain paulus caiming xiong richard socher deep reinforced model abstractive proceedings international marization conference learning representations volume yevgeniy puzikov iryna gurevych nlg challenge neural models templates proceedings international conference ural language generation alexander rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages lisbon portugal association computational linguistics supplemental material reconstructor model details reconstructor based speaker task rst follow data preprocessing steps puzikov gurevych cludes delexicalization module deals sparsely occurring attributes near mapping values placeholder tokens mrs possible values attributes attributes fewer seven unique values remaining attributes near handled delexicalized placeholders ing puzikov gurevych way reconstructor needs predict presence attributes boolean variable attributes corresponding categorical variable use layer directional gru cho shared sentence encoder concatenate latent vectors tions construct directional encoded vector single word vector gru gru words contribute equally ing attribute use attention mechanism bahdanau determine importance single word gated sentence vector task calculated task specic sentence representation input layers softmax outputs turning probability vector attributes hyperparameters structured generation use beam size tuned maximize normalized average metrics velopment set abstractive summarization use beam size tuned maximize rouge development set
