p e s l c s c v v x r neural abstractive text summarization sequence sequence models tian shi virginia tech yaser keneshloo virginia tech naren ramakrishnan virginia tech chandan k reddy virginia tech past years neural abstractive text summarization sequence sequence models gained lot popularity interesting techniques proposed improve models making capable handling different challenges saliency fluency human readability generate high quality summaries generally speaking techniques differ categories network structure parameter inference decoding generation concerns efficiency parallelism training model paper provide comprehensive literature survey different models abstractive text summarization viewpoint network structures training strategies summary generation algorithms models proposed language modeling generation tasks machine translation later applied abstractive text summarization provide brief review models survey develop open source library neural abstractive text summarizer nats toolkit abstractive text summarization extensive set experiments conducted widely cnn daily mail dataset examine effectiveness different neural network components finally benchmark models implemented nats recently released datasets newsroom bytecup ccs concepts information systems summarization computing methodologies natural language processing natural language generation neural networks theory computation reinforcement learning additional key words phrases abstractive text summarization sequence sequence models attention model pointer generator network deep reinforcement learning beam search acm reference format tian shi yaser keneshloo naren ramakrishnan chandan k reddy neural abstractive text summarization sequence sequence models acm trans data sci article january pages introduction modern era big data retrieving useful information large number textual ments challenging task unprecedented growth availability blogs news articles reports explosive automatic text summarization provides effective solution summarizing documents task text summarization condense long uments short summaries preserving important information meaning authors addresses tian shi edu virginia tech yaser keneshloo edu virginia tech naren ishnan vt edu virginia tech chandan k reddy vt edu virginia tech permission digital hard copies work personal classroom use granted fee provided copies distributed profit commercial advantage copies bear notice citation page copyrights components work owned acm honored abstracting credit permitted copy republish post servers redistribute lists requires prior specific permission fee request permissions org association computing machinery acm trans data sci vol article publication date january shi al documents having short summaries text content retrieved processed digested effectively efficiently generally speaking ways perform text rization extractive abstractive method considered extractive words phrases sentences summaries selected source articles relatively simple produce grammatically correct sentences generated maries usually persist salient information source articles good performance w t human written summaries hand abstractive text summarization attracted attention capable generating novel words language generation models conditioned representation source documents strong potential producing high quality summaries verbally innovative easily incorporate external knowledge category deep neural network based models achieved better performance terms commonly evaluation measures rouge score compared traditional extractive approaches paper ily focus recent advances recurrent neural network rnn based sequence sequence models task abstractive text summarization rnn based models pointer generator network models fig successfully applied variety natural language processing nlp tasks machine translation headline generation text summarization speech recognition inspired success neural machine translation nmt rush et al introduced neural attention model attention based encoder neural network language model nnlm decoder abstractive sentence summarization task achieved significant performance improvement conventional methods chopra et al extended model replacing feed forward nnlm recurrent neural network rnn model equipped convolutional attention based encoder rnn elman lstm decoder outperforms state art models commonly benchmark datasets e gigaword corpus nallapati et al introduced novel elements rnn encoder decoder architecture address critical problems abstractive text summarization including following feature rich encoder capture keywords switching generator pointer model vocabulary oov words hierarchical attention capture hierarchical document structures established benchmarks models cnn daily mail dataset consists pairs news articles multi sentence highlights summaries dataset introduced abstractive text summarization models concentrated compressing short documents single sentence summaries task summarizing long documents multi sentence summaries models shortcomings accurately reproduce salient information source documents efficiently handle oov words tend suffer sentence level repetitions generating unnatural summaries tackle challenges et al proposed pointer generator network implicitly combines abstraction extraction pointer generator architecture copy words source texts pointer generate novel words vocabulary generator pointing copying mechanism factual information reproduced accurately oov words taken care summaries subsequent studies achieved state art performance demonstrated effectiveness pointing copying mechanism problem addressed coverage mechanism intra temporal intra decoder attention mechanisms heuristic approaches like forcing decoder output trigram testing acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models training strategies non trivial issues current framework e exposure bias inconsistency training testing measurements based neural bilistic language model models usually trained maximizing likelihood ground truth tokens given previous ground truth tokens hidden states teacher forcing algorithm fig testing time fig previous ground truth tokens unknown replaced tokens generated model generated tokens exposed decoder training decoding error accumulate quickly sequence generation known exposure bias issue mismatch measurements performance models usually estimated non differentiable evaluation metrics rouge bleu scores inconsistent log likelihood function cross entropy loss training phase problems alleviated curriculum learning reinforcement learning rl approaches training curriculum reinforcement learning approaches bengio et al proposed curriculum learning approach known scheduled sampling slowly change input decoder ground truth tokens model generated ones proposed meta algorithm bridges gap training testing practical solution avoiding exposure bias ranzato et al proposed sequence level training algorithm called mixer mixed incremental cross entropy reinforce consists cross entropy training reinforce curriculum learning reinforce use user defined task specific reward e non differentiable evaluation metrics combining curriculum learning proposed model capable addressing issues models reinforce suffers high variance gradient estimators instability training bahdanau et al proposed actor critic based rl method relatively lower variance gradient estimators actor critic method additional critic network trained compute value functions given policy actor network model actor network trained based estimated value functions assumed exact critic network hand rennie et al introduced self critical sequence training method scst lower variance compared reinforce need second critic network applications abstractive text summarization rl algorithms training models achieved success variety language generation tasks image captioning machine translation dialogue generation specific abstractive text summarization lin et al introduced coarse fine attention framework purpose summarizing long documents model parameters learned reinforce algorithm zhang et al reinforce algorithm curriculum learning strategy sentence simplification task paulus et al applied self critic policy gradient algorithm training model copying mechanism obtained state art performance terms rouge scores proposed mixed objective function combines rl loss traditional cross entropy loss method leverage non differentiable evaluation metrics improve readability celikyilmaz et al introduced novel deep communicating agents method abstractive summarization adopted rl loss objective function pasunuru et al applied self critic policy gradient algorithm train pointer generator network introduced novel rewards e saliency entailment rewards addition rouge metric generated summaries salient logically entailed li et al proposed training framework based actor critic method actor network attention based model critic network consists maximum likelihood estimator global summary quality estimator distinguish acm trans data sci vol article publication date january shi al generated ground truth summaries neural network binary classifier chen et al proposed compression paraphrase multi step procedure abstractive text summarization extracts salient sentences documents rewrites model advantage actor critic algorithm optimize sentence extractor better extraction strategy keneshloo et al conducted comprehensive summary rl methods applications training models different nlp tasks implemented rl algorithms open source library com yaserkl constructed pointer generator network base model rnn based models prevalent models attained state art performance sequence modeling language generation tasks rnn especially long short term memory lstm gated recurrent unit gru based encoder decoder models standard rnn models difficult train vanishing exploding gradients problems lstm solution vanishing gradients problem address exploding gradients issue issue recently solved gradient norm clipping strategy critical problem rnn based models computation constraint long sequences inherent sequential dependence nature words current hidden state rnn function previous hidden states dependence rnn parallelized sequence time step dimension fig training evaluation training major challenge long sequences computation time memory constraints gpus recently found convolutional neural network cnn based decoder models potential alleviate aforementioned problem better performance terms following considerations model parallelized training evaluation computational complexity model linear respect length sequences model short paths pairs input output tokens propagate gradient signals efficiently kalchbrenner et al introduced bytenet model adopts dimensional convolutional neural network fixed depth encoder decoder decoder cnn stacked hidden representation encoder cnn ensures shorter path input output proposed bytenet model achieved state art performance character level machine translation task parallelism linear time computational complexity bradbury et al proposed quasi recurrent neural network qrnn encoder decoder architecture encoder decoder composed convolutional layers called dynamic average pooling layers convolutional layers allow computations completely parallel mini batches sequence time step dimensions require time compared computation demands lstm despite sequential dependence presents pooling layers framework demonstrated effective outperforming lstm based models character level machine translation task significantly higher computational speed recently gehring et al attempted build cnn based models apply large scale benchmark datasets sequence modeling authors proposed convolutional encoder model encoder composed succession convolutional layers demonstrated strong performance machine translation constructed convolutional architecture replacing lstm decoder cnn decoder bringing novel elements including gated linear units multi step attention model enables computations network elements parallelized training decoding faster rnn models achieved competitive performance acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models fig overall taxonomy topics models neural abstractive text summarization machine translation benchmark datasets rencently model applied abstractive document summarization outperforms pointer generator network cnn daily mail dataset vaswani et al constructed novel network architecture called transformer depends feed forward networks multi head attention mechanism achieved superior performance machine translation task significantly training time currently large transformers pre trained massive text corpus self supervised objectives achieved superior results variety downstream nlp tasks machine understanding question answering abstractive text summarization zhang et al demonstrated pre trained encoder decoder model outperform previous state art results datasets fine tuning limited supervised examples shows pre trained models promising candidates zero shot low resource summarization tasks studies far primarily focused pointer generator network training neural networks rl algorithms cnn based architectures transformers studies aim improve performance rnn models task abstractive text summarization different perspectives broaden applications network structure attention way boost performance models design better network structures zhou et al introduced information filter selective gate network encoder decoder model control information flow encoder decoder constructing second level representation source texts gate network zeng et al introduced read mechanism improve quality representations source texts tan et al built graph ranking model hierarchical encoder decoder framework enables model capture salient information source documents generate accurate fluent non redundant summaries xia et al proposed deliberation network passes decoding process multiple times liberation process polish sequences generated previous decoding process li et al incorporated sequence variational auto encoders decoder capture latent structure generated summaries extraction abstraction way improve abstractive text summarization use salient information extraction process hsu et al proposed unified acm trans data sci vol article publication date january shi al framework takes advantage extractive abstractive summarization novel attention mechanism combination sentence level attention based extractive summarization word level attention based pointer generator network inspired intuition words attended sentences lower attention scores chen et al introduced multi step procedure compression paraphrase abstractive summarization extracts salient sentences documents rewrites order final summaries li et al introduced guiding generation model keywords source texts retrieved extractive model guide network applied encode obtain key information representations guide summary generation process long documents compared short articles texts moderate lengths challenges arise long documents difficulty capturing salient information nallapati et al proposed hierarchical attention model capture hierarchical structures long documents models scale long sequences ling et al introduced coarse fine attention mechanism hierarchically reads attends long documents document split chunks texts stochastically selecting chunks texts training approach scale linearly number chunks instead number tokens cohan et al proposed discourse aware attention model similar idea hierarchical attention model model applied large scale datasets scientific papers e arxiv pubmed datasets tan et al introduced graph based attention model built hierarchical encoder decoder framework pagerank algorithm calculate saliency scores sentences multi task learning multi task learning promising research direction problem allows models handle different tasks pasunuru et al introduced multi task learning framework incorporates knowledge entailment generation task abstractive text summarization task sharing decoder parameters proposed novel framework composed auxiliary tasks e question generation entailment generation improve model capturing saliency entailment abstractive text summarization model different tasks share encoder decoder attention layers mccann et al introduced natural language decathlon challenge spans different tasks including question answering machine translation summarization proposed multitask question answering network jointly learn tasks task specific modules parameters tasks mapped framework question answering given context beam search beam search algorithms commonly decoding different language generation tasks generated candidate sequences usually lacking diversity words k candidates nearly identical k size beam li et al replaced log likelihood objective function neural probabilistic language model maximum mutual information mmi neural conversation models remedy problem idea applied neural machine translation nmt model bi directional dependency source target texts proposed simple fast decoding algorithm generate diverse candidates shown performance improvement abstractive text summarization task vijayakumar et al proposed generating diverse outputs optimizing diversity augmented objective function method referred diverse beam search dbs algorithm applied image captioning machine translation visual question generation tasks cibils et al introduced algorithm uses dbs generate summaries picks candidates according acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models maximal marginal relevance assumption useful candidates close source document far away proposed algorithm boosted performance pointer generator network cnn daily mail dataset despite research papers published area neural abstractive text tion survey papers provide comprehensive study paper systematically review current advances models abstractive text summarization task perspectives including network structures training strategies sequence generation addition literature survey implemented methods open source library nats com nats extensive set experiments conducted benchmark text summarization datasets order examine importance different network components main contributions paper summarized follows provide comprehensive literature survey current advances models emphasis abstractive text summarization conduct detailed review techniques tackle different challenges rnn decoder architectures review different strategies training models approaches generating summaries provide open source library implements models systematically investigate effects different network elements summarization performance rest paper organized follows overall taxonomy topics models neural abstractive text summarization shown fig comprehensive list papers published till date topic neural abstractive text summarization summarized table section introduce basic framework extensions including attention mechanism pointing copying mechanism repetition handling improving encoder decoder summarizing long documents combining extractive models section summarizes different training strategies including word level training methods cross entropy training sentence level training rl algorithms section discuss generating summaries beam search algorithm diverse beam decoding algorithms section present details implementations discuss experimental results cnn daily mail newsroom bytecup biendata com competition datasets conclude survey section rnn encoder decoder framework section review different rnn based encoder decoder models neural abstractive text summarization start basic framework attention mechanism describe advanced network structures handle different challenges text summarization repetition vocabulary oov words highlight existing problems proposed solutions framework basics vanilla framework abstractive text summarization composed encoder decoder encoder reads source article denoted j transforms hidden states j decoder takes hidden states context input outputs summary y yt xi yj hot representations tokens source article summary respectively use j t represent number tokens document length original source document summary respectively summarization task defined inferring summary y given source article acm trans data sci vol article publication date january shi al fig basic model sos eos represent start end sequence respectively encoders decoders feed forward networks cnn rnn rnn architectures especially long short term memory lstm gated recurrent unit gru widely adopted models fig shows basic rnn model bi directional lstm encoder lstm decoder bi directional lstm considered usually gives better document representations compared forward lstm encoder reads sequence input tokens turns sequences hidden states h j right left arrows bi directional lstm input sequence encoded denote forward backward temporal dependencies respectively superscript e shortcut notation indicate encoder decoding decoder takes ce encoded representations source article e hidden cell states input generates summary simple encoder decoder model encoded vectors initialize hidden cell states lstm decoder example initialize follows h e tanh ce j h e j ce j ce cd j hd superscript d denotes decoder concatenation operator decoding step update hidden state hd t conditioned previous hidden states input tokens e hd t eyt explicitly express cell states input t output lstm hidden states passed parts model vocabulary distribution calculated pvocab t pvocab t t vector dimension size vocabulary v element vt vector v probability generating target token w vocabulary v denoted pvocab t w lstm based encoder decoder framework foundation neural abstractive text summarization models problems model example encoder trained propagation time paths encoder output relatively far apart limits propagation gradient signals accuracy human readability generated summaries low lot oov repetitions rest section discuss different models proposed literature resolve issues producing better quality summaries attention mechanism attention mechanism achieved great success commonly models different natural language processing nlp tasks machine translation image captioning neural abstractive text summarization attention based encoder decoder architecture shown fig decoder takes encoded rest paper use unk e unknown words denote oov words acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models representations e final hidden cell states source article input selectively focuses parts article decoding step example suppose want compress source input kylian mbappe scored goals second half minutes send france world cup quarter finals thrilling win argentina saturday short version france beat argentina enter quarter finals generating token beat decoder need attend thrilling win parts text attention achieved alignment mechanism computes attention distribution source tokens lets decoder know attend produce target token encoder decoder framework depicted fig given hidden states encoder e attention distribution e j t source tokens calculated follows j current decoder hidden state hd j j e t j t j t k alignment score t j alternatives suggested hd t obtained content based score function j hd t hd j walignhd tanh walign j hd t balign dot general concat j t noted number additional parameters dot general concat t respectively represents proaches dimension vector general concat commonly score functions abstractive text summarization drawbacks dot method requires hd dimension attention distribution naturally define source context vector target word j j ze t e t jhe j current decoder hidden state hd wz hd t t attention hidden state t hd t finally vocabulary distribution calculated hd t pvocab t softmax t decoder hidden state hd t updated t eyt hd lstm hd t hd input concatenation eyt hd t pointing copying mechanism pointing copying mechanism represents class approaches generate target tokens directly copying input sequences based attention weights naturally applied abstractive text summarization summaries articles share vocabulary importantly capable dealing vocabulary oov words variety studies shown boost performance incorporating acm trans data sci vol article publication date january shi al fig attention based model pointer generator network pointing copying mechanism framework section review alternatives mechanism abstractive text summarization pointer softmax basic architecture pointer softmax described follows consists fundamental components short list softmax location softmax switching network decoding step t short list softmax pvocab t calculated eq predict target tokens vocabulary location softmax gives locations tokens copied source article target yt based attention weights e t components switching network designed determine predict token vocabulary copy source article oov token switching network multilayer mlp sigmoid activation function estimates probability pgen t generating tokens vocabulary based context vector ze t hidden state hd pgen t ws zze t ws hhd t bs pgen t scalar sigmoid activation function final probability producing target token yt given concatenation vectors pgen t pvocab t t e t switching generator pointer similar switching network pointer softmax switching generator pointer equipped switch determines generate token vocabulary point source article decoding step switch explicitly modeled pgen t ws zze t ws hhd t ws eeyt bs switch turned decoder produces word vocabulary distribution pvocab t eq decoder generates pointer based attention distribution t eq e pj arg maxj j e e t j pj position token source article pointer activated embedding pointed token ex j input decoding step copynet copynet differentiable network architecture easily trained end end manner framework probability generating target token combination probabilities modes e generate mode copy mode copynet represents unique tokens vocabulary source sequence v x respectively builds extended vocabulary vext v x unk vocabulary distribution extended vocabulary calculated p vext yt pc yt acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models p pc defined vext e z v unk pc yt z j expc xj yt x xj obtained eq z normalization factor shared equations calculated t pointer generator network pointer generator network differentiable network architecture fig similar copynet vocabulary distribution extended vocabulary vext calculated p pgen t pgen t pc yt pgen t obtained eq vocabulary distribution attention distribution pc yt defined follows pvocab t yt v unk pc yt j yt e t j yt x pointer generator network base model abstractive text summarization models table finally noted pgen t copynet pointer generator network viewed soft switch choose generation copying different hard switch e pgen t pointer softmax switching generator pointer repetition handling critical challenges attention based models generated sequences repetitions attention mechanism tends ignore past alignment information summarization headline generation tasks model generated summaries suffer word level sentence level repetitions specific summaries consist sentences cnn daily mail dataset newsroom dataset section review approaches proposed overcome repetition problem temporal attention temporal attention method originally proposed deal attention deficiency problem neural machine translation nmt nallapati et al found overcome problem repetition generating multi sentence summaries prevents model attending parts source article tracking past attention weights formally given attention score t j eq define temporal attention score attention distribution context vector eq calculated exp t j exp s e t j exp s e j stemp t j t temp t j stemp t j k stemp t k ze t temp t j j seen eq decoding step input tokens highly attended lower attention score normalization time dimension result decoder repeatedly attend source article acm trans data sci vol article publication date january shi al intra decoder attention intra decoder attention technique handle repetition problem long sequence generations compared regular attention based models allows decoder attend tokens source article track previously decoded tokens summary decoder repeatedly produce information t calculated manner t intra decoder attention scores denoted sd attention score t j attention weight token expressed t t t k attention distribution calculate decoder context vector taking linear combination decoder hidden states e hd decoder encoder context vector calculate vocabulary distribution t zd t d t hd coverage coverage model proposed nmt task address problems standard attention mechanism tends ignore past alignment information recently et al introduced coverage mechanism abstractive text summarization task model defined coverage vector ue t sum attention distributions previous decoding steps e ue e t j contains accumulated t attention information token source article previous decoding steps coverage vector additional input calculate attention score j t j tanh walign j hd ue t balign result attention current decoding time step aware attention previous decoding steps defined novel coverage loss ensure decoder repeatedly attend locations generating multi sentence summaries coverage loss defined covlosst e t j ue t j j upper bounded distraction coverage mechanism known distraction document summarization task addition distraction mechanism attention proposed distraction mechanism encoder context vectors mechanisms prevent model attending certain regions source article repeatedly formally ze given context vector current decoding step ze eq distracted context vector ze dist defined ze dist ze j j wdist z whist z diagonal parameter matrices t historical context vectors ze ze t whist zze t t improving encoded representations lstm bi directional lstm commonly models abstractive text summarization representations source articles believed sub optimal section review approaches aim improve encoding process replace bi directional gru seen abstractive summarization papers eq t hd acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models selective encoding selective encoding model proposed abstractive sentence summarization task built attention based encoder decoder framework introduces selective gate network encoder purpose distilling salient information source articles second layer representation distilled representation source article constructed representation lstm layer bi directional gru encoder work formally distilled representation token source article defined sel j gatesel denotes selective gate token xj calculated follows gatesel j gatesel j wsel hhe j wsel senhe sen bsel j distilled representations decoding gate network control information flow encoder decoder select salient information boosts performance sentence summarization task sen read encoding intuitively read mechanism motivated human readers read article times writing summary simulate cognitive process read encoder reads source article twice outputs level representations read lstm encodes tokens article respectively second read use lstm encode source text based outputs read formally encoder hidden state second read updated sen j j j hidden states generation j j ex j j sen second read passed decoders summary j improving decoder embedding weight sharing sharing embedding weights decoder practical approach boost performance allows reuse semantic syntactic information embedding matrix summary generation suppose embedding matrix represented wemb formulate matrix summary generation eq wproj sharing model weights number emb parameters significantly standard model number parameters wproj t t represents dimension vector h denotes size vocabulary deep recurrent generative decoder drgd conventional encoder decoder models calculate hidden states attention weights entirely deterministic fashion limits capability representations results low quality summaries incorporating variational auto encoders vaes encoder decoder framework provides practical solution problem inspired variational rnn proposed model highly structured sequential data li et al introduced model drgd aims capture latent structure information summaries improve summarization quality model employs gru basic recurrent model encoder decoder consistent survey paper explain ideas lstm instead lstm layers calculate decoder hidden state updated t decoding step t t eyt attention weights calculated encoder hidden state layer t eqs second layer hidden state layer hidden state t j context vector e decoder hidden state t t t t acm trans data sci vol article publication date january shi al updated t hd t t t t eyt t finally decoder hidden state obtained hd referred deterministic hidden state vae incorporated decoder capture latent structure information summaries represented multivariate gaussian distribution reparameterization trick latent variables expressed t t noise variable n gaussian parameters t t network calculated t wvae henc bvae hidden vector encoding process vae defined t wvae henc bvae t t henc t wenc hhd t latent structure variables output hidden states hdec wenc wenc yeyt henc t benc formulated hdec t wdec t t bdec finally vocabulary distribution calculated pvocab t t primarily focused network structure drgd section details vae derivations found drgd vae incorporated decoder model recent works vae attention layer sentence compression task summarizing long document compared sentence summarization abstractive summarization long documents relatively investigated recently attention based models pointing copying mechanism shown power summarizing long documents tokens performance improvement primarily attributes copying repetition redundancy avoiding techniques long documents need consider important factors generate high quality summaries saliency e significance fluency coherence novelty usually models combined beam search decoding algorithm generate fluent human readable sentences section review models aim improve performance long document summarization perspective saliency models long document summarization usually consists encoder chical architecture capture hierarchical structure source documents level salient information includes important sentences chunks texts sections paragraphs lower level salient information represents keywords use term chunk represent level information hierarchical encoder encodes tokens chunk chunk representation encodes ent chunks document document representation paper consider single layer forward word chunk encoders suppose hidden states chunk word j chunk represented hchk hidden state hd decoding step t calculate word level attention weight wd t wd t j l wd t chk t chk t t wd t chk t j j kl attention weight chk t schk hd t calculated eq section review alignment scores swd t j k hd time calculate chunk level hwd j current decoder deep communicating agents model requires multiple layers bi directional lstm falls scope survey acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models different models based hierarchical encoder task long document text summarization hierarchical attention intuition hierarchical attention words important chunks attended chunk level attention distribution chk t word level attention distribution wd calculate scaled word level attention distribution scale t j chk t wd j l chk t wd t kl j scale t j scaled attention calculate context vector eq e ze t noted hierarchical attention framework different hierarchical attention network proposed chunk representation obtained zwd t instead hidden state word encoder j j wd t hwd j discourse aware attention idea discourse aware attention similar hierarchical attention giving eq main difference attention models scaled attention distribution discourse aware attention calculated scale t j chk t swd t j l chk t swd t kl k swd t j alignment score instead attention weight coarse fine attention coarse fine attention proposed tational efficiency similar hierarchical attention proposed model chunk level attention word level attention instead word level hidden states chunks calculating context vector attention method samples chunk chunk level attention distribution calculates context vector j scale t ze test time stochastic sampling chunks replaced t j greedy search hwd j graph based attention aforementioned hierarchical attention mechanism itly captures chunk level salient information importance chunk determined solely attention weight contrast graph based attention framework allows late saliency scores explicitly pagerank algorithm graph vertices edges chunks texts similarities respectively formally decoding time step t saliency scores input chunks obtained f t w t adj adjacent matrix w adj similarity chunks calculated w adj dadj diagonal j matrix equal sum ith column w adj damping factor wparhchk hchk j vector t defined t t topic details t t finally graph based attention distribution chunk obtained chk t t t t t chunks rank higher previous decoding step e t efficient way select salient information source documents initialized seen graph based attention mechanism focus provides f t acm trans data sci vol article publication date january shi al table overview different models neural abstractive text summarization year reference rush et al highlights attention based summarization abs lopyrev et al ranzato et al simple attention sequence level training chopra et al nallapati et al miao et al chen et al gulcehre et al gu et al zeng et al takase et al et al paulus et al zhou et al xia et al nema et al tan et al ling et al recurrent attentive summarizer switch generator pointer temporal attention hierarchical attention auto encoding sentence compression forced attention sentence compression pointer network distraction pointer softmax copynet read copy mechanism abstract meaning representation amr based abs pointer generator network coverage deep reinforced model intra temporal intra decoder attention weight sharing selective encoding abstractive sentence summarization deliberation networks query based diversity based attention graph based attention framework bag words convolution attention neural network language model nnlm lstm lstm xent elman lstm elman lstm xent dad mixer xent convolution encoder attentive encoder elman lstm rnn feature rich encoder rnn xent training optimizer datasets xent sgd duc gigaword metrics rouge rmsprop sgd gigaword gigaword bleu rouge bleu sgd duc gigaword rouge adadelta duc gigaword cnn dm rouge adam gigaword rouge encoder compressor decoder gru gru gru gru gru gru lstm gru hierarchical read encoder lstm attention based amr encoder nnlm lstm lstm lstm lstm gru gru xent sgd lstm lstm gru query encoder document encoder gru hierarchical encoder lstm xent xent xent xent xent xent xent adadelta adadelta sgd sgd cnn lcsts gigaword lcsts duc gigaword rouge rouge rouge rouge xent sgd duc gigaword rouge xent adadelta cnn dm xent rl adam cnn dm duc gigaword msr atc gigaword debatepedia cnn dm cnn dailymail cnn dm newsela wikismall wikilarge duc lcsts adadelta adam adam sgd adam adadelta rl rl xent vae gan rouge meter rouge human rouge rouge rouge rouge rouge ppl bleu fkgl sari rouge rouge human rouge meteor bleu cider d rouge rouge human coarse fine attention lstm lstm zhang et al sentence simplification reinforcement learning lstm lstm li et al liu et al deep recurrent generative decoder drgd adversarial training pasunuru et al multi task entailment generation gehring et al fan et al convolutional position embeddings gated linear unit multi step attention convolutional controllable gru gru vae pointer generator network adadelta cnn dm lstm document encoder premise encoder lstm summary entailment decoder cnn cnn objective adam duc gigaword snli xent adam duc gigaword cnn cnn xent adam duc cnndm extraction abstraction extractive summarization approaches usually better performance comparing tive approaches especially respect rouge measures advantages extractive approaches summarize source articles extracting salient snippets sentences directly documents abstractive approaches rely word level attention mechanism determine relevant words target words decoding step section review studies attempted improve performance abstractive summarization combining extractive models extractor pointer generator network model proposes unified framework tries leverage sentence level salient information extractive model incorporate abstractive model pointer generator network formally inspired hierarchical attention mechanism replaced attention distribution e t abstractive acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models table overview different models neural abstractive text summarization chen et al hsu et al li et al li et al lin et al song et al cohan et al year reference highlights celikyilmaz et al deep communicating agents framework lstm lstm semantic cohesion loss reinforce selected sentence rewriting abstraction extraction inconsistency loss lstm encoder extractor abstractor extractor gru abstractor pointer generator network actor critic gru gru cnn dm nyt duc cnn dm sgd adam training optimizer datasets objective xent rl objective rl rl adadelta adadelta cnn dm duc cnn dm lcsts cnn dm pasunuru et al multi reward optimization lstm lstm rl adam abstraction extraction key information guide network kign global encoding convolutional gated unit rl rouge saliency entailment structured infused copy mechanisms discourse aware attention kign lstm framework pointer generator network xent adadelta lstm lstm xent adam gigaword lcsts rouge duc cnn dm snli multinli squad gigaword pointer generator network adam objective xent objective hierarchical rnn lstm encoder lstm multi task encoder decoder framework adagrad pubmed arxiv adam duc gigaword squad snli rouge meteor pointer generator network xent adagrad cnn dm rouge metrics rouge human rouge human rouge human rouge rouge rouge human rouge human rouge guo et al cibils et al wang et al multi task summarization entailment question generation diverse beam search plagiarism extraction scores topic aware attention kryciski et al improve abstraction gehrmann et al zhang et al jiang et al attention abstraction extraction learning summarize radiology findings closed book training chung et al main pointer generator chen et al iterative text summarization cnn cnn rl lstm encoder decoder contextual model language model xent rl pointer generator network pointer generator network background encoder pointer generator network closed book decoder pointer generator network document encoder gru encoder gru decoder iterative unit objective xent objective rl xent objective gigaword cnn dm lcsts cnn dm cnn dm nyt radiology reports duc cnn dm asynchronous gradient descent optimizer adagrad adam adam rouge rouge novel n gram test human rouge novel rouge rouge meteor adadelta cnn dm rouge adam duc cnn dm rouge model scaled version scale t k wd extra sentence level salient score sentence word position j decoding step t different salient scores sentence level attention weights obtained deep neural network known extractor attention weights expressed scale t j t j t t t j wd extra t j extra training addition cross entropy coverage loss pointer generator network paper proposed losses e extractor loss inconsistency loss n log n extractor loss train extractor defined lext n n n ground truth label nth sentence n total number sentences inconsistency loss expressed linc k t set k attended words t total number words summary intuitively inconsistency loss ensure sentence level attentions extractive model word level attentions abstractive model consistent words word level attention weights high corresponding sentence level attention weights high t log j extra t j j k e key information guide network kign approach uses guiding generation anism leverages key salient information e keywords guide decoding process step procedure keywords extracted source articles textrank algorithm second kign encodes key information incorporates decoder guide generation summaries technically speaking use bi directional lstm acm trans data sci vol article publication date january shi al j t w d alignhe alignhd w key hkey hkey n ws hhd t encode key information output vector concatenation hidden states e hkey n length key information sequence alignment e anism modified alignhkey similarly soft switch t j ws keyhkey bs pointer generator network calculated pgen t ws zze t reinforce selected sentence rewriting models introduced survey built encoder decoder framework encoder reads source articles turns vector representations decoder takes encoded vectors input generates summaries unlike models reinforce selected sentence rewriting model consists models extractive model extractor designed extract salient sentences source article second abstractive model abstractor paraphrases compresses extracted sentences short summary abstractor network standard attention based model copying mechanism handling oov words extractor network encoder uses cnn encode tokens obtains representations sentences uses lstm encode sentences represent source document sentence level representations decoder lstm designed recurrently extract salient sentences document pointing mechanism model achieved state art performance cnn daily mail dataset demonstrated computationally efficient pointer generator network training strategies section review different strategies train models abstractive text summarization discussed categories training methodologies e level sequence level training commonly teacher forcing algorithm entropy training belong category different rl based algorithms fall second discuss basic ideas different training algorithms applications models text summarization comprehensive survey deep rl models found word level training word level training language models represents methodologies try optimize tions token example abstractive text summarization given source article model generates summary y probability p represents model parameters e weights w bias neural language model probability expanded p p t t t multiplier p t known likelihood conditional probability token yt given previous ones denoted y t intuitively text generation process described follows starting special token sos start sequence model generates token yt time t probability p t pvocab t yt token obtained sampling method greedy search e yt arg maxyt pvocab t fig generated token fed decoding step generation stopped model outputs eos end sequence token length reaches user defined maximum threshold section review different approaches learning model parameters e start commonly end end training approach e cross entropy training different methods avoiding problem exposure bias acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models fig generation process greedy search training teacher forcing algorithm c illustration scheduled sampling c cross entropy training xent learn model parameters xent maximizes log likelihood observed sequences ground truth yt e log p log p yt y t t t equivalent minimizing cross entropy xent loss lossxent log p training strategy fig algorithm known teacher forcing algorithm training uses observed tokens ground truth input aims improve probability observed token decoding step testing relies predicted tokens previous decoding step major difference training testing fig predicted tokens observed ones discrepancy accumulated time yields summaries different ground truth summaries problem known exposure bias scheduled sampling scheduled sampling algorithm known data demonstrator dad proposed solve exposure bias problem shown fig training input decoding step comes sampler decide model generated token yt step observed token yt training data sampling based bernoulli distribution pi yt y yt pdad probability token training data y yt binary indicator function scheduled sampling algorithm pdad annealing scheduling function decreases training time suggested bengio et al scheduling function different forms e dad pdad k k linear decay exponential decay inverse sigmoid decay k training step parameter guarantees pdad strategy referred curriculum learning algorithm main intuition algorithm beginning stage model random parameters generates relevant correct tokens decoder takes ground truth tokens training data input training proceeds model gradually reduces probability ground truth tokens end training model assumes trained generate reasonable tokens decoder completely rely predictions acm trans data sci vol article publication date january shi al end end backprop algorithm method exposes model predictions training decoding step uses xent train model parameters input ground truth token model generated token instead fusion k tokens decoding step k hyper parameter ysampk specifically model samples k tokens denoted t vocabulary distribution pvocab t scale probabilities follows t t psamp t ysampi t pvocab t ysampi t sampj j pvocab t y t obtain vector embedding space esamp t fused vector served input decoding step noted makes use dad practice sampler determine fused vectors embeddings ground truth tokens input psamp t ysampi ey sampi t t sequence level training sequence level training deep rl algorithms recently received lot popularity area neural abstractive text summarization ability incorporate user defined metrics including non differentiable rouge scores train neural networks section review different policy gradient algorithms train abstractive text summarization models actor critic algorithms related work readers encouraged publications rl setting generating sequence tokens summary considered sequential decision making process encoder decoder model viewed agent reads source article initializes internal state hidden cell states lstm decoding step t updates state takes action y t v e picking token vocabulary according policy p t vocabulary viewed action space end decoding produce sequence actions y t observe reward t usually rouge scores context text summarization rl algorithms update agent comparing action sequence based current policy optimal action sequence e ground truth summary section start commonly reinforce training models introduce mixer algorithm improve convergence rate stability training self critic sequence training approach shows low variance gradient estimator y y y y reinforce goal reinforce find parameters maximize expected rewards loss function defined negative expected reward e l t equation rewritten t represents y t y y y t l y t y y t t y represents set contains possible sequences optimize policy respect model parameters derivative loss function obtain l y t y y t y y t t y t log y t t acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models abstractive text summarization task policy expressed p y according eq equation expressed follows l y t y t t p t log p t t t ey p y ey p y ey t p y t t log p t t t t t ey t p y t log p t t t t t reward gradient estimator gradient model parameters updated t propagated node computational graph l learning rate seen eq computing gradient requires sample sequences practical presence possible number sequences instead reinforce approximates expectation single sample gradient expressed follows l log p t t t t t problems associated method high variance gradient estimator makes use sample train model practical solution alleviate problem introducing baseline reward denoted gradient e l ey t p y t log p t t t t t baseline b arbitrary function depend y t way t b change expectation gradient ey t complete derivations equation found practice gradient baseline approximated t log p t p y t l t b log p t t t t better ways sampling sequence different approaches calculate baseline found mixer training models reinforce suffer slow convergence fail large action space poor initialization refers randomly initialize parameters start random policy alleviate problem ranzato et al modified reinforce incorporating idea curriculum learning strategy proposed mixer algorithm algorithm trained model n ensure rl starts better policy batch sequence cross entropy loss t steps reinforce remaining steps integer number training continued n n integer number increased reinforce steps continued training n acm trans data sci vol article publication date january t t t t shi al process repeat sequence trained reinforce algorithm shown better performance greedy generation compared xent dad task abstractive text summarization self critic sequence training scst main idea scst use testing time inference algorithm baseline function reinforce suppose greedy search fig sample actions testing training iteration model generates action sequences y greedy greedy search second y t according scst baseline defined reward greedy sequence gradient loss function scst expressed t sampled distribution p t t l t greedy t log p t t according eq scst shown low variance effectively optimized mini batch sgd compared reinforce demonstrated effective improving performance models task abstractive text summarization work authors following rl loss train model lrl t greedy t log p t t model performs better trained xent terms rouge scores human readability generated summaries low alleviate problem authors defined mixed loss function rl xent e lmixed lrl lxent hyper parameter model trained mixed loss achieve better human readability rouge scores better obtained xent scheduled sampling reducing exposure bias scheduling function constant pdad reviewed different rnn encoder decoder architectures training strategies sections position generate summaries given source articles summary generation generally speaking goal summary generation find optimal sequence y t y t arg max y t y log p t arg max t y log p t t t y represents set contains possible sequences summaries elements exact inference intractable practice v represents output vocabulary section review beam search algorithm extensions approximating exact inference greedy beam search shown fig generate sub optimal sequence greedy search e y t arg max v log p yt t decoding step t greedy search computationally efficient human readability generated summaries low acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models algorithm beam search algorithm decoding models input source article beam size b summary length t model parameters output b best summaries initialize output sequences q seq accumulated probabilities q prob decoded tokens q word states hidden cell states lstm q states t context vectors q ctx t j encoder compute update q states encoder states t initialize candidates q cand seq q cand prob q cand word q cand states q cand ctx repeating q seq q prob q word q states q ctx b times respectively b t decoder lstm cell input j q word t b b b t b t hidden states hd t b b cell states compute p ycand t q states q ctx select b candidate words ycand select corresponding probability p ycand t b b context vector ze cd update elements q cand seq update elements q cand states update elements q cand ctx update q cand prob t b b q cand word hd eq ze t b b t t b b ycand t b b cd t b b end flatten q cand prob choose b best hypotheses update q seq t q prob q word q states q ctx corresponding candidates end beam search algorithm compromise greedy search exact inference commonly employed different language generation tasks beam search search algorithm generates sequences left right retaining b scoring b sequence fragments decoding step formally denote decoded b sequence fragments known hypotheses time step t y y y t b scores s bm t determines b probable words ycand t b b expand score expanded fragment e new hypotheses ycand t b fragment t calculate p ycand t b t ycand s bm t ycand s bm b updated t b b s bm s cand t p ycand t b b t s bm t b initialized b b labels current hypothesis word candidate respectively t b initialized s cand t b b t s bm log p ycand t b s bm t acm trans data sci vol article publication date january shi al yields b b expanded fragments e new hypotheses b scores retained decoding step procedure repeated eos token generated algorithm pseudo codes beam search algorithm generating summaries models given beam size b batch size diversity promoting algorithms despite widespread applications beam search algorithm suffered lacking diversity beam words b hypotheses differ couple tokens end sequences limits applications summarization systems wastes computational resources important promote diversity generated sequences ippolito et al performed extensive analysis different post training decoding algorithms aim increase diversity decoding shown power oversampling e sampling additional candidates filtering desired number improving diversity sacrificing performance section briefly introduce studies aim increase diversity beam search algorithm abstractive summarization models maximum mutual information mmi mmi based methods originally proposed neural conversation models applied tasks machine translation summarization basic intuition desired model account dependency target source consider likelihood source given target achieved replacing log likelihood target e log p eq pairwise mutual information source target defined log p y p p y training model parameters learned maximizing mutual information generating sequences objective expressed follows y arg max y log p y p y arg max y y log p log p y obvious calculating p y intractable approximation methods proposed literature alleviate problem approximation method summarized following steps train models model parameters generate diverse n list sequences based achieve goal method calculating scores beam search algorithm modified k t k log p yt t k s beam adding term k model explicitly encourages hypotheses different parents e different k results diverse results parameter known diversity rate indicates degree diversity integrated beam search algorithm rank n list linearly combining ranking score candidate n list defined follows log log task specific auxiliary term parameters learned minimum error rate training development dataset acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models diverse beam search dbs dbs approach aims increase diversity standard beam search algorithm partitions hypotheses g groups decoding step sequentially performs beam search group based dissimilarity augmented scoring function t b b log p ycand t b b t b t t t b s cand s cand parameter t represents diversity function measures dissimilarity distance candidate ycand t b group sequences groups standard beam search applied group intuitively generated sequences different groups different penalty diversity functions dbs combined pointer generator network improve diversity model produced summaries implementations experiments apart comprehensive literature survey detailed review different techniques network structures training strategies summary generations developed open source library nats com nats based rnn framework abstractive text summarization section introduce details implementations systematically experiment different network elements hyper parameters public available datasets e cnn daily mail newsroom bytecup implementations nats equipped following important features attention based framework implemented attention based model shown fig encoder decoder chosen lstm gru attention scores calculated alignment methods given eq pointer generator network based attention based framework implemented pointer generator network discussed section intra temporal attention mechanism temporal attention work ment methods intra decoder attention mechanism alignment method intra decoder attention attention mechanism coverage mechanism handle repetition problem implemented coverage anism discussed section coverage switched coverage loss set weight sharing mechanism discussed section weight sharing mechanism boost performance significantly fewer parameters beam search algorithm implemented efficient beam search algorithm handle case batch size unknown words replacement similar implemented heuristic unknown words replacement technique boost performance theoretically pointer generator network generate oov words copying mechanism unk extended vocabulary decoding completed manually check unk summaries replace words source articles attention weights meta algorithm attention based model datasets cnn daily mail dataset cnn daily mail dataset com abisee cnn dailymail consists k news articles paired highlights known acm trans data sci vol article publication date january shi al table basic statistics cnn daily mail dataset pairs article length headline length summary length cnn daily mail dev train test newsroom dev train test bytecup dev test train multi sentence summaries summarized basic statistics dataset table primarily versions dataset version anonymizes entities second keeps original texts paper second version obtained processed data et al com jafferwilson data cnn dailymail newsroom dataset cornell newsroom dataset recently released consists million article summary pairs million publicly available training evaluating summarization systems newsroom library com clic lab newsroom scrape extract raw data texts tokenized spacy package developed data processing tool tokenize texts prepare input nats survey created datasets text summarization headline generation respectively basic statistics shown table bytecup dataset byte cup international machine learning contest biendata com competition released new dataset referred bytecup dataset survey headline generation task consists million pieces articles million released training experiments create training development testing sets based training dataset texts tokenized stanford corenlp package prepared data processing tool basic statistics dataset shown table parameter settings experiments set dimension word embeddings hidden states encoder decoder respectively training embeddings learned scratch adam hyper parameter stochastic optimization learning rate fixed mini batches size gradient clipping maximum gradient norm datasets vocabulary consists k words shared source target cnn daily mail dataset truncate source articles tokens limit length summaries tokens newsroom dataset source articles summaries headlines truncated respectively bytecup dataset lengths source articles headlines limited tokens respectively training run epochs cnn daily mail dataset epochs newsroom bytecup dataset testing set size beam rouge evaluations recall oriented understudy gisting evaluation rouge scores introduced standard metrics evaluating abstractive text summarization models determine quality summarization counting number overlapping units e n grams word sequences word pairs machine generated golden standard human written summaries different rouge measures unigram bigram rouge l longest common subsequence widely single document abstractive summarization paper different models evaluated pyrouge python org pypi package provides precision recall f score measures acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models table rouge scores cnn daily mail dataset experiments model id attention coverage r l general general general general general general dot dot dot concat concat concat concat concat concat concat generator temporal decoder weight sharing experiments cnn daily mail dataset past years cnn daily mail dataset standard benchmark dataset evaluating performance different summarization models generate sentence summaries relatively longer documents experiments systematically investigated effects network components framework summarization performance including alignment methods attention mechanism pointing mechanism intra temporal attention iv intra decoder attention v weight sharing vi coverage mechanism experimental results shown table effectively represent different models id model consists letter followed binary indicators corresponding important components letters g d c denote alignment methods general dot concat respectively indicates component switched respectively clearly seen performance basic attention based models e close tests oov tokens generated summaries performing rouge evaluations results relatively lower rouge precision scores rouge f scores lower reported literature comparing find pointing mechanism significantly improves performance attention based models analyzing summaries observed tokens copied source articles results summaries similar ones generated extractive discussed section advantage pointing mechanism effectively handle oov tokens remaining components tested pointer generator network comparing intra temporal attention increases rouge points capability reducing repetitions models combine intra temporal attention concat failed training epochs report results intra decoder attention observe boost performance model adding weight sharing mechanism case concat models intra decoder attention better performance weight sharing mechanism boost performance models according comparison aforementioned models extractive models attempt extract sentences source articles acm trans data sci vol article publication date january shi al table rouge scores newsroom bytecup datasets newsroom summary model newsroom title r l adopt weight sharing mechanism fewer parameters finally find coverage mechanism significantly boost performance rouge points consistent results presented noted coverage mechanism work concat attention mechanism according section bytecup r l r l experiments newsroom bytecup datasets tested nats toolkit newsroom dataset released recently ments tokenized raw data different packages generated versions dataset task text summarization versions task headline generation experimental results obtained models released testing set shown table observed performs better cnn daily mail data table achieves better rouge scores text summarization headline generation tasks newsroom dataset finally summarize results bytecup headline generation dataset table achieves slightly better scores conclusion future directions successful applications models neural abstractive text marization prominent research topic gained lot attention industry academia paper provided comprehensive survey recent advances models task abstractive text summarization work primarily focuses challenges associated neural network architectures model parameter inference mechanisms summary generation procedures solutions different models algorithms provided taxonomy topics overview different models abstractive text summarization survey developed open source toolkit nats equipped important features including attention pointing mechanism repetition handling beam search experiments summarized experimental results different models literature widely cnn daily mail dataset conducted extensive experiments dataset nats examine effectiveness different neural network components finally established benchmarks recently released datasets e newsroom bytecup despite advances models abstractive text summarization task research challenges worth pursing future large transformers large scale models pre trained massive text corpora self supervised objectives fine tuned downstream tasks achieved state art performance variety summarization benchmark datasets pre trained encoder decoder model outperform previous state art results datasets fine tuning limited supervised examples shows pre trained models promising candidates zero shot low resource summarization reinforcement learning rl rl based training strategies incorporate user defined metrics including non differentiable ones rewards train summarization models metrics rouge bertscore saliency entailment rewards inferred natural language inference task improve current models rl leveraging external resources characteristics different datasets summary generation based summarization models rely beam search algorithm generate summaries recently based approaches achieved success open ended language generation acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models increase diversity generated texts promising research direction potentially increase novelty generated summaries sacrificing quality datasets based summarization models widely trained evaluated news corpora journalistic writing style promotes leading paragraphs news articles summaries causes models favor extraction abstraction alleviate problem researchers introduced datasets domains future new datasets likely released build better abstractive summarization systems evaluation automatic evaluation protocols e rouge bertscore sufficient evaluate overall quality generated summaries access critical features like factual correctness fluency relevance generated summaries human experts future research direction line building better evaluation systems current metrics capture important features agree humans example attempts generic text generation acknowledgments work supported national science foundation grants references mehdi allahyari seyedamin pouriyeh mehdi assefi saeid safaei elizabeth d trippe juan b gutierrez krys kochut text summarization techniques brief survey arxiv preprint dzmitry bahdanau philemon brakel kelvin xu anirudh goyal ryan lowe joelle pineau aaron courville yoshua bengio actor critic algorithm sequence prediction arxiv preprint dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate arxiv preprint dzmitry bahdanau jan chorowski dmitriy serdyuk philemon brakel yoshua bengio end end attention based large vocabulary speech recognition acoustics speech signal processing icassp ieee international conference ieee lalit bahl peter brown peter de souza robert mercer maximum mutual information estimation hidden markov model parameters speech recognition acoustics speech signal processing ieee international conference vol ieee hareesh bahuleyan lili mou olga vechtomova pascal poupart variational attention sequence sequence models coling david balduzzi muhammad ghifary strongly typed recurrent neural networks proceedings international conference international conference machine learning volume jmlr org samy bengio oriol vinyals navdeep jaitly noam shazeer scheduled sampling sequence prediction recurrent neural networks advances neural information processing systems yoshua bengio rjean ducharme pascal vincent christian jauvin neural probabilistic language model journal machine learning research feb yoshua bengio patrice simard paolo frasconi learning long term dependencies gradient descent difficult ieee transactions neural networks adam l berger vincent j della pietra stephen della pietra maximum entropy approach natural language processing computational linguistics neelima bhatia arunima jaiswal automatic text summarization s methods review cloud system big data engineering confluence international conference ieee samuel bowman gabor angeli christopher potts christopher d manning large annotated corpus learning natural language inference proceedings conference empirical methods natural language processing james bradbury stephen merity caiming xiong richard socher quasi recurrent neural networks arxiv preprint asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents abstractive summarization proceedings conference north american chapter association computational acm trans data sci vol article publication date january shi al linguistics human language technologies volume long papers vol danqi chen jason bolton christopher d manning thorough examination cnn daily mail reading comprehension task proceedings annual meeting association computational linguistics volume long papers vol qian chen xiaodan zhu zhenhua ling si wei hui jiang distraction based neural networks modeling documents proceedings fifth international joint conference artificial intelligence aaai press xiuying chen shen gao chongyang tao yan song dongyan zhao rui yan iterative document representation learning summarization polishing proceedings conference empirical methods natural language processing yen chun chen mohit bansal fast abstractive summarization reinforce selected sentence rewriting proceedings annual meeting association computational linguistics volume long papers association computational linguistics jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings annual meeting association computational linguistics volume long papers vol kyunghyun cho bart van merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio learning phrase representations rnn encoder decoder statistical machine translation proceedings conference empirical methods natural language processing emnlp sumit chopra michael auli alexander m rush abstractive sentence summarization attentive recurrent neural networks proceedings conference north american chapter association computational linguistics human language technologies junyoung chung caglar gulcehre kyunghyun cho yoshua bengio empirical evaluation gated recurrent neural networks sequence modeling nips workshop deep learning december junyoung chung kyle kastner laurent dinh kratarth goel aaron c courville yoshua bengio recurrent latent variable model sequential data advances neural information processing systems tong lee chung bin xu yongbin liu chunping ouyang main point generator summarizing focus international conference database systems advanced applications springer andr cibils claudiu musat andreea hossman michael baeriswyl diverse beam search increased novelty abstractive summarization arxiv preprint kevin clark minh thang luong quoc v le christopher d manning electra pre training text encoders discriminators generators international conference learning representations arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang nazli goharian discourse aware attention model abstractive summarization long documents proceedings conference north american chapter association computational linguistics human language technologies volume short papers vol vipul dalal latesh g malik survey extractive abstractive text summarization techniques emerging trends engineering technology icetet international conference ieee dipanjan das andr ft martins survey automatic text summarization literature survey language statistics ii course cmu yann n dauphin angela fan michael auli david grangier language modeling gated convolutional networks international conference machine learning jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language understanding proceedings conference north american chapter association computational linguistics human language technologies volume long short papers carl doersch tutorial variational autoencoders arxiv preprint li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou hsiao wuen hon unified language model pre training natural language understanding generation advances neural information processing systems jeffrey l elman finding structure time cognitive science alexander richard fabbri irene li tianwei suyi li dragomir radev multi news large scale multi document summarization dataset abstractive hierarchical model proceedings annual meeting association computational linguistics angela fan david grangier michael auli controllable abstractive summarization arxiv preprint acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models angela fan david grangier michael auli controllable abstractive summarization proceedings workshop neural machine translation generation mahak gambhir vishal gupta recent automatic text summarization techniques survey artificial intelligence review jonas gehring michael auli david grangier yann dauphin convolutional encoder model neural machine translation proceedings annual meeting association computational linguistics volume long papers vol jonas gehring michael auli david grangier denis yarats yann n dauphin convolutional sequence sequence learning international conference machine learning sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference empirical methods natural language processing kevin gimpel dhruv batra chris dyer gregory shakhnarovich systematic exploration diversity machine translation proceedings conference empirical methods natural language processing alex graves navdeep jaitly end end speech recognition recurrent neural networks international conference machine learning max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies proceedings conference north american chapter association computational linguistics human language technologies volume long papers vol jiatao gu zhengdong lu hang li victor ok li incorporating copying mechanism sequence sequence learning proceedings annual meeting association computational linguistics volume long papers vol caglar gulcehre sungjin ahn ramesh nallapati bowen zhou yoshua bengio pointing unknown words proceedings annual meeting association computational linguistics volume long papers vol han guo ramakanth pasunuru mohit bansal soft layer specific multi task summarization entailment question generation proceedings annual meeting association computational linguistics volume long papers association computational linguistics shengbo guo scott sanner probabilistic latent maximal marginal relevance proceedings international acm sigir conference research development information retrieval acm taher h haveliwala topic sensitive pagerank proceedings international conference world wide web acm karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend advances neural information processing systems sepp hochreiter yoshua bengio paolo frasconi jrgen schmidhuber al gradient flow recurrent nets difficulty learning long term dependencies sepp hochreiter jrgen schmidhuber long short term memory neural computation ari holtzman jan buys li du maxwell forbes yejin choi curious case neural text degeneration arxiv preprint wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun unified model extractive abstractive summarization inconsistency loss arxiv preprint dichao hu introductory survey attention mechanisms nlp problems arxiv preprint luyang huang lingfei wu lu wang knowledge graph augmented abstractive summarization semantic driven cloze reward arxiv preprint hakan inan khashayar khosravi richard socher tying word vectors word classifiers loss framework language modeling arxiv preprint daphne ippolito reno kriz joao sedoc maria kustikova chris callison burch comparison diverse decoding methods conditional language models proceedings annual meeting association computational linguistics yichen jiang mohit bansal closed book training improve summarization encoder memory proceedings conference empirical methods natural language processing nal kalchbrenner lasse espeholt karen simonyan aaron van den oord alex graves koray kavukcuoglu neural machine translation linear time arxiv preprint yaser keneshloo naren ramakrishnan chandan k reddy deep transfer reinforcement learning text summarization proceedings siam international conference data mining siam acm trans data sci vol article publication date january shi al yaser keneshloo tian shi naren ramakrishnan chandan k reddy deep reinforcement learning sequence sequence models arxiv preprint arxiv byeongchang kim hyunwoo kim gunhee kim abstractive summarization reddit posts level memory networks proceedings conference north american chapter association computational linguistics human language technologies volume long short papers diederik p kingma jimmy ba adam method stochastic optimization arxiv preprint diederik p kingma max welling auto encoding variational bayes arxiv preprint guillaume klein yoon kim yuntian deng jean senellart alexander rush open source toolkit neural machine translation proceedings acl system demonstrations mahnaz koupaee william yang wang wikihow large scale text summarization dataset arxiv preprint jonathan krause justin johnson ranjay krishna li fei fei hierarchical approach generating descriptive image paragraphs proceedings ieee conference computer vision pattern recognition alex krizhevsky ilya sutskever geoffrey e hinton imagenet classification deep convolutional neural networks advances neural information processing systems wojciech kryscinski nitish shirish keskar bryan mccann caiming xiong richard socher neural text summarization critical evaluation proceedings conference empirical methods natural language processing international joint conference natural language processing emnlp ijcnlp wojciech kryciski romain paulus caiming xiong richard socher improving abstraction text summarization proceedings conference empirical methods natural language processing mike lewis yinhan liu naman goyal marjan ghazvininejad abdelrahman mohamed omer levy ves stoyanov luke zettlemoyer bart denoising sequence sequence pre training natural language generation translation comprehension arxiv preprint chenliang li weiran xu si li sheng gao guiding generation abstractive text summarization based key information guide network proceedings conference north american chapter association computational linguistics human language technologies volume short papers vol jiwei li michel galley chris brockett jianfeng gao bill dolan diversity promoting objective function neural conversation models proceedings conference north american chapter association computational linguistics human language technologies jiwei li dan jurafsky mutual information diverse decoding improve neural machine translation arxiv jiwei li monroe dan jurafsky simple fast diverse decoding algorithm neural generation arxiv preprint preprint jiwei li monroe alan ritter dan jurafsky michel galley jianfeng gao deep reinforcement learning dialogue generation proceedings conference empirical methods natural language processing piji li lidong bing wai lam actor critic based training framework abstractive summarization arxiv preprint piji li wai lam lidong bing zihao wang deep recurrent generative decoder abstractive text summarization proceedings conference empirical methods natural language processing chin yew lin rouge package automatic evaluation summaries text summarization branches junyang lin xu sun shuming ma qi su global encoding abstractive summarization proceedings annual meeting association computational linguistics volume short papers association computational linguistics jeffrey ling alexander rush coarse fine attention models document summarization proceedings workshop new frontiers summarization linqing liu yao lu min yang qiang qu jia zhu hongyan li generative adversarial network abstractive text summarization arxiv preprint xiaodong liu pengcheng weizhu chen jianfeng gao multi task deep neural networks natural language understanding proceedings annual meeting association computational linguistics yang liu mirella lapata text summarization pretrained encoders proceedings conference empirical methods natural language processing international joint conference natural language processing emnlp ijcnlp acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models yinhan liu myle ott naman goyal jingfei du mandar joshi danqi chen omer levy mike lewis luke zettlemoyer veselin stoyanov roberta robustly optimized bert pretraining approach arxiv preprint elena lloret manuel palomar text summarisation progress literature review artificial intelligence konstantin lopyrev generating news headlines recurrent neural networks arxiv preprint review thang luong hieu pham christopher d manning effective approaches attention based neural machine translation proceedings conference empirical methods natural language processing inderjeet mani mark t maybury advances automatic text summarization mit press joshua maynez shashi narayan bernd bohnet ryan mcdonald faithfulness factuality abstractive summarization arxiv preprint bryan mccann nitish shirish keskar caiming xiong richard socher natural language decathlon multitask learning question answering arxiv preprint yishu miao phil blunsom language latent variable discrete generative models sentence compression proceedings conference empirical methods natural language processing yajie miao mohammad gowayyed florian metze eesen end end speech recognition deep rnn models wfst based decoding automatic speech recognition understanding asru ieee workshop ieee rada mihalcea paul tarau textrank bringing order text proceedings conference empirical methods natural language processing n moratanch s chitrakala survey abstractive text summarization circuit power computing technologies iccpct international conference ieee ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents ramesh nallapati bowen zhou cicero dos santos glar gulehre bing xiang abstractive text summarization sequence sequence rnns conll courtney napoles matthew gormley benjamin van durme annotated gigaword proceedings joint workshop automatic knowledge base construction web scale knowledge extraction association computational linguistics shashi narayan shay b cohen mirella lapata dont details summary aware convolutional neural networks extreme summarization proceedings conference empirical methods natural language processing preksha nema mitesh m khapra anirban laha balaraman ravindran diversity driven attention model query based abstractive summarization proceedings annual meeting association computational linguistics volume long papers vol ani nenkova kathleen mckeown al automatic summarization foundations trends information retrieval franz josef och minimum error rate training statistical machine translation proceedings annual meeting association computational linguistics volume association computational linguistics lawrence page sergey brin rajeev motwani terry winograd pagerank citation ranking bringing order web technical report stanford infolab kishore papineni salim roukos todd ward wei jing zhu bleu method automatic evaluation machine translation proceedings annual meeting association computational linguistics association computational linguistics razvan pascanu tomas mikolov yoshua bengio difficulty training recurrent neural networks international conference machine learning ramakanth pasunuru mohit bansal multi reward reinforced summarization saliency entailment proceedings conference north american chapter association computational linguistics human language technologies volume short papers vol ramakanth pasunuru han guo mohit bansal improving abstractive summarization entailment generation proceedings workshop new frontiers summarization romain paulus caiming xiong richard socher deep reinforced model abstractive summarization arxiv preprint pavan kartheek rachabathuni survey abstractive summarization techniques inventive computing informatics icici international conference ieee acm trans data sci vol article publication date january shi al dragomir r radev eduard hovy kathleen mckeown introduction special issue summarization computational linguistics colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li peter j liu exploring limits transfer learning unified text text transformer arxiv preprint marcaurelio ranzato sumit chopra michael auli wojciech zaremba sequence level training recurrent neural networks arxiv preprint steven j rennie etienne marcheret youssef mroueh jerret ross vaibhava goel self critical sequence training image captioning computer vision pattern recognition cvpr ieee conference ieee danilo jimenez rezende shakir mohamed daan wierstra stochastic backpropagation approximate inference deep generative models international conference machine learning alexander m rush sumit chopra jason weston neural attention model abstractive sentence summarization proceedings conference empirical methods natural language processing horacio saggion thierry poibeau automatic text summarization past present future multi source multilingual information extraction summarization springer baskaran sankaran haitao mi yaser al onaizan abe ittycheriah temporal attention model neural machine translation corr abigail peter j liu christopher d manning point summarization pointer generator networks proceedings annual meeting association computational linguistics volume long papers association computational linguistics thibault sellam dipanjan das ankur p parikh bleurt learning robust metrics text generation arxiv preprint eva sharma chen li lu wang bigpatent large scale dataset abstractive coherent summarization proceedings annual meeting association computational linguistics shiqi shen yong cheng zhongjun wei hua wu maosong sun yang liu minimum risk training neural machine translation proceedings annual meeting association computational linguistics volume long papers vol shi qi shen yan kai lin cun chao tu yu zhao zhi yuan liu mao song sun al recent advances neural headline generation journal computer science technology tian shi ping wang chandan k reddy leafnats open source toolkit live demo system neural abstractive text summarization proceedings conference north american chapter association computational linguistics demonstrations kaiqiang song lin zhao fei liu structure infused copy mechanisms abstractive summarization proceedings international conference computational linguistics association computational linguistics ilya sutskever training recurrent neural networks university toronto toronto ontario canada ilya sutskever oriol vinyals quoc v le sequence sequence learning neural networks advances neural information processing systems richard s sutton andrew g barto reinforcement learning introduction mit press sho takase jun suzuki naoaki okazaki tsutomu hirao masaaki nagata neural headline generation abstract meaning representation proceedings conference empirical methods natural language processing jiwei tan xiaojun wan jianguo xiao abstractive document summarization graph based attentional neural model proceedings annual meeting association computational linguistics volume long papers vol zhaopeng tu zhengdong lu yang liu xiaohua liu hang li modeling coverage neural machine translation proceedings annual meeting association computational linguistics volume long papers vol aron van den oord sander dieleman heiga zen karen simonyan oriol vinyals alex graves nal kalchbrenner andrew senior koray kavukcuoglu d wavenet generative model raw audio isca speech synthesis workshop ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention need advances neural information processing systems arun venkatraman martial hebert j andrew bagnell improving multi step prediction learned time series models ninth aaai conference artificial intelligence acm trans data sci vol article publication date january neural abstractive text summarization sequence sequence models rakesh m verma daniel lee extractive summarization limits compression generalized model heuristics computacin y sistemas ashwin k vijayakumar michael cogswell ramprasath r selvaraju qing sun stefan lee david crandall dhruv batra diverse beam search decoding diverse solutions neural sequence models arxiv preprint oriol vinyals meire fortunato navdeep jaitly pointer networks advances neural information processing systems li wang junlin yao yunzhe tao li zhong wei liu qiang du reinforced topic aware tional sequence sequence model abstractive text summarization proceedings international joint conference artificial intelligence aaai press lex weaver nigel tao optimal reward baseline gradient based reinforcement learning proceedings seventeenth conference uncertainty artificial intelligence morgan kaufmann publishers inc fei liu yang gao christian m meyer steffen eger wei zhao maxime peyrard moverscore text generation evaluating contextualized embeddings earth mover distance proceedings conference empirical methods natural language processing association computational linguistics hong kong china paul j werbos backpropagation time proc ieee ronald j williams simple statistical gradient following algorithms connectionist reinforcement learning ronald j williams david zipser learning algorithm continually running fully recurrent neural reinforcement learning springer networks neural computation yuxiang wu baotian hu learning extract coherent summary deep reinforcement learning yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al google s neural machine translation system bridging gap human machine translation arxiv preprint yingce xia fei tian lijun wu jianxin lin tao qin nenghai yu tie yan liu deliberation networks sequence generation pass decoding advances neural information processing systems kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel yoshua bengio attend tell neural image caption generation visual attention international conference machine learning yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang ming zhou prophetnet predicting future n gram sequence sequence pre training arxiv preprint zhilin yang zihang dai yiming yang jaime carbonell russ r salakhutdinov quoc v le xlnet generalized autoregressive pretraining language understanding advances neural information processing systems zichao yang diyi yang chris dyer xiaodong alex smola eduard hovy hierarchical attention networks document classification proceedings conference north american chapter association computational linguistics human language technologies wojciech zaremba ilya sutskever reinforcement learning neural turing machines revised arxiv preprint wenyuan zeng wenjie luo sanja fidler raquel urtasun efficient summarization read copy mechanism arxiv preprint jingqing zhang yao zhao mohammad saleh peter j liu pegasus pre training extracted sentences abstractive summarization arxiv preprint tianyi zhang varsha kishore felix wu kilian q weinberger yoav artzi bertscore evaluating text generation bert arxiv preprint xingxing zhang mirella lapata sentence simplification deep reinforcement learning proceedings conference empirical methods natural language processing xingxing zhang mirella lapata furu wei ming zhou neural latent extractive document summarization proceedings conference empirical methods natural language processing yuhao zhang daisy yi ding tianpei qian christopher d manning curtis p langlotz learning summarize radiology findings emnlp qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural document rization jointly learning score select sentences proceedings annual meeting association computational linguistics volume long papers vol qingyu zhou nan yang furu wei ming zhou selective encoding abstractive sentence summarization proceedings annual meeting association computational linguistics volume long papers vol acm trans data sci vol article publication date january
