lcsts a large scale chinese short text summarization dataset baotian hu qingcai chen fangze zhu intelligent computing research center harbin institute of technology shenzhen graduate school baotianchina qingcai com abstract automatic text summarization is widely regarded as the highly difcult problem partially because of the lack of large text summarization data set due to the great challenge of constructing the large scale summaries for full text in this per we introduce a large corpus of nese short text summarization dataset structed from the chinese microblogging website sina weibo which is released to the this corpus consists of over million real chinese short texts with short summaries given by the author of each text we also manually tagged the relevance of short summaries with their corresponding short texts based on the corpus we introduce recurrent neural network for the summary generation and achieve promising results which not only shows the usefulness of the proposed pus for short text summarization research but also provides a baseline for further search on this topic introduction nowadays individuals or organizations can ily share or post information to the public on the social network take the popular chinese croblogging website sina weibo as an example the people s daily one of the media in china posts more than tens of weibos analogous to tweets each day most of these weibos are written and highly informative because of the text length limitation less chinese ters such data is regarded as naturally annotated web resources sun if we can mine these high quality data from these naturally annotated web resources it will be benecial to the research that has been hampered by the lack of data hitsz edu cn article html figure a weibo posted by people s daily in the natural language processing nlp community automatic text summarization is a hot and difcult task a good summarization system should understand the whole text and re organize the information to generate coherent informative and signicantly short summaries which convey important information of the original text hovy and lin martins most of tional abstractive summarization methods divide the process into two phrases bing et al first key textual elements are extracted from the original text by using unsupervised methods or guistic knowledge and then unclear extracted components are rewritten or paraphrased to duce a concise summary of the original text by using linguistic rules or language generation niques although extensive researches have been done the linguistic quality of abstractive mary is still far from satisfactory recently deep learning methods have shown potential abilities to learn representation hu et al zhou et al and generate language bahdanau et al sutskever et al from large scale data by utilizing gpus many researchers realize that we are closer to generate abstractive tions by using the deep learning methods ever the publicly available and high quality large scale summarization data set is still very rare and not easy to be constructed manually for ple the popular document summarization dataset and have only hundreds of human written english text summarizations the in this problem is even worse for chinese nist gov data html nist gov nist e f l c s c v v i x r a figure diagram of the process for creating the dataset per we take one step back and focus on ing lcsts the large scale chinese short text summarization dataset by utilizing the naturally annotated web resources on sina weibo figure shows one weibo posted by the people s daily in order to convey the import information to the lic quickly it also writes a very informative and short summary in the blue circle of the news our goal is to mine a large scale high quality short text summarization dataset from these texts this paper makes the following contributions we introduce a large scale chinese short text summarization dataset to our knowledge it is the largest one to date we provide standard splits for the dataset into large scale training set and human labeled test set which will be easier for benchmarking the related methods we explore the properties of the dataset and sample instances for manually checking and scoring the quality of the dataset we perform recurrent neural network based encoder decoder method on the dataset to generate summary and get ing results which can be used as one baseline of the task related work our work is related to recent works on automatic text summarization and natural language ing based on naturally annotated web resources which are briey introduced as follows automatic text summarization in some form has been studied since since then most searches are related to extractive summarizations by analyzing the organization of the words in the document nenkova and mckeown luhn since it needs labeled data sets for pervised machine learning methods and labeling dataset is very intensive some researches focused on the unsupervised methods mihalcea the scale of existing data sets are usually very small most of them are less than for example dataset contains ments and each document is provided with two words human summaries our work is also related to the headline generation which is a task to generate one sentence of the text it entitles colmenares et al construct a million nancial news headline dataset written in english for line generation colmenares et al ever the data set is not publicly available naturally annotated web resources based natural language processing is proposed by sun sun naturally annotated web sources is the data generated by users for nicative purposes such as web pages blogs and microblogs we can mine knowledge or useful data from these raw data by using marks generated by users unintentionally jure et al track lion mainstream media sites and blogs and mine a set of novel and persistent temporal patterns in the news cycle leskovec et al sepandar et al use the users naturally annotated pattern we feel and i feel to extract the feeling sentence tion which is used to collect the world s emotions in this work we use the naturally annotated sources to construct the large scale chinese short text summarization data to facilitate the research on text summarization data collection a lot of popular chinese media and organizations have created accounts on the sina weibo they use their accounts to post news and information these accounts are veried on the weibo and beled by a blue v in order to guarantee the ity of the crawled text we only crawl the veried organizations weibos which are more likely to be clean formal and informative there are a lot of human intervention required in each step the cess of the data collection is shown as figure and user crawlerselectingtext andextractingdata setsocial mediaraw textseedschosen usersusers collection summarized as follows we rst collect very popular tion users as seeds they come from the domains of politic economic military movies game and such as people s daily the economic observe press the ministry of national defense and we then crawl fusers followed by these seed users and lter them by using human written rules such as the user must be blue veried the number of followers is more than million and we use the chosen users and text crawler to crawl their weibos we lter clean and extract short text summary pairs about rules are used to tract high quality pairs these rules are concluded by peoples via carefully investigating of the raw text we also remove those paris whose short text length is too short less than characters and length of summaries is out of data properties the dataset consists of three parts shown as ble and the length distributions of texts are shown as figure part i is the main content of lcsts that tains short text summary pairs these pairs can be used to train supervised learning model for summary generation part ii contains the human labeled short text summary pairs with the score ranges from to that indicates the relevance between the short text and the corresponding summary denotes the least relevant and denotes the most relevant for annotating this part we recruit volunteers each pair is only labeled by one notator these pairs are randomly sampled from part i and are used to analysize the distribution of pairs in the part i figure illustrates examples of different scores from the examples we can see that pairs scored by or are very relevant to the corresponding summaries these summaries are highly informative concise and signicantly short compared to original text we can also see that many words in the summary do not appear in the original text which indicates the signicant difference of our dataset from sentence sion datasets the summaries of pairs scored by or are highly abstractive and relatively hard to conclude the summaries from the short text they are more likely to be headlines or comments stead of summaries the statistics show that the percent of score and is less than of the figure box plot of lengths for short segmented short and segmented sum the red line denotes the median and the edges of the box the quartiles st data which can be ltered by using trained er part iii contains pairs for this part annotators label the same texts and we tract the text with common scores this part is independent from part i and part ii in this work we use pairs scored by and of this part as the test set for short text summary generation task part i part ii part iii number of pairs human score human score human score human score human score number of pairs human score human score human score human score human score table data statistics experiment recently recurrent neural network rnn have shown powerful abilities on speech tion graves et al machine tion sutskever et al and automatic dialog response shang et al however there is rare research on the automatic text summarization by using deep models in this section we use rnn as encoder and decoder to generate the summary of short text we use the part i as the training set figure the graphical depiction of rnn encoder and decoder framework without context figure the graphical depiction of the rnn coder and decoder framework with context text is segmented into chinese words by using the vocabulary is limited to we adopt two deep architectures the local text is not used during decoding we use the rnn as encoder and it s last hidden state as the input of decoder as shown in figure the context is used during decoding following danau et al we use the combination of all the hidden states of encoder as input of the decoder as shown in figure for the rnn we adopt the gated recurrent unit gru which is proposed by chung et al and has been proved comparable to lstm chung et al all the parameters including the embeddings of the two architectures are randomly initialized and adadelta zeiler is used to update the learning rate after the model is trained the beam search is used to generate the best summaries in the process of decoding and the size of beam is set to in our experiment python org pypi figure five examples of different scores and the subset of part iii which is scored by and as test set two approaches are used to preprocess the data character based method we take the chinese character as input which will reduce the ulary size to word based method the thechiefsecretaryofthewaterdevisionoftheministryofwaterresources revealedtodayatapressconference accordingtothejust completedassessmentofwaterresourcesmanagementsystem someprovincesareclosedtotheredlineindicator someprovincesareovertheredlineindicator implementstrictlywaterresourcesassessmentandtheapprovalofwaterlicensing somewaterprojectwillbe userspreferenceofshoppingthroughpcscannotbechangedintheshortterm mobileterminalswillbecomethestrategicdevelopmentdirection andalso itwillbecomeoffdlinedrivingfromondlinedriving thefirstandsecondtiercitiesarefacinggrowthdifficulties however amongthem guangzhou beijing shenzhen daweizhang fromcentalinepropertyagency thepriceofhouseislikelytoriseandhardtofall theirexplanationsarethatacommunicationsbreakdownandheavyrainledtoadatauploadextension comparingtolastyear analysisoftheoutsideworldbelievethatitisrelatedtotherecentofficialintensiveantitrustinvestigation severalcaseswillnotscareforeigninvestorsaway decodercontext generator model rnn rnn context data word char word char r l table the experiment result word and char denote the word based and based input respectively figure an example of the generated summaries for evaluation we adopt the rouge metrics proposed by lin and hovy which has been proved strongly correlated with human uations and rouge l are used because the standard rouge package is used for evaluating english summarization tems we transform the chinese words to cal ids to adapt to the systems all the models are trained on the gpus tesla for about one week table lists the experiment results as we can see in figure the summaries generated by rnn with context are very close to human written summaries which indicates that if we feed enough data to the rnn encoder and decoder it may erate summary almost from scratch the results also show that the rnn with text outperforms rnn without context on both character and word based input this result cates that the internal hidden states of the rnn encoder can be combined to represent the context of words in summary and also the performances of the character based input outperform the based input as shown in figure the summary generated by rnn with context by inputing the character based short text is relatively good while berouge com pages default aspx figure an example of the generated summaries with unks the the summary generated by rnn with context on word based input contains many unks this may attribute to that the segmentation may lead to many unks in the vocabulary and text such as the person name and organization name for ple is a company name which is not in the vocabulary of word based rnn the rnn summarizer has to use the unks to replace the in the process of decoding conclusion and future work we constructed a large scale chinese short text summarization dataset and performed rnn based methods on it which achieved some promising sults this is just a start of deep models on this task and there is much room for improvement we take the whole short text as one sequence this may not be very reasonable because most of short texts contain several sentences a hierarchical rnn li et al is one possible direction the rare word problem is also very important for the ation of the summaries especially when the input is word based instead of character based it is also a hot topic in the neural generative models such as neural translation luong et al which can benet to this task we also plan to construct a large document summarization data set by using naturally annotated web resources acknowledgments industry development shenzhen supported by national this work is ral science foundation of china and strategic funds special ing of and we thank to baolin peng lin ma li yu and the anonymous reviewers for their insightful comments references bahdanau et al dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation by jointly learning to align and translate corr bing et al lidong bing piji li yi liao wai lam weiwei guo and rebecca passonneau abstractive multi document summarization via phrase selection and merging in proceedings of the acl ijcnlp pages beijing china july association for computational linguistics chung et al junyoung chung c aglar gulcehre kyunghyun cho and yoshua bengio pirical evaluation of gated recurrent neural networks on sequence modeling corr chung et al junyoung chung c aglar gulcehre kyunghyun cho and yoshua bengio gated feedback recurrent neural networks corr colmenares et al carlos a colmenares rina litvak amin mantrach and fabrizio vestri heads headline generation as sequence prediction using an abstract feature rich in proceddings of conference of the space north american chapter of the association for computational linguistics human language nologies naacl hlt graves al alex graves abdel rahman speech hamed and geoffrey e hinton recognition with deep recurrent neural networks corr hovy and eduard hovy and chin yew lin automated text summarization and the in proceedings of a workshop on marist system held at baltimore maryland october tipster pages stroudsburg pa usa association for computational linguistics et al baotian hu zhengdong lu hang li and qingcai chen convolutional neural network architectures for matching natural language sentences in advances in neural information cessing systems pages curran ciates inc et al jure leskovec lars backstrom and jon kleinberg meme tracking and the in proceedings of dynamics of the news cycle the acm sigkdd international conference on knowledge discovery and data mining kdd pages et al jiwei li minh thang luong and dan jurafsky a hierarchical neural autoencoder for paragraphs and documents acl in proceedings of lin and chin yew lin and e h hovy automatic evaluation of summaries using in proceedings n gram co occurrence statistics of language technology conference naacl edmonton canada h p luhn the automatic creation of literature abstracts ibm journal of research and development luong et al thang luong ilya sutskever quoc v le oriol vinyals and wojciech zaremba addressing the rare word problem in neural machine translation corr dipanjan das andr f t martins a survey on automatic text summarization cal report cmu rada mihalcea graph based ranking algorithms for sentence extraction applied to text summarization in proceedings of the annual meeting of the association for tional linguistics companion volume spain nenkova and ani nenkova and leen mckeown automatic summarization foundations and trend in information retrieval shang et al lifeng shang zhengdong lu and hang li neural responding machine for short text conversation corr mao song sun natural language procesing based on naturaly annotated web sources journal of chinese information ing sutskever et al ilya sutskever oriol vinyals and quoc v v le sequence to sequence learning with neural networks in advances in ral information processing systems pages matthew d zeiler adadelta corr an adaptive learning rate method et al xiaoqiang zhou baotian hu cai chen buzhou tang and xiaolong wang answer sequence learning with neural networks for answer selection in community question answering in proceedings of the acl ijcnlp pages beijing china july association for computational linguistics
