a m l c s c v v i x r a using statistical and semantic models for multi document summarization divyanshu daiya lnm institute of information technology jaipur rajasthan com anukarsh singh lnm institute of information technology jaipur rajasthan com mukesh jadon lnm institute of information technology jaipur rajasthan com abstract we report a series of experiments with different semantic models on top of ious statistical models for extractive text summarization though statistical models may better capture word co occurrences and distribution around the text they fail to detect the context and the sense of tences as a whole semantic els help us gain better insight into the text of sentences we show that how ing weights between different models can help us achieve signicant results on ious benchmarks learning pre trained vectors used in semantic models further on given corpus can give addition spike in performance using weighing techniques in between various statistical models too further renes our result for cal models we have used tf idf trank jaccard cosine similarities for semantic models we have used based model and proposed two models based on glove vectors and facebook s infersent we tested our approach on duc dataset generating word summaries we have discussed the system algorithms analysis and also proposed and tested possible improvements rouge scores lin were used to compare to other summarizers introduction automatic text summarization deals with the task of condensing documents into a summary whose level is similar to a human generated mary it is mostly distributed into two distinct domains i e abstractive summarization and tractive summarization abstractive both the authors have contributed equally to this work tion dejong et al involves models to it then presents duce the crux of the document a summary consisting of words and phrases that were not there in the actual document sometimes even et al a state of art method proposed by wenyuan zeng zeng et al produces such summaries with length restricted to there have been many cent developments that produce optimal results but it is still in a developing phase it highly relies on natural language processing techniques which is still evolving to match human standards these shortcomings make abstractive tion highly domain selective as a result their plication is skewed to the areas where nlp niques have been superlative extractive rization on the other hand uses different ods to identify the most informative dominant tences through the text and then present the sults ranking them accordingly in this paper we have proposed two novel stand alone tion methods the rst method is based on glove model pennington et al other is based on facebook s infersent conneau et al we have also discussed how we can effectively subdue shortcomings of one model by using it in coalition with models which capture the view that other faintly held related work a vast number of methods have been used for document summarization some of the methods include determining the length and positioning radev et al of sentences in the text deducing centroid terms to nd the importance of text radev et al and setting a threshold on average tf idf scores bag of words approach i e making sentence word freq matrix using a signature set of words and assigning them weights to use them as a criterion for importance measure lin and hovy have also been it great semantic summarizers in other words miller et al common among semantic used summarization using weights on frequency words nenkova et al describes that high frequency terms can be used to deduce the core of document like lexical while similarity is based on the assumption that portant sentences are identied by strong chains gupta et al barrera and verma relates murdock sentences that employ words with the same meaning synonyms or other semantic relation to nd it uses wordnet similarity among words that apply to word frequency algorithm of speech ging and sense disambiguation summarizers are graphical summarizers like textrank have also provided results textrank benchmark assigns weights to important keywords from the document using graph based model and sentences which capture most of those concepts keywords barrera and verma are ranked higher mihalcea and tarau uses textrank google s pagerank brin and page for graphical modeling though semantic and graphical models may better capture the sense of document but miss out on statistical view there summarizers of there have nt been many studies made in the area et al conducted some preliminary research but there is nt much there on benchmark tests to our knowledge we use a mixture of statistical and semantic models assign weights among them by training on eld specic corpora as there is a signicant variation in choices among different elds we support our proposal with expectations that shortcomings posed by one model can be lled with positives from others we deploy experimental analysis to test our proposition a void hybrid is proposed approach for statistical analysis we use similarity matrices word co n gram model andtf idf matrix for semantic analysis we use custom glove based model wordnet based model and facebook infersent conneau et al based model for multi document summarization after training on corpus we assign weights among the different techniques we store the sense vector for documents along with weights for future ence for single document summarization rstly we calculate the sense vector for that document and calculate the nearest vector from the stored vectors we use the weights of the nearest vector we will describe the ow for semantic and tical models separately prepossessing we discuss in detail the steps that are common for both statistical and semantic models sentence tokenizer we use nltk sentence tokenizer sent tokenize based on punkt tokenizer pre trained on a pus it can differentiate between mr mrs and other abbreviations and the normal sentence boundaries kiss and strunk given a document d we tokenize it into sentences as sn cleaning replacing all the special characters with spaces for easier word tagging and tokenizing word tokenizer we use nltk word tokenizer which is a penn treebankstyle tokenizer to tokenize words we calculate the total unique words in the document if we can write any sentence si wi wj wk i n then the number of unique words can be represented i j k l m n t otalsentences m t otaluniquewords using stastical models similarity correlation matrices frequency matrix generation our tokenized words contain redundancy due to digits and sitional words such as and but which carry little information such words are termed stop words wilbur and sirotkin we moved stop words and words occurring in and of the documents considering the word frequency over all documents after the moval the no of unique words left in the ticular document be p where p m where m is the total no of unique words in our tokenized list inally we now formulate a matrix fnp where n we use two similarity measures initialize r as is the total number of sentences and p is the total number of unique words left in the document ement eij in the matrix fnp denotes frequency of jth unique word in the ith sentence similarity correlation matrix generation we now have have sentence word frequency vector sfi as where fia denotes frequency of ath unique word in the ith sentence we now compute sentence sfj jaccard similarity cosine similarity we generate the similarity matrix simj nn for each of the similarity measure where j indexes the similarity measure element eij of simj nn denotes similarity between ith and jth sentence consequentially we will end up with nn and nn corresponding to each similarity sure jaccard similarity for some sets a and b a b c and y respectively the card similarity is dened jaccard b b b generate pagerank or probability distribution trix p p p where p in original paper denoted bility with which a randomly browsing user lands on a particular page for the summarization task they denote how strongly a sentence is connected with rest of document or how well sentence tures multiple views concepts the steps are as p p p n n n dene d probability that randomly chosen sentence is in summary and as measure of change i e to stop computation when ence between to successive r computations recedes below using cosine similarity matrix nn we generate the following equation as a measure for relation between cosine similarity the cosine distance tween u and v is dened r nn r cosine b u v repeat last step until where u v is the dot product of u and v take top ranking sentences in r for pagerank pagerank algorithm page et al devised to rank web pages forms the core of google search it roughly works by ranking pages ing to the number and quality of outsourcing links from the page for nlp a pagerank based nique textrank has been a major breakthrough in the eld textrank based summarization has seeded exemplary results on benchmarks we use a naive textrank analogous for our task given n sentences sn we intend to mary tf idf term of words is the count of how many times a word occurs in the given ument inverse document is the number of times word occurs in complete corpus infrequent words through corpus will have higher weights while weights for more frequent words will be depricated underlying steps for tf idf summarization are create a count vector additional pre processing f rw rw rw part of tagging we tag the words using nltk pos tagger build a tf idf matrix wm n with element lemmatization we use ntlk lemmatizer wi as with pos tags passed as contexts wi j tfi j log n dfi here tfi j denotes term frequency of ith word in jth sentence and log n dfi sents the idf frequency score each sentence taking into tion only nouns we use nltk pos tagger for identifying nouns j p n n oi j np j applying positional weighing j o t o sentence index t total sentences in document j summarize using top ranking sentences using semantic models we proceed in the same way as we did for tistical models all the pre processing steps main nearly same we can make a little change by using lemmatizer instead of stemmer stemming involves removing the derivational afxes end of words by heuristic analysis in hope to achieve base form lemmatization on the other hand volves rstly pos tagging santorini and after morphological and vocabulary analysis ducing the word to its base form stemmer put for goes is goe while lemmatized output with the verb passed as pos tag is go though lemmatization may have little more time head as compared to stemming it necessarily vides better base word reductions since net pedersen et al and glove both require dictionary look ups in order for them to work well we need better base word mappings hence lemmatization is preferred using wordnet we generated similarity matrices in the case of statistical models we will do the same here but for sentence similarity measure we use the method devised by dao dao and simpson the method is dened as word sense we use the adapted version of lesk as devised by dao to rive the sense for each word sentence pair similarity for each pair of sentences we create semantic similarity trix s let a and b be two sentences of lengths m and n respectively then the sultant matrix s will be of size m n with element si denoting semantic similarity tween sense synset of word at position i in sentence a and sense synset of word at sition j in sentence b which is calculated by path length similarity using is a nym hyponym hierarchies it uses the idea that shorter the path length higher the larity to calculate the path length we ceed in following for two words and with synsets and respectively smn sn sj sn we formulate the problem of capturing mantic similarity between sentences as the problem of computing a maximum total matching weight of a bipartite graph where x and y are two sets of disjoint nodes we use the hungarian method kuhn to solve this problem finally we get bipartite matching matrix b with entry bi denoting matching between and to obtain the overall similarity we use dice coefcient encode each sentence to generate its vector i i representation i i b calculate similarity between sentence pair using cosine distance with threshold set to and ing lengths of sentence a and b respectively populate similarity matrix n n using previous step we perform the previous step over all pairs to generating summaries generate the similarity matrix using glove model glove model provides us with a convenient method to represent words as vectors using tors representation for words we generate vector representation for sentences we work in the lowing order represent each tokenized word wi in its tor form i i i i represent each sentence into vector using following equation sv fi i i i x where fi j being frequency of wi in sj calculate similarity between sentences using cosine distance between two sentence tors populate similarity matrix n n using previous step the model using facebook s infersent infersent is a state of the art supervised sentence encoding technique conneau et al it outperformed another state of the art sentence encoder skipthought on several benchmarks the sts benchmark like ehu stswiki index stsbenchmark is trained on stanford natural language inference snli bowman et al using seven dataset chitectures long short term memory lstm forward and gated recurrent units backward gru with hidden states concatenated bi directional lstms bilstm with min max pooling self attentive network and hcn erarchical convolutional networks the network performances are task corpus specic steps to generate similarity matrix gru n n are tf idf scores and textrank allows us to directly rank sentences and choose k top sentences where k is how many sentences user want in the mary on the other hand the similarity matrix based approach is used in case of all semantic models and similarity correlation based cal models to rank sentences from similarity trix we can use following ranking through relevance score for each sentence si in similarity matrix the relevance score is n j p we can now choose k top ranking tences by rscores higher the rscore higher the rank of sentence hierarchical clustering given a similarity matrix simn let sa an individual element then cal clustering is performed as initialize a empty list r choose element with highest similarity value let it be si where j j c replace values in column and row i in following sd i sd j n d n replace entries corresponding to umn and row i by zeros add i and to r if they are not already there repeat steps until single single zero element remains for remaining non zero element apply step and minate g we will have rank list r in the end we can now choose k top ranking sentences from r performances of different models over the training data to ne tune our summary single document summarization after generating summary from a particular model our aim is to compute summaries through overlap of different models let us have g summaries from g different models for pth marization model let the k sentences contained sump p now for our list of sentences sn we dene cweight as weight obtained for each sentence using g models cw si p here si is a function which returns if sentence is in summary of jth model otherwise zero wi is weight assigned to each model without training wi g i g multi document domain specic summarization the elemental concept we here use machine learning based approach to further increase the quality of our rization technique is that we use training set of u domain specic documents with gold standard human composed summaries provided we ne tune our weights wii g for different models taking score f measure powers as factor f precision recall precision recall we proceed in the following for each document in training set generate summary using each model independently compute the w t gold summary for each model assign the weights using wi p i g v i u here i ith document denotes for jth model in we now obtain cweight as we did previously and formulate cumulative summary capturing the sensus of different models we hence used a pervised learning algorithm to capture the mean domain specic single document summarization as we discussed earlier summarization models are eld selective some models tend to perform remarkably better than others in certain elds so instead of assigning uniform weights to all models we can go by the following approach for each set of documents we train on we generate document vector using bidirectional gru bahdanau et al as described by zichao yang yang et al each document we then generate complete pus vector as cdocs i i i ap i v x where v is total training set size p is number of features in document vector we save cdocs and weights corresponding to each corpus for each single document summarization task we generate given texts document tor perform nearest vector search over all stored cdocs apply weights corresponding to that corpus experiments on we our evaluate approaches understanding conferences nist the dataset has tasks in total we work on task it task contains news documents cluster for multi document summarization only character summaries are provided for each for evaluation we use rogue an cluster automatic summary evaluation metric it was rstly used for duc data set now it has become a benchmark for evaluation of automated summaries rouge is a correlation metric for xed length summaries populated using n gram co occurrence for comparison between model summary and to be evaluated summary separate scores for and gram matching are kept we use a bi gram based matching technique for our task table average scores for different combination of models table average scores for base ods models a b c d e f rou score a jaccard cosine similarity matrix b textrank c tfidf d wordnet based model e glove vec based model f infersent based model model jaccard cosine textrank tfidf wordnet based model glove vec based model infersent based model rou ge in the table we try different model pairs with weights trained on corpus for task we have displayed mean scores for base els we have calculated nal scores taking into consideration all normalizations stemming matizing and clustering techniques and the ones providing best results were used we generally expected wordnet glove based semantic models to perform better given they better capture crux of the sentence and compute similarity using the same but instead they performed average this is attributed to the fact they assigned high ilarity scores to not so semantically related tences we also observe that combinations with tf idf and similarity cosine offer nearly same results the infersent based summarizer performed exceptionally well we initially used pre trained features to generate tence vectors through infersent conclusion future work we can see that using a mixture of semantic and statistical models offers an improvement over stand alone models given better training data results can be further improved using specic labeled data can provide a further increase in performances of glove and wordnet models some easy additions that can be worked on are unnecessary parts of the sentence can be trimmed to improve summary further using better algorithm to capture sentence vector through glove model can improve sults query specic summarizer can be mented with little additions for generating summary through model laps we can also try graph based methods or different clustering techniques chin yew lin rouge a package for automatic evaluation of summaries text marization branches out references bahdanau et al dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation by jointly learning to align and translate arxiv preprint barrera and araly barrera and rakesh verma combining syntax and semantics for automatic extractive single document in international conference on rization gent text processing and computational linguistics pages springer bowman et al samuel r bowman gabor geli christopher potts and christopher d ning a large annotated corpus for arxiv preprint ing natural language inference conneau et al alexis conneau douwe kiela holger schwenk loic barrault and antoine supervised learning of universal tence representations from natural language ence data arxiv preprint dao and thanh ngoc dao and troy simpson measuring similarity between tences the code project gupta et al pankaj gupta vijay shankar luri and ishant vats summarizing text by ranking text units according to shallow linguistic features in advanced communication technology icact international conference on pages ieee kiss and tibor kiss and jan strunk unsupervised multilingual sentence boundary detection computational linguistics harold w kuhn the hungarian method for the assignment problem naval search logistics nrl michael lesk automatic sense disambiguation using machine readable ies how to tell a pine cone from an ice cream cone in proceedings of the annual international conference on systems documentation pages acm lin and chin yew lin and eduard hovy the automated acquisition of topic signatures for text summarization in proceedings of the conference on computational linguistics volume pages association for computational guistics mihalcea and rada mihalcea and paul rau textrank bringing order into text in proceedings of the conference on empirical methods in natural language processing miller et al george a miller richard beckwith christiane fellbaum derek gross and katherine j miller introduction to wordnet an on line lexical database international journal of phy vanessa g murdock pects of sentence retrieval technical report massachusetts univ amherst dept of computer science nenkova et al ani nenkova lucy wende and kathleen mckeown a compositional context sensitive multi document summarizer exploring the factors that inuence summarization in proceedings of the annual international acm sigir conference on research and development in information retrieval pages acm page et al lawrence page sergey brin rajeev motwani and terry winograd the pagerank citation ranking bringing order to the web cal report stanford infolab pedersen et al ted pedersen siddharth han and jason michelizzi wordnet ilarity measuring the relatedness of concepts in demonstration papers at hlt naacl pages association for computational linguistics pennington et al jeffrey pennington richard socher and christopher manning glove in global vectors for word representation ceedings of the conference on empirical ods in natural language processing emnlp pages david martin powers tion from precision recall and measure to roc formedness markedness and correlation radev et al dragomir r radev hongyan jing magorzata stys and daniel tam based summarization of multiple documents mation processing management rocktaschel et al tim rocktaschel edward grefenstette karl moritz hermann tomas reasoning cisky and phil blunsom corr about entailment with neural attention beatrice santorini part speech tagging guidelines for the penn treebank project revision technical reports cis page wilbur and w john wilbur and karl sirotkin the automatic identication of stop words journal of information science wong et al kam fai wong mingli wu and wenjie li extractive summarization in ing supervised and semi supervised learning proceedings of the international conference on computational linguistics volume pages association for computational linguistics yang et al zichao yang diyi yang chris dyer xiaodong he alex smola and eduard hovy hierarchical attention networks for document sication in proceedings of the conference of the north american chapter of the association for computational linguistics human language nologies pages zeng et al wenyuan zeng wenjie luo sanja fidler and raquel urtasun efcient marization with read again and copy mechanism corr
