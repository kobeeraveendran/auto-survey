r l c s c v v x r extractive summarization limits compression generalized model rakesh verma daniel lee computer science department university houston houston tx usa uh edu uh edu abstract promise alleviate information overload text tion attracted attention researchers remained challenge rst prove empirical limits recall scores extractive summarizers duc datasets rouge evaluation single document multi document summarization tasks dene concept compressibility document present new model summarization generalizes existing models literature integrates dimensions summarization viz abstractive versus extractive gle versus multi document syntactic versus semantic finally examine new existing single document summarization algorithms single framework compare state art summarizers duc data introduction automatic text summarization holy grail people battling information overload acute time attracted researchers diverse elds remained challenge especially case single news articles single document summarization competition document understanding conferences duc abandoned years automatic summarizers outperform baseline summary consisting rst words news article outperform baseline statistically signicant way summarization extractive abstractive extractive summarization sentences chosen given input abstractive summarization sentences generated new representation output extractive summarization popular explore herent limits performance systems generalize existing research supported nsf grants cns dge surprisingly despite attention extractive summarization received knowledge explored question models summarization dene compressibility document plore concept documents genres unify new existing heuristics summarization single framework contributions limitations single multi document extractive rization comparison respect gold standard human constructed abstractive summaries duc data section specically documents duc datasets compared rouge tive summaries average unigram recall rouge evaluations extractive summarizer better returning document practice worse cause size constraint summaries multi document summarization limits ways concatenate documents set examine ument performs summary respect manual abstractive summaries study document measures manual summaries average performance uments set inspired view documents summaries introduce plore generalized model summarization section unies different dimensions abstractive versus extractive single document syntactic versus semantic prove appendix constructing certain extractive summaries isomorphic min cover problem sets shows optimal summary problem np complete greedy heuristic gives multiplicative logarithmic approximation based model dene compressibility document study notion different genres articles including news ticles scientic articles short stories present new existing heuristics single document tion represent different time compressibility trade offs compare existing summarizers proven duc datasets metrics proposed section use rouge popularity ease use correlation human evaluations related work summarization literature focuses single document document summarization algorithms frameworks limits performance summarization systems pointed competitive marization systems typically extractive selecting representative sentences concatenating compressing squeeze sentences constraint summarization literature vast refer reader recent survey fairly comprehensive summarization search sampling literature focus recent research evaluation work single document extractive summarization single document marization explicitly model extraction compression results showed wide variation subset documents duc dataset focused topic coherence graphical structure separate importance coherence topic coverage functions thors present results single document summarization subset plos medicine articles duc dataset mentioning number articles algorithm combining syntactic semantic features sented graph based summarization methods systems compared newly devised supervised method dataset yahoo multi document extractive summarization multi document rization extraction redundancy compression sentences eled integer linear programming approximation algorithms supervised semi supervised learning based extractive summarization studied course single document summarization considered special case experimental results presented important special case papers cited paragraph abstractive summarization abstractive summarization systems include frameworks frameworks single document summarization sented multi document summarization frameworks metrics evaluation course rouge metric uating summaries human evaluators nist scoring summaries seven different metrics linguistic quality mid approach example choice rouge based popularity ease use correlation human assessment choice rouge congurations includes found best according paper limits extractive summarization instances rouge evaluations include best schemes shown usually bigram trigram stemming stopword elimination include results stopword nation modication original parameters limited size generated summary removed option single document summarization study limits extractive summarization pretend document summary needs evaluated human abstractive summaries created nist experts course precision mary low focus recall f score letting ment recall size human summary words table shows duc dataset document considered summaries evaluated set word human stractive summaries average unigram score approximately tables figures use following abbreviations r n means rouge metric n gram matching lowercase s denotes use stopword removal option metric range metric range table rouge recall duc document summary table rouge recall duc document summary means average words human stractive summaries appear documents extractive automatic summarizers extract sentences documents given summarization clearly extractive summarizer recall score year single document summarization competition held nist higher documents dataset general recall score lower summaries limited words documents arbitrarily long establish limit rouge recall scores extractive summarizers duc datasets duc dataset unique documents include word human abstractive summaries note extractive summaries exactly words precision higher recall score tion score upper bounded highest possible recall score single document summarization extractive summarizer average score better considered light best current extractive single document summarizers achieve limit duc datasets e rouge insights table comparing increase lower range recall values stopword removal occurred document app c deeper analysis rouge found remove numbers stopword removal option document table numbers addition rouge treats numbers comma character decimals different numbers e boosted recall stopword removal summaries signicantly decreased unigram count overlapping unigrams document summary drop discovery documents long descriptive explanations end lower recall values stopword removal tabel shows steep drop lower range values looking lower scoring documents documents usually explanations events summary skipped explanations multi document extractive summarization multi document summarization different scenarios explore limits extractive summarization rst ments belonging topic concatenated document treated summary second compare document summary respect model summaries average results documents belonging topic multi document summarization experiments data duc datasets data grouped document clusters cluster held documents single topic petition duc focused english document clusters total document clusters document cluster average documents duc documents clusters minimum documents set note scores low best scores reported super document approach consider overlap ments cluster human summaries clusters limit recall create super documents super document tion documents given document set super documents evaluated rouge model human summaries tive summary limited words recall perfect extractive system reach limit results seen table table metric range table rouge recall duc super document summary metric range table rouge recall duc super document summary averaging results individual documents different spective upper limit extractive systems treat document summary compare human summaries documents articles related specic topic documents viewed dalone perspective experiment obtained rouge recall document averaged cluster distribution ages presented figure figure best distribution average duc duc respectively best system approximated duc duc general model summarization introduce model study implications consider process human summarization starting point document contains fig distribution avg duc fig distribution avg duc sequence sentences turn sequences words human given document summarize human choose tences extract document like extractive summarizers human rst tries understand document e builds abstract mental resentation writes summary document based formulate model semantic summarization abstract world thought specialized syntactic summarization words place thought units hypothesize document collection thought units important mapping sentences thought units natural mapping implication inclusion partial implication necessarily implication mapping associate degree represent sentence includes thought unit partially summary structed sentences necessarily document cover important thought units possible e maximize importance score thought units selected given size constraint c dene mally single multi document summarization model naturally represent abstractive versus extractive dimension summarization let s denote innite set sentences t innite set thought units s t r mapping associates non negative real number sentence s thought unit t measures degree thought unit implied sentence s given document d nite sequence sentences s let s nite set sentences d t d t nite set thought units d thoughts assembled sentences document sequencing train thought prefer thought units sentence dened complete thought imposes certain importance thought units denoted scoring function wd t r size document denoted example total number words sentences document size constraint c summary function e percentage xed number words sentences case constant function summary d denoted s nite sequence sentences attempts represent thought units d best possible constraint c size summary measured procedure measuring notations thought unit t t d dene score assigned expressing thought unit t t t s formally summarization problem select maximize u tt d t subject constraint c note model represent aspects summary coherence imposing constraint sequencing thought units mary consistent ordering thought units document multi document case given corpus dn di sequencing sentences thought units conict documents resolve conicts constructing single summary corpus multi document marization hypothesize wcorpus total ordering maximally consistent wdi mean thought units signed relative importance document collection includes relative order imposed w wise w chooses relative order best represented collection based majority documents ways previous denition extends multi document summarization replace wd wcorpus t d t corpus multi document case summary coherence ned constraint sequencing thought units summary maximally consistent sequencing thought units documents conicting cases makes choices implied wcorpus function w crucial ingredient allows capture ing chosen w thought units sentence bag words models popular previous work note w need respect sequencing sense required decreasing non increasing function sequence position exibility needed w t document structure dened model covers abstractive summarization directly based sentences restricted d extractive marization need impose additional constraint single document multi document summarization important special cases model follows restricting t boolean valued function gives rise bership model avoids partial membership restricting constant function rise bag thought units model treat thought units thought units limited words words minus stopwords key phrases document extractive constraint previous models means optimization problem model np hard np complete constant function t boolean valued theorem optimization problem model np hard np complete t boolean valued constant function thought units words words minus stopwords key phrases document sentence size summary size constraint measured syntactic units np complete cases extractive coverage summarization collectively proof reduction set cover problem proof appendix based generalized model dene denition extractive compressibility document smallest size collection sentences document cover thought units thought units words word extractive compressibility denition abstractive compressibility document smallest size collection arbitrary sentences cover thought units thought units words word abstractive compressibility denition compression rate incompressibility document ned n size compressibility document n original size document similarly dene corresponding compressibility notions key phrases words minus stopwords thought units investigate compressibility different genres news articles entic papers short studies purpose news articles scientic papers short stories collected news articles randomly selected sources covered disasters disaster recovery tion critical infrastructures scientic papers following ve topics cancer research nanotechnology physics nlp security chosen random short stories cather crane chekhov kate chopin ohenry randomly selected experiments showed large sentence counts lead decrease imcompressibility figure shows direct lationship document size incompressibility fig imcompressibility vs sentence count algorithms single document summarization implemented new existing heuristcs tool called summ written python heuristics revolve tf idf ranking shown tasks involving summarization tf idf ranks importance words corpus ranking tem compared popular keyword identication algorithms found competitive results paper authors compared textrank singlerank expandrank keycluster latent semantics analysis latent dirichlet analysis tf idf n keywords n varied steps duc documents extracted gorithm score calculated human summaries models experiments showed tf idf consistently performs better algorithms apply domain single document tion dene corpus document documents referred inverse document frequency individual sentences terms remain words value sentence sum tf idf scores words sentence docsumm includes greedy dynamic programming based rithms greedy algorithms use chosen scoring metric evaluate sentence document simply selects highest scoring sentence til given threshold words met word covered document choices scoring metrics options normalization weights stemming toggled uation appendix b gives brief description options docsumm includes dynamic programming algorithms provides optimal solution e minimum number sentences necessary cover words document viewed bound maximum pression document extractive summary algorithm approach builds set covers subsets original document s thought units e words experiments beginning smallest unit gle word implement version based recursion algorithm quickly runs time space repeated computations addition optimal algorithm docsumm implements version algorithm presented mcdonald frames problem document summarization maximization scoring function based vance redundancy essence selected sentences scored higher vance scored lower redundancy sentences document sidered inclusion exclusion basis problem document rization reduces knapsack problem mcdonald s algorithm approximate inclusion exclusion algorithm inuences score sentences couple greedy algorithms dynamic gramming algorithm docsumm appeared rest new results results include experiments running time comparisons docsumm s algorithms addition compare performance measures docsumm duc duc datasets run times dataset running times created sampling sentences book genesis created documents increasing lengths length measured verses verse count ranged uments greater sentences dynamic algorithm runs memory results exhaustive algorithm table shows slight increases time document size increase tdf signicant increase running time verse count greedy size greedy greedy tdf table running times algorithms milliseconds summarization compare heuristics single document rization duc duc dataset unique documents duc dataset compared summaries docsumm algorithms results line analysis domains algorithm truncated solution set soon threshold words covered rouge scores algorithms line compressibility performances size algorithms performed similarly best algorithm rouge scores rouge lcs respectively tdf algorithm performance signicantly different comparison unique articles duc dataset compare greedy dynamic solutions following classes tems line single document summarizers synsem best extractive summarizer kkv ii ve systems duc competition iii trank iv mead mcdonald algorithm vi duc baseline summaries consisting rst words news articles baseline algorithm rouge lcs size tdf mcdonald mead textrank synsem kkv baseline n n n table scores word summaries duc documents duc competition systems managed higher score baseline comparison manual abstracts system summaries truncated exactly words exceed limit note results synsem unique articles duc dataset unfortunately authors port rouge bigram rouge lcs rouge l scores kkv s results remove duplicate articles duc dataset agged entries table results comparable addition kkv report rouge lcs scores observe rouge unigram scores dynamic optimal algorithm performs best rithms docsumm falls baseline sider rouge bigram scores dynamic greedy outperform rest eld surprisingly margin performance pronounced rouge lcs scores conclusions future work shown limits recall automatic extractive summarization duc datasets rouge evaluations limits current art systems evaluated duc data achieving limit recall single document summarization best tems multi document summarization achieving limit encouraging news time work ing summarization explored compressibility ized model new existing heuristics single document summarization knowledge compressibility way dened studied new concept plan investigate future work believe compressibility prove useful measure study performance automatic summarization systems authorship detection instance authors shown consistently compressible thank reviewers cicling constructive comments acknowledgments references almeida m b martins f fast robust compressive summarization dual position multi task learning acl pp barrera verma r combining syntax semantics automatic extractive document summarization cicling vol lncs pp berg kirkpatrick t gillick d klein d jointly learning extract compress proceedings annual meeting association computational linguistics human language technologies volume pp association computational guistics boudin f mougard h favre b concept based summarization integer linear gramming concept pruning multiple optimal solutions proceedings conference empirical methods natural language processing pp association computational linguistics lisbon portugal september org anthology carenini g cheung j c k extractive vs nlg based abstractive summarization ative text effect controversiality proceedings fifth international natural language generation conference pp association computational guistics cheung j c k penn g unsupervised sentence enhancement automatic tion emnlp pp chopra s auli m rush m harvard s abstractive sentence summarization attentive recurrent neural networks proceedings naacl pp dang h t owczarzak k overview tac update summarization task ceedings text analysis conference pp erkan g radev d r lexrank graph based lexical centrality salience text rization journal articial intelligence research pp filatova e hatzivassiloglou v formal model information selection sentence text extraction proceedings international conference tational linguistics p association computational linguistics gambhir m gupta v recent automatic text summarization techniques survey artif intell rev ganesan k zhai c han j opinosis graph based approach abstractive tion highly redundant opinions proceedings international conference computational linguistics pp association computational linguistics gillick d favre b scalable global model summarization proceedings workshop integer linear programming natural langauge processing pp sociation computational linguistics graham y evaluating automatic summarization bleu shades rouge proceedings conference empirical methods natural language ing emnlp lisbon portugal september pp hirao t yoshida y nishino m yasuda n nagata m single document summarization tree knapsack problem proceedings conference empirical methods natural language processing pp association computational linguistics seattle washington usa october aclweb org hochbaum d s approximation algorithms np hard problems pws publishing co kumar n srinathan k varma v knowledge induced graph theoretical model tract abstract single document summarization computational linguistics ligent text processing pp springer li c liu y liu f zhao l weng f improving multi documents summarization sentence compression based expanded constituent parse trees emnlp pp citeseer lin c hovy e automatic evaluation summaries n gram co occurrence tics htl naacl liu f liu y abstractive speech summarization exploring unsupervised pervised approaches spoken utterance compression audio speech language cessing ieee transactions mani maybury m advances automatic summarization mit press cambridge massachusetts martins f smith n summarization joint model sentence extraction compression proceedings workshop integer linear programming natural langauge processing pp association computational linguistics mcdonald r study global inference algorithms multi document summarization proc ecir springer mehdad y stent thadani k radev d billawala y buchner k extractive rization strict length constraints proceedings tenth international conference language resources evaluation lrec meseure m ranking systems evaluation keywords keyphrases detection tech rep department computer science university houston houston tx usa november cs uh edu mihalcea r tarau p textrank bringing order text proceedings ference empirical methods natural language processing emnlp ing sigdat special interest group acl held conjunction acl july barcelona spain pp aclweb anthology nenkova automatic text summarization newswire lessons learned ument understanding conference aaai pp parveen d ramsl h strube m topical coherence graph based extractive rization proceedings conference empirical methods natural language processing emnlp lisbon portugal september pp passonneau r j chen e guo w perin d automated pyramid scoring summaries distributional semantics proceedings annual meeting tion computational linguistics acl august soa bulgaria volume short papers pp rush m chopra s weston j neural attention model abstractive sentence marization proceedings conference empirical methods natural guage processing emnlp lisbon portugal september pp takamura h okumura m text summarization model based maximum coverage lem variant proceedings conference european chapter association computational linguistics pp association computational linguistics tratz s hovy e h summarization evaluation transformed basic elements ceedings text analysis conference tac gaithersburg maryland usa november vanderwende l banko m menezes event centric summary generation working notes duc pp wong k wu m li w extractive summarization supervised semi supervised learning coling international conference computational linguistics proceedings conference august manchester uk pp yogatama d liu f smith n extractive summarization maximizing semantic ume proceedings conference empirical methods natural language processing emnlp lisbon portugal september pp yoshida y suzuki j hirao t nagata m dependency based discourse parser document summarization proceedings conference empirical methods natural language processing emnlp pp association computational linguistics doha qatar october aclweb org appendix proof theorem reduction set cover problem np hardness given universe u family s subsets u cover subfamily c s union u set cover problem input pair u s number k question cover size k reduce set cover summarization follows member u u select thought unit t t clause c expresses t set s family construct sentence s consists clauses corresponding members s boolean valued assemble sentences document capacity constraint c k represents number sentences select summary easy cover corresponds summary maximizes utility satises capacity constraint vice versa course document constructed somewhat repetitive real single documents redundancy connectivity clauses appearing sentence ensured choosing facts person s life example np complete cases theorem extractive coverage summarization collectively case easy design greedy strategy gives logarithmic approximation ratio optimal dynamic programming exponential worst case b appendix docsumm tool option size tdf description scoring based lenght sentence tf based document idf based document removes stopwords applies stemming words removes duplicate words sentence normalizes scores sentence word count updates scores greedy selection enables summary mode sets number words summary table options docsumm tool c appendix document duc fig original document fig model summary fig model summary
