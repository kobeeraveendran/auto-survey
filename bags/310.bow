l u j l c s c v v i x r a dataset for automatic summarization of russian news ilya moscow institute of physics and technology moscow russia ilya edu abstract automatic text summarization has been studied in a variety of domains and languages however this does not hold for the russian language to overcome this issue we present gazeta the rst dataset for summarization of russian news we describe the properties of this dataset and benchmark several extractive and abstractive models we demonstrate that the dataset is a valid task for methods of text marization for russian additionally we prove the pretrained model to be useful for russian text summarization keywords text summarization russian language dataset introduction text summarization is the task of creating a shorter version of a document that captures essential information methods of automatic text summarization can be extractive or abstractive extractive methods copy chunks of original documents to form a summary in this case the task usually reduces to tagging words or sentences the ing summary will be grammatically coherent especially in the case of sentence copying however this is not enough for high quality summarization as a good summary should paraphrase and generalize an original text recent advances in the eld are usually utilizing abstractive models to get better summaries these models can generate new words that do not exist in original texts it allows them to compress text in a better way via sentence fusion and paraphrasing before the dominance of sequence to sequence models the most common approach was extractive the approach s design allows us to use classic machine learning methods various neural network architectures such as rnns or transformers and pretrained models such as bert the approach can still be useful on some datasets but modern abstractive methods outperform extractive ones on cnn dailymail dataset since pointer generators various pretraining tasks such as mlm masked language model and nsp next sentence prediction used in bert or denoising autoencoding used in bart allow models to incorporate rich language knowledge to understand original documents and generate grammatically correct and reasonable summaries ilya gusev in recent years many novel text summarization datasets have been revealed xsum focuses on very abstractive summaries newsroom has more than a million pairs multi news reintroduces multi document summarization however datasets for any language other than english are still scarce for sian there are only headline generation datasets such as ria corpus the main aim of this paper is to x this situation by presenting a russian rization dataset and evaluating some of the existing methods on it moreover we adapted the model initially used for machine translation to the summarization task the bart model was successfully used for text summarization on english datasets so it is natural for mbart to handle the same task for all trained languages we believe that text summarization is a vital task for many news agencies and news aggregators it is hard for humans to compose a good summary so automation in this area will be useful for news editors and readers more text summarization is one of the benchmarks for general natural language understanding models our contributions are as follows we introduce the rst russian tion dataset in the news we benchmark extractive and abstractive methods on this dataset to inspire further work in the area finally we adopt the mbart model to summarize russian texts and it achieves the best results of all benchmarked data source there are several requirements for a data source first we wanted news maries as most of the datasets in english are in this domain second these summaries should be human generated third no legal issues should exist with data and its publishing the last requirement was hard to fulll as many news agencies have explicit restrictions for publishing their data and tend not to reply to any letters gazeta ru was one of the agencies with explicit permission on their website to use their data for non commercial purposes moreover they have summaries for many of their articles there are also requirements for content of summaries we do not want maries to be fully extractive as it would be a much easier task and consequently it would not be a good benchmark for abstractive models we collected texts dates urls titles and summaries of all articles from the website s foundation to march we parsed summaries as the content of a meta tag with description property a small percentage of all articles had a summary com ilyagusev gazeta com ilyagusev summarus dataset for automatic summarization of russian news cleaning after the scraping we did cleaning we removed summaries with more than words and less than words texts with more than words pairs with less than unigram intersection and more than unigram intersection the examples outside these borders contained either fully extractive summaries or not summaries at all moreover we removed all data earlier than the of june because the meta tag texts were not news summaries the complete code of a cleaning phase is available online with a raw version of the dataset statistics the resulting dataset consists of text summary pairs to form training validation and test datasets these pairs were sorted by time we dene the rst pairs as the training dataset the proceeding pairs as the validation dataset and the remaining pairs as the test dataset it is still essential to randomly shue the training dataset before training any models to reduce time bias even more statistics of the dataset can be seen in table summaries of the training part of the dataset are shorter on average than summaries of validation and test parts we also provide statistics on lemmatized texts and summaries we compute normal forms of words using the package numbers in the common ul row show size of an intersection between lemmas vocabularies of texts and summaries these numbers are almost similar to numbers in the unique lemmas row of summaries columns it means that almost all lemmas of the summaries are presented in original texts table dataset statistics after lowercasing train validation test text summary text summary text summary unique words uw unique lemmas ul dates pairs common ul min words max words avg words avg sentences avg uw avg ul we depict the distribution of tokens counts in texts in figure and the distribution of tokens counts in summaries is in figure the training dataset com kmike ilya gusev has a smoother distribution of text lengths in comparison with validation and test datasets it also has an almost symmetrical distribution of summaries lengths while validation and test distributions are skewed fig documents distribution by count of tokens in a text fig documents distribution by count of tokens in a summary to evaluate the dataset s bias towards extractive or abstractive methods we measured the percentage of novel n grams in summaries results are presented in table and show that more than of summaries bi grams do not exist in original texts this number decreases to if we consider dierent word forms and calculate it on lemmatized bi grams although we can not directly compare these numbers with cnn dailymail or any other english dataset as this statistic is heavily language dependent we should state that it is for cnn dailymail and for xsum from this we can conclude that the bias towards extractive methods can exist another way to evaluate the abstractiveness is by calculating metrics of oracle summaries the term is dened in to evaluate all benchmark models we used rouge metrics for cnn dailymail oracle summaries score f and for our dataset it is f dataset for automatic summarization of russian news table average of novel n grams uni grams lemmatized uni grams bi grams lemmatized bi grams tri grams train val test bpe we extensively utilized byte pair encoding bpe tokenization in most of the described models for russian the models that use bpe tokenization performs better than those that use word tokenization as it enables the use of rich phology and decreases the number of unknown tokens the encoding was trained on the training dataset using the sentencepiece library lowercasing we lower cased all texts and summaries in most of our experiments it is a troversial decision on the one hand we reduced vocabulary size and focused on the essential properties of models but on the other hand we lost important information for a model to receive moreover if we speak about our rization system s possible end users it is better to generate summaries in the original case we provide a non lower cased version of the dataset as the main version for possible future research benchmark methods we used several groups of methods textrank and lexrank are fully unsupervised extractive summarization methods summarunner is a vised extractive method pg copynet are abstractive summarization methods unsupervised methods this group of methods does not have any access to reference summaries and utilizes only original texts all of the considered methods in this group extract whole sentences from a text not separated words textrank textrank is a classic graph based method for unsupervised text summarization it splits a text into sentences calculates a similarity matrix for every distinct pair of them and applies the pagerank algorithm to obtain nal ilya gusev scores for every sentence after that it takes the best sentences by the score as a predicted summary we used textrank implementation from the summa library it denes sentence similarity as a function of a count of common words between sentences and lengths of both sentences lexrank continuous lexrank can be seen as a modication of the trank that utilizes tf idf of words to compute sentence similarity as idf modied cosine similarity a continuous version uses an original similarity trix and a base version performs binary discretization of this matrix by the threshold we used lexrank implementation from lexrank python lsa latent semantic analysis can be used for text summarization it structs a matrix of terms by sentences with term frequencies applies singular value decomposition to it and searches right singular vectors maximum values the search represents nding the best sentence describing the kth topic we evaluated this method with sumy supervised extractive methods methods in this group have access to reference summaries and the task for them is seen as sentences binary classication for every sentence in an original text the algorithm must decide whether to include it in the predicted summary to perform the reduction to this task we rst need to nd subsets of original sentences that are most similar to reference summaries to nd these so called oracle summaries we used a greedy algorithm similar to summarunnner per and bertsumext paper the algorithm generates a summary consisting of multiple sentences which maximize the score against a reference summary summarunner summarunner is one of the simplest and yet eective neural approaches to extractive summarization it uses layer hierarchical rnn and positional embeddings to choose a binary label for every sentence we used our implementation on top of the allennlp framework along with generator implementation abstractive methods all of the tested models in this group are based on a sequence to sequence work pointer generator and copynet were trained only on our training dataset and mbart was pretrained on texts of languages extracted from the mon crawl we performed no additional pretraining though it is possible to utilize russian headline generation datasets here com summanlp textrank com crabcamp lexrank com miso belica sumy com allenai allennlp dataset for automatic summarization of russian news pointer generator pointer generator is a modication of a sequence sequence rnn model with attention the generation phase samples words not only from the vocabulary but from the source text based on attention bution furthermore the second modication the coverage mechanism prevents the model from attending to the same places many times to handle repetition in summaries copynet copynet is another variation of sequence to sequence rnn model with attention with slightly dierent copying mechanism we used the stock plementation from allennlp for summarization bart and are sequence sequence transformer models with autoregressive decoder trained on the noising autoencoding task unlike the preceding pretrained models like bert they focus on text generation even in the pretraining phase mbart was pretrained on the monolingual corpora for languages ing russian in the original paper it was successfully used for machine tion bart was used for text summarization so it is natural to try a pretrained model for russian summarization we used training and prediction scripts from fairseq however it is possible to convert the model for using it within huggingface s we had to truncate input for every text to tokens to t the model in gpu memory we also used unk token instead of language codes to condition results automatic evaluation we measured the quality of summarization with three sets of automatic metrics rouge bleu meteor all of them are used in various text generation tasks and are based on the overlaps of n grams rouge and teor are prevalent in text summarization research and bleu is a primary automatic metric in machine translation blue is a precision based metric and does not take recall into account while rouge uses both recall and based metrics in a balanced way and meteor weight for the recall part is higher than weight for the precision part all three sets of metrics are not perfect as we only have only one version of a reference summary for each text while it is possible to generate many correct summaries for a given text some of these summaries can even have zero n gram overlap with reference ones com pytorch fairseq com huggingface transformers ilya gusev we lower cased and tokenized reference and predicted summaries with razdel tokenizer to unify the methodology across all models we suggest to all further researchers to use the same evaluation script table automatic scores for all models on the test set greedy oracle textrank lexrank lsa summarunner copynet pg small pg words pg big pg small coverage finetuned mbart rouge l bleu meteor we provide all the results in table and are the most basic baselines where we choose the rst the rst two or the rst three sentences of every text as our summary is a strong baseline as it was in cnn dailymail dataset the oracle summarization is an upper bound for extractive methods unsupervised methods give summaries that are very dissimilar to the original ones lexrank is the best of unsupervised methods in our experiments the summrunner model has the best meteor score and high bleu and rouge scores in figure summarunner has a bias towards the sentences at the beginning of the text compared to the oracle summaries in contrast lexrank sentence positions are almost uniformly distributed except for the rst sentence it seems that more complex extractive models should perform better on this dataset but unfortunately we did not have time to prove it to evaluate an abstractiveness of the model we used extraction and giarism scores the plagiarism score is a normalized length of the longest common sequence between a text and a summary the extraction score is a more sophisticated metric it computes normalized lengths of all long non overlapping common sequences between a text and a summary and ensures that the sum of these normalized lengths is between and as for abstractive models has the best result among all the models in terms of rouge and bleu however figure shows that it has fewer dataset for automatic summarization of russian news fig proportion of extracted sentences according to their position in the original document novel n grams than pointer generator with coverage consequently it has worser extraction and plagiarism scores table table extraction scores on the test set extraction score plagiarism score reference pg small coverage finetuned summarunner human evaluation we also did side by side annotation of mbart and human summaries with yandex a russian crowdsourcing platform we sampled text and summary pairs from the test dataset and generated a new summary for every text we showed a title a text and two possible summaries for every example nine people annotated every example we asked them which summary is better and provided them three options left summary wins draw right summary wins the side of the human summary was random annotators were required to pass training exam and their work was continuously evaluated through the control pairs honeypots yandex ilya gusev fig proportion of novel n grams in model generated summaries on the test set majority table human side by side evaluation votes for winner reference wins wins table shows the sults of the annotation there were no full draws so we exclude them from the table wins in more than cases we can not just conclude that it performs on a superhuman level from these results we did not ask our annotators to evaluate the ness of the summaries in any way reference summaries are usually too provocative and subjective while generates highly extractive summaries without any errors and with many essential details and annotators tend to like it the annotation task should be changed to evaluate the abstractiveness of the model even so that is an cellent result for table shows examples of mbart losses against reference summaries in the rst example there is an unnamed entity in the rst sentence by them in the second example the factual error and repetition exist in the last example the last sentence is not cohesive dataset for automatic summarization of russian news table mbart summaries that lost lacma art lm gala gucci conclusion we present the rst corpus for text summarization in the russian language we demonstrate that most of the text summarization methods work well for russian without any special modications moreover performs exceptionally well even if it was not initially designed for text summarization in the russian language we wanted to extend the dataset using data from other sources but there were signicant legal issues in most cases as most of the sources explicitly forbid any publishing of their data even in non commercial purposes in future work we will pre train bart ourselves on standard russian text collections and open news datasets furthermore we will try the headline eration as a pretraining task for this dataset we believe it will increase the performance of the models references sutskever i vinyals i le q sequence to sequence learning with neural works in proceedings of the international conference on neural information processing systems vol pp cambridge mit press wong k wu m li w extractive summarization using supervised and supervised learning in proceedings of the international conference on putational linguistics pp coling organizing committee ilya gusev hochreiter s schmidhuber j long short term memory in neural computation vol issue pp nallapati r zhai f zhou b summarunner a recurrent neural network based sequence model for extractive summarization of documents in proceedings of the thirty first aaai conference on articial intelligence pp vaswani a shazeer n parmar n uszkoreit j jones l gomez a kaiser polosukhin i attention is all you need in advances in neural information processing systems pp devlin j chang m lee k toutanova k bert pre training of deep tional transformers for language understanding in proceedings of the ence of the north american chapter of the association for computational tics human language technologies vol pp minneapolis minnesota see a liu p manning c get to the point summarization with generator networks in proceedings of the annual meeting of the association for computational linguistics vol pp association for computational linguistics vancouver liu y lapata m text summarization with pretrained encoders in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing ijcnlp pp association for computational linguistics hong kong lewis m liu y goyal n ghazvininejad m mohamed a levy o anov v zettlemoyer l bart denoising sequence to sequence pre training for natural language generation translation and comprehension in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural language processing ijcnlp pp association for computational linguistics hong kong liu y gu j goyal n li x edunov s ghazvininejad m lewis m zettlemoyer l multilingual denoising pre training for neural machine tion arxiv preprint narayan s cohen s lapata m do nt give me the details just the mary aware convolutional neural networks for extreme summarization in proceedings of the conference on empirical methods in natural language processing brussels grusky m naaman m artzi y newsroom a dataset of million maries with diverse extractive strategies in proceedings of the conference of the american chapter of the association for computational linguistics human language technologies association for computational linguistics new orleans fabbri a li i she t li s radev d multi news a large scale document summarization dataset and abstractive hierarchical mode in ings of the annual meeting of the association for computational linguistics pp association for computational linguistics florence gavrilov d kalaidin p malykh v self attentive model for headline generation in azzopardi l stein b fuhr n mayr p hau c hiemstra d eds advances in information retrieval ecir lecture notes in computer science vol springer cham dataset for automatic summarization of russian news mihalcea r tarau p textrank bringing order into text in proceedings of the conference on empirical methods in natural language processing pp association for computational linguistics barcelona erkan g radev d lexrank graph based lexical centrality as salience in text summarization in journal of articial intelligence research vol issue ai access foundation barrios f lopez f argerich l wachenchauzer r variations of the similarity function of textrank for automated summarization arxiv preprint bahdanau d cho k bengio y neural machine translation by jointly learning to align and translate in international conference on learning representations gardner m grus j neumann m tafjord o dasigi p liu n peters m schmitz m zettlemoyer l allennlp a deep semantic natural language cessing platform arxiv preprint gu j lu z li h li v incorporating copying mechanism in sequence sequence learning in proceedings of the annual meeting of the association for computational linguistics vol pp association for computational linguistics gong y liu x generic text summarization using relevance measure and latent semantic analysis in proceedings of the annual international acm sigir conference on research and development in information retrieval pp lin c rouge a package for automatic evaluation of summaries in text marization branches out pp barcelona papineni k roukos s ward t zhu w j bleu a method for automatic evaluation of machine translation annual meeting of the association for putational linguistics pp denkowski m lavie a meteor universal language specic translation uation for any target language in proceedings of the eacl workshop on statistical machine translation kudo t richardson j sentencepiece a simple and language independent word tokenizer and detokenizer for neural text processing proceedings of the conference on empirical methods in natural language processing system strations pp cibils a musat c hossmann a baeriswyl m diverse beam search for increased novelty in abstractive summarization arxiv preprint ott m edunov s baevski a fan a gross s ng n grangier d auli m fairseq a fast extensible toolkit for sequence modeling in proceedings of naacl hlt demonstrations korobov m morphological analyzer and generator for russian and ukrainian languages in analysis of images social networks and texts pp
