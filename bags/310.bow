dataset automatic summarization russian news ilya moscow institute physics technology moscow russia ilya edu abstract automatic text summarization studied variety domains languages hold russian language overcome issue present gazeta rst dataset summarization russian news describe properties dataset benchmark extractive abstractive models demonstrate dataset valid task methods text marization russian additionally prove pretrained model useful russian text summarization keywords text summarization russian language dataset introduction text summarization task creating shorter version document captures essential information methods automatic text summarization extractive abstractive extractive methods copy chunks original documents form summary case task usually reduces tagging words sentences ing summary grammatically coherent especially case sentence copying high quality summarization good summary paraphrase generalize original text recent advances eld usually utilizing abstractive models better summaries models generate new words exist original texts allows compress text better way sentence fusion paraphrasing dominance sequence sequence models common approach extractive approach design allows use classic machine learning methods neural network architectures rnns transformers pretrained models bert approach useful datasets modern abstractive methods outperform extractive ones cnn dailymail dataset pointer generators pretraining tasks mlm masked language model nsp sentence prediction bert denoising autoencoding bart allow models incorporate rich language knowledge understand original documents generate grammatically correct reasonable summaries ilya gusev recent years novel text summarization datasets revealed xsum focuses abstractive summaries newsroom million pairs multi news reintroduces multi document summarization datasets language english scarce sian headline generation datasets ria corpus main aim paper situation presenting russian rization dataset evaluating existing methods adapted model initially machine translation summarization task bart model successfully text summarization english datasets natural mbart handle task trained languages believe text summarization vital task news agencies news aggregators hard humans compose good summary automation area useful news editors readers text summarization benchmarks general natural language understanding models contributions follows introduce rst russian tion dataset news benchmark extractive abstractive methods dataset inspire work area finally adopt mbart model summarize russian texts achieves best results benchmarked data source requirements data source wanted news maries datasets english domain second summaries human generated legal issues exist data publishing requirement hard fulll news agencies explicit restrictions publishing data tend reply letters gazeta agencies explicit permission website use data non commercial purposes summaries articles requirements content summaries want maries fully extractive easier task consequently good benchmark abstractive models collected texts dates urls titles summaries articles website foundation march parsed summaries content meta tag description property small percentage articles summary com ilyagusev gazeta com ilyagusev summarus dataset automatic summarization russian news cleaning scraping cleaning removed summaries words words texts words pairs unigram intersection unigram intersection examples outside borders contained fully extractive summaries summaries removed data earlier june meta tag texts news summaries complete code cleaning phase available online raw version dataset statistics resulting dataset consists text summary pairs form training validation test datasets pairs sorted time dene rst pairs training dataset proceeding pairs validation dataset remaining pairs test dataset essential randomly shue training dataset training models reduce time bias statistics dataset seen table summaries training dataset shorter average summaries validation test parts provide statistics lemmatized texts summaries compute normal forms words package numbers common row size intersection lemmas vocabularies texts summaries numbers similar numbers unique lemmas row summaries columns means lemmas summaries presented original texts table dataset statistics lowercasing train validation test text summary text summary text summary unique words unique lemmas dates pairs common min words max words avg words avg sentences avg avg depict distribution tokens counts texts figure distribution tokens counts summaries figure training dataset com kmike ilya gusev smoother distribution text lengths comparison validation test datasets symmetrical distribution summaries lengths validation test distributions skewed fig documents distribution count tokens text fig documents distribution count tokens summary evaluate dataset bias extractive abstractive methods measured percentage novel grams summaries results presented table summaries grams exist original texts number decreases consider dierent word forms calculate lemmatized grams directly compare numbers cnn dailymail english dataset statistic heavily language dependent state cnn dailymail xsum conclude bias extractive methods exist way evaluate abstractiveness calculating metrics oracle summaries term dened evaluate benchmark models rouge metrics cnn dailymail oracle summaries score dataset dataset automatic summarization russian news table average novel grams uni grams lemmatized uni grams grams lemmatized grams tri grams train val test bpe extensively utilized byte pair encoding bpe tokenization described models russian models use bpe tokenization performs better use word tokenization enables use rich phology decreases number unknown tokens encoding trained training dataset sentencepiece library lowercasing lower cased texts summaries experiments troversial decision hand reduced vocabulary size focused essential properties models hand lost important information model receive speak rization system possible end users better generate summaries original case provide non lower cased version dataset main version possible future research benchmark methods groups methods textrank lexrank fully unsupervised extractive summarization methods summarunner vised extractive method copynet abstractive summarization methods unsupervised methods group methods access reference summaries utilizes original texts considered methods group extract sentences text separated words textrank textrank classic graph based method unsupervised text summarization splits text sentences calculates similarity matrix distinct pair applies pagerank algorithm obtain nal ilya gusev scores sentence takes best sentences score predicted summary textrank implementation summa library denes sentence similarity function count common words sentences lengths sentences lexrank continuous lexrank seen modication trank utilizes idf words compute sentence similarity idf modied cosine similarity continuous version uses original similarity trix base version performs binary discretization matrix threshold lexrank implementation lexrank python lsa latent semantic analysis text summarization structs matrix terms sentences term frequencies applies singular value decomposition searches right singular vectors maximum values search represents nding best sentence describing kth topic evaluated method sumy supervised extractive methods methods group access reference summaries task seen sentences binary classication sentence original text algorithm decide include predicted summary perform reduction task rst need subsets original sentences similar reference summaries called oracle summaries greedy algorithm similar summarunnner bertsumext paper algorithm generates summary consisting multiple sentences maximize score reference summary summarunner summarunner simplest eective neural approaches extractive summarization uses layer hierarchical rnn positional embeddings choose binary label sentence implementation allennlp framework generator implementation abstractive methods tested models group based sequence sequence work pointer generator copynet trained training dataset mbart pretrained texts languages extracted mon crawl performed additional pretraining possible utilize russian headline generation datasets com summanlp textrank com crabcamp lexrank com miso belica sumy com allenai allennlp dataset automatic summarization russian news pointer generator pointer generator modication sequence sequence rnn model attention generation phase samples words vocabulary source text based attention bution furthermore second modication coverage mechanism prevents model attending places times handle repetition summaries copynet copynet variation sequence sequence rnn model attention slightly dierent copying mechanism stock plementation allennlp summarization bart sequence sequence transformer models autoregressive decoder trained noising autoencoding task unlike preceding pretrained models like bert focus text generation pretraining phase mbart pretrained monolingual corpora languages ing russian original paper successfully machine tion bart text summarization natural try pretrained model russian summarization training prediction scripts fairseq possible convert model huggingface truncate input text tokens model gpu memory unk token instead language codes condition results automatic evaluation measured quality summarization sets automatic metrics rouge bleu meteor text generation tasks based overlaps grams rouge teor prevalent text summarization research bleu primary automatic metric machine translation blue precision based metric recall account rouge uses recall based metrics balanced way meteor weight recall higher weight precision sets metrics perfect version reference summary text possible generate correct summaries given text summaries zero gram overlap reference ones com pytorch fairseq com huggingface transformers ilya gusev lower cased tokenized reference predicted summaries razdel tokenizer unify methodology models suggest researchers use evaluation script table automatic scores models test set greedy oracle textrank lexrank lsa summarunner copynet small words big small coverage finetuned mbart rouge bleu meteor provide results table basic baselines choose rst rst rst sentences text summary strong baseline cnn dailymail dataset oracle summarization upper bound extractive methods unsupervised methods summaries dissimilar original ones lexrank best unsupervised methods experiments summrunner model best meteor score high bleu rouge scores figure summarunner bias sentences beginning text compared oracle summaries contrast lexrank sentence positions uniformly distributed rst sentence complex extractive models perform better dataset unfortunately time prove evaluate abstractiveness model extraction giarism scores plagiarism score normalized length longest common sequence text summary extraction score sophisticated metric computes normalized lengths long non overlapping common sequences text summary ensures sum normalized lengths abstractive models best result models terms rouge bleu figure shows fewer dataset automatic summarization russian news fig proportion extracted sentences according position original document novel grams pointer generator coverage consequently worser extraction plagiarism scores table table extraction scores test set extraction score plagiarism score reference small coverage finetuned summarunner human evaluation annotation mbart human summaries yandex russian crowdsourcing platform sampled text summary pairs test dataset generated new summary text showed title text possible summaries example people annotated example asked summary better provided options left summary wins draw right summary wins human summary random annotators required pass training exam work continuously evaluated control pairs honeypots yandex ilya gusev fig proportion novel grams model generated summaries test set majority table human evaluation votes winner reference wins wins table shows sults annotation draws exclude table wins cases conclude performs superhuman level results ask annotators evaluate ness summaries way reference summaries usually provocative subjective generates highly extractive summaries errors essential details annotators tend like annotation task changed evaluate abstractiveness model cellent result table shows examples mbart losses reference summaries rst example unnamed entity rst sentence second example factual error repetition exist example sentence cohesive dataset automatic summarization russian news table mbart summaries lost lacma art gala gucci conclusion present rst corpus text summarization russian language demonstrate text summarization methods work russian special modications performs exceptionally initially designed text summarization russian language wanted extend dataset data sources signicant legal issues cases sources explicitly forbid publishing data non commercial purposes future work pre train bart standard russian text collections open news datasets furthermore try headline eration pretraining task dataset believe increase performance models references sutskever vinyals sequence sequence learning neural works proceedings international conference neural information processing systems vol cambridge mit press wong extractive summarization supervised supervised learning proceedings international conference putational linguistics coling organizing committee ilya gusev hochreiter schmidhuber long short term memory neural computation vol issue nallapati zhai zhou summarunner recurrent neural network based sequence model extractive summarization documents proceedings thirty aaai conference articial intelligence vaswani shazeer parmar uszkoreit jones gomez kaiser polosukhin attention need advances neural information processing systems devlin chang lee toutanova bert pre training deep tional transformers language understanding proceedings ence north american chapter association computational tics human language technologies vol minneapolis minnesota liu manning point summarization generator networks proceedings annual meeting association computational linguistics vol association computational linguistics vancouver liu lapata text summarization pretrained encoders proceedings conference empirical methods natural language processing international joint conference natural language processing ijcnlp association computational linguistics hong kong lewis liu goyal ghazvininejad mohamed levy anov zettlemoyer bart denoising sequence sequence pre training natural language generation translation comprehension proceedings conference empirical methods natural language processing international joint conference natural language processing ijcnlp association computational linguistics hong kong liu goyal edunov ghazvininejad lewis zettlemoyer multilingual denoising pre training neural machine tion arxiv preprint narayan cohen lapata details mary aware convolutional neural networks extreme summarization proceedings conference empirical methods natural language processing brussels grusky naaman artzi newsroom dataset million maries diverse extractive strategies proceedings conference american chapter association computational linguistics human language technologies association computational linguistics new orleans fabbri radev multi news large scale document summarization dataset abstractive hierarchical mode ings annual meeting association computational linguistics association computational linguistics florence gavrilov kalaidin malykh self attentive model headline generation azzopardi stein fuhr mayr hau hiemstra eds advances information retrieval ecir lecture notes computer science vol springer cham dataset automatic summarization russian news mihalcea tarau textrank bringing order text proceedings conference empirical methods natural language processing association computational linguistics barcelona erkan radev lexrank graph based lexical centrality salience text summarization journal articial intelligence research vol issue access foundation barrios lopez argerich wachenchauzer variations similarity function textrank automated summarization arxiv preprint bahdanau cho bengio neural machine translation jointly learning align translate international conference learning representations gardner grus neumann tafjord dasigi liu peters schmitz zettlemoyer allennlp deep semantic natural language cessing platform arxiv preprint incorporating copying mechanism sequence sequence learning proceedings annual meeting association computational linguistics vol association computational linguistics gong liu generic text summarization relevance measure latent semantic analysis proceedings annual international acm sigir conference research development information retrieval lin rouge package automatic evaluation summaries text marization branches barcelona papineni roukos ward zhu bleu method automatic evaluation machine translation annual meeting association putational linguistics denkowski lavie meteor universal language specic translation uation target language proceedings eacl workshop statistical machine translation kudo richardson sentencepiece simple language independent word tokenizer detokenizer neural text processing proceedings conference empirical methods natural language processing system strations cibils musat hossmann baeriswyl diverse beam search increased novelty abstractive summarization arxiv preprint ott edunov baevski fan gross grangier auli fairseq fast extensible toolkit sequence modeling proceedings naacl hlt demonstrations korobov morphological analyzer generator russian ukrainian languages analysis images social networks texts
