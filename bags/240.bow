semi supervised multiple representation behavior learning ruqian lua shengluan houa c ainstitute computing technology key lab iip chinese academy sciences beijing china bacademy mathematics systems sciences key lab madis chinese academy sciences beijing china cuniversity chinese academy sciences beijing china t c o l c s c v v x r abstract propose novel paradigm semi supervised learning ssl semi supervised multiple representation havior learning ssmrbl ssmrbl aims tackle diculty learning grammar natural language parsing data natural language texts labels marking data parsing trees grammar rule pieces labels compound structured labels require hard work training ssmrbl incremental learning process learn representation appropriate solution dealing scarce labeled training data age big data heavy workload learning compound structured labels present typical example ssmrbl behavior learning form ical approach domain based multiple text summarization dbmts dbmts works framework rhetorical structure theory rst ssmrbl includes representations text embedding representing tion contained texts grammar model representing parsing behavior rst representation learned embedded digital vectors called impacts low dimensional space grammar model learned iterative way automatic domain oriented multi text summarization approach proposed based representations discussed experimental results large scale chinese dataset sogouca indicate proposed method brings good performance labeled texts training respect dened automated metrics keywords semi supervised representation learning semi supervised multiple representation behavior learning incremental learning modular learning rhetorical structure theory multi text summarization lexical core introduction usually machine learning programmers embarrassed scarce labeled training data experts said nt worry lots source data unlabeled useful idea people invented semi supervised learning ssl detects common features labeled unlabeled examples help determine model characteristics dierent types ssl seen supervised classication learning sscl semi supervised representation learning ssrl semi supervised reinforcement learning ssrinfl semi supervised behavior learning ssbl situation leads invention development ssl lack massive set training data diculty complexity training model precisely diculty complexity training sample model examples include learning play analyzing set playing records learning prove mathematical theorem reading set mathematical theorems proves learning command campaign reading historical records military campaigns nally introduce later section paper learn grammar natural languages certain domain manually parsing limited set natural language texts common property examples enormous cost training single example usually tolerate high cost corresponding author email addresses ac cn ruqian lu com shengluan hou preprint submitted elsevier october fig relations dierent ssl schemas generally speaking ssl algorithm type usually presents multi lateral style military campaign example tells ocers conduct campaign concerns human behavior ssbl ocers learn war practices positive negative lessons practice experiences fostered practical combats ssrinfl ocers write strategic tactic plans campaign summaries general lessons acquired materials written textbooks training ocers ssrl technically ssl algorithm type dicult complicated design deal firstly concerns complicated problems generally nt xed unique solutions contrary solutions usually heuristic probabilistic massive data set training secondly massive data set trained usually growing steadily big data stream incremental approach needed thirdly usually involves multi factors multi views single criterion deciding resulting model fourthly given problem ssrl type algorithm usually limited nd single representation case rst representation learned result wanted nal result second representations needed play dierent roles learning process paradigm semi supervised multiple representation behavior learning ssmrbl paper present example ssmrbl project domain based multiple text tion dbmts working years given set ds natural language texts application domain d limited subset t selected ds training data text t parsed manually parsing tree based domain knowledge process programmer s behavior collected computer form grammar tree structures form basis reduction rules contexts parsing recorded building preference rules resolving shift reduce reduce reduce conicts parsing tree text set reduction preference rules contributed considered label text set label s forms grammar considered representation training set learning semi supervised cope unlabeled data introduce representation digital vector space text embedding makes use closeness concept digital space select representative texts initial labeling transfer labels unlabeled data select contents automatic summarization order forward special characteristics approach semi supervised behavioral representation learning learning grammar learning way analyze texts particular domain relations dierent ssl schemas illustrated fig contributions work summarized follows propose ssmrbl novel paradigm incremental multiple representation learning learn representation according specic tasks ssmrbl appropriate solution dealing scarce labeled data framework ssmrbl instance project dbmts dbmts oriented multi text summarization architecture rst parse texts discourse trees extract summary units representative selection strategy incorporate rhetorical structure theory rst dbmts learned grammar model tributed rhetorical structure grammar arsg basic processing unit elementary discourse unit edu key concept rst corresponds clause simple sentence investigate factors involved representation learning summary generation perform extensive experiments propose novel automated metrics report ndings remainder paper organized follows section related works section learning text embedding e rst representation detailed section section attributed rhetorical structure grammar obtain initial grammar model section learn grammar model iterative way domain oriented multi text summarization introduced section experimental results analysis section conclude paper section related works research builds previous works eld semi supervised learning representation learning rhetorical structure theory multi text summarization semi supervised learning aims improve generalization supervised tasks unlabeled data ssl usually combine unlabeled data smaller set labeled data gain better data representation classication accuracy wide range applications text classication discourse relation classication object detection based labeled unlabeled instances problem ssl dened learning function labeled data unlabeled data ssl contains learning paradigms transductive learning inductive learning transductive learning directly apply function unlabeled observed instances training time hand aim inductive learning learn parameterized function generalizable unobserved instances basic assumption ssl nearby nodes tend labels representation learning process learning representations data easier extract useful information building classiers predictors word embedding rst introduced bengio et al continuous vector representation captures semantic syntactic information word word embeddings proven useful nlp tasks embeddings longer texts phrase sentence texts necessary circumstances deriving based word embedding methods instance vector addition simple way consider sum words word orders recurrent neural network text embedding concatenation output states rnn bidirectional rnn word embeddings lstm gru commonly convolutional neural network series convolutional feature maps word embedding matrix text representation concatenation convolutional lter outputs max pooling time note rhetorical structure theory rst theory proposed mann thompson century text structuring rst aims investigate clauses sentences text spans connect logical topological way rst explains text coherence postulating hierarchical connected tree structure denote rs tree given text role function play respect parts text leaf node rs tree called elementary discourse unit edu rst assumes edu text span natural language sentence text nucleus satellite rs tree binary tree branches pair nucleus satellite nuclei rhetorically nucleus center considered signicant satellite modify nucleus signicant rst widely natural language analysis understanding generation need combine meanings larger text units according number input texts text summarization broadly categorized single text tion sds multi text summarization mds mds complicated problems tackled contradiction redundancy complementarity generating summarization multiple texts decades kinds approaches proposed including graph based methods lexical chain based methods constraint optimization based methods traditional machine learning based methods deep learning based methods deep learning based approaches able age large scale training data achieved competitive better performance traditional methods availability large scale corpora methods hand grouped abstractive ones extractive counterparts contrast extractive summarization summary composed subset tences words lifted input texts abstractive summarization concerns generation new sentences new phrases retaining meaning source complex extractive ones extractive approaches mainstream ones work novel framework mainly based related works attempts teach system parse texts discourse structure trees based small set human annotated texts determine edu selected produce nal summary multiple texts way summary generated method extractive text embedding learn representation rst representation domain texts given vector set target digital space text corpus mapped vector space texts similar mappings space close neighbors usually domain related text introduces review current state related domain major content texts manifested comments reviews consisting set domain concepts expressions form basis grammar reduction arsg paper lexical core dened quadruple domain concepts representing domain news reviews semantics summarized known ve ws factors news reports following example assume following text news january today imf noticed downward risk global economy growth surpassed peak value imf reduced growth anticipation developed new markets developing economies resp uno s report pointed usa s economy growth rate going eu s growth rate continue china s rate text contains following lexical cores imf notice global economy downward risk imf anticipation growth economies reduced usa economy growth rate going eu economy growth rate continue china economy growth rate careful analysis articles news reports found articles time tion time information present beginning articles time information small number time changes time changes imply object changes event cases table shows excerpts news articles hand time information important component domain news reviews respect semantics s useful necessary import time information lexical cores based observations empirically time information component lexical core procedure embedding follows computer scans text extract lexical cores use q t o s denote lexical core t o s means time agent object state change respectively attention focused domain oriented texts design lexical cores particular taken terms consideration occur frequently texts type domain news example weight ve ws components lexical core t o s corresponds resp components represented phrase price rice unied function g maps lexical core vector w dimensional space r phrase embedding technique requirement o s vector r components w dimensional vector w dimension size phrase embedding given lexical core represented vector space r text usually provides lexical cores embedding text represented addition called impact vectors representing lexical cores lexical core impact vectors space r consider separate spaces easing discussion space lexical cores impacts core space impact space resp chinese text cpi table excerpts news reports english translation september chen xiongxiong deputy minister miit invited guest zhongpu forum gave report entitled accelerating digital tion manufacturing industry chen shaoxiong pointed founding china country s manufacturing industry maintained sustained rapid development outstanding contributions china s economic construction especially tional congress communist party china turing industry achieved historic breakthroughs achieved comprehensive strength historic achievements steadily improved innovation capability signicantly enhanced industrial structure accelerated opment environment continuously optimized openness greatly improved solid foundation laid achieving years goal latest monitoring data tianjin investigation team national statistical bureau shows cpi city gust increased year year food prices rose non food prices rose consumer prices rose service prices rose january august city s average cpi increased period previous year august city s cpi rose food prices rose non food prices rose consumer prices rose service prices rose fig simplied illustration core space assume w omit moment rst component t core note consider impacts cores space impact space components lexical core equation fullled loss function training lexical core embedding t o o s w o s t o s t t o l t o t o correspond corrupted t o s respectively fig illustration text embedding text represented impact vector vector space coordinates computed trained lexical cores according algorithm way embedding domain text derived basis following procedures algorithm embed texts vector space input set d natural language texts application domain extract lexical cores form t o s edus text embed lexical core vector simply called core w dimensional space r embedding nents t o s w coordinates lexical core w dimension phrase embedding text calculate impact core vectors impact considered embedded mapping text fig w dimensional space cores impacts vectors attributed rhetorical structure grammar behavioral representation previous paper introduced approach attribute grammar based text summarization main idea follows assume large set texts given relating application domain d build domain knowledge base dkb domain concepts domain relations consider terminal non terminal symbols grammar use rhetorical relations glue build production rules terminal non terminal grammar symbols text text introduce dierent kinds attributes including rhetorical relations grammar attributed way attributed rhetorical structure grammar start denition simple version probability attribute grammar denition simple attributed rhetorical structure grammar sarsg tuple rs dre kcp rre rs start symbol dre set domain relations kcp set domain concepts rre set rhetorical relations synthesized attributes attribute arithmetical logical function grammar symbols arguments set production rules attached attribute equations rs dre non terminals rre kcp terminals production rule following form ae ae attribute equations denoting values d s attributes calculated b attributes d left parent symbol b right child symbols x y n s s n n n n means nucleus s means satellite b d domain relations l reason logic formula conrming rule legal production rule reducing string ab d current parsing context impacts lexical cores domain texts impacts distributed cubes fig text embedding rst representation attribute equation ae function calculating attribute values production s left symbol d attribute values production s right arguments denition attributed rhetorical structure grammar arsg seven tuple rs dre kcp rre rs dre kcp rre sarsg set probabilistic production rules attached attributes reasons denition set shift reduce precedence rules production rule following form ae n means production rule sarsg followed positive integer n calculating probability reducing ab d dynamically case reduction required precisely k productions ae ni probability j th production chosen equal set precedence tuples tuple form n j ni b c sl fa b c ps b c rl fa b c pr abc string neighboring grammar symbols parsing sl fa b c short shift logic formula reason shifting parser c rl fa b c short reduce logic formula reason reducing b dre truth values sl fa b c rl fa b c depend attribute values b c ps pr probabilities ps pr tuples resolving shift reduce conicts training sample set initial grammar model basic idea follows rst decide target application domain d aid domain oriented dictionaries thesaurus build knowledge base kb d including domain concepts domain relations domain concepts abstract concrete entities domain relations states state changes domain concepts example domain world economy trade wet domain concepts market price stock country bank ination balance improved expended domain relations given set ds texts d want learn grammar g section ds based kb texts d parsed g produce parsing tree text domain concepts relations considered terminal non terminal symbols g context sensitive designed attributed grammar rhetorical relations important attributes grammar rules binary rhetorical relation parent node attributes nucleus satellite nucleus nucleus child nodes accordingly parsing result binary tree parsing tree called attributed rhetorical structure tree given subset ds ds training process parsing texts ds cooperation programmer computer process consists sub processes rst sub process programmer mimics machine compiler scanning text left right decides shift reduce right action transforms text sentential step step parsing tree constructed level level time computer observes collects records information action human programmer particular computer detects human shows dierent behaviors parsing context produce separate grammar rule behavior record number occurrences rule finally second sub process text training set generated parsing tree set grammar rules computer synthesizes rule sets form wanted grammar possibly conicting rules grammar probabilistic number instances rule represents weight numbers application grammar calculate probability rule apply precisely rst sub process time programmer makes decision shift reduce computer records decision context sentential decision decision reduce machine produces rough reduction rule shown b l f recorded context sentential reduction reason reduction time human parser assign additional information enrich form note additional information missing default meaning represented following straight forward care care strait forward means d accepts attribute values b nt care means rhetorical relations b valid values note matter decision shift reduce rough preference rule produced computer following form b c sl fa b c b c rl fa b c sl fa b fa b c denotes context shift b b c reduction b c blank spaces parentheses saved possible probability values second sub process instances rule form collected calculate weight production rules form parent d b unique rule gets form ae means string ab reduces d decision reduction parents dk reducing ab divide rules k groups rules th group share parent symbol di assume number rule instances th group ni obtain k probabilistic rules means reduction ab parsing possibility reducing ab d j equal needs runtime calculation decide rule n j ni aek fk nk way instances divided groups b c sl fa b c ns b c rl fa b c nr means context ab c computer noted ns times shift nr times reduce human parser makes generated grammar g probabilistic computer parser g meets context future application perform shift reduce probability ns decision compile time nr iterative grammar model development learn second representation section shown embed texts d w dimensional space lexical core mapped vector called core text mapped impact sum core vectors section learn target representation attributed rhetorical structure grammar illustrated algorithm algorithm learn attributed rhetorical structure grammar input set ds natural language texts application domain d perform algorithm embed texts ds lexical cores sets vectors impacts cores w dimensional space r perform algorithm select representative subset s s s set impacts training learn initial model starting attributed rhetorical structure grammar perform algorithm recursively attach labels s s s learn target model attributed rhetorical structure grammar set s ds according dierent strategies representative selection algorithm contains alternative versions rithm algorithm algorithm learning initial model xed threshold q representative selection provide alternative version algorithm selects representatives based number population cube tactic suitable case size cubes big impacts cubes far algorithm steps algorithm step replaced new step algorithm learn initial model control number threshold input set s impacts distributed w dimensional space r determine w dimensional cube c smallest edge lengths c large contain set s determine positive integers divide cube smaller cubes lengths small cube edges resp count numbers impacts small cube calculate divergence impact imp sum covers euclidian distances pairs b cores composing imp b coordinates cube containing x determine positive integer q cube randomly q impacts selected p impacts contains p q impacts probability selecting impact divergence divided sum divergences impacts cube remove cubes let s set impacts ds domain texts embedded s ds representative set texts s representative impacts s use ds training set performing procedure build initial model starting attributed rhetorical structure grammar algorithm learn initial model percentage value threshold p q determine positive integer q cube randomly impacts selected p total number impacts cube probability selecting impact divergence divided sum divergences impacts cube remove cubes note strategies algorithm algorithm selecting tative set impacts initial model depends concrete situation programmer s decision basic idea step cluster unlabeled unparsed texts labeled ones according mutual distances closeness texts measured according dierence impacts r areas machine learning research necessary possible train immense data relevant current topic assume limit controlling learning algorithm limit concrete number percentage value data trained data corpus important said training label case expensive discuss practical cases section particular training impacts mentioned step algorithm step algorithm step algorithm counted threshold need manual processing human algorithm incrementally learn improve model input constants k dene distance impact set m impact e f m let s set impacts selected algorithm algorithm s set remaining impacts training limit reached let s s e s s goto step algorithm s build enhanced model let s e s s s s s s s let k select s s e s s s algorithm s build increased model s s s s s s end end spatial division impacts algorithm s running illustrated fig algorithm denote text produced impact e algorithm enhance model input s e s grammar rule r number instances r end end functions algorithms illustrated fig fig illustrative form fig gr means set grammar rules means number instances rule set increased notice dierence model enhancement round round fig learn grammar iteratively space division algorithm increase model input s perform actions described section ds s parsing tree corresponding set grammar rules model increase fig model enhancement numbers dierent grammar rule types unchanged numbers rule instances subject change increase case model increase possibly new rule types generated fig note concept impacts dierent concept centroid literature radev et al centroid dened group words statistically represent cluster texts impacts paper dened vector sums cores embedding space multiple domain texts summarization rhetorical trees section representations discussed work help automatic domain texts summarization mention algorithm previous paper making summary single text established grammar model representing text parsing tree attributed rhetorical structure tree algorithm summarize single domain text sketch input rhetorical structure tree number n requested summary length traverse tree according following principles starts root tree node traversed traversed child node node exists nucleus rst traversal going nucleus child selected rst keeps going reaching leaf signicant node outputted balanced reaching leaf restarts traversal highest brother node ancestor stops requested number edus outputted algorithm performs search rhetorical tree obtain priority sequence edus leaves tree number leaf shows priority alphabetically summary sequence selection fig sections described novel technique learn natural language grammar supervised way multiple text summarization following grammar learned fig semi supervised behavioral representation learning overview fig learn grammar iteratively second representation compare fig tricks applying grammar real summarization scripts acknowledge quantity texts summarized immense scopes texts distributed want grasp summarize texts provide mainstream information given domain leads strategy given isolated pieces information concentrating main stream information contained subset given texts summarized purpose use embedding technique select representative texts shown following algorithm worth noting dierent usual approaches summarizing texts given text set approach evaluates signicance given text summarizes texts signicance degree signicance text mean representativeness strategy makes approach ecient particular case texts summarized immense number n algorithm number edus number words dierent traditional text summarization approaches method based lexical cores rst techniques note possible process usual problems appear summary information redundancy inconsistency based technique technique multiple text summarization stay focus attention paper consider later publications fig balanced search rhetorical tree algorithm summarize multiple domain texts input arsg g learned algorithm set t texts summarized domain d requested length n edus extractive content summary t algorithm embed lexical cores t core space space embedded lexical cores impact space space impacts calculated cores algorithm modication cube q equals selected impact attached number impacts cube weight determine representative ordered sequence res impacts impact space order determined weights impacts remove impacts belonging res impact space use g parse set t es texts embedded res priority sequence s t r attributed rhetorical trees priority determined weight impact e step algorithm generate priority sequence s edus text ti t help rhetorical trees t r use priority function xi y j calculate synthesized priority j th edu d j th text ti t es xi s t r priority y j s priority summarization edus selected according order values priority function experiments evaluation results expensive create large parallel summarization corpus common case texts summarize instances summaries step problem paper pose ssmrbl especially behavior learning form grammatical approach domain based multiple text summarization apply method publically available chinese dataset sogouca large scale chinese dataset crawled provided sogou labs dozens chinese news websites ing news reports reviews text sogouca contains elds url docno contenttitle content leveraging url information categorize texts corresponding domain work choose domain d finance texts finance domain selected perimental dataset work texts domain news reports comments preprocessing including delete short lines ignore extreme long lines unlike english manipulate text word level word segmentation needed chinese text processing hanlp chinese word mentation speech pos tagging named entity recognition ner chinese natural language processing tool preprocessing statistics texts listed table rst extract lexical cores text automatic way named entity recognition techniques regular expressions tool detect time components e agent object state change sogou com labs resource php table statistics texts finance domain sogouca number texts number ponents agent number ponents object number nents s state change average number edus text recognized pos tag keyword extraction algorithms scan text segment edu sequence average number edus segmentation shown table table number components type denotes number unique components appear sogouca texts total number unique recognized times texts larger components components employed train embedding embed lexical cores vector space split dataset training validation test sets according proportion resulting number texts sets respectively trained model sets instances created training set tuned hyper parameters instances validation set tested model test set learn rst representation text embedding according algorithm model trained adam optimizer initial learning rate margin based objective function margin set dissimilarity measure d set norm dimension time agent object state change embeddings set w core space dimensional space finally impact text calculated sum lexical core embeddings considered embedded mapping text procedure lexical core embedded core space text represented impact impact space multi text summarization missing determine topic similar clusters contribute multiple text source generating summary intuitively texts similar corresponding impacts near impact space respect euclidian distance validated keyword based method randomly sampled bigger cube contains impacts dierent texts applied rake rapid automatic keyword extraction unsupervised domain independent method extracting keywords individual texts extract keywords text statistical results closer impacts common keywords corresponding texts share average text similarity recall closer impacts achieved quantitatively proved assumption following summarization step based impacts classify texts small clusters approximate texts according algorithm learn target representation e arsg assumed section threshold controlling data trained dataset learning procedure concrete number algorithm percentage value algorithm validate assumption tested dierent combination parameters set q picking near training far iteration according algorithm quantitative results number new labeled texts shown table note picking near k denotes number new labeled texts k th iteration training far k experiment intuitive list results k table labeled texts derived labeled training data build initial grammar model training far steps k obtain considerable number labeled texts concrete number percentage value threshold following experiments set k step apply algorithm multi text summarization measuring quality generated summary commonly evaluation metric text summarization rouge rouge evaluates n gram co occurrences summary pairs works comparing automatically produced summary set reference summaries method semi supervised assume small number rst labeled texts assume generating summary multiple texts topic parallel texts summary pair dened proxy metrics evaluate generated summaries example summaries build quantization standard consider following automatic statistics sentiment polarity accuracy summary reect consistent overall sentiment original texts address problem resources chinese sentiment analysis limited translated table quantitative results picking near training far iteration concrete threshold algorithm q k q k percentage threshold algorithm q k q k parameters initial picking near training far picking near training far picking near training far number labeled texts scale mate grammars title text english open source machine translation services identied sentiment polarity english version directly leveraging english oriented algorithm result taken sentiment chinese text trained cnn based sentiment analyzer given text predicts sentiment polarity summary compute accuracy sentiment analyzer s predicted polarity equal average polarity texts cluster novelty summary edus similarities summary edus lower case redundancy compute redundancy averaging scores l edus average averaged scores novelty summary edus computed novelty avg bedu s appearance named entities summary contain named entities nes texts cluster obtain score computing number unique nes appeared original texts divided number unique nes appeared summary word overlap score word overlap score measure generated summary lates original texts compute score scores summary text cluster average scores formally score generated summary multiple texts computed s core s n e w s n e w represents sentiment polarity accuracy novelty summary edus appearance named entities word overlap score respectively weights empirically set range score initial labeled texts obtained manually complex time consuming ducted experiments dierent number labeled texts initial grammar learning dierent total number training texts set size set number training texts combination parameters applied algorithm learn arsg applied algorithm clustered texts set parameter n algorithm table shows scores generated summaries reect eect initial labeled texts performance summary training texts labeled texts model obtain relatively good performance little promotion adding training texts labeled texts maybe misleading training texts table eect initial labeled texts performance summary n table experimental results model comparing baselines model n lead textrank dbmts n assess quality summaries conducted experiments test set lines method generates summary parallel texts summary pair select representative supervised multi text summarization approaches baselines including strong baseline lead graph based method textrank deep learning based method lead takes rst edus text cluster length limit texts assumed ordered according weight assigned algorithm textrank builds graph adds sentence vertices overlap sentences treated relation connects sentences graph based ranking algorithm applied convergence sentences sorted based nal score greedy algorithm employed impose diversity penalty sentence select summary sentences applies cnn project sentences continuous vector space uses textrank sentence ranking cnnlm extracts sentence representation cnn combines sentence tion representations context words predict word cnnlm trained unsupervised scheme resembles cbow scheme nishing training sentence adjacent graph based cosine similarity built textrank denote method dbmts training texts labeled texts automated score model baselines shown table dbmts model outperforms baselines proposed evaluation metric obtains slightly promotion n comparing performance better n tables observe proposed method brings good performance small labeled texts respect dened automated metrics concluding remarks note features approach firstly approach cross representational implemented parallel natural language level embedded digital level secondly behaviorally training natural language grammar hard work train text corpus necessity thirdly training set arbitrary taken optimally selected according representation capability helps optimize algorithm fourthly approach incremental new data texts added time generalize improve grammar fifthly approach modular grammar implemented group modules rule pieces reorganized meet dierent needs situations sixthly approach exible arsg grammar seventh approach universal idea applied semi supervised learning training transition data labels hard work work supported national key research development program china grant national natural science foundation china acknowledgments references references pp ahmed m s khan l sisc text classication approach semi supervised subspace clustering ieee international conference data mining workshops ieee pp banijamali e ghodsi semi supervised representation learning based probabilistic labeling arxiv preprint barzilay r elhadad m lexical chains text summarization acl workshop intelligent scalable text summarization bengio y ducharme r vincent p jauvin c neural probabilistic language model journal machine learning research berg kirkpatrick t gillick d klein d jointly learning extract compress proceedings annual meeting association computational linguistics human language technologies volume association computational linguistics pp canhasi e graph based models multi document summarization ph d thesis doktora tezi ljubljana universitesi slovenya chapelle o scholkopf b zien semi supervised learning chapelle o al eds reviews ieee transactions chen y wang x guan y automatic text summarization based lexical chains international conference natural neural networks computation springer pp cheng j lapata m neural summarization extracting sentences words proceedings annual meeting association computational linguistics volume long papers pp cheng y xu w z w wu h sun m liu y semi supervised learning neural machine translation proceedings annual meeting association computational linguistics volume long papers pp dai m le q v semi supervised sequence learning advances neural information processing systems pp fattah m hybrid machine learning model multi document summarization applied intelligence finn c yu t fu j abbeel p levine s generalizing skills semi supervised reinforcement learning arxiv preprint gambhir m gupta v recent automatic text summarization techniques survey articial intelligence review gillick d favre b scalable global model summarization proceedings workshop integer linear programming natural langauge processing pp h hanlp han language processing url com hankcs hanlp hernault h bollegala d ishizuka m semi supervised approach improve classication infrequent discourse relations feature vector extension proceedings conference empirical methods natural language processing pp hou s huang y fei c zhang s lu r holographic lexical chain application chinese text summarization asia pacic web apweb web age information management waim joint conference web big data springer pp kedzie c mckeown k daume iii h content selection deep learning models summarization proceedings conference empirical methods natural language processing pp kim y convolutional neural networks sentence classication proceedings conference empirical methods natural language processing emnlp pp kingma d p ba j adam method stochastic optimization arxiv preprint kipf t n welling m semi supervised classication graph convolutional networks arxiv preprint lin c y rouge package automatic evaluation summaries text summarization branches lu r hou s wang c huang y fei c zhang s attributed rhetorical structure grammar domain text summarization arxiv mann w c thompson s rhetorical structure theory functional theory text organization text interdisciplinary mcdonald r study global inference algorithms multi document summarization european conference information preprint journal study discourse retrieval springer pp mihalcea r tarau p textrank bringing order text proceedings conference empirical methods natural mikolov t chen k corrado g dean j ecient estimation word representations vector space arxiv preprint peyrard m simple theoretical model importance summarization proceedings conference association computational linguistics pp radev d r jing h stys m tam d centroid based summarization multiple documents information processing management rose s engel d cramer n cowley w automatic keyword extraction individual documents text mining applications language processing theory rosenberg c hebert m schneiderman h semi supervised self training object detection models wacv motion rush m chopra s weston j neural attention model abstractive sentence summarization proceedings conference empirical methods natural language processing pp taboada m c mann w rhetorical structure theory looking moving ahead discourse studies weston j ratle f mobahi h collobert r deep learning semi supervised embedding neural networks tricks trade springer pp yang z cohen w w salakhutdinov r revisiting semi supervised learning graph embeddings proceedings international conference international conference machine learning volume jmlr org pp yin w pei y optimizing sentence modeling selection document summarization fourth international joint conference articial intelligence
