summarize outline and elaborate long text generation via hierarchical supervision from extractive summaries xiaofei sun chun fan zijun sun yuxian meng fei and jiwei of computer science and technology zhejiang university computer center of peking university cheng laboratory shannon ai xiaofei sun zijun sun yuxian meng jiwei com edu cn edu abstract long text generation remains a challenge the difculty of generating coherent long texts lies in the fact that existing models ingly focus on the tasks of local word tion and can not make high level plans on what to generate or capture the high level discourse dependencies between chunks of texts inspired by how humans write where a list of bullet points or a catalog is rst outlined and then each bullet point is expanded to form the whole article we propose soe a pipelined system tha involves of summarizing outlining and elaborating for long text generation the model rst outlines the summaries for ent segments of long texts and then elaborates on each bullet point to generate the ing segment to avoid the labor intensive cess of summary soliciting we propose the construction strategy which extracts segment summaries in an unsupervised manner by lecting its most informative part to reconstruct the segment the proposed generation system comes with the following merits the summary vides high level guidances for text generation and avoids the local minimum of individual word predictions the high level discourse dependencies are captured in the conditional dependencies between summaries and are served during the summary expansion process and additionally we are able to consider signicantly more contexts by representing contexts as concise summaries extensive experiments demonstrate that soe produces long texts with signicantly better quality along with faster convergence speed introduction despite that recent large scale pretrained language models plms peters et al devlin et al liu et al yang et al clark et al radford et al li et al brown et al are able to produce high quality passages that can be hardly recognized by humans zellers et al most of the generated good texts are within very limited length e hundreds of tokens for most cases guo et al bao et al yan et al and generating ent long texts remains a challenge radford et al tan et al the difculty lies in the fact that existing models generate texts in a by word manner predicting each subsequent token given its proceeding contexts using the softmax objective this word by word strategy ingly focuses on the prediction of local words and can not make high level plans on what to generate this results in the fact that long texts generated by current models are usually repetitive generic and self contradictory shen et al to address these issues the coarse eration strategy is proposed fan et al xu et al yao et al mao et al in coarse generation a list of keywords or a short prompt is rst generated serving as a mary of the original text the prompt is then fed to a model as an input to output the complete text the coarse generation strategy cantly improves generation over the word by word strategy but still suffers from the following comings a limited capacity of the prompt a gle keyword list or prompt does not have enough capacity to summarize all the texts of long sages since long texts are usually consists of eral parts each of which focuses on a specic pect or topic zhou et al narayan et al guan et al the usage of the generation strategy is thus limited to texts that can be summarized by a single prompt e short stories this explains why text length erated by the progressive generation model is still limited e the introduced writingprompts dataset t c o l c s c v v i x r a in fan et al has an average length of ries around and the average length of prompts is ignorance of high level discourse pendency the coarse generation strategy does not capture discourse level dependencies li and jurafsky jernite et al which handle the high level information ow and tions between segments of texts the ignorance of discourse level dependencies results in texts ing for coherence humans write in a hierarchical top down ner before writing a thousand word long essay a human usually rst prepares a list of bullet points or a catalogue and then expands them to form the whole article the sentence level coherence tween these bullet points is preserved when the bullet points are expanded providing guarantees that the full text is coherent to mimic this top down manner of human ing in this paper we propose soe a pipelined system tha involves of summarizing outlining and expanding for long text generation the model rst outlines the summaries for different segments of long texts which actually mimics the process of humans outlining bullet points next the model elaborates on each bullet point to generate the responding segment the proposed strategy comes with the following merits since each segment is associated with its own summary rather than the entire text sharing a single prompt the capacity of summaries to reconstruct the full text can be guaranteed the conditional generation bility between summaries captures the high level discourse dependencies and these dependencies are preserved when they are expanded to segments this naturally resolves the incapability of modeling discourse level dependencies in the coarse c this model is able to generation approach consider signicantly larger amount of contexts by representing chunks of contexts as concise maries empirically we do not readily have summaries for segments in hand the model thus needs to learn to summarize in an unsupervised manner spired the gehrmann et al zhou et al liu et al zhong et al we propose the reconstruction strategy which extracts segment summaries by selecting its most tive part to reconstruct the segment extensive experiments demonstrate that soe produces long texts with signicantly better quality than existing baselines the rest of this paper is organized as follows section presents related works followed by tion reviewing some backgrounds section troduces our proposed approach in detail section and section respectively present the experiment results and ablation studies last we give a sion in section related work generating long texts there are two lines of work to generate long text this rst line of work tackles the problem from the model perspective new model structures kitaev et al child et al dai et al ye et al guo et al sukhbaatar et al correia et al beltagy et al zaheer et al li et al are designed to give the model the ability to congest more texts given limited memories or computing power for example transformer xl dai et al a modier to transformers vaswani et al uses a segment level recurrence mechanism to able learning long term dependencies child et al correia et al kitaev et al beltagy et al zaheer et al proposed to sparsify transformers by focusing only on a tion of attention connections tay et al placed the dot product self attention with learned synthetic attention weights li et al used an lstm predictor to automatically learn attention connections adapted to downstream tasks the second line of researches focuses on oping new generation strategies efforts have been devoted to the idea of planning then generation or coarse generation wiseman et al sha et al gehrmann et al wiseman et al moryossef et al puduppully et al hua and wang shen et al fu et al which greatly inspires this work in coarse generation a list of keywords or a short sentence is rst generated providing ances to generate the full text a recent work from tan et al takes a multi step strategy which progressively renes the generated incomplete text until reaching a specied stage similar ideas have also been applied to text summarization where gehrmann et al proposed a bottom up method that rst identies phrases within a ment that are likely included in its summary our work is also inspired by the strategy of hierarchical generation li et al yu et al lapati et al liu and lapata which consider text units with bigger granularity li et al proposed hierarchical lstms that arrange tokens sentences and paragraphs in a cal structure with different levels of lstms turing compositionality shen et al used multi level structures to learn a vae model for generating long coherent text similar strategies are applied to the video captioning problem where yu et al exploited hierarchical rnns for video caption generation extractive summarization extractive summarization refers to the problem of selecting part of the input text as its summary a fundamental problem in extractive summarization is to score constituent texts units e phrases sentences or paragraphs and select highly ranked as the summary haghighi and vanderwende used word frequencies in the input text to assign scores to words which are then in turn used to score sentences higher ranked sentences are selected as the summary of the input text liu et al presented a two stage extractive abstractive framework which rst coarsely identies salient information followed by a generation model used to rene it neural models have been widely used for scoring cao et al ren et al zhou et al liu and lapata tuned bert devlin et al to score each sentence for extractive summarization zhang et al computed token similarity in each sentence using bert contextual embeddings to serve as an automatic evaluation metric for text generation background we begin by reviewing the task of text generation language modeling refers to the process of culating the probability of a sequence y yt where each yi denotes a constituent token of the probability can be computed by decomposing the joint distribution into a product of conditional distributions over tokens t t where y is the partial sequence of tokens generated previously during training the model is optimized to minimize the negative log likelihood nll yd log during inference the model decodes a token at each time step t according to t based on the softmax functions yt where wout is the output word embedding trix and ht is the hidden state at time step t ous smoothing models have been proposed to avoid overtting et al pereyra et al sequence to sequence generation models generate a target sequence y conditioning on a given source sequence which differs from language models lms in terms of whether or not conditioning on another input sequence similar to lms the probability of the target sequence can be typically factorized as t t models are also optimized to minimize the nll log in the rest of this paper we unify the notation of and by setting for lms different architectures have been proposed to model t ing transformers vaswani et al lstms luong et al and cnns dauphin et al at test time sequences are usually ated using beam search or its variants to promote diversity vijayakumar et al model details for soe in this section we describe the details of soe notations a long sequence of tokens y yk is rst sliced into a series of snippets yis where k denotes the number of constituent snippets here we use the bold font to denote snippets and the normal font y to denote tokens the ber of tokens n within each snippet is a parameter we also use superscript i to denote the index of a snippet and subscript l to denote the index of a token each yi consists a sequence of tokens yi yi ni where ni denotes the length of yi our goal is to generate a subset of y denoted by yjk yk given its proceeding snippets denoted by j each snippet yi is associated with a short summary si l denotes tokens and mi is the number of tokens in si mi where si yi that contain the word which can be expressed as nw log nd where nw is the word count nd is ndw the total number of documents and ndw is the total number of documents containing the word textrank textrank mihalcea and tarau is a weighted graph with tokens as nodes and the similarity between nodes as edges we use bert devlin et al to compute the similarities between sentences and then rank them based on the textrank algorithm reconstruction a summary should be more formative than non summary sentences that is a summary should have the most ability to reconstuct the full text to measure the degree of a sentence s reconstruction ability we use a model to predict the original given text the summary tence the probability of which is regarded as the reconstruction score suppose there are n sentences in yi e n and yi and yi j denotes the j th sentence in yi the struction score for yi j denoted by j is given as follows j log j to obtain j we train another model where the input is yi j for each j and the output is yi by sequentially predicting tokens in yi given the trained model we are to rank all sentences in yi and use the one with the highest score as the golden summary outlining segment summaries in the summary generation stage we can not serve yjk and our goal is to sequentially generate sjk given y j i i s i the generation of summary si can be factorized into sequentially generating the constituent word within it i s i l y i s i this process ends until generating a special of sequence token eos or reaching a specied summary length m we use the transformer base architecture vaswani et al as the backbone figure an overview of the proposed method given proceeding tokens i we rst sequentially generate summaries sjk for each snippet next we expand each summary s to form the full text yjk pipeline overview instead of generating all constituent words in y one by one we adopt a hierarchical strategy the process of generating yjk is decoupled into the following two stages outlining segment summaries we quentially generate the summary si for each snippet conditioning on the summaries for previous pets this mimics the process of catalog generation when humans write expanding summaries to texts we pand each summary to the full segment by quentially generating its constituent words an overview of the proposed method is shown in figure extracting golden summaries at the training time we need to learn to generate summaries but this is not straightforward because the golden summary si for the snippet yi is not readily at hand manually soliciting summaries like fan et al is both costly and slow we thus propose to take the idea of unsupervised tractive summarization and for each snippet yi we extract its summary unsupervisedly and use the extracted si as the golden summary for learning we investigate the following extractive methods to access the importance of selecting summary tences the rst three of which are similar to liu et al random for comparing purposes we use a dom sentence as the summary tf idf we take the sentence with the highest average tf idf score ramos as the golden summary a word is assigned a score by idf that scales proportionally to the number of times the word appears in the document and is offset by the number of documents in the corpus source target segment segment segment k k k to take into account more contexts we adopt the segment level recurrence strategy similar to dai et al where the hidden states computed for far away snippets are xed and cached to be reused for the next new snippet gradients are not propagated to these far away snippets for memory and computation efciency this strategy allows the model to exploit information in history to the largest extent expanding summaries to texts next we expand each summary si to the full text for each segment by sequentially generating its constituent words i si l y i which has the same termination conditions as in the summarization generation training and inference training for summary generation the takes i s i as the input former model and is optimized by minimizing the nll loss log i s i due to the memory tion we limit y i to proceeding tokens and i to tokens at training it is worth noting that the tokens of y i mostly come from the ment right before i e while s i comes from multiple proceeding segments since the summary is more concise takes i as for the summary expanding stage the former model input and is optimized by minimizing the nll loss log i the two models i e the mary generation and the summary expansion model share parameters with a task specic token pended to the start to notify the model on what to generate summaries or segments inference at test time we rst use beam search with beam size to generate summaries given the generated summary beam search is used again to generate the corresponding segment we consider more contexts at test time where y i is limited to tokens and s i is limited to tokens additionally we augment the vanilla beam search with the strategy of mutual information reranking li et al fang et al the key point of mutual information is to instead of merely handling the uni directional dependency from the source to target based on the forward probability log it models the tual dependency between the source and target in sequence to sequence generation i e the tion of the forward probability log and the backward probability log specically in our case for summary generation is generated as follows arg max log i s log where i si is the backward probability of predicting the proceeding summary given since direct decoding from eq is infeasible we follow the practical solution in li et al where we rst generate an n list based on the forward probability i s and then rerank the n list by combing the forward ability and the backward probability similar strategy can also be applied to the mary expanding stage where yi is obtained as lows yi arg max log i si yi log the backward probability predicts the proceeding segment given the current segment again beam search is combined with reranking to approximately nd the optimal result slicing texts based on coherence scores one more thing we need to care about is how to slice the text into segments the simplest way is to slice the full text equally but this is sub optimal since the break point could be in the middle of two closely related sentences and one segment might contain multiple aspects we thus propose a slicing strategy based on sentence level coherent scores using the next sentence prediction nsp from bert devlin et al we are able to measure the coherence score between two consecutive sentences with dex i and i denoted by i given a full text y let t denote the number sentences in y and denote the ith sentence given a xed value k for the number sliced segments y will be sliced into k segments simplify as where we train a model to predict the proceeding summary given the current summary i e yk where each yk consists of a group of consecutive sentences from y let gk denotes the list of indexes of sentence in original y where denotes the index of the rst sentence in gk denotes the second sentence let rk denote the number of sentences in gk we wish to maximize the coherence scores tween two consecutive sentences within the same segment and minimize the score between two secutive sentences belonging to different segments giving the following objective to optimize k l where the coherence score between the ending sentence of a segment and the starting sentence of the next segment given j eq can be readily solved using linear programming experiments in this section we present experiment results for different methods to generate summaries we nd that the performance of reconstruction consistently outperforms the rest in our preliminary results we thus only report results from reconstruction in the section we will get back to analysis on different summary generation methods in the ablation study section datasets we need a corpus of contiguous and long text to test soe we use two word level datasets merity et al and the bookcorpus dataset zhu et al contains m training words from k articles with an average length of k words per article can be used to test the ability of modeling long term dependencies the bookcorpus dataset is a more suitable dataset for our purpose with much longer and more contiguous texts it contains a total number of roughly billion words and million sentences from books with an average length of k words for each book the average number of words per sentence is for both datasets we predict the last tokens at test time baselines transformer xl transformers with level recurrence strategy dai et al rally constitutes a baseline the model sequentially generates texts in a word by word fashion writingprompts rst predicts a list of keywords or a single prompt and then generates the full text given the prompt fan et al different from fan et al where golden prompts for stories are available we do not readily have the golden prompts we thus use the extractive strategies scribed in section i the tf idf method to pick the keyword list as the prompt denoted by writingprompts keyword and the reconstruction method to select the highest ranking sentence as the prompt denoted by writingprompts sentence progressive writingprompts the progressive strategy proposed in tan et al which volves multiple stages of prompt generation each stage produces a more ne grained sequence than the stage that comes before and is used as the put to generate the prompt for the next stage we follow the protocols in tan et al and use the tf idf score to obtain golden prompts for each stage the number of stages is set to for all models we used use adam kingma and ba with learning rate of rate warmup over the rst steps and linear decay of the learning rate we use a dropout rate of on all layers including the softmax layer evaluations we use the following evaluation metrics to evaluate the quality of different generation models from different perspectives perplexity ppl perplexity measures how ent a piece of generated text could be dai et al we use ppl as the basic evaluation metric in our experiments diversity perplexity can not measure how diverse the generated text is we thus use the scaled ber of distinct unigrams and bigrams to demonstrate the degree of diversity li et al for generated texts adversarial success inspired by adversarial evaluations bowman et al kannan and vinyals li et al we use the ial success metric which is dened as the fraction model bookcorpus perplexity parameters perplexity parameters vanilla writingprompts keyword writingprompts sentence progressive writingprompts soe vanilla soe base large m m m m m m m m m m m m m m table perplexity of different models on and bookcorpus vanilla stands for our implementation of transformer xl dai et al model diversity adversarial success adversarial success s level coherence nsp vanilla writingprompts keyword writingprompts sentence progressive writingprompts soe base msj table results of different models in terms of diversity adversarial success msj and sentence level coherence on the bookcorpus corpus vanilla stands for our implementation of transformer xl dai et al d n stands for distinct and mi stands the results for mutual information reranking model large vanilla soe table results of different models with large volumes in terms of diversity on the bookcorpus dataset of a model successfully fooling a trained ator to believe that machine generated texts are from humans the evaluator is a binary tion model at the training time it takes as inputs machine generated texts and original texts and are trained to discriminate them at test time sarial success is the value acc where acc notes the accuracy of the trained evaluator ing machine generated texts as machine generated higher values of adversarial success denotes better text quality ms jaccard msj msj measures the similarity of the n gram frequencies between the generated texts and the golden texts montahaei et al we report and sentence level coherence ppl msj and sity scores do not reect the sentence level ence of generated texts we adopt the strategy in tan et al where next sentence prediction nsp from pretrained bert model devlin et al is used as a metric to measure the ence between each sentence and its next sentence we report average nsp scores for all consecutive sentence pairs within the generated text results table shows the results of perplexity for ent models on the and bookcorpus datasets on both datasets soe achieves the lowest ppl compared to baselines transformer xl dai et al writingprompts fan et al and progressive tan et al in particular for we gain a ppl decrease and against our implemented transformer xl writingprompts sentence and progressive while having the same or even fewer parameters similar trend can be observed on bookcorpus table and table show the results for msj diversity adversarial success and level coherence scores as can be seen writingprompt based models generally outperform the transformer xl model which adopts the by word generation strategy this validates the superiority of two step generation strategy over the naive word by word generation strategy for long text generation the progressive prompt model which involves multi step of eration and expanding outperforms the one step the writingprompt keyword and sentence model which is in accord with our pectation soe achieves signicantly better results compared to vanilla writingprompts and sive models in terms of all evaluation metrics ing that the proposed method can produce more uent coherent and diverse texts the consistent performance boosts on all metrics demonstrate the importance of modeling discourse level cies and necessity of summary expanding strategy for long text generation additionally enhanced by mutual information mi we observe additional performance boosts pecially for diversity and adversarial success this is in accord with our expectation since mutual information is able to build bidirectional dencies between the source and the target models enhanced with mutual information can generate better summaries and the phenomenon of generic and repetitive generation can be alleviated li et al leading to more diverse results ablation studies the effect of segment length the size of the segment can be neither too big nor too small extremely long segments might tain too many aspects or topics for the summary to summarize in which case the model will erate into the writingprompts model fan et al for too short segments the summary not provide high level guidance we thus need to nd the sweet spot for the segment length figure shows results on the bookcorpus dataset it is clear from the gure that too short segments and too long segments both lead to inferior performances the effect of summary generation strategies it is worthwhile to explore how different summary extraction methods affect the nal performances to this end we conduct experiments on the corpus dataset using different summary extraction methods i e random textrank tf idf and construction table shows the results we rst compare the ppl for summary generation where the reconstruction model achieves the lowest ppl figure ppl on the bookcorpus dataset w t ent segment lengths method summary ppl text vanilla random textrank tf idf reconstruction table performances of different summary extracion methods described in section vanilla is the plain model that generates tokens one by one without maries and thus produces summaries that are the easiest it is also to predict given proceeding contexts interesting to see that across all summary tion strategies ppl for summarization generation is signicantly larger than text prediction which is reasonable since summaries for the upcoming segment requires more generalization abilities and there are more diverse options for what the next segment should talk about than the local choices for what the next sentence should talk about for the nal text generation ppl struction achieves the best results in terms of ppl and textrank and tf idf are better than vanilla interestingly the strategy of using random sentences as summaries performs worse than without summaries which can be explained by providing no guidances is better than incorrect guidances the effect of coherence based text slicing we replace the coherence based text slicing egy with the naive equal slicing strategy and see how this will negatively affect the performance on the bookcorpus dataset we observe an increase of summary generation ppl from to and idfreconstruction an increase of ppl from to in ken generation which demonstrates the importance of slicing text into coherent segments for tion but it is also worth noting that even with the native equal slicing strategy soe still performs signicantly better than other baseline models decoupling the effects of summaries the positive effects from summaries are two fold it provides high level guidances for segment generation and with far away segments being concisely represented by summaries it gives the model the ability to consider longer contexts to quantitatively measure the inuences from both aspects we conduct the following experiments at test time for the computation i s i and i the model can only access maries for segments that are used as contexts in other words only summaries within the kens of proceeding contexts can be fed as inputs this is different from the original version of soe in which s can extend to proceeding contexts until the limit of tokens is reached we did not train the model but add this limitation at test time on the bookcorpus dataset this leads to an crease of in ppl vs and a decrease of and in vs and vs simplifying i s i here we explore different simplications for i s i for i s i the current mary is generated based on both previous maries and segment tokens we can simplify it as i where previous segment tokens are not fed as inputs to predict the summary which will signicantly decreases computing complexity on the bookcorpus dataset we observe an increase of ppl in summary generation from to which subsequently leads to an increase of ppl from to in token generation convergence speed figure convergence speed for different models model converges faster than then vanilla because of the high level guidance from prompts conclusion in this paper we propose a two step hierarchical generation strategy for long text generation the model rst generates the summary for each ment conditioning on previous summaries and next each summary is expanded to form the full text segment the proposed strategy provides level guidances for local text generation and ables high level discourse dependencies to be tured extensive experiments demonstrate that soe produces long texts with signicantly better quality references hangbo bao li dong furu wei wenhui wang nan yang xiaodong liu yu wang songhao piao feng gao ming zhou and hsiao wuen hon pseudo masked language models for ed language model pre training iz beltagy matthew e peters and arman cohan longformer the long document transformer samuel r bowman luke vilnis oriol vinyals drew m dai rafal jozefowicz and samy gio generating sentences from a continuous space at last we investigate how quickly different els converge results are shown in figure with the guidance of extracted summaries soe has a conspicuously faster convergence speed where at about k training steps it has approximately reached the best result while the other two models vanilla and writingprompts do not converge until k training steps the writingprompts tom b brown benjamin mann nick ryder melanie subbiah jared kaplan prafulla dhariwal arvind neelakantan pranav shyam girish sastry amanda askell al language models are few shot learners arxiv preprint ziqiang cao furu wei li dong sujian li and ming zhou ranking with recursive neural works and its application to multi document rization in aaai pages citeseer keywordours rewon child scott gray alec radford and generating long ilya sutskever quences with sparse transformers arxiv preprint kevin clark minh thang luong quoc v le and christopher d manning electra pre training text encoders as discriminators rather than tors arxiv preprint goncalo m correia vlad niculae and andre f t martins adaptively sparse transformers in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural guage processing emnlp ijcnlp pages hong kong china association for tional linguistics zihang dai zhilin yang yiming yang jaime bonell quoc le and ruslan salakhutdinov transformer xl attentive language models beyond in proceedings of the a xed length context annual meeting of the association for tional linguistics pages florence italy association for computational linguistics yann n dauphin angela fan michael auli and david grangier language modeling with gated volutional networks in international conference on machine learning pages jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language ing arxiv preprint angela fan mike lewis and yann dauphin erarchical neural story generation arxiv preprint hao fang saurabh gupta forrest iandola rupesh k srivastava li deng piotr dollar jianfeng gao aodong he margaret mitchell john c platt al from captions to visual concepts and back in proceedings of the ieee conference on computer vision and pattern recognition pages yao fu yansong feng and john p cunningham paraphrase generation with latent bag of words sebastian gehrmann falcon dai henry elder and alexander rush end to end content and in plan selection for data to text generation ceedings of the international conference on natural language generation pages tilburg university the netherlands association for putational linguistics sebastian gehrmann yuntian deng and alexander m rush bottom up abstractive summarization arxiv preprint the aaai conference on articial intelligence ume pages jiaxian guo sidi lu han cai weinan zhang yong yu and jun wang long text generation via adversarial training with leaked information arxiv preprint qipeng guo xipeng qiu pengfei liu yunfan shao xiangyang xue and zheng zhang transformer in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota ation for computational linguistics aria haghighi and lucy vanderwende ing content models for multi document tion in proceedings of human language gies the annual conference of the north american chapter of the association for tional linguistics pages xinyu hua and lu wang sentence level tent planning and style specication for neural text generation yacine jernite samuel r bowman and david tag discourse based objectives for fast supervised sentence representation learning arxiv preprint anjuli kannan and oriol vinyals ial evaluation of dialogue models arxiv preprint diederik p kingma and jimmy ba adam a method for stochastic optimization arxiv preprint nikita kitaev ukasz kaiser and anselm levskaya reformer the efcient transformer arxiv preprint chunyuan li xiang gao yuan li xiujun li baolin peng yizhe zhang and jianfeng gao mus organizing sentences via pre trained modeling of a latent space arxiv preprint jiwei li michel galley chris brockett jianfeng gao and bill dolan a diversity promoting tive function for neural conversation models arxiv preprint jiwei li michel galley chris brockett jianfeng gao and bill dolan a diversity promoting jective function for neural conversation models in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies pages san diego california association for computational linguistics jian guan yansen wang and minlie huang story ending generation with incremental encoding in proceedings of and commonsense knowledge jiwei li and dan jurafsky neural net models for open domain discourse coherence arxiv preprint jiwei li minh thang luong and dan jurafsky a hierarchical neural autoencoder for paragraphs and documents arxiv preprint jiwei li will monroe tianlin shi sebastien jean alan ritter and dan jurafsky ial learning for neural dialogue generation arxiv preprint xiaoya li yuxian meng qinghong han fei wu and jiwei li sac accelerating and structuring self attention via sparse adaptive connection arxiv preprint peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer generating wikipedia by arxiv preprint summarizing long sequences yang liu and mirella lapata hierarchical transformers for multi document summarization in proceedings of the association for computational linguistics pages florence italy association for tational linguistics the annual meeting of yang liu and mirella lapata text tion with pretrained encoders yinhan liu myle ott naman goyal jingfei du dar joshi danqi chen omer levy mike lewis luke zettlemoyer and veselin stoyanov roberta a robustly optimized bert pretraining proach arxiv preprint minh thang luong hieu pham and christopher d manning effective approaches to based neural machine translation arxiv preprint huanru henry mao bodhisattwa prasad majumder lian mcauley and garrison w cottrell proving neural story generation by targeted common sense grounding arxiv preprint stephen merity caiming xiong james bradbury and richard socher pointer sentinel mixture els arxiv preprint rada mihalcea and paul tarau textrank ing order into text in proceedings of the ference on empirical methods in natural language processing pages ehsan montahaei danial alihosseini and mahdieh leymani baghshah jointly measuring sity and quality in text generation models arxiv preprint amit moryossef yoav goldberg and ido dagan step by step separating planning from realization in neural data to text generation ramesh nallapati bowen zhou caglar gulcehre bing xiang al abstractive text tion using sequence to sequence rnns and beyond arxiv preprint shashi narayan shay b cohen and mirella just the ata do nt give me the details summary topic aware convolutional neural works for extreme summarization arxiv preprint gabriel pereyra george tucker jan chorowski ukasz kaiser and geoffrey hinton izing neural networks by penalizing condent output distributions arxiv preprint matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer deep contextualized word in proceedings of the resentations ence of the north american chapter of the ation for computational linguistics human guage technologies volume long papers pages new orleans louisiana association for computational linguistics ratish puduppully li dong and mirella lapata data to text generation with content selection and planning alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever language models are unsupervised multitask learners openai blog j ramos using tf idf to determine word vance in document queries pengjie ren zhumin chen zhaochun ren furu wei jun ma and maarten de rijke leveraging contextual sentence relations for extractive rization using a neural attention model in ings of the international acm sigir ence on research and development in information retrieval pages lei sha lili mou tianyu liu pascal poupart sujian li baobao chang and zhifang sui planning neural text generation from structured data arxiv preprint dinghan shen asli celikyilmaz yizhe zhang liqun chen xin wang jianfeng gao and lawrence carin towards generating long and coherent text with multi level latent variable models arxiv preprint tianxiao shen victor quach regina barzilay and tommi jaakkola blank language models arxiv preprint sainbayar sukhbaatar edouard grave piotr janowski and armand joulin adaptive tention span in transformers in proceedings of the annual meeting of the association for tational linguistics pages florence italy association for computational linguistics bowen tan zichao yang maruan ai shedivat eric p xing and zhiting hu progressive generation of long text hierarchical recurrent neural networks in ings of the ieee conference on computer vision and pattern recognition pages manzil zaheer guru guruganesh avinava dubey joshua ainslie chris alberti santiago ontanon philip pham anirudh ravula qifan wang li yang et al big bird transformers for longer quences arxiv preprint rowan zellers ari holtzman hannah rashkin yonatan bisk ali farhadi franziska roesner and yejin choi defending against neural fake news in advances in neural information ing systems pages tianyi zhang varsha kishore felix wu kilian q weinberger and yoav artzi bertscore arxiv preprint uating text generation with bert ming zhong pengfei liu yiran chen danqing wang xipeng qiu and xuanjing huang tive summarization as text matching arxiv preprint deyu zhou linsen guo and yulan he ral storyline extraction model for storyline tion from news articles in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long papers pages new orleans louisiana ation for computational linguistics qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao neural ment summarization by jointly learning to score and select sentences arxiv preprint yukun zhu ryan kiros richard zemel ruslan salakhutdinov raquel urtasun antonio torralba and sanja fidler aligning books and movies towards story like visual explanations by watching movies and reading books yi tay dara bahri donald metzler da cheng juan zhe zhao and che zheng synthesizer thinking self attention in transformer models arxiv preprint ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in i guyon u v luxburg s bengio h wallach r fergus s vishwanathan and r nett editors advances in neural information cessing systems pages curran ciates inc ashwin k vijayakumar michael cogswell prasath r selvaraju qing sun stefan lee david crandall and dhruv batra diverse beam search decoding diverse solutions from neural quence models arxiv preprint sam wiseman stuart m shieber and alexander m rush challenges in data to document ation sam wiseman stuart m shieber and alexander m rush learning neural templates for text eration ziang xie sida i wang jiwei li daniel levy aiming nie dan jurafsky and andrew y ng data noising as smoothing in neural network language models arxiv preprint jingjing xu xuancheng ren yi zhang qi zeng xiaoyan cai and xu sun a based model for promoting coherence among tences in narrative story generation arxiv preprint yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang and ming zhou prophetnet predicting future n gram for sequence to sequence pre training zhilin yang zihang dai yiming yang jaime bonell russ r salakhutdinov and quoc v le xlnet generalized autoregressive pretraining for language understanding in advances in neural formation processing systems pages lili yao nanyun peng ralph weischedel kevin knight dongyan zhao and rui yan and write towards better automatic storytelling zihao ye qipeng guo quan gan xipeng qiu and zheng zhang bp transformer modelling long range context via binary partitioning arxiv preprint haonan yu jiang wang zhiheng huang yi yang and wei xu video paragraph captioning using throughout her career ayola has been outspoken on the subject of racial discrimination in the tainment industry describing her motivation she states i am not an overtly political person i just want fairness ayola believes that black actors receive less recognition than their white counterparts explaining if youu get a show with six stars and one is black you are more likely to see interviews with the ve white actors they are not being sold as a reason to watch shh e believes that her career would have taken her in a different direction were she not of ethnic origin stating i could not have played any of the roles i have playee d on tv if i was white i am very aware of where the glass ceiling is and it s still very low and expectations are still very low she has noted having casting directors accept the notion of characters being both black and welsh to be a particular problem explaining that i get offered a lot of very different roles but tt hey re never welsh the one time i was asked to play a welsh character on screen was in tiger bay for bbc wales but i know if that series had been called radyy r park or cyncoed close i wouldn t have been in it in ayola founded a production company and directed a short lm entitled persephone s playground she presented the lm at the cannes lm festival using it as part of her campaign for increased black representation in theatre lms and television the project however was largely unsuccessful with ayola stating it just made me decide that if there s anything i don t want to do it s produce lms because i m rubbish i was so bad with the budget that i just said yes to everything and then had too worry about how to pay for things at the end in ayola offered her support to the action for southern africa campaign dignity period aiming to provide affordable sanitary protection to zimbabwean women the hand over of command of military operations from interfet to untaet was completed on february australia continued to support the un peacekeeping operation with between and personnel as well as landing craft and blackhawk helicopters and remained the largest contributor of personnel to the peacekeeping mission during these operations australian forces regularly clashed with pro indonesian militia and on a number of occasions indonesian forces as well especially along the border with west timor signicant actions occurred in unk mota unk and at unk in october however with the security situation stabilised the bulk of the australian and un forces were withdrawn by two australians died from non battle related causes while a number were wounded in action the unexpected deployment to east timor in led to signicant changes in australian defence policy and to an enhancement of the adf s ability to conduct operations outside australia this successful deployment was the rst time a large australian military force had operated outside of australia since the vietnam war and revealed shortcomings in the adf s ability to mount and sustain such operations in response the defence white paper placed a greater emphasis on preparing the adf for overseas deployments the australian government committed to improve the adf s capabilities by improving the readiness and equipment of adf units in may adf personnel were again deployed to east timor as part of operation astute following unrest between elements of the timor leste defence force australian forces were involved in a number skirmishes during this time including a heavy clash with rebels commanded by alfredo reinado at same on march however by early the security situation had been stabilised and just australian personnel remained to train the local security forces as part of a small international force following a drawdown the international stabilisation force commenced withdrawing from timor leste in november a process which was completed in april in the spanish conquistador hernan cortes passed within a few kilometres of the ruins of tikal but did not mention them in his letters after spanish friar andres avendano became lost in the peten forests in early he described a ruin that may well have been tikal as is often the case with huge ancient ruins knowledge of the site was never completely lost in the region it seems that local people never forgot about tikal and they guided guatemalan expeditions to the ruins in the some or third hand accounts of tikal appeared in print starting in the century continuing through the writings of john lloyd stephens in the early century stephens and his illustrator frederick catherwood heard rumors of a lost city with white building tops towering above the jungle during their travels in the region because of the site s remoteness from modern towns however no explorers visited tikal until modesto mendez and ambrosio tut respectively the commissioner and the governor of peten visited it in artist eusebio lara accompanied them and their account was published in germany in several other expeditions came to further investigate map and photograph tikal in the century including alfred p maudslay in and the early century pioneering archaeologists started to clear map and record the ruins in the in a small airstrip was built at the ruins which previously could only be reached by several days travel through the jungle on foot or mule in the tikal project began to map the city on a scale not previously seen in the maya area from through major archaeological excavations were carried out by the university of pennsylvania tikal project they mapped much of the site and excavated and restored many of the structures excavations directed by edwin m shook and later by william coe of the university investigated the north acropolis and the central plaza from to the tikal project recorded over monuments at the site in the guatemalan government began a further archeological project at tikal which continued through to table examples of extracted summaries from the reconstruction method extracted summaries are marked in purple
