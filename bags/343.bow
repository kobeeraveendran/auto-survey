summarize outline elaborate long text generation hierarchical supervision extractive summaries xiaofei sun chun fan zijun sun yuxian meng fei jiwei computer science technology zhejiang university computer center peking university cheng laboratory shannon xiaofei sun zijun sun yuxian meng jiwei com edu edu abstract long text generation remains challenge difculty generating coherent long texts lies fact existing models ingly focus tasks local word tion high level plans generate capture high level discourse dependencies chunks texts inspired humans write list bullet points catalog rst outlined bullet point expanded form article propose soe pipelined system tha involves summarizing outlining elaborating long text generation model rst outlines summaries ent segments long texts elaborates bullet point generate ing segment avoid labor intensive cess summary soliciting propose construction strategy extracts segment summaries unsupervised manner lecting informative reconstruct segment proposed generation system comes following merits summary vides high level guidances text generation avoids local minimum individual word predictions high level discourse dependencies captured conditional dependencies summaries served summary expansion process additionally able consider signicantly contexts representing contexts concise summaries extensive experiments demonstrate soe produces long texts signicantly better quality faster convergence speed introduction despite recent large scale pretrained language models plms peters devlin liu yang clark radford brown able produce high quality passages hardly recognized humans zellers generated good texts limited length hundreds tokens cases guo bao yan generating ent long texts remains challenge radford tan difculty lies fact existing models generate texts word manner predicting subsequent token given proceeding contexts softmax objective word word strategy ingly focuses prediction local words high level plans generate results fact long texts generated current models usually repetitive generic self contradictory shen address issues coarse eration strategy proposed fan yao mao coarse generation list keywords short prompt rst generated serving mary original text prompt fed model input output complete text coarse generation strategy cantly improves generation word word strategy suffers following comings limited capacity prompt gle keyword list prompt capacity summarize texts long sages long texts usually consists eral parts focuses specic pect topic zhou narayan guan usage generation strategy limited texts summarized single prompt short stories explains text length erated progressive generation model limited introduced writingprompts dataset fan average length ries average length prompts ignorance high level discourse pendency coarse generation strategy capture discourse level dependencies jurafsky jernite handle high level information tions segments texts ignorance discourse level dependencies results texts ing coherence humans write hierarchical ner writing thousand word long essay human usually rst prepares list bullet points catalogue expands form article sentence level coherence tween bullet points preserved bullet points expanded providing guarantees text coherent mimic manner human ing paper propose soe pipelined system tha involves summarizing outlining expanding long text generation model rst outlines summaries different segments long texts actually mimics process humans outlining bullet points model elaborates bullet point generate responding segment proposed strategy comes following merits segment associated summary entire text sharing single prompt capacity summaries reconstruct text guaranteed conditional generation bility summaries captures high level discourse dependencies dependencies preserved expanded segments naturally resolves incapability modeling discourse level dependencies coarse model able generation approach consider signicantly larger contexts representing chunks contexts concise maries empirically readily summaries segments hand model needs learn summarize unsupervised manner spired gehrmann zhou liu zhong propose reconstruction strategy extracts segment summaries selecting tive reconstruct segment extensive experiments demonstrate soe produces long texts signicantly better quality existing baselines rest paper organized follows section presents related works followed tion reviewing backgrounds section troduces proposed approach detail section section respectively present experiment results ablation studies sion section related work generating long texts lines work generate long text rst line work tackles problem model perspective new model structures kitaev child dai guo sukhbaatar correia beltagy zaheer designed model ability congest texts given limited memories computing power example transformer dai modier transformers vaswani uses segment level recurrence mechanism able learning long term dependencies child correia kitaev beltagy zaheer proposed sparsify transformers focusing tion attention connections tay placed dot product self attention learned synthetic attention weights lstm predictor automatically learn attention connections adapted downstream tasks second line researches focuses oping new generation strategies efforts devoted idea planning generation coarse generation wiseman sha gehrmann wiseman moryossef puduppully hua wang shen greatly inspires work coarse generation list keywords short sentence rst generated providing ances generate text recent work tan takes multi step strategy progressively renes generated incomplete text reaching specied stage similar ideas applied text summarization gehrmann proposed method rst identies phrases ment likely included summary work inspired strategy hierarchical generation lapati liu lapata consider text units bigger granularity proposed hierarchical lstms arrange tokens sentences paragraphs cal structure different levels lstms turing compositionality shen multi level structures learn vae model generating long coherent text similar strategies applied video captioning problem exploited hierarchical rnns video caption generation extractive summarization extractive summarization refers problem selecting input text summary fundamental problem extractive summarization score constituent texts units phrases sentences paragraphs select highly ranked summary haghighi vanderwende word frequencies input text assign scores words turn score sentences higher ranked sentences selected summary input text liu presented stage extractive abstractive framework rst coarsely identies salient information followed generation model rene neural models widely scoring cao ren zhou liu lapata tuned bert devlin score sentence extractive summarization zhang computed token similarity sentence bert contextual embeddings serve automatic evaluation metric text generation background begin reviewing task text generation language modeling refers process culating probability sequence denotes constituent token probability computed decomposing joint distribution product conditional distributions tokens partial sequence tokens generated previously training model optimized minimize negative log likelihood nll log inference model decodes token time step according based softmax functions wout output word embedding trix hidden state time step ous smoothing models proposed avoid overtting pereyra sequence sequence generation models generate target sequence conditioning given source sequence differs language models lms terms conditioning input sequence similar lms probability target sequence typically factorized models optimized minimize nll log rest paper unify notation setting lms different architectures proposed model ing transformers vaswani lstms luong cnns dauphin test time sequences usually ated beam search variants promote diversity vijayakumar model details soe section describe details soe notations long sequence tokens rst sliced series snippets yis denotes number constituent snippets use bold font denote snippets normal font denote tokens ber tokens snippet parameter use superscript denote index snippet subscript denote index token consists sequence tokens denotes length goal generate subset denoted yjk given proceeding snippets denoted snippet associated short summary denotes tokens number tokens contain word expressed log word count ndw total number documents ndw total number documents containing word textrank textrank mihalcea tarau weighted graph tokens nodes similarity nodes edges use bert devlin compute similarities sentences rank based textrank algorithm reconstruction summary formative non summary sentences summary ability reconstuct text measure degree sentence reconstruction ability use model predict original given text summary tence probability regarded reconstruction score suppose sentences denotes sentence struction score denoted given follows log obtain train model input output sequentially predicting tokens given trained model rank sentences use highest score golden summary outlining segment summaries summary generation stage serve yjk goal sequentially generate sjk given generation summary factorized sequentially generating constituent word process ends generating special sequence token eos reaching specied summary length use transformer base architecture vaswani backbone figure overview proposed method given proceeding tokens rst sequentially generate summaries sjk snippet expand summary form text yjk pipeline overview instead generating constituent words adopt hierarchical strategy process generating yjk decoupled following stages outlining segment summaries quentially generate summary snippet conditioning summaries previous pets mimics process catalog generation humans write expanding summaries texts pand summary segment quentially generating constituent words overview proposed method shown figure extracting golden summaries training time need learn generate summaries straightforward golden summary snippet readily hand manually soliciting summaries like fan costly slow propose idea unsupervised tractive summarization snippet extract summary unsupervisedly use extracted golden summary learning investigate following extractive methods access importance selecting summary tences rst similar liu random comparing purposes use dom sentence summary idf sentence highest average idf score ramos golden summary word assigned score idf scales proportionally number times word appears document offset number documents corpus source target segment segment segment account contexts adopt segment level recurrence strategy similar dai hidden states computed far away snippets xed cached reused new snippet gradients propagated far away snippets memory computation efciency strategy allows model exploit information history largest extent expanding summaries texts expand summary text segment sequentially generating constituent words termination conditions summarization generation training inference training summary generation takes input model optimized minimizing nll loss log memory tion limit proceeding tokens tokens training worth noting tokens come ment right comes multiple proceeding segments summary concise takes summary expanding stage model input optimized minimizing nll loss log models mary generation summary expansion model share parameters task specic token pended start notify model generate summaries segments inference test time rst use beam search beam size generate summaries given generated summary beam search generate corresponding segment consider contexts test time limited tokens limited tokens additionally augment vanilla beam search strategy mutual information reranking fang key point mutual information instead merely handling uni directional dependency source target based forward probability log models tual dependency source target sequence sequence generation tion forward probability log backward probability log specically case summary generation generated follows arg max log log backward probability predicting proceeding summary given direct decoding infeasible follow practical solution rst generate list based forward probability rerank list combing forward ability backward probability similar strategy applied mary expanding stage obtained lows arg max log log backward probability predicts proceeding segment given current segment beam search combined reranking approximately optimal result slicing texts based coherence scores thing need care slice text segments simplest way slice text equally sub optimal break point middle closely related sentences segment contain multiple aspects propose slicing strategy based sentence level coherent scores sentence prediction nsp bert devlin able measure coherence score consecutive sentences dex denoted given text let denote number sentences denote ith sentence given xed value number sliced segments sliced segments simplify train model predict proceeding summary given current summary consists group consecutive sentences let denotes list indexes sentence original denotes index rst sentence denotes second sentence let denote number sentences wish maximize coherence scores tween consecutive sentences segment minimize score secutive sentences belonging different segments giving following objective optimize coherence score ending sentence segment starting sentence segment given readily solved linear programming experiments section present experiment results different methods generate summaries performance reconstruction consistently outperforms rest preliminary results report results reconstruction section analysis different summary generation methods ablation study section datasets need corpus contiguous long text test soe use word level datasets merity bookcorpus dataset zhu contains training words articles average length words article test ability modeling long term dependencies bookcorpus dataset suitable dataset purpose longer contiguous texts contains total number roughly billion words million sentences books average length words book average number words sentence datasets predict tokens test time baselines transformer transformers level recurrence strategy dai rally constitutes baseline model sequentially generates texts word word fashion writingprompts rst predicts list keywords single prompt generates text given prompt fan different fan golden prompts stories available readily golden prompts use extractive strategies scribed section idf method pick keyword list prompt denoted writingprompts keyword reconstruction method select highest ranking sentence prompt denoted writingprompts sentence progressive writingprompts progressive strategy proposed tan volves multiple stages prompt generation stage produces grained sequence stage comes generate prompt stage follow protocols tan use idf score obtain golden prompts stage number stages set models use adam kingma learning rate rate warmup rst steps linear decay learning rate use dropout rate layers including softmax layer evaluations use following evaluation metrics evaluate quality different generation models different perspectives perplexity ppl perplexity measures ent piece generated text dai use ppl basic evaluation metric experiments diversity perplexity measure diverse generated text use scaled ber distinct unigrams bigrams demonstrate degree diversity generated texts adversarial success inspired adversarial evaluations bowman kannan vinyals use ial success metric dened fraction model bookcorpus perplexity parameters perplexity parameters vanilla writingprompts keyword writingprompts sentence progressive writingprompts soe vanilla soe base large table perplexity different models bookcorpus vanilla stands implementation transformer dai model diversity adversarial success adversarial success level coherence nsp vanilla writingprompts keyword writingprompts sentence progressive writingprompts soe base msj table results different models terms diversity adversarial success msj sentence level coherence bookcorpus corpus vanilla stands implementation transformer dai stands distinct stands results mutual information reranking model large vanilla soe table results different models large volumes terms diversity bookcorpus dataset model successfully fooling trained ator believe machine generated texts humans evaluator binary tion model training time takes inputs machine generated texts original texts trained discriminate test time sarial success value acc acc notes accuracy trained evaluator ing machine generated texts machine generated higher values adversarial success denotes better text quality jaccard msj msj measures similarity gram frequencies generated texts golden texts montahaei report sentence level coherence ppl msj sity scores reect sentence level ence generated texts adopt strategy tan sentence prediction nsp pretrained bert model devlin metric measure ence sentence sentence report average nsp scores consecutive sentence pairs generated text results table shows results perplexity ent models bookcorpus datasets datasets soe achieves lowest ppl compared baselines transformer dai writingprompts fan progressive tan particular gain ppl decrease implemented transformer writingprompts sentence progressive having fewer parameters similar trend observed bookcorpus table table results msj diversity adversarial success level coherence scores seen writingprompt based models generally outperform transformer model adopts word generation strategy validates superiority step generation strategy naive word word generation strategy long text generation progressive prompt model involves multi step eration expanding outperforms step writingprompt keyword sentence model accord pectation soe achieves signicantly better results compared vanilla writingprompts sive models terms evaluation metrics ing proposed method produce uent coherent diverse texts consistent performance boosts metrics demonstrate importance modeling discourse level cies necessity summary expanding strategy long text generation additionally enhanced mutual information observe additional performance boosts pecially diversity adversarial success accord expectation mutual information able build bidirectional dencies source target models enhanced mutual information generate better summaries phenomenon generic repetitive generation alleviated leading diverse results ablation studies effect segment length size segment big small extremely long segments tain aspects topics summary summarize case model erate writingprompts model fan short segments summary provide high level guidance need sweet spot segment length figure shows results bookcorpus dataset clear gure short segments long segments lead inferior performances effect summary generation strategies worthwhile explore different summary extraction methods affect nal performances end conduct experiments corpus dataset different summary extraction methods random textrank idf construction table shows results rst compare ppl summary generation reconstruction model achieves lowest ppl figure ppl bookcorpus dataset ent segment lengths method summary ppl text vanilla random textrank idf reconstruction table performances different summary extracion methods described section vanilla plain model generates tokens maries produces summaries easiest predict given proceeding contexts interesting summary tion strategies ppl summarization generation signicantly larger text prediction reasonable summaries upcoming segment requires generalization abilities diverse options segment talk local choices sentence talk nal text generation ppl struction achieves best results terms ppl textrank idf better vanilla interestingly strategy random sentences summaries performs worse summaries explained providing guidances better incorrect guidances effect coherence based text slicing replace coherence based text slicing egy naive equal slicing strategy negatively affect performance bookcorpus dataset observe increase summary generation ppl idfreconstruction increase ppl ken generation demonstrates importance slicing text coherent segments tion worth noting native equal slicing strategy soe performs signicantly better baseline models decoupling effects summaries positive effects summaries fold provides high level guidances segment generation far away segments concisely represented summaries gives model ability consider longer contexts quantitatively measure inuences aspects conduct following experiments test time computation model access maries segments contexts words summaries kens proceeding contexts fed inputs different original version soe extend proceeding contexts limit tokens reached train model add limitation test time bookcorpus dataset leads crease ppl decrease simplifying explore different simplications current mary generated based previous maries segment tokens simplify previous segment tokens fed inputs predict summary signicantly decreases computing complexity bookcorpus dataset observe increase ppl summary generation subsequently leads increase ppl token generation convergence speed figure convergence speed different models model converges faster vanilla high level guidance prompts conclusion paper propose step hierarchical generation strategy long text generation model rst generates summary ment conditioning previous summaries summary expanded form text segment proposed strategy provides level guidances local text generation ables high level discourse dependencies tured extensive experiments demonstrate soe produces long texts signicantly better quality references hangbo bao dong furu wei wenhui wang nan yang xiaodong liu wang songhao piao feng gao ming zhou hsiao wuen hon pseudo masked language models language model pre training beltagy matthew peters arman cohan longformer long document transformer samuel bowman luke vilnis oriol vinyals drew dai rafal jozefowicz samy gio generating sentences continuous space investigate quickly different els converge results shown figure guidance extracted summaries soe conspicuously faster convergence speed training steps approximately reached best result models vanilla writingprompts converge training steps writingprompts tom brown benjamin mann nick ryder melanie subbiah jared kaplan prafulla dhariwal arvind neelakantan pranav shyam girish sastry amanda askell language models shot learners arxiv preprint ziqiang cao furu wei dong sujian ming zhou ranking recursive neural works application multi document rization aaai pages citeseer keywordours rewon child scott gray alec radford generating long ilya sutskever quences sparse transformers arxiv preprint kevin clark minh thang luong quoc christopher manning electra pre training text encoders discriminators tors arxiv preprint goncalo correia vlad niculae andre martins adaptively sparse transformers proceedings conference empirical methods natural language processing international joint conference natural guage processing emnlp ijcnlp pages hong kong china association tional linguistics zihang dai zhilin yang yiming yang jaime bonell quoc ruslan salakhutdinov transformer attentive language models proceedings xed length context annual meeting association tional linguistics pages florence italy association computational linguistics yann dauphin angela fan michael auli david grangier language modeling gated volutional networks international conference machine learning pages jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language ing arxiv preprint angela fan mike lewis yann dauphin erarchical neural story generation arxiv preprint hao fang saurabh gupta forrest iandola rupesh srivastava deng piotr dollar jianfeng gao aodong margaret mitchell john platt captions visual concepts proceedings ieee conference computer vision pattern recognition pages yao yansong feng john cunningham paraphrase generation latent bag words sebastian gehrmann falcon dai henry elder alexander rush end end content plan selection data text generation ceedings international conference natural language generation pages tilburg university netherlands association putational linguistics sebastian gehrmann yuntian deng alexander rush abstractive summarization arxiv preprint aaai conference articial intelligence ume pages jiaxian guo sidi han cai weinan zhang yong jun wang long text generation adversarial training leaked information arxiv preprint qipeng guo xipeng qiu pengfei liu yunfan shao xiangyang xue zheng zhang transformer proceedings conference north american chapter association computational linguistics human language technologies volume long short papers pages minneapolis minnesota ation computational linguistics aria haghighi lucy vanderwende ing content models multi document tion proceedings human language gies annual conference north american chapter association tional linguistics pages xinyu hua wang sentence level tent planning style specication neural text generation yacine jernite samuel bowman david tag discourse based objectives fast supervised sentence representation learning arxiv preprint anjuli kannan oriol vinyals ial evaluation dialogue models arxiv preprint diederik kingma jimmy adam method stochastic optimization arxiv preprint nikita kitaev ukasz kaiser anselm levskaya reformer efcient transformer arxiv preprint chunyuan xiang gao yuan xiujun baolin peng yizhe zhang jianfeng gao mus organizing sentences pre trained modeling latent space arxiv preprint jiwei michel galley chris brockett jianfeng gao bill dolan diversity promoting tive function neural conversation models arxiv preprint jiwei michel galley chris brockett jianfeng gao bill dolan diversity promoting jective function neural conversation models proceedings conference north american chapter association tional linguistics human language technologies pages san diego california association computational linguistics jian guan yansen wang minlie huang story ending generation incremental encoding proceedings commonsense knowledge jiwei dan jurafsky neural net models open domain discourse coherence arxiv preprint jiwei minh thang luong dan jurafsky hierarchical neural autoencoder paragraphs documents arxiv preprint jiwei monroe tianlin shi sebastien jean alan ritter dan jurafsky ial learning neural dialogue generation arxiv preprint xiaoya yuxian meng qinghong han fei jiwei sac accelerating structuring self attention sparse adaptive connection arxiv preprint peter liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser noam shazeer generating wikipedia arxiv preprint summarizing long sequences yang liu mirella lapata hierarchical transformers multi document summarization proceedings association computational linguistics pages florence italy association tational linguistics annual meeting yang liu mirella lapata text tion pretrained encoders yinhan liu myle ott naman goyal jingfei dar joshi danqi chen omer levy mike lewis luke zettlemoyer veselin stoyanov roberta robustly optimized bert pretraining proach arxiv preprint minh thang luong hieu pham christopher manning effective approaches based neural machine translation arxiv preprint huanru henry mao bodhisattwa prasad majumder lian mcauley garrison cottrell proving neural story generation targeted common sense grounding arxiv preprint stephen merity caiming xiong james bradbury richard socher pointer sentinel mixture els arxiv preprint rada mihalcea paul tarau textrank ing order text proceedings ference empirical methods natural language processing pages ehsan montahaei danial alihosseini mahdieh leymani baghshah jointly measuring sity quality text generation models arxiv preprint amit moryossef yoav goldberg ido dagan step step separating planning realization neural data text generation ramesh nallapati bowen zhou caglar gulcehre bing xiang abstractive text tion sequence sequence rnns arxiv preprint shashi narayan shay cohen mirella ata details summary topic aware convolutional neural works extreme summarization arxiv preprint gabriel pereyra george tucker jan chorowski ukasz kaiser geoffrey hinton izing neural networks penalizing condent output distributions arxiv preprint matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word proceedings resentations ence north american chapter ation computational linguistics human guage technologies volume long papers pages new orleans louisiana association computational linguistics ratish puduppully dong mirella lapata data text generation content selection planning alec radford jeffrey rewon child david luan dario amodei ilya sutskever language models unsupervised multitask learners openai blog ramos idf determine word vance document queries pengjie ren zhumin chen zhaochun ren furu wei jun maarten rijke leveraging contextual sentence relations extractive rization neural attention model ings international acm sigir ence research development information retrieval pages lei sha lili mou tianyu liu pascal poupart sujian baobao chang zhifang sui planning neural text generation structured data arxiv preprint dinghan shen asli celikyilmaz yizhe zhang liqun chen xin wang jianfeng gao lawrence carin generating long coherent text multi level latent variable models arxiv preprint tianxiao shen victor quach regina barzilay tommi jaakkola blank language models arxiv preprint sainbayar sukhbaatar edouard grave piotr janowski armand joulin adaptive tention span transformers proceedings annual meeting association tational linguistics pages florence italy association computational linguistics bowen tan zichao yang maruan shedivat eric xing zhiting progressive generation long text hierarchical recurrent neural networks ings ieee conference computer vision pattern recognition pages manzil zaheer guru guruganesh avinava dubey joshua ainslie chris alberti santiago ontanon philip pham anirudh ravula qifan wang yang big bird transformers longer quences arxiv preprint rowan zellers ari holtzman hannah rashkin yonatan bisk ali farhadi franziska roesner yejin choi defending neural fake news advances neural information ing systems pages tianyi zhang varsha kishore felix kilian weinberger yoav artzi bertscore arxiv preprint uating text generation bert ming zhong pengfei liu yiran chen danqing wang xipeng qiu xuanjing huang tive summarization text matching arxiv preprint deyu zhou linsen guo yulan ral storyline extraction model storyline tion news articles proceedings conference north american chapter association computational linguistics human language technologies volume long papers pages new orleans louisiana ation computational linguistics qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural ment summarization jointly learning score select sentences arxiv preprint yukun zhu ryan kiros richard zemel ruslan salakhutdinov raquel urtasun antonio torralba sanja fidler aligning books movies story like visual explanations watching movies reading books tay dara bahri donald metzler cheng juan zhe zhao che zheng synthesizer thinking self attention transformer models arxiv preprint ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need guyon luxburg bengio wallach fergus vishwanathan nett editors advances neural information cessing systems pages curran ciates inc ashwin vijayakumar michael cogswell prasath selvaraju qing sun stefan lee david crandall dhruv batra diverse beam search decoding diverse solutions neural quence models arxiv preprint sam wiseman stuart shieber alexander rush challenges data document ation sam wiseman stuart shieber alexander rush learning neural templates text eration ziang xie sida wang jiwei daniel levy aiming nie dan jurafsky andrew data noising smoothing neural network language models arxiv preprint jingjing xuancheng ren zhang zeng xiaoyan cai sun based model promoting coherence tences narrative story generation arxiv preprint yan weizhen yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang ming zhou prophetnet predicting future gram sequence sequence pre training zhilin yang zihang dai yiming yang jaime bonell russ salakhutdinov quoc xlnet generalized autoregressive pretraining language understanding advances neural formation processing systems pages lili yao nanyun peng ralph weischedel kevin knight dongyan zhao rui yan write better automatic storytelling zihao qipeng guo quan gan xipeng qiu zheng zhang transformer modelling long range context binary partitioning arxiv preprint haonan jiang wang zhiheng huang yang wei video paragraph captioning career ayola outspoken subject racial discrimination tainment industry describing motivation states overtly political person want fairness ayola believes black actors receive recognition white counterparts explaining youu stars black likely interviews white actors sold reason watch shh believes career taken different direction ethnic origin stating played roles playee white aware glass ceiling low expectations low noted having casting directors accept notion characters black welsh particular problem explaining offered lot different roles hey welsh time asked play welsh character screen tiger bay bbc wales know series called radyy park cyncoed close wouldn ayola founded production company directed short entitled persephone playground presented cannes festival campaign increased black representation theatre lms television project largely unsuccessful ayola stating decide don want produce lms rubbish bad budget said yes worry pay things end ayola offered support action southern africa campaign dignity period aiming provide affordable sanitary protection zimbabwean women hand command military operations interfet untaet completed february australia continued support peacekeeping operation personnel landing craft blackhawk helicopters remained largest contributor personnel peacekeeping mission operations australian forces regularly clashed pro indonesian militia number occasions indonesian forces especially border west timor signicant actions occurred unk mota unk unk october security situation stabilised bulk australian forces withdrawn australians died non battle related causes number wounded action unexpected deployment east timor led signicant changes australian defence policy enhancement adf ability conduct operations outside australia successful deployment rst time large australian military force operated outside australia vietnam war revealed shortcomings adf ability mount sustain operations response defence white paper placed greater emphasis preparing adf overseas deployments australian government committed improve adf capabilities improving readiness equipment adf units adf personnel deployed east timor operation astute following unrest elements timor leste defence force australian forces involved number skirmishes time including heavy clash rebels commanded alfredo reinado march early security situation stabilised australian personnel remained train local security forces small international force following drawdown international stabilisation force commenced withdrawing timor leste november process completed april spanish conquistador hernan cortes passed kilometres ruins tikal mention letters spanish friar andres avendano lost peten forests early described ruin tikal case huge ancient ruins knowledge site completely lost region local people forgot tikal guided guatemalan expeditions ruins hand accounts tikal appeared print starting century continuing writings john lloyd stephens early century stephens illustrator frederick catherwood heard rumors lost city white building tops towering jungle travels region site remoteness modern towns explorers visited tikal modesto mendez ambrosio tut respectively commissioner governor peten visited artist eusebio lara accompanied account published germany expeditions came investigate map photograph tikal century including alfred maudslay early century pioneering archaeologists started clear map record ruins small airstrip built ruins previously reached days travel jungle foot mule tikal project began map city scale previously seen maya area major archaeological excavations carried university pennsylvania tikal project mapped site excavated restored structures excavations directed edwin shook later william coe university investigated north acropolis central plaza tikal project recorded monuments site guatemalan government began archeological project tikal continued table examples extracted summaries reconstruction method extracted summaries marked purple
