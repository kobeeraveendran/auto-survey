concept pointer network abstractive summarization wang gao huang zhou school computer science technology beijing institute technology beijing china beijing engineering research center high volume language information processing cloud computing applications edu cn t c o l c s c v v x r abstract quality abstractive summary copy salient source texts summaries tend generate new conceptual words express concrete details inspired popular pointer generator sequence sequence model paper presents concept pointer network improving aspects abstractive summarization network ages knowledge based context aware tualizations derive extended set date concepts model points appropriate choice cept set original source text joint proach generates abstractive summaries higher level semantic concepts ing model optimized way adapts different data based novel method distantly supervised learning guided reference summaries testing set overall proposed approach provides tistically signicant improvements eral state art models gigaword datasets human ation model s abstractive abilities supports quality summaries duced framework introduction abstractive summarization abs gained overwhelming success owing tremendous development sequence sequence model variants rush et al chopra et al paulus et al guo et al gao et al tandem els pointer generator developed et al solution tackle rare words vocabulary oov problem associated generative based models idea use attention pointer determine ability generating word corresponding author figure copies keyword source text generates new concepts convey meaning ulary distribution source text pointer generator networks extensively cepted abs community cacy long document summaries chen bansal hsu et al title tion sun et al current power abstractive marization falls short potential ample figure shows model pointer mechanism marked direct pointer likely merely copy parts original text form summary keywords phrases athletes conversely like summary based s derstanding detail words expressed higher level concepts drawn world edge like word group replace athletes ofcials indicates good summary simply copy original rial generate new abstract concepts reect high level semantics pointer generator network solely considers source material generate summary adequately satisfy needs high quality abstractive summarization argue concepts greater ability press deeper meanings verbatim words essential explore potential ing concepts world knowledge assist abstractive summarization developed model points informative source texts leverages conceptual words human edge summaries generates paper propose novel model based concept pointer generator ages generation conceptual abstract words hidden benet model viates oov problems model uses pointer network capture salient information source text employs pointer generalize detailed words according upper level expressions finally output consistent language model generator unique concept pointer set concept candidates particular word drawn huge knowledge base set candidates adheres concept distribution probability concept erated linked strongly candidate resents word concept bution iteratively updated better explain target word given context source rial inherent semantics texts learned concept pointer points suitable expressive concepts words tion function adaptive cater different datasets distantly supervised training network optimized end end forcement learning distant supervision strategy complement improve summary overall contributions paper novel concept pointer generator network leverages context aware conceptualization concept pointer jointly tegrated generator deliver tive abstract oriented summaries novel distant supervision training strategy favors model adaptation generalization sults performance outperforms accepted evaluation based reinforcement learning optimization test dataset terms rouge metrics statistical analysis titative results human evaluations parative experiments state art models shows proposed method provides promising performance related work abstractive summarization supposedly digests understands source content consequently generated summaries typically zation wording form new sentences historically abstractive tion performed rule based tence selection dorr et al key tion extraction genest lapalme tactic parsing bing et al recently models attention played dominant role generating abstractive summaries rush et al chopra et al nallapati et al zhou et al extensions approach include intra decoder attention paulus et al coverage vectors et al decrease etition phrasing copy mechanism gu et al integrated models tackle oov problem zhou et al went propose copies complete sequences input sentence tain readability generated summary pointer mechanism vinyals et al drawn attention text summarization et al technique vides potential solution rare words oov extends abstractive summarization exible way c elikyilmaz et al pointer generator models effectively adaptive extractor abstractor networks chen bansal summaries erated incorporating pointer generator multiple relevant tasks guo et al question entailment generation multiple source texts sun et al work particularly targets problem abstraction rare abstract meaning resentation amr transform sentence concept graph merge similar concept nodes form new summary graph liu et al concepts incorporated auxiliary features guo et al kryscinski et al weber et al dene number new n grams primary criteria abstractiveness makes sense cases believe abstraction means ing detailed content higher level semantically related concepts motivated opment model proposed paper proposed model concept pointer generator neural abstractive summarization scribed generation process tial input summarized shorter sequential output neural network suppose sequential input xi xn quence n number words index input shorter e summarized sequence output denoted y yt ym number m words t indicates time step figure shows model consists sub modules encoder decoder module proposed concept pointer generator ule encoder decoder framework process formulated encoder decoder framework consists encoder attention equipped decoder use layer bi directional lstm rnn encoder layer uni directional lstm rnn decoder attention mechanism h h h n formally encoder produces sequential den states h n corresponding positions bi directional hi flst m word xi sequence represented concatenation e hi bi directional hidden states h decoder generates target h mary vocabulary distribution based context vector h t following process p t x h st hidden state decoder time step t h t context vector time step t w w trainable parameters sfm short softmax function context vector h t computed weighted sum hidden representations source text weight denoted tion t h t t ihi t hhi w sst softmax function normalizes vector distribution input position v w h w s trainable parameters pointer networks use attention pointer lect segments input outputs vinyals et al pointer network suitable mechanism extracting salient tion remaining exible interface model generating abstractive summarization et al proposed model essentially upgrade tion integrates new concept pointer network unied framework context aware conceptualization understanding instances word requires taxonomic knowledge base relates words concept space model use isa taxonomy called microsoft concept wang et al serve purpose reasons wang wang graph provides huge concept space multi word terms cover concepts worldly facts concepts instances relationships second relationships concepts entities probabilistic measure strongly related bilities trustworthy given rived evidence found billions webpages search log data existing taxonomies model data driven easily adaptable probabilities acteristics microsoft concept graph suitable choice model detailed amples available appendix concept graph species probability instance belongs concept c given word distribution set related concepts raises question identify context appropriate concept word distributional set candidate cepts instance apple context ple good health tends associated concept fruit instead company mally given word xi training sentence set k concept candidates ci linked word knowledge base distributional probabilities concepts e ck microsoft concept graph derived probase project public data downloaded provided api research microsoft com home api current version mined billions web pages containing m unique concepts m unique entities m isa relations figure architecture model blue bar represents attention distribution inputs purple bar represents concept distribution inputs noted distribution sparse word upper concept green bar represents vocabulary distribution generated component p nd suitable concept updated context represented vector h equation time step t task t t case generating summaries given dated contexts weighted update tional concept candidates needs performed model updated weight denoted j estimated softmax classier jointly conditioned hidden representation word hi context vectors h t cept vectors j h t j w trainable parameter cj vector jth concept candidate representation input embeddings concept probability existing knowledge base updated weights based contexts j context aware conceptualized probability jth concept word xi p c j nally estimated j j p c ck tunable parameter theoretically end number k relevant concepts word ci bility distribution set learned j p p c p c concept pointer generator basic pointer generator network contains sub modules pointer network p c k generation network modules jointly determine probabilities words nal generated summary ation probability pgen generation network et al learned pgen hh t w sst w bgen sigmoid function pointer network model consists pointer source text concept pointer relevant concepts arisen source content separate ers calculated follows rst pointer taken based attention distribution t source text second concept pointer erated concept distribution source text scaled element wise attention bution train model given likelihood concept current context updates performed ways hard assignment concept receives highest score selected update argmax p c c arg max j j index maximized generated weight based contexts p obtained eqs random selection concept didates trained randomly update parameters random p c c j p c j represents selected concept index considering baseline generation work pointer networks nal distribution pgen t c t wi w wi w updated c arg max c difference random choices demonstrated experiments tion objective learning basic mle baseline objective derived ing likelihood training eration given reference summary y y document ing objective minimize negative likelihood target word sequence y y lm le log p y t y evaluation based reinforcement learning rl similar paulus et al policy gradient methods directly optimize discrete target uation metrics rouge basic idea explore new sequences compare best greedy baseline sequence baseline sequence y sampled sequence ys ated compared reference quence y compute rewards respectively rl training stage arate output candidates time step duced ys sampled probability tion p ys y baseline output training objective minimize ys ys log p ys lrl noteworthy samples ys selected wide range vocabularies extended concept candidates strategy ensures model learns generate sequences higher rewards better exploring set close concepts combination objectives yield improved task specic scores ing better language model lf inal lrl le soft switch objectives model pre trained mle loss switch nal loss distant supervision ds model adaption intuition summary document pairs dissimilar testing set model retrained adapt weaken inuence dissimilarity nal loss result training model better ts cic testing data challenge explicit supervision labels indicate training set close testing set new training paradigm needed answer need provide end end ity model developed simple approach labeling summary document pairs ing kullback leibler kl divergence training reference summary set ing documents way training pairs distantly labelled training model specically representations ence summaries testing set computed summing involved word embeddings given testing document xd l n d n d size testing corpus based representation document xd l number ment words involved reference summary represented y t malize vectors softmax function cater kl calculation model adaption distant labels dened y xd xd lds n d l lmle dkl indicates kl divergence tween y xd l constant ter tuned adaption testing set divergences averaged testing set indicates overall distances testing set reference document pairs way samples training corpus distantly annotated relevant irrelevant model adaption noting model pre trained mle loss switching distantly supervised training experiments datasets evaluate effectiveness proposed model conducted training ing popular datasets rst glish gigaword fifth edition corpus parker et al replicated pre processing steps rush et al obtain ing testing data pre processing corpus contained m sentence summary pairs training set k pairs development set pairs titles removed testing set numbered pairs second dataset testing dataset consists document headline mary pairs document paired reference summaries written humans evaluation metrics rouge lin evaluation metric measures quality summary computing lapping lexical elements candidate summary reference summary following previous practice assessed unigram bigram rg l longest common quence lcs noted english testing set contains references different lengths testing set xes mary length bytes training setups initialize word dings d vectors ne tune ing training concepts share embeddings words vocabulary size set source target text den state size set vocabulary size increased cepts w t different number k concept candidates word note generated concepts unks subsequently deleted code available github com wprojectsn codes vocabularies candidate concepts cluded trained models single gtx tan gpu machine adagrad mizer batch size minimize loss initial learning rate accumulator value set respectively gradient clipping maximum gradient norm time decoding summaries produced beam search size hyper parameter settings word trained concept pointer generator iterations yielded best performance took optimization rl rewards rg l k iterations k iterations gigaword took supervised training k iterations k iterations gigaword baselines following state art lines comparators rush et al tuned abs model tional features luong nmt luong et al layer lstm encoder decoder elman chopra et al convolution coder elman rnn decoder tion layer bilstm encoder layer lstm decoder equipped lsent nallapati et al uses tention temporal attention track past tive weights decoder restrains etition later sequences seass zhou et al includes additional selective gate trol information ow encoder coder pointer generator et al integrated pointer network model implemented baseline coverage mechanism focus baseline models include pointer generator based extensions guo et al li et al cgu lin et al sets convolutional gated unit self attention global encoding results analysis following analysis focuses investigating model rst able generate stract new concepts second overall quality performs baselines quantitative analysis results presented table observe model outperformed strong art models datasets metrics terms gigaword pointer generator performance improvements concept pointer statistically nicant p metrics rouge evaluation option rouge evaluation option oov summary length oov major challenge current abstractive table rouge evaluation results gigaword rouge recall test set results mark taken corresponding papers underlined scores best additional optimization bold scores best optimization strategies mark indicates improvements baselines concept pointer statistically signicant tailed t test p models rush et al luong nmt luong et al ras elman chopra et al lsent nallapati et al seass zhou et al impl pointer generator impl et al pointer cov guo et al sel eram li et al cgu lin et al concept pointer concept concept gigaword rg l rg l table oov problem analysis percentages unk generated words generating unk w t following models gigaword datasets method pointer generator concept pointer gigaword table abstractiveness percentage novel n grams gigaword dataset models pointer generator concept pointer reference summary novel n gram gram gram gram rization models generating longer maries unks focus model showed improvements regard table counted number unks generated summary words measured portions testing sets oov ages dropped gigaword demonstrates model effective viating oov problems result supports superior concept pointer line pointer generator statistics found summaries generated cept pointer averaged words pointer generator summaries averaged words summary shows concept pointer able capture salient content generating relatively longer summaries abstractiveness according bansal abstractiveness scores puted percentage novel n grams chen generated summaries included source documents shown table compared human written summaries receive highest novelty terms abstractiveness concept pointer generator achieves closest performance human written summaries result demonstrates advantage model producing new abstract concepts model designed improve semantic relevance promote higher abstraction generated summary examples found appendix b baseline analysis training strategies evaluate relative impact training strategy model tested different nations comparison baselines context aware conceptualization gate impact training ber concepts k concept update egy mentioned eqs chose table human evaluation scoring models terms abstraction overall quality human uators higher better score range indicates improvements baselines concept pointer statistically signicant method pointer generator concept pointer abstraction overall quality noticeable effect testing set stantially semantically different training set provides improvement rl close analysis conclude ds strategy better model adaption abstractive summarization human evaluations explore correctness model man judgment conducted manual tion post graduate volunteers ily following criteria assess ated summaries abstraction e abstract concepts contained summary appropriate overall quality e summary readable informative relevant conduct uation randomly selected examples duc testing set asked teers subjectively assess summaries example consisted article maries e summary model pointer generator model proposed cept pointer model volunteers chose best summaries articles according criteria multiple choices viously summaries randomly shufed model produce unknown prevent bias scores model ranked times volunteers chose summary w t criteria averaged ber participants results presented table model outperformed model pointer generator et al criteria step manually inspect maries generated model ples presented appendix b found summaries abstract written summary likely ing tendency model copy segments source text rearrange phrases gigaword c random c random gigaword c arg max c arg max figure rouge metrics gigaword w t different number concept dates updates conducted hard assignment arg max random selection c c random ferent number concept candidates e k context aware ization update strategy performance fully evaluated rouge metrics shown figure results vary slightly ing number concepts random lection strategy eq shown figure indicates random strategy sensitive number extracted ics concept pointer able point correct cepts multiple candidates figure optimum settings clearly parent e k gigaword k overall hard assignment strategy eq provided best performance practical terms random selection eq performs stably different settings training ds vs rl shown ble model distant supervision strategy concept reinforcement learning concept rior basic concept pointer generator datasets relative improvement concept concept ranged inferior concept gigaword comparing results clear ds training l summary overall approach produce high level concepts correct lations compared baselines strates solution promising research rection pursue additionally ated summaries long uent informative conclusion paper presents novel concept pointer ator model improve abstractive tion model generate concept oriented maries propose novel distant vision strategy model adaption different datasets empirical subjective ments model makes statistically signicant quality improvement state art baselines popular datasets acknowledgement work supported national ral science foundation china partially supported research foundation beijing municipal science nology commission partially supported ministry tion china mobile research foundation references lidong bing piji li yi liao wai lam weiwei guo rebecca j passonneau abstractive multi document summarization phrase selection proceedings annual merging meeting association computational guistics international joint conference natural language processing asian ation natural language processing acl volume long papers pages asli c elikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents proceedings abstractive summarization conference north american chapter association computational linguistics man language technologies naacl hlt volume long papers pages yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting arxiv preprint sumit chopra michael auli alexander m rush abstractive sentence summarization tentive recurrent neural networks conference north american chapter association computational linguistics human language nologies pages bonnie dorr david zajic richard schwartz hedge trimmer parse trim approach headline generation proceedings naacl text summarization workshop volume pages association computational guistics yang gao yang wang luyang liu yidi guo heyan huang neural abstractive rization fusing global generative topics neural computing applications pierre etienne genest guy lapalme framework abstractive summarization text text generation workshop monolingual text text generation pages proceedings jiatao gu zhengdong lu hang li victor ok incorporating copying mechanism li proceedings sequence sequence learning annual meeting association putational linguistics volume long papers volume pages han guo ramakanth pasunuru mohit bansal soft layer specic multi task summarization entailment question generation ceedings annual meeting tion computational linguistics acl ume long papers pages yidi guo heyan huang yang gao chi lu conceptual multi layer neural network model headline generation chinese computational guistics natural language processing based naturally annotated big data pages springer wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun ed model extractive abstractive arxiv preprint rization inconsistency loss wojciech kryscinski romain paulus caiming xiong richard socher improving abstraction text summarization proceedings conference empirical methods natural guage processing pages haoran li junnan zhu jiajun zhang chengqing zong ensure correctness incorporate entailment knowledge mary stractive sentence summarization proceedings international conference computational linguistics coling pages chin yew lin rouge package matic evaluation summaries text summarization branches acm international conference tion knowledge management pages acm noah weber leena shekhar niranjan nian kyunghyun cho controlling coding abstractive summaries based networks arxiv preprint qingyu zhou nan yang furu wei ming zhou selective encoding abstractive sentence summarization proceedings annual meeting association computational guistics volume long papers pages qingyu zhou nan yang furu wei ming zhou sequential copying networks proceedings thirty second aaai conference articial intelligence pages junyang lin xu sun shuming ma qi su global encoding abstractive summarization proceedings annual meeting sociation computational linguistics acl volume short papers pages fei liu jeffrey flanigan sam thomson norman sadeh noah smith tive summarization semantic representations arxiv preprint thang luong hieu pham christopher d ning effective approaches attention based proceedings neural machine translation conference empirical methods natural language processing emnlp pages ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre bing xiang abstractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning conll pages robert parker david graff junbo kong ke chen kazuaki maeda english gigaword fth tion dvd philadelphia linguistic data consortium romain paulus caiming xiong richard socher deep reinforced model abstractive marization arxiv preprint alexander m rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing emnlp pages abigail peter j liu christopher d ning point summarization proceedings pointer generator networks annual meeting association putational linguistics acl volume long papers pages fei sun peng jiang hanxiao sun changhua pei wenwu ou xiaobo wang multi source pointer network product title summarization proceedings acm international ence information knowledge management pages acm oriol vinyals meire fortunato navdeep jaitly pointer networks advances neural formation processing systems pages zhongyuan wang haixun wang association standing short texts putational linguistics acl tutorial zhongyuan wang haixun wang ji rong wen yanghua xiao inference approach proceedings sic level categorization
