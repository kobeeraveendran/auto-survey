text segmentation cross segment attention michal lukasik boris dadachev goncalo simoes kishore papineni google research mlukasik bdadachev gsimoes com abstract document discourse segmentation fundamental nlp tasks pertaining breaking text constituents commonly help downstream tasks mation retrieval text summarization work propose transformer based chitectures provide comprehensive parisons previously proposed approaches standard datasets establish new state art reducing particular ror rates large margin cases analyze model sizes build models fewer parameters keeping good performance tating real world applications introduction text segmentation traditional nlp task breaks text constituents according ned requirements applied documents case objective create logically coherent sub document units units ments structure interest paragraphs sections task referred document segmentation simply text segmentation figure ample document segmentation wikipedia task typically evaluated koshorek badjatiya documents multi modal cover multiple aspects topics breaking ument uni modal segments help improve speed stream applications example document segmentation shown improve information retrieval indexing document units instead documents llopis shtekh applications summarization information extraction benet text segmentation koshorek early life marriage franklin delano roosevelt born january hudson valley town hyde park new york businessman james roosevelt second wife sara ann delano aides began refer time president friend gossip linking romantically appeared newspapers legacy roosevelt widely considered important gures history united states inuential gures century roosevelt appeared postage stamps figure illustration text segmentation ample wikipedia page president roosevelt aim document segmentation breaking raw text sequence logically coherent sections early life marriage legacy ple related task called discourse segmentation breaks pieces text sub sentence elements called elementary discourse units edus edus minimal units discourse analysis ing rhetorical structure theory mann thompson figure examples edu segmentations sentences example sentence annuities rarely good idea age withdrawal restrictions poses following edus annuities rarely good idea age withdrawal restrictions rst ment second justication discourse analysis addition key step discourse analysis joty discourse segmentation shown improve number downstream tasks text summarization helping identify grained sub sentence units different levels importance creating summary multiple neural approaches recently proposed document discourse tion koshorek proposed use sentence annuities rarely good idea age withdrawal restrictions sentence wanted investment simple secure certicate deposit offers return worth getting excited figure example discourse segmentations rst dataset carlson tations edus separated character hierarchical lstms document tion simultaneously introduced attention based model document mentation discourse segmentation wang obtained state art results course segmentation pretrained contextual embeddings peters new scale dataset document segmentation based wikipedia introduced koshorek providing realistic setup evaluation previously small scale synthetic datasets choi dataset choi approaches evaluated ferent datasets compared furthermore rely rnns instead recent transformers vaswani cases use contextual embeddings shown help classical nlp tasks devlin work aim addressing tions bring following contributions compare recent approaches posed independently text discourse segmentation koshorek wang public datasets introduce new model architectures based transformers bert style textual embeddings document course segmentation tasks analyze strengths weaknesses architecture establish new state art simple paradigm argued earliest text segmentation rithms achieve competitive performance current neural era conduct ablation studies analyzing portance context size model size literature review document segmentation early research efforts focused unsupervised text tation quantifying lexical cohesion small text segments hearst choi hard precisely dene tify lexical cohesion approximated counting word repetitions tationally expensive unsupervised bayesian proaches popular utiyama hara eisenstein mota unsupervised algorithms suffer main drawbacks hard specialize given domain cases naturally deal multi scale issues desired segmentation granularity paragraph section ter necessarily task dependent vised learning provides way addressing property supervised algorithms focus recent works particular multiple neural approaches proposed task sequence ing algorithm proposed sentence encoded lstm tokens lstm sentence encodings label sentence ending segment koshorek authors consider large dataset based wikipedia report improvements supervised text segmentation methods work sequence sequence model proposed input encoded bigru segment endings generated pointer network vinyals authors report signicant improvements sequence beling approaches dataset composed articial documents created ing segments random articles brown corpus choi lastly badjatiya consider attention based cnn lstm model evaluate small scale datasets discourse segmentation contrary document segmentation discourse segmentation ically framed supervised learning task challenge applying supervised proaches type segmentation fact available dataset task limited carlson reason approaches discourse segmentation usually rely nal annotations resources help models generalize early approaches discourse tation based features linguistic tations pos tags parsing trees soricut marcu xuan bach joty performance systems highly dependent quality tions recent approaches started rely end end neural network models need linguistic annotations obtain high quality results relying instead pretrained models obtain word sentence representations example work proposes sequence model getting sequence glove pennington word embeddings input generating edu breaks approach utilizes elmo pretrained embeddings lstm architecture achieves state art results task wang architectures propose model architectures tation uses local context candidate break leverage context input candidate break mean potential segment boundary models rely preprocessing technique simply feed raw input word piece sub word tokenizer use word piece tokenizer implementation open sourced bert release devlin precisely english uncased variant vocabulary size word pieces cross segment bert rst model represent candidate break left right local contexts quences word piece tokens come respectively candidate break main motivation model simplicity local contexts sub optimal longer distance linguistic artifacts likely help locating breaks simple model departure recent trends favoring chical models conceptually appealing model documents interesting note local context common proach earlier text segmentation models hearst studying semantic shift comparing word distributions candidate break figure illustrate model input composed cls token followed contexts concatenated separated sep token necessary short contexts padded left right tokens sep special tokens duced bert devlin stand respectively classication token typically classication tasks representation entire input sequence separator token padding token input fed encoder vaswani tialized publicly available bertlarge model bertlarge model layers uses dimensional embeddings tion heads model tuned task released bert checkpoint supports quences tokens word pieces study effect length contexts denote context conguration ber word piece tokens sep token lstm second proposed model illustrated ure starts encoding sentence bertlarge independently tensors produced sentence fed lstm responsible capturing representation sequence sentences indenite size encoding sentence bert sequences start cls token mentation decision sentence level document segmentation use cls token input lstm cases segmentation decision word level discourse segmentation obtain bert sequence output use left piece word input lstm note context short discourse mentation task fully encoded single pass bert alternatively encode word independently considering words consist single word piece encoding deep transformer encoder somewhat wasteful computing resources model reduce bert inputs maximum sentence size tokens keeping size small helps reduce training inference times computational cost transformers cross segment bert lstm hierarchical bert figure proposed segmentation models illustrating document segmentation task cross segment bert model left feed model local context surrounding potential segment break tokens left tokens right lstm model center rst encode sentence bert model feed sentence representations lstm hierarchical bert model right rst encode sentence bert feed output sentence representations transformer based model self attention particular increases ically input length lstm responsible handling diverse potentially large sequence sentences linear tional complexity practice set maximum document length sentences longer ments split consecutive non overlapping chunks sentences treated independent documents essense hierarchical nature model close recent neural approaches koshorek hierarchical bert model hierarchical bert model encodes documents replacing document level lstm encoder lstm model transformer encoder architecture similar hibert model document summarization zhang encoding sentence independently cls token representations sentences passed document encoder able late different sentences cross attention illustrated figure quadratic computational cost formers use limits lstm input sequence sizes word pieces sentence sentences document number model parameters parable proposed models use layers sentence document encoders total layers order use bertbase checkpoint experiments use attention heads dimensional word piece embeddings study alternative initialization dures initializing sentence document coders bertbase pre training model weights wikipedia procedure described zhang summarized masked sentence prediction objective ogously masked token pre training objective bert model hierarchical bert tency literature evaluation methodology datasets perform experiments datasets commonly literature document segmentation periments choi discourse segmentation experiments rst dataset summarize statistics datasets table dataset koshorek contains thousand articles snapshot english wikipedia domly partitioned train development test sets use original splits provided authors segmentation granularities possible dataset predict section boundaries average number segments document average segment length sentences found preprocessing methodology dataset cross segmenttokenstransformert contextright bilstmdocumentsentencespredn transformerdocumentsentencespredn able effect nal numerical results lar ltering lists code snippets cial elements original preprocessing script koshorek fair comparison choi choi dataset choi early dataset containing synthetic documents concatenated extracts news articles document segments ment created sampling document brown corpus sampling random segment length sentences dataset originally evaluate supervised segmentation algorithms ill designed evaluate supervised algorithms use dataset best effort attempt allow comparison previous literature create splits standard splits exist randomly sampled documents test set documents validation set leaving documents training following evaluation brown corpus contains documents documents sampled necessarily resulting data leakage different splits use aged future research rst perform experiments discourse segmentation rst discourse treebank rst carlson dataset composed wall street journal articles penn treebank marcus split train set composed cles test set composed articles found choice validation set held train set large impact model performance reason conduct fold cross validation report average test set metrics dataset discourse tion segmentation decisions intra sentence level context decisions sentence order evaluation consistent systems literature decided use sentence splits available dataset human annotate reason cases edus manually annotated overlap sentences cases merge sentences docs sections sentences train dev test choi train choi dev choi test rst train rst test edus docs sentences table statistics datasets metrics following trend studies text mentation soricut marcu evaluate approaches precision recall score regard internal boundaries segments tion include boundary sentence document trivial categorize positive boundary lead articial ination results allow comparison existing literature use metric beeferman evaluate results choi dataset note lower scores indicate better performance set customary half average ment size reference segmentation metric harsh score takes account near misses important note metric known suffer biases example penalizing false negatives false positives discounting errors close document extremities pevzner hearst results table report results document discourse segmentation experiments datasets presented section clude state art baselines compared proposed independently short time period hierarchical lstm koshorek segbot wang include human annotation baseline wang providing additional reference point rst dataset trained els estimate standard deviations posed models able calculate precision recall precision lstm koshorek segbot wang rst recall choi cross segment bert lstm hier bert human wang table test set results text segmentation discourse segmentation baselines models possible estimate standard deviations bootstrapping test set times hierarchical lstm code trained checkpoint publicly released train models adamw mizer loshchilov hutter dropout rate linear warmup procedure learning rates set sen maximize score validation sets dataset expensive els especially dataset trained models google cloud tpus table models perform baselines datasets reducing relative error margins best baseline respectively rst choi datasets improvements statistically signicant datasets errors impressively low choi dataset important point small scale thetic dataset limited ument concatenation extracts random news articles articially easy task previous neural baseline achieved low error margin dataset segment bert model obtains good results compared hierarchical models attend candidate break aligns expectation locally attending ment break sufcient expect large semantic shifts articial nature dataset hierarchical models sentence encoder followed document encoder perform rst dataset reminder discourse segmentation task segmenting individual sentences notion document context order study hierarchical structure necessary discourse segmentation trained model lstm making predictions directly bert decreased score worth noting known lstm downsides particularly apparent model harder train signicantly slower training inference hierarchical bert model ent initialization methods document segmentation datasets choi dataset hibert initialization model fully trained end end hierarchical bert similarly zhang necessary good results small dataset size contrary obtained slightly better results initializing levels hierarchy bertbase dataset model took longer converge initializations random levels hierarchy bertbase lower level random upper level gave worse results surprising result table good performance cross segment bert model datasets relies local context predictions bert checkpoints pre trained things sentence prediction task clear priori cross segment bert model able detect subtle semantic shifts evaluate ness model tried longer contexts particular considered cross segment bert contexts achieving recall precision scores encoding document hierarchical manner transformers improve cross segment bert dataset suggests bert self attention mechanism applied candidate segment breaks limited context case powerful separately encoding sentence ing information encoded sentences section analyze impact context length results segment bert model analyses section perform additional analyses ablation studies better understand tion models experiments revolve cross segment bert model choose model advantages alternatives outperforms baselines previously ported state art results competitive complex cal approaches considered conceptually close original bert model devlin code open source simple ment uses local document context fore require encoding entire ment segment potentially small piece text interest application text segmentation ing document writer composing document example save time effort task proposed lukasik zens aligned industrial applications google docs explore provide recommend related entities writer real time text segmentation help authors structuring ment better suggesting section break appropriate motivated tion analyze context needed reliably predict section break role trailing context size aforementioned application helpful use little trailing break text possible way suggest tion breaks sooner reducing context size speeds model cost quadratic quence length end study effect trailing context size going word piece tokens set experiments held leading context size xed tokens tuned bertbase batch size examples learning rate results experiments shown figure results intuitive clear figure analysis importance right text length solid red line dashed blue line denotes hierarchical lstm baseline encoding context koshorek performance drops smaller trailing context smaller overall text answer ran experiment tokens left tokens right experiment attains score smaller tokens proposed break clearly crucial model sees sides break aligns intuition word distributions fore true segment break typically different hearst presenting model distributions tokens proposed break leads poor formance experiment replaced running text sorted list frequent tokens seen larger context tokens padding necessary tuned bertbase experiment attains score compared running text suggests high performing models ing counting tokens detect semantic shift role transformer architecture best cross segment bert model relies bertlarge powerful model slow expensive run large scale applications ofine analysis web search online document processing google docs crosoft ofce large models prohibitively expensive table shows effect model size performance experiments ized training models pre trained right context length word segment bert basehier lstm architecture parameters table effect model architecture sults bert paper devlin rst experiments initialized bertlarge bertbase respectively overall larger model better formance experiments suggest addition size conguration matters dimensional model layers outperform dimensional model fewer layers new state art standard deviations better previous reported table gain came steep cost model size unsatisfactory large size hinders possibility model scale low latency desirable application wang section explore smaller models better formance model distillation model distillation seen previous section mance degrades quickly smaller fore practical networks tive pre tuning approach distillation popular technique build small networks bucila hinton instead training directly small model segmentation data binary bels instead leverage knowledge learnt best network called context teacher follows record tions precisely output logits teacher model dataset small student model trained combination cross entropy loss true labels mse loss mimick teacher logits tive weight objectives treated hyperparameter distillation results presented table distilled models perform better architecture parameters table distillation results dataset models trained directly training data teacher increasing scores points notice distillation allows pact models signicantly outperform vious state art unfortunately directly compare model sizes koshorek rely subset dings public archive includes vocabulary items including phrases likely model fair hierarchical lstm model relies dozens millions embedding parameters tuned training million lstm parameters conclusion paper introduce new model chitectures text segmentation tasks segment bert model uses local context candidate breaks chical models lstm hierarchical bert evaluated models ment discourse segmentation dard datasets compared recent neural approaches experiments showed models improve current state art particular found cross segment bert model extremely competitive chical models focus recent research efforts chalkidis zhang surprising suggests local context sufcient cases plicity suggest trying baseline tackling segmentation problems datasets naturally results imply chical models disregarded showed strong contenders convinced applications local context sufcient tried encoders level hierarchy experiments suggest deep transformer encoders useful coding long complex inputs documents document segmentation applications lstms proved useful discourse segmentation rnns general useful long documents able deal long input sequences finally performed ablation studies better understand role context model size sequently showed distillation effective technique build compact models use practical settings future work plan investigate different techniques apply problem text segmentation including data augmentation wei zou lukasik methods regularization mitigating labeling noise jiang lukasik references pinkesh badjatiya litton kurisinkel manish gupta vasudeva varma attention based neural text segmentation corr doug beeferman adam berger john lafferty statistical models text segmentation chine learning cristian bucila rich caruana alexandru niculescu mizil model compression ceedings twelfth acm sigkdd international conference knowledge discovery data ing philadelphia usa august pages lynn carlson daniel marcu mary ellen okurovsky building discourse tagged pus framework rhetorical structure theory proceedings second sigdial workshop discourse dialogue ilias chalkidis ion androutsopoulos nikolaos aletras neural legal judgment prediction proceedings conference english association computational linguistics acl florence italy july august ume long papers pages freddy choi advances domain pendent linear text segmentation proceedings north american chapter association computational linguistics conference naacl pages stroudsburg usa tion computational linguistics jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language standing corr jacob eisenstein hierarchical text segmentation human multi scale lexical cohesion guage technologies conference north ican chapter association computational linguistics proceedings pages marti hearst texttiling segmenting text multi paragraph subtopic passages computational linguistics geoffrey hinton oriol vinyals jeffrey dean distilling knowledge neural network nips deep learning representation ing workshop haoming jiang pengcheng weizhu chen aodong liu jianfeng gao tuo zhao smart robust efcient tuning trained natural language models principled regularized optimization shaq joty giuseppe carenini raymond gabriel murray discourse analysis plications proceedings annual ing association computational linguistics tutorial abstracts pages florence italy sociation computational linguistics shaq joty giuseppe carenini raymond codra novel discriminative framework rhetorical analysis computational linguistics omri koshorek adir cohen noam mor michael rotman jonathan berant text mentation supervised learning task corr jing aixin sun shaq joty segbot generic neural text segmentation model pointer proceedings seventh network international joint conference articial ligence pages international joint conferences articial intelligence zation junyi jessy kapil thadani amanda stent role discourse units near extractive rization proceedings annual meeting special interest group discourse logue pages los angeles association computational linguistics fernando llopis antonio ferrandez rodrguez jose luis vicedo gonzalez text tion efcient information retrieval ings international conference putational linguistics intelligent text ing cicling pages berlin springer verlag ilya loshchilov frank hutter weight decay regularization adam fixing corr michal lukasik srinadh bhojanapalli aditya krishna menon sanjiv kumar label arxiv preprint smoothing mitigate label noise michal lukasik himanshu jain aditya menon ungyeon kim srinadh bhojanapalli felix sanjiv kumar semantic label smoothing sequence sequence problems proceedings conference empirical methods ral language processing michal lukasik richard zens content plorer recommending novel entities ment writer proceedings conference empirical methods natural language ing pages brussels belgium tion computational linguistics william mann sandra thompson rhetorical structure theory functional ory text organization text interdisciplinary journal study discourse mitchell marcus grace kim mary ann marcinkiewicz robert macintyre ann bies mark ferguson karen katz britta schasberger penn treebank annotating predicate argument structure proceedings workshop human language technology hlt pages stroudsburg usa association computational linguistics pedro mota maxine eskenazi lusa coheur beamseg joint model multi document proceedings mentation topic identication conference computational natural language learning conll pages ciation computational linguistics jeffrey pennington richard socher christopher manning glove global vectors word resentation proceedings conference empirical methods natural language ing emnlp pages doha qatar ciation computational linguistics matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word sentations corr lev pevzner marti hearst critique improvement evaluation metric text mentation comput linguist gennady shtekh polina kazakova nikita nikitinsky nikolay skachkov applying topic mentation document level information retrieval proceedings central eastern european software engineering conference russia cee secr pages new york usa acm radu soricut daniel marcu sentence level discourse parsing syntactic lexical proceedings human mation guage technology conference north chapter association computational linguistics pages masao utiyama hitoshi isahara tical model domain independent text proceedings annual meeting tion association computational linguistics pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need guyon luxburg bengio wallach fergus vishwanathan nett editors advances neural information cessing systems pages curran ciates inc oriol vinyals meire fortunato navdeep jaitly proceedings pointer networks international conference neural tion processing systems volume pages cambridge usa mit press yizhong wang sujian jingfeng yang fast accurate neural discourse proceedings conference tation empirical methods natural language processing pages association putational linguistics jason wei kai zou eda easy data mentation techniques boosting performance proceedings text classication tasks conference empirical methods ral language processing international joint conference natural language processing emnlp ijcnlp pages hong kong china association computational linguistics yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin son xiaobing liu ukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith jason riesa alex nick oriol vinyals greg corrado macduff hughes jeffrey dean google neural machine translation system bridging gap human machine translation corr ngo xuan bach nguyen minh akira mazu reranking model discourse proceedings mentation subtree features annual meeting special interest group discourse dialogue pages seoul south korea association computational linguistics xingxing zhang furu wei ming zhou bert document level pre training hierarchical bidirectional transformers document proceedings annual meeting tion association computational linguistics pages
