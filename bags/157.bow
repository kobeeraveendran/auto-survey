abstractive extractive text summarization document context vector recurrent neural networks chandra khatri amazon sunnyvale california com gyanit singh ebay inc san jose california com nish parikh google mountain view california com l u j l c s c v v x r abstract sequence sequence learning recently abstractive extractive summarization current study models ebay product description tion propose novel document context based els rnns abstractive extractive summarizations tuitively similar humans reading title abstract contextual information reading document gives humans high level idea document use idea propose models started contextual information rst time step input tain better summaries manner output summaries document centric generic overcoming major hurdles generative models generate context user behavior seller provided information train evaluate models human extracted golden summaries document contextual models outperform standard models generating human extracted summaries hibitively expensive scale propose semi supervised technique extracting approximate summaries training models scale semi supervised models uated human extracted summaries found similar ecacy provide comparison tive extractive summarizers contextual non contextual evaluation dataset overall provide methodologies use evaluate proposed techniques large document marization furthermore found techniques highly eective case existing techniques ccs concepts information systems summarization computing ologies neural networks semi supervised learning settings keywords text summarization recurrent neural networks natural language processing information retrieval extraction abstraction language modeling topic signature deep learning commerce work author ebay inc work author ebay inc permission digital hard copies work personal classroom use granted fee provided copies distributed prot commercial advantage copies bear notice citation rst page copyrights party components work honored uses contact owner deep learning day london uk copyright held owner figure snapshot product snippet appears ebay mobile app product titled mens travel hiking itary tactical army camo sling backpack chest shoulder bag html description hidden click row item description making current page easy consume user friendly introduction document summarization applications mains internet search engines provide query context specic summary snippets search experience news websites use summaries brief articles social media use content targeting e commerce websites use summaries better browse experience item product highlights paper leverage data sets popular ebay presenting users product summary decreases user s nitive load evaluate product s relevance purchase intent leading higher engagement better browsing experience thermore trac mobile sites applications ing item summaries relevant limited real estate available mobile sites design point view relevant item summaries entire html ments figure depicts picture snippet ebay mobile application text summarization techniques extractive tive extraction key sentences objects extracted modifying objects obtained key phrase ad sentence extraction keeping sentences intact abstraction involves paraphrasing context aware tences understanding language abstractive niques generally requires large scale data documents sponding summaries training models example news titles considered summaries articles sidered document instances summarizing deep learning day august london uk chandra khatri gyanit singh nish parikh content generated party web systems legal straints modication content cases summaries extracted generated majority content written products marketplace provided sellers marketplace legal constraint revising tent work propose technique generation corporating document context based generative els abstractive extractive summarization context akin humans reading title abstract know key details delving document paper describe techniques generating context documents ing user behavior information provided document creators rnns abstraction extraction benet feeding document context rst time step sequence sequence learning evaluate summaries erated methodology found contextual document preferred humans human extracted summaries training extractive models like extractive rnns approach training scalable extracting summaries millions hundreds thousands documents prohibitively expensive sequence quence model generally perform best large scale data fore proposed novel approach document context extracting approximate summaries semi supervised fashion large scale training abstractive sequence sequence models generally trained titles subtitles adopt similar approach context helps scaling training furthermore form heuristics trained model ranking sentences document based likelihood sentence document summary sentence inference adopted idea state art techniques automatic speech recognition rnn ranking potential outcomes n gram based language models following main contributions obtaining context vectors documents extractive abstractive summarization tasks automatically extracting approximate summaries ating training data enable large scale semi supervised learning extractive summarization shown competitive supervised learning techniques rnn cnn rnn extractive summarization abstractive summarization large documents document context vectors improving learning text summarization novel approach shown beat state art similar settings comparing abstractive extractive summarization setting sequence sequence learning previous work summarization techniques explored past decades following popular techniques surface level approaches consider title words cue words e important best extracting relevant sentences corpus based approaches leverage structural distribution words internal external corpus e wordnet summarization cohesion based approaches considers cohesive relations tween concepts text antonyms repetitions synonyms lexical chains graph based approaches popular text summarization techniques sentence text represented vertex graph constructed sentences edges correspond inter connections sentences lexrank textrank techniques machine learning based approaches document tion converted supervised semi supervised learning problem supervised learning approaches hints clues key phrases topic words blacklist words label tences positive negative classes sentences manually tagged scalable labels established nary classier trained obtaining scores likelihood score pertaining sentence tion techniques explored literature regard classication based approaches generalize ecient extracting document specic maries training data machine learning approach contains bel sentences irrespective document document level information provided approaches provide prediction irrespective document providing document context models alleviates problem contributions paper abstractive summarization techniques prevalent literature extractive ones harder involves writing sentences performed manually scalable requires natural language generation techniques common abstraction techniques structured tic graph tree based ontology rule e template based approaches mentioned work face challenge scalability large scale evaluation generalize complexity constraints research date focused primarily extractive methods advancements natural language generation niques making possible generate reasonable summaries short descriptions abstraction reinforcement learning cnns abstractive summarization current advancements short summaries short documents furthermore exist work extractive abstractive techniques compared similar setting approaches techniques based approaches ciently map input sequences description document map output sequence summary require large amounts data examples model tends learn mapping tween input sequence output sequence generate cient summaries corresponding input document found models currently work smaller ument summarizations lines document mapping abstractive extractive text summarization deep learning day august london uk headlines phrase representation els providing benchmark results machine translation speech recognition tasks performed summarization tasks dialog systems evaluation alog systems facing challenges e marizing long documents long sequence depicted reversing source sentence provides better results architecturally abstractive models similar change encoder novel document contextual lstm stead simple attention based mechanism document vector described section furthermore unlike related ous work models current study summarizing large uments longer characters generate relatively large summaries characters beam search vocabulary strictions constraints furthermore found ture based abstractive summarization compared related rnn cnn variations extractive summarization novel addition paper khatri et al describes summarization ecommerce setting user actions infer document context websites host user generated documents consumption users provide myriads ways document discovery example users ecommerce website discover relevant ument intent search recommendation modules ious topical pages sellers content creators creating document product pages provide lot metadata ument example title tags categorical section describe use historical information document creator document consumer generate document context context later combined word embeddings obtained skip gram model negative sampling sgns generate document context vectors dcv use ment context score sentences document algorithmic labeling large scale semi supervised learning obtain imate summaries stop words high frequency words ulary considered obtaining context words following subsections let v vocabulary corpora assume stop words removed corpora generate cabulary let n vocabulary size e n document d generate unit vectors s s denotes cd seller b b denotes browse vectors dimension n document d c sions later combine vectors sgns vectors dcv brevity drop d superscript rest section vectors generated document q q denotes queries cd cd c c document creators seller lists product sale ebay create document includes title key value meta data xed dimensions like condition brand size color type verbose description containing images videos html tion select leaf ebay taxonomy c ln figure snapshot taxonomy ebay taxa collectibles art expanded lower taxa example antiques li taxon taxonomy taxonomy ebay laminar family given taxa seller attach leaf taxonomy tree document figure shows small snapshot ebay taxonomy omy tree maintained generated domain experts high quality example document titled mens travel hiking military tactical army camo sling backpack chest shoulder bag chosen following leaf sporting goods outdoor sports camping hiking hiking backpacks day packs seller context vector cs induced seller provided metadatams title subtitle taxon key valued metadata brand color w v let cs w value dimension w cs dened follows cs w ms w seller metadata normlaize vector cs unit vector cs use weight obtained context word later obtain document context vector document giving relevance words based weights technique limited ebay ecommerce based pages approach easily extended general textual documents available online form webpages e title metadata articles e title sub titles abstract keywords category c document consumers buyers discover relevant inventory search recommendations topical pages e ebay com hiking backpacks html amazon com hiking backpacks bags external sources like advertising discovery browse paths search trails click trails ends document extract relevant words ument discovery path search example user searches queries like tactical sling backpack tary backpack lands document titled mens travel ing military tactical army camo sling backpack chest shoulder bag words queries contains user thought descriptive information document information aggregated statistically cross large number users provide great context document let qsetd queries discover document d browsed titles documents user discovered document deep learning day august london uk chandra khatri gyanit singh nish parikh recommendation topical page dimension associated word w values vectors dened qsetd w qsetd cb w browsed w browsed cq cb normalize vector cb cq unit vector ilar weights obtained context words document creators obtain weights words provided ment consumers readers basically signicant tion provided readers based experience ior leverage information provided readers ther obtain context words corresponding weights later obtain document context vector c document context vectors document d dened unit vectors s s denotes cd seller b b denotes browse combine form cumulative context unit vector cd adding c c vector q q denotes queries cd cd c cd q cd s q b cd s cd s q b parameters ne tuned based historical demand estimated ratio trac volumes dierent channels expert set existing priors importance channels trac weight value word dimension cd idf cd d precisely value dimension word w dened d f w cd w d word w cd combine context word embeddings obtained sgns generate vector document vd let msg n s matrix dimension n xk k dimension word embeddings n vocabulary size corpora row word w msg n s word embedding word w dene vd cd d msg n s note dimension vd like word embeddings scoring sentences document context set document let s contains sentnecesd tences d describe use document context score sentences score generating algorithmic labels large scale semi supervised learning obtain mate summaries details section sentence s sentnecesd s wk sscor e vd w s score corresponds weighted sum words sentence weight words incorporate frequencies seller provided metadata buyer s search history browse tory leading discovery said product inverse document frequence idf score word idf score word simply inverse documents containing word documents ebay idf score corresponds topical document level relevance word lary highly common words stop words nearly zero idf score score obtain score sentence use sentence score rank tence document select till reach acters sentences summary document product b test summaries obtained technique correlates user expectations observed statistically nicant lift sales user engagement implies summaries obtained semi supervised approach ful use technique generate training data vised extraction based summarization techniques e rnn models work primarily focuses adding context initial state rnns abstractive extractive text summarizations paring state art techniques extraction use labeled supervised semi supervised data tion use titles subtitles training models nomenclature base model recurrent neural network type neural network extension feed forward nn feed tion activations ow round loop essentially information prior observation current vations predictions notations borrowed sutskever et al rnn putes output sequence yt given input sequence xt corresponding following equations ht hx xt w hh ht yt w yh ht framework works alignment output sequences size input size output size input output sequences dierent rnns encoding decoding mechanisms cho et al theoretically framework work found rnns encoding decoding nisms nd diculties mapping long sequences long term dependencies gated recurrent units grus long short term memory lstms solve problem troducing gates network prevent vanishing gradient problem associated rnns long term dependencies current study lstms extractive tive summarizations standard lstm sequence xed length passed input encoded xed dimension vector v decoded output sequence words summarize lstm estimates following abstractive extractive text summarization deep learning day august london uk t t yt xt abstractive summarization document fed input training summaries fed output extractive rnn trained standard supervised cation setup performing soft max encoded layer contextual recurrent neural network abstractive contextual rnn ac rnn rnn architecture document context vector described section passed input rst time step ument sequence encoder idea pre learned document context vector vd passed input ning encoding stage model converges faster learns summaries corresponding ment generic sequences basic idea reader aware title document abstract cation provides better understanding high level terpretation document makes model able provide specic summaries corresponding documents document context vector vd rst time step sentially changes encoding vector lstm decoder similar architecture mentioned ing input time decoder vector obtained encoder unlike decoding mechanisms output time t word cabulary output document vocabulary considered time prediction making inference faster sutskever et al proposed beam search obtain likely sentence machine translation task perform heuristics trained model ranking sentences document based likelihood sentence summary sentence inference likelihood sentence dened likelihood decoding sentence given encoded input abstractive model extraction inference adopted idea state art techniques automatic speech tion rnn ranking potential outcomes n gram based language model address issues avoid generic short output issues quence sequence models obtaining grammatically correct sentences ebay users avoid poor customer experience c avoiding legal push backs sellers extractive contextual rnn ec rnn consists encoder encoder ec rnn replica coder ac rnn document context vector input time embedding representation words passed input model output encoder binary classication sentence summary sentence softmax note sentence starts context vector classication sentence happens given context document sentence way sentence classied summary sentence table description distribution ebay products total vocabulary size median document length median number words median sentence length median nbr words sentence characters characters document furthermore given context vector vd extra information provided tence dierentiates sentences ument major drawback state art cation approaches sentences classied summary sentence non contextual rnn architectures current setup rnns trained document context vector termed non contextual rnns recently architectures proposed regards main models explored current study abstractive rnn rnn abstractive rnn ditional sequence sequence model lstm suggested model exactly similar ac rnn text input time input time rnn token start xed sized input output sequences ing curtailing padding extractive rnn e rnn rnn sication tasks generates state art results extractive rnn non context version ec rnn proposed section mentioned embeddings words pre calculated ing skip gram negative sampling sgns technique input corresponding word encoding layer tures extracted embedding classication task convolutional rnn cnn rnn convolution based lstm performed extremely text classication tasks furthermore convolution attention based encoder short summarization tasks cnn lstm sify sentences technique e rnn dierence cnn extract sequences level phrase representations suggested zhou et al lstm able capture local features phrases global temporal sentence semantics cnns multiple lters max pooling dropout extract high level phrase representations passed lstm classication softmax datasets table describes distribution ebay description vocabulary size dataset k words median document length words datasets kinds datasets current study deep learning day august london uk chandra khatri gyanit singh nish parikh human extracted snippets items documents responding details titles url description provided mans extracting summaries task extract rank sentences descriptions given ebay item url items items evaluations golden set items training dierent models semi supervised large scale summarization tion sentences item descriptions extracted ranked based relevance document context section provides information rank sentence order relevance given document s contextual details eral techniques proposed shen et al lin et al based query thematic similarity topic signature tent semantic analysis expanded topics wikipedia obtained best results duc summarization task similar approaches mentioned obtain imate summaries ebay item descriptions evaluation ebay reviewers found summaries generated ing document context based approach high quality training models given quality maries ebay launched feature mobile applications site b test found showing summaries approach high monetary value compared ing summary snippet models trained semi supervised approach evaluated golden test set identify vance technique data generation classication task ec rnn c rnn cnn rnn classication based tive summarization techniques need labeled data training classication tasks sentences blacklist terms labeled non summary class sentences scoring high document context metric considered positive sentences blacklist terms words phrases contain item document level information frequently ebay returns stars obtained terms human curation statistical analysis ebay item descriptions description items tences tagged positive negative sentences scoring high document context metric having list terms left tagged context sentence ranking new approach uated b test production data tagged high precision work step evaluating context based sentence ranking architecture details experimentation training details model architectures abstractive context rnn ac rnn abstractive rnn rnn trained deep lstm layers described sutskever et al cells dimension word embeddings wanted nd relative dierence ter adding context rnns summarization task kept parameters models parameters settings table parameter setting abstractive approaches like ac rnn rnn parameters input description length output summary length optimization method learning rate batch size lstm parameters value words words stochastic gradient descent momentum reduced half epoch uniform distribution table parameter setting extractive approaches like rnn e rnn parameters maximum sentence length optimization method learning rate batch size lstm parameters value words adam uniform distribution table parameter setting convolutional rnn parameters maximum sentence length dropout probability learning rate filter sizes convolution batch size cnn lstm parameters max pool size value words random normal centered standard deviation model details worked best case mentioned table extractive context rnn ec rnn extractive rnn e rnn trained lstm layers cells dimension word embedding wanted nd tive dierence adding context rnns summarization task kept parameters models parameters settings model details worked best case mentioned table convolutional rnn cnn rnn consists neural works cnn high level phrase representations lstms obtaining temporal sequential nature text single layer convolution lter size equal gle layer lstm cells dimension word embeddings parameters settings model details worked best case mentioned table abstractive extractive text summarization deep learning day august london uk table dierent type experiments metrics experiments evaluation setting classication similarity ranking description given sentence classify summary sentence given golden summaries nd similarity score given sentences rank order relevance summary evaluation metric accuracy precision recall f score rouge rouge l bleu tf idf cosine similarity topic similarity ndcg mean average precision experiments evaluation metrics split k human extracted summaries dataset k k parts use k evaluations data tained semi supervised technique training ing models supervised train models k human extracted summaries models trained dierent settings target summary lengths use sentence sentence sentence summary lengths semi supervised use k documents approximate maries generated document context training models scale use k human extracted summaries evaluation purposes baseline fuzzy summarization strategy dom sentences picked summary output results supervised setting section present result supervised models marization puproses trained models k human extracted summaries evaluated k human extracted maries table compiles performance result summarization strategies clear rnn s outperform niques naive bayes svm lsa lexrank textrank rouge bleu token similarity abstractive extractive rnn s adding document context improves metrics example bleu abstractive rnn respectively document context added ac rnn bleu creases respectively similarly extractive rnn e rnn vs extractive contextual rnn ec rnn changes bleu changes large target summaries sentence long tive contextual rnns perform best followed abstractive textual rnns example ec rnn ac rnn similarly bleu compared rnn small target summaries sentence long observe abstractive context rnn ac rnn outperform extractive rnn ec rnn ec rnn compared ac rnn target summaries sentences long results shared table target summary length creases ecacy extractive contextual rnn increases stractive contextual rnns table present classication metrics extractive els models good job separating summary sentences non summary sentences task ec rnn outperforms methodologies table results supervised task human extracted summaries k training k evaluation tions target summary sentence long tences long abstractive context rnn ac rnn performs best metrics short summaries extractive context rnn ec rnn performs best longer summaries adding context created improvements rnn models bleu topic sim model fuzzy e rnn ec rnn cnn rnn rnn ac rnn nb svm lsa lexrank textrank fuzzy e rnn ec rnn cnn rnn rnn ac rnn nb svm lsa lexrank textrank rouge token sim target snippet length sentence target snippet length sentence table describe ranking metrics summarization models assign summary sentences relevance sentences relevance score task generate ranking sentences way picks summary sentences non summary tences task context aware rnns win tive context rnn ec rnn highest deep learning day august london uk chandra khatri gyanit singh nish parikh table results supervised task human extracted summaries k training k evaluation ations target summary target summaries extremely long sentence extractive context rnn rnn performs best adding context created provements rnn models bleu topic sim model fuzzy v rnn c rnn cnn rnn rnn ac rnn nb svm lsa lexrank textrank rouge token sim target snippet length sentence table classication extractive supervised model classes non summary training k human judged k human judged extractive context rnn rnn shows best performance note classication stractive models model accuracy nb svm e rnn cnn rnn ec rnn precision recall f score table supervised model ranking evaluation training k human judged k human judged tive context rnn ec rnn shows best performance model fuzzy e rnn ec rnn cnn rnn rnn ac rnn nb svm lsa lexrank textrank results semi supervised setting section share result training algorithmically generated approximate summaries approximate summaries generated document context extractive rnn classication approaches like svm nb human labeled table classication semi supervised extractive pervised model classes non summary training k algorithmically labeled data k human judged ec rnn shows best performance model accuracy e rnn cnn rnn ec rnn e rnn cnn rnn ec rnn precision recall semi supervised supervised f score algorithmically labeled data training stractive rnn use title subtitles learning table compares classication metric extractive els trained human extracted summaries cally labeled approximate summaries seen training models large scale approximate summaries lead drop precision recall accuracy table compares summarization results rnn els abstractive vs extractive training k approximate maries lead drop metric example drops ec rnn supervised ec rnn semi supervised bleu remains comparable dataset k dataset ranking metrics compared table size data increases training abstractive methods performance increases tremendously example ac rnn vs k vs k documents respectively result hold map increase data increased k k ac rnn extractive rnn trained approximate summaries nt drop ranking metrics k human extracted vs k approximate summaries remains cases overall seen abstractive rnns improving data extractive rnn able generate near similar formance large scale approximate summaries small scale human extracted summaries adding document context rnn approximate summaries boost performance abstractive extractive rnn conclusion proposed novel document context model stractive extractive summarization shown rnns models powerful beat state art summarization approaches e commerce setting idea adding contextual information rst time step encoding input output sequence label mapping aligns humans generally humans tend read title abstract gather contextual information reading entire document articles gives humans high level understanding document abstractive extractive text summarization deep learning day august london uk table result summarization semi supervised tractive supervised model training k algorithmically labeled data k human judged extractive text rnn ec rnn shows best performance abstractive contextual rnns signicant improvements large training data model token sim semi supervised rouge bleu topic sim e rnn ec rnn cnn rnn rnn ac rnn e rnn ec rnn cnn rnn rnn ac rnn supervised table ranking metrics semi supervised extractive supervised model training k algorithmically labeled data k human judged abstractive contextual rnns improvements large training data tractive contextual rnns improvements model semi supervised supervised e rnn ec rnn cnn rnn rnn ac rnn incorporated model generate richer ment specic summaries training performed human tagged supervised setting large scale semi supervised tracted summaries found based rnn tion techniques performs state art summarization techniques rnns contextual rnns outperform non rnns similarity ranking measures contextual rnns found best performing followed contextual rnns large summaries shorter maries abstractive contextual rnns outperform techniques followed extractive rnns attention cated setting possible improve abstractive niques depicted abstractive rnns contextual contextual extraction tasks beat tractive systems found large scale semi supervised data training improves performance models evaluation dataset training extractive models proximate summaries leads better results compared relatively smaller human tagged supervised data think advantage large scale training outperforms noise approximating maries recommend researchers incorporate context tasks e machine translation task references text summarization tensorow googleblog text summarization tensorow html accessed r m badry s eldin d s elzanfally text summarization international journal latent semantic analysis framework comparative study computer applications r barzilay m elhadad lexical chains text summarization vances automatic text summarization pages t baumel m eyal m elhadad query focused abstractive summarization incorporating query relevance multi document coverage summary length constraints models arxiv preprint j cheng m lapata neural summarization extracting sentences words arxiv preprint k cho b van merrinboer c gulcehre d bahdanau f bougares h schwenk y bengio learning phrase representations rnn encoder decoder statistical machine translation arxiv preprint s chopra m auli m rush s harvard abstractive sentence rization attentive recurrent neural networks proceedings naacl pages j chung c gulcehre k cho y bengio empirical evaluation gated current neural networks sequence modeling arxiv preprint d das f martins survey automatic text summarization literature survey language statistics ii course cmu j l elman finding structure time cognitive science g erkan d r radev lexrank graph based lexical centrality salience text summarization journal articial intelligence research j gehring m auli d grangier d yarats y dauphin convolutional quence sequence learning arxiv preprint y gong x liu generic text summarization relevance measure latent semantic analysis proceedings annual international acm sigir conference research development information retrieval pages acm f guo metallinou c khatri al topic based evaluation conversational bots arxiv preprint u khandelwal neural text summarization c khatri n parikh s solanki al snippet extractor recurrent neural works text summarization industry scale patent c khatri n parikh s solanki al snippet generation item description summarizer patent c khatri s voleti s veeraraghavan n parikh islam s mahmood n garg big data big v singh algorithmic content generation products data ieee international conference pages ieee p li w lam l bing z wang deep recurrent generative decoder abstractive text summarization arxiv preprint c lin rouge package automatic evaluation summaries text summarization branches proceedings workshop volume barcelona spain k lopyrev generating news headlines recurrent neural networks arxiv preprint h p luhn automatic creation literature abstracts ibm journal search development r mihalcea p tarau textrank bringing order texts association computational linguistics t mikolov sutskever k chen g s corrado j dean distributed resentations words phrases compositionality advances neural information processing systems pages g miller wordnet lexical database english commun acm nov f moawad m aref semantic graph reduction approach abstractive text summarization computer engineering systems icces seventh international conference pages ieee r nallapati f zhai zhou summarunner recurrent neural network arxiv based sequence model extractive summarization documents preprint r nallapati b zhou c gulcehre b xiang al abstractive text arxiv preprint marization sequence sequence rnns v nastase topic driven multi document summarization encyclopedic knowledge spreading activation proceedings conference pirical methods natural language processing pages association computational linguistics r paulus c xiong r socher deep reinforced model abstractive summarization arxiv preprint ram r prasad c khatri venkatesh conversational ai science alexa prize arxiv preprint deep learning day august london uk chandra khatri gyanit singh nish parikh m rush s chopra j weston neural attention model abstractive sentence summarization arxiv preprint g singh n parikh n sundaresn user behavior zero recall ecommerce proceedings international acm sigir conference queries search development information retrieval pages acm h f song g r yang x wang training excitatory inhibitory recurrent neural networks cognitive tasks simple exible framework plos comput biol j steinberger k jezek evaluation measures text summarization puting informatics sutskever j martens g e hinton generating text recurrent proceedings international conference machine ral networks learning pages sutskever o vinyals q v le sequence sequence learning neural advances neural information processing systems pages networks venkatesh c khatri ram f guo et al evaluating comparing conversational agents arxiv preprint s wiseman m rush sequence sequence learning beam search optimization arxiv preprint w xiong wu f alleva j droppo x huang stolcke microsoft conversational speech recognition system arxiv preprint y xu j lau t baldwin t cohn decoupling encoder decoder works abstractive document summarization aclweb c zhou c sun z liu f lau c lstm neural network text tion arxiv preprint
