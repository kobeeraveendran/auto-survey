clustering of deep contextualized representations for summarization of biomedical texts milad moradi matthias samwald institute for artificial intelligence and decision support center for medical statistics informatics and intelligent systems medical university of vienna vienna austria milad moradivastegani matthias ac at abstract from summarizers contextualized recent years in that incorporate domain knowledge into the process of text summarization have outperformed generic methods especially for summarization of biomedical texts however construction and maintenance of domain knowledge bases are intense tasks requiring significant manual annotation in this paper we demonstrate representations that extracted the pre trained deep language model bert can be effectively used to measure the similarity between sentences and to quantify the informative content the results show that our based the performance of biomedical summarization although the summarizer does not use any sources of domain knowledge it can capture the context of sentences more accurately than the comparison methods the source code and data are available at com biotextsu mm bert based summ summarizer can improve introduction text summarization is the process of identifying the most important contents within a document and producing a shorter version of the text that conveys those important ideas many publicly available summarizers use generic features such as the position and length of sentences the term frequency the presence of some cue phrases to assess importance of sentences specifically in the biomedical domain it has been shown that these generic measures can not be as efficient as domain specific methods that incorporate sources of domain knowledge to the as such knowledge represent the text on a concept based level much effort has been invested in using sources of domain ontologies taxonomies controlled vocabularies to capture the context in which the input text appears these methods have improved the performance of biomedical summarization since they quantify the informative content by considering the semantics behind the sentences rather than considering only generic features however building maintaining and utilizing sources of domain knowledge can be challenging and time consuming tasks leading the research community to develop a new generation of context aware methods that use neural network based language models in recent years the usage of pre trained deep language models received significant attention for a wide variety of natural language processing nlp tasks in this approach unsupervised training is conducted on a large corpus of text and the resulting model can then be fine tuned on a supervised task or can be used directly to extract numeric features for input text the usage of deep pre trained language models has recently obtained state of the art results for a wide variety of nlp tasks from transformers in this paper we propose a novel biomedical text summarizer that uses the bidirectional encoder representations bert language model to capture the context in which sentences appear within an input document bert was pre trained on large text corpora wikipedia and bookcorpus and after a tuning step it can achieve state of the art results on a wide variety of nlp tasks it is also possible to directly extract and use contextualized embeddings learnt by bert without any further training or fine tuning steps as we do in this paper the sentences more accurately summarizers that use domain knowledge than showing clustering of deep that can representations contextualized improve the performance of biomedical text summarization summarization method our summarization method consists of four main steps figure illustrates the overall architecture of the summarizer preprocessing the the summarizer performs a preprocessing step in order to prepare the input text for the subsequent steps those parts of the text that seem to be unimportant for appearing in the summary are discarded these parts can vary based on the input document and user structure of requirements in our case since evaluations are performed on biomedical scientific articles the main text of input article is retained and any other parts such as headers of sections and subsections figures and tables are discarded this information can be added to the final summary if needed next the text is split into sentences and each sentence is split into tokens for this purpose we use the natural language tool kit nltk library mapping text representations to contextualized in the second step we utilize bert to extract contextualized embeddings the tokens are used as the input of the feature extraction script the output is a json file containing activation values of the hidden layers two different versions of bert with different model sizes are currently available bert base contains layers hidden units in each layer attention heads per unit and a total number of million parameters bert large contains layers hidden units in each layer attention heads per unit and a total number of million parameters we use both bert base and bert large in our experiments to assess the impact of different model sizes on the quality of summaries after the feature extraction step each token is represented as a contextualized embedding with a size of or based on the size of bert model next contextualized representation is computed for each sentence by a figure the overall architecture of our based biomedical text summarizer summarizer uses to quantify utilizing the bert language model our summarizer computes a contextual representation i e an n dimensional vector for every sentence it applies a hierarchical clustering algorithm to find multiple groups of sentences such that those sentences nearby in the vector space fall into the the same cluster the the contextualized embeddings informative content of sentences and assess the similarity between them the idea is that those sentences within the same cluster share similar context subsequently the summarizer selects the most informative sentences of each cluster to generate the final summary we evaluate the performance of our bert based summarizer on a corpus of biomedical scientific articles the results show that our summarizer can improve the performance of biomedical text summarization compared to generic methods and biomedical summarizers that utilize domain knowledge the main contributions of this paper can be summarized as follows utilizing a pre trained bidirectional unsupervised language model biomedical text summarization for demonstrating the bert based summarizer can capture the context of that averaging over all the representations of tokens belonging to the sentence sentence clustering the contextualized embedding of each sentence represents the context in which the sentence appears therefore nearby sentences in the vector space can share similar context the summarizer uses a clustering step to group the sentences into a number of clusters such that those in the same cluster are the most similar in terms of their representations in the vector space we use an agglomerative hierarchical clustering algorithm in this step the clustering algorithm starts by specifying the number of final clusters i e the parameter k in each iteration those two clusters that are the most similar or the nearest are merged and the number of clusters reduces by one the similarity or distance between two clusters is computed by averaging over all similarity or distance values between each sentence of the first cluster and each sentence of the second one the clustering algorithm proceeds until the number of clusters reaches k the similarity or distance between sentences can be computed using different measures we run the clustering step with two widely used measures separately i e cosine similarity and euclidean distance let rn and qn be the contextualized representations of two given sentences cosine similarity and euclidean distance between these two vectors are computed as follows at the end of this step there is a set of clusters each one containing a set of related sentences summary generation now the summarizer needs to decide which sentences are the most relevant and informative to be included in the summary since those sentences within the same cluster share some important content of the input text the summarizer selects sentences from all the clusters to cover as many important ideas as possible each cluster contributes to the summary in proportion to its size as follows where ni is the number of sentences that should be selected from ith cluster n is the size of summary specified by the compression rate is the size of ith cluster and is the size of input document in order to select the most informative and related sentences of each cluster a within cluster score is computed for each sentence as follows where wcsi j is the within cluster score of ith sentences belonging to jth cluster is the size of jth cluster and sq is the similarity between two sentences si and sq such that sisq note that the value of sq is computed using either measures cosine similarity or euclidean distance just as same as the measure used in the clustering algorithm next the summarizer ranks the sentences of each cluster based on the within cluster scores for each cluster ci top ranked sentences are extracted according to ni the summarizer arranges the selected sentences in the same order they appear in the input text and produces the final summary experiments and results evaluation corpora and metrics we randomly retrieve and articles from biomed central to construct development and evaluation corpora respectively the abstract of each article is used as the model summary this approach of creating corpora has been widely adopted in biomedical text summarization according to the size of both the corpora is large enough to allow the results to be statistically significant we use the rouge toolkit to assess the quality of summaries produced by automatic methods higher scores returned by rouge metrics refer to higher content overlap between system and model summaries in our evaluations we use and metrics and quantify the content overlap in terms of shared unigrams and bigrams respectively parameterization the parameter k specifies the number of final clusters in the clustering algorithm a similarity measure is used in both the sentence clustering and summary generation steps we assess the performance of our summarization method in comparison to other summarizers we evaluate the performance of our summarization method against four comparison methods i e cibs the bayesian biomedical summarizer summa and cibs uses umls concepts in combination with itemset mining and clustering to identify and extract important sentences the bayesian summarizer applies a probabilistic heuristic on concepts to produce an informative summary summa and texlexan employ generic features such as the length and position of sentences the frequency of terms the presence of cue terms table presents rouge scores obtained by the methods the bert based summarizer reports the highest scores compared to the scores obtained by the comparison methods the bert based summarizer can the performance of biomedical text summarization according to a wilcoxon signed rank text with a confidence interval of significantly improve conclusion the results show that contextualized embeddings learnt by bert can be effectively used for biomedical text summarization it is shown that this type of contextual representations can convey the context of sentences more accurately than the comparison methods that utilize sources of domain knowledge this study can be an initial step toward employing this type of language models for developing domain specific nlp systems text summarization to extend our research we plan to utilize this type of language models trained on biomedical text corpora and investigate their usefulness in biomedical text summarization future work may include the usage of contextual representations to address problems such as biomedical named entity recognition question answering and information extraction that need to accurately capture the context of text biomedical especially in references m gambhir and v gupta recent automatic text summarization techniques a survey artificial intelligence review vol pp k cosine similarity euclidean distance table rouge scores obtained by the based summarizer in parameterization experiments bert based summarizer cibs bayesian summarizer summa texlexan table rouge scores obtained by our based summarizer and the comparison methods different settings varying the number of clusters in the range and using measures of cosine similarity and euclidean distance separately for brevity reasons we only report results obtained when the summarizer utilizes bert large since the scores are higher than those of bert base in all experiments the compression rate is set to table presents the rouge scores obtained by the summarizer using different settings the scores are presented for both the cosine similarity and euclidean distance the summarizer obtains the highest scores when for smaller values of k some important sentences are merged with sentences of larger clusters they may lose their chance for inclusion in the summary in this case some informative sentences may be excluded from summaries leading to a decrease in the quality of summarization on the other hand when higher values are assigned to k some unimportant sentences leave large clusters construct a new cluster and contribute to the summary in this case a number of non informative sentences may appear in the summary decreasing the scores sourceforge for arxiv transformers language preprint bidirectional understanding c lin looking for a few good metrics automatic summarization evaluation how many samples are enough in ntcir h saggion summa a robust and adaptable summarization tool traitement automatique langues vol for recent journal approach identifying review of topic based of to intelligence text m moradi cibs a biomedical sentence summarizer using clustering biomedical informatics vol pp m moradi and n ghadiri different approaches important concepts in probabilistic biomedical text summarization artificial intelligence in medicine vol pp r mishra j bian m fiszman c r weir s jonnalagadda j mostafa al text summarization in the biomedical domain a research systematic journal of biomedical informatics vol pp l plaza a daz and p gervs a semantic biomedical graph based in summarisation artificial medicine vol pp m moradi and n ghadiri quantifying the informativeness for biomedical literature summarization an itemset mining method computer methods and programs in biomedicine vol pp m moradi frequent itemsets as meaningful for summarizing events biomedical texts in international conference on computer and knowledge engineering iccke pp m moradi concept based and text multi document summarization isfahan university of technology w w fleuren and w alkema application of text mining in the biomedical domain methods vol pp j turian l ratinov and y bengio word representations a simple and general method for semi supervised learning presented at the proceedings of the annual meeting of for computational linguistics uppsala sweden m e peters w ammar c bhagavatula and sequence r power tagging with bidirectional language models arxiv preprint a radford k narasimhan t salimans and language i understanding by generative pre training url amazonaws us com openai assets covers languageunsupervised language understanding paper pdf semi supervised the association in graphs improving biomedical sutskever m e peters m neumann m iyyer m gardner c clark k lee al deep contextualized word representations arxiv preprint j devlin m chang k lee and k toutanova bert pre training of deep
