p e s p t t s v v x r large scale analysis zipf s law english texts isabel moreno sanchez francesc font clos alvaro recerca edici c campus bellaterra barcelona spain facultat de universitat de barcelona barcelona spain dated september despite paradigm quantitative linguistics zipf s law words suers main problems formulation ambiguous validity tested rigorously statistical point view confronted representatively large number texts summarize current support zipf s law texts anecdotic try solve issues studying dierent versions zipf s law tting available english texts project gutenberg database consisting texts use state art tools tting goodness tests carefully tailored peculiarities text statistics remarkably versions zipf s law consisting pure power law form complementary cumulative distribution function word frequencies able t texts database signicance level domain frequencies maximum value free parameter exponent pacs numbers introduction zipf s law constitutes striking quantitative regularity usage language states large piece text frequency use n word decreases rareness r text r symbol mately hyperbolic way e n denotes proportionality technically r called rank common e rare word signed r second common r slightly general formulation includes parameter form exponent rank frequency relation takes form power law n r value close pattern found dierent guages literary styles time periods levels phological abstraction fascinatingly law claimed codes tion music timbres sounds disparate discrete systems individual units agents gather dierent classes example employees rms believers religions sects plants units mass animals present ecosystems visitors links web pages telephone calls users abundance proteins single cell attempts nd tion diverse solution raised consensus despite quantitative character zipf s law usually checked qualitative way plotting arithm frequency n versus logarithm rank r looking domain roughly ear behavior slope close rened approach consists tting straight line double logarithmic plot linear regression authors recently pointed limitations method applied probability distributions advantages cally unbiased minimum variance procedure maximum likelihood ml estimation tions invariant reparameterizations consider ml estimation reliable procedure estimation parametric models maximum likelihood exist number data large furthermore particular case linguistics search zipf s law traditionally performed limited sets texts dozen typical research article recently large corpora considered representative collections dierent texts aggregated single bag instead separated texts deals enormous mixed text rare words considered e high frequencies zipf s law holds large collections present agreement zipf s law rough approximation lexical statistics range lidity totally unknown e ignore good zipf s law order account appearance words texts work level precision texts fail extra diculty emerges recognizes dened nature zipf s law fact law formulations rst eq counts frequency words sake clarity words counted referred word types order distinguish repetition called token second formulation zipf s law arises counting frequency word types performs second statistics counts values frequency repeated word types frequency means frequency n considered random variable realize rank normalized imum value text empirical estimation complementary cumulative distribution function n derivative expression inverse eq yields continuous approximation probability mass function frequency n obtains power law n new exponent fullling yields values close expression given eq fact rst approach followed zipf s usually considered equivalent eq derived continuum limit expressions lent asymptotically large n consequently wants precise natural question follows true zipf s law know priori zipf s laws better describes real texts argue representations eq n eq better statistical purposes dently functional dependency provide clear rank frequency representation given presents diculties peculiar ture rank variable ref zipf like texts randomly generated following eq ing original ranks hidden happens real situation found rank reconstructed sample deviated considerably original ranks taking large values power law happens high probability ing ml estimations exponent highly biased kolmogorov smirnov test rejected power law hypothesis original ranks power law argue problem escaped upper truncated power law distribution ducing additional parameter truncation order avoid inconsistency rank sentation high values second problem rank true random variable values assigned posteriori sample e text analyzed means rank statistical uctuations ra rb frequency larger construction frequency b necessarily happen true random variable negative correlation tween variable frequency occurrence makes power law hypothesis harder reject fact ated p values uniformly distributed found tting truncated power laws simulated power law rank frequency representations problem avoided choosing low upper truncation parameter yielding short range ranks uctuations little expense disregarding tant data inconvenience impossibility ization non truncated power law comprises values exponent smaller yields sity introducing truncation parameter artifactual e present real system leads conclusion reliable method parameter estimation ml frequentist framework directly applied rank frequency sentation contrast representation terms distribution frequencies devoid problems n dened random variable representation paper statistical ference alternative arguments ref purpose paper quantify large big data scale dierent versions zipf s law ranges validity section present tify zipf like distributions going briey explain selected tting method goodness test corpus texts ation detailed following section presents results special attention statistical cance dependence text length finally end conclusions technical appendices zipf like distributions n changing implicit introduction contrast continuous random variables discrete case power law probability mass function lead power law complementary lative distribution survival function versa let specify denition functions n usual convenience usual strict inequality sign non strict inequality relation consider values random variable takes given n discrete starting integer value taking values n innity study x parameter order t distribution tail large n smooth approximate simplication lies n equivalence eqs clearly wrong small n rst distribution consider power law form f n n normalized version eq n denotes hurwitz zeta function ensures normalization expressions preliminary analysis terms distribution ref contrast power law leads second case n n n note corresponds power law empirical rank frequency relation finally interesting consider frequency distribution derived mandelbrot ranks generated independently power law eq denotes gamma function case power law derlying theoretical rank frequency relation note written beta function y analogous expression confuse distribution beta distribution cases easy dened normalized probability distributions n takes values n positive integer limit n yield n referred power law tail power law exponent easy n n stirling s formula main dierence tween distributions smaller values n taking convex shape log scale seen concave concave convex methodology data order t distributions dierent texts test goodness ts use maximum likelihood estimation followed smirnov ks test procedure similar proposed ref xed problems resulting search optimum arise case method ml estimation proceeds following simple way given set data n probability mass function parameterized noted f n n log likelihood function obtained ni ln ni n assuming data points ni dent words calculating likelihood data generated independently f n ml estimation obtained value maximizes undertake case numerically brent s method distribution log likelihood function takes ln g g simple form geometric mean set ni tions closed form expression possible use eq directly mentioned goodness test kolmogorov smirnov statistic discrete case value calculated monte carlo simulations fact value exponent calculated data going tested theoretically computed p value positively biased paper use monte carlo simulations test proper simulation distributions explained appendix ber small value leads rejection t perform multiple testing incorporate bonferroni like correction fact corrections increase number non rejected null hypothesis decrease ber type errors inating performance ts case goodness tests like corrections acceptance e non rejection ts strict order apply methodology consider set texts stored encoding project gutenberg database accessed july texts correspond dierent languages styles time periods works literature western cultural tradition parts text pertain piece sideration copyright notes headers removed automatized process books tered step mainly dard delimiters discarded total e perform study restrict subset texts english represent e important characteristic text length l counting number word tokens contains turns database l expands small values tokens distribution shown fig observe roughly uniform tion l decay consider english texts consist word tokens smaller texts statistical value texts select actual word types punctuation signs numbers character dierent letters considered count frequencies n primary object study values frequencies text able gshare order facilitate reproducibility results summary apply described tting goodness procedure ml estimation kolmogorov smirnov test total texts english project gutenberg database ferent possibilities distribution frequencies eq eq eq yields ts associated p values total analyze interpret follows results contrary previous studies number texts considered order tens scale approach taken work requires statistical analysis tting results case case tation hand rst focus distribution p values fig fig texts truly generated mechanism following given distribution corresponding p values distribution uniformly distributed zero seen fig case furthermore texts small p values ts fig estimation probability density function text length l english project gutenberg database logarithmic binning bins decade texts tokens considered power law t tail yields exponent distributions texts yield high p values implies conclude database generated distributions rejected good descriptions large subsets database garding distribution clear histogram p values discarded good description distribution frequencies non negligible subset texts concentrate remaining options eventually quantify better describes corpus essence interested version zipf s law distribution ts better reasonable ber texts range validity simple parameter distributions outcome independently signicance level long resolution given number monte carlo simulations tio number texts tted distribution tted nearly constant taking value example considering signicance level e minimum p value equal fig shows distribution ts texts tribution ts percentages include texts tted distributions taneously number stant ratio decreases signicance level increased implicit values fig given aforementioned ratio independent signicance level fair distribution provides compared better description database visual tion performance ts display fig word frequency distribution longest texts p distributions fig histograms p values obtained like distributions tted texts english project gutenberg histograms count number texts bin width note poor performance distribution best performance power law approximations histograms respective exponents shown guide eye fig complementary cumulative distributions e vival functions p values obtained tions tted texts english project gutenberg corresponds normalization integral previous gure included fourth curve fraction texts p values ts higher value marked abscissa note values p play role signicance level value p shown order higher resolution question address dependence performance ts text length l order asses note shape histograms fig distinguish groups texts lie fig complementary cumulative distribution bility mass function texts chronicle london letters charles mary lamb edited e v lucas popular history france f guizot texts earliest times vol ones largest length l respectively fulll p ts respectively exponent takes values case numberoftextsp massfunctionts zero bin p value strictly rest taking group e texts p partitioning dierent subsets according text length e looking distribution p conditioned p dierent ranges l holds shape resulting distribution p strongly depend l shown fig contrast number texts yield p value near zero certainly varies l fig order compare performances function text length l consider single value signicance level greater zero results signicance level relative terms fig shows distribution ts texts distribution small values l tokens larger texts distribution clearly outperforms distribution relevant l signicance level distribution able t texts l larger gure shows case matter signicance level collapse curves fig conrms fact fig infer signicance level equal stability performance ts dierent signicance levels arises served fact distributions p values conditioned p nearly identical dierent l shown fig order check consistency tting procedure perform direct comparison models likelihood ratio lr test apply test texts tted considering signicance level tions log likelihood ratio distributions ln n null hypothesis models equally good describe data mally distributed zero mean variance estimated n variance random ln large absolute values variable ln lead rejection null hypothesis table merges results lr test ous procedure based ml estimation plus ks test total number texts previously tted displayed depending sign ing log ratio account sign obtained value product statistical uctuations true value zero sign trusted order discriminate models bility null hypothesis obtaining absolute value log ratio greater empirical value fig estimated probability density functions p values conditioned p separating dierent ranges text length l values correspond tting word cies distribution distribution vide distribution text length intervals texts distribution rst seven groups length displayed value statistics distribution p values greater displayed fig distribution happens groups intervals li range computed erfc complementary error function statistically signicant yield plr equivalently signicance level signicant absolute value greater rc results shown table ii note lr test conclude t good bad compares relative performance ts words lr test selects particular distribution distribution yield bad t absolute terms mismatch fig number texts p value near zero p dierent ranges l divided number texts ranges ts distributions values l denote geometric mean ranges containing texts higher value t l tokens denotes worst performance results tests time ml ks method selects distribution lr test ther supports selection signicant results selects option shown table ii exclusively exclusively total lr total ml ks table number texts tted signicance level ml ks procedure separated columns according sign positive means likelihood greater conversely negative sign indication signicance signicant lr tests table ii taking texts frequency distributions approximated draw attention distribution estimated exponents e parameter original formulation zipf s law plies fig shows certainly tributed bell like shape check eect text length l distribution nd decreasing trend l seen fig tested observation artifact fig histograms showing fraction accepted texts distributions function text length dierent signicance levels upper curves middle lower concrete range l ratio number texts p number texts range calculated curves removing distribution rescaling rescale yaxis number p case showing relative performance t regard l independent signicance level bins selected contain texts exclusively exclusively total lr test rc rc table ii number texts signicant lr test level favouring distribution rc distribution rc dierent outcomes ml ks procedure level note cases correspond subset previous table tional row shows number texts tted distribution notice case signicant lr test guarantee good t fig estimation probability density nent texts yielding p t t curves calculated histograms normal nel smoothing method implemented matlab ksdensity function estimated mean standard deviation respectively t tting method synthetic texts generated xed behavior cal explanation fact notice trend disagreement claims ref stability exponent demonstrated single growing text e comparing small parts text conclusions zipf s law probably intriguing time studied experimental law quantitative linguistics extremely popular wider sense science complex systems previous literature vast far know work constitutes rst large scale analysis zipf s law single aggregated texts position grounded statement validity zipf s law texts let rst briey summarize key nical points study analyzed total english texts project database rigorous tting procedures tested described zipf like tributions choice distributions haustive limited dierent terpretations understood zipf s law sense having perfect power law probability mass function word frequencies complementary cumulative distribution function empirical estimation leads rank frequency relation sample rank frequency relation fig estimated probability density ts dierent length ranges divided groups accepted texts percentiles according l previous gure normal kernel smoothing method applied distribution distribution underlying population remarkably resulting butions unique parameter cases exponent asymptotic power law bility mass function frequency left explore complicated extensions zipf s law form large corpus obvious cluding additional parameters provide good ts larger number texts case proper model selection require balance number parameters parsimony aim paper t texts possible test performance simplest like distributions strict conservative work requiring versions zipf s law hold range frequencies n tail distribution selves strictest range demands markable e standard signicance level text lengths word kens considered texts cally compatible pure power law mentary cumulative distribution function represented distribution fig state corpus consideration appropriate version zipf s law given probability mass function n n equivalently complementary cumulative bution function n broad coverage project gutenberg corpus speculate distribution t large fraction generic non technical english texts course testing speculation possible corpora impossible task shown conclusions relative performance pure power law ability mass function given distribution distribution robust respect changes signicance level twice texts tically compatible distribution patible signicance level obviously absolute terms number accepted texts varies signicance level conclude tribution gives better description english texts distribution corpus considered work conclude distribution rst derived mandelbrot irrelevant tion texts corpus finally corroborated exponent zipf s law certainly varies text text previously claimed approaches dening zipf s law ingly value originally proposed zipf frequent ones believe analysis constitutes major ment understanding zipf s law astonishing good simplest parameter zipf like tions perform large set texts particularly strict set requirements imposed sharp contrast instance zipf s law raphy distribution income power law valid tail sponding largest sizes happens distribution word frequency large text corpora mentioned zipf s law subject debate probably continue years cast doubt validity basis particular examples clear modern times big data large computational pabilities eorts large scale analysis zipf s law hope paper constitutes rst step direction appendix simulation discrete zipf like distributions testing procedure need simulated ples discrete distributions dened n recipe simulation arbitrary positive integer value lower cut o simpler start auxiliary distribution simulation simulation fixed given parameter want set random numbers cumulative tion function discrete power law rst generate random number u uniform distribution interval umax umax turns values yield continuous power law sc script c distinguishes continuous distribution discrete analogous taking n equal integer x e n yields discrete distribution desired x n n integer recipe n equivalent x generate u uniform distribution calculate n means called rejection method lated integers distributed following simulation integers following key point achieve high performance rejection method use good auxiliary function e leads low rejection rate certainly case framework explained simulation case steps generate n generate v uniform distribution unit interval n accepted v rejected c rejection constant given c easy check maximum reached n decreasing function acceptance condition simplied taking dition devoid calculation hurwitz zeta function generalization method ref choice auxiliary distribution function justied small value c takes expected number generated values n accept instance c simulation proceeding similarly case low values c c limit maximum numerically seen reached n summary steps generate n generate v uniform distribution unit interval n accepted rejected acknowledgments grateful project gutenberg initiative help maintain alive s pueyo provided bibliographic wisdom zipf s law ecology serra assistance ml estimation m enjoys tract collaborative mathematics project la caixa foundation research projects work included spanish mineco agaur h baayen word frequency distributions kluwer drecht m baroni distributions text ludeling m kyto editors corpus linguistics international handbook volume pages mouton de gruyter berlin d zanette statistical patterns written language arxiv s t piantadosi zipf s law natural language ical review future directions psychon bull rev d zanette m montemurro dynamics text eration realistic zipf s distribution j quant guist corral g boleda r ferrer cancho zipf s law word frequencies word forms versus lemmas long texts plos j corral m boguna m haro j ll cos measuring evolution contemporary western popular music sci rep m haro j p herrera corral zipf s law short time timbral codings speech music environmental sound signals plos w li zipf s law glottom r l axtell zipf distribution u s rm sizes science clauset c r shalizi m e j newman law distributions empirical data siam rev s pueyo r jovani comment keystone tualism drives pattern power function science j camacho r v sole scaling ecological size spectra europhys lett l adamic b huberman zipf s law internet glottometrics m e j newman power laws pareto distributions zipf s law cont phys c furusawa k kaneko zipf s law gene sion phys rev lett h simon class skew distribution functions biomet g miller eects intermittent silence j psychol r ferrer cancho r v sole eort origins scaling human language proc natl acad sci u s m mitzenmacher brief history generative els power law lognormal distributions internet math saichev y malevergne d sornette theory zipf s law general power law distributions gibrat s law proportional growth lecture notes economics mathematical systems springer verlag berlin b corominas murtra j fortuny r v sole gence zipf s law evolution communication phys rev e j peterson p d dixit k dill maximum tropy framework nonexponential distributions proc natl acad sci usa b corominas murtra r hanel s thurner derstanding scaling history dependent processes collapsing sample space proc natl acad sci usa r ferrer cancho b random texts exhibit real zipf s law like rank distribution plos r dickman n r moloney e g altmann sis information theoretic model communication j stat mech theory exp page w li p miramontes g cocho fitting ranked linguistic data parameter functions entropy m l goldstein s morris g g yen problems tting power law distribution eur phys j b h bauke parameter estimation power law tions maximum likelihood methods eur phys j b e p white b j enquist j l green ing exponent power law frequency distributions ecol g casella r l berger statistical inference duxbury pacic grove edition deluca corral fitting goodness test non truncated truncated power law distributions acta geophys f font clos g boleda corral scaling law zipf s law relation heaps law new j phys r ferrer cancho r v sole regimes frequency words origin complex lexicons zipf s law revisited j quant linguist m petersen j n tenenbaum s havlin h e ley m perc languages cool expand lometric scaling decreasing need new words sci rep m gerlach e g altmann stochastic model vocabulary growth natural languages phys rev x j r williams j p bagrow c m danforth p s dodds text mixing shapes anatomy frequency distributions modern zipan mechanics natural language arxiv b mandelbrot theory word frequencies related markovian models discourse r son editor structure language mathematical aspects pages american mathematical society providence ri corral r ferrer cancho preparation n kolmogorov foundations theory bility chelsea pub co new york edition e g altmann m gerlach statistical laws guistics arxiv f font clos corral log log convexity type token growth zipf s systems phys rev lett m abramowitz stegun editors handbook mathematical functions dover new york y pawitan likelihood statistical modelling inference likelihood oxford oxford w h press s teukolsky w t vetterling b p flannery numerical recipes c cambridge university press cambridge edition corral f font j camacho non characteristic half lives radioactive decay phys rev e corral deluca r ferrer cancho cal recipe t discrete power law distributions arxiv h abdi bonferroni sidak corrections multiple comparisons n j salkind editor encyclopedia measurement statistics pages sage sand oaks j m bland d g altman multiple signicance tests bonferroni method brit med j y benjamini y hochberg controlling false covery rate practical powerful approach tiple testing j roy stat roc b project gutenberg gutenberg org wikipedia accessed august wikipedia org wiki project gutenberg q h vuong likelihood ratio tests model selection non nested hypotheses econometrica y malevergne v pisarenko d sornette testing pareto lognormal distributions uniformly powerful unbiased test applied tribution cities phys rev e dragulescu v m yakovenko exponential power law probability distributions wealth income united kingdom united states physica l devroye non uniform random variate generation springer verlag new york
