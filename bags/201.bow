hierarchical transformers multi document summarization yang liu mirella lapata institute language cognition computation school informatics university edinburgh yang abstract paper develop neural rization model effectively process multiple input documents distill tive summaries model augments ously proposed transformer architecture liu ability encode ments hierarchical manner represent cross document relationships attention mechanism allows share information opposed simply concatenating text spans processing sequence model learns latent dependencies tual units advantage plicit graph representations focusing larity discourse relations empirical results wikisum dataset demonstrate proposed architecture brings substantial provements strong baselines introduction automatic summarization enjoyed renewed interest recent years thanks ity neural network models ability learn continuous representations recourse preprocessing tools linguistic annotations availability large scale datasets sandhaus hermann grusky containing hundreds thousands summary pairs driven development neural architectures summarizing single uments approaches shown ing results sequence sequence models encode source document decode abstractive summary ilmaz paulus gehrmann multi document summarization task producing summaries clusters code data available com nlpyang hiersumm cally related documents received icantly attention partly paucity suitable data application learning methods high quality multi document rization datasets document clusters paired multiple reference summaries written mans produced document derstanding text analysis conferences duc tac relatively small range examples training ral models attempt drive research ther liu tap potential wikipedia propose methodology ating large scale dataset wikisum document summarization hundreds sands instances wikipedia articles specically lead sections viewed summaries topics indicated title florence natural language processing documents cited wikipedia articles web pages returned google section titles queries seen source cluster lead section purports summarize aside difculties obtaining ing data major obstacle application end end models multi document tion sheer size number source uments large result practically infeasible given memory limitations current hardware train model codes vectors subsequently generates summary liu propose stage architecture tive model rst selects subset salient passages subsequently abstractive model generates summary conditioning extracted subset selected passages concatenated sequence transformer vaswani architecture suited guage modeling long sequences decode summary related work model liu takes important rst step abstractive document summarization considers multiple input documents concatenated sequence agnostic hierarchical tures relations exist uments example different web pages repeat content include additional tent present contradictory information discuss fact different light radev realization cross document links portant information nating redundancy creating overall coherent summaries led widespread adoption graph based models multi document marization erkan radev christensen wan parveen strube graphs conveniently capture ships textual units document lection easily constructed sumption text spans represent graph nodes edges semantic links isolating salient paper develop neural tion model effectively process ple input documents distill abstractive maries model augments previously posed transformer architecture ability encode multiple documents hierarchical ner represent cross document relationships attention mechanism allows share information multiple documents opposed simply concatenating text spans feeding sequence model way model automatically learns richer tural dependencies textual units corporating established insights earlier work advantageously proposed architecture easily benet information external model replacing inter document attention graph matrix computed based basis lexical similarity erkan radev discourse relations christensen evaluate model wikisum dataset experimentally proposed tecture brings substantial improvements eral strong baselines dition simple ranking module scores documents based usefulness target summary greatly boost performance multi document summarization system previous multi document summarization methods extractive operating graph based representations sentences passages proaches vary depending edge weights computed based cosine similarity idf weights words erkan radev discourse relations christensen specic algorithm adopted ranking text units inclusion nal summary eral variants pagerank algorithm adopted literature erkan radev order compute importance salience passage recursively based entire graph recently yasunaga propose neural version framework salience estimated features extracted tence embeddings graph convolutional works kipf welling applied relation graph representing cross document links abstractive approaches met limited success systems generate summaries based sentence fusion technique ties fragments conveying common information documents combines tences barzilay mckeown filippova strube bing neural abstractive models achieved ing results single document summarization paulus gehrmann celikyilmaz tension sequence sequence architectures multi document summarization ward apart lack sufcient training data neural models face computational challenge processing multiple source ments previous solutions include model fer zhang lebanoff liu sequence sequence model pretrained single document summarization data tuned duc multi document benchmarks unsupervised models relying reconstruction jectives chu liu liu propose methodology constructing large scale summarization datasets stage model rst extracts salient information source documents uses decoder architecture attend long sequences generate summary low setup viewing multi document marization supervised machine learning troduced vaswani generates summary token token attending source input use beam search length penalty decoding process generate uent longer maries paragraph ranking unlike liu rank paragraphs based similarity title based cosine similarity adopt based approach logistic regression model applied paragraph calculate score dicating selected rization use recurrent neural networks long short term memory units lstm hochreiter schmidhuber represent tle source paragraph utm wtm upn wpn wti wpj word embeddings tokens uti upj updated vectors token applying lstms max pooling operation title vectors obtain xed length representation utm concatenate vector upi ken paragraph apply non linear formation extract features matching title paragraph second max pooling tion yields nal paragraph vector finally estimate paragraph selected use linear transformation moid function score indicating graph summarization input paragraphs receive scores model trained minimizing cross entropy loss ground truth scores denoting relatedness paragraph gold standard summary adopt recall paragraph figure pipeline multi document tion system source paragraphs rst ranked ones serve input encoder decoder model generates target summary lem purpose assume access large labeled datasets source documents summary contrast approach use pairs learning based ranker abstractive model hierarchically encode input documents ability learn latent relations uments additionally incorporate information encoded known graph representations model description follow liu treating eration lead wikipedia sections document summarization task input pothetical system title wikipedia cle collection source documents output wikipedia article rst section source documents webpages cited erences section wikipedia article search results returned google title article query source documents relatively long split multiple paragraphs line breaks formally given title input paragraphs retrieved wikipedia citations search engine task generate lead section wikipedia article summarization system illustrated ure input paragraphs numerous possibly lengthy instead directly applying abstractive system rst rank marize ones summarizer follows successful encoder decoder architecture bahdanau encoder codes input text hidden representations decoder generates target summaries based representations paper focus exclusively encoder model decoder follows transformer architecture ranked paragraphssource paragraphsparagraph rankerencoderpara para ldecoderabstractive summarizertarget summary gold target text testing input graphs ranked based model predicted scores ordering ated rst paragraphs selected input second abstractive stage paragraph encoding instead treating selected paragraphs long sequence develop cal model based transformer architecture vaswani capture inter paragraph relations model composed cal global transformer layers stacked freely let tij denote token ranked paragraph model takes vectors tokens input layer input output written embeddings input tokens rst represented word dings let wij denote embedding signed tij transformer recurrent model assign special tional embedding peij tij indicate sition token input calculate positional embeddings follow vaswani use sine cosine tions different frequencies embedding element sequence indicates dimension bedding vector dimension positional encoding corresponds sinusoid xed offset represented linear function enables model distinguish relative positions input elements multi document summarization token tij positions need considered rank paragraph position token paragraph positional embedding peij represents positions concatenation added word ding wij obtain nal input vector peij wij peij local transformer layer local transformer layer encode textual information tokens graph local transformer layer vanilla transformer layer vaswani composed sub layers layernorm layer normalization posed mhatt head attention mechanism introduced vaswani allows token attend tokens different attention tions ffn layer feed forward work relu hidden activation function global transformer layer global transformer layer exchange formation multiple paragraphs shown figure rst apply multi head pooling eration paragraph different heads code paragraphs different attention weights head inter paragraph attention mechanism applied paragraph collect information paragraphs attention generating context vector capture contextual information input nally context vectors concatenated linearly transformed added vector token fed feed forward layer updating resentation token global information multi head pooling obtain xed length paragraph representations apply pooling operation instead resentation paragraph introduce multi head pooling mechanism paragraph weight distributions tokens calculated allowing model exibly encode paragraphs different representation subspaces attending different words let denote output vector transformer layer token tij input current layer paragraph head nhead rst form input vectors attention scores value vectors head calculate probability distribution tokens paragraph based attention scores rdheadd weights dhead nhead dimension head number tokens apply weighted summation linear transformation layer tion obtain vector headz paragraph headz ijbz rdheaddhead weight model exibly incorporate multiple heads paragraph having multiple tention distributions focusing different views input inter paragraph attention model pendencies multiple paragraphs inter paragraph attention mechanism similar self attention inter paragraph attention allows paragraph attend paragraphs calculating attention distribution headz headz headz contextz rdheaddhead query key value vectors linearly formed headz vaswani rdhead represents context contextz tor generated self attention operation paragraphs number input graphs figure provides schematic view inter paragraph attention feed forward networks update token representations contextual information rst fuse information heads nating context vectors applying linear transformation weight rdd contextnhead figure global transformer layer different ors indicate different heads multi head pooling inter paragraph attention add input token vector feed layer feed forward network relu activation function way layer normalization gij rdf rddf weights hidden size feed forward later way token paragraph collect information paragraphs hierarchical efcient manner graph informed attention inter paragraph attention mechanism viewed learning latent graph representation self attention weights input paragraphs previous work shown lar latent representations benecial stream nlp tasks liu lapata kim williams niculae fernandes work multi document summarization taken tage explicit graph representations ing different facets summarization task multi head poolingmulti head poolinghead paragraph attentioninter paragraph attentioninter paragraph attentioncontext thisisparaonefeed forwardfeed forwardfeed forwardfeed forwardcontext thisisparatwofeed forwardfeed forwardfeed forwardfeed forwardthisisparaonethisisparatwo capturing redundant information senting passages referring event entity advantage hierarchical easily incorporate graphs ternal model generate better summaries experimented established graph representations discuss briey low inherent model restricts graph eling relationships paragraphs instead rst graph aims capture lexical relations graph nodes correspond graphs edge weights cosine similarities based idf representations paragraphs second graph aims capture discourse builds lations christensen approximate discourse graph adg yasunaga paragraphs edges graphs drawn counting occurring tities discourse markers connecting adjacent paragraphs appendix details adgs constructed represent graphs matrix weight edge connecting paragraphs inject graph hierarchical transformer simply tuting learned heads tion calculating context vector head modied gio experimental setup wikisum dataset scripts urls provided liu crawl wikipedia articles source reference documents cessfully crawled original documents urls invalid ing documents retrieved ther removed clone paragraphs exact copies parts wikipedia articles paragraphs source documents bigram recall target summary higher average input paragraphs paragraph tokens average length target mary tokens split dataset instances training dation test methods rouge recall similarity ranking table rouge recall target summary paragraphs obtained idf cosine ity ranking model ranking summarization stages encode source paragraphs target maries subword tokenization piece kudo richardson lary consists subwords shared source target paragraph ranking train regression model calculated recall lin paragraph target mary ground truth score hidden size lstms set dropout dropout probability linear layers adagrad duchi learning rate optimization compare ranking model method proposed liu use idf cosine similarity paragraph article title rank input paragraphs rst paragraphs ordered paragraph set produced ranker similarity based method respectively concatenate paragraphs calculate rouge recall gold target text results shown table ranker effectively extracts related paragraphs produces informative input stream summarization task training conguration abstractive els apply dropout probability fore linear layers label smoothing szegedy smoothing factor training traditional sequence sequence manner maximum likelihood estimation optimizer adam kingma learning rate applied learning rate warmup rst steps decay vaswani transformer based models den units feed forward hidden size layers models trained gpus nvidia titan steps model lead lexrank tokens ranking tokens tokens tokens dmca tokens tokens tokens similarity graph tokens discourse graph train tokens test tokens rouge table test set results wikisum dataset rouge gradient accumulation training time models approximately consistent selected best checkpoints based performance validation set report averaged results test set decoding use beam search beam size length penalty decode end sequence token reached comparison systems compared transformer posed hierarchical strong baselines lead simple baseline concatenates tle ranked paragraphs extracts rst tokens set length ground truth target lexrank erkan radev graph based extractive summarizer build graph paragraphs nodes edges weighted idf cosine similarity run pagerank like algorithm graph rank select paragraphs length ground truth summary reached flat transformer baseline applies transformer based encoder decoder model token sequence layer transformer title ranked paragraphs concatenated truncated tokens dmca best performing model liu shorthand transformer decoder memory compressed tion transformer decoder compressed key value attention convolutional layer model layers liu hidden size feed forward hidden size title ranked paragraphs concatenated truncated tokens hierarchical transformer model proposed paper model tecture layer network attention layers global tention layers model takes title paragraphs input produce target summary leads proximately input tokens instance results automatic evaluation evaluated rization quality rouge lin report unigram bigram overlap means assessing mativeness longest common subsequence rouge means assessing uency table summarizes results rst block table includes extractive systems lead lexrank second block includes eral variants flat transformer based models dmca rest table presents results hierarchical transformer seen abstractive models generally perform extractive ones flat transformer achieves best results input length set tokens longer input kens actually hurts performance transformer input tokens model model lead dmca rating table hierarchical transformer versions thereof paragraph position multi head pooling global transformer layer table system scores based questions answered amt participants summary quality rating forms dmca presented tokens adding external graph help summarization cess similarity graph vious inuence results discourse graph boosts rouge found performance erarchical transformer improves model presented longer input test time shown row table ing input tokens summarization quality improves board suggests model potentially generate better summaries increasing training time table summarizes ablation studies aiming assess contribution individual components experiments conrmed encoding graph position addition token position paragraph benecial row multi head pooling model number heads set global transformer layer model local transformer layers encoder human evaluation addition automatic evaluation assessed system performance eliciting human judgments randomly lected test instances rst evaluation study quantied degree summarization models retain key information documents following question answering paradigm clarke lapata narayan created set questions based gold summary assumption contains important information input graphs examined participants able answer questions reading system summaries access gold summary questions system swer better summarization ated questions total varying case transformer models questions gold summary examples questions answers given table adopted scoring mechanism clarke lapata correct answers marked partially correct ones system score average question scores second evaluation study assessed quality summaries asking pants rank taking account lowing criteria informativeness mary convey important facts topic question fluency summary uent grammatical succinctness mary avoid repetition best worst ing louviere labor intensive alternative paired comparisons shown produce reliable results rating scales kiritchenko mohammad ticipants presented gold summary summaries generated systems asked decide summary best worst relation gold standard taking account criteria mentioned rating system computed percentage times chosen best minus times selected worst ratings range worst best evaluations conducted zon mechanical turk platform responses hit participants evaluated summaries duced lead baseline flat transformer dmca hierarchical transformer evaluated systems variants achieved best performance automatic evaluations shown table evaluations participants overwhelmingly prefer model wise comparisons systems statistically signicant way anova tukey hsd tests examples system output provided table pentagoet archeological district pentagoet archeological district national historic landmark district located southern edge bagaduce peninsula castine maine site fort pentagoet century fortied trading post established fur traders french acadia site center trade local abenaki marked effective western border acadia new england site english control returned france treaty breda fort destroyed dutch raiders site designated national historic landmark public park national historic landmark district pentagoet archeological district castine maine located abenaki indians use site trading center pentagoet archeological district national historic landmark district located castine maine district forms traditional homeland abenaki indians particular penobscot tribe colonial period abenakis frequented fortied trading post site bartering moosehides sealskins beaver furs exchange european commodities pentagoet archeological district national historic landmark district located southern edge bagaduce peninsula treaty breda pentagoet archeological district national historic landmark district located southern edge bagaduce peninsula treaty breda listed national register historic places pentagoet archeological district national historic landmark district located castine maine district forms traditional homeland abenaki indians particular penobscot tribe district listed national register historic places pentagoet archeological district national historic landmark district located castine maine district forms traditional homeland abenaki indians particular penobscot tribe colonial period abenaki frequented fortied trading post site bartering moosehides sealskins beaver furs exchange european commodities melanesian whistler melanesian whistler vanuatu whistler pachycephala chlorura species passerine bird whistler family pachycephalidae found loyalty islands vanuatu vanikoro far eastern solomons found melanesian whistler species passerine bird whistler family pachycephalidae loyalty islands vanuatu vanikoro far south eastern solomons australian golden whistler pachycephala pectoralis species bird found forest woodland mallee mangrove scrub australia interior north populations resident south eastern australia migrate north winter melanesian whistler caledonica species bird family muscicapidae endemic melanesia australian golden whistler pachycephala chlorura species bird family pachycephalidae endemic fiji melanesian whistler pachycephala chlorura species bird family pachycephalidae endemic fiji table gold human authored summaries questions based answers shown square brackets automatic summaries produced baseline flat transformer dmca liu hierachical transformer conclusions paper conceptualized abstractive document summarization machine learning problem proposed new model able encode multiple input documents chically learn latent relations ditionally incorporate structural information known graph representations demonstrated importance learning based approach selecting documents marize experimental results model produces summaries uent formative outperforming competitive systems wide margin future like apply hierarchical transformer question answering related textual inference tasks acknowledgments like thank laura perez beltrachini help preprocessing dataset research supported google phd ship rst author authors gratefully knowledge nancial support european research council award number references jimmy lei jamie ryan kiros geoffrey arxiv preprint ton layer normalization dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly learning align translate proceedings international conference learning resentations san diego california regina barzilay kathleen mckeown sentence fusion multidocument news rization computational linguistics lidong bing piji liao wai lam weiwei guo rebecca passonneau abstractive document summarization phrase selection merging proceedings annual ing association computational linguistics international joint conference ral language processing volume long papers pages beijing china asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents proceedings abstractive summarization conference north american chapter association computational linguistics man language technologies volume long pers pages new orleans louisiana janara christensen mausam stephen soderland coherent oren etzioni proceedings document summarization conference north american chapter association computational linguistics man language technologies pages lanta georgia association computational guistics eric chu peter liu unsupervised neural multi document abstractive summarization arxiv preprint james clarke mirella lapata discourse constraints document compression tional linguistics john duchi elad hazan yoram singer adaptive subgradient methods online learning journal machine stochastic optimization learning research katja filippova michael strube sentence fusion dependency graph compression ceedings conference empirical ods natural language processing pages honolulu hawaii sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference cal methods natural language processing pages brussels belgium max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies proceedings conference north american chapter association computational linguistics man language technologies volume long pers pages new orleans louisiana karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend advances ral information processing systems pages curran associates inc sepp hochreiter jurgen schmidhuber neural computation long short term memory yoon kim carl denton luong hoang der rush structured attention networks proceedings international conference learning representations toulon france diederik kingma jimmy adam method stochastic optimization arxiv preprint thomas kipf max welling supervised classication graph convolutional proceedings international networks conference learning representations san juan puerto rico svetlana kiritchenko saif mohammad best worst scaling reliable rating scales case study sentiment intensity annotation proceedings annual meeting sociation computational linguistics pages vancouver canada gunes erkan dragomir radev lexrank graph based lexical centrality salience text summarization journal articial intelligence search taku kudo john richardson sentencepiece simple language independent subword enizer detokenizer neural text processing arxiv preprint patrick fernandes miltiadis allamanis marc brockschmidt structured neural tion proceedings international ference learning representations new orleans louisiana logan lebanoff fei liu automatic tion vague words sentences privacy cies proceedings conference pirical methods natural language processing pages brussels belgium chin yew lin rouge package automatic text summarization evaluation summaries branches proceedings shop pages barcelona spain association computational linguistics peter liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser noam shazeer generating wikipedia ing long sequences proceedings national conference learning representations vancouver canada yang liu mirella lapata learning tured text representations transactions ciation computational linguistics jordan louviere terry flynn anthony fred john marley best worst scaling ory methods applications cambridge sity press shulei zhi hong deng yunlun yang unsupervised multi document summarization framework based neural document model proceedings coling tional conference computational linguistics technical papers pages osaka japan shashi narayan shay cohen mirella lapata ranking sentences extractive tion reinforcement learning proceedings conference north american ter association computational linguistics human language technologies volume long pers pages new orleans louisiana vlad niculae andre martins claire cardie dynamic computation graphs sparse latent structure proceedings conference empirical methods natural guage processing pages brussels gium daraksha parveen michael strube document summarization bipartite graphs proceedings workshop graph based methods natural language cessing pages doha qatar abigail peter liu christopher manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers pages vancouver canada christian szegedy vincent vanhoucke sergey ioffe jon shlens zbigniew wojna rethinking inception architecture computer vision ieee conference computer vision tern recognition cvpr ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need advances neural information cessing systems pages curran ciates inc xiaojun wan exploration document impact graph based multi document tion proceedings conference pirical methods natural language processing pages honolulu hawaii adina williams andrew drozdov samuel bowman latent tree learning models tify meaningful structure sentences tions association computational tics yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey google neural machine translation system bridging gap arxiv preprint man machine translation michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan dragomir radev graph based neural multi document summarization proceedings ence computational natural language learning conll pages vancouver canada romain paulus caiming xiong richard socher deep reinforced model abstractive marization proceedings international conference learning representations ver canada jianmin zhang jiwei tan xiaojun wan adapting neural single document summarization model abstractive multi document tion pilot study proceedings tional conference natural language generation dragomir radev common theory mation fusion multiple text sources step cross document structure sigdial workshop discourse dialogue pages hong kong china evan sandhaus new york times annotated corpus linguistic data consortium philadelphia appendix describe similarity discourse graphs discussed section created graphs added hierarchical model means enhance summary quality section details similarity graph similarity graph based idf cosine similarity nodes graph paragraphs rst represent paragraph bag words calculate idf value vik token tik paragraph paragraphs adjacent source webpage connected discourse markers nal edge weight weighted sum vik count word graph total number paragraphs total number paragraphs taining word obtain idf vector paragraph paragraph pairs calculate cosine similarity idf vectors use weight edge connecting pair graph remove edges weights lower discourse graphs build approximate discourse graph adg follow christensen yasunaga original adg makes use complex features create simplied version features nodes graph paragraphs occurring entities paragraph extract set entities paragraph ner recognizer use entities type person norp fac org gpe loc event work art law paragraph pair count eij number entities exact match discourse markers use following plicit discourse markers identify edges adjacent paragraphs source webpage comparatively thermore time mediately instead sure likewise theless nonetheless notably regardless similarly unlike addition turn exchange case event nally later cially result example fact day api entityrecognizer
