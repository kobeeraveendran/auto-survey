hierarchical transformers for multi document summarization yang liu and mirella lapata institute for language cognition and computation school of informatics university of edinburgh yang ac uk ed ac uk a m l c s c v v i x r a abstract in this paper we develop a neural rization model which can effectively process multiple input documents and distill tive summaries our model augments a ously proposed transformer architecture liu et al with the ability to encode ments in a hierarchical manner we represent cross document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a at sequence our model learns latent dependencies among tual units but can also take advantage of plicit graph representations focusing on larity or discourse relations empirical results on the wikisum dataset demonstrate that the proposed architecture brings substantial provements over several strong baselines introduction automatic summarization has enjoyed renewed interest in recent years thanks to the ity of neural network models and their ability to learn continuous representations without recourse to preprocessing tools or linguistic annotations the availability of large scale datasets sandhaus hermann et al grusky et al containing hundreds of thousands of summary pairs has driven the development of neural architectures for summarizing single uments several approaches have shown ing results with sequence to sequence models that encode a source document and then decode it into an abstractive summary see et al ilmaz et al paulus et al gehrmann et al multi document summarization the task of producing summaries from clusters of code and data is available at com nlpyang hiersumm cally related documents has received icantly less attention partly due to the paucity of suitable data for the application of learning methods high quality multi document rization datasets i e document clusters paired with multiple reference summaries written by mans have been produced for the document derstanding and text analysis conferences duc and tac but are relatively small in the range of a few hundred examples for training ral models in an attempt to drive research ther liu et al tap into the potential of wikipedia and propose a methodology for ating a large scale dataset wikisum for document summarization with hundreds of sands of instances wikipedia articles specically lead sections are viewed as summaries of various topics indicated by their title e florence or natural language processing documents cited in the wikipedia articles or web pages returned by google using the section titles as queries are seen as the source cluster which the lead section purports to summarize aside from the difculties in obtaining ing data a major obstacle to the application of end to end models to multi document tion is the sheer size and number of source uments which can be very large as a result it is practically infeasible given memory limitations of current hardware to train a model which codes all of them into vectors and subsequently generates a summary from them liu et al propose a two stage architecture where an tive model rst selects a subset of salient passages and subsequently an abstractive model generates the summary while conditioning on the extracted subset the selected passages are concatenated into a at sequence and the transformer vaswani et al an architecture well suited to guage modeling over long sequences is used to decode the summary related work although the model of liu et al takes an important rst step towards abstractive document summarization it still considers the multiple input documents as a concatenated at sequence being agnostic of the hierarchical tures and the relations that might exist among uments for example different web pages might repeat the same content include additional tent present contradictory information or discuss the same fact in a different light radev the realization that cross document links are portant information nating redundancy and creating overall coherent summaries has led to the widespread adoption of graph based models for multi document marization erkan and radev christensen et al wan parveen and strube graphs conveniently capture the ships between textual units within a document lection and can be easily constructed under the sumption that text spans represent graph nodes and edges are semantic links between them in isolating salient in this paper we develop a neural tion model which can effectively process ple input documents and distill abstractive maries our model augments the previously posed transformer architecture with the ability to encode multiple documents in a hierarchical ner we represent cross document relationships via an attention mechanism which allows to share information across multiple documents as opposed to simply concatenating text spans and feeding in this them as a at sequence to the model way the model automatically learns richer tural dependencies among textual units thus corporating well established insights from earlier work advantageously the proposed architecture can easily benet from information external to the model i e by replacing inter document attention with a graph matrix computed based on the basis of lexical similarity erkan and radev or discourse relations christensen et al we evaluate our model on the wikisum dataset and show experimentally that the proposed tecture brings substantial improvements over eral strong baselines we also nd that the dition of a simple ranking module which scores documents based on their usefulness for the target summary can greatly boost the performance of a multi document summarization system most previous multi document summarization methods are extractive operating over graph based representations of sentences or passages proaches vary depending on how edge weights are computed e based on cosine similarity with tf idf weights for words erkan and radev or on discourse relations christensen et al and the specic algorithm adopted for ranking text units for inclusion in the nal summary eral variants of the pagerank algorithm have been adopted in the literature erkan and radev in order to compute the importance or salience of a passage recursively based on the entire graph more recently yasunaga et al propose a neural version of this framework where salience is estimated using features extracted from tence embeddings and graph convolutional works kipf and welling applied over the relation graph representing cross document links abstractive approaches have met with limited success a few systems generate summaries based on sentence fusion a technique which ties fragments conveying common information across documents and combines these into tences barzilay and mckeown filippova and strube bing et al although neural abstractive models have achieved ing results on single document summarization see et al paulus et al gehrmann et al celikyilmaz et al the tension of sequence to sequence architectures to multi document summarization is less ward apart from the lack of sufcient training data neural models also face the computational challenge of processing multiple source ments previous solutions include model fer zhang et al lebanoff and liu where a sequence to sequence model is pretrained on single document summarization data and tuned on duc multi document benchmarks or unsupervised models relying on reconstruction jectives ma et al chu and liu liu et al propose a methodology for constructing large scale summarization datasets and a two stage model which rst extracts salient information from source documents and then uses a decoder only architecture that can attend to very long sequences to generate the summary we low their setup in viewing multi document marization as a supervised machine learning troduced in vaswani et al it generates a summary token by token while attending to the source input we also use beam search and a length penalty wu et al in the decoding process to generate more uent and longer maries paragraph ranking unlike liu et al who rank paragraphs based on their similarity with the title using tf based cosine similarity we adopt a based approach a logistic regression model is applied to each paragraph to calculate a score dicating whether it should be selected for rization we use two recurrent neural networks with long short term memory units lstm hochreiter and schmidhuber to represent tle t and source paragraph p utm wtm upn wpn where wti wpj are word embeddings for tokens in t and p and uti upj are the updated vectors for each token after applying the lstms a max pooling operation is then used over title vectors to obtain a xed length representation ut ut utm we concatenate ut with the vector upi of each ken in the paragraph and apply a non linear formation to extract features for matching the title and the paragraph a second max pooling tion yields the nal paragraph vector p pi ut pn finally to estimate whether a paragraph should be selected we use a linear transformation and a moid function s where s is the score indicating whether graph p should be used for summarization all input paragraphs pl receive scores sl the model is trained by minimizing the cross entropy loss between si and ground truth scores yi denoting the relatedness of a paragraph to the gold standard summary we adopt recall of paragraph pi against figure pipeline of our multi document tion system l source paragraphs are rst ranked and the ones serve as input to an encoder decoder model which generates the target summary lem and for this purpose assume access to large labeled datasets i e source documents summary in contrast to their approach we use a pairs learning based ranker and our abstractive model can hierarchically encode the input documents with the ability to learn latent relations across uments and additionally incorporate information encoded in well known graph representations model description we follow liu et al in treating the eration of lead wikipedia sections as a document summarization task the input to a pothetical system is the title of a wikipedia cle and a collection of source documents while the output is the wikipedia article s rst section source documents are webpages cited in the erences section of the wikipedia article and the top search results returned by google with the title of the article as the query since source documents could be relatively long they are split into multiple paragraphs by line breaks more formally given title t and l input paragraphs pl retrieved from wikipedia citations and a search engine the task is to generate the lead section d of the wikipedia article our summarization system is illustrated in ure since the input paragraphs are numerous and possibly lengthy instead of directly applying an abstractive system we rst rank them and marize the ones our summarizer follows the very successful encoder decoder architecture bahdanau et al where the encoder codes the input text into hidden representations and the decoder generates target summaries based on these representations in this paper we focus exclusively on the encoder part of the model our decoder follows the transformer architecture ranked paragraphssource paragraphsparagraph rankerencoderpara l para ldecoderabstractive summarizertarget summary gold target text d as yi in testing input graphs are ranked based on the model predicted scores and an ordering rl is ated the rst paragraphs are selected as input to the second abstractive stage paragraph encoding instead of treating the selected paragraphs as a very long sequence we develop a cal model based on the transformer architecture vaswani et al to capture inter paragraph relations the model is composed of several cal and global transformer layers which can be stacked freely let tij denote the j token in the i ranked paragraph ri the model takes vectors ij for all tokens as input for the l former layer the input will be and the output is written as xl ij ij embeddings input tokens are rst represented by word dings let wij rd denote the embedding signed to tij since the transformer is a recurrent model we also assign a special tional embedding peij to tij to indicate the sition of the token within the input to calculate positional embeddings we follow vaswani et al and use sine and cosine tions of different frequencies the embedding ep for the th element in a sequence is d d where indicates the i th dimension of the bedding vector because each dimension of the positional encoding corresponds to a sinusoid for any xed offset o can be represented as a linear function of ep which enables the model to distinguish relative positions of input elements in multi document summarization token tij has two positions that need to be considered namely i the rank of the paragraph and j the position of the token within the paragraph positional embedding peij rd represents both positions via concatenation and is added to word ding wij to obtain the nal input vector ij peij ei ij wij peij local transformer layer a local transformer layer is used to encode textual information for tokens within each graph the local transformer layer is the same as the vanilla transformer layer vaswani et al and composed of two sub layers h xl where layernorm is layer normalization posed in ba et al mhatt is the head attention mechanism introduced in vaswani et al which allows each token to attend to other tokens with different attention tions and ffn is a two layer feed forward work with relu as hidden activation function global transformer layer a global transformer layer is used to exchange formation across multiple paragraphs as shown in figure we rst apply a multi head pooling eration to each paragraph different heads will code paragraphs with different attention weights then for each head an inter paragraph attention mechanism is applied where each paragraph can collect information from other paragraphs by attention generating a context vector to capture contextual information from the whole input nally context vectors are concatenated linearly transformed added to the vector of each token and fed to a feed forward layer updating the resentation of each token with global information multi head pooling to obtain xed length paragraph representations we apply a pooling operation instead of using only one resentation for each paragraph we introduce a multi head pooling mechanism where for each paragraph weight distributions over tokens are calculated allowing the model to exibly encode paragraphs in different representation subspaces by attending to different words let ij rd denote the output vector of the last transformer layer for token tij which is used as input for the current layer for each paragraph ri for head z nhead we rst form the input vectors into attention scores az ij and value vectors bz ij then for each head we calculate a probability distribution az ij over tokens within the paragraph based on attention scores az ij w z ij w bz a ij ij n ij az ij a and w z rdheadd are where w z weights dhead d nhead is the dimension of each head n is the number of tokens in ri we next apply a weighted summation with other linear transformation and layer tion to obtain vector headz i for the paragraph headz i z c ijbz az ij n where w z rdheaddhead is the weight the model can exibly incorporate multiple heads with each paragraph having multiple tention distributions thereby focusing on different views of the input inter paragraph attention we model the pendencies across multiple paragraphs with an inter paragraph attention mechanism similar to self attention inter paragraph attention allows for each paragraph to attend to other paragraphs by calculating an attention distribution w z qz kz i w z i w z vz m q headz i headz i v headz i t kz i t kz i vz contextz i vz i i kz rdheaddhead are query where key and value vectors that are linearly formed from headz as in vaswani et al i rdhead represents the context contextz tor generated by a self attention operation over all paragraphs m is the number of input graphs figure provides a schematic view of inter paragraph attention feed forward networks we next update token representations with contextual information we rst fuse information from all heads by nating all context vectors and applying a linear transformation with weight wc rdd i contextnhead ci i figure a global transformer layer different ors indicate different heads in multi head pooling and inter paragraph attention we then add ci to each input token vector and feed it to a two layer feed forward network with relu as the activation function and a way layer normalization on top ij gij ij ci ij xl ij where rdf d and rddf are the weights df is the hidden size of the feed forward later this way each token within paragraph ri can collect information from other paragraphs in a hierarchical and efcient manner graph informed attention the inter paragraph attention mechanism can be viewed as learning a latent graph representation self attention weights of the input paragraphs although previous work has shown that lar latent representations are benecial for stream nlp tasks liu and lapata kim et al williams et al niculae et al fernandes et al much work in multi document summarization has taken tage of explicit graph representations each ing on different facets of the summarization task multi head poolingmulti head poolinghead paragraph attentioninter paragraph attentioninter paragraph attentioncontext thisisparaonefeed forwardfeed forwardfeed forwardfeed forwardcontext thisisparatwofeed forwardfeed forwardfeed forwardfeed forwardthisisparaonethisisparatwo e capturing redundant information or senting passages referring to the same event or entity one advantage of the hierarchical former is that we can easily incorporate graphs ternal to the model to generate better summaries we experimented with two well established graph representations which we discuss briey low however there is nothing inherent in our model that restricts us to these any graph eling relationships across paragraphs could have been used instead our rst graph aims to capture lexical relations graph nodes correspond to graphs and edge weights are cosine similarities based on tf idf representations of the paragraphs our second graph aims to capture discourse it builds an lations christensen et al approximate discourse graph adg yasunaga et al over paragraphs edges between graphs are drawn by counting a co occurring tities and discourse markers e however nevertheless connecting two adjacent paragraphs see the appendix for details on how adgs are constructed we represent such graphs with a matrix g where is the weight of the edge connecting paragraphs i and we can then inject this graph into our hierarchical transformer by simply tuting one of its learned heads with g tion for calculating the context vector for this head is modied as m gio experimental setup wikisum dataset we used the scripts and urls provided in liu et al to crawl wikipedia articles and source reference documents we cessfully crawled of the original documents some urls have become invalid and ing documents could not be retrieved we ther removed clone paragraphs which are exact copies of some parts of the wikipedia articles these were paragraphs in the source documents whose bigram recall against the target summary was higher than on average each input has paragraphs and each paragraph has tokens the average length of the target mary is tokens we split the dataset with instances for training for dation and for test methods rouge l recall similarity ranking table rouge l recall against target summary for paragraphs obtained with tf idf cosine ity and our ranking model for both ranking and summarization stages we encode source paragraphs and target maries using subword tokenization with piece kudo and richardson our lary consists of subwords and is shared for both source and target paragraph ranking to train the regression model we calculated the recall lin of each paragraph against the target mary and used this as the ground truth score the hidden size of the two lstms was set to and dropout with dropout probability of was used before all linear layers adagrad duchi et al with learning rate is used for optimization we compare our ranking model against the method proposed in liu et al who use the tf idf cosine similarity between each paragraph and the article title to rank the input paragraphs we take the rst paragraphs from the ordered paragraph set produced by our ranker and the similarity based method respectively we concatenate these paragraphs and calculate their rouge l recall against the gold target text the results are shown in table we can see that our ranker effectively extracts related paragraphs and produces more informative input for the stream summarization task training conguration in all abstractive els we apply dropout with probability of fore all linear layers label smoothing szegedy et al with smoothing factor is also used training is in traditional sequence to sequence manner with maximum likelihood estimation the optimizer was adam kingma and ba with learning rate of and we also applied learning rate warmup over the rst steps and decay as in vaswani et al all transformer based models had den units the feed forward hidden size was for all layers all models were trained on gpus nvidia titan xp for steps we used model lead lexrank ft tokens no ranking ft tokens ft tokens ft tokens t dmca tokens ht tokens ht tokens similarity graph ht tokens discourse graph ht train on tokens test on tokens rouge l table test set results on the wikisum dataset using rouge gradient accumulation to keep training time for all models approximately consistent we selected the best checkpoints based on performance on the validation set and report averaged results on the test set during decoding we use beam search with beam size and length penalty with wu et al we decode until an end of sequence token is reached comparison systems we compared the transformer against several posed hierarchical strong baselines lead is a simple baseline that concatenates the tle and ranked paragraphs and extracts the rst k tokens we set k to the length of the ground truth target lexrank erkan and radev is a used graph based extractive summarizer we build a graph with paragraphs as nodes and edges weighted by tf idf cosine similarity we run a pagerank like algorithm on this graph to rank and select paragraphs until the length of the ground truth summary is reached flat transformer ft is a baseline that applies a transformer based encoder decoder model to a at token sequence we used a layer transformer the title and ranked paragraphs were concatenated and truncated to and tokens t dmca is the best performing model of liu et al and a shorthand for transformer decoder with memory compressed tion they only used a transformer decoder and compressed the key and value in attention with a convolutional layer the model has layers as in liu et al its hidden size is and its feed forward hidden size is the title and ranked paragraphs were concatenated and truncated to tokens hierarchical transformer ht is the model proposed in this paper the model tecture is a layer network with attention layers at the bottom and global tention layers at the top the model takes the title and paragraphs as input to produce a target summary which leads to proximately input tokens per instance results automatic evaluation we evaluated rization quality using rouge lin we report unigram and bigram overlap and as a means of assessing mativeness and the longest common subsequence rouge l as a means of assessing uency table summarizes our results the rst block in the table includes extractive systems lead lexrank the second block includes eral variants of flat transformer based models ft t dmca while the rest of the table presents the results of our hierarchical transformer ht as can be seen abstractive models generally perform extractive ones the flat transformer achieves best results when the input length is set to tokens while longer input i e kens actually hurts performance the transformer with input tokens model ht ht pp ht mp ht gt rl qa model lead ft t dmca ht rating table hierarchical transformer and versions thereof without paragraph position pp multi head pooling mp and global transformer layer gt table system scores based on questions answered by amt participants and summary quality rating forms ft and even t dmca when the latter is presented with tokens adding an external graph also seems to help the summarization cess the similarity graph does not have an vious inuence on the results while the discourse graph boosts rouge l by we also found that the performance of the erarchical transformer further improves when the model is presented with longer input at test time as shown in the last row of table when ing on input tokens summarization quality improves across the board this suggests that the model can potentially generate better summaries without increasing training time table summarizes ablation studies aiming to assess the contribution of individual components our experiments conrmed that encoding graph position in addition to token position within each paragraph is benecial see row pp as well as multi head pooling mp is a model where the number of heads is set to and the global transformer layer gt is a model with only local transformer layers in the encoder human evaluation in addition to automatic evaluation we also assessed system performance by eliciting human judgments on randomly lected test instances our rst evaluation study quantied the degree to which summarization models retain key information from the documents following a question answering qa paradigm clarke and lapata narayan et al we created a set of questions based on the gold summary under the assumption that it contains the most important information from the input graphs we then examined whether participants were able to answer these questions by reading system summaries alone without access to the gold summary the more questions a system can swer the better it is at summarization we ated questions in total varying from two to was not the case with the other transformer models four questions per gold summary examples of questions and their answers are given in table we adopted the same scoring mechanism used in clarke and lapata i e correct answers are marked with partially correct ones with and otherwise a system s score is the average of all question scores our second evaluation study assessed the all quality of the summaries by asking pants to rank them taking into account the lowing criteria informativeness does the mary convey important facts about the topic in question fluency is the summary uent and grammatical and succinctness does the mary avoid repetition we used best worst ing louviere et al a less labor intensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales kiritchenko and mohammad ticipants were presented with the gold summary and summaries generated from out of systems and were asked to decide which summary was the best and which one was the worst in relation to the gold standard taking into account the criteria mentioned above the rating of each system was computed as the percentage of times it was chosen as best minus the times it was selected as worst ratings range from worst to best both evaluations were conducted on the zon mechanical turk platform with responses per hit participants evaluated summaries duced by the lead baseline the flat transformer t dmca and our hierarchical transformer all evaluated systems were variants that achieved the best performance in automatic evaluations as shown in table on both evaluations participants overwhelmingly prefer our model ht all wise comparisons among systems are statistically signicant using a one way anova with tukey hsd tests p examples of system output are provided in table pentagoet archeological district the pentagoet archeological district is a national historic landmark district located at the southern edge of the bagaduce peninsula in castine maine it is the site of fort pentagoet a century fortied trading post established by fur traders of french acadia from to this site was a center of trade with the local abenaki and marked the effective western border of acadia with new england from to the site was under english control after which it was returned to france by the treaty of breda the fort was destroyed in by dutch raiders the site was designated a national historic landmark in it is now a public park a national historic landmark district what is the pentagoet archeological district castine maine where is it located what did the abenaki indians use the site for trading center the pentagoet archeological district is a national historic landmark district located in castine maine this district forms part of the traditional homeland of the abenaki indians in particular the penobscot tribe in the colonial period abenakis frequented the fortied trading post at this site bartering moosehides sealskins beaver and other furs in exchange for european commodities pentagoet archeological district is a national historic landmark district located at the southern edge of the bagaduce peninsula in treaty of breda the pentagoet archeological district is a national historic landmark district located at the southern edge of the bagaduce peninsula in treaty of breda it was listed on the national register of historic places in the pentagoet archeological district is a national historic landmark district located in castine maine this district forms part of the traditional homeland of the abenaki indians in particular the penobscot tribe the district was listed on the national register of historic places in the pentagoet archeological district is a national historic landmark district located in castine maine this district forms part of the traditional homeland of the abenaki indians in particular the penobscot tribe in the colonial period abenaki frequented the fortied trading post at this site bartering moosehides sealskins beaver and other furs in exchange for european commodities d l o g a q d a e l t f a c m d t t h melanesian whistler d the melanesian whistler or vanuatu whistler pachycephala chlorura is a species of passerine bird in the l whistler family pachycephalidae it is found on the loyalty islands vanuatu and vanikoro in the far o g eastern solomons where is it found a what is the melanesian whistler a species of passerine bird in the whistler family pachycephalidae q loyalty islands vanuatu and vanikoro in the far south eastern solomons d the australian golden whistler pachycephala pectoralis is a species of bird found in forest woodland mallee a mangrove and scrub in australia except the interior and most of the north most populations are resident but e l some in south eastern australia migrate north during the winter t the melanesian whistler p caledonica is a species of bird in the family muscicapidae it is endemic to f melanesia the australian golden whistler pachycephala chlorura is a species of bird in the family pachycephalidae which is endemic to fiji a c m d t t the melanesian whistler pachycephala chlorura is a species of bird in the family pachycephalidae which is h endemic to fiji table gold human authored summaries questions based on them answers shown in square brackets and automatic summaries produced by the baseline the flat transformer ft t dmca liu et al and our hierachical transformer ht conclusions in this paper we conceptualized abstractive document summarization as a machine learning problem we proposed a new model which is able to encode multiple input documents chically learn latent relations across them and ditionally incorporate structural information from well known graph representations we have also demonstrated the importance of a learning based approach for selecting which documents to marize experimental results show that our model produces summaries which are both uent and formative outperforming competitive systems by a wide margin in the future we would like to apply our hierarchical transformer to question answering and related textual inference tasks acknowledgments we would like to thank laura perez beltrachini for her help with preprocessing the dataset this research is supported by a google phd ship to the rst author the authors gratefully knowledge the nancial support of the european research council award number references jimmy lei ba jamie ryan kiros and geoffrey e arxiv preprint ton layer normalization dzmitry bahdanau kyunghyun cho and yoshua gio neural machine translation by jointly learning to align and translate in in proceedings of the international conference on learning resentations san diego california regina barzilay and kathleen r mckeown sentence fusion for multidocument news rization computational linguistics lidong bing piji li yi liao wai lam weiwei guo and rebecca passonneau abstractive document summarization via phrase selection and merging in proceedings of the annual ing of the association for computational linguistics and the international joint conference on ral language processing volume long papers pages beijing china asli celikyilmaz antoine bosselut xiaodong he and yejin choi deep communicating agents for in proceedings of the abstractive summarization conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages new orleans louisiana janara christensen mausam stephen soderland and towards coherent oren etzioni in proceedings of the document summarization conference of the north american chapter of the association for computational linguistics man language technologies pages lanta georgia association for computational guistics eric chu and peter j liu unsupervised neural multi document abstractive summarization arxiv preprint james clarke and mirella lapata discourse constraints for document compression tional linguistics john duchi elad hazan and yoram singer adaptive subgradient methods for online learning journal of machine and stochastic optimization learning research katja filippova and michael strube sentence fusion via dependency graph compression in ceedings of the conference on empirical ods in natural language processing pages honolulu hawaii sebastian gehrmann yuntian deng and alexander rush bottom up abstractive summarization in proceedings of the conference on cal methods in natural language processing pages brussels belgium max grusky mor naaman and yoav artzi newsroom a dataset of million summaries with diverse extractive strategies in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages new orleans louisiana karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom teaching chines to read and comprehend in advances in ral information processing systems pages curran associates inc sepp hochreiter and jurgen schmidhuber neural computation long short term memory yoon kim carl denton luong hoang and der m rush structured attention networks in proceedings of the international conference on learning representations toulon france diederik p kingma and jimmy ba adam a method for stochastic optimization arxiv preprint thomas n kipf and max welling supervised classication with graph convolutional in proceedings of the international networks conference on learning representations san juan puerto rico svetlana kiritchenko and saif mohammad best worst scaling more reliable than rating scales a case study on sentiment intensity annotation in proceedings of the annual meeting of the sociation for computational linguistics pages vancouver canada gunes erkan and dragomir r radev lexrank graph based lexical centrality as salience in text summarization journal of articial intelligence search taku kudo and john richardson sentencepiece a simple and language independent subword enizer and detokenizer for neural text processing arxiv preprint patrick fernandes miltiadis allamanis and marc brockschmidt structured neural tion in proceedings of the international ference on learning representations new orleans louisiana logan lebanoff and fei liu automatic tion of vague words and sentences in privacy cies in proceedings of the conference on pirical methods in natural language processing pages brussels belgium chin yew lin rouge a package for automatic in text summarization evaluation of summaries branches out proceedings of the shop pages barcelona spain association for computational linguistics peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer generating wikipedia by ing long sequences in proceedings of the national conference on learning representations vancouver canada yang liu and mirella lapata learning tured text representations transactions of the ciation for computational linguistics jordan j louviere terry n flynn and anthony fred john marley best worst scaling ory methods and applications cambridge sity press shulei ma zhi hong deng and yunlun yang an unsupervised multi document summarization in framework based on neural document model proceedings of coling the tional conference on computational linguistics technical papers pages osaka japan shashi narayan shay b cohen and mirella lapata ranking sentences for extractive tion with reinforcement learning in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long pers pages new orleans louisiana vlad niculae andre f t martins and claire cardie towards dynamic computation graphs via sparse latent structure in proceedings of the conference on empirical methods in natural guage processing pages brussels gium daraksha parveen and michael strube document summarization using bipartite graphs in proceedings of the workshop on graph based methods for natural language cessing pages doha qatar abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proceedings of the nual meeting of the association for computational linguistics volume long papers pages vancouver canada christian szegedy vincent vanhoucke sergey ioffe jon shlens and zbigniew wojna rethinking the inception architecture for computer vision in the ieee conference on computer vision and tern recognition cvpr ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in advances in neural information cessing systems pages curran ciates inc xiaojun wan an exploration of document impact on graph based multi document tion in proceedings of the conference on pirical methods in natural language processing pages honolulu hawaii adina williams andrew drozdov and samuel r bowman do latent tree learning models tify meaningful structure in sentences tions of the association for computational tics yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al google s neural machine translation system bridging the gap between in arxiv preprint man and machine translation michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan and dragomir radev graph based neural multi document summarization in proceedings of the ence on computational natural language learning conll pages vancouver canada romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization in proceedings of the international conference on learning representations ver canada jianmin zhang jiwei tan and xiaojun wan adapting neural single document summarization model for abstractive multi document tion a pilot study in proceedings of the tional conference on natural language generation dragomir radev a common theory of mation fusion from multiple text sources step one cross document structure in sigdial workshop on discourse and dialogue pages hong kong china evan sandhaus the new york times annotated corpus linguistic data consortium philadelphia a appendix we describe here how the similarity and discourse graphs discussed in section were created these graphs were added to the hierarchical former model as a means to enhance summary quality see section for details a similarity graph the similarity graph s is based on tf idf cosine similarity the nodes of the graph are paragraphs we rst represent each paragraph pi as a bag of words then we calculate the tf idf value vik for each token tik in a paragraph if two paragraphs pi are adjacent in one source webpage and they are connected with one of the above discourse markers will be otherwise it will be the nal edge weight is the weighted sum of and vik nd where n is the count of word t in the graph nd is the total number of paragraphs and is the total number of paragraphs taining the word we thus obtain a tf idf vector for each paragraph then for all paragraph pairs pi we calculate the cosine similarity of their tf idf vectors and use this as the weight for the edge connecting the pair in the graph we remove edges with weights lower than a discourse graphs to build the approximate discourse graph adg d we follow christensen et al and yasunaga et al the original adg makes use of several complex features here we create a simplied version with only two features nodes in this graph are again paragraphs co occurring entities for each paragraph pi we extract a set of entities ei in the paragraph using the ner recognizer we only use entities with type person norp fac org gpe loc event work of art law for each paragraph pair pi pj we count eij the number of entities with exact match discourse markers we use the following plicit discourse markers to identify edges between two adjacent paragraphs in a source webpage again also another comparatively thermore at the same time however mediately indeed instead to be sure likewise meanwhile moreover theless nonetheless notably otherwise regardless similarly unlike in addition even in turn in exchange in this case in any event nally later as well cially as a result example in fact then the day before io api entityrecognizer
