hierarchical transformers multi document summarization yang liu mirella lapata institute language cognition computation school informatics university edinburgh yang ac uk ed ac uk m l c s c v v x r abstract paper develop neural rization model effectively process multiple input documents distill tive summaries model augments ously proposed transformer architecture liu et al ability encode ments hierarchical manner represent cross document relationships attention mechanism allows share information opposed simply concatenating text spans processing sequence model learns latent dependencies tual units advantage plicit graph representations focusing larity discourse relations empirical results wikisum dataset demonstrate proposed architecture brings substantial provements strong baselines introduction automatic summarization enjoyed renewed interest recent years thanks ity neural network models ability learn continuous representations recourse preprocessing tools linguistic annotations availability large scale datasets sandhaus hermann et al grusky et al containing hundreds thousands summary pairs driven development neural architectures summarizing single uments approaches shown ing results sequence sequence models encode source document decode abstractive summary et al ilmaz et al paulus et al gehrmann et al multi document summarization task producing summaries clusters code data available com nlpyang hiersumm cally related documents received icantly attention partly paucity suitable data application learning methods high quality multi document rization datasets e document clusters paired multiple reference summaries written mans produced document derstanding text analysis conferences duc tac relatively small range examples training ral models attempt drive research ther liu et al tap potential wikipedia propose methodology ating large scale dataset wikisum document summarization hundreds sands instances wikipedia articles specically lead sections viewed summaries topics indicated title e florence natural language processing documents cited wikipedia articles web pages returned google section titles queries seen source cluster lead section purports summarize aside difculties obtaining ing data major obstacle application end end models multi document tion sheer size number source uments large result practically infeasible given memory limitations current hardware train model codes vectors subsequently generates summary liu et al propose stage architecture tive model rst selects subset salient passages subsequently abstractive model generates summary conditioning extracted subset selected passages concatenated sequence transformer vaswani et al architecture suited guage modeling long sequences decode summary related work model liu et al takes important rst step abstractive document summarization considers multiple input documents concatenated sequence agnostic hierarchical tures relations exist uments example different web pages repeat content include additional tent present contradictory information discuss fact different light radev realization cross document links portant information nating redundancy creating overall coherent summaries led widespread adoption graph based models multi document marization erkan radev christensen et al wan parveen strube graphs conveniently capture ships textual units document lection easily constructed sumption text spans represent graph nodes edges semantic links isolating salient paper develop neural tion model effectively process ple input documents distill abstractive maries model augments previously posed transformer architecture ability encode multiple documents hierarchical ner represent cross document relationships attention mechanism allows share information multiple documents opposed simply concatenating text spans feeding sequence model way model automatically learns richer tural dependencies textual units corporating established insights earlier work advantageously proposed architecture easily benet information external model e replacing inter document attention graph matrix computed based basis lexical similarity erkan radev discourse relations christensen et al evaluate model wikisum dataset experimentally proposed tecture brings substantial improvements eral strong baselines nd dition simple ranking module scores documents based usefulness target summary greatly boost performance multi document summarization system previous multi document summarization methods extractive operating graph based representations sentences passages proaches vary depending edge weights computed e based cosine similarity tf idf weights words erkan radev discourse relations christensen et al specic algorithm adopted ranking text units inclusion nal summary eral variants pagerank algorithm adopted literature erkan radev order compute importance salience passage recursively based entire graph recently yasunaga et al propose neural version framework salience estimated features extracted tence embeddings graph convolutional works kipf welling applied relation graph representing cross document links abstractive approaches met limited success systems generate summaries based sentence fusion technique ties fragments conveying common information documents combines tences barzilay mckeown filippova strube bing et al neural abstractive models achieved ing results single document summarization et al paulus et al gehrmann et al celikyilmaz et al tension sequence sequence architectures multi document summarization ward apart lack sufcient training data neural models face computational challenge processing multiple source ments previous solutions include model fer zhang et al lebanoff liu sequence sequence model pretrained single document summarization data tuned duc multi document benchmarks unsupervised models relying reconstruction jectives ma et al chu liu liu et al propose methodology constructing large scale summarization datasets stage model rst extracts salient information source documents uses decoder architecture attend long sequences generate summary low setup viewing multi document marization supervised machine learning troduced vaswani et al generates summary token token attending source input use beam search length penalty wu et al decoding process generate uent longer maries paragraph ranking unlike liu et al rank paragraphs based similarity title tf based cosine similarity adopt based approach logistic regression model applied paragraph calculate score dicating selected rization use recurrent neural networks long short term memory units lstm hochreiter schmidhuber represent tle t source paragraph p utm wtm upn wpn wti wpj word embeddings tokens t p uti upj updated vectors token applying lstms max pooling operation title vectors obtain xed length representation ut ut utm concatenate ut vector upi ken paragraph apply non linear formation extract features matching title paragraph second max pooling tion yields nal paragraph vector p pi ut pn finally estimate paragraph selected use linear transformation moid function s s score indicating graph p summarization input paragraphs pl receive scores sl model trained minimizing cross entropy loss si ground truth scores yi denoting relatedness paragraph gold standard summary adopt recall paragraph pi figure pipeline multi document tion system l source paragraphs rst ranked ones serve input encoder decoder model generates target summary lem purpose assume access large labeled datasets e source documents summary contrast approach use pairs learning based ranker abstractive model hierarchically encode input documents ability learn latent relations uments additionally incorporate information encoded known graph representations model description follow liu et al treating eration lead wikipedia sections document summarization task input pothetical system title wikipedia cle collection source documents output wikipedia article s rst section source documents webpages cited erences section wikipedia article search results returned google title article query source documents relatively long split multiple paragraphs line breaks formally given title t l input paragraphs pl retrieved wikipedia citations search engine task generate lead section d wikipedia article summarization system illustrated ure input paragraphs numerous possibly lengthy instead directly applying abstractive system rst rank marize ones summarizer follows successful encoder decoder architecture bahdanau et al encoder codes input text hidden representations decoder generates target summaries based representations paper focus exclusively encoder model decoder follows transformer architecture ranked paragraphssource paragraphsparagraph rankerencoderpara l para ldecoderabstractive summarizertarget summary gold target text d yi testing input graphs ranked based model predicted scores ordering rl ated rst paragraphs selected input second abstractive stage paragraph encoding instead treating selected paragraphs long sequence develop cal model based transformer architecture vaswani et al capture inter paragraph relations model composed cal global transformer layers stacked freely let tij denote j token ranked paragraph ri model takes vectors ij tokens input l layer input output written xl ij ij embeddings input tokens rst represented word dings let wij rd denote embedding signed tij transformer recurrent model assign special tional embedding peij tij indicate sition token input calculate positional embeddings follow vaswani et al use sine cosine tions different frequencies embedding ep th element sequence d d indicates th dimension bedding vector dimension positional encoding corresponds sinusoid xed offset o represented linear function ep enables model distinguish relative positions input elements multi document summarization token tij positions need considered rank paragraph j position token paragraph positional embedding peij rd represents positions concatenation added word ding wij obtain nal input vector ij peij ei ij wij peij local transformer layer local transformer layer encode textual information tokens graph local transformer layer vanilla transformer layer vaswani et al composed sub layers h xl layernorm layer normalization posed ba et al mhatt head attention mechanism introduced vaswani et al allows token attend tokens different attention tions ffn layer feed forward work relu hidden activation function global transformer layer global transformer layer exchange formation multiple paragraphs shown figure rst apply multi head pooling eration paragraph different heads code paragraphs different attention weights head inter paragraph attention mechanism applied paragraph collect information paragraphs attention generating context vector capture contextual information input nally context vectors concatenated linearly transformed added vector token fed feed forward layer updating resentation token global information multi head pooling obtain xed length paragraph representations apply pooling operation instead resentation paragraph introduce multi head pooling mechanism paragraph weight distributions tokens calculated allowing model exibly encode paragraphs different representation subspaces attending different words let ij rd denote output vector transformer layer token tij input current layer paragraph ri head z nhead rst form input vectors attention scores az ij value vectors bz ij head calculate probability distribution az ij tokens paragraph based attention scores az ij w z ij w bz ij ij n ij az ij w z rdheadd w z weights dhead d nhead dimension head n number tokens ri apply weighted summation linear transformation layer tion obtain vector headz paragraph headz z c ijbz az ij n w z rdheaddhead weight model exibly incorporate multiple heads paragraph having multiple tention distributions focusing different views input inter paragraph attention model pendencies multiple paragraphs inter paragraph attention mechanism similar self attention inter paragraph attention allows paragraph attend paragraphs calculating attention distribution w z qz kz w z w z vz m q headz headz v headz t kz t kz vz contextz vz kz rdheaddhead query key value vectors linearly formed headz vaswani et al rdhead represents context contextz tor generated self attention operation paragraphs m number input graphs figure provides schematic view inter paragraph attention feed forward networks update token representations contextual information rst fuse information heads nating context vectors applying linear transformation weight wc rdd contextnhead ci figure global transformer layer different ors indicate different heads multi head pooling inter paragraph attention add ci input token vector feed layer feed forward network relu activation function way layer normalization ij gij ij ci ij xl ij rdf d rddf weights df hidden size feed forward later way token paragraph ri collect information paragraphs hierarchical efcient manner graph informed attention inter paragraph attention mechanism viewed learning latent graph representation self attention weights input paragraphs previous work shown lar latent representations benecial stream nlp tasks liu lapata kim et al williams et al niculae et al fernandes et al work multi document summarization taken tage explicit graph representations ing different facets summarization task multi head poolingmulti head poolinghead paragraph attentioninter paragraph attentioninter paragraph attentioncontext thisisparaonefeed forwardfeed forwardfeed forwardfeed forwardcontext thisisparatwofeed forwardfeed forwardfeed forwardfeed forwardthisisparaonethisisparatwo e capturing redundant information senting passages referring event entity advantage hierarchical easily incorporate graphs ternal model generate better summaries experimented established graph representations discuss briey low inherent model restricts graph eling relationships paragraphs instead rst graph aims capture lexical relations graph nodes correspond graphs edge weights cosine similarities based tf idf representations paragraphs second graph aims capture discourse builds lations christensen et al approximate discourse graph adg yasunaga et al paragraphs edges graphs drawn counting co occurring tities discourse markers e connecting adjacent paragraphs appendix details adgs constructed represent graphs matrix g weight edge connecting paragraphs inject graph hierarchical transformer simply tuting learned heads g tion calculating context vector head modied m gio experimental setup wikisum dataset scripts urls provided liu et al crawl wikipedia articles source reference documents cessfully crawled original documents urls invalid ing documents retrieved ther removed clone paragraphs exact copies parts wikipedia articles paragraphs source documents bigram recall target summary higher average input paragraphs paragraph tokens average length target mary tokens split dataset instances training dation test methods rouge l recall similarity ranking table rouge l recall target summary paragraphs obtained tf idf cosine ity ranking model ranking summarization stages encode source paragraphs target maries subword tokenization piece kudo richardson lary consists subwords shared source target paragraph ranking train regression model calculated recall lin paragraph target mary ground truth score hidden size lstms set dropout dropout probability linear layers adagrad duchi et al learning rate optimization compare ranking model method proposed liu et al use tf idf cosine similarity paragraph article title rank input paragraphs rst paragraphs ordered paragraph set produced ranker similarity based method respectively concatenate paragraphs calculate rouge l recall gold target text results shown table ranker effectively extracts related paragraphs produces informative input stream summarization task training conguration abstractive els apply dropout probability fore linear layers label smoothing szegedy et al smoothing factor training traditional sequence sequence manner maximum likelihood estimation optimizer adam kingma ba learning rate applied learning rate warmup rst steps decay vaswani et al transformer based models den units feed forward hidden size layers models trained gpus nvidia titan xp steps model lead lexrank ft tokens ranking ft tokens ft tokens ft tokens t dmca tokens ht tokens ht tokens similarity graph ht tokens discourse graph ht train tokens test tokens rouge l table test set results wikisum dataset rouge gradient accumulation training time models approximately consistent selected best checkpoints based performance validation set report averaged results test set decoding use beam search beam size length penalty wu et al decode end sequence token reached comparison systems compared transformer posed hierarchical strong baselines lead simple baseline concatenates tle ranked paragraphs extracts rst k tokens set k length ground truth target lexrank erkan radev graph based extractive summarizer build graph paragraphs nodes edges weighted tf idf cosine similarity run pagerank like algorithm graph rank select paragraphs length ground truth summary reached flat transformer ft baseline applies transformer based encoder decoder model token sequence layer transformer title ranked paragraphs concatenated truncated tokens t dmca best performing model liu et al shorthand transformer decoder memory compressed tion transformer decoder compressed key value attention convolutional layer model layers liu et al hidden size feed forward hidden size title ranked paragraphs concatenated truncated tokens hierarchical transformer ht model proposed paper model tecture layer network attention layers global tention layers model takes title paragraphs input produce target summary leads proximately input tokens instance results automatic evaluation evaluated rization quality rouge lin report unigram bigram overlap means assessing mativeness longest common subsequence rouge l means assessing uency table summarizes results rst block table includes extractive systems lead lexrank second block includes eral variants flat transformer based models ft t dmca rest table presents results hierarchical transformer ht seen abstractive models generally perform extractive ones flat transformer achieves best results input length set tokens longer input e kens actually hurts performance transformer input tokens model ht ht pp ht mp ht gt rl qa model lead ft t dmca ht rating table hierarchical transformer versions thereof paragraph position pp multi head pooling mp global transformer layer gt table system scores based questions answered amt participants summary quality rating forms ft t dmca presented tokens adding external graph help summarization cess similarity graph vious inuence results discourse graph boosts rouge l found performance erarchical transformer improves model presented longer input test time shown row table ing input tokens summarization quality improves board suggests model potentially generate better summaries increasing training time table summarizes ablation studies aiming assess contribution individual components experiments conrmed encoding graph position addition token position paragraph benecial row pp multi head pooling mp model number heads set global transformer layer gt model local transformer layers encoder human evaluation addition automatic evaluation assessed system performance eliciting human judgments randomly lected test instances rst evaluation study quantied degree summarization models retain key information documents following question answering qa paradigm clarke lapata narayan et al created set questions based gold summary assumption contains important information input graphs examined participants able answer questions reading system summaries access gold summary questions system swer better summarization ated questions total varying case transformer models questions gold summary examples questions answers given table adopted scoring mechanism clarke lapata e correct answers marked partially correct ones system s score average question scores second evaluation study assessed quality summaries asking pants rank taking account lowing criteria informativeness mary convey important facts topic question fluency summary uent grammatical succinctness mary avoid repetition best worst ing louviere et al labor intensive alternative paired comparisons shown produce reliable results rating scales kiritchenko mohammad ticipants presented gold summary summaries generated systems asked decide summary best worst relation gold standard taking account criteria mentioned rating system computed percentage times chosen best minus times selected worst ratings range worst best evaluations conducted zon mechanical turk platform responses hit participants evaluated summaries duced lead baseline flat transformer t dmca hierarchical transformer evaluated systems variants achieved best performance automatic evaluations shown table evaluations participants overwhelmingly prefer model ht wise comparisons systems statistically signicant way anova tukey hsd tests p examples system output provided table pentagoet archeological district pentagoet archeological district national historic landmark district located southern edge bagaduce peninsula castine maine site fort pentagoet century fortied trading post established fur traders french acadia site center trade local abenaki marked effective western border acadia new england site english control returned france treaty breda fort destroyed dutch raiders site designated national historic landmark public park national historic landmark district pentagoet archeological district castine maine located abenaki indians use site trading center pentagoet archeological district national historic landmark district located castine maine district forms traditional homeland abenaki indians particular penobscot tribe colonial period abenakis frequented fortied trading post site bartering moosehides sealskins beaver furs exchange european commodities pentagoet archeological district national historic landmark district located southern edge bagaduce peninsula treaty breda pentagoet archeological district national historic landmark district located southern edge bagaduce peninsula treaty breda listed national register historic places pentagoet archeological district national historic landmark district located castine maine district forms traditional homeland abenaki indians particular penobscot tribe district listed national register historic places pentagoet archeological district national historic landmark district located castine maine district forms traditional homeland abenaki indians particular penobscot tribe colonial period abenaki frequented fortied trading post site bartering moosehides sealskins beaver furs exchange european commodities d l o g q d e l t f c m d t t h melanesian whistler d melanesian whistler vanuatu whistler pachycephala chlorura species passerine bird l whistler family pachycephalidae found loyalty islands vanuatu vanikoro far o g eastern solomons found melanesian whistler species passerine bird whistler family pachycephalidae q loyalty islands vanuatu vanikoro far south eastern solomons d australian golden whistler pachycephala pectoralis species bird found forest woodland mallee mangrove scrub australia interior north populations resident e l south eastern australia migrate north winter t melanesian whistler p caledonica species bird family muscicapidae endemic f melanesia australian golden whistler pachycephala chlorura species bird family pachycephalidae endemic fiji c m d t t melanesian whistler pachycephala chlorura species bird family pachycephalidae h endemic fiji table gold human authored summaries questions based answers shown square brackets automatic summaries produced baseline flat transformer ft t dmca liu et al hierachical transformer ht conclusions paper conceptualized abstractive document summarization machine learning problem proposed new model able encode multiple input documents chically learn latent relations ditionally incorporate structural information known graph representations demonstrated importance learning based approach selecting documents marize experimental results model produces summaries uent formative outperforming competitive systems wide margin future like apply hierarchical transformer question answering related textual inference tasks acknowledgments like thank laura perez beltrachini help preprocessing dataset research supported google phd ship rst author authors gratefully knowledge nancial support european research council award number references jimmy lei ba jamie ryan kiros geoffrey e arxiv preprint ton layer normalization dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly learning align translate proceedings international conference learning resentations san diego california regina barzilay kathleen r mckeown sentence fusion multidocument news rization computational linguistics lidong bing piji li yi liao wai lam weiwei guo rebecca passonneau abstractive document summarization phrase selection merging proceedings annual ing association computational linguistics international joint conference ral language processing volume long papers pages beijing china asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents proceedings abstractive summarization conference north american chapter association computational linguistics man language technologies volume long pers pages new orleans louisiana janara christensen mausam stephen soderland coherent oren etzioni proceedings document summarization conference north american chapter association computational linguistics man language technologies pages lanta georgia association computational guistics eric chu peter j liu unsupervised neural multi document abstractive summarization arxiv preprint james clarke mirella lapata discourse constraints document compression tional linguistics john duchi elad hazan yoram singer adaptive subgradient methods online learning journal machine stochastic optimization learning research katja filippova michael strube sentence fusion dependency graph compression ceedings conference empirical ods natural language processing pages honolulu hawaii sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference cal methods natural language processing pages brussels belgium max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies proceedings conference north american chapter association computational linguistics man language technologies volume long pers pages new orleans louisiana karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend advances ral information processing systems pages curran associates inc sepp hochreiter jurgen schmidhuber neural computation long short term memory yoon kim carl denton luong hoang der m rush structured attention networks proceedings international conference learning representations toulon france diederik p kingma jimmy ba adam method stochastic optimization arxiv preprint thomas n kipf max welling supervised classication graph convolutional proceedings international networks conference learning representations san juan puerto rico svetlana kiritchenko saif mohammad best worst scaling reliable rating scales case study sentiment intensity annotation proceedings annual meeting sociation computational linguistics pages vancouver canada gunes erkan dragomir r radev lexrank graph based lexical centrality salience text summarization journal articial intelligence search taku kudo john richardson sentencepiece simple language independent subword enizer detokenizer neural text processing arxiv preprint patrick fernandes miltiadis allamanis marc brockschmidt structured neural tion proceedings international ference learning representations new orleans louisiana logan lebanoff fei liu automatic tion vague words sentences privacy cies proceedings conference pirical methods natural language processing pages brussels belgium chin yew lin rouge package automatic text summarization evaluation summaries branches proceedings shop pages barcelona spain association computational linguistics peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser noam shazeer generating wikipedia ing long sequences proceedings national conference learning representations vancouver canada yang liu mirella lapata learning tured text representations transactions ciation computational linguistics jordan j louviere terry n flynn anthony fred john marley best worst scaling ory methods applications cambridge sity press shulei ma zhi hong deng yunlun yang unsupervised multi document summarization framework based neural document model proceedings coling tional conference computational linguistics technical papers pages osaka japan shashi narayan shay b cohen mirella lapata ranking sentences extractive tion reinforcement learning proceedings conference north american ter association computational linguistics human language technologies volume long pers pages new orleans louisiana vlad niculae andre f t martins claire cardie dynamic computation graphs sparse latent structure proceedings conference empirical methods natural guage processing pages brussels gium daraksha parveen michael strube document summarization bipartite graphs proceedings workshop graph based methods natural language cessing pages doha qatar abigail peter j liu christopher d manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers pages vancouver canada christian szegedy vincent vanhoucke sergey ioffe jon shlens zbigniew wojna rethinking inception architecture computer vision ieee conference computer vision tern recognition cvpr ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention need advances neural information cessing systems pages curran ciates inc xiaojun wan exploration document impact graph based multi document tion proceedings conference pirical methods natural language processing pages honolulu hawaii adina williams andrew drozdov samuel r bowman latent tree learning models tify meaningful structure sentences tions association computational tics yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al google s neural machine translation system bridging gap arxiv preprint man machine translation michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan dragomir radev graph based neural multi document summarization proceedings ence computational natural language learning conll pages vancouver canada romain paulus caiming xiong richard socher deep reinforced model abstractive marization proceedings international conference learning representations ver canada jianmin zhang jiwei tan xiaojun wan adapting neural single document summarization model abstractive multi document tion pilot study proceedings tional conference natural language generation dragomir radev common theory mation fusion multiple text sources step cross document structure sigdial workshop discourse dialogue pages hong kong china evan sandhaus new york times annotated corpus linguistic data consortium philadelphia appendix describe similarity discourse graphs discussed section created graphs added hierarchical model means enhance summary quality section details similarity graph similarity graph s based tf idf cosine similarity nodes graph paragraphs rst represent paragraph pi bag words calculate tf idf value vik token tik paragraph paragraphs pi adjacent source webpage connected discourse markers nal edge weight weighted sum vik nd n count word t graph nd total number paragraphs total number paragraphs taining word obtain tf idf vector paragraph paragraph pairs pi calculate cosine similarity tf idf vectors use weight edge connecting pair graph remove edges weights lower discourse graphs build approximate discourse graph adg d follow christensen et al yasunaga et al original adg makes use complex features create simplied version features nodes graph paragraphs co occurring entities paragraph pi extract set entities ei paragraph ner recognizer use entities type person norp fac org gpe loc event work art law paragraph pair pi pj count eij number entities exact match discourse markers use following plicit discourse markers identify edges adjacent paragraphs source webpage comparatively thermore time mediately instead sure likewise theless nonetheless notably regardless similarly unlike addition turn exchange case event nally later cially result example fact day io api entityrecognizer
