controllableabstractivesummarizationangelafandavidgrangierfacebookairesearchmenlopark california usamichaelauliabstractcurrentmodelsfordocumentsummarizationdisregarduserpreferencessuchasthedesiredlength style theentitiesthattheusermightbeinterestedin orhowmuchofthedocu menttheuserhasalreadyread wepresentaneuralsummarizationmodelwithasimplebuteffectivemechanismtoenableuserstospecifythesehighlevelattributesinordertocontroltheshapeofthefinalsummariestobettersuittheirneeds withuserinput oursystemcanproducehighqualitysummariesthatfollowuserpreferences withoutuserin wesetthecontrolvariablesautomatically onthefulltextcnn dailymaildataset rougeandhumanevaluation summarizationalgorithmsareeitherextractiveorabstractive extractivealgorithmsformsummariesbypastingtogetherrelevantportionsoftheinput thisworkfocusesonabstractivesummarizationand incontrasttopreviouswork describesmech anismsthatenablethereadertocontrolimportantaspectsofthegeneratedsummary thereadercanselectthedesiredlengthofthesummarydependingonhowdetailedtheywouldlikethesummarytobe thereadercanrequirethetexttofocusonentitiestheyhaveaparticularinterestin weletthereaderchoosethestyleofthesummarybasedontheirfavoritesourceofinformation aparticularnewssource finally weallowthereadertospecifythattheyonlywanttosummarizeaportionofthearticle forexampletheremainingparagraphstheyhaventread ourworkbuildsuponsequence weintroduceastraightforwardandextensiblecontrollablesummarizationmodeltoenableperson alizedgenerationandfullyleveragethatautomaticsummariesaregeneratedatthereadersrequest ourcomparisonwithstate artmodelsonthestandardcnn sentencesummarizationnewscorpus highlightstheadvantageofourapproach onboththeentity weoutperformpreviouspointer basedmodelstrainedwithmaximumlikelihooddespitetherelativesimplicityofourmodel wedemonstrateinablindhumanevaluationstudythatourmodelgeneratessummariespreferredbyhumanreaders sequenceourapproachbuildsupontheconvolutionalmodelofgehringetal theencoderanddecoder theweightsarepredictedfromthecurrentdecoderstates allowingthedecodertoemphasizethepartsoftheinputdocumentwhicharethemostrelevantforgeneratingthenexttoken weusemulti hopattention attentionisappliedateachlayerofthedecoder danauetal attentioninthedecodertoenablethemodeltoreferbacktopreviouslygeneratedwords thisallowsthedecodertokeeptrackofitsprogressandreducesthegenerationofre tocombineencoderanddecoderattention wealternatebetweeneachtypeofattentionateverylayer muchpriorworkonthecnn dailymailbench instead werelyonsub wordtokenizationandweightsharing weshowthissimpleapproachisveryeffective specifically weusebyte pair wesharetherepresentationofthetokensintheencoderanddecoderembeddingsandinthelastdecoderlayer constrainedsummarizationsummarizationallowsareaderwithlimitedtimetoquicklycomprehendtheessenceofadocument controllingsummarylengthenablesreadingwithdifferenttimebudgets adocumentmightbesummarizedasafive wordheadline asinglesentenceoraparagraph eachprovidingmoreandmoredetail toenabletheusertocontrollength wefirstquan tizesummarylengthintodiscretebins eachrepresent ingasizerange lengthbinsarechosensothattheyeachcontainroughlyanequalnumberoftrainingdoc uments wethenexpandtheinputvocabularywithspecialwordtypestoindicatethelengthbinofthedesiredsummary whichallowsgenerationtobecondi tioneduponthisdiscretelengthvariable fortraining weprependtheinputofoursummarizerwithamarkerthatindicatesthelengthoftheground truthsummary attesttime wecontrolthelengthofgeneratedtextbyprependingaparticularlengthmarkertoken outputlengthiseasilycontrolledbychangingthelengthmarkerandsupplyinggroundtruthmarkersdrasticallyimprovessummaryquality wecompareourmethodtokikuchietal centricsummarizationthereadermightbeinterestedinadocumenttolearnaboutspecificentities suchaspeopleorlocations forexample asportsfanreadingaboutarecentgamemightwanttofocusthesummaryontheperformanceoftheirfavoriteplayer toenableentity centricsummaries wefirstanonymizeentitiesbyreplacingalloccurrencesofagivenentityinadocumentbythesametoken fortraining wealsoanonymizethecorrespondingreferencesummary thisabstractsawaythesurfaceform allowingourapproachtoscaletomanyentitiesandgeneralizetounseenones wethenexpressthatanentityshouldbepresentinthegeneratedsummarybyprependingtheentitytokentotheinput ineffect thisinstructsthemodeltofocusonsentencesthatmentionthemarkedentities attrainingtime weprependeachdocumentwithmarkersreferringtoanentityfromtheground truthsummary toensuretheentityrequestisinformative weprovideanentitythatispresentintheground truthbutnotpresentinthesummarygeneratedbythebase linemodel attesttime wemayspecifyanyentitymarkerthatwewishthesummarytocontain weshowthathigheraccuracyisachievedwhenwespecifyentitiesfromthefirstfewsentencesofadocumentorifwesupplymarkerstakenfromthereferencesummarytoillustratespecificuserpreferences weextendthisapproachtomultipleentitymarkersandexperimentwithappend ingallground weshowthatpro vidingmoreentitiesimprovessummarizationquality specificsummarizationtextsourcessuchasnewspapersandmagazinesoftenhavespecificstyleguidelinestoprovideaconsistent experience readersareaccustomedtothestylesoftheirfavoritesources weenableuserstospecifyapreferredsourcestyleforasummary similartolengthandentities fortraining wepreprendtheinputwiththemarkercorrespondingtotheground truthsource atinference wecontrolthestyleofgeneratedsummarybyprependingdifferentmarkers styleproducessummariesthatareclosertothereferencesummary weadditionallyprovideexamplesofdistinctsummariesresultingfromchangingsource styleconditioning readersmaywanttheflexibilityofonlysummarizingcertainportionsofadocument forexample areaderwhohasreadthefirstfewparagraphswouldwantasum maryoftheremainingtexttocoverwhattheymissed trainingandevaluatingremaindersummarizationrequiresspecificdata namelyadatasetoffulldocumentswithpositionmarkersseparatingthealreadyreadportionfromtheremainderpartalongwiththecorrespondingsummaries suchadatasetisnotreadilyavailableandwouldbechallengingtocollect toenableremaindersummarizationwithoutsuchdata wealignsummariestofulldocuments ourprocedurematcheseachreferencesummarysentencetoitsbestmatchingdocumentsentencebasedonrouge foranypositioninthedocument inourexperiment weconsiderasreadportionsallarticlepositionslocatedatthemiddleoftwoalignmentpoints thebaselinemodelpredictsafullsummary disregardingtheseparationofthereadportionfromtheremainder inferencealignment afullsummaryisgeneratedfromthebaselinemodelandthesummaryisshortenedwithouralignmentprocedure thede codedsummarysentencesthataligntotheremainderportioncomposethesummaryoftheremainder themodelistrainedtomapthedocumentremainderstotheremaindersummariesonpre alignedtrainingdata thismodelisnotgiventhereadportionofthearticle themodelreceivesbothreadportionofthearticleandtheremainderseparatedbyaspecialtoken itistrainedtopredicttheremaindersummary wedistinguishthereadandremainderpartofthearticlebyusingdistinctsetsofpositionembeddings readportionandtheremainderofthedocument extractiveandabstrac tivemethodshavebenefitedfromadvancesinnaturallanguageprocessing patternrecognition recently sequence lapatietal neuralabstractivesummarizationhasbuiltuponadvancesfrommachinetranslationandrelatedfields summarizationalsohasdistinctchal lenges thegenerationofmulti sentencesummariesdiffersfromsinglesentencetranslation left rightdecodersneedtobeawareoftheirpreviousgener ationatalargertimescale otherwisemodelstendtoproducerepeatedtext toaddressthisimpedi decoderattention themostcommonautomaticmetrictoassesssummarization combiningbothstrategiesisfoundtoperformbestinhumanevaluations astrainingwithrlaloneoftenproducesnon grammaticaltext ourworkbuildsuponpriorresearch whichenablefastertraining thiscontrastswithprior weborrowintra hopintra attentioninspiredbymulti tofacilitatecopyinginputentities wesharethewordrepresen ingsummariesgiventhesourcedocument ourmodelisamenabletorl butthisaspectislargelyorthogonaltoourmaingoal controllablesummarization thefieldfollowsrecentadvancesingenerativemodels suchastheintroductionofvariationalauto buildinguponunconditionedgeneration con trollablegenerationisanemergingresearchfield researchincomputervisionincludesstyletrans textgenerationworkfocusesoncontrollingtenseorsentimentwithvariationalauto shenetal takenoetal suchaslength othershaveworkedonstyle turestotranslatetextindifferentdomains kikuchietal timerestrictionsandtraining timelengthtokenembeddings motivatedbysimplicity ourworkreliesonconditionallanguagemodelinganddoesnotrequireadversarialtraining latentvariablemodelssuchasvariationalauto encoders orpointernetworks weleavetheassessmentofhowadditionallatentvariablesmightimproveuponourresultstofuturework itconsistsofnewsarticlesalongwithmulti sentencesummaries onaverage weevaluateontwoversionsofthedata fornon bpemodels inputandoutputvocabularieshaveresp sentencesumma rizationtask wetrainonenglishgigawordfollowingtheprotocolofrushetal headlineofnewsarticles architecture training andgeneration forcnn dailymail forduc similartogehringetal wereducethelearningratebyanorderofmagnitudewhenthevalidationperplexityceasestoimprove com facebookresearch fairseq toavoidrepetition wepreventthedecoderfromgeneratingthesametrigrammorethanonce followingpaulusetal evaluation onthecnn dailymailbenchmark lapatietal notethat althoughsimple thisbaselineisnotoutperformedbyallmodels forhumanevaluation weconductahumanevaluationstudyusingamazonmechanicalturkandthetestsetgenerationoutputofseeetal weana lyzetheperformanceoftheremaindersummarizationtaskanddemonstratetheadvantageofmodelingboththereadandremainderportionsofthedocument addingintra themodestimprovementislikelybecausethetwofeaturesaddressasimilarproblemofavoidingrepeatedgenera tions bpeimprovestheabilitytocopypropernounsandrareinflections bothofwhicharedifficulttomodelinword basedvocabular ies lastly wefindtuningthemin baselinewithoutcontrolvariables eachrowaddafeatureontopofthepreviousrowfeatures summarizationwithoraclecontroltosimulateuserpreference intra fixedcontrolvariablesonentity anonymizedtext evenwithfixedvariables thecontrollablemodelimprovesrougecomparedtomlalternatives entitiesonwhichitfocuseson wefirstevaluatetheeffectofprovidingtheoraclereferencevariablesatdecodingtime thissimulatesausersettingtheirpreferencestospecificvalues wethenassesstheeffectofprovidingnon referencecontrolvariables allcontrolvariablesimprovethesummaryquality butlengthcontrolhasthemost summarizationwithfixedcontrolvariablesonoriginaltext evenwithafixedsetting thecontrolledsummarizationmodelimprovesrouge impact followedbyentitycontrolandsourcestyle theadvantagesofeachcontrolvariablecumu lativelyproduceanevenstrongersummary thisimprovementisduetotwoeffects rouge thebaselinestrugglesatpredictingcorrectlengths thelatterisduetolargeuncertaintyinsummarylength evenhumanshavedifficultypredictingthecorrectlength themodelisshowntorespectlengthmarkers entitycontrolhaslessimpactonrougecom thisismainlybecauseoursummariesoftenalreadycontainmostentitiesfromtheground truthwithouttheneedforadditionalinstruction wethenrepeattheexperimentwitheachentityfromthefullarticle wereporthowoftentheentity centricmodelgeneratesasummarythatactuallycon tainstherequestedentity whileforallentitiesfromtheinput themodelmen inbothset tings theseratesaremuchhigherthanthebaseline themodelhasdifficultygeneratingsummarieswithentitieswhichareunlikelytoappearinthehumanreferences unimportantentitiesfarfromthebe ginningofthearticle source stylecontrolistheleastimpactfulcontrolintermsofrouge generally weobservethatgeneratedsummariesinthedailymail stylearemorerepetitiveandslightlylongerthanthecnn stylesummaries thismatchesthedifferencesbetweenthetwosourcesinthereferencetext theimpactofstylerequestsmightbegreaterwitharichersetofstyles infuturework weplantoevaluateondatasetswherevariedstylesareavailable wecanalsosetthecontrolvariablesautomaticallyinabsenceofreaderdesiderata forlengthandsource style wesetthevariabletoaconstantvaluethatmaximizesrougeonthevalidationset forentitycontrol anonymizedver inbothcases ourmethodisadvantageousoveralternatives ontheoriginaltext ontheentity anonymizedtext trainingobjectivesareorthogonaltoourworkoncontrolvariablesandweexpectreinforcementlearningtoequallybenefitourmodel tion notably rougeimprovesmoreforshortertextevaluation likelybecauserequestingashorterdocu mentallowsthemodeltoplanitsgeneration compar ingtokikuchietal incontrast wesimplyprovidethedesiredlengthasaspecialtokenandshowthissimpleapproachiseffective lastly wenotethatlength lengthcontrolvssummarylength baselineentity fractionofrequestedentityactuallyoccurringindecodedsummaries tocnn dailymailsincetruncatedrecall rougeeval uationdoesnotpenalizelengthmismatchstrongly overall theimprovementsfromautomaticcontrolshowthatabettermodelcanbeobtainedbyprovidingadditionalinformationduringtraining whenthemodelisnotrequiredtopredictthesummarylengthortheentitiesofinterest itcanassignmorecapacitytogeneratingtextconditionedonthesevariables thisisparticularlyusefulforvariableswhicharehardtopredictfromtheinputduetointrinsicuncertaintylikelength insubsequentwork weplantoexplorearchitecturestoexplicitlydividethepredictionofcontrolvariablesandsequence sequencemapping inferencealign remaindersummarization generally thistaskismoredifficultthansummarizingtheentirearticle thelengthofbothreadportionsandsummariesvariesgreatly itisdifficultforthemodeltodistinguishinformationspe cifictotheremainingportionofthedocumentfromthegeneralpointofthearticle despitethis whenmodelstrainedonsummarizingtheremainderaretaskedwithsummarizingonlyfulldocuments theper ourbaselinealwayspresentsthefullsummary gardlessoftheportionofthearticlepresentedasinput amongourthreeproposedmethods formingtheremaindersummariespost inferenceperformspoorlyasitde pendslargelyonalignmentquality thenewsarticlesarerepetitive soonesummarysentencecanaligntomultiplelocationsinthesource trainingthemodeltoperformremaindersummarizationsignificantlyimprovesourresults modelsthatreceiveonlythere wehypothesizethatpresentingthereadportionofthearticleimprovesthequalityasthemodelcanfocusonthenewinformationintheremainder anexplicitmethodforeliminatingredundancybetweenthereadandtheremainderisrelevantfuturework remaindersummarylengthisparticularlydifficulttopredict wethereforerelyonlengthcontrol wedecodethetestdatawiththissettingwhichpro partitioningisnotanaccuratelengthmodelandwehypothesizethatlengthcontrolcouldprovideagreaterimprovementwithabettermodel thisyear becauseofreallyhighsummerrainfall whichledtogreatfoodavailability thisyear becauseofreallyhighsummerrainfall whichledtogreatfoodavailability alcoholcontent summarywithsource stylecontrolbluehighlightsdifferenttextrequestingcnn style hewaswearingbulletproofvest butroundenteredinhisarmandwentthroughhischest requestingdailymail style drinkandcelebrityparties hellbeabletogambleinacasino buyadrinkinapuborseethehorrorfilm summarieswithvarioussettingsforusercontrolvariablesandremaindersummarization seeetal ourmodelcanthereforeimprovesummaryqualityinadiscernibleway asanaside wefindthatrougeandratingsagreeintwo thirdsofthecases whereatleastfouroutoffivehumansagree levelattributesofgeneratedsummaries suchaslength source style entitiesofinterest andsummarizingonlyremainingportionsofadocument wesimulateuserpreferencesforthesevariablesbysettingthemtooraclevaluesandshowlargerougegains thecontrolvariablesareeffectivewithoutuserinputwhichwedemonstratebyassigningthemfixedvaluestunedonaheld outset thisout performscomparablestateoftheartsummarizationmodelsforbothrougeandhumanevaluation acknowledgmentswethankyanndauphinforhelp fuldiscussions wethankjonasgehringanddenisyaratsforwritingthefairseqtoolkitusedfortheseexperiments referencesdzmitrybahdanau kyunghyuncho andyoshuabengio neuralmachinetranslationbyjointlylearningtoalignandtranslate bowman lukevilnis oriolvinyals drewm dai rafaljozefowicz andsamybengio generatingsentencesfromacontinuousspace inconll sumitchopra michaelauli andalexandermrush abstractivesentencesummarizationwithattentiverecurrentneuralnetworks dipanjandasandandreftmartins asurveyonautomatictextsummarization yannn dauphin angelafan michaelauli anddavidgrangier languagemodelingwithgatedconvolutionalnetworks jessicaficlerandyoavgoldberg controllinglinguisticstyleaspectsinneurallanguagegeneration katjafilippova sentenceandpassagesummariza tionforquestionanswering andmatthiasbethge aneuralalgorithmofartisticstyle jonasgehring michaelauli davidgrangier denisyarats convolutionalsequencetosequencelearning ianj goodfellow jeanpouget abadie mehdimirza bingxu davidwarde farley sherjilozair courville andyoshuabengio generativeadversarialnets innips karlmoritzhermann tomaskocisky edwardgrefen stette lasseespeholt willkay mustafasuleyman andphilblunsom teachingmachinestoreadandcomprehend zhitinghu zichaoyang xiaodanliang ruslansalakhutdinov andericp xing towardcontrolledgenerationoftext inicml yutakikuchi grahamneubig ryoheisasano hiroyatakamura andmanabuokumura controllingoutputlengthinneuralencoder decoders diederikp kingmaandmaxwelling auto encodingvariationalbayes catherinekobus josepcrego andjeansenellart domaincontrolforneuralmachinetranslation inproceedingsoftheinternationalconferencerecentadvancesinnaturallanguageprocessing incomaltd varna bulgaria guillaumelample neilzeghidour nicolasusunier antoinebordes ludovicdenoyer andmarcaurelioranzato fadernetworks manipulatingimagesbyslidingattributes yannlecun bernhardeboser johnsdenker donniehenderson richardehoward wayneehubbard andlawrencedjackel handwrittendigitrecognitionwithaback propagationnetwork chin yewlin rouge apackageforautomaticevaluationofsummaries inworkshopontextsummarizationbranchesout luhn theautomaticcreationofliteratureabstracts minh thangluong quocv ilyasutskever oriolvinyals andlukaszkaiser multi tasksequencetosequencelearning thangluong hieupham manning effectiveapproachestoattention basedneuralmachinetranslation kathleenmckeown textgeneration cambridgeuniversitypress rameshnallapati feifeizhai andbowenzhou summarunner arecurrentneuralnetworkbasedsequencemodelforextractivesummarizationofdocuments rameshnallapati bowenzhou caglargulcehre bingxiang etal abstractivetextsummarizationusingsequence sequencernnsandbeyond con aninenkova kathleenmckeown etal auto maticsummarization razvanpascanu tomasmikolov andyoshuabengio onthedifficultyoftrainingrecurrentneuralnetworks inicml romainpaulus caimingxiong andrichardsocher adeepreinforcedmodelforabstractivesummarization sairajeswar sandeepsubramanian francisdutil christopherjosephpal andaaronc courville adversarialgenerationofnaturallanguage workshoponrepresentationlearningfornlp alexandermrush seasharvard sumitchopra andjasonweston aneuralattentionmodelforsentencesummarization abigailsee peterjliu andchristopherdmanning gettothepoint summarizationwithpointer generatornetworks ricosennrich barryhaddow andalexandrabirch controllingpolitenessinneuralmachinetranslationviasideconstraints humanlanguagetechnologies associationforcomputa tionallinguistics sandiego california ricosennrich barryhaddow andalexandrabirch neuralmachinetranslationofrarewordswithsubwordunits tianxiaoshen taolei reginabarzilay jaakkola styletransferfromnon paralleltextbycross alignment ilyasutskever jamesmartens georgee dahl hinton ontheimportanceofini tializationandmomentumindeeplearning inicml ilyasutskever oriolvinyals andquocvle sequencetosequencelearningwithneuralnetworks junsuzukiandmasaakinagata cutting offredundantrepeatinggenerationsforneuralabstractivesummarization shunsuketakeno masaakinagata andkazuhideyamamoto controllingtargetfeaturesinneuralmachinetranslationviaprefixconstraints asianfederationofnaturallanguageprocessing taipei taiwan ashishvaswani noamshazeer nikiparmar jakobuszkoreit llionjones aidanngomez lukaszkaiser andilliapolosukhin attentionisallyouneed oriolvinyals meirefortunato andnavdeepjaitly pointernetworks oriolvinyals alexandertoshev samybengio anddumitruerhan showandtell aneuralimagecaptiongenerator lantaoyu weinanzhang junwang andyongyu seqgan sequencegenerativeadversarialnetswithpolicygradient inaaai junbozhao kim zhang rush andy cun adversariallyregularizedautoencodersforgeneratingdiscretestructures
