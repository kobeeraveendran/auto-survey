proceedings of the twenty seventh international joint conference on articial intelligence areinforcedtopic awareconvolutionalsequence to com ethz ch y edu com edu eduabstractinthispaper weproposeadeeplearningapproachtotackletheautomaticsummarizationtasksbyincorporatingtopicinformationintotheconvolu tionalsequence to timization throughjointlyattendingtotopicsandword levelalignment ourapproachcanimprovecoherence diversity andinformativenessofgen eratedsummariesviaabiasedprobabilitygenera tionmechanism ontheotherhand reinforcementtraining likescst directlyoptimizesthepro posedmodelwithrespecttothenon differentiablemetricrouge whichalsoavoidstheexposurebiasduringinference wecarryouttheexperimen talevaluationwithstate of the artmethodsoverthegigaword theempiricalresultsdemonstratethesuperiorityofourproposedmethodintheabstractivesummarization itisofinteresttogenerateinformativeandrepresentativenaturallanguagesummarieswhicharecapableofretainingthemainideasofsourcearticles thekeychallengesinautomatictextsummarizationarecorrectlyevaluatingandselectingimpor tantinformation efcientlylteringredundantcontents andproperlyaggregatingrelatedsegmentsandmakinghuman readablesummaries comparedtoothernlptasks theau tomaticsummarizationhasitsowndifculties forexample unlikemachinetranslationtaskswhereinputandoutputse quencesoftensharesimilarlengths summarizationtasksaremorelikelytohaveinputandoutputsequencesgreatlyim balanced besides machinetranslationtasksusuallyhavesomedirectword levelalignmentbetweeninputandoutputsequences whichislessobviousinsummarization therearetwogenresofautomaticsummarizationtech niques namely extractionandabstraction thegoalofex umentandconcatenatingthemverbatim thereforethesum mariescouldbeparaphrasedinmoregeneralterms otherthanextraction abstractivemethodsshouldbeabletoprop erlyrewritethecoreideasofthesourcedocumentandassurethatthegeneratedsummariesaregrammaticallycorrectandhumanreadable whichisclosetothewayhowhumansdosummarizationandthusisofinteresttousinthispaper recently deepneuralnetworkmodelshavebeenwidelyusedfornlptasks inparticular theattentionbasedsequence to however rnn basedmod elsaremorepronetogradientvanishingduetotheirchainstructureofnon linearitiescomparedtothehierarchicalstruc tureofcnn inad dition thetemporaldependenceamongthehiddenstatesofrnnspreventsparallelizationovertheelementsofase quence whichmakesthetraininginefcient inthispa per weproposeanewapproachbasedontheconvolutionalsequence to awareattentionmechanism tothebestofourknowledge thisistherstworkforauto maticabstractivesummarizationthatincorporatesthetopicinformation whichcanprovidethemedandcontextualalign mentinformationintodeeplearningarchitectures inaddi tion themaincontri erationmechanismtoincorporatethetopicinformationintoanautomaticsummarizationmodel whichintro ducescontextualinformationtohelpthemodelgeneratemorecoherentsummarieswithincreaseddiversity proceedings of the twenty seventh international joint conference on articial intelligence differentiablesummarizationmetricrouge whichalsoremediestheexposurebiasissue ourproposedmodelyieldshighaccuracyforabstractivesummarization advancingthestate of the artmethods whichselectimportantcontentsoftextandcombinethemverbatimtoproduceasummary ontheotherhand abstractivesummarizationmodelsareabletoproduceagrammaticalsummarywithanovelexpression basedsequence to however veryfewmethodshaveexploredtheperformanceofconvolu tionalstructureonsummarizationtasks comparedtornns tages includingefcienttrainingbyleveragingparallelcom puting andmitigatingthegradientvanishingproblemduetofewernon notably there basedmodelsinthelanguagemodelingandmachinetranslationtasks erallimitations first themodelistrainedbyminimizingamaximum likelihoodlosswhichissometimesinconsistentwiththemetricthatisevaluatedonthesentencelevel e inaddition zatoetal moreimportantly levelalignmentwhichmaybeinsufcientforsummariza tionandpronetoincoherentgeneratedsummaries there fore thehigherlevelalignmentcouldbeapotentialassist forexample thetopicinformationhasbeenintroducedtoarnn basedsequence to awareconvolutionalsequence to sequencemodelinthissection weproposethereinforcedtopic awarecon volutionalsequence to sequencemodel whichconsistsofaconvolutionalarchitecturewithbothinputwordsandtopics ajointmulti stepattentionmechanism agraphicalillustrationofthetopic awareconvolutionalarchitecture tomright thenwejointlyattendtowordsandtopicsbycomput topicencoderrepresentations finally weproducethetargetsequencethroughabiasedprobabilitygenerationmechanism structure andareinforcementlearningprocedure thegraph icalillustrationofthetopic inthispaper twoconvolutionalblocksareemployed associatedwiththeword levelandtopic levelembeddings respectively weintroducetheformerinthissectionandthelatterinnext alongwiththenewjointattentionandthebiasedgenerationmechanism wealsoaddapositionalembed ding toretaintheor derinformation thus similarly convolutionallayerbothencoderanddecodernetworksarebuiltbystackingsev eralconvolutionallayers supposethatthekernelhaswidthofkandtheinputembeddingdimensionisd theconvolu proceedings of the twenty seventh international joint conference on articial intelligence namely y wisemultipli cation andtheoutputofgluisinrd wedenotetheoutputsofthel takethedecoderforillustration theconvolutionunitionthel thlayeriscomputedbyresidualconnectionsashli multi stepattentiontheattentionmechanismisintroducedtomakethemodelaccesshistoricalinformation tocomputetheattention werstembedthecurrentdecoderstatehliasdli mentjiscomputedasadotproductbetweendliandtheout putzuojofthelastencoderblockuo onceclihasbeencomputed awareattentionmechanismatopicmodelisatypeofstatisticalmodelfordiscoveringtheabstractideasorhiddensemanticstructuresthatoccurinacollectionofsourcearticles inthispaper weemploythetopicmodeltoacquirelatentknowledgeofdocumentsandincorporateatopic awaremechanismintothemulti stepattention whichisexpectedtobringpriorknowledgefortextsummarization duringpre training weuseldatoassigntopicstotheinputtexts thetopnnon universalwordswiththehighestproba bilitiesofeachtopicarechosenintothetopicvocabularyk whilethevocabularyoftextsisdenotedasv weembeditasbeforetoattainwi however wherekisthesizeoftopicvocabulary theembeddingmatrixdtopicisnor malizedfromthecorrespondingpre trainedtopicdistributionmatrix whoserowisproportionaltothenumberoftimesthateachwordisassignedtoeachtopic inthiscase theposi tionalembeddingvectorsarealsoaddedtotheencoderanddecoderelements respectively toobtainthenaltopicem jointattentionagainwetakethedecoderforillustration followingthecon volutionallayerintroducedbefore wecanobtaintheconvo lutionunitionthel thlayerinthedecoderoftopiclevelas duringdecoding levelencoderblockut thentheconditionalinput cli both cliandcliareaddedtotheoutputofthecorrespondingdecoderlayer hliandareapartoftheinputto biasedprobabilitygenerationfinally leveldecoderoutputshloandtopic leveldecoderoutputs proceedings of the twenty seventh international joint conference on articial intelligence wherezisthenormalizer hloiand hltidenotethei thtopdecoderoutputsofwordandtopic respectively andiistheone whenthecandidatewordwisatopicword webiasthegen erationdistributionbythetopicinformation otherwise weignorethetopicpart tosomeextent thecomplexityofthesearchspaceisreducedbyintroducingthetopicbiassinceimportantwordsaremorelikelytobegenerateddirectly likelihoodlossateachde codingstep namely truthoutputsequence minimizingtheobjectiveineq optimalresultswithrespecttotheevaluationmetrics suchasrougewhichmeasuresthesentence levelaccuracyofthegeneratedsummaries thesub ingdatainsteadofitsowndistribution duringthetrainingprocess modelsarefedbyground truthoutputsequencestopredictthenextword whereasduringinferencetheygeneratethenextwordgiventhepredictedwordsasinputs therefore inthetestprocess theerrorofeachstepaccumulatesandleadstothedeteriorationofperformance thesecondreasonforsub optimalitycomesfromtheex ibilityofsummaries themaximum likelihoodobjectivere wardsmodelsthatcanpredictexactlythesamesummariesasreferenceswhilepenalizingthosethatproducedifferenttextseventhoughtheyaresemanticallysimilar providingmulti plereferencesummariesishelpfulyetinsufcientsincetherearealternativestorephraseagivensummary therefore min imizingtheobjectiveineq ertyofsummarization rouge ontheotherhand providesmoreexibleevaluation encouragingmodelstofocusmoreonsemanticmeaningsthanonword levelcorrespondences inordertoaddresssuchissues weutilizeself criticalse dientalgorithmforreinforcementlearning todirectlymax imizethenon differentiablerougemetric duringrein forcementlearning wegeneratetwooutputsequencesgiventheinputsequencex itydistribution andtheotheroutputsequenceysisgeneratedbysamplingfromthedistribution afterobtainingrougescoresofbothsequencesasourrewards i e withscst wecandirectlyoptimizethediscreteevalua tionmetric inaddition theself criticaltest minister talks leader elections ofcials opens poultry free eu army urges world talks foreign investment malaysia thailand meet vietnam u s examplesoftopicwordsforthegigawordcorpus andimprovestraining testtimeconsistency sinceduringlearningwesetthebaselineofthereinforcealgorithmastherewardobtainedbythecurrentmodelinthetest timeinference thescstexposesthemodeltoitsowndistribu avoidingtheexposurebiasissueandthusimprovingthetestperformance weconsiderthreedatasetstoevaluatetheper formanceofdifferentmethodsintheabstractivetextsum marizationtask first theinputsummarypairsconsistofthehead lineandtherstsentenceofthesourcearticles thedatasetisastandardsummarizationevalu ationset unlikethegigawordcorpus generatedreferencesummaries whichmakestheevaluationmoreobjective followingthesettingintheoriginalpaper weusetherstpartoflcstsdatasetfortraining summarypairs trainthecorpusfortopicembeddinginitializationandprovidecandidatesforthebiasedprobabilitygenerationpro cess thetopicembeddingvaluesarenormalizedtoadistri inthispaper notethattheuniversalwordsarelteredoutduringpre training allembeddings nist gov data html proceedings of the twenty seventh international joint conference on articial intelligence topic accuracyonthegigawordcorpusintermsofthefull l bestperformanceoneachscoreisdisplayedinboldface topic accuracyontheinternaltestsetofgigawordcorpusintermsofthefull l bestperformanceoneachscoreisdisplayedinboldface dingandtheoutputproducedbythedecoderbeforethenallinearlayer wealsoadoptthesamedimensionalityforthesizeoflinearlayermappingbe tweenhiddenandembeddingstates dationrougescorestopsincreasingaftereachepochun wersttraintheba sictopic awareconvolutionalmodelwithrespecttoastan dardmaximumlikelihoodobjective andthenswitchtofur ments moreover wechoosetherouge lmetricasthereinforcementrewardfunction nesterovsacceleratedgradi withthemini by stepjustication first awaremodelorreinforcementlearningistested respectively thenwecombinethetwotoshowtheperfor manceofourreinforced topic wereportexamplesofsummariesd thesrilankangovernmentonwednesdayannouncedtheclosureofgovernmentschoolswithimmediateeffectasamilitarycampaignagainsttamilseparatistsescalatedinthenorthofthecountry r srilankaclosesschoolsaswarescalatesor srilankaclosesschoolswithimmediateeffectot srilankaclosesschoolsinwakeofmilitaryattacksd r uscitizenwhospiedforeastgermansgivensuspendedsentenceor usmangetssuspendedjailtermforcommunistspyingot usmanjailedforespionaged malaysianprimeministermahathirmohamadindicatedhewouldsoonrelinquishcontroloftherulingpartytohisdeputyanwaribrahim r mahathirwantsleadershipchangetobesmoothor malaysiasmahathirtorelinquishcontrolofrulingpartyot malaysiasmahathirtosubmitcontrolofrulingpartyd afrenchcrocodilefarmsaidithadsteppedupeffortstobreedoneoftheworldsmostendangeredspecies theindianunk withthehopeofultimatelyreturninganimalstotheirhabitatinsouthasia r frenchfarmoffershopeforendangeredasiancrocsunkpictureor frenchcrocodilefarmstepsupeffortstobreedendangeredspeciesot examplesofgeneratedsummariesonthegigawordcor pus d sourcedocument r referencesummary or outputofthereinforced ot outputofthereinforced topic thewordsmarkedinbluearetopicwordsnotinthereferencesummaries thewordsmarkedinredaretopicwordsneitherinthereferencesummariesnorinthesourcedocuments thefull ods basedneuralmodelsfortextsumma rization theras elmanmodelintroducesaconditionalrnn inwhichtheconditionerisprovidedbyaconvolutionalattention basedencoder thewords basedattentionmodelwhichimplementsalarge vocabularytrick besides mumrisktrainingstrategywhichdirectlyoptimizesmodelparametersinsentencelevelwithrespecttotheevaluationmetrics to sequenceframeworkwithaselectiveencodingmodel theresultshavedemonstratedthatboththetopic awaremoduleandtherein forcementlearningprocesscanimprovetheaccuracyontextsummarization moreover l wealsoevaluateourproposedmodelonthissetandpresentthere again ourproposedmodelachievesthebestperformanceintermsofallthethreerougescores tofurtherdemonstratetheimprovementofreadabilityanddiversitybythetopicinformation wealsopresentsomequal itativeresultsbyrandomlyextractingseveralsummariesfromtest wecomparethereferencesummariestothesummariesgeneratedbyourproposedmodelwithorwithouttopic awaremechanism wecanobservethatwhenthetopicmodelisadopted itcangener atesomeaccuratelydeliveredtopicwordswhicharenotin proceedings of the twenty seventh international joint conference on articial intelligence topic l bestperformanceoneachscoreisdisplayedinboldface thereferencesummariesortheoriginaltexts itisbelievedthatthejointlearningwithapre trainedtopicmodelcanof fermoreinsightfulinformationandimprovethediversityandreadabilityforthesummarization onlydataset wetrainthemodelsonthegigawordcorpusrstandthenevaluatetheirperformanceontheducdataset asthestan dardpractice wereporttherecall lmetricsinthisexperiment topic lmetrics duetothesimilarityofthetwodatasets wedonotpro videqualitativesummarizationexamplesinthisexperiment sincethisisalarge scalechinesedataset suitabledatapreprocessingapproachesshouldbeproposedrst basically therearetwoapproachestopreprocessingthechinesedataset character basedandword based thefor mertakeseachchinesecharacterastheinput whilethelattersplitsaninputsentenceintochinesewords huetal shenetal guetal thecopynet withbothcharacter basedandword basedpreprocessingbyincorporatingthecopyingmechanismintothesequence to sequenceframework inthiswork weadopttheword basedapproachaswebelievethatinthecaseofchinese wordsaremorerelevanttolatentknowl edgeofdocumentsthancharactersare directlyemployingthepack agetoevaluatechinesesummarieswouldyieldunderratedresults inordertoevaluatethesummarizationonthelc stsdataset characterstonumericalids onwhichwethenperformtherougeevaluation sincenotallpreviousworkexplicitlymentionedwhetherword berouge com pages default topic accuracyonthelcstsdatasetintermsofthefull l inlastthreerows theword levelrougescoresarepresentedontheleftandthecharacter levelontheright orcharacter basedrougemetricswerereported weeval uateourproposedmodelwithbothmetricsinordertoob tainacomprehensivecomparison basedscore character basedscore wecanalsoobservethatthecharacter basedresultsofourreinforced topic regardingtoword basedrougescores lmet rics however lscores wesuspectthatitmaybepartlycausedbythebiasedprobabilitygenerationmechanismthatinuenceswordorder whichrequiresfurtherstudies inadditiontorougescores theex amplesdemonstratethatthetopic awaremechanismcanalsoimprovethediversityinchinesesummarizationtasks weproposeatopic tion itisdemonstratedthatthenewtopic awareattentionmechanismintroducessomehigh levelcontextualinforma tionforsummarization theperformanceoftheproposedmodeladvancesstate of the artmethodsonvariousbench markdatasets inaddition ourmodelcanproducesummarieswithbetterinformativeness coherence anddiversity notethattheexperimentsinthisworkaremainlybasedonthesentencesummarization inthefuture weaimtoevalu ateourmodelonthedatasetswherethesourcetextscanbelongparagraphsormulti documents moreover wealsonotethathowtoevaluatetheperformanceonchinesesummariesremainsanopenproblem itisalsoofgreatinteresttostudyonthissubjectinthefuture proceedings of the twenty seventh international joint conference on articial intelligence examplesofsummariesd accordingtothenoticeonthefurtherpromotionandapplicationofnewenergyvehicles ot thenationaldevelopmentandreformcommissionissuedapolicyonfurtherpromotionandapplicationofnewenergyvehiclesd inrecentyears theserviceindustryofsoftwareandinformationtechnologyinchengduhasbeengrowingrapidly rankingrstamongthecitiesinmidwestchina chengduhasbecomechinaswesternsiliconvalley r chengdumakeseveryefforttobuildthewesternsiliconvalleyor thereportofchengdusoftwareandinformationtechnologyserviceindustrydevelopmenthasbeenreleasedot theserviceindustryofsoftwareandinformationtechnologyinchengdurocketstomakeitthewesternsiliconvalleyd thereporterlearnedfromthexinjiangdevelopmentandreformcommissionthattheinitialrailwayconstructionprojectfromkorlatogolmudhadbeenontenderingprocedure thebeltandroadstrategybenetsxinjiang unk unk therailwayfrom unk thedaybefore thereportersofcommercialnewslearnedfromtheshanghaiinternationalweddingphotographicequipmentexhibition whichhasbeenleadinganddeningthedomesticweddingindustry generationnewlymarriedcouplesbyself decidedweddingdecoration weddingprocessandforms r generationnewlymarriedcouplesor or shanghaiinternationalweddingphotographicequipmentexhibitionwasheldot ot examplesofgeneratedsummariesonthelcstsdataset d sourcedocument r referencesummary or outputofthereinforced ot outputofthereinforced topic thewordsmarkedinbluearetopicwordsnotinthereferencesummaries thewordsmarkedinredaretopicwordsneitherinthereferencesummariesnorinthesourcedocuments allthetextsarecarefullytranslatedfromchinese proceedings of the twenty seventh international joint conference on articial intelligence kyunghyuncho andyoshuabengio neuralmachinetranslationbyjointlylearningtoalignandtranslate leenrmckeown sentencefusionformultidocu mentnewssummarization bleietal andrewyng andmichaelijordan latentdirichletallocation choetal bartvanmerrienboer caglargulcehre dzmitrybahdanau fethibougares holgerschwenk andyoshuabengio learningphraserepresentationsusingrnnencoder decoderforstatisticalmachinetranslation chopraetal michaelauli andalexandermrush abstractivesentencesummarizationwithattentiverecurrentneuralnetworks humanlanguagetechnologies dauphinetal angelafan michaelauli anddavidgrangier languagemodelingwithgatedconvolutionalnetworks gehringetal michaelauli davidgrangier denisyarats andyannndauphin convo lutionalsequencetosequencelearning englishgi gawordcorpus guetal zhengdonglu hangli andvictorokli incorporatingcopyingmecha nisminsequence to sequencelearning longshort termmemory huetal qingcaichen andfangzezhu lcsts alargescalechineseshorttextsummariza tiondataset kraaijetal martijnspitters andanettehulth headlineextractionbasedonacombina tionofuni andmultidocumentsummarizationtechniques inproceedingsoftheaclworkshoponautomaticsum marization yewlin rouge apackageforauto maticevaluationofsummaries intextsummarizationbranchesout vol barcelona nallapatietal bingxiang andbowenzhou sequence to sequencernnsfortextsumma rization nallapatietal bowenzhou caglargulcehre bingxiang etal abstractivetextsum marizationusingsequence to sequencernnsandbeyond netoetal alexfreitas andcelsokaest ner automatictextsummarizationusingamachinelearn ingapproach advancesinarticialintelligence overetal hoadang anddonnahar man ducincontext paszkeetal samgross andsoumithchintala paulusetal caimingxiong andrichardsocher adeepreinforcedmodelforabstractivesummarization corr ranzatoetal sumitchopra michaelauli andwojciechzaremba sequenceleveltrainingwithrecurrentneuralnetworks rennieetal etiennemarcheret youssefmroueh jarretross andvaibhavagoel self criticalsequencetrainingforimagecaptioning rushetal sumitchopra andjasonweston aneuralattentionmodelforab stractivesentencesummarization shenetal yuzhao zhiyuanliu maosongsun etal neuralheadlinegenera tionwithsentence wiseoptimization sutskeveretal jamesmartens georgedahl andgeoffreyhinton ontheimportanceofinitializationandmomentumindeeplearning ininternationalconferenceonmachinelearning sutskeveretal oriolvinyals andquocvle sequencetosequencelearningwithneuralnetworks inadvancesinneuralinformationprocessingsystems j williamsandd zipser alearningalgorithmforcontinuallyrunningfullyrecurrentneuralnetworks xingetal weiwu yuwu jieliu yalouhuang mingzhou andwei yingma topicawareneuralresponsegeneration inaaai zhouetal nanyang furuwei andmingzhou selectiveencodingforabstractivesentencesummarization
