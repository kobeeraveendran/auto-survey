proceedings seventh international joint conference articial intelligence areinforcedtopic awareconvolutionalsequence com ethz edu com edu eduabstractinthispaper weproposeadeeplearningapproachtotackletheautomaticsummarizationtasksbyincorporatingtopicinformationintotheconvolu tionalsequence timization throughjointlyattendingtotopicsandword levelalignment ourapproachcanimprovecoherence diversity andinformativenessofgen eratedsummariesviaabiasedprobabilitygenera tionmechanism ontheotherhand reinforcementtraining likescst directlyoptimizesthepro posedmodelwithrespecttothenon differentiablemetricrouge whichalsoavoidstheexposurebiasduringinference wecarryouttheexperimen talevaluationwithstate artmethodsoverthegigaword theempiricalresultsdemonstratethesuperiorityofourproposedmethodintheabstractivesummarization itisofinteresttogenerateinformativeandrepresentativenaturallanguagesummarieswhicharecapableofretainingthemainideasofsourcearticles thekeychallengesinautomatictextsummarizationarecorrectlyevaluatingandselectingimpor tantinformation efcientlylteringredundantcontents andproperlyaggregatingrelatedsegmentsandmakinghuman readablesummaries comparedtoothernlptasks theau tomaticsummarizationhasitsowndifculties forexample unlikemachinetranslationtaskswhereinputandoutputse quencesoftensharesimilarlengths summarizationtasksaremorelikelytohaveinputandoutputsequencesgreatlyim balanced machinetranslationtasksusuallyhavesomedirectword levelalignmentbetweeninputandoutputsequences whichislessobviousinsummarization therearetwogenresofautomaticsummarizationtech niques extractionandabstraction thegoalofex umentandconcatenatingthemverbatim thereforethesum mariescouldbeparaphrasedinmoregeneralterms otherthanextraction abstractivemethodsshouldbeabletoprop erlyrewritethecoreideasofthesourcedocumentandassurethatthegeneratedsummariesaregrammaticallycorrectandhumanreadable whichisclosetothewayhowhumansdosummarizationandthusisofinteresttousinthispaper recently deepneuralnetworkmodelshavebeenwidelyusedfornlptasks inparticular theattentionbasedsequence rnn basedmod elsaremorepronetogradientvanishingduetotheirchainstructureofnon linearitiescomparedtothehierarchicalstruc tureofcnn inad dition thetemporaldependenceamongthehiddenstatesofrnnspreventsparallelizationovertheelementsofase quence whichmakesthetraininginefcient inthispa weproposeanewapproachbasedontheconvolutionalsequence awareattentionmechanism tothebestofourknowledge thisistherstworkforauto maticabstractivesummarizationthatincorporatesthetopicinformation whichcanprovidethemedandcontextualalign mentinformationintodeeplearningarchitectures inaddi tion themaincontri erationmechanismtoincorporatethetopicinformationintoanautomaticsummarizationmodel whichintro ducescontextualinformationtohelpthemodelgeneratemorecoherentsummarieswithincreaseddiversity proceedings seventh international joint conference articial intelligence differentiablesummarizationmetricrouge whichalsoremediestheexposurebiasissue ourproposedmodelyieldshighaccuracyforabstractivesummarization advancingthestate artmethods whichselectimportantcontentsoftextandcombinethemverbatimtoproduceasummary ontheotherhand abstractivesummarizationmodelsareabletoproduceagrammaticalsummarywithanovelexpression basedsequence veryfewmethodshaveexploredtheperformanceofconvolu tionalstructureonsummarizationtasks comparedtornns tages includingefcienttrainingbyleveragingparallelcom puting andmitigatingthegradientvanishingproblemduetofewernon notably basedmodelsinthelanguagemodelingandmachinetranslationtasks erallimitations themodelistrainedbyminimizingamaximum likelihoodlosswhichissometimesinconsistentwiththemetricthatisevaluatedonthesentencelevel inaddition zatoetal moreimportantly levelalignmentwhichmaybeinsufcientforsummariza tionandpronetoincoherentgeneratedsummaries fore thehigherlevelalignmentcouldbeapotentialassist forexample thetopicinformationhasbeenintroducedtoarnn basedsequence awareconvolutionalsequence sequencemodelinthissection weproposethereinforcedtopic awarecon volutionalsequence sequencemodel whichconsistsofaconvolutionalarchitecturewithbothinputwordsandtopics ajointmulti stepattentionmechanism agraphicalillustrationofthetopic awareconvolutionalarchitecture tomright thenwejointlyattendtowordsandtopicsbycomput topicencoderrepresentations finally weproducethetargetsequencethroughabiasedprobabilitygenerationmechanism structure andareinforcementlearningprocedure thegraph icalillustrationofthetopic inthispaper twoconvolutionalblocksareemployed associatedwiththeword levelandtopic levelembeddings respectively weintroducetheformerinthissectionandthelatterinnext alongwiththenewjointattentionandthebiasedgenerationmechanism wealsoaddapositionalembed ding toretaintheor derinformation similarly convolutionallayerbothencoderanddecodernetworksarebuiltbystackingsev eralconvolutionallayers supposethatthekernelhaswidthofkandtheinputembeddingdimensionisd theconvolu proceedings seventh international joint conference articial intelligence wisemultipli cation andtheoutputofgluisinrd wedenotetheoutputsofthel takethedecoderforillustration theconvolutionunitionthel thlayeriscomputedbyresidualconnectionsashli multi stepattentiontheattentionmechanismisintroducedtomakethemodelaccesshistoricalinformation tocomputetheattention werstembedthecurrentdecoderstatehliasdli mentjiscomputedasadotproductbetweendliandtheout putzuojofthelastencoderblockuo onceclihasbeencomputed awareattentionmechanismatopicmodelisatypeofstatisticalmodelfordiscoveringtheabstractideasorhiddensemanticstructuresthatoccurinacollectionofsourcearticles inthispaper weemploythetopicmodeltoacquirelatentknowledgeofdocumentsandincorporateatopic awaremechanismintothemulti stepattention whichisexpectedtobringpriorknowledgefortextsummarization duringpre training weuseldatoassigntopicstotheinputtexts thetopnnon universalwordswiththehighestproba bilitiesofeachtopicarechosenintothetopicvocabularyk whilethevocabularyoftextsisdenotedasv weembeditasbeforetoattainwi wherekisthesizeoftopicvocabulary theembeddingmatrixdtopicisnor malizedfromthecorrespondingpre trainedtopicdistributionmatrix whoserowisproportionaltothenumberoftimesthateachwordisassignedtoeachtopic inthiscase theposi tionalembeddingvectorsarealsoaddedtotheencoderanddecoderelements respectively toobtainthenaltopicem jointattentionagainwetakethedecoderforillustration followingthecon volutionallayerintroducedbefore wecanobtaintheconvo lutionunitionthel thlayerinthedecoderoftopiclevelas duringdecoding levelencoderblockut thentheconditionalinput cli cliandcliareaddedtotheoutputofthecorrespondingdecoderlayer hliandareapartoftheinputto biasedprobabilitygenerationfinally leveldecoderoutputshloandtopic leveldecoderoutputs proceedings seventh international joint conference articial intelligence wherezisthenormalizer hloiand hltidenotethei thtopdecoderoutputsofwordandtopic respectively andiistheone whenthecandidatewordwisatopicword webiasthegen erationdistributionbythetopicinformation weignorethetopicpart tosomeextent thecomplexityofthesearchspaceisreducedbyintroducingthetopicbiassinceimportantwordsaremorelikelytobegenerateddirectly likelihoodlossateachde codingstep truthoutputsequence minimizingtheobjectiveineq optimalresultswithrespecttotheevaluationmetrics suchasrougewhichmeasuresthesentence levelaccuracyofthegeneratedsummaries thesub ingdatainsteadofitsowndistribution duringthetrainingprocess modelsarefedbyground truthoutputsequencestopredictthenextword whereasduringinferencetheygeneratethenextwordgiventhepredictedwordsasinputs inthetestprocess theerrorofeachstepaccumulatesandleadstothedeteriorationofperformance thesecondreasonforsub optimalitycomesfromtheex ibilityofsummaries themaximum likelihoodobjectivere wardsmodelsthatcanpredictexactlythesamesummariesasreferenceswhilepenalizingthosethatproducedifferenttextseventhoughtheyaresemanticallysimilar providingmulti plereferencesummariesishelpfulyetinsufcientsincetherearealternativestorephraseagivensummary min imizingtheobjectiveineq ertyofsummarization rouge ontheotherhand providesmoreexibleevaluation encouragingmodelstofocusmoreonsemanticmeaningsthanonword levelcorrespondences inordertoaddresssuchissues weutilizeself criticalse dientalgorithmforreinforcementlearning todirectlymax imizethenon differentiablerougemetric duringrein forcementlearning wegeneratetwooutputsequencesgiventheinputsequencex itydistribution andtheotheroutputsequenceysisgeneratedbysamplingfromthedistribution afterobtainingrougescoresofbothsequencesasourrewards withscst wecandirectlyoptimizethediscreteevalua tionmetric inaddition theself criticaltest minister talks leader elections ofcials opens poultry free army urges world talks foreign investment malaysia thailand meet vietnam examplesoftopicwordsforthegigawordcorpus andimprovestraining testtimeconsistency sinceduringlearningwesetthebaselineofthereinforcealgorithmastherewardobtainedbythecurrentmodelinthetest timeinference thescstexposesthemodeltoitsowndistribu avoidingtheexposurebiasissueandthusimprovingthetestperformance weconsiderthreedatasetstoevaluatetheper formanceofdifferentmethodsintheabstractivetextsum marizationtask theinputsummarypairsconsistofthehead lineandtherstsentenceofthesourcearticles thedatasetisastandardsummarizationevalu ationset unlikethegigawordcorpus generatedreferencesummaries whichmakestheevaluationmoreobjective followingthesettingintheoriginalpaper weusetherstpartoflcstsdatasetfortraining summarypairs trainthecorpusfortopicembeddinginitializationandprovidecandidatesforthebiasedprobabilitygenerationpro cess thetopicembeddingvaluesarenormalizedtoadistri inthispaper notethattheuniversalwordsarelteredoutduringpre training allembeddings nist gov data html proceedings seventh international joint conference articial intelligence topic accuracyonthegigawordcorpusintermsofthefull bestperformanceoneachscoreisdisplayedinboldface topic accuracyontheinternaltestsetofgigawordcorpusintermsofthefull bestperformanceoneachscoreisdisplayedinboldface dingandtheoutputproducedbythedecoderbeforethenallinearlayer wealsoadoptthesamedimensionalityforthesizeoflinearlayermappingbe tweenhiddenandembeddingstates dationrougescorestopsincreasingaftereachepochun wersttraintheba sictopic awareconvolutionalmodelwithrespecttoastan dardmaximumlikelihoodobjective andthenswitchtofur ments wechoosetherouge lmetricasthereinforcementrewardfunction nesterovsacceleratedgradi withthemini stepjustication awaremodelorreinforcementlearningistested respectively thenwecombinethetwotoshowtheperfor manceofourreinforced topic wereportexamplesofsummariesd thesrilankangovernmentonwednesdayannouncedtheclosureofgovernmentschoolswithimmediateeffectasamilitarycampaignagainsttamilseparatistsescalatedinthenorthofthecountry srilankaclosesschoolsaswarescalatesor srilankaclosesschoolswithimmediateeffectot srilankaclosesschoolsinwakeofmilitaryattacksd uscitizenwhospiedforeastgermansgivensuspendedsentenceor usmangetssuspendedjailtermforcommunistspyingot usmanjailedforespionaged malaysianprimeministermahathirmohamadindicatedhewouldsoonrelinquishcontroloftherulingpartytohisdeputyanwaribrahim mahathirwantsleadershipchangetobesmoothor malaysiasmahathirtorelinquishcontrolofrulingpartyot malaysiasmahathirtosubmitcontrolofrulingpartyd afrenchcrocodilefarmsaidithadsteppedupeffortstobreedoneoftheworldsmostendangeredspecies theindianunk withthehopeofultimatelyreturninganimalstotheirhabitatinsouthasia frenchfarmoffershopeforendangeredasiancrocsunkpictureor frenchcrocodilefarmstepsupeffortstobreedendangeredspeciesot examplesofgeneratedsummariesonthegigawordcor pus sourcedocument referencesummary outputofthereinforced outputofthereinforced topic thewordsmarkedinbluearetopicwordsnotinthereferencesummaries thewordsmarkedinredaretopicwordsneitherinthereferencesummariesnorinthesourcedocuments thefull ods basedneuralmodelsfortextsumma rization theras elmanmodelintroducesaconditionalrnn inwhichtheconditionerisprovidedbyaconvolutionalattention basedencoder thewords basedattentionmodelwhichimplementsalarge vocabularytrick mumrisktrainingstrategywhichdirectlyoptimizesmodelparametersinsentencelevelwithrespecttotheevaluationmetrics sequenceframeworkwithaselectiveencodingmodel theresultshavedemonstratedthatboththetopic awaremoduleandtherein forcementlearningprocesscanimprovetheaccuracyontextsummarization wealsoevaluateourproposedmodelonthissetandpresentthere ourproposedmodelachievesthebestperformanceintermsofallthethreerougescores tofurtherdemonstratetheimprovementofreadabilityanddiversitybythetopicinformation wealsopresentsomequal itativeresultsbyrandomlyextractingseveralsummariesfromtest wecomparethereferencesummariestothesummariesgeneratedbyourproposedmodelwithorwithouttopic awaremechanism wecanobservethatwhenthetopicmodelisadopted itcangener atesomeaccuratelydeliveredtopicwordswhicharenotin proceedings seventh international joint conference articial intelligence topic bestperformanceoneachscoreisdisplayedinboldface thereferencesummariesortheoriginaltexts itisbelievedthatthejointlearningwithapre trainedtopicmodelcanof fermoreinsightfulinformationandimprovethediversityandreadabilityforthesummarization onlydataset wetrainthemodelsonthegigawordcorpusrstandthenevaluatetheirperformanceontheducdataset asthestan dardpractice wereporttherecall lmetricsinthisexperiment topic lmetrics duetothesimilarityofthetwodatasets wedonotpro videqualitativesummarizationexamplesinthisexperiment sincethisisalarge scalechinesedataset suitabledatapreprocessingapproachesshouldbeproposedrst basically therearetwoapproachestopreprocessingthechinesedataset character basedandword based thefor mertakeseachchinesecharacterastheinput whilethelattersplitsaninputsentenceintochinesewords huetal shenetal guetal thecopynet withbothcharacter basedandword basedpreprocessingbyincorporatingthecopyingmechanismintothesequence sequenceframework inthiswork weadopttheword basedapproachaswebelievethatinthecaseofchinese wordsaremorerelevanttolatentknowl edgeofdocumentsthancharactersare directlyemployingthepack agetoevaluatechinesesummarieswouldyieldunderratedresults inordertoevaluatethesummarizationonthelc stsdataset characterstonumericalids onwhichwethenperformtherougeevaluation sincenotallpreviousworkexplicitlymentionedwhetherword berouge com pages default topic accuracyonthelcstsdatasetintermsofthefull inlastthreerows theword levelrougescoresarepresentedontheleftandthecharacter levelontheright orcharacter basedrougemetricswerereported weeval uateourproposedmodelwithbothmetricsinordertoob tainacomprehensivecomparison basedscore character basedscore wecanalsoobservethatthecharacter basedresultsofourreinforced topic regardingtoword basedrougescores lmet rics lscores wesuspectthatitmaybepartlycausedbythebiasedprobabilitygenerationmechanismthatinuenceswordorder whichrequiresfurtherstudies inadditiontorougescores theex amplesdemonstratethatthetopic awaremechanismcanalsoimprovethediversityinchinesesummarizationtasks weproposeatopic tion itisdemonstratedthatthenewtopic awareattentionmechanismintroducessomehigh levelcontextualinforma tionforsummarization theperformanceoftheproposedmodeladvancesstate artmethodsonvariousbench markdatasets inaddition ourmodelcanproducesummarieswithbetterinformativeness coherence anddiversity notethattheexperimentsinthisworkaremainlybasedonthesentencesummarization inthefuture weaimtoevalu ateourmodelonthedatasetswherethesourcetextscanbelongparagraphsormulti documents wealsonotethathowtoevaluatetheperformanceonchinesesummariesremainsanopenproblem itisalsoofgreatinteresttostudyonthissubjectinthefuture proceedings seventh international joint conference articial intelligence examplesofsummariesd accordingtothenoticeonthefurtherpromotionandapplicationofnewenergyvehicles thenationaldevelopmentandreformcommissionissuedapolicyonfurtherpromotionandapplicationofnewenergyvehiclesd inrecentyears theserviceindustryofsoftwareandinformationtechnologyinchengduhasbeengrowingrapidly rankingrstamongthecitiesinmidwestchina chengduhasbecomechinaswesternsiliconvalley chengdumakeseveryefforttobuildthewesternsiliconvalleyor thereportofchengdusoftwareandinformationtechnologyserviceindustrydevelopmenthasbeenreleasedot theserviceindustryofsoftwareandinformationtechnologyinchengdurocketstomakeitthewesternsiliconvalleyd thereporterlearnedfromthexinjiangdevelopmentandreformcommissionthattheinitialrailwayconstructionprojectfromkorlatogolmudhadbeenontenderingprocedure thebeltandroadstrategybenetsxinjiang unk unk therailwayfrom unk thedaybefore thereportersofcommercialnewslearnedfromtheshanghaiinternationalweddingphotographicequipmentexhibition whichhasbeenleadinganddeningthedomesticweddingindustry generationnewlymarriedcouplesbyself decidedweddingdecoration weddingprocessandforms generationnewlymarriedcouplesor shanghaiinternationalweddingphotographicequipmentexhibitionwasheldot examplesofgeneratedsummariesonthelcstsdataset sourcedocument referencesummary outputofthereinforced outputofthereinforced topic thewordsmarkedinbluearetopicwordsnotinthereferencesummaries thewordsmarkedinredaretopicwordsneitherinthereferencesummariesnorinthesourcedocuments allthetextsarecarefullytranslatedfromchinese proceedings seventh international joint conference articial intelligence kyunghyuncho andyoshuabengio neuralmachinetranslationbyjointlylearningtoalignandtranslate leenrmckeown sentencefusionformultidocu mentnewssummarization bleietal andrewyng andmichaelijordan latentdirichletallocation choetal bartvanmerrienboer caglargulcehre dzmitrybahdanau fethibougares holgerschwenk andyoshuabengio learningphraserepresentationsusingrnnencoder decoderforstatisticalmachinetranslation chopraetal michaelauli andalexandermrush abstractivesentencesummarizationwithattentiverecurrentneuralnetworks humanlanguagetechnologies dauphinetal angelafan michaelauli anddavidgrangier languagemodelingwithgatedconvolutionalnetworks gehringetal michaelauli davidgrangier denisyarats andyannndauphin convo lutionalsequencetosequencelearning englishgi gawordcorpus guetal zhengdonglu hangli andvictorokli incorporatingcopyingmecha nisminsequence sequencelearning longshort termmemory huetal qingcaichen andfangzezhu lcsts alargescalechineseshorttextsummariza tiondataset kraaijetal martijnspitters andanettehulth headlineextractionbasedonacombina tionofuni andmultidocumentsummarizationtechniques inproceedingsoftheaclworkshoponautomaticsum marization yewlin rouge apackageforauto maticevaluationofsummaries intextsummarizationbranchesout vol barcelona nallapatietal bingxiang andbowenzhou sequence sequencernnsfortextsumma rization nallapatietal bowenzhou caglargulcehre bingxiang etal abstractivetextsum marizationusingsequence sequencernnsandbeyond netoetal alexfreitas andcelsokaest ner automatictextsummarizationusingamachinelearn ingapproach advancesinarticialintelligence overetal hoadang anddonnahar man ducincontext paszkeetal samgross andsoumithchintala paulusetal caimingxiong andrichardsocher adeepreinforcedmodelforabstractivesummarization corr ranzatoetal sumitchopra michaelauli andwojciechzaremba sequenceleveltrainingwithrecurrentneuralnetworks rennieetal etiennemarcheret youssefmroueh jarretross andvaibhavagoel self criticalsequencetrainingforimagecaptioning rushetal sumitchopra andjasonweston aneuralattentionmodelforab stractivesentencesummarization shenetal yuzhao zhiyuanliu maosongsun etal neuralheadlinegenera tionwithsentence wiseoptimization sutskeveretal jamesmartens georgedahl andgeoffreyhinton ontheimportanceofinitializationandmomentumindeeplearning ininternationalconferenceonmachinelearning sutskeveretal oriolvinyals andquocvle sequencetosequencelearningwithneuralnetworks inadvancesinneuralinformationprocessingsystems williamsandd zipser alearningalgorithmforcontinuallyrunningfullyrecurrentneuralnetworks xingetal weiwu yuwu jieliu yalouhuang mingzhou andwei yingma topicawareneuralresponsegeneration inaaai zhouetal nanyang furuwei andmingzhou selectiveencodingforabstractivesentencesummarization
