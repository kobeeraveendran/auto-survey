saliency maps generation automatic text summarization david tuckey krysia broda alessandra russo department computing imperial college london david broda ac uk l u j g l s c v v x r abstract saliency map generation techniques explainable ai literature broad range machine learning applications goal question limits approaches paper apply complex tasks wise relevance propagation lrp sequence attention model trained text marization dataset obtain unexpected saliency maps discuss rightfulness nations argue need quantitative way testing counterfactual case judge fulness saliency maps suggest protocol check validity importance attributed input saliency maps obtained capture real use input features network use example discuss careful need accepting explanation introduction lime algorithm ribeiro et al nation techniques focusing nding importance features regard specic prediction soared ways nding saliency maps called heat maps way like visualize interested paper use technique extreme task highlights questions validity evaluation approach like rst set vocabulary use agree saliency maps explanations similar attribution human explanation process miller prefer importance mapping input attribution tion talk importance input relevance score regard model s computation lusion human understanding model result exist multiple ways generate saliency maps input non linear classiers bach et al tavon et al samek et al refer reader adadi berrada survey explainable ai general use paper layer wise relevance gation lrp bach et al aims redistributing value classifying function input obtain importance attribution rst created explain classication neural networks image recognition tasks later successfully applied text convolutional neural networks cnn arras et al short term memory lstm networks sentiment analysis arras et al goal paper test limits use technique complex tasks notion input importance simple topic classication sentiment analysis changed classication task generative task chose complex text translation easily nd word word spondence importance input output chose text summarization consider abstractive informative text summarization meaning write summary words retain important information inal text refer reader radev et al details task different variants exist success deep sequence sequence models text translation bahdanau et al approaches applied text summarization tasks rush et al et al nallapati et al use tures apply lrp obtain saliency map word generated summaries supposed represent use input features element output sequence observe saliency maps text nearly identical related attention distribution propose way check validity creating seen terfactual experiment synthesis saliency maps technique arras et al cases help identify important input features need rigorously check tance attributions trusting regardless mapping makes sense nally argue process identifying important input features verifying saliency maps important generation step task model present section baseline model et al trained cnn daily mail dataset reproduce results et al apply lrp dataset training task cnn daily mail dataset nallapati et al text summarization dataset adapted deepmind answering dataset hermann et al contains thousand news articles coupled maries sentences summaries fact highlights articles provided media articles average length words maries words training pairs test pairs similarly et al limit ing training prediction input text words generate summaries words pad shorter texts unknown token truncate longer texts embed texts summaries vocabulary size recreating parameters et al model baseline model deep sequence sequence coder decoder model attention encoder rectional long short term cell hochreiter schmidhuber decoder single lstm cell attention mechanism attention mechanism puted bahdanau et al use greedy search decoding train end end including words embeddings embedding size den state size lstm cells obtained summaries train parameters network epochs achieve results qualitatively alent results et al obtain summaries broadly relevant text match target summaries observe problems wrong reproduction factual details replacing rare words common alternatives repeating non sense sentence figure example summary obtained compared target target summary marseille prosecutor says far videos crash investigation despite media reports journalists bild paris match dent video clip real editor says andreas lubitz informed lufthansa training school episode severe depression airline says generated summary s unk found crash board ight video found source close investigation video found source close investigation truncated figure example target generated generated summary text summaries generate far valid summaries information texts sufcient look attribution lrp pick general subject original text layer wise relevance propagation present section layer wise relevance gation lrp bach et al technique attribute importance input features adapted model generated saliency maps lrp redistributes output model layer input transmitting information backwards layers propagated backwards portance relevance lrp particularity attribute negative positive relevance positive relevance posed represent evidence led classier s result negative relevance represents evidence participated negatively prediction mathematical description initialize relevance output layer value predicted class softmax describe locally propagation backwards relevance layer layer normal neural network layers use form lrp epsilon stabilizer bach et al write relevance received neuron layer l neuron j layer l wl j dl ij zl j j j j wl network s weight parameter set ij training activation neuron layer l stabilizing term set dl dimension l th layer bias neuron j layer l zl j relevance neuron computed sum relevance received lstm cells use method arras et al solve problem posed element wise multiplications vectors arras et al noted computation happened inside lstm cell volved gate vector vector containing tion gate vector containing value essentially ltering second vector allow passing relevant information considering propagate relevance element wise multiplication operation upper layer s relevance information vector gate vector generation saliency maps use method transmit relevance tention mechanism encoder bahdanau s attention bahdanau et al uses element wise cations depict figure transmission end output layer input decoder attention mechanism bidirectional encoder sum relevance word embedding token s relevance arras et al way generate saliency maps differs bit usual context lrp essentially nt passes decoder attention figure representation propagation relevance output input mechanism previous decoding time step passed encoder takes account relevance transiting direction bidirectional nature encoding lstm cell classication word summary generate relevance attribution rst words generated summary point repeat means text obtain different saliency maps supposed represent relevance input specic generated word summary experimental results section present results extracting butions sequence sequence model trained stractive text summarization rst discuss ference different saliency maps obtain propose protocol validate mappings observations rst observation text saliency maps identical mapping highlights mainly input words slight ations importance figure example nearly identical attributions distant unrelated words summary saliency map generated lrp uncorrelated attention distribution participated generation output word tion distribution changes drastically words generated summary impacting signicantly tribution input text deleted experiment relevance propagated attention mechanism encoder nt observe changes saliency map seen evidence attention tion explanation prediction misleading information received decoder importance allocates attention state low happen application information transmitted encoder decoder attention mechanism decoding step figure left saliency map truncated input text second generated word right saliency map cated input text generated word investigation difference mappings marginal changes marginally quantifying ence attention distribution saliency map multiple tasks possible future work second observation saliency map nt highlight right things input summary generates saliency maps figure correspond summary figure nt word video highlighted input text important output allows question good saliency maps sense question actually sent network s use input features truthfulness attribution regard computation meaning attribution truthful regard putation actually highlights important input features network attended prediction proceed measure truthfulness attributions validating quantitatively validating attributions propose validate saliency maps similar way arras et al incrementally deleting important words input text observe change sulting generated summaries rst dene important unimportant input words mean saliency maps texts relevance transmitted lrp positive negative average absolute value relevance saliency maps obtain ranking relevant words idea input words negative relevance impact resulting generated word ing positively word relevance close zero important try different methods like averaging raw relevance eraging scaled absolute value negative relevance scaled constant factor absolute value average deliver best results delete incrementally important words words highest average input compared control experiment consists deleting tant word compare degradation resulting maries obtain mitigated results texts observe quick degradation deleting important words observed deleting unimportant words figure test examples nt observe nicant difference settings figure unk deleting important words unk unk unk unk unk unk unk unk unk unk unk unk unk unk deleting important words unk lmed magazine unk video found source close investigation unk said video recovered phone wreckage site truncated figure summary figure generated deleting important unimportant words input text observe signicant difference summary degradation experiments decoder repeats unknown token argue second summary figure better rst makes better sentences model generates inaccurate summaries wish statement allows attribution ated text origin summaries figure truthful regard network s computation use studies example text origin figure nt draw conclusions attribution generated interesting point saliency map nt look better meaning apparent way determining truthfulness regard tion quantitative validation brings believe simpler tasks saliency maps deleting important words unk mass index carried taliban unk mass index china s strike hard campaign notion deleting important words unk mass index carried wake horric attack school peshawar government issued ban executions figure summary test text generated deleting important unimportant words input text observe signicant difference summary degradation experiments sense example highlighting animal image classication task actually representing network attended way dened saying counterfactual case experiment important words input deleted different summary factuals difcult dene image cation example applying mask image ltering colour pattern believe dening counterfactual testing allows sure evaluate truthfulness attributions weight trust conclusion work implemented applied lrp sequence sequence model trained complex task usual text summarization previous work solve difculties posed lrp lstm cells adapted technique bahdanau et al tention mechanism observed peculiar behaviour saliency maps words output summary cal uncorrelated attention distribution proceeded validate attributions averaging absolute value relevance saliency maps obtain ranking word important important proceeded delete showed cases saliency maps ful network s computation meaning light input features network focused showed cases saliency maps capture important input features brought cuss fact attributions sufcient selves need dene counter factual case test measure truthful saliency maps future work look saliency maps generated applying lrp pointer generator networks compare current results mathematically justifying average validating saliency maps additional work needed validation saliency maps counterfactual tests exploitation evaluation saliency map important step overlooked samek et al wojciech samek alexander binder gregoire montavon klaus robert muller evaluating visualization ieee deep neural network learned actions neural networks learning systems sebastian lapuschkin et al abigail peter j liu christopher d manning point summarization arxiv preprint generator networks references adadi berrada amina adadi mohammed berrada peeking inside black box survey ieee access explainable articial intelligence xai arras et al leila arras franziska horn gregoire montavon klaus robert muller wojciech samek relevant text document interpretable machine learning approach plos arras et al leila arras gregoire montavon klaus robert muller wojciech samek explaining recurrent neural network predictions sentiment analysis bach et al sebastian bach alexander binder gregoire montavon frederick klauschen klaus robert muller wojciech samek pixel wise explanations non linear classier decisions layer wise relevance propagation plos bahdanau et al dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate arxiv preprint hermann et al karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend pages hochreiter schmidhuber sepp hochreiter jurgen schmidhuber long short term memory neural computation miller tim miller explanation articial gence insights social sciences articial gence montavon et al montavon wojciech samek klaus robert muller methods preting understanding deep neural networks digital signal processing feb nallapati et al ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre bing xiang abstractive text summarization sequence sequence rnns arxiv preprint radev et al dragomir r radev eduard hovy kathleen mckeown introduction special issue summarization computational linguistics ribeiro et al marco tulio ribeiro sameer singh carlos guestrin trust plaining predictions classier proceedings acm sigkdd international conference knowledge discovery data mining pages rush et al alexander m rush sumit chopra neural attention model arxiv preprint jason weston stractive sentence summarization sep
