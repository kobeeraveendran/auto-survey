saliency maps generation automatic text summarization david tuckey krysia broda alessandra russo department computing imperial college london david broda abstract saliency map generation techniques explainable literature broad range machine learning applications goal question limits approaches paper apply complex tasks wise relevance propagation lrp sequence attention model trained text marization dataset obtain unexpected saliency maps discuss rightfulness nations argue need quantitative way testing counterfactual case judge fulness saliency maps suggest protocol check validity importance attributed input saliency maps obtained capture real use input features network use example discuss careful need accepting explanation introduction lime algorithm ribeiro nation techniques focusing nding importance features regard specic prediction soared ways nding saliency maps called heat maps way like visualize interested paper use technique extreme task highlights questions validity evaluation approach like rst set vocabulary use agree saliency maps explanations similar attribution human explanation process miller prefer importance mapping input attribution tion talk importance input relevance score regard model computation lusion human understanding model result exist multiple ways generate saliency maps input non linear classiers bach tavon samek refer reader adadi berrada survey explainable general use paper layer wise relevance gation lrp bach aims redistributing value classifying function input obtain importance attribution rst created explain classication neural networks image recognition tasks later successfully applied text convolutional neural networks cnn arras short term memory lstm networks sentiment analysis arras goal paper test limits use technique complex tasks notion input importance simple topic classication sentiment analysis changed classication task generative task chose complex text translation easily word word spondence importance input output chose text summarization consider abstractive informative text summarization meaning write summary words retain important information inal text refer reader radev details task different variants exist success deep sequence sequence models text translation bahdanau approaches applied text summarization tasks rush nallapati use tures apply lrp obtain saliency map word generated summaries supposed represent use input features element output sequence observe saliency maps text nearly identical related attention distribution propose way check validity creating seen terfactual experiment synthesis saliency maps technique arras cases help identify important input features need rigorously check tance attributions trusting regardless mapping makes sense nally argue process identifying important input features verifying saliency maps important generation step task model present section baseline model trained cnn daily mail dataset reproduce results apply lrp dataset training task cnn daily mail dataset nallapati text summarization dataset adapted deepmind answering dataset hermann contains thousand news articles coupled maries sentences summaries fact highlights articles provided media articles average length words maries words training pairs test pairs similarly limit ing training prediction input text words generate summaries words pad shorter texts unknown token truncate longer texts embed texts summaries vocabulary size recreating parameters model baseline model deep sequence sequence coder decoder model attention encoder rectional long short term cell hochreiter schmidhuber decoder single lstm cell attention mechanism attention mechanism puted bahdanau use greedy search decoding train end end including words embeddings embedding size den state size lstm cells obtained summaries train parameters network epochs achieve results qualitatively alent results obtain summaries broadly relevant text match target summaries observe problems wrong reproduction factual details replacing rare words common alternatives repeating non sense sentence figure example summary obtained compared target target summary marseille prosecutor says far videos crash investigation despite media reports journalists bild paris match dent video clip real editor says andreas lubitz informed lufthansa training school episode severe depression airline says generated summary unk found crash board ight video found source close investigation video found source close investigation truncated figure example target generated generated summary text summaries generate far valid summaries information texts sufcient look attribution lrp pick general subject original text layer wise relevance propagation present section layer wise relevance gation lrp bach technique attribute importance input features adapted model generated saliency maps lrp redistributes output model layer input transmitting information backwards layers propagated backwards portance relevance lrp particularity attribute negative positive relevance positive relevance posed represent evidence led classier result negative relevance represents evidence participated negatively prediction mathematical description initialize relevance output layer value predicted class softmax describe locally propagation backwards relevance layer layer normal neural network layers use form lrp epsilon stabilizer bach write relevance received neuron layer neuron layer network weight parameter set training activation neuron layer stabilizing term set dimension layer bias neuron layer relevance neuron computed sum relevance received lstm cells use method arras solve problem posed element wise multiplications vectors arras noted computation happened inside lstm cell volved gate vector vector containing tion gate vector containing value essentially ltering second vector allow passing relevant information considering propagate relevance element wise multiplication operation upper layer relevance information vector gate vector generation saliency maps use method transmit relevance tention mechanism encoder bahdanau attention bahdanau uses element wise cations depict figure transmission end output layer input decoder attention mechanism bidirectional encoder sum relevance word embedding token relevance arras way generate saliency maps differs bit usual context lrp essentially passes decoder attention figure representation propagation relevance output input mechanism previous decoding time step passed encoder takes account relevance transiting direction bidirectional nature encoding lstm cell classication word summary generate relevance attribution rst words generated summary point repeat means text obtain different saliency maps supposed represent relevance input specic generated word summary experimental results section present results extracting butions sequence sequence model trained stractive text summarization rst discuss ference different saliency maps obtain propose protocol validate mappings observations rst observation text saliency maps identical mapping highlights mainly input words slight ations importance figure example nearly identical attributions distant unrelated words summary saliency map generated lrp uncorrelated attention distribution participated generation output word tion distribution changes drastically words generated summary impacting signicantly tribution input text deleted experiment relevance propagated attention mechanism encoder observe changes saliency map seen evidence attention tion explanation prediction misleading information received decoder importance allocates attention state low happen application information transmitted encoder decoder attention mechanism decoding step figure left saliency map truncated input text second generated word right saliency map cated input text generated word investigation difference mappings marginal changes marginally quantifying ence attention distribution saliency map multiple tasks possible future work second observation saliency map highlight right things input summary generates saliency maps figure correspond summary figure word video highlighted input text important output allows question good saliency maps sense question actually sent network use input features truthfulness attribution regard computation meaning attribution truthful regard putation actually highlights important input features network attended prediction proceed measure truthfulness attributions validating quantitatively validating attributions propose validate saliency maps similar way arras incrementally deleting important words input text observe change sulting generated summaries rst dene important unimportant input words mean saliency maps texts relevance transmitted lrp positive negative average absolute value relevance saliency maps obtain ranking relevant words idea input words negative relevance impact resulting generated word ing positively word relevance close zero important try different methods like averaging raw relevance eraging scaled absolute value negative relevance scaled constant factor absolute value average deliver best results delete incrementally important words words highest average input compared control experiment consists deleting tant word compare degradation resulting maries obtain mitigated results texts observe quick degradation deleting important words observed deleting unimportant words figure test examples observe nicant difference settings figure unk deleting important words unk unk unk unk unk unk unk unk unk unk unk unk unk unk deleting important words unk lmed magazine unk video found source close investigation unk said video recovered phone wreckage site truncated figure summary figure generated deleting important unimportant words input text observe signicant difference summary degradation experiments decoder repeats unknown token argue second summary figure better rst makes better sentences model generates inaccurate summaries wish statement allows attribution ated text origin summaries figure truthful regard network computation use studies example text origin figure draw conclusions attribution generated interesting point saliency map look better meaning apparent way determining truthfulness regard tion quantitative validation brings believe simpler tasks saliency maps deleting important words unk mass index carried taliban unk mass index china strike hard campaign notion deleting important words unk mass index carried wake horric attack school peshawar government issued ban executions figure summary test text generated deleting important unimportant words input text observe signicant difference summary degradation experiments sense example highlighting animal image classication task actually representing network attended way dened saying counterfactual case experiment important words input deleted different summary factuals difcult dene image cation example applying mask image ltering colour pattern believe dening counterfactual testing allows sure evaluate truthfulness attributions weight trust conclusion work implemented applied lrp sequence sequence model trained complex task usual text summarization previous work solve difculties posed lrp lstm cells adapted technique bahdanau tention mechanism observed peculiar behaviour saliency maps words output summary cal uncorrelated attention distribution proceeded validate attributions averaging absolute value relevance saliency maps obtain ranking word important important proceeded delete showed cases saliency maps ful network computation meaning light input features network focused showed cases saliency maps capture important input features brought cuss fact attributions sufcient selves need dene counter factual case test measure truthful saliency maps future work look saliency maps generated applying lrp pointer generator networks compare current results mathematically justifying average validating saliency maps additional work needed validation saliency maps counterfactual tests exploitation evaluation saliency map important step overlooked samek wojciech samek alexander binder gregoire montavon klaus robert muller evaluating visualization ieee deep neural network learned actions neural networks learning systems sebastian lapuschkin abigail peter liu christopher manning point summarization arxiv preprint generator networks references adadi berrada amina adadi mohammed berrada peeking inside black box survey ieee access explainable articial intelligence xai arras leila arras franziska horn gregoire montavon klaus robert muller wojciech samek relevant text document interpretable machine learning approach plos arras leila arras gregoire montavon klaus robert muller wojciech samek explaining recurrent neural network predictions sentiment analysis bach sebastian bach alexander binder gregoire montavon frederick klauschen klaus robert muller wojciech samek pixel wise explanations non linear classier decisions layer wise relevance propagation plos bahdanau dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate arxiv preprint hermann karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend pages hochreiter schmidhuber sepp hochreiter jurgen schmidhuber long short term memory neural computation miller tim miller explanation articial gence insights social sciences articial gence montavon montavon wojciech samek klaus robert muller methods preting understanding deep neural networks digital signal processing feb nallapati ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre bing xiang abstractive text summarization sequence sequence rnns arxiv preprint radev dragomir radev eduard hovy kathleen mckeown introduction special issue summarization computational linguistics ribeiro marco tulio ribeiro sameer singh carlos guestrin trust plaining predictions classier proceedings acm sigkdd international conference knowledge discovery data mining pages rush alexander rush sumit chopra neural attention model arxiv preprint jason weston stractive sentence summarization sep
