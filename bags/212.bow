saliency maps generation for automatic text summarization david tuckey krysia broda alessandra russo department of computing imperial college london david broda a ac uk l u j g l s c v v i x r a abstract saliency map generation techniques are at the front of explainable ai literature for a broad range of machine learning applications our goal is to question the limits of these approaches on more in this paper we apply complex tasks wise relevance propagation lrp to a to sequence attention model trained on a text marization dataset we obtain unexpected saliency maps and discuss the rightfulness of these nations we argue that we need a quantitative way of testing the counterfactual case to judge the fulness of the saliency maps we suggest a protocol to check the validity of the importance attributed to the input and show that the saliency maps obtained sometimes capture the real use of the input features by the network and sometimes do not we use this example to discuss how careful we need to be when accepting them as explanation introduction ever since the lime algorithm ribeiro et al nation techniques focusing on nding the importance of put features in regard of a specic prediction have soared and we now have many ways of nding saliency maps also called heat maps because of the way we like to visualize them we are interested in this paper by the use of such a technique in an extreme task that highlights questions about the validity and evaluation of the approach we would like to rst set the vocabulary we will use we agree that saliency maps are not explanations in themselves and that they are more similar to attribution which is only one part of the human explanation process miller we will prefer to call this importance mapping of the input an attribution rather than an tion we will talk about the importance of the input relevance score in regard to the model s computation and not make lusion to any human understanding of the model as a result there exist multiple ways to generate saliency maps over the input for non linear classiers bach et al tavon et al samek et al we refer the reader to adadi and berrada for a survey of explainable ai in general we use in this paper layer wise relevance gation lrp bach et al which aims at redistributing the value of the classifying function on the input to obtain the importance attribution it was rst created to explain the classication of neural networks on image recognition tasks it was later successfully applied to text using convolutional neural networks cnn arras et al and then short term memory lstm networks for sentiment analysis arras et al our goal in this paper is to test the limits of the use of such a technique for more complex tasks where the notion of input importance might not be as simple as in topic classication or sentiment analysis we changed from a classication task to a generative task and chose a more complex one than text translation in which we can easily nd a word to word spondence importance between input and output we chose text summarization we consider abstractive and informative text summarization meaning that we write a summary in our own words and retain the important information of the inal text we refer the reader to radev et al for more details on the task and the different variants that exist since the success of deep sequence to sequence models for text translation bahdanau et al the same approaches have been applied to text summarization tasks rush et al see et al nallapati et al which use tures on which we can apply lrp we obtain one saliency map for each word in the generated summaries supposed to represent the use of the input features for each element of the output sequence we observe that all the saliency maps for a text are nearly identical and related with the attention distribution we propose a way to check their validity by creating what could be seen as a terfactual experiment from a synthesis of the saliency maps using the same technique as in arras et al we show that in some but not all cases they help identify the important input features and that we need to rigorously check tance attributions before trusting them regardless of whether or not the mapping makes sense to us we nally argue that in the process of identifying the important input features verifying the saliency maps is as important as the generation step if not more the task and the model we present in this section the baseline model from see et al trained on the cnn daily mail dataset we reproduce the results from see et al to then apply lrp on it dataset and training task the cnn daily mail dataset nallapati et al is a text summarization dataset adapted from the deepmind answering dataset hermann et al it contains around three hundred thousand news articles coupled with maries of about three sentences these summaries are in fact highlights of the articles provided by the media themselves articles have an average length of words and the maries of words we had training pairs and test pairs similarly to see et al we limit ing training and prediction the input text to words and generate summaries of words we pad the shorter texts using an unknown token and truncate the longer texts we embed the texts and summaries using a vocabulary of size thus recreating the same parameters as see et al the model the baseline model is a deep sequence to sequence coder decoder model with attention the encoder is a rectional long short term cell hochreiter and schmidhuber and the decoder a single lstm cell with attention mechanism the attention mechanism is puted as in bahdanau et al and we use a greedy search for decoding we train end to end including the words embeddings the embedding size used is of and the den state size of the lstm cells is of obtained summaries we train the parameters of the network for about epochs until we achieve results that are qualitatively alent to the results of see et al we obtain summaries that are broadly relevant to the text but do not match the target summaries very well we observe the same problems such as wrong reproduction of factual details replacing rare words with more common alternatives or repeating non sense after the third sentence we can see in figure an example of summary obtained compared to the target one target summary marseille prosecutor says so far no videos were used in the crash investigation despite media reports journalists at bild and paris match are very dent the video clip is real an editor says andreas lubitz had informed his lufthansa training school of an episode of severe depression airline says generated summary s the unk was found in a crash on the board ight the video was found by a source close to the investigation the video was found by a source close to the investigation truncated figure top example of target generated bottom generated summary for the same text the summaries we generate are far from being valid summaries of the information in the texts but are sufcient to look at the attribution that lrp will give us they pick up the general subject of the original text layer wise relevance propagation we present in this section the layer wise relevance gation lrp bach et al technique that we used to attribute importance to the input features together with how we adapted it to our model and how we generated the saliency maps lrp redistributes the output of the model from the put layer to the input by transmitting information backwards through the layers we call this propagated backwards portance the relevance lrp has the particularity to attribute negative and positive relevance a positive relevance is posed to represent evidence that led to the classier s result while negative relevance represents evidence that participated negatively in the prediction mathematical description we initialize the relevance of the output layer to the value of the predicted class before softmax and we then describe locally the propagation backwards of the relevance from layer to layer for normal neural network layers we use the form of lrp with epsilon stabilizer bach et al we write down the relevance received by the neuron i of layer l from the neuron j of layer l wl j dl ij zl i j j j j where wl is the network s weight parameter set during ij training i is the activation of neuron i on layer l is the stabilizing term set to and dl is the dimension of the l th layer is the bias for neuron j of layer l zl j the relevance of a neuron is then computed as the sum of the relevance he received from the above for lstm cells we use the method from arras et al to solve the problem posed by the element wise multiplications of vectors arras et al noted that when such computation happened inside an lstm cell it always volved a gate vector and another vector containing tion the gate vector containing only value between and is essentially ltering the second vector to allow the passing of relevant information considering this when we propagate relevance through an element wise multiplication operation we give all the upper layer s relevance to the information vector and none to the gate vector generation of the saliency maps we use the same method to transmit relevance through the tention mechanism back to the encoder because bahdanau s attention bahdanau et al uses element wise cations as well we depict in figure the transmission to end from the output layer to the input through the decoder attention mechanism and then the bidirectional encoder we then sum up the relevance on the word embedding to get the token s relevance as arras et al the way we generate saliency maps differs a bit from the usual context in which lrp is used as we essentially do nt it passes through the decoder and attention figure representation of the propagation of the relevance from the output to the input mechanism for each previous decoding time step then is passed onto the encoder which takes into account the relevance transiting in both direction due to the bidirectional nature of the encoding lstm cell have one classication but one for each word in the summary we generate a relevance attribution for the rst words of the generated summary as after this point they often repeat themselves this means that for each text we obtain different saliency maps each one supposed to represent the relevance of the input for a specic generated word in the summary experimental results in this section we present our results from extracting butions from the sequence to sequence model trained for stractive text summarization we rst have to discuss the ference between the different saliency maps we obtain and then we propose a protocol to validate the mappings first observations the rst observation that is made is that for one text the saliency maps are almost identical indeed each mapping highlights mainly the same input words with only slight ations of importance we can see in figure an example of two nearly identical attributions for two distant and unrelated words of the summary the saliency map generated using lrp is also uncorrelated with the attention distribution that participated in the generation of the output word the tion distribution changes drastically between the words in the generated summary while not impacting signicantly the tribution over the input text we deleted in an experiment the relevance propagated through the attention mechanism to the encoder and did nt observe much changes in the saliency map it can be seen as evidence that using the attention tion as an explanation of the prediction can be misleading it is not the only information received by the decoder and the importance it allocates to this attention state might be very low what seems to happen in this application is that most of the information used is transmitted from the encoder to the decoder and the attention mechanism at each decoding step figure left saliency map over the truncated input text for the second generated word the right saliency map over the cated input text for the generated word investigation we see that the difference between the mappings is marginal just changes marginally how it is used quantifying the ence between attention distribution and saliency map across multiple tasks is a possible future work the second observation we can make is that the saliency map does nt seem to highlight the right things in the input for the summary it generates the saliency maps on figure correspond to the summary from figure and we do nt see the word video highlighted in the input text which seems to be important for the output this allows us to question how good the saliency maps are in the sense that we question how well they actually sent the network s use of the input features we will call that truthfulness of the attribution in regard to the computation meaning that an attribution is truthful in regard to the putation if it actually highlights the important input features that the network attended to during prediction we proceed to measure the truthfulness of the attributions by validating them quantitatively validating the attributions we propose to validate the saliency maps in a similar way as arras et al by incrementally deleting important words from the input text and observe the change in the sulting generated summaries we rst dene what important and unimportant input words mean across the saliency maps per texts relevance transmitted by lrp being positive or negative we average the absolute value of the relevance across the saliency maps to obtain one ranking of the most relevant words the idea is that input words with negative relevance have an impact on the resulting generated word even if it is not ing positively while a word with a relevance close to zero should not be important at all we did however also try with different methods like averaging the raw relevance or eraging a scaled absolute value where negative relevance is scaled down by a constant factor the absolute value average seemed to deliver the best results we delete incrementally the important words words with the highest average in the input and compared it to the control experiment that consists of deleting the least tant word and compare the degradation of the resulting maries we obtain mitigated results for some texts we observe a quick degradation when deleting important words which are not observed when deleting unimportant words see figure but for other test examples we do nt observe a nicant difference between the two settings see figure the unk deleting most important words unk unk unk unk unk unk unk unk unk unk unk unk unk unk deleting least important words the unk was lmed by the magazine and the unk the video was found by a source close to the investigation the unk said the video was recovered from a phone at the wreckage site truncated figure summary from figure generated after deleting important and unimportant words from the input text we observe a signicant difference in summary degradation between the two experiments where the decoder just repeats the unknown token over and over one might argue that the second summary in figure is better than the rst one as it makes better sentences but as the model generates inaccurate summaries we do not wish to make such a statement this however allows us to say that the attribution ated for the text at the origin of the summaries in figure are truthful in regard to the network s computation and we may use it for further studies of the example whereas for the text at the origin of figure we should nt draw any further conclusions from the attribution generated one interesting point is that one saliency map did nt look better than the other meaning that there is no apparent way of determining their truthfulness in regard of the tion without doing a quantitative validation this brings us to believe that even in simpler tasks the saliency maps might deleting most important words the unk mass index was carried out against the taliban in the unk mass index was part of china s strike hard campaign against the notion that the was deleting least important words the unk mass index was carried out in the wake of the horric attack on a school in peshawar the government has issued a ban on executions in the figure summary from another test text generated after deleting important and unimportant words from the input text we observe less signicant difference in summary degradation between the two experiments make sense to us for example highlighting the animal in an image classication task without actually representing what the network really attended too or in what way we dened without saying it the counterfactual case in our experiment would the important words in the input be deleted we would have a different summary such factuals are however more difcult to dene for image cation for example where it could be applying a mask over an image or just ltering a colour or a pattern we believe that dening a counterfactual and testing it allows us to sure and evaluate the truthfulness of the attributions and thus weight how much we can trust them conclusion in this work we have implemented and applied lrp to a sequence to sequence model trained on a more complex task than usual text summarization we used previous work to solve the difculties posed by lrp in lstm cells and adapted the same technique for bahdanau et al tention mechanism we observed a peculiar behaviour of the saliency maps for the words in the output summary they are almost all cal and seem uncorrelated with the attention distribution we then proceeded to validate our attributions by averaging the absolute value of the relevance across the saliency maps we obtain a ranking of the word from the most important to the least important and proceeded to delete one or another we showed that in some cases the saliency maps are ful to the network s computation meaning that they do light the input features that the network focused on but we also showed that in some cases the saliency maps seem to not capture the important input features this brought us to cuss the fact that these attributions are not sufcient by selves and that we need to dene the counter factual case and test it to measure how truthful the saliency maps are future work would look into the saliency maps generated by applying lrp to pointer generator networks and compare to our current results as well as mathematically justifying the average that we did when validating our saliency maps some additional work is also needed on the validation of the saliency maps with counterfactual tests the exploitation and evaluation of saliency map are a very important step and should not be overlooked samek et al wojciech samek alexander binder gregoire montavon and klaus robert muller evaluating the visualization of ieee what a deep neural network has learned actions on neural networks and learning systems sebastian lapuschkin see et al abigail see peter j liu and christopher d manning get to the point summarization with arxiv preprint generator networks references adadi and berrada amina adadi and mohammed berrada peeking inside the black box a survey on ieee access explainable articial intelligence xai arras et al leila arras franziska horn gregoire montavon klaus robert muller and wojciech samek what is relevant in a text document an interpretable machine learning approach plos one arras et al leila arras gregoire montavon klaus robert muller and wojciech samek explaining recurrent neural network predictions in sentiment analysis bach et al sebastian bach alexander binder gregoire montavon frederick klauschen klaus robert muller and wojciech samek on pixel wise explanations for non linear classier decisions by layer wise relevance propagation plos one bahdanau et al dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation by jointly learning to align and translate arxiv preprint hermann et al karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend pages hochreiter and schmidhuber sepp hochreiter and jurgen schmidhuber long short term memory neural computation miller tim miller explanation in articial gence insights from the social sciences articial gence montavon et al montavon wojciech samek and klaus robert muller methods for preting and understanding deep neural networks digital signal processing feb nallapati et al ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre and bing xiang abstractive text summarization using sequence to sequence rnns and beyond arxiv preprint radev et al dragomir r radev eduard hovy and kathleen mckeown introduction to the special issue on summarization computational linguistics ribeiro et al marco tulio ribeiro sameer singh and carlos guestrin why should i trust you plaining the predictions of any classier proceedings of the acm sigkdd international conference on knowledge discovery and data mining pages rush et al alexander m rush sumit chopra and a neural attention model for arxiv preprint jason weston stractive sentence summarization sep
