a large scale indonesian dataset for text summarization fajri koto jey han lau timothy baldwin school of computing and information systems the university of melbourne unimelb edu au jeyhan com edu au v o n l c s c v v i x r a abstract in this paper we introduce a large scale donesian summarization dataset we vest articles from com an online news portal and obtain document summary pairs we leverage pre trained guage models to develop benchmark tive and abstractive summarization methods over the dataset with multilingual and lingual bert based models we include a thorough error analysis by examining generated summaries that have low rouge scores and expose both issues with rouge self as well as with extractive and abstractive summarization models introduction despite having the fourth largest speaker tion in the world with million native indonesian is under represented in nlp one son is the scarcity of large datasets for different tasks such as parsing text classication and marization in this paper we attempt to bridge this gap by introducing a large scale indonesian corpus for text summarization neural models have driven remarkable progress in summarization in recent years particularly for abstractive summarization one of the rst studies was rush et al where the authors proposed an encoder decoder model with attention to erate headlines for english gigaword documents graff et al subsequent studies introduced pointer networks nallapati et al see et al summarization with content selection hsu et al gehrmann et al graph based attentional models tan et al and deep inforcement learning paulus et al more recently we have seen the widespread adoption visualcapitalist most spoken of pre trained neural language models for rization e bert liu and lapata bart lewis et al and pegasus zhang et al progress in summarization research has been driven by the availability of large scale glish datasets including k cnn daily mail document summary pairs hermann et al and nyt articles sandhaus which have been widely used in abstractive tion research see et al gehrmann et al paulus et al lewis et al zhang et al news articles are a natural date for summarization datasets as they tend to be well structured and are available in large volumes more recently english summarization datasets in other avours domains have been developed e xsum has k documents with highly abstractive summaries narayan et al bigpatent is a summarization dataset for the legal domain sharma et al reddit tifu is sourced from social media kim et al and cohan et al proposed using scientic publications from arxiv and pubmed for abstract summarization this paper introduces the rst large scale marization dataset for indonesian sourced from the com online news portal over a year period it covers various topics and events that happened primarily in indonesia from ber to october below we present tails of the dataset propose benchmark extractive and abstractive summarization methods that age both multilingual and monolingual pre trained bert models we further conduct error analysis to better understand the limitations of current models over the dataset as part of which we reveal not just modelling issues but also problems with rouge to summarize our contributions are we release a large scale indonesian summarization pus with over k documents an order of figure example articles and summaries from to the left is the original document and summary and to the right is an english translation for illustrative purposes we additionally highlight sentences that the summary is based on noting that such highlighting is not available in the dataset nitude larger than the current largest indonesian summarization dataset and one of the largest english summarization datasets in we present statistics to show that the summaries in the dataset are reasonably abstractive and vide two test partitions a standard test set and an extremely abstractive test set we develop benchmark extractive and abstractive tion models based on pre trained bert models and we conduct error analysis on the basis of which we share insights to drive future research on indonesian text summarization data construction com is an online indonesian news tal which has been running since august and provides news across a wide range of ics including politics business sport ogy health and entertainment according to the alexa ranking of websites at the time of com is ranked in indonesia and globally the website produces daily articles along data can be accessed at alexa com topsites with a short description for its rss feed the summary is encapsulated in the javascript able window kmklabs article and the key shortdescription while the article is in the main body of the associated html page we vest this data over a year window from tober to october to create a scale summarization corpus comprising document summary pairs in terms of ing we remove formatting and html entities e quot and lowercase all words and segment sentences based on simple punctuation heuristics we provide example articles and summaries with english translations for expository purposes ing that translations are not part of the dataset in figure as a preliminary analysis of the document summary pairs over the year period we binned the pairs into chronologically ordered groups taining of the data each and computed the proportion of novel n grams order to in the summary relative to the source document based on the results in figure we can see that the portion of novel n grams drops over time implying that the summaries of more recent articles are less dokumen com jakarta gara gara berusaha kabur dimintamenunjukkan barang hasil curian rosihan bin usman tersangkapencurian tas wisatawan asing baru baru ini tersungkur ditembakaparat kepolisian resor denpasar barat bali sebelumnya rosihanditangkap massa setelah mencuri tas nicholas dreyden wisatawanasing asal inggris tas yang berisi dokumen keimigrasian dan suratpenting itu diambil rosihan setelah mengelabui korban kalimat dengan kata setelahnya tidak seorang pencuri tas wisatawan asing ditembak polisi ia berusahakabur diminta menunjukan hasil curian karena itu polisimenembaknya com jakarta because of trying to escape when asked toshow stolen goods rosihan bin usman a suspect of the theft of aforeign tourist bag recently fell down shot by the west denpasarresort police bali previously rosihan was arrested by the mob afterstealing the bag of nicholas dreyden a foreign tourist from england the bag containing immigration documents and important letters wastaken by rosihan after tricking the victim sentences with words are abbreviated from a foreign tourist bag thief was shot by police he tried to run awaywhen asked to show the loot because of this the police shot him dokumen com jakarta organisasi negara negara pengeksporminyak opec mengakui mengalami kesulitan untuk harga minyak dunia itu lantaran harga minyak terusmelonjak sepanjang tahun ini hingga kini harga minyak mentahdunia masih mencapai tingkat tertinggi sejak pecah teluksepuluh tahun silam kalimat dengan kata tidak sebelumnya opec telah produksi minyaksebanyak tiga kali dalam enam bulan terakhir pertama april hinggajuni dengan kenaikan mencapai ribu barel dan terakhir september ini opec kembali menaikkan produksi sebesar ribubarel per hari kalimat kata setelahnya tidak opec kesulitan menjaga stabilitas harga minyak dunia lantaran hargaminyak dipasaran terus melonjak padahal opec telah tiga kalimenaikkan produksi dalam enam bulan terakhir com jakarta the organization of petroleum exportingcountries opec has admitted that it is having difficulty stability of world oil prices that because oil prices continue tosoar this year until now world crude oil prices have still reached level since the gulf war broke out ten years ago sentences with words are abbreviated from fact opec had previously revised oil production three times in thelast six months first april to june with an increase of thousandbarrels and last this september opec has again increased productionby thousand barrels per day sentences with words are abbreviated from opec is struggling to maintain the stability of world oil pricesbecause oil prices on the market continue to soar in fact opec hasraised production three times in the past six months variant doc train dev test of novel n grams canonical xtreme table statistics for the canonical and xtreme ants of our data the percentage of novel n grams is based on the combined dev and test set figure proportion of novel n grams over time in the summaries abstractive for this reason we decide to use the earlier articles october to jan as the development and test documents to create a more challenging dataset this setup also means there is less topic overlap between training and ment test documents allowing us to assess whether the summarization models are able to summarize unseen topics for the training development and test partitions we use a splitting ratio of in addition to this canonical partitioning of the data we provide an xtreme variant inspired by xsum narayan et al whereby we discard development and test document summary pairs where the summary has fewer than novel grams leaving the training data unchanged creating a smaller more challenging data conguration summary statistics for the canonical and xtreme variants are given in table we next present a comparison of canonical partitioning and indosum the current largest indonesian summarization dataset as tailed in section kurniawan and louvan in terms of number of documents in table is approximately times larger than indosum the current largest indonesian rization dataset although articles and summaries in are slightly shorter to understand the abstractiveness of the maries in the two datasets in table we present rouge scores for the simple baseline of using the rst n sentences as an extractive summary lead n and the percentage of novel n grams in the summary we use and for indosum and respectively based on the average number of sentences in the summaries table we see that has consistently lower rouge scores and rl for n it also has a substantially higher proportion of novel n grams this suggests that the summaries in are more abstractive than indosum to create a ground truth for extractive rization we follow cheng and lapata and nallapati et al in greedily selecting the subset of sentences in the article that maximizes the rouge score based on the reference summary as a result each sentence in the article has a nary label to indicate whether they should be cluded as part of an extractive summary extractive summaries created this way will be referred to as oracle to denote the upper bound performance of an extractive summarization system summarization models we follow liu and lapata in building tive and abstractive summarization models using bert as an encoder to produce contextual sentations for the word tokens the architecture of both models is presented in figure we tokenize words with wordpiece and append prex and sep sufx tokens to each sentence to ther distinguish the sentences we add even odd ment embeddings ta tb based on the order of the sentence to the word embeddings for instance for a document with sentences the segment embeddings are ta tb ta tb tion embeddings p are also used to denote the position of each token the wordpiece segment and position embeddings are summed together and provided as input to bert bert produces a series of contextual tations for the word tokens which we feed into a second transformer encoder decoder for the tractive abstractive summarization model we tail the architecture of these two models in tions and note that this second transformer is initialized with random parameters i e it is not pre trained for the pre trained bert encoder we use statistics are based on the entire dataset encompassing the training dev and test data jul aug apr aug oct gram dataset doc article summary train dev test vocab vocab indosum k k k k table a comparison of indosum and and denote the average number of words and sentences respectively dataset lead n rl of novel n grams indosum table abstractiveness of the summaries in indosum and tilingual bert mbert and our own indobert koto et al to appear indobert is a base model we trained ourselves using sian documents from three sources sian wikipedia m words news articles m words from tempo tala et al and and the indonesian web corpus m words medved and suchomel in total the training data has m words we implement indobert using the huggingface and follow the default conguration of bert base uncased hidden size hidden layers attention heads and feed forward we train indobert with pieces vocabulary for million steps extractive model after the document is processed by bert we have a contextualized embedding for every word token in the document to learn inter sentential relationships we use the cls embeddings xsm to represent the sentences to which we add a sentence level positional ding p and feed them to a transformer encoder figure an mlp layer with sigmoid activation is applied to the output of the transformer encoder to predict whether a sentence should be extracted i e ys we train the model with binary pre trained mbert is sourced from github com google research bert com tempo co we use only the articles from the training partition figure architecture of the extractive and abstractive summarization models cross entropy and update all model parameters including bert during training note that the rameters in the transformer encoder and the mlp layer are initialized randomly and learned from scratch the transformer encoder is congured as lows layers hidden size feed forward and heads in terms of training parameters we train using the adam optimizer with learning rate lr step where warmup we train for steps on gb gpus and form evaluation on the development set every steps at test time we select sentences for the tractive summary according to two conditions the summary must consist of at least two sentences and at least words these values were set based on the average number of sentences and the minimum number of words in a summary we also apply trigram blocking to reduce redundancy paulus et al henceforth we refer to this model as bertext mbert modeltransformer layer pre trained transformer decodersoftmax layergeneration of yabstractive modellearned from abstractive model similar to the extractive model we have a second transformer to process the contextualized dings from bert in this case we use a transformer decoder instead i e an attention mask is used to prevent the decoder from attending to future time steps as we are learning to generate an abstractive summary but unlike the extractive model we use the bert embeddings for all tokens as input to the transformer decoder as we do not need sentence representations we add to these bert dings a second positional encoding before feeding them to the transformer decoder figure the transformer decoder is initialized with random rameters i e no pre training the transformer decoder is congured as lows layers hidden size feed forward and heads following liu and pata we use a different learning rate for bert and the decoder when training the model step and step for bert and the transformer decoder respectively both networks are trained with the adam optimizer for steps on gb gpus and ated every steps for summary generation we use beam width trigram blocking and a length penalty wu et al to generate at least two sentences and at least words similar to the extractive model henceforth the abstractive model will be ferred to as bertabs we additionally ment with a third variant bertextabs where we use the weights of the ne tuned bert in bertext for the encoder instead of off the shelf bert weights experiment and results we use three rouge lin scores as evaluation metrics unigram overlap gram overlap and rl longest common quence overlap in addition we also provide bertscore as has recently been used for machine translation evaluation zhang et al we use the development set to select the best checkpoint during training and report the evaluation scores for the canonical and xtreme test sets in table for both test sets the tion models are trained using the same training com tiiiger set but they are tuned with a different ment set see section for details in addition to the bert models we also include two generator models see et al the base model ptgen and the model with coverage penalty we rst look at the baseline lead n and cle results is the best lead n baseline for this is unsurprising given that in table the average summary length was tences we also notice there is a substantial gap between oracle and points for and points for bertscore depending on the test set this suggests that the baseline of using the rst few sentences as an extractive summary is ineffective comparing the performance between the canonical and xtreme test sets we see a stantial drop in performance for both lead n and oracle highlighting the difculty of the xtreme test set due to its increased abstractiveness for the pointer generator models we see little improvement when including the coverage anism vs ptgen implying that there is minimal repetition in the output of ptgen we suspect this is due to the summaries being relatively short sentences with words on average a similar observation is reported by narayan et al for xsum where the maries are similarly short a single sentence with words on average next we look at the bert models overall they perform very well with both the mbert and dobert models outperforming the lead n lines and ptgen models by a comfortable margin indobert is better than mbert approximately rouge point better on average over most metrics showing that a monolingually trained bert is a more effective pre trained model than the lingual variant the best performance is achieved by indobert s bertextabs in the canonical test set the improvement over is and bertscore points in the xtreme test set bertextabs suffers a stantial drop compared to the canonical test set rouge and bertscore points although the performance gap between it and is about the same use the default hyper parameter conguration ommended by the original authors for the pointer generator models model oracle ptgen bertext mbert bertabs mbert bertextabs mbert bertext indobert bertabs indobert bertextabs indobert canonical test set xtreme test set rl bs rl bs table rouge results for the canonical and xtreme test sets all rouge and rl scores have a condence interval of at most as reported by the ofcial rouge script bs is berscore computed with bert base multilingual cased layer as suggested by zhang et al error analysis extractive in this section we analyze errors made by the and abstractive bertext bertextabs models to better understand their behaviour we use the mbert version of these models in our analysis error analysis of extractive summaries we hypothesized that the disparity between cle and bertext point difference for in the canonical test set was due to the number of extracted sentences to test this when extracting sentences with bertext we set the total number of extracted sentences to be the same as the ber of sentences in the oracle summary ever we found minimal benet using this approach suggesting that the disparity is not a result of the number of extracted sentences to investigate this further we present the quency of sentence positions that are used in the summary in oracle and bertext for the ical test set in figure we can see that bertext tends to over select the rst two sentences as the in terms of proportion of summary error analysis is based on mbert rather than dobert simply because this was the best performing model at the time the error analysis was performed while indobert ultimately performed slightly better given that the two models are structurally identical we would expect to see a similar pattern of results bertext summaries involve the rst two tences in comparison only of oracle summaries use sentences in these positions one may argue that this is because the training and test data have different distributions under our logical partitioning strategy recall that the test set is sampled from the earliest articles but that does not appear to be the case as figure shows the distribution of sentence positions in the training data is very similar to the test data of oracle summaries involve the rst two sentences error analysis of abstractive summaries to perform error analysis for bertextabs we randomly sample documents with an score in the canonical test set which accounts for nearly of the test documents two native indonesian speakers examined these samples to manually assess the quality of the summaries and score them on a point ordinal scale bad average and good each annotator is presented with the source document the reference summary and the summary generated by bertextabs in addition to the overall quality evaluation we also asked the annotators to analyze a number of grained attributes in the summaries abbreviations the system summary uses breviations that are different to the reference summary distribution of sentence positions for oracle and bertext in the canonical test set distribution of sentence positions for oracle in the ing set figure position of oracle predicted extractive summaries category bad avg good samples abbreviation morphology paraphrasing lack of coverage wrong focus un details from doc un details not from doc table error analysis for samples with morphology the system summary uses phological variants of the same lemmas tained in the reference summary synonyms paraphrasing the system mary contains paraphrases of the reference summary lack of coverage the system summary lacks coverage of certain details that are present in the reference summary wrong focus the system summarizes a ent aspect focus of the document to the ence summary unnecessary details from document the tem summary includes unimportant but ally correct information unnecessary details not from document the system summary includes unimportant and factually incorrect information tions average good are resolved as follows bad average bad and good average good we only have four examples with bad good agreement which we resolved through discussion interestingly more than half of our samples were found to have good summaries the primary reasons why these summaries have low rouge scores are paraphrasing and the inclusion of additional but valid details ations and morphological differences also appear to be important factors these results underline a problem with the rouge metric in that it is able to detect good summaries that use a different set of words to the reference summary one way forward is to explore metrics that consider sentence semantics beyond word overlap such as meteor banerjee and lavie and and question answering system based evaluation such as apes eyal et al and qags wang et al another way is to create more ence summaries which will help with the issue of the system summaries including validly different details to the single reference looking at the results for average summaries middle column bertextabs occasionally fails to capture salient information of the maries have coverage issues and contain unnecessary but valid details they also tend to use paraphrases which further impacts on a lower rouge score finally the bad system summaries have similar coverage issues and also tend to have a very different focus compared to the we present a breakdown of the different error types in table inter annotator agreement for the overall quality assessment is high pearson s r disagreements in the quality label bad we suggest that bertscore should be used as the canonical evaluation metric for the dataset but leave ical validation of its superiority for indonesian summarization evaluation to future work figure two examples to highlight error categories used in our error analysis reference summary in figure we show two representative ples from bertextabs the rst example is sidered good by our annotators but due to viations morphological differences paraphrasing and additional details compared to the reference summary the rouge score is in this ample the gold summary uses the abbreviation kepmenakertrans while bertextabs generates the full phrase keputusan menteri tenaga kerja dan transmigrasi which is correct the example also uses paraphrases invites strong criticism to explain dissatisfaction and there are morphological ences in words such as tuntutan noun vs tut verb the low rouge score here highlights the fact that the bigger issue is with rouge itself rather than the summary the second example is considered to be bad with the following issues lack of coverage wrong focus and contains unnecessary details that are not from the article the rst sentence president abdurrahman wahid was absent has nothing to do with the original article creating a different focus and confusion in the overall summary to summarize coverage focus and the sion of other details are the main causes of low quality summaries our analysis reveals that breviations and paraphrases are another cause of summaries with low rouge scores but that is an issue with rouge rather than the summaries couragingly hallucination generating details not in the original document is not a major issue for these models notwithstanding that almost of bad samples contain hallucinations related datasets previous studies on indonesian text summarization have largely been extractive and used small scale datasets gunawan et al developed an supervised summarization model over k news ticles using heuristics such as sentence length word frequency and title features in a similar vein najibullah trained a naive bayes model to extract summary sentences in a article dataset dokumen com jakarta langkah reshuffle yang dilakukan presidenabdurrahman wahid agaknya tak mendapat restu buktinya wakilpresiden megawati sukarnoputri kembali tidak hadir dalampelantikan tiga menteri bidang ekonomi rabu kalimat dengan kata setelahnya tidak manusia wapres sukarnoputri kembali tidak hadir dalam pelantikantiga menteri baru dalam reshufle juni megawati juga tak munculdalam pelantikan karena merasa tak dilibatkan dalam reshufflekabinet ringkasan sistem abdurrahman wahid kembali tidak hadir dalam pelantikantiga menteri bidang ekonomi ketidaksepakatan soal perombakankabinet itu juga terjadi juni silam presiden meminta mereka lebihmenjaga koordinasi antarmenteri of error analysis lack of coverage wrong focus and details that are not from the com jakarta the reshuffle step was taken by presidentabdurrahman wahid apparently did not get the blessing the proof vice president megawati sukarnoputri was again not present at theinauguration of three ministers in the economic sector sentences with words are abbreviated from summary vice president megawati sukarnoputri is not present at theinauguration of three new ministers again in the reshuffle on june also did not appear in the inauguration because she felt notinvolved in the cabinet reshuffle system summary abdurrahman wahid was again absent from theinauguration of three ministers in the economic sector disagreementabout the cabinet reshuffle also occurred june ago the presidentasked them to maintain more coordination between ministries dokumen com jakarta protes masih menyambutkeputusan menteri tenaga kerja dan transmigrasi nomor kebijakan yang sengaja dikeluarkan sebagai wujud perubahankeputusan sebelumnya sampai sekarang masih mengundangkecaman dari pekerja indonesia itulah sebabnya merekamenuntut kepmenakertrans baru dicabut karena dinilai merugikanpekerja kalimat dengan kata tidak itu spsi secara tegas menolak segala bentuk negosiasi kalimat dengan kata setelahnya tidak manusia pemberlakuan kepmenakertrans masih mengundang rasatidak puas dada sejumlah pekerja indonesia maka lahirlahtuntutan agar peraturan yang dinilai merugikan dicabut ringkasan sistem keputusan menteri tenaga kerja dan transmigrasi nomor mengundang kecaman keras dari pekerja indonesia merekamenuntut kepmenakertrans dicabut karena dinilai merugikan pekerja spsi menolak negosiasi of error analysis abbreviation morphoplogy synonyms paraphrashing and details from the com jakarta protests still resonate with welcomingminister of manpower and transmigration decree no thispolicy which was deliberately issued as an amendment to theprevious decision until now still invites harsh criticism from workersin indonesia that is why they demand to revoke the newkepmenakertrans because it is considered detrimental to workers sentences with words are abbreviated from spsi firmly rejected all forms of negotiation sentences with words are abbreviated from summary the enactment of kepmenakertrans still invites thedissatisfaction of indonesian workers hence demands to revoke theregulation arose as it was considered to be detrimental system summary of manpower and transmigration decree number of strong criticism from workers in indonesia they demand torevoke kepmenakertrans because it is considered detrimental toworkers spsi rejects negotiations aristoteles et al and silvia et al ply genetic algorithms to a summarization dataset with less than articles these studies do not use rouge for evaluation and the datasets are not publicly available koto released a dataset for chat rization by manually annotating chat logs from whatsapp however this dataset contains only documents the largest summarization data to date is indosum kurniawan and louvan which has approximately k news articles with manually written summaries based on our sis however the summaries of indosum are highly extractive beyond indonesian there is only a handful of non english summarization datasets that are of cient size to train modern deep learning rization methods over including lcsts hu et al which contains million chinese short texts constructed from the sina weibo croblogging website and es news gonzalez et al which comprises spanish news articles with summaries lcsts documents are relatively short less than chinese characters while es news is not publicly available our goal is to create a benchmark corpus for indonesian text summarization that is both large scale and publicly available conclusion we release a large scale summarization corpus for indonesian our dataset comes with two test sets a canonical test set and an xtreme variant that is more abstractive we present results for several benchmark summarization models in part based on indobert a new pre trained bert model for indonesian we further conducted sive error analysis as part of which we identied a number of issues with rouge based evaluation for indonesian acknowledgments we are grateful to the anonymous reviewers for their helpful feedback and suggestions in this search fajri koto is supported by the australia awards scholarship aas funded by the ment of foreign affairs and trade dfat tralia this research was undertaken using the lief hpc gpgpu facility hosted at the university of whatsapp melbourne this facility was established with the assistance of lief grant references aristoteles aristoteles yeni herdiyeni ahmad ridha and julio adisantoso text feature weighting for summarization of document bahasa indonesia ijcsi international using genetic algorithm nal of computer science issues satanjeev banerjee and alon lavie meteor an automatic metric for mt evaluation with proved correlation with human judgments in ceedings of the acl workshop on intrinsic and trinsic evaluation measures for machine tion summarization pages jianpeng cheng and mirella lapata neural marization by extracting sentences and words in proceedings of the annual meeting of the sociation for computational linguistics volume long papers volume pages arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and nazli goharian a discourse aware attention model for abstractive summarization of long documents in naacl hlt annual conference of the north american chapter of the association for putational linguistics human language gies volume pages matan eyal tal baumel and michael elhadad question answering as an automatic evaluation ric for news article summarization in naacl hlt annual conference of the north american chapter of the association for computational guistics pages sebastian gehrmann yuntian deng and alexander m rush bottom up abstractive summarization in proceedings of empirical methods in natural language processing pages j gonzalez l hurtado e segarra f granada and e sanchis summarization of spanish talk shows with siamese hierarchical tion networks applied sciences david graff junbo kong ke chen and kazuaki maeda english gigaword linguistic data consortium d gunawan a pasaribu r f rahmat and r budiarto automatic text summarization for indonesian language using textteaser iop conference series materials science and engineering karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend neural information processing systems pages wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun a unied model for extractive and abstractive summarization in proceedings of the using inconsistency loss annual meeting of the association for tational linguistics pages baotian hu qingcai chen and fangze zhu sts a large scale chinese short text summarization dataset in proceedings of the conference on empirical methods in natural language processing pages byeongchang kim hyunwoo kim and gunhee kim abstractive summarization of reddit posts with multi level memory networks in naacl hlt annual conference of the north american chapter of the association for computational guistics pages fajri koto a publicly available indonesian pora for automatic abstractive and extractive chat summarization in proceedings of the tional conference on language resources and uation lrec fajri koto afshin rahimi jey han lau and timothy baldwin to appear indolem and indobert a benchmark dataset and pre trained language model for indonesian nlp in proceedings of the ternational conference on computational tics coling kemal kurniawan and samuel louvan sum a new benchmark dataset for indonesian text in international conference summarization on asian language processing ialp pages mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer bart denoising sequence to sequence training for natural language generation translation and comprehension in proceedings of the nual meeting of the association for computational linguistics pages chin yew lin rouge a package for in text matic evaluation of summaries rization branches out proceedings of the workshop pages yang liu and mirella lapata text tion with pretrained encoders in conference on empirical methods in natural language ing pages marek medved and vt suchomel indonesian web corpus idwac in lindat clarin digital brary at the institute of formal and applied tics ufal faculty of mathematics and physics charles university ahmad najibullah indonesian text tion based on naive bayes method proceeding of the international seminar and conference ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based sequence model for extractive tion of documents in proceedings of the thirtieth aaai conference on articial intelligence pages ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre and bing xiang abstractive text summarization using sequence sequence rnns and beyond in proceedings of the signll conference on computational natural language learning pages shashi narayan shay b cohen and mirella lapata do nt give me the details just the summary topic aware convolutional neural networks for treme summarization in emnlp ference on empirical methods in natural language processing pages romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization in proceedings of the international conference on learning representations alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of tence summarization cal methods in natural language processing pages evan sandhaus the new york times annotated corpus linguistic data consortium abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proceedings of the nual meeting of the association for computational linguistics pages eva sharma chen li and lu wang patent a large scale dataset for abstractive and coherent summarization in acl the nual meeting of the association for computational linguistics pages silvia pitri rukmana vivi regina aprilia derwin suhartono rini wongso and meiliana marizing text for indonesian language by using tent dirichlet allocation and genetic algorithm in international conference on electrical ing computer science and informatics pages f tala j kamps k e muller and m rijke the impact of stemming on information retrieval in bahasa indonesia in the meeting of tional linguistics in the netherlands jiwei tan xiaojun wan and jianguo xiao abstractive document summarization with a in proceedings of based attentional neural model the annual meeting of the association for putational linguistics volume long papers ume pages alex wang kyunghyun cho and mike lewis asking and answering questions to evaluate the in proceedings of tual consistency of summaries the annual meeting of the association for putational linguistics pages yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin johnson xiaobing liu ukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes and jeffrey dean google s neural machine translation system bridging the gap between human and machine translation arxiv preprint jingqing zhang yao zhao mohammad saleh and ter liu pegasus pre training with tracted gap sentences for abstractive summarization in icml international conference on machine learning tianyi zhang varsha kishore felix wu kilian q weinberger and yoav artzi bertscore in iclr evaluating text generation with bert eighth international conference on learning representations
