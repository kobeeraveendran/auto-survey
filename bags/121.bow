faithful to the original fact aware neural abstractive summarization ziqiang furu wenjie sujian of computing the hong kong polytechnic university hong kong kong polytechnic university shenzhen research institute china research beijing china laboratory of computational linguistics peking university moe china cszqcao polyu edu hk com edu cn v o n r i s c v v i x r a abstract unlike extractive summarization abstractive summarization has to fuse different parts of the source text which inclines to create fake facts our preliminary study reveals nearly of the outputs from a state of the art neural summarization system suffer from this problem while previous abstractive summarization approaches usually focus on the improvement of informativeness we argue that faithfulness is also a tal prerequisite for a practical abstractive summarization tem to avoid generating fake facts in a summary we age open information extraction and dependency parse nologies to extract actual fact descriptions from the source text the dual attention sequence to sequence framework is then proposed to force the generation conditioned on both the source text and the extracted fact descriptions ments on the gigaword benchmark dataset demonstrate that our model can greatly reduce fake summaries by tably the fact descriptions also bring signicant improvement on informativeness since they often condense the meaning of the source text introduction the exponentially growing online information has tated the development of effective automatic summarization systems in this paper we focus on an increasingly ing task i e abstractive sentence summarization rush chopra and weston which generates a shorter sion of a given sentence while attempting to preserve its original meaning this task is different from level summarization since it is hard to apply the mon extractive techniques over and yen ing existing sentences to form the sentence summary is impossible early studies on sentence summarization volve handcrafted rules zajic et al syntactic tree pruning knight and marcu and statistical machine translation techniques banko mittal and witbrock recently the application of the attentional sequence sequence framework has attracted growing attention in this area rush chopra and weston chopra et al nallapati et al contribution during internship at microsoft research copyright association for the advancement of articial intelligence www aaai org all rights reserved source target the repatriation of at least bosnian moslems was postponed friday after the unhcr pulled out of the rst joint scheme to return refugees to their homes in northwest bosnia repatriation of bosnian moslems postponed bosnian moslems postponed after unhcr pulled out of bosnia table an example of fake summaries generated by the state of the art model stands for a digit masked ing preprocessing as we know sentence summarization inevitably needs to fuse different parts in the source sentence and is stractive consequently the generated summaries often match with the original relations and yield fake facts our preliminary study reveals that nearly of the outputs from a state of the art system suffer from this lem previous researches are usually devoted to increasing summary informativeness however one of the most sential prerequisites for a practical abstractive tion system is that the generated summaries must accord with the facts expressed in the source we refer to this pect as summary faithfulness in this paper a fake mary may greatly misguide the comprehension of the inal text look at an illustrative example of the tion result using the state of the art model nallapati et al in table the actual subject of the verb poned is repatriation nevertheless probably because the entity bosnian moslems is closer to postponed in the source sentence the summarization system wrongly regards bosnian moslems as the subject and counterfeits a fact bosnian moslems postponed meanwhile the system generates another fake fact unhcr pulled out of bosnia and puts it into the summary consequently although the informativeness and readability of this summary are high its meaning departs far from the original this sort of summaries is nearly useless in practice since the fact fabrication is a serious problem itively encoding existing facts into the summarization tem should be an ideal solution to avoid fake generation to achieve this goal the rst step is to extract the facts from the source sentence in the relatively mature task of open information extraction openie banko et al a fact is usually represented by a relation triple consisting of ject predicate object for example given the source tence in table the popular openie tool angeli mar and manning generates two relation triples cluding repatriation was postponed friday and unhcr pulled out of rst joint scheme obviously these triples can help rectify the mistakes made by the model ever the relation triples are not always extractable e from the imperative sentences hence we further adopt a dency parser and supplement with the subject predicate and predicate object tuples identied from the parse tree of the sentence this is also inspired by the work of parse tree based sentence compression e knight and marcu we represent a fact through merging words in a triple or tuples to form a short sentence dened as a fact description fact descriptions actually form the skeletons of sentences thus we incorporate them as an additional input source text in our model our experiments reveal that the words in the extracted fact descriptions are more likely to be included in the actual summaries than the entire words in the source sentences that is fact descriptions clearly vide the right guidance for summarization next using both source sentence and fact descriptions as input we extend the state of the art attentional model nallapati et al to fully leverage their information specially we use two recurrent neural network rnn encoders to read the tence and fact descriptions in parallel with respective tion mechanisms our model computes the sentence and fact context vectors it then merges the two vectors according to their relative reliabilities finally a rnn decoder makes use of the integrated context to generate the summary by word since our summarization system encodes facts to enhance faithfulness we call it ftsum to verify the effectiveness of ftsum we conduct sive experiments on the gigaword sentence summarization benchmark dataset rush chopra and weston the results show that our model greatly reduces the fake maries by compared to the state of the art work due to the compression nature of fact descriptions the use of them also brings the signicant improvement in terms of automatic informativeness evaluation the butions of our work can be summarized as follows to the best of our knowledge we are the rst to explore the faithfulness problem of abstractive summarization we propose a dual attention model to push the ation to follow the original facts since the fact descriptions often condense the meaning of the source sentence they also bring the signicant benet to promote informativeness fact description extraction based on our observation of summaries generated by state of the art models suffer from fact fabrication such as the mismatch between the predicate and its subject or object therefore we propose to explicitly encode existing fact descriptions into the model we leverage popular tools of open information extraction openie and dependency sentence triples i saw a cat sitting on the desk i saw cat i saw cat sitting i saw cat sitting on desk table examples of openie triples in different ities we extract the following fact description i saw cat sitting on desk parser for this purpose openie refers to the extraction of entity relations from the open domain text in openie a fact is typically interpreted as a relation triple consisting of ject predicate object we join all the items in a triple i e subject predicate object since it usually acts as a concise sentence an example of the openie outputs is presented in table as we can see openie may extract multiple triples to reect an identical fact in different granularities in some extreme cases one relation can yield over triple variants which brings high redundancy and burdens the computation cost of the model to balance redundancy and fact ness we remove a relation triple if all its words are covered by another one for example only the last fact description i e i saw cat sitting on desk in table is reserved when different fact descriptions are extracted at the end we use a special separator to concatenate them to accelerate the encoding process which is explained by eq and openie is able to give a complete description of the entity relations however it is worth noting that the relation triples are not always extractable e from the imperative tences in fact about of the openie outputs are empty on our dataset these empty instances are likely to damage the robustness of our model as observed although the plete relation triples are not always available the subject predicate or predicate object tuples are almost present in each sentence therefore we leverage the dependency parser to dig out the appropriate tuples to supplement the fact descriptions a dependency parser converts a sentence into the labeled governor dependent tuples we extract the predicate related tuples according to the labels nsubj jpass csubj csubjpass and dobj to acquire more complete fact descriptions we also reserve the important modiers cluding the adjectival numeric nummod and noun compound compound we then merge the tuples ing the same words and order words based on the original sentence to form the fact descriptions take the dependency tree in fig as an example the output of openie is empty for this sentence based on the dependency parser we rstly lter the following predicate related tuples prices opened opened tuesday dealers said and the modify head ples taiwan price share price lower tuesday these tuples are then merged to form two fact descriptions taiwan share prices opened lower tuesday dealers said in the experiments we employ the popular nlp pipeline stanford corenlp manning et al to handle nie and dependency parse at the same time we bine the fact descriptions derived from both parts and screen out the fact descriptions with the pattern somebody said declared announced which are usually meaningless figure a dependency tree example the meaning of the dependency labels can be referred to de marneffe and manning we extract the following two fact descriptions taiwan share prices opened lower tuesday dealers said and insignicant referring to the copy ratios in table words in fact descriptions are more likely to be used in the summary than the words in the original sentence it dicates that fact descriptions truly condense the meaning of sentences to a large extent the above statistics also supports the practice of dependency parse based compressive marization knight and marcu however the length sum of extracted fact descriptions is shorter than the actual summary in of the sentences and of the sentences even hold empty fact descriptions in addition from table we can nd that on average one key source word is missing in the fact descriptions thus without the source sentence we can not reply on fact descriptions alone to generate maries as an example the gru at the time step i is dened as follows hi the bigru consists of a forward gru and a backward hn gru suppose the corresponding outputs are hn respectively then the composite hidden and state of a word is the concatenation of the two gru sentations i e hi hi hi for the relation sequence r since it contains multiple dependent fact descriptions we introduce boundary tors to separate their hidden states specially the value of is dened as follows source avglen count sentence fact table comparisons between source sentences and lations avglen is the average number of tokens means the proportion of source tokens can be found in the summary fact aware neural summarization model framework as shown in figure our model consists of three ules including two encoders and a dual attention decoder equipped with a context selection gate network the tence encoder reads the input words xn and builds its corresponding representation hx n wise the relation encoder converts the fact descriptions r rk into hidden states hr with the spective attention mechanisms our model computes the tence and relation context vectors cx at each ing time step t the gate network is followed to merge the context vectors according to their relative associations with the current generation the decoder produces summaries yl word by word conditioned on the tailored context vector which embeds the semantics of both source sentence and fact descriptions t and encoders the input includes the source sentence and the fact scriptions r for each sequence we employ the bidirectional gated recurrent unit bigru encoder cho et al to construct its semantic representation take the sentence then is used to reset the gru state in eq i ri is otherwise i ihi in this way all the fact descriptions will start with the same zero vector in other words they are encoded independently finally both sentence hidden states hx i and relation den states hr i are fed to the decoder dual attention decoder previous models have developed some task specic modications on the decoder such as to incorporate the copying mechanism gu et al and coverage nism see liu and manning as this paper focuses on the faithfulness problem we use the most popular coder i e gru with attentions bahdanau cho and gio at each decoding time step t gru reads the vious output and context vector as inputs to pute new hidden state st st ct since we have both sentence and relation representations as input we develop two attentional layers to construct the overall context vector for instance the context tation of the sentence at time step t is computed as luong pham and manning t i hx ex i t i j t ihx i cx t t i i t j figure model framework t and where mlp stands for multi layer perceptrons the context vector of the relation can be computed similarly we bine cx to build the overall context vector we plore two alternative combination approaches the rst one is called ftsumc which simply concatenates two context vectors ct cx t the other approach is denoted as ftsumg where we also use mlp to build a gate network and combine context tors with the weighted sum gt ct gt cx t t t gt cr t where means the element wise dot experiments show that ftsumg signicantly outperforms ftsumc and the gate values apparently reect the relative reliability of tence and fact descriptions finally the softmax layer is introduced to generate the next word based on previous word context vector ct and current decoder state st ot wcct wsst t where w stands for a weight matrix learning the learning goal is to maximize the estimated probability of the actual summary we adopt the common negative likelihood nll as the loss function r r where d denotes the training dataset and stands for the model parameters we use adam kingma and ba with mini batches as the optimization algorithm we set the learning rate and the mini batch size to lar to zhou et al we evaluate the model performance on the development set for every batches and halve the dataset count avgsourcelen avgtargetlen train dev m test table data statistics for the english gigaword sourcelen is the average input sentence length and getlen is the average headline length learning rate if the cost increases for consecutive dations in addition we apply gradient clipping pascanu mikolov and bengio with range during ing to enhance the stability of the model experiments datasets we conduct experiments on the annotated english word corpus as with rush chopra and weston this parallel corpus is produced by pairing the rst tence in the news article and its headline as the summary with heuristic rules the training and development datasets are built through the released by rush chopra and weston the script also performs various basic text normalization including tokenization lower casing ing all digit characters with and mask the words appearing less than times with a unk tag it comes up with about m sentence headline pairs as the training set and k pairs as the development set we use the same gigaword test set as rush chopra and weston it contains sentence headline pairs following rush chopra and weston we remove pairs with empty titles leading to slightly different accuracy compared with rush chopra and weston the statistics of the gigaword corpus is presented in table evaluation metric we adopt rouge lin for automatic evaluation rouge has been the standard evaluation metric for duc com facebook namas encoderdual attention encoderattentiongrumlpcontext shared tasks since it measures the quality of mary by computing overlapping lexical units between the candidate summary and actual summaries such as unigram bigram and longest common subsequence lcs ing the common practice we report unigram bi gram and rouge l lcs in the following experiments and mainly consider informativeness while rouge l is supposed to be linked to readability in addition we manually inspect whether the ated summaries accord with the facts in the original tences we mark summaries into three categories ful fake and unclear the last one refers to the case where a generated summary is too incomplete to judge its faithfulness such as just producing a unk tag implementation details since the dataset has already masked infrequent words with the unk tag we reserve all the rest words in the training set as a result the sizes of source and target vocabularies are and respectively with reference to nallapati et al we leverage the popular framework as the starting point and set the size of word embeddings to we initialize word embeddings with glove ton socher and manning all the gru hidden state dimensions are xed to we use dropout srivastava et al with probability with the decoder we use the beam search of size to generate the summary and strict the maximal length of a summary to words we nd that the average system summary length from all our els about words is very much consistent with that of the ground truth on the development set without any special tuning baselines we compare our proposed model with the following six state of the art baselines abs rush chopra and weston used an attentive cnn encoder and nnlm decoder to summarize the tence rush chopra and weston further tuned the abs model with additional features to balance the stractive and extractive tendency ras elman as the extension of the abs model it used a convolutional attention based encoder and an rnn coder chopra et al nallapati et al used a full rnn model and added the hand crafted features such as pos tag and ner to enhance the encoder representation luong nmt luong pham and manning applied the two layer lstms neural machine translation model with hidden units in each layer att we implement the standard attentional with and denote this baseline as att use the rouge evaluation option com material model abs ras elman att ftsumc ftsumg perplexity table final perplexity on the development set cates the value is cited from the corresponding paper and luong nmt do not provide this value model abs ras elman luong nmt ftsumc ftsumg rg l table rouge performance indicates statistical signicance of the corresponding model with respect to the baseline model on the condence interval in the ofcial rouge script rg refers to rouge for short informativeness evaluation at rst look at the nal cost values during training in ble we can see that our model achieves the lowest ity compared against the state of the art systems it is also noted that ftsumg largely outperforms ftsumc which veries the importance of context selection the rouge scores are then reported in table although the focus of our model focuses is to improve faithfulness the rouge scores it receives are also much higher than the other methods note that and have utilized a series of hand crafted features but our model is totally data driven even though our model surpasses by and by on when fact descriptions are ignored our model is equivalent to the standard attentional model therefore it is safe to conclude that fact descriptions have signicant contribute to the increase of rouge scores one probable reason is that fact descriptions are much more formative than the original sentence as shown in table it also largely explains why ftsumg is superior to ftsumc ftsumc treats the source sentence and relations equally while ftsumg tells the fact descriptions are often more liable as discussed in more detail later faithfulness evaluation next we conduct manual evaluation to inspect the ness of the generated summaries specially we randomly select sentences from the test set then we classify the generated summaries as faithful fake or unclear for the sake of a complete comparison we present the sults of our system ftsumg together with the the attentional model as shown in table about of the model att ftsumg count category faithful fake unclear faithful fake unclear table faithfulness performance on the test set att outputs gives disinformation this number greatly duces to by our model nearly of summaries erated by our model is faithful which makes our model far more practical we nd that att tends to copy the words closer to the predicate and regard them as its subject and object however this is not always reasonable and thus it is actually counterfeiting messages in comparison the fact scriptions indeed designate the relations between a predicate and its subject and object as a result generation in line with the fact descriptions is usually able to keep the faithfulness we illustrate the examples of defective outputs in ble as shown att often attempts to fuse different parts in the source sentence to form the summary no matter whether these phrases are relevant or not for instance treats bosnian moslems as the subject of postponed and bosnia as the object of pulled out of in example by contract since the fact description point out the tual subject and object the output of our model is faithful in fact it is exactly the same as the target summary in ample neither att nor our model achieves satisfactory performance att again mismatches the object while our model fails to produce a complete sentence to take a closer look we nd the target summary of this sentence is what strange it merely focuses on the prepositional phrase after taking a stoke rather than the main clause as usual since the main clause is hard to summarize and there is no high quality fact description extracted our model fails to give a complete summary it is also noteworthy that given multiple long fact tions the generation of our model sometimes traps into one item for instance there are two long fact descriptions in example and our model only utilizes the rst one for eration as a result despite the high faithfulness the mativeness is somewhat damaged therefore it seems more reliable to introduce the coverage mechanism see liu and manning to handle the cases like this one we leave it as our future work gate analysis as shown in table ftsumg achieves much higher rouge scores than ftsumc now we investigate what the gate network eq actually learns the changes of the gate values on the development set during training are shown in fig at the beginning the average gate value exceeds which means the generation is biased to the source sentence as training proceeds the model realizes that the fact scriptions are more reliable resulting in a consecutive drop of the gate value finally the average gate value is figure gates change during training ally stabilized to interestingly the ratio of sentence and relation gate values i e is extremely close to the ratio of copying proportions shown in table i e it seems that our model dicts the copy proportion and normalizes it as the gate value then look at the standard deviation of gates to our surprise its change is nearly anti symmetric to the mean value the nal standard deviation reaches about of the mean gate value thus still many sentences can dominate the ation this strange observation urges us to carefully check the summaries with top gate values in the velopment set we nd fact descriptions in the cases are empty and nearly contains the unk tag our model believes these fact descriptions have not much worth to guide generation instead there is no empty fact tions and only unk tag in the bottom cases hence these fact descriptions are usually informative enough in addition we nd the instances with the lowest gate values often hold the following target summary fact description pair target country share prices close open percent fact country share prices slumped dropped rose higher lower percent the extracted fact description itself is already a proper mary that is why fact descriptions are particularly preferred in generation related work abstractive sentence summarization chopra et al aims to produce a shorter version of a given sentence while preserving its meaning unlike document level tion it is impossible for this task to apply the common tractive techniques e cao et al early studies for sentence summarization included rule based methods zajic et al syntactic tree pruning knight and marcu and statistical machine translation niques banko mittal and witbrock recently the application of encoder decoder structures has attracted growing attention in this area rush chopra and weston proposed the abs model which sisted of an attentive convolutional neural network cnn source relations target att ftsum source the repatriation of at least bosnian moslems was postponed friday after the unhcr pulled out of the rst joint scheme to return refugees to their homes in northwest bosnia unhcr pulled out of rst joint scheme repatriation was postponed friday unhcr return refugees to their homes repatriation of bosnian moslems postponed fake bosnian moslems postponed after unhcr pulled out of bosnia faithful repatriation of bosnian moslems postponed davis love said he was thinking of making the world cup of golf a full time occupation after taking a stroke lead over japan in the event with us partner fred couples here on saturday relations making world cup full time occupation taking stroke lead target att ftsum americans lead unk by strokes fake davis love says he is thinking of the world cup unclear love in the world cup of golf example example example source relations target att ftsum the us space shuttle atlantis separated from the orbiting russian mir space station early saturday after three days of test runs for life in a future space facility nasa announced us space shuttle atlantis separated from orbiting russian mir space station us space shuttle atlantis runs after three days of test for line in future space facility atlantis mir part ways after three day space collaboration by emmanuel unk unclear space shuttle atlantis separated after days of test runs for life faithful space shuttle atlantis separated from mir table examples of defective outputs we use bold font to indicate the problematic parts encoder and an neural network language model decoder chopra et al extended their work by replacing the coder with recurrent neural network rnn nallapati et al followed this line and developed a full rnn based sequence to sequence framework sutskever vinyals and le experiments on the gigaword test set rush chopra and weston show that the above models achieve state of the art performance in addition to the direct application of the general framework researchers attempted to import various ties of summarization for example nallapati et al enriched the encoder with hand crafted features such as named entities and pos tags these features played tant roles in traditional feature based summarization tems gu et al found that a large proportion of words in the summary were copied from the source text fore they proposed copynet which considered the copying mechanism during generation later cao et al tended this work by directly measuring the copying nism within neural attentions meanwhile they modied the decoder to reect the rewriting behavior in summarization recently see liu and manning used the coverage mechanism to discourage repetition there were also studies to modify the loss function to t the evaluation metrics for instance ayana liu and sun applied minimum risk training strategy to maximize the rouge scores of erated summaries paulus xiong and socher used reinforcement learning algorithm to optimize a mixed jective function of likelihood and rouge scores notably previous researches usually focused on the provement of summary informativeness to the best of our knowledge we are the rst to explore the faithfulness lem of abstractive summarization conclusion and future work this paper investigates the faithfulness problem in tive summarization we employ popular openie and dency parse tools to extract fact descriptions in the source sentence then we propose the dual attention work to force the generation conditioned on both source tence and the fact descriptions experiments on the word benchmark demonstrate that our model greatly reduce fake summaries by in addition since the fact tions often condense the meaning of the sentence the import of them also brings signicant improvement on ness we believe our work can be extended in various aspects on the one hand we plan to improve our decoder with the copying mechanism and coverage mechanism which is ther adapted to summarization on the other hand we are interested in the automatic evaluation of summary acknowledgments the work described in this paper was supported by research grants council of hong kong polyu tional natural science foundation of china and and the hong kong polytechnic university bcdv nallapati r zhou b gulcehre c xiang b et al abstractive text summarization using sequence to sequence rnns and beyond arxiv preprint over p and yen j introduction to an intrinsic evaluation of generic news text summarization tems in proceedings of duc pascanu r mikolov t and bengio y on the culty of training recurrent neural networks in international conference on machine learning paulus r xiong c and socher r a deep forced model for abstractive summarization arxiv preprint pennington j socher r and manning c d glove global vectors for word representation in empirical ods in natural language processing emnlp rush a m chopra s and weston j a neural tention model for abstractive sentence summarization arxiv preprint rush a m chopra s and weston j a neural attention model for abstractive sentence summarization in proceedings of emnlp see a liu p j and manning c d get to the point summarization with pointer generator networks arxiv preprint srivastava n hinton g e krizhevsky a sutskever i and salakhutdinov r dropout a simple way to vent neural networks from overtting journal of machine learning research sutskever i vinyals o and le q v sequence to sequence learning with neural networks in advances in neural information processing systems zajic d dorr b j lin j and schwartz r multi candidate reduction sentence compression as a tool for document summarization tasks information processing management zhou q yang n wei f and zhou m tive encoding for abstractive sentence summarization arxiv preprint references angeli g premkumar m j and manning c d leveraging linguistic structure for open domain information in proceedings of the annual meeting of extraction the association for computational linguistics acl ayana s s liu z and sun m neural line generation with minimum risk training arxiv preprint bahdanau d cho k and bengio y neural chine translation by jointly learning to align and translate arxiv preprint banko m cafarella m j soderland s broadhead m and etzioni o open information extraction from the web in ijcai volume banko m mittal v o and witbrock m j line generation based on statistical translation in ings of the annual meeting on association for putational linguistics association for tional linguistics cao z wei f dong l li s and zhou m ranking with recursive neural networks and its application to multi document summarization in proceedings of aaai cao z wei f li s li w zhou m and wang h learning summary prior representation for extractive summarization proceedings of acl short papers cao z chu w li w and li s joint copying and restricted generation for paraphrase in proceedings of aaai cho k van merrienboer b gulcehre c bahdanau d bougares f schwenk h and bengio y learning phrase representations using rnn encoder decoder for tical machine translation arxiv preprint chopra s auli m rush a m and harvard s abstractive sentence summarization with attentive recurrent neural networks proceedings of naacl de marneffe m and manning c d stanford typed dependencies manual technical report technical port stanford university gu j lu z li h and li v o ing copying mechanism in sequence to sequence learning arxiv preprint kingma d and ba j adam a method for stochastic optimization arxiv preprint knight k and marcu d summarization beyond sentence extraction a probabilistic approach to sentence compression articial intelligence lin c rouge a package for automatic evaluation of summaries in proceedings of the acl workshop luong m pham h and manning c d tive approaches to attention based neural machine tion arxiv preprint manning c d surdeanu m bauer j finkel j bethard s j and mcclosky d the stanford corenlp in proceedings of acl ural language processing toolkit system demonstrations
