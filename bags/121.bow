faithful original fact aware neural abstractive summarization ziqiang furu wenjie sujian computing hong kong polytechnic university hong kong kong polytechnic university shenzhen research institute china research beijing china laboratory computational linguistics peking university moe china cszqcao polyu edu hk com edu cn v o n r s c v v x r abstract unlike extractive summarization abstractive summarization fuse different parts source text inclines create fake facts preliminary study reveals nearly outputs state art neural summarization system suffer problem previous abstractive summarization approaches usually focus improvement informativeness argue faithfulness tal prerequisite practical abstractive summarization tem avoid generating fake facts summary age open information extraction dependency parse nologies extract actual fact descriptions source text dual attention sequence sequence framework proposed force generation conditioned source text extracted fact descriptions ments gigaword benchmark dataset demonstrate model greatly reduce fake summaries tably fact descriptions bring signicant improvement informativeness condense meaning source text introduction exponentially growing online information tated development effective automatic summarization systems paper focus increasingly ing task e abstractive sentence summarization rush chopra weston generates shorter sion given sentence attempting preserve original meaning task different level summarization hard apply mon extractive techniques yen ing existing sentences form sentence summary impossible early studies sentence summarization volve handcrafted rules zajic et al syntactic tree pruning knight marcu statistical machine translation techniques banko mittal witbrock recently application attentional sequence sequence framework attracted growing attention area rush chopra weston chopra et al nallapati et al contribution internship microsoft research copyright association advancement articial intelligence www aaai org rights reserved source target repatriation bosnian moslems postponed friday unhcr pulled rst joint scheme return refugees homes northwest bosnia repatriation bosnian moslems postponed bosnian moslems postponed unhcr pulled bosnia table example fake summaries generated state art model stands digit masked ing preprocessing know sentence summarization inevitably needs fuse different parts source sentence stractive consequently generated summaries match original relations yield fake facts preliminary study reveals nearly outputs state art system suffer lem previous researches usually devoted increasing summary informativeness sential prerequisites practical abstractive tion system generated summaries accord facts expressed source refer pect summary faithfulness paper fake mary greatly misguide comprehension inal text look illustrative example tion result state art model nallapati et al table actual subject verb poned repatriation probably entity bosnian moslems closer postponed source sentence summarization system wrongly regards bosnian moslems subject counterfeits fact bosnian moslems postponed system generates fake fact unhcr pulled bosnia puts summary consequently informativeness readability summary high meaning departs far original sort summaries nearly useless practice fact fabrication problem itively encoding existing facts summarization tem ideal solution avoid fake generation achieve goal rst step extract facts source sentence relatively mature task open information extraction openie banko et al fact usually represented relation triple consisting ject predicate object example given source tence table popular openie tool angeli mar manning generates relation triples cluding repatriation postponed friday unhcr pulled rst joint scheme obviously triples help rectify mistakes model relation triples extractable e imperative sentences adopt dency parser supplement subject predicate predicate object tuples identied parse tree sentence inspired work parse tree based sentence compression e knight marcu represent fact merging words triple tuples form short sentence dened fact description fact descriptions actually form skeletons sentences incorporate additional input source text model experiments reveal words extracted fact descriptions likely included actual summaries entire words source sentences fact descriptions clearly vide right guidance summarization source sentence fact descriptions input extend state art attentional model nallapati et al fully leverage information specially use recurrent neural network rnn encoders read tence fact descriptions parallel respective tion mechanisms model computes sentence fact context vectors merges vectors according relative reliabilities finally rnn decoder makes use integrated context generate summary word summarization system encodes facts enhance faithfulness ftsum verify effectiveness ftsum conduct sive experiments gigaword sentence summarization benchmark dataset rush chopra weston results model greatly reduces fake maries compared state art work compression nature fact descriptions use brings signicant improvement terms automatic informativeness evaluation butions work summarized follows best knowledge rst explore faithfulness problem abstractive summarization propose dual attention model push ation follow original facts fact descriptions condense meaning source sentence bring signicant benet promote informativeness fact description extraction based observation summaries generated state art models suffer fact fabrication mismatch predicate subject object propose explicitly encode existing fact descriptions model leverage popular tools open information extraction openie dependency sentence triples saw cat sitting desk saw cat saw cat sitting saw cat sitting desk table examples openie triples different ities extract following fact description saw cat sitting desk parser purpose openie refers extraction entity relations open domain text openie fact typically interpreted relation triple consisting ject predicate object join items triple e subject predicate object usually acts concise sentence example openie outputs presented table openie extract multiple triples reect identical fact different granularities extreme cases relation yield triple variants brings high redundancy burdens computation cost model balance redundancy fact ness remove relation triple words covered example fact description e saw cat sitting desk table reserved different fact descriptions extracted end use special separator concatenate accelerate encoding process explained eq openie able complete description entity relations worth noting relation triples extractable e imperative tences fact openie outputs dataset instances likely damage robustness model observed plete relation triples available subject predicate predicate object tuples present sentence leverage dependency parser dig appropriate tuples supplement fact descriptions dependency parser converts sentence labeled governor dependent tuples extract predicate related tuples according labels nsubj jpass csubj csubjpass dobj acquire complete fact descriptions reserve important modiers cluding adjectival numeric nummod noun compound compound merge tuples ing words order words based original sentence form fact descriptions dependency tree fig example output openie sentence based dependency parser rstly lter following predicate related tuples prices opened opened tuesday dealers said modify head ples taiwan price share price lower tuesday tuples merged form fact descriptions taiwan share prices opened lower tuesday dealers said experiments employ popular nlp pipeline stanford corenlp manning et al handle nie dependency parse time bine fact descriptions derived parts screen fact descriptions pattern somebody said declared announced usually meaningless figure dependency tree example meaning dependency labels referred de marneffe manning extract following fact descriptions taiwan share prices opened lower tuesday dealers said insignicant referring copy ratios table words fact descriptions likely summary words original sentence dicates fact descriptions truly condense meaning sentences large extent statistics supports practice dependency parse based compressive marization knight marcu length sum extracted fact descriptions shorter actual summary sentences sentences hold fact descriptions addition table nd average key source word missing fact descriptions source sentence reply fact descriptions generate maries example gru time step dened follows hi bigru consists forward gru backward hn gru suppose corresponding outputs hn respectively composite hidden state word concatenation gru sentations e hi hi hi relation sequence r contains multiple dependent fact descriptions introduce boundary tors separate hidden states specially value dened follows source avglen count sentence fact table comparisons source sentences lations avglen average number tokens means proportion source tokens found summary fact aware neural summarization model framework shown figure model consists ules including encoders dual attention decoder equipped context selection gate network tence encoder reads input words xn builds corresponding representation hx n wise relation encoder converts fact descriptions r rk hidden states hr spective attention mechanisms model computes tence relation context vectors cx ing time step t gate network followed merge context vectors according relative associations current generation decoder produces summaries yl word word conditioned tailored context vector embeds semantics source sentence fact descriptions t encoders input includes source sentence fact scriptions r sequence employ bidirectional gated recurrent unit bigru encoder cho et al construct semantic representation sentence reset gru state eq ri ihi way fact descriptions start zero vector words encoded independently finally sentence hidden states hx relation den states hr fed decoder dual attention decoder previous models developed task specic modications decoder incorporate copying mechanism gu et al coverage nism liu manning paper focuses faithfulness problem use popular coder e gru attentions bahdanau cho gio decoding time step t gru reads vious output context vector inputs pute new hidden state st st ct sentence relation representations input develop attentional layers construct overall context vector instance context tation sentence time step t computed luong pham manning t hx ex t j t ihx cx t t t j figure model framework t mlp stands multi layer perceptrons context vector relation computed similarly bine cx build overall context vector plore alternative combination approaches rst called ftsumc simply concatenates context vectors ct cx t approach denoted ftsumg use mlp build gate network combine context tors weighted sum gt ct gt cx t t t gt cr t means element wise dot experiments ftsumg signicantly outperforms ftsumc gate values apparently reect relative reliability tence fact descriptions finally softmax layer introduced generate word based previous word context vector ct current decoder state st ot wcct wsst t w stands weight matrix learning learning goal maximize estimated probability actual summary adopt common negative likelihood nll loss function r r d denotes training dataset stands model parameters use adam kingma ba mini batches optimization algorithm set learning rate mini batch size lar zhou et al evaluate model performance development set batches halve dataset count avgsourcelen avgtargetlen train dev m test table data statistics english gigaword sourcelen average input sentence length getlen average headline length learning rate cost increases consecutive dations addition apply gradient clipping pascanu mikolov bengio range ing enhance stability model experiments datasets conduct experiments annotated english word corpus rush chopra weston parallel corpus produced pairing rst tence news article headline summary heuristic rules training development datasets built released rush chopra weston script performs basic text normalization including tokenization lower casing ing digit characters mask words appearing times unk tag comes m sentence headline pairs training set k pairs development set use gigaword test set rush chopra weston contains sentence headline pairs following rush chopra weston remove pairs titles leading slightly different accuracy compared rush chopra weston statistics gigaword corpus presented table evaluation metric adopt rouge lin automatic evaluation rouge standard evaluation metric duc com facebook namas encoderdual attention encoderattentiongrumlpcontext shared tasks measures quality mary computing overlapping lexical units candidate summary actual summaries unigram bigram longest common subsequence lcs ing common practice report unigram bi gram rouge l lcs following experiments mainly consider informativeness rouge l supposed linked readability addition manually inspect ated summaries accord facts original tences mark summaries categories ful fake unclear refers case generated summary incomplete judge faithfulness producing unk tag implementation details dataset masked infrequent words unk tag reserve rest words training set result sizes source target vocabularies respectively reference nallapati et al leverage popular framework starting point set size word embeddings initialize word embeddings glove ton socher manning gru hidden state dimensions xed use dropout srivastava et al probability decoder use beam search size generate summary strict maximal length summary words nd average system summary length els words consistent ground truth development set special tuning baselines compare proposed model following state art baselines abs rush chopra weston attentive cnn encoder nnlm decoder summarize tence rush chopra weston tuned abs model additional features balance stractive extractive tendency ras elman extension abs model convolutional attention based encoder rnn coder chopra et al nallapati et al rnn model added hand crafted features pos tag ner enhance encoder representation luong nmt luong pham manning applied layer lstms neural machine translation model hidden units layer att implement standard attentional denote baseline att use rouge evaluation option com material model abs ras elman att ftsumc ftsumg perplexity table final perplexity development set cates value cited corresponding paper luong nmt provide value model abs ras elman luong nmt ftsumc ftsumg rg l table rouge performance indicates statistical signicance corresponding model respect baseline model condence interval ofcial rouge script rg refers rouge short informativeness evaluation rst look nal cost values training ble model achieves lowest ity compared state art systems noted ftsumg largely outperforms ftsumc veries importance context selection rouge scores reported table focus model focuses improve faithfulness rouge scores receives higher methods note utilized series hand crafted features model totally data driven model surpasses fact descriptions ignored model equivalent standard attentional model safe conclude fact descriptions signicant contribute increase rouge scores probable reason fact descriptions formative original sentence shown table largely explains ftsumg superior ftsumc ftsumc treats source sentence relations equally ftsumg tells fact descriptions liable discussed detail later faithfulness evaluation conduct manual evaluation inspect ness generated summaries specially randomly select sentences test set classify generated summaries faithful fake unclear sake complete comparison present sults system ftsumg attentional model shown table model att ftsumg count category faithful fake unclear faithful fake unclear table faithfulness performance test set att outputs gives disinformation number greatly duces model nearly summaries erated model faithful makes model far practical nd att tends copy words closer predicate regard subject object reasonable actually counterfeiting messages comparison fact scriptions designate relations predicate subject object result generation line fact descriptions usually able faithfulness illustrate examples defective outputs ble shown att attempts fuse different parts source sentence form summary matter phrases relevant instance treats bosnian moslems subject postponed bosnia object pulled example contract fact description point tual subject object output model faithful fact exactly target summary ample att model achieves satisfactory performance att mismatches object model fails produce complete sentence closer look nd target summary sentence strange merely focuses prepositional phrase taking stoke main clause usual main clause hard summarize high quality fact description extracted model fails complete summary noteworthy given multiple long fact tions generation model traps item instance long fact descriptions example model utilizes rst eration result despite high faithfulness mativeness somewhat damaged reliable introduce coverage mechanism liu manning handle cases like leave future work gate analysis shown table ftsumg achieves higher rouge scores ftsumc investigate gate network eq actually learns changes gate values development set training shown fig beginning average gate value exceeds means generation biased source sentence training proceeds model realizes fact scriptions reliable resulting consecutive drop gate value finally average gate value figure gates change training ally stabilized interestingly ratio sentence relation gate values e extremely close ratio copying proportions shown table e model dicts copy proportion normalizes gate value look standard deviation gates surprise change nearly anti symmetric mean value nal standard deviation reaches mean gate value sentences dominate ation strange observation urges carefully check summaries gate values velopment set nd fact descriptions cases nearly contains unk tag model believes fact descriptions worth guide generation instead fact tions unk tag cases fact descriptions usually informative addition nd instances lowest gate values hold following target summary fact description pair target country share prices close open percent fact country share prices slumped dropped rose higher lower percent extracted fact description proper mary fact descriptions particularly preferred generation related work abstractive sentence summarization chopra et al aims produce shorter version given sentence preserving meaning unlike document level tion impossible task apply common tractive techniques e cao et al early studies sentence summarization included rule based methods zajic et al syntactic tree pruning knight marcu statistical machine translation niques banko mittal witbrock recently application encoder decoder structures attracted growing attention area rush chopra weston proposed abs model sisted attentive convolutional neural network cnn source relations target att ftsum source repatriation bosnian moslems postponed friday unhcr pulled rst joint scheme return refugees homes northwest bosnia unhcr pulled rst joint scheme repatriation postponed friday unhcr return refugees homes repatriation bosnian moslems postponed fake bosnian moslems postponed unhcr pulled bosnia faithful repatriation bosnian moslems postponed davis love said thinking making world cup golf time occupation taking stroke lead japan event partner fred couples saturday relations making world cup time occupation taking stroke lead target att ftsum americans lead unk strokes fake davis love says thinking world cup unclear love world cup golf example example example source relations target att ftsum space shuttle atlantis separated orbiting russian mir space station early saturday days test runs life future space facility nasa announced space shuttle atlantis separated orbiting russian mir space station space shuttle atlantis runs days test line future space facility atlantis mir ways day space collaboration emmanuel unk unclear space shuttle atlantis separated days test runs life faithful space shuttle atlantis separated mir table examples defective outputs use bold font indicate problematic parts encoder neural network language model decoder chopra et al extended work replacing coder recurrent neural network rnn nallapati et al followed line developed rnn based sequence sequence framework sutskever vinyals le experiments gigaword test set rush chopra weston models achieve state art performance addition direct application general framework researchers attempted import ties summarization example nallapati et al enriched encoder hand crafted features named entities pos tags features played tant roles traditional feature based summarization tems gu et al found large proportion words summary copied source text fore proposed copynet considered copying mechanism generation later cao et al tended work directly measuring copying nism neural attentions modied decoder reect rewriting behavior summarization recently liu manning coverage mechanism discourage repetition studies modify loss function t evaluation metrics instance ayana liu sun applied minimum risk training strategy maximize rouge scores erated summaries paulus xiong socher reinforcement learning algorithm optimize mixed jective function likelihood rouge scores notably previous researches usually focused provement summary informativeness best knowledge rst explore faithfulness lem abstractive summarization conclusion future work paper investigates faithfulness problem tive summarization employ popular openie dency parse tools extract fact descriptions source sentence propose dual attention work force generation conditioned source tence fact descriptions experiments word benchmark demonstrate model greatly reduce fake summaries addition fact tions condense meaning sentence import brings signicant improvement ness believe work extended aspects hand plan improve decoder copying mechanism coverage mechanism ther adapted summarization hand interested automatic evaluation summary acknowledgments work described paper supported research grants council hong kong polyu tional natural science foundation china hong kong polytechnic university bcdv nallapati r zhou b gulcehre c xiang b et al abstractive text summarization sequence sequence rnns arxiv preprint p yen j introduction intrinsic evaluation generic news text summarization tems proceedings duc pascanu r mikolov t bengio y culty training recurrent neural networks international conference machine learning paulus r xiong c socher r deep forced model abstractive summarization arxiv preprint pennington j socher r manning c d glove global vectors word representation empirical ods natural language processing emnlp rush m chopra s weston j neural tention model abstractive sentence summarization arxiv preprint rush m chopra s weston j neural attention model abstractive sentence summarization proceedings emnlp liu p j manning c d point summarization pointer generator networks arxiv preprint srivastava n hinton g e krizhevsky sutskever salakhutdinov r dropout simple way vent neural networks overtting journal machine learning research sutskever vinyals o le q v sequence sequence learning neural networks advances neural information processing systems zajic d dorr b j lin j schwartz r multi candidate reduction sentence compression tool document summarization tasks information processing management zhou q yang n wei f zhou m tive encoding abstractive sentence summarization arxiv preprint references angeli g premkumar m j manning c d leveraging linguistic structure open domain information proceedings annual meeting extraction association computational linguistics acl ayana s s liu z sun m neural line generation minimum risk training arxiv preprint bahdanau d cho k bengio y neural chine translation jointly learning align translate arxiv preprint banko m cafarella m j soderland s broadhead m etzioni o open information extraction web ijcai volume banko m mittal v o witbrock m j line generation based statistical translation ings annual meeting association putational linguistics association tional linguistics cao z wei f dong l li s zhou m ranking recursive neural networks application multi document summarization proceedings aaai cao z wei f li s li w zhou m wang h learning summary prior representation extractive summarization proceedings acl short papers cao z chu w li w li s joint copying restricted generation paraphrase proceedings aaai cho k van merrienboer b gulcehre c bahdanau d bougares f schwenk h bengio y learning phrase representations rnn encoder decoder tical machine translation arxiv preprint chopra s auli m rush m harvard s abstractive sentence summarization attentive recurrent neural networks proceedings naacl de marneffe m manning c d stanford typed dependencies manual technical report technical port stanford university gu j lu z li h li v o ing copying mechanism sequence sequence learning arxiv preprint kingma d ba j adam method stochastic optimization arxiv preprint knight k marcu d summarization sentence extraction probabilistic approach sentence compression articial intelligence lin c rouge package automatic evaluation summaries proceedings acl workshop luong m pham h manning c d tive approaches attention based neural machine tion arxiv preprint manning c d surdeanu m bauer j finkel j bethard s j mcclosky d stanford corenlp proceedings acl ural language processing toolkit system demonstrations
