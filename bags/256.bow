level transformer auxiliary coherence modeling improved text segmentation goran swapna web science research group university mannheim uni mannheim de testing service ets org n j l c s c v v x r abstract breaking structure long texts semantically coherent segments makes texts readable supports downstream applications like summarization retrieval starting apparent link text coherence segmentation introduce novel supervised model text segmentation simple explicit coherence modeling model neural architecture consisting chically connected transformer networks multi task learning model couples sentence level segmentation objective coherence objective differentiates rect sequences sentences corrupt ones proposed model dubbed coherence aware text segmentation cats yields state art segmentation performance tion benchmark datasets furthermore coupling cats cross lingual word embeddings demonstrate fectiveness zero shot language transfer successfully segment texts languages unseen training introduction natural language texts result deliberate cognitive effort author consist semantically coherent segments text tion deals automatically breaking structure text topically contiguous segments e aims identify points topic shift hearst choi brants chen tsochantaridis riedl biemann du buntine johnson glavas nanni ponzetto koshorek et al reliable tion results texts readable humans facilitates downstream tasks like automated text summarization angheluta de busser moens bokaei sameti liu passage retrieval huang et al shtekh et al topical classication zirn et al dialog modeling manuvinakurike et al zhao kawahara text coherence inherently tied text segmentation intuitively text segment expected coherent text spanning different segments consider e text figure topical segments snippets coherent sentences relate amsterdam s history sentences terdam s geography contrast contain sentences figure snippet illustrating relation e dependency text coherence segmentation topics coherent signals fourth sentence starts new segment given duality text segmentation ence surprising methods text segmentation capture coherence implicitly unsupervised tation models rely probabilistic topic modeling brants chen tsochantaridis riedl biemann du buntine johnson semantic similarity sentences glavas nanni ponzetto indirectly relate text coherence similarly recently proposed state art supervised neural mentation model koshorek et al directly learns predict binary sentence level segmentation decisions explicit mechanism modeling coherence work contrast propose supervised neural model text segmentation explicitly takes coherence account augment segmentation prediction tive auxiliary coherence modeling objective posed model dubbed coherence aware text segmentation cats encodes sentence sequence cally connected transformer networks vaswani et al devlin et al similar koshorek et al cats main learning objective binary sentence level tation prediction cats augments tation objective auxiliary coherence based amsterdam younger dutch cities nijmegen rotterdam utrecht amsterdam granted city rights century amsterdam flourished trade hanseatic league amsterdam located western netherlands river amstel ends city centre connects numerous canals amsterdam metres feet sea level tive pushes model predict higher coherence original text snippets corrupt e fake sentence sequences empirically auxiliary coherence objective level transformer model text segmentation tlt ts yields state art performance multiple benchmarks cats model auxiliary coherence modeling ther signicantly improves segmentation tlt ts cats robust domain transfer thermore demonstrate models effectiveness zero shot language transfer coupled cross lingual word ding models trained english wikipedia cessfully segment texts unseen languages ing best performing unsupervised segmentation model glavas nanni ponzetto wide margin cats coherence aware level transformer text segmentation figure illustrates high level architecture cats model snippet text sequence sentences xed length input model token encodings catenation pretrained word embedding positional embedding sentences rst encoded tokens token level transformer vaswani et al feed sequence obtained sentence representations second sentence level transformer transformed e contextualized sentence representations fed feed forward segmentation classier makes binary segmentation prediction sentence additionally feed encoding snippet e sentence sequence coherence regressor feed forward net predicts coherence score follows scribe component detail transformer based segmentation segmentation decision sentence clearly depend content context e formation neighboring sentences work employ encoding stack attention based architecture vaswani et al ize token representations sentence portantly sentence representations snippet choose transfomer encoders recently reported outperform recurrent encoders range nlp tasks devlin et al radford et al shaw uszkoreit vaswani faster train recurrent nets sentence encoding let s sk denote single training instance snippet consisting k sentences let sentence si ti t xed size sequence t tokens following devlin et al prepend sentence si special sentence start token ti ti ruder sgaard vulic glavas et al comprehensive overview methods inducing cross lingual word embeddings trim pad sentences longer shorter t tokens figure high level depiction coherence aware text segmentation cats model input snippet sentence granted city rights century located sk metres word embedding lookuppositional embeddingtoken encoding layertoken level transformermulti head attentionadd normalizefeed forward netadd normalizenttxsentence representations sss sk multi head attentionadd normalizefeed forward netadd normalizentsxsentence level transformerfeed forward netsoftmax sk transformed sentence representationssegmentation classifierfeed forward netcoherence regressorcoherencescoresegmentation sentence ti aiming use transformed representation token sentence encoding encode j k j t vector ti ken ti j concatenation de dimensional word ding dp dimensional embedding position j use pretrained word embeddings x training learn positional embeddings model s parameters let transform t denote encoder stack transformer model vaswani et al consisting nt t layers coupling multi head attention net feed forward net apply transform t token sequence snippet sentence tti transform t sentence encoding transformed vector sentence start token tti sentence contextualization sentence encodings produced transform t capture content sentence context employ second sentence level transformer transform s nt s layers produce context informed sentence representations prepend sequence non contextualized sentence beddings xed embedding denoting snippet start token order capture encoding snippet e sequence k sentences transformed embedding token transform s transformed vector encoding snippet s segmentation classication finally contextualized tence vectors ssi segmentation classier layer feed forward net coupled softmax function yi softmax ssiwseg bseg wseg bseg classier s parameters let yi true segmentation label th sentence segmentation loss jseg simple negative log likelihood sentences n snippets training batch jseg ln yn n k auxiliary coherence modeling given obvious dependency segmentation coherence pair segmentation task auxiliary task predicting snippet coherence effect couple true snippet s original text corrupt e incoherent snippet s created randomly shufing eliminates need additional self attention layer aggregating transformed token vectors sentence encoding details encoding stack transformer model original publication vaswani et al order sentences s randomly replacing sentences s document sentences let s s pair true snippet corrupt terpart respective encodings obtained level transformer encodings rect snippet scrambled snippet presented coherence regressor independently generates coherence score scalar output coherence regressor ys ys wc bc r regressor s parameters jointly softmax normalize scores s s softmax want force model produce higher coherence score correct snippet s corrupt counterpart s dene following contrastive margin based coherence objective jcoh max coh coh margin like larger creating training instances presumed training corpus contains documents generally longer snippet size k annotated segmentation sentence level create training stances sliding sentence window size k uments sentences stride sake auxiliary coherence modeling original snippet s create corrupt counterpart s following ruption procedure rst randomly shufe order sentences s percent snippets random selection additionally replace sentences shufed snippet probability randomly chosen tences non overlapping document snippets inference inference time given long document need binary segmentation decision sentence model individual sentences input sequences k sentences e snippets makes context segmentation prediction sentence create multiple different sequences k tive sentences contain sentence model obtain multiple segmentation predictions sentence know apriori snippets containing sentence s reliable respect segmentation prediction s consider ble snippets containing s words inference time unlike training create snippets sliding window k sentences document stride let sentence window stride m th sentence general case found k different snippets m k m m k m m k s sk set k different snippets containing sentence s average mentation probabilities predicted sentence s snippets pseg s ys sk k sks finally predict s starts new segment pseg s condence threshold tuned rameter model cross lingual zero shot transfer models require language specic features pretrained word embeddings input conceptually easily transferred guage means cross lingual word embedding space ruder sgaard vulic glavas et al let monolingual embedding space source language english use training let independently trained embedding space target language want transfer segmentation model transfer model need project target language vectors language space plethora recently posed methods inducing projection based cross lingual embeddings faruqui dyer smith et al artetxe labaka agirre vulic et al inter alia opt supervised alignment model based solving procrustes problem smith et al simplicity competitive performance zero shot guage transfer nlp models glavas et al given limited size word translation training dictionary d tain linear projection matrix follows xs xt subsets lingual spaces align vectors training translations pairs d obtain language fer segmentation model straightforward embeddings words projected space experimental setup rst describe datasets training evaluation provide details comparative evaluation setup model optimization data k corpus koshorek et al leveraged manual structuring wikipedia pages sections tomatically create large segmentation annotated corpus k consists documents created english en wikipedia pages divided training development test portions train mize evaluate models respective portions k dataset rst element e index predicted vector y denotes positive segmentation probability standard test corpora koshorek et al ally created small evaluation set allow comparative evaluation unsupervised segmentation models e graphseg model glavas nanni ponzetto evaluation large datasets prohibitively slow years synthetic dataset choi standard becnhmark text tation models choi dataset contains documents concatenation paragraphs randomly pled brown corpus choi dataset divided subsets containing documents specic variability segment lengths e segments tences finally evaluate performance models small datasets cities elements created chen et al wikipedia pages dedicated cities world chemical elements respectively languages order test performance transformer based models zero shot language fer setup prepared small evaluation datasets languages analogous dataset created koshorek et al english en wikipedia ated cs fi tr datasets consisting randomly selected pages czech cs finnish fi turkish tr wikipedia respectively comparative evaluation evaluation metric following previous work riedl biemann glavas nanni ponzetto koshorek et al adopt standard text segmentation measure pk beeferman berger lafferty evaluation metric pk score probability model makes wrong prediction rst tence randomly sampled snippet k sentences belong segment e probability model ing segment sentences different ment different segments sentences segment following glavas nanni ponzetto koshorek et al set k half average ground truth segment size dataset baseline models compare cats state art neural segmentation model koshorek et al graphseg glavas nanni ponzetto state art unsupervised text segmentation model additionally sanity check evaluate random baseline assigns positive segmentation label tence probability corresponds ratio total number segments according gold tion total number sentences dataset koshorek et al evaluate models choi corpus specic subsets language transfer experiments selected target guages different families linguistic typologies w t english source language czech like english indo european language slavic language unlike english fusional type finnish uralic language fusionally agglutinative type turkish turkic language agglutinative type model conguration model variants evaluate variants level transformer text segmentation model auxiliary coherence modeling rst model tlt ts imizes segmentation objective jseg cats ond model multi task learning model alternately minimizes segmentation objective jseg ence objective jcoh adopt balanced alternate training regime cats single parameter update based minimization jseg followed single parameter update based optimization jcoh word embeddings experiments use dimensional monolingual fasttext word embeddings trained common crawl corpora respective guages en cs fi tr induce cross lingual word embedding space needed zero shot language fer experiments projecting cs fi tr monolingual embedding spaces en embedding space following smith et al glavas et al create training dictionaries d learning projection matrices machine translating frequent en words cs fi tr model optimization optimize hyperparameters including data preparation parameters like snippet size k cross validation development portion k dataset found following uration lead performance tlt ts cats training instance preparation snippet size k sentences t tokens scrambling bilities conguration transformers nt t nt s layers attention heads layer model hyperparameters sitional embedding size dp coherence objective contrastive margin coh found different optimal inference thresholds segmentation ts model coherence aware cats model trained tlt ts cats batches n snippets k sentences adam timization algorithm kingma ba initial learning rate set results discussion rst present discuss results models ts cats yield previously introduced en uation datasets report analyze models mance cross lingual zero shot transfer experiments com large hyperparameter space large training set searched limited size grid hyperparameter rations likely better performing conguration reported found extensive grid search tune transformer hyperparameters adopt recommended values vaswani et al lter size dropout probabilities attention layers feed forward relu layers base evaluation table shows models performance ve en evaluation datasets transformer based models tlt ts cats outperform competing supervised model koshorek et al hierarchical encoder based current components board improved formance tlt ts respect model koshorek et al consistent improvements transformer based architectures yield comparison models based recurrent components nlp tasks vaswani et al devlin et al gap formance particularly wide pk points ements dataset evaluation elements test set arguably closest true domain transfer train portion k set contains pages similar type found cities test sets contain wikipedia pages chemical elements pages elements test set suggest tlt ts cats offer robust domain transfer recurrent model koshorek et al cats consistently outperforms ts empirically conrms usefulness explicit herence modeling text segmentation koshorek et al report human performance dataset mere pk point better performance coherence aware cats model unsupervised graphseg model glavas nanni ponzetto outperform supervised els synthetic choi dataset believe primarily synthetic choi dataset accurately segmented based simple lexical overlaps word embedding similarities graphseg relies similarities averaged word embeddings trained challenging real world k lexical overlap insufcient accurate segmentation supervised models learn ment based deeper natural language understanding learn encode lexical overlap reliable segmentation signal additionally graphseg evaluated separately subset choi dataset provided gold minimal segment size facilitates improves predicted segmentations zero shot cross lingual transfer table results zero shot cross lingual transfer experiments setting use based models trained english k dataset segment texts x cs fi tr datasets languages baseline additionally evaluate graphseg glavas nanni ponzetto language agnostic model requiring pretrained word embeddings test language input choi dataset albeit different domain thetic impedes direct performance comparisons evaluation datasets non parametric random shufing test yeh k choi cities p elements model model type k choi cities elements random graphseg koshorek et al tlt ts cats unsupervised unsupervised supervised supervised supervised table performance text segmentation models ve english evaluation datasets graphseg model glavas nanni ponzetto evaluated independently different subcorpora choi dataset indicated asterisk model random graphseg tlt ts cats cs fi tr table performance text segmentation models shot language transfer setting x cs fi tr datasets transformer based models tlt ts cats outperform unsupervised graphseg model marginally better random line wide margin coherence aware cats model signicantly better p fi p cs tr tlt ts model trained optimize segmentation objective results fi tr datasets directly parable results reported en table datasets different languages contain mutually comparable wikipedia pages results table suggest drop performance cross lingual transfer big encouraging suggests possible zero shot language transfer reliably segment texts resourced languages ing sufciently large gold segmented data needed directly train language specic segmentation models robust neural segmentation models particular related work work address task text segmentation provide detailed account existing segmentation models cats model auxiliary based objective additionally provide brief overview research modeling text coherence text segmentation text segmentation tasks come main avors linear e sequential text segmentation chical segmentation level segments ther broken sub segments cal segmentation received non negligible research tion yaari eisenstein du buntine son vast majority proposed models cluding work focus linear segmentation hearst beeferman berger lafferty choi brants chen tsochantaridis misra et al riedl biemann glavas nanni ponzetto koshorek et al inter alia pioneering segmentation efforts hearst proposed unsupervised texttiling algorithm based lexical overlap adjacent sentences graphs choi computes similarities tences similar fashion renormalizes local context segments obtained divisive clustering utiyama isahara fragkou petridis kehagias minimize segmentation cost exhaustive search dynamic programming following assumption topical cohesion guides segmentation text number segmentation proaches based topic models proposed brants chen tsochantaridis induce latent tions text snippets probabilistic latent semantic ysis hofmann segment based similarities tween latent representations adjacent snippets misra et al riedl biemann leverage topic tors snippets obtained latent dirichlet allocation model blei ng jordan misra et al nds globally optimal segmentation based ties snippets topic vectors dynamic programming riedl biemann adjust texttiling model hearst use topic vectors instead sparse ized representations snippets malioutov barzilay proposed rst based model text segmentation segment lecture transcripts rst inducing fully connected sentence graph edge weights corresponding cosine similarities tween sparse bag word sentence vectors running minimum normalized multiway cut algorithm obtain segments glavas nanni ponzetto propose graphseg graph based segmentation algorithm similar nature malioutov barzilay uses dense sentence vectors obtained aggregating word embeddings compute intra sentence similarities performs tation based cliques similarity graph finally koshorek et al identify wikipedia free large scale source manually segmented texts train supervised segmentation model train neural model hierarchically combines bidirectional lstm networks report massive improvements pervised segmentation range evaluation datasets model presented work similar hierarchical chitecture uses transfomer networks instead recurrent encoders crucially cats additionally denes auxiliary coherence objective coupled primary segmentation objective multi task learning model text coherence measuring text coherence amounts predicting score indicates meaningful order information text majority proposed text coherence models grounded formal theories text coherence entity grid model barzilay lapata based centering theory grosz weinstein joshi arguably popular entity grid model represent texts matrices encoding grammatical roles entities different sentences tity grid model extensions elsner niak feng hirst feng lin hirst nguyen joty require text preprocessed entities extracted grammatical roles assigned prohibits end end model training contrast li hovy train neural model couples recurrent recursive sentence encoders convolutional encoder sentence sequences end end fashion limited size datasets gold coherence scores models architecture conceptually similar use transformer networks encode sentences sentence sequences goal supporting text segmentation aiming predict exact coherence scores model require gold coherence labels instead devise coherence objective contrasts original text snippets corrupted sentence sequences conclusion segmentation text depends local herence existing segmentation models capture coherence implicitly lexical semantic overlap adjacent sentences work presented cats novel vised model text segmentation couples segmentation prediction explicit auxiliary coherence modeling cats neural architecture consisting hierarchically nected transformer networks lower level sentence coder generates input higher level encoder sentence sequences train model multi task learning setup learning predict sentence segmentation labels original text snippets coherent corrupt sentence sequences cats yields state art performance text segmentation benchmarks zero shot language transfer setting coupled cross lingual word embedding space successfully segment texts target languages unseen training effective text segmentation coherence modeling simple use fully randomly shufed sequences examples highly incoherent text subsequent work investigate negative instances different degree incoherence elaborate objectives auxiliary modeling text coherence references angheluta r de busser r moens m use topic segmentation automatic summarization proc workshop automatic tion artetxe m labaka g agirre e robust self learning method fully unsupervised cross lingual mappings word embeddings proc acl barzilay r lapata m modeling local ence entity based approach computational linguistics beeferman d berger lafferty j statistical models text segmentation machine learning blei d m ng y jordan m latent dirichlet allocation journal machine learning research bokaei m h sameti h liu y extractive summarization multi party meetings discourse segmentation natural language engineering brants t chen f tsochantaridis topic based document segmentation probabilistic latent semantic analysis proc cikm acm chen h branavan s barzilay r karger d r global models document structure latent tions proc human language technologies annual conference north american chapter association computational linguistics ation computational linguistics choi f y advances domain independent linear text segmentation meeting north american chapter association computational linguistics devlin j chang m lee k toutanova k bert pre training deep bidirectional transformers guage understanding arxiv preprint du l buntine w johnson m topic proc mentation structured topic model conference north american chapter sociation computational linguistics human language technologies eisenstein j hierarchical text segmentation multi scale lexical cohesion proc hlt naacl association computational linguistics elsner m charniak e extending entity grid entity specic features proc annual meeting association computational linguistics human language technologies faruqui m dyer c improving vector space word representations multilingual correlation proc eacl feng v w hirst g extending based coherence model multiple ranks proc conference european chapter association computational linguistics association computational linguistics tion computational linguistics volume long papers radford narasimhan k salimans t sutskever improving language understanding generative pre training technical report preprint riedl m biemann c topictiling text tation algorithm based lda proc acl student research workshop association computational linguistics ruder s sgaard vulic survey lingual embedding models arxiv preprint shaw p uszkoreit j vaswani self attention relative position representations proc conference north american chapter association computational linguistics human language gies volume short papers shtekh g kazakova p nikitinsky n skachkov n exploring inuence topic segmentation tion retrieval quality international conference internet science springer smith s l turban d h hamblin s hammerla n y ofine bilingual word vectors orthogonal transformations inverted softmax proc iclr utiyama m isahara h statistical model domain independent text segmentation proc acl vaswani shazeer n parmar n uszkoreit j jones l gomez n kaiser polosukhin attention need advances neural information processing systems vulic glavas g reichart r korhonen need fully unsupervised cross lingual dings proceedings conference empirical methods natural language processing national joint conference natural language processing emnlp ijcnlp yaari y segmentation expository texts chical agglomerative clustering proc ranlp yeh accurate tests statistical cance result differences proc coling zhao t kawahara t joint learning dialog act segmentation recognition spoken dialog neural networks proc ijcnlp zirn c glavas g nanni f eichorts j schmidt h classifying topics detecting topic shifts political manifestos proceedings tional conference advances computational sis political text university zagreb feng v w lin z hirst g impact deep hierarchical discourse structures evaluation text coherence proc coling international conference computational linguistics technical papers fragkou p petridis v kehagias dynamic programming algorithm linear text segmentation journal intelligent information systems glavas g litschko r ruder s vulic properly evaluate cross lingual word embeddings strong baselines comparative analyses ceptions proceedings annual meeting association computational linguistics rence italy association computational linguistics glavas g nanni f ponzetto s p unsupervised text segmentation semantic relatedness graphs proc fifth joint conference lexical computational semantics grosz b j weinstein s joshi k centering framework modeling local coherence discourse computational linguistics hearst m multi paragraph segmentation tory text proc annual meeting association computational linguistics association putational linguistics hofmann t probabilistic latent semantic analysis proc fifteenth conference uncertainty articial intelligence morgan kaufmann publishers inc huang x peng f schuurmans d cercone n robertson s e applying machine learning text segmentation information retrieval information retrieval kingma d p ba j adam method tic optimization arxiv preprint koshorek o cohen mor n rotman m berant j text segmentation supervised learning task proc conference north american chapter association computational linguistics human language technologies volume short papers li j hovy e model coherence based distributed sentence representation proc conference empirical methods natural language cessing emnlp malioutov barzilay r minimum cut model spoken lecture segmentation proc coling acl association computational linguistics manuvinakurike r paetzel m qu c schlangen d devault d incremental dialogue act segmentation fast paced interactive dialogue systems proc annual meeting special interest group discourse dialogue misra h yvon f jose j m cappe o text segmentation topic modeling analytical study proc cikm acm nguyen d t joty s neural local coherence model proc annual meeting
