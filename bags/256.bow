two level transformer and auxiliary coherence modeling for improved text segmentation goran and swapna and web science research group university of mannheim uni mannheim de testing service ets org n a j l c s c v v i x r a abstract breaking down the structure of long texts into semantically coherent segments makes the texts more readable and supports downstream applications like summarization and retrieval starting from an apparent link between text coherence and segmentation we introduce a novel supervised model for text segmentation with simple but explicit coherence modeling our model a neural architecture consisting of two chically connected transformer networks is a multi task learning model that couples the sentence level segmentation objective with the coherence objective that differentiates rect sequences of sentences from corrupt ones the proposed model dubbed coherence aware text segmentation cats yields state of the art segmentation performance on a tion of benchmark datasets furthermore by coupling cats with cross lingual word embeddings we demonstrate its fectiveness in zero shot language transfer it can successfully segment texts in languages unseen in training introduction natural language texts are more often than not a result of a deliberate cognitive effort of an author and as such consist of semantically coherent segments text tion deals with automatically breaking down the structure of text into such topically contiguous segments i e it aims to identify the points of topic shift hearst choi brants chen and tsochantaridis riedl and biemann du buntine and johnson glavas nanni and ponzetto koshorek et al reliable tion results with texts that are more readable for humans but also facilitates downstream tasks like automated text summarization angheluta de busser and moens bokaei sameti and liu passage retrieval huang et al shtekh et al topical classication zirn et al or dialog modeling manuvinakurike et al zhao and kawahara text coherence is inherently tied to text segmentation intuitively the text within a segment is expected to be more coherent than the text spanning different segments consider e the text in figure with two topical segments snippets and are more coherent than and all sentences relate to amsterdam s history and all sentences to terdam s geography in contrast and contain sentences figure snippet illustrating the relation i e dependency between text coherence and segmentation from both topics and being more coherent than and signals that the fourth sentence starts a new segment given this duality between text segmentation and ence it is surprising that the methods for text segmentation capture coherence only implicitly unsupervised tation models rely either on probabilistic topic modeling brants chen and tsochantaridis riedl and biemann du buntine and johnson or semantic similarity between sentences glavas nanni and ponzetto both of which only indirectly relate to text coherence similarly a recently proposed state of the art supervised neural mentation model koshorek et al directly learns to predict binary sentence level segmentation decisions and has no explicit mechanism for modeling coherence in this work in contrast we propose a supervised neural model for text segmentation that explicitly takes coherence into account we augment the segmentation prediction tive with an auxiliary coherence modeling objective our posed model dubbed coherence aware text segmentation cats encodes a sentence sequence using two cally connected transformer networks vaswani et al devlin et al similar to koshorek et al cats main learning objective is a binary sentence level tation prediction however cats augments the tation objective with an auxiliary coherence based amsterdam is younger than dutch cities such as nijmegen rotterdam and utrecht amsterdam was granted city rights in either or in the century amsterdam flourished because of trade with the hanseatic league amsterdam is located in the western netherlands the river amstel ends in the city centre and connects to numerous canals amsterdam is about metres feet below sea level tive which pushes the model to predict higher coherence for original text snippets than for corrupt i e fake sentence sequences we empirically show that even without the auxiliary coherence objective the two level transformer model for text segmentation tlt ts yields state of the art performance across multiple benchmarks that the full cats model with the auxiliary coherence modeling ther signicantly improves the segmentation and that both tlt ts and cats are robust in domain transfer thermore we demonstrate models effectiveness in zero shot language transfer coupled with a cross lingual word ding our models trained on english wikipedia cessfully segment texts from unseen languages ing the best performing unsupervised segmentation model glavas nanni and ponzetto by a wide margin cats coherence aware two level transformer for text segmentation figure illustrates the high level architecture of the cats model a snippet of text a sequence of sentences of xed length is an input to the model token encodings are a catenation of a pretrained word embedding and a positional embedding sentences are rst encoded from their tokens with a token level transformer vaswani et al next we feed the sequence of obtained sentence representations to the second sentence level transformer transformed i e contextualized sentence representations are next fed to the feed forward segmentation classier which makes a binary segmentation prediction for each sentence we additionally feed the encoding of the whole snippet i e the sentence sequence to the coherence regressor a feed forward net which predicts a coherence score in what follows we scribe each component in more detail transformer based segmentation the segmentation decision for a sentence clearly does not depend only on its content but also on its context i e formation from neighboring sentences in this work we employ the encoding stack of the attention based former architecture vaswani et al to ize both token representations in a sentence and more portantly sentence representations within the snippet we choose transfomer encoders because they have recently been reported to outperform recurrent encoders on a range of nlp tasks devlin et al radford et al shaw uszkoreit and vaswani and they are faster to train than recurrent nets sentence encoding let s sk denote a single training instance a snippet consisting of k sentences and let each sentence si ti t be a xed size sequence of t tokens following devlin et al we prepend each sentence si with a special sentence start token ti ti ruder sgaard and vulic glavas et al for a comprehensive overview of methods for inducing cross lingual word embeddings trim pad sentences longer shorter than t tokens figure high level depiction of the coherence aware text segmentation cats model input snippet sentence was granted city rights in the century is located in the sk is about metres word embedding lookuppositional embeddingtoken encoding layertoken level transformermulti head attentionadd normalizefeed forward netadd normalizenttxsentence representations sss sk multi head attentionadd normalizefeed forward netadd normalizentsxsentence level transformerfeed forward netsoftmax sk transformed sentence representationssegmentation classifierfeed forward netcoherence regressorcoherencescoresegmentation each sentence ti aiming to use the transformed representation of that token as the sentence encoding we encode each j i k j t with a vector ti ken ti j which is the concatenation of a de dimensional word ding and a dp dimensional embedding of the position j we use pretrained word embeddings and x them in training we learn positional embeddings as model s parameters let transform t denote the encoder stack of the transformer model vaswani et al consisting of nt t layers each coupling a multi head attention net with a feed forward net we then apply transform t to the token sequence of each snippet sentence tti transform t the sentence encoding is then the transformed vector of the sentence start token tti sentence contextualization sentence encodings produced with transform t only capture the content of the sentence itself but not its context we thus employ a second sentence level transformer transform s with nt s layers to produce context informed sentence representations we prepend each sequence of non contextualized sentence beddings with a xed embedding denoting the snippet start token in order to capture the encoding of the whole snippet i e sequence of k sentences as the transformed embedding of the token transform s with the transformed vector being the encoding of the whole snippet s segmentation classication finally contextualized tence vectors ssi go into the segmentation classier a layer feed forward net coupled with softmax function yi softmax ssiwseg bseg with wseg and bseg as classier s parameters let yi be the true segmentation label of the i th sentence the segmentation loss jseg is then the simple negative log likelihood over all sentences of all n snippets in the training batch jseg ln yn i i n k auxiliary coherence modeling given the obvious dependency between segmentation and coherence we pair the segmentation task with an auxiliary task of predicting snippet coherence to this effect we couple each true snippet s from the original text with a corrupt i e incoherent snippet s created by randomly shufing the eliminates the need for an additional self attention layer for aggregating transformed token vectors into a sentence encoding more details on the encoding stack of the transformer model see the original publication vaswani et al order of sentences in s and randomly replacing sentences from s with other document sentences let s s be a pair of a true snippet and its corrupt terpart and their respective encodings obtained with the two level transformer the encodings of the rect snippet and the scrambled snippet are then presented to the coherence regressor which independently generates a coherence score for each of them the scalar output of the coherence regressor is ys ys with wc and bc r as regressor s parameters we then jointly softmax normalize the scores for s and s softmax we want to force the model to produce higher coherence score for the correct snippet s than for its corrupt counterpart s we thus dene the following contrastive margin based coherence objective jcoh max coh where coh is the margin by which we would like to be larger than creating training instances our presumed training corpus contains documents that are generally longer than the snippet size k and annotated for segmentation at the sentence level we create training stances by sliding a sentence window of size k over uments sentences with a stride of for the sake of auxiliary coherence modeling for each original snippet s we create its corrupt counterpart s with the following ruption procedure we rst randomly shufe the order of sentences in s for percent of snippets random selection we additionally replace sentences of the shufed snippet with the probability with randomly chosen tences from other non overlapping document snippets inference at inference time given a long document we need to make a binary segmentation decision for each sentence our model however does not take individual sentences as input but rather sequences of k sentences i e snippets and makes in context segmentation prediction for each sentence since we can create multiple different sequences of k tive sentences that contain some sentence our model can obtain multiple segmentation predictions for the same sentence as we do not know apriori which of the snippets containing the sentence s is the most reliable with respect to the segmentation prediction for s we consider all ble snippets containing s in other words at inference time unlike in training we create snippets by sliding the window of k sentences over the document with the stride of let the sentence window with the stride of the m th sentence will in the general case be found in k different snippets m k m m k m m k s sk be the set of at most k different snippets containing a sentence s we then average the mentation probabilities predicted for the sentence s over all snippets in pseg s ys sk k sks finally we predict that s starts a new segment if pseg s where is the condence threshold tuned as a rameter of the model cross lingual zero shot transfer models that do not require any language specic features other than pretrained word embeddings as input can at least conceptually be easily transferred to another guage by means of a cross lingual word embedding space ruder sgaard and vulic glavas et al let be the monolingual embedding space of the source language most often english which we use in training and let be the independently trained embedding space of the target language to which we want to transfer the segmentation model to transfer the model we need to project target language vectors from to the language space there is a plethora of recently posed methods for inducing projection based cross lingual embeddings faruqui and dyer smith et al artetxe labaka and agirre vulic et al inter alia we opt for the supervised alignment model based on solving the procrustes problem smith et al due to its simplicity and competitive performance in zero shot guage transfer of nlp models glavas et al given a limited size word translation training dictionary d we tain the linear projection matrix between and as follows with xs and xt as subsets of lingual spaces that align vectors from training translations pairs from d once we obtain the language fer of the segmentation model is straightforward we put the embeddings of words from the projected space experimental setup we rst describe datasets used for training and evaluation and then provide the details on the comparative evaluation setup and model optimization data k corpus koshorek et al leveraged the manual structuring of wikipedia pages into sections to tomatically create a large segmentation annotated corpus k consists of documents created from english en wikipedia pages divided into training development and test portions we train mize and evaluate our models on respective portions of the k dataset rst element i e index of the predicted vector y denotes the positive segmentation probability standard test corpora koshorek et al ally created a small evaluation set to allow for comparative evaluation against unsupervised segmentation models e the graphseg model of glavas nanni and ponzetto for which evaluation on large datasets is prohibitively slow for years the synthetic dataset of choi was used as a standard becnhmark for text tation models choi dataset contains documents each of which is a concatenation of paragraphs randomly pled from the brown corpus choi dataset is divided into subsets containing only documents with specic variability of segment lengths e segments with or with tences finally we evaluate the performance of our models on two small datasets cities and elements created by chen et al from wikipedia pages dedicated to the cities of the world and chemical elements respectively other languages in order to test the performance of our transformer based models in zero shot language fer setup we prepared small evaluation datasets in other languages analogous to the dataset created by koshorek et al from english en wikipedia we ated cs fi and tr datasets consisting of randomly selected pages from czech cs finnish fi and turkish tr wikipedia respectively comparative evaluation evaluation metric following previous work riedl and biemann glavas nanni and ponzetto koshorek et al we also adopt the standard text segmentation measure pk beeferman berger and lafferty as our evaluation metric pk score is the probability that a model makes a wrong prediction as to whether the rst and last tence of a randomly sampled snippet of k sentences belong to the same segment i e the probability of the model ing the same segment for the sentences from different ment or different segments for the sentences from the same segment following glavas nanni and ponzetto koshorek et al we set k to the half of the average ground truth segment size of the dataset baseline models we compare cats against the state the art neural segmentation model of koshorek et al and against graphseg glavas nanni and ponzetto the state of the art unsupervised text segmentation model additionally as a sanity check we evaluate the random baseline it assigns a positive segmentation label to a tence with the probability that corresponds to the ratio of the total number of segments according to the gold tion and total number of sentences in the dataset koshorek et al we evaluate our models on the whole choi corpus and not on specic subsets our language transfer experiments we selected target guages from different families and linguistic typologies w t english as our source language czech is like english an indo european language but as a slavic language it is unlike english fusional by type finnish is an uralic language fusionally agglutinative by type whereas turkish is a turkic language agglutinative by type model conguration model variants we evaluate two variants of our two level transformer text segmentation model with and without the auxiliary coherence modeling the rst model tlt ts imizes only the segmentation objective jseg cats our ond model is a multi task learning model that alternately minimizes the segmentation objective jseg and the ence objective jcoh we adopt a balanced alternate training regime for cats in which a single parameter update based on the minimization of jseg is followed by a single parameter update based on the optimization of jcoh word embeddings in all our experiments we use dimensional monolingual fasttext word embeddings trained on the common crawl corpora of respective guages en cs fi and tr we induce a cross lingual word embedding space needed for the zero shot language fer experiments by projecting cs fi and tr monolingual embedding spaces to the en embedding space following smith et al glavas et al we create training dictionaries d for learning projection matrices by machine translating most frequent en words to cs fi and tr model optimization we optimize all hyperparameters including the data preparation parameters like the snippet size k via cross validation on the development portion of the k dataset we found the following uration to lead to performance for both tlt ts and cats training instance preparation snippet size of k sentences with t tokens scrambling bilities conguration of transformers nt t nt s layers and with attention heads per layer in both other model hyperparameters sitional embedding size of dp coherence objective contrastive margin of coh we found different optimal inference thresholds for the segmentation only ts model and for the coherence aware cats model we trained both tlt ts and cats in batches of n snippets each with k sentences using the adam timization algorithm kingma and ba with the initial learning rate set to results and discussion we rst present and discuss the results that our models ts and cats yield on the previously introduced en uation datasets we then report and analyze models mance in the cross lingual zero shot transfer experiments com the large hyperparameter space and large training set we only searched over a limited size grid of hyperparameter rations it is thus likely that a better performing conguration than the one reported can be found with a more extensive grid search do not tune other transformer hyperparameters but rather adopt the recommended values from vaswani et al lter size of and dropout probabilities of for both attention layers and feed forward relu layers base evaluation table shows models performance on ve en evaluation datasets both our transformer based models tlt ts and cats outperform the competing supervised model of koshorek et al a hierarchical encoder based on current components across the board the improved formance that tlt ts has with respect to the model of koshorek et al is consistent with improvements that transformer based architectures yield in comparison with models based on recurrent components in other nlp tasks vaswani et al devlin et al the gap in formance is particularly wide pk points for the ements dataset evaluation on the elements test set is arguably closest to a true domain transfer while the train portion of the k set contains pages similar in type to those found in and cities test sets it does not contain any wikipedia pages about chemical elements all such pages are in the elements test set this would suggest that tlt ts and cats offer more robust domain transfer than the recurrent model of koshorek et al cats and consistently outperforms ts this empirically conrms the usefulness of explicit herence modeling for text segmentation moreover koshorek et al report human performance on the dataset of which is a mere one pk point better than the performance of our coherence aware cats model the unsupervised graphseg model of glavas nanni and ponzetto seems to outperform all supervised els on the synthetic choi dataset we believe that this is primarily because by being synthetic the choi dataset can be accurately segmented based on simple lexical overlaps and word embedding similarities and graphseg relies on similarities between averaged word embeddings and because by being trained on a much more challenging real world k on which lexical overlap is insufcient for accurate segmentation supervised models learn to ment based on deeper natural language understanding and learn not to encode lexical overlap as reliable segmentation signal additionally graphseg is evaluated separately on each subset of the choi dataset for each of which it is provided the gold minimal segment size which further facilitates and improves its predicted segmentations zero shot cross lingual transfer in table we show the results of our zero shot cross lingual transfer experiments in this setting we use our based models trained on the english k dataset to segment texts from the x cs fi tr datasets in other languages as a baseline we additionally evaluate graphseg glavas nanni and ponzetto as a language agnostic model requiring only pretrained word embeddings of the test language as input choi dataset albeit from a different domain is thetic which impedes direct performance comparisons with other evaluation datasets to the non parametric random shufing test yeh for k choi and cities p for and elements model model type k choi cities elements random graphseg koshorek et al tlt ts cats unsupervised unsupervised supervised supervised supervised table performance of text segmentation models on ve english evaluation datasets graphseg model glavas nanni and ponzetto was evaluated independently on different subcorpora of the choi dataset indicated with an asterisk model random graphseg tlt ts cats cs fi tr table performance of text segmentation models in shot language transfer setting on the x cs fi tr datasets both our transformer based models tlt ts and cats outperform the unsupervised graphseg model which seems to be only marginally better than the random line by a wide margin the coherence aware cats model is again signicantly better p for fi and p for cs and tr than the tlt ts model which was trained to optimize only the segmentation objective while the results on the fi tr datasets are not directly parable to the results reported on the en see table because the datasets in different languages do not contain mutually comparable wikipedia pages results in table still suggest that the drop in performance due to the cross lingual transfer is not big this is quite encouraging as it suggests that it is possible to via the zero shot language transfer rather reliably segment texts from under resourced languages ing sufciently large gold segmented data needed to directly train language specic segmentation models that is robust neural segmentation models in particular related work in this work we address the task of text segmentation we thus provide a detailed account of existing segmentation models because our cats model has an auxiliary based objective we additionally provide a brief overview of research on modeling text coherence text segmentation text segmentation tasks come in two main avors linear i e sequential text segmentation and chical segmentation in which top level segments are ther broken down into sub segments while the cal segmentation received a non negligible research tion yaari eisenstein du buntine and son the vast majority of the proposed models cluding this work focus on linear segmentation hearst beeferman berger and lafferty choi brants chen and tsochantaridis misra et al riedl and biemann glavas nanni and ponzetto koshorek et al inter alia in one of the pioneering segmentation efforts hearst proposed an unsupervised texttiling algorithm based on the lexical overlap between adjacent sentences and graphs choi computes the similarities between tences in a similar fashion but renormalizes them within the local context the segments are then obtained through divisive clustering utiyama and isahara and fragkou petridis and kehagias minimize the segmentation cost via exhaustive search with dynamic programming following the assumption that topical cohesion guides the segmentation of the text a number of segmentation proaches based on topic models have been proposed brants chen and tsochantaridis induce latent tions of text snippets using probabilistic latent semantic ysis hofmann and segment based on similarities tween latent representations of adjacent snippets misra et al and riedl and biemann leverage topic tors of snippets obtained with the latent dirichlet allocation model blei ng and jordan while misra et al nds a globally optimal segmentation based on the ties of snippets topic vectors using dynamic programming riedl and biemann adjust the texttiling model of hearst to use topic vectors instead of sparse ized representations of snippets malioutov and barzilay proposed a rst based model for text segmentation they segment lecture transcripts by rst inducing a fully connected sentence graph with edge weights corresponding to cosine similarities tween sparse bag of word sentence vectors and then running a minimum normalized multiway cut algorithm to obtain the segments glavas nanni and ponzetto propose graphseg a graph based segmentation algorithm similar in nature to malioutov and barzilay which uses dense sentence vectors obtained by aggregating word embeddings to compute intra sentence similarities and performs tation based on the cliques of the similarity graph finally koshorek et al identify wikipedia as a free large scale source of manually segmented texts that can be used to train a supervised segmentation model they train a neural model that hierarchically combines two bidirectional lstm networks and report massive improvements over pervised segmentation on a range of evaluation datasets the model we presented in this work has a similar hierarchical chitecture but uses transfomer networks instead of recurrent encoders crucially cats additionally denes an auxiliary coherence objective which is coupled with the primary segmentation objective in a multi task learning model text coherence measuring text coherence amounts to predicting a score that indicates how meaningful the order of the information in the text is the majority of the proposed text coherence models are grounded in formal theories of text coherence among which the entity grid model barzilay and lapata based on the centering theory of grosz weinstein and joshi is arguably the most popular the entity grid model represent texts as matrices encoding the grammatical roles that the same entities have in different sentences the tity grid model as well as its extensions elsner and niak feng and hirst feng lin and hirst nguyen and joty require text to be preprocessed entities extracted and grammatical roles assigned to them which prohibits an end to end model training in contrast li and hovy train a neural model that couples recurrent and recursive sentence encoders with a convolutional encoder of sentence sequences in an end to end fashion on limited size datasets with gold coherence scores our models architecture is conceptually similar but we use transformer networks to both encode sentences and sentence sequences with the goal of supporting text segmentation and not aiming to predict exact coherence scores our model does not require gold coherence labels instead we devise a coherence objective that contrasts original text snippets against corrupted sentence sequences conclusion though the segmentation of text depends on its local herence existing segmentation models capture coherence only implicitly via lexical or semantic overlap of adjacent sentences in this work we presented cats a novel vised model for text segmentation that couples segmentation prediction with explicit auxiliary coherence modeling cats is a neural architecture consisting of two hierarchically nected transformer networks the lower level sentence coder generates input for the higher level encoder of sentence sequences we train the model in a multi task learning setup by learning to predict sentence segmentation labels and that original text snippets are more coherent than corrupt sentence sequences we show that cats yields state of art performance on several text segmentation benchmarks and that it can in a zero shot language transfer setting coupled with a cross lingual word embedding space successfully segment texts from target languages unseen in training although effective for text segmentation our coherence modeling is still rather simple we use only fully randomly shufed sequences as examples of highly incoherent text in subsequent work we will investigate negative instances of different degree of incoherence as well as more elaborate objectives for auxiliary modeling of text coherence references angheluta r de busser r and moens m the use of topic segmentation for automatic summarization in proc of the workshop on automatic tion artetxe m labaka g and agirre e a robust self learning method for fully unsupervised cross lingual mappings of word embeddings in proc of acl barzilay r and lapata m modeling local ence an entity based approach computational linguistics beeferman d berger a and lafferty j statistical models for text segmentation machine learning blei d m ng a y and jordan m i latent dirichlet allocation journal of machine learning research bokaei m h sameti h and liu y extractive summarization of multi party meetings through discourse segmentation natural language engineering brants t chen f and tsochantaridis i topic based document segmentation with probabilistic latent semantic analysis in proc of cikm acm chen h branavan s barzilay r and karger d r global models of document structure using latent tions in proc of human language technologies the annual conference of the north american chapter of the association for computational linguistics ation for computational linguistics choi f y advances in domain independent linear text segmentation in meeting of the north american chapter of the association for computational linguistics devlin j chang m lee k and toutanova k bert pre training of deep bidirectional transformers for guage understanding arxiv preprint du l buntine w and johnson m topic in proc of the mentation with a structured topic model conference of the north american chapter of the sociation for computational linguistics human language technologies eisenstein j hierarchical text segmentation from multi scale lexical cohesion in proc of hlt naacl association for computational linguistics elsner m and charniak e extending the entity grid with entity specic features in proc of the annual meeting of the association for computational linguistics human language technologies faruqui m and dyer c improving vector space word representations using multilingual correlation in proc of eacl feng v w and hirst g extending the based coherence model with multiple ranks in proc of the conference of the european chapter of the association for computational linguistics association for computational linguistics tion for computational linguistics volume long papers radford a narasimhan k salimans t and sutskever i improving language understanding by generative pre training technical report preprint riedl m and biemann c topictiling a text tation algorithm based on lda in proc of acl student research workshop association for computational linguistics ruder s sgaard a and vulic i a survey of lingual embedding models arxiv preprint shaw p uszkoreit j and vaswani a self attention with relative position representations in proc of the conference of the north american chapter of the association for computational linguistics human language gies volume short papers shtekh g kazakova p nikitinsky n and skachkov n exploring inuence of topic segmentation on tion retrieval quality in international conference on internet science springer smith s l turban d h hamblin s and hammerla n y ofine bilingual word vectors orthogonal transformations and the inverted softmax in proc of iclr utiyama m and isahara h a statistical model for domain independent text segmentation in proc of acl vaswani a shazeer n parmar n uszkoreit j jones l gomez a n kaiser and polosukhin i attention is all you need in advances in neural information processing systems vulic i glavas g reichart r and korhonen a do we really need fully unsupervised cross lingual dings in proceedings of the conference on empirical methods in natural language processing and the national joint conference on natural language processing emnlp ijcnlp yaari y segmentation of expository texts by chical agglomerative clustering in proc of ranlp yeh a more accurate tests for the statistical cance of result differences in proc of coling zhao t and kawahara t joint learning of dialog act segmentation and recognition in spoken dialog using neural networks in proc of ijcnlp zirn c glavas g nanni f eichorts j and schmidt h classifying topics and detecting topic shifts in political manifestos in proceedings of the tional conference on the advances in computational sis of political text university of zagreb feng v w lin z and hirst g the impact of deep hierarchical discourse structures in the evaluation of text coherence in proc of coling the international conference on computational linguistics technical papers fragkou p petridis v and kehagias a a dynamic programming algorithm for linear text segmentation journal of intelligent information systems glavas g litschko r ruder s and vulic i how to properly evaluate cross lingual word embeddings on strong baselines comparative analyses and some ceptions in proceedings of the annual meeting of the association for computational linguistics rence italy association for computational linguistics glavas g nanni f and ponzetto s p unsupervised text segmentation using semantic relatedness graphs in proc of the fifth joint conference on lexical and computational semantics grosz b j weinstein s and joshi a k centering a framework for modeling the local coherence of discourse computational linguistics hearst m a multi paragraph segmentation of tory text in proc of the annual meeting on association for computational linguistics association for putational linguistics hofmann t probabilistic latent semantic analysis in proc of the fifteenth conference on uncertainty in articial intelligence morgan kaufmann publishers inc huang x peng f schuurmans d cercone n and robertson s e applying machine learning to text segmentation for information retrieval information retrieval kingma d p and ba j adam a method for tic optimization arxiv preprint koshorek o cohen a mor n rotman m and berant j text segmentation as a supervised learning task in proc of the conference of the north american chapter of the association for computational linguistics human language technologies volume short papers li j and hovy e a model of coherence based on distributed sentence representation in proc of the conference on empirical methods in natural language cessing emnlp malioutov i and barzilay r minimum cut model for spoken lecture segmentation in proc of coling acl association for computational linguistics manuvinakurike r paetzel m qu c schlangen d and devault d toward incremental dialogue act segmentation in fast paced interactive dialogue systems in proc of the annual meeting of the special interest group on discourse and dialogue misra h yvon f jose j m and cappe o text segmentation via topic modeling an analytical study in proc of cikm acm nguyen d t and joty s a neural local coherence model in proc of the annual meeting of the
