level transformer auxiliary coherence modeling improved text segmentation goran swapna web science research group university mannheim uni mannheim testing service ets org abstract breaking structure long texts semantically coherent segments makes texts readable supports downstream applications like summarization retrieval starting apparent link text coherence segmentation introduce novel supervised model text segmentation simple explicit coherence modeling model neural architecture consisting chically connected transformer networks multi task learning model couples sentence level segmentation objective coherence objective differentiates rect sequences sentences corrupt ones proposed model dubbed coherence aware text segmentation cats yields state art segmentation performance tion benchmark datasets furthermore coupling cats cross lingual word embeddings demonstrate fectiveness zero shot language transfer successfully segment texts languages unseen training introduction natural language texts result deliberate cognitive effort author consist semantically coherent segments text tion deals automatically breaking structure text topically contiguous segments aims identify points topic shift hearst choi brants chen tsochantaridis riedl biemann buntine johnson glavas nanni ponzetto koshorek reliable tion results texts readable humans facilitates downstream tasks like automated text summarization angheluta busser moens bokaei sameti liu passage retrieval huang shtekh topical classication zirn dialog modeling manuvinakurike zhao kawahara text coherence inherently tied text segmentation intuitively text segment expected coherent text spanning different segments consider text figure topical segments snippets coherent sentences relate amsterdam history sentences terdam geography contrast contain sentences figure snippet illustrating relation dependency text coherence segmentation topics coherent signals fourth sentence starts new segment given duality text segmentation ence surprising methods text segmentation capture coherence implicitly unsupervised tation models rely probabilistic topic modeling brants chen tsochantaridis riedl biemann buntine johnson semantic similarity sentences glavas nanni ponzetto indirectly relate text coherence similarly recently proposed state art supervised neural mentation model koshorek directly learns predict binary sentence level segmentation decisions explicit mechanism modeling coherence work contrast propose supervised neural model text segmentation explicitly takes coherence account augment segmentation prediction tive auxiliary coherence modeling objective posed model dubbed coherence aware text segmentation cats encodes sentence sequence cally connected transformer networks vaswani devlin similar koshorek cats main learning objective binary sentence level tation prediction cats augments tation objective auxiliary coherence based amsterdam younger dutch cities nijmegen rotterdam utrecht amsterdam granted city rights century amsterdam flourished trade hanseatic league amsterdam located western netherlands river amstel ends city centre connects numerous canals amsterdam metres feet sea level tive pushes model predict higher coherence original text snippets corrupt fake sentence sequences empirically auxiliary coherence objective level transformer model text segmentation tlt yields state art performance multiple benchmarks cats model auxiliary coherence modeling ther signicantly improves segmentation tlt cats robust domain transfer thermore demonstrate models effectiveness zero shot language transfer coupled cross lingual word ding models trained english wikipedia cessfully segment texts unseen languages ing best performing unsupervised segmentation model glavas nanni ponzetto wide margin cats coherence aware level transformer text segmentation figure illustrates high level architecture cats model snippet text sequence sentences xed length input model token encodings catenation pretrained word embedding positional embedding sentences rst encoded tokens token level transformer vaswani feed sequence obtained sentence representations second sentence level transformer transformed contextualized sentence representations fed feed forward segmentation classier makes binary segmentation prediction sentence additionally feed encoding snippet sentence sequence coherence regressor feed forward net predicts coherence score follows scribe component detail transformer based segmentation segmentation decision sentence clearly depend content context formation neighboring sentences work employ encoding stack attention based architecture vaswani ize token representations sentence portantly sentence representations snippet choose transfomer encoders recently reported outperform recurrent encoders range nlp tasks devlin radford shaw uszkoreit vaswani faster train recurrent nets sentence encoding let denote single training instance snippet consisting sentences let sentence xed size sequence tokens following devlin prepend sentence special sentence start token ruder sgaard vulic glavas comprehensive overview methods inducing cross lingual word embeddings trim pad sentences longer shorter tokens figure high level depiction coherence aware text segmentation cats model input snippet sentence granted city rights century located metres word embedding lookuppositional embeddingtoken encoding layertoken level transformermulti head attentionadd normalizefeed forward netadd normalizenttxsentence representations sss multi head attentionadd normalizefeed forward netadd normalizentsxsentence level transformerfeed forward netsoftmax transformed sentence representationssegmentation classifierfeed forward netcoherence regressorcoherencescoresegmentation sentence aiming use transformed representation token sentence encoding encode vector ken concatenation dimensional word ding dimensional embedding position use pretrained word embeddings training learn positional embeddings model parameters let transform denote encoder stack transformer model vaswani consisting layers coupling multi head attention net feed forward net apply transform token sequence snippet sentence tti transform sentence encoding transformed vector sentence start token tti sentence contextualization sentence encodings produced transform capture content sentence context employ second sentence level transformer transform layers produce context informed sentence representations prepend sequence non contextualized sentence beddings xed embedding denoting snippet start token order capture encoding snippet sequence sentences transformed embedding token transform transformed vector encoding snippet segmentation classication finally contextualized tence vectors ssi segmentation classier layer feed forward net coupled softmax function softmax ssiwseg bseg wseg bseg classier parameters let true segmentation label sentence segmentation loss jseg simple negative log likelihood sentences snippets training batch jseg auxiliary coherence modeling given obvious dependency segmentation coherence pair segmentation task auxiliary task predicting snippet coherence effect couple true snippet original text corrupt incoherent snippet created randomly shufing eliminates need additional self attention layer aggregating transformed token vectors sentence encoding details encoding stack transformer model original publication vaswani order sentences randomly replacing sentences document sentences let pair true snippet corrupt terpart respective encodings obtained level transformer encodings rect snippet scrambled snippet presented coherence regressor independently generates coherence score scalar output coherence regressor regressor parameters jointly softmax normalize scores softmax want force model produce higher coherence score correct snippet corrupt counterpart dene following contrastive margin based coherence objective jcoh max coh coh margin like larger creating training instances presumed training corpus contains documents generally longer snippet size annotated segmentation sentence level create training stances sliding sentence window size uments sentences stride sake auxiliary coherence modeling original snippet create corrupt counterpart following ruption procedure rst randomly shufe order sentences percent snippets random selection additionally replace sentences shufed snippet probability randomly chosen tences non overlapping document snippets inference inference time given long document need binary segmentation decision sentence model individual sentences input sequences sentences snippets makes context segmentation prediction sentence create multiple different sequences tive sentences contain sentence model obtain multiple segmentation predictions sentence know apriori snippets containing sentence reliable respect segmentation prediction consider ble snippets containing words inference time unlike training create snippets sliding window sentences document stride let sentence window stride sentence general case found different snippets set different snippets containing sentence average mentation probabilities predicted sentence snippets pseg sks finally predict starts new segment pseg condence threshold tuned rameter model cross lingual zero shot transfer models require language specic features pretrained word embeddings input conceptually easily transferred guage means cross lingual word embedding space ruder sgaard vulic glavas let monolingual embedding space source language english use training let independently trained embedding space target language want transfer segmentation model transfer model need project target language vectors language space plethora recently posed methods inducing projection based cross lingual embeddings faruqui dyer smith artetxe labaka agirre vulic inter alia opt supervised alignment model based solving procrustes problem smith simplicity competitive performance zero shot guage transfer nlp models glavas given limited size word translation training dictionary tain linear projection matrix follows subsets lingual spaces align vectors training translations pairs obtain language fer segmentation model straightforward embeddings words projected space experimental setup rst describe datasets training evaluation provide details comparative evaluation setup model optimization data corpus koshorek leveraged manual structuring wikipedia pages sections tomatically create large segmentation annotated corpus consists documents created english wikipedia pages divided training development test portions train mize evaluate models respective portions dataset rst element index predicted vector denotes positive segmentation probability standard test corpora koshorek ally created small evaluation set allow comparative evaluation unsupervised segmentation models graphseg model glavas nanni ponzetto evaluation large datasets prohibitively slow years synthetic dataset choi standard becnhmark text tation models choi dataset contains documents concatenation paragraphs randomly pled brown corpus choi dataset divided subsets containing documents specic variability segment lengths segments tences finally evaluate performance models small datasets cities elements created chen wikipedia pages dedicated cities world chemical elements respectively languages order test performance transformer based models zero shot language fer setup prepared small evaluation datasets languages analogous dataset created koshorek english wikipedia ated datasets consisting randomly selected pages czech finnish turkish wikipedia respectively comparative evaluation evaluation metric following previous work riedl biemann glavas nanni ponzetto koshorek adopt standard text segmentation measure beeferman berger lafferty evaluation metric score probability model makes wrong prediction rst tence randomly sampled snippet sentences belong segment probability model ing segment sentences different ment different segments sentences segment following glavas nanni ponzetto koshorek set half average ground truth segment size dataset baseline models compare cats state art neural segmentation model koshorek graphseg glavas nanni ponzetto state art unsupervised text segmentation model additionally sanity check evaluate random baseline assigns positive segmentation label tence probability corresponds ratio total number segments according gold tion total number sentences dataset koshorek evaluate models choi corpus specic subsets language transfer experiments selected target guages different families linguistic typologies english source language czech like english indo european language slavic language unlike english fusional type finnish uralic language fusionally agglutinative type turkish turkic language agglutinative type model conguration model variants evaluate variants level transformer text segmentation model auxiliary coherence modeling rst model tlt imizes segmentation objective jseg cats ond model multi task learning model alternately minimizes segmentation objective jseg ence objective jcoh adopt balanced alternate training regime cats single parameter update based minimization jseg followed single parameter update based optimization jcoh word embeddings experiments use dimensional monolingual fasttext word embeddings trained common crawl corpora respective guages induce cross lingual word embedding space needed zero shot language fer experiments projecting monolingual embedding spaces embedding space following smith glavas create training dictionaries learning projection matrices machine translating frequent words model optimization optimize hyperparameters including data preparation parameters like snippet size cross validation development portion dataset found following uration lead performance tlt cats training instance preparation snippet size sentences tokens scrambling bilities conguration transformers layers attention heads layer model hyperparameters sitional embedding size coherence objective contrastive margin coh found different optimal inference thresholds segmentation model coherence aware cats model trained tlt cats batches snippets sentences adam timization algorithm kingma initial learning rate set results discussion rst present discuss results models cats yield previously introduced uation datasets report analyze models mance cross lingual zero shot transfer experiments com large hyperparameter space large training set searched limited size grid hyperparameter rations likely better performing conguration reported found extensive grid search tune transformer hyperparameters adopt recommended values vaswani lter size dropout probabilities attention layers feed forward relu layers base evaluation table shows models performance evaluation datasets transformer based models tlt cats outperform competing supervised model koshorek hierarchical encoder based current components board improved formance tlt respect model koshorek consistent improvements transformer based architectures yield comparison models based recurrent components nlp tasks vaswani devlin gap formance particularly wide points ements dataset evaluation elements test set arguably closest true domain transfer train portion set contains pages similar type found cities test sets contain wikipedia pages chemical elements pages elements test set suggest tlt cats offer robust domain transfer recurrent model koshorek cats consistently outperforms empirically conrms usefulness explicit herence modeling text segmentation koshorek report human performance dataset mere point better performance coherence aware cats model unsupervised graphseg model glavas nanni ponzetto outperform supervised els synthetic choi dataset believe primarily synthetic choi dataset accurately segmented based simple lexical overlaps word embedding similarities graphseg relies similarities averaged word embeddings trained challenging real world lexical overlap insufcient accurate segmentation supervised models learn ment based deeper natural language understanding learn encode lexical overlap reliable segmentation signal additionally graphseg evaluated separately subset choi dataset provided gold minimal segment size facilitates improves predicted segmentations zero shot cross lingual transfer table results zero shot cross lingual transfer experiments setting use based models trained english dataset segment texts datasets languages baseline additionally evaluate graphseg glavas nanni ponzetto language agnostic model requiring pretrained word embeddings test language input choi dataset albeit different domain thetic impedes direct performance comparisons evaluation datasets non parametric random shufing test yeh choi cities elements model model type choi cities elements random graphseg koshorek tlt cats unsupervised unsupervised supervised supervised supervised table performance text segmentation models english evaluation datasets graphseg model glavas nanni ponzetto evaluated independently different subcorpora choi dataset indicated asterisk model random graphseg tlt cats table performance text segmentation models shot language transfer setting datasets transformer based models tlt cats outperform unsupervised graphseg model marginally better random line wide margin coherence aware cats model signicantly better tlt model trained optimize segmentation objective results datasets directly parable results reported table datasets different languages contain mutually comparable wikipedia pages results table suggest drop performance cross lingual transfer big encouraging suggests possible zero shot language transfer reliably segment texts resourced languages ing sufciently large gold segmented data needed directly train language specic segmentation models robust neural segmentation models particular related work work address task text segmentation provide detailed account existing segmentation models cats model auxiliary based objective additionally provide brief overview research modeling text coherence text segmentation text segmentation tasks come main avors linear sequential text segmentation chical segmentation level segments ther broken sub segments cal segmentation received non negligible research tion yaari eisenstein buntine son vast majority proposed models cluding work focus linear segmentation hearst beeferman berger lafferty choi brants chen tsochantaridis misra riedl biemann glavas nanni ponzetto koshorek inter alia pioneering segmentation efforts hearst proposed unsupervised texttiling algorithm based lexical overlap adjacent sentences graphs choi computes similarities tences similar fashion renormalizes local context segments obtained divisive clustering utiyama isahara fragkou petridis kehagias minimize segmentation cost exhaustive search dynamic programming following assumption topical cohesion guides segmentation text number segmentation proaches based topic models proposed brants chen tsochantaridis induce latent tions text snippets probabilistic latent semantic ysis hofmann segment based similarities tween latent representations adjacent snippets misra riedl biemann leverage topic tors snippets obtained latent dirichlet allocation model blei jordan misra nds globally optimal segmentation based ties snippets topic vectors dynamic programming riedl biemann adjust texttiling model hearst use topic vectors instead sparse ized representations snippets malioutov barzilay proposed rst based model text segmentation segment lecture transcripts rst inducing fully connected sentence graph edge weights corresponding cosine similarities tween sparse bag word sentence vectors running minimum normalized multiway cut algorithm obtain segments glavas nanni ponzetto propose graphseg graph based segmentation algorithm similar nature malioutov barzilay uses dense sentence vectors obtained aggregating word embeddings compute intra sentence similarities performs tation based cliques similarity graph finally koshorek identify wikipedia free large scale source manually segmented texts train supervised segmentation model train neural model hierarchically combines bidirectional lstm networks report massive improvements pervised segmentation range evaluation datasets model presented work similar hierarchical chitecture uses transfomer networks instead recurrent encoders crucially cats additionally denes auxiliary coherence objective coupled primary segmentation objective multi task learning model text coherence measuring text coherence amounts predicting score indicates meaningful order information text majority proposed text coherence models grounded formal theories text coherence entity grid model barzilay lapata based centering theory grosz weinstein joshi arguably popular entity grid model represent texts matrices encoding grammatical roles entities different sentences tity grid model extensions elsner niak feng hirst feng lin hirst nguyen joty require text preprocessed entities extracted grammatical roles assigned prohibits end end model training contrast hovy train neural model couples recurrent recursive sentence encoders convolutional encoder sentence sequences end end fashion limited size datasets gold coherence scores models architecture conceptually similar use transformer networks encode sentences sentence sequences goal supporting text segmentation aiming predict exact coherence scores model require gold coherence labels instead devise coherence objective contrasts original text snippets corrupted sentence sequences conclusion segmentation text depends local herence existing segmentation models capture coherence implicitly lexical semantic overlap adjacent sentences work presented cats novel vised model text segmentation couples segmentation prediction explicit auxiliary coherence modeling cats neural architecture consisting hierarchically nected transformer networks lower level sentence coder generates input higher level encoder sentence sequences train model multi task learning setup learning predict sentence segmentation labels original text snippets coherent corrupt sentence sequences cats yields state art performance text segmentation benchmarks zero shot language transfer setting coupled cross lingual word embedding space successfully segment texts target languages unseen training effective text segmentation coherence modeling simple use fully randomly shufed sequences examples highly incoherent text subsequent work investigate negative instances different degree incoherence elaborate objectives auxiliary modeling text coherence references angheluta busser moens use topic segmentation automatic summarization proc workshop automatic tion artetxe labaka agirre robust self learning method fully unsupervised cross lingual mappings word embeddings proc acl barzilay lapata modeling local ence entity based approach computational linguistics beeferman berger lafferty statistical models text segmentation machine learning blei jordan latent dirichlet allocation journal machine learning research bokaei sameti liu extractive summarization multi party meetings discourse segmentation natural language engineering brants chen tsochantaridis topic based document segmentation probabilistic latent semantic analysis proc cikm acm chen branavan barzilay karger global models document structure latent tions proc human language technologies annual conference north american chapter association computational linguistics ation computational linguistics choi advances domain independent linear text segmentation meeting north american chapter association computational linguistics devlin chang lee toutanova bert pre training deep bidirectional transformers guage understanding arxiv preprint buntine johnson topic proc mentation structured topic model conference north american chapter sociation computational linguistics human language technologies eisenstein hierarchical text segmentation multi scale lexical cohesion proc hlt naacl association computational linguistics elsner charniak extending entity grid entity specic features proc annual meeting association computational linguistics human language technologies faruqui dyer improving vector space word representations multilingual correlation proc eacl feng hirst extending based coherence model multiple ranks proc conference european chapter association computational linguistics association computational linguistics tion computational linguistics volume long papers radford narasimhan salimans sutskever improving language understanding generative pre training technical report preprint riedl biemann topictiling text tation algorithm based lda proc acl student research workshop association computational linguistics ruder sgaard vulic survey lingual embedding models arxiv preprint shaw uszkoreit vaswani self attention relative position representations proc conference north american chapter association computational linguistics human language gies volume short papers shtekh kazakova nikitinsky skachkov exploring inuence topic segmentation tion retrieval quality international conference internet science springer smith turban hamblin hammerla ofine bilingual word vectors orthogonal transformations inverted softmax proc iclr utiyama isahara statistical model domain independent text segmentation proc acl vaswani shazeer parmar uszkoreit jones gomez kaiser polosukhin attention need advances neural information processing systems vulic glavas reichart korhonen need fully unsupervised cross lingual dings proceedings conference empirical methods natural language processing national joint conference natural language processing emnlp ijcnlp yaari segmentation expository texts chical agglomerative clustering proc ranlp yeh accurate tests statistical cance result differences proc coling zhao kawahara joint learning dialog act segmentation recognition spoken dialog neural networks proc ijcnlp zirn glavas nanni eichorts schmidt classifying topics detecting topic shifts political manifestos proceedings tional conference advances computational sis political text university zagreb feng lin hirst impact deep hierarchical discourse structures evaluation text coherence proc coling international conference computational linguistics technical papers fragkou petridis kehagias dynamic programming algorithm linear text segmentation journal intelligent information systems glavas litschko ruder vulic properly evaluate cross lingual word embeddings strong baselines comparative analyses ceptions proceedings annual meeting association computational linguistics rence italy association computational linguistics glavas nanni ponzetto unsupervised text segmentation semantic relatedness graphs proc fifth joint conference lexical computational semantics grosz weinstein joshi centering framework modeling local coherence discourse computational linguistics hearst multi paragraph segmentation tory text proc annual meeting association computational linguistics association putational linguistics hofmann probabilistic latent semantic analysis proc fifteenth conference uncertainty articial intelligence morgan kaufmann publishers inc huang peng schuurmans cercone robertson applying machine learning text segmentation information retrieval information retrieval kingma adam method tic optimization arxiv preprint koshorek cohen mor rotman berant text segmentation supervised learning task proc conference north american chapter association computational linguistics human language technologies volume short papers hovy model coherence based distributed sentence representation proc conference empirical methods natural language cessing emnlp malioutov barzilay minimum cut model spoken lecture segmentation proc coling acl association computational linguistics manuvinakurike paetzel schlangen devault incremental dialogue act segmentation fast paced interactive dialogue systems proc annual meeting special interest group discourse dialogue misra yvon jose cappe text segmentation topic modeling analytical study proc cikm acm nguyen joty neural local coherence model proc annual meeting
