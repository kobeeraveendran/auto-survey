enhancing extractive text summarization topic aware graph neural networks peng cui yuanchao liu school computer science technology harbin institute technology pcui lhu hit edu abstract text summarization aims compress textual document short summary keeping salient information extractive approaches widely text summarization fluency efficiency existing extractive models hardly capture sentence relationships particularly long documents ignore effect topical information capturing important contents address issues paper proposes graph neural network extractive summarization model enabling capture sentence relationships efficiently graph structured document representation model integrates joint neural topic model ntm discover latent topics provide document level features sentence selection experimental results demonstrate model substantially achieves state art results cnn nyt datasets considerably outperforms existing approaches scientific paper datasets consisting longer documents indicating better robustness document genres lengths discussions topical information help model preselect salient contents entire document interprets effectiveness long document summarization introduction text summarization important task natural language processing help people rapidly acquire important information large sum documents previous summarization approaches mainly classified categories abstractive extractive neural based abstractive models usually use framework sutskever generate word word summary encoding document contrast extractive models directly select important sentences original document aggregate summary abstractive models generally flexible produce disfluent ungrammatical summary texts liu lapata extractive models advantages factuality efficiency cao despite success modeling long range inter sentence relationships summarization remains challenge hierarchical networks usually applied problem modeling document sequence sequences cohan zhang empirical observations liu lapata showed use paradigm model sentence relationships provide performance gain summarization hierarchical approaches slow train tend overfit xiao carenini recently graph neural networks gnns widely explored model cross sentence relationships summarization task critical step framework build effective document graph studies yasunaga built document graphs based discourse analysis approach depends external tools lead problems semantically fragmented output liu wang liu built word sentence document graph based word appearance statistical graph building approach hardly captures semantic level relationships model document graph summarization effectively remains open question corresponding author work org licenses licensed creative commons attribution international license license details critical point summarization modeling global information plays key role sentence selection xiao carenini pre trained language models considerably boost performance summarization liu lapata zhang effectively capture context features poor modeling document level information particularly long documents designed sentences short paragraph abovementioned weaknesses paper proposes novel graph based extractive summarization model encode entire document pre trained bert devlin learn contextual sentence representations discover latent topics joint neural topic model ntm miao srivastava sutton second build heterogeneous document graph consisting sentence topic nodes simultaneously update representations modified graph attention network gat velikovi representations sentence nodes extracted compute final labels intuitively topic sentence document graph following advantages graph propagation sentence representations enriched topical information considered kind document level feature help model important contents entire document topic nodes act intermediary bridge distance sentences model efficiently capture inter sentence relationships evaluate model standard datasets including news articles scientific papers experimental results effectiveness superiority summarize contributions threefold conduct quantitative exploration effect latent topics document summarization provide intuitive understanding topical information help summarize documents propose novel graph based neural extractive summarization model innovatively incorporates latent topics graph propagation joint neural topic model best knowledge propose applying ntm extractive text summarization task experimental results demonstrate proposed model achieves competitive results compared state art extractive models news datasets considerably outperforms existing approaches scientific paper datasets consisting longer documents indicating better robustness document genres lengths related work neural extractive summarization neural networks achieved remarkable results extractive summarization existing works mainly regard extractive summarization sequence labeling task nallapati zhang dong sentence ranking task narayan pre trained language models provided substantial performance gain summarization liu lapata zhang current work model inter sentence relationships graph encoder enrich sentence representations topical information bert encoder graph based summarization early works textrank mihalcea tarau lexrank erkan radev built document graphs basis inter sentence similarity extracted summary sentences unsupervised manner recently application gnns document summarization attracted considerable interests yasunaga fernandes wang liu existing gnn based summarization models build document graphs basis words sentences contrary explore effects high level semantic units latent topics topic modeling summarization topic modeling powerful approach learning document features rarely applied document summarization wei proposed build document graph consisting words sentences topic nodes learn graph markov chain zheng proposed summarize multiple documents mining cross document subtopics narayan recommended enriching word representation topical information unlike discover latent topics neural topic model summarization best knowledge ntm applied extractive summarization task figure overall architecture model topic graphsum graph attention layer right square nodes denote sentence representations output document encoder right circular nodes denote topic representations learned ntm left model section describes model topic aware graph neural network document summarization topic graphsum figure presents overview architecture given arbitrary document consists sentences objective model learn sequence binary labels represents sentence included summary model generally consists parts document encoder neural topic model graph attention layer given input document document encoder learns contextual representations sentence pre trained bert ntm aims learn document topic distribution group topic representations graph attention layer builds heterogeneous document graph topics sentences simultaneously update node representations graph encoding sentence representations combined topics sent sentence classifier compute final labels elucidate document encoder bert bidirectional transformer encoder pre trained large corpus similar previous works liu lapata employ modified version bert generates local context aware hidden representations sentences specifically insert tokens beginning end sentence respectively tokens bert layer learn hidden states represents word sentence represent tokens sentence represents hidden state corresponding token bert encoding regard hidden states corresponding sentence contextual representations enriched topic information neural topic model ntm based variational autoencoder vae kingma welling framework learns latent topic encoding decoding process let bag words representation bert document neural topic modelgraph attention encodersentence sentences given document vocabulary encoder prior parameters parameterizing topic distribution decoder networks functions linear transformations relu activation decoder regarded step document generation process employ gaussian softmax miao draw topic distribution latent topic variable topic distribution predefined topic number second learn probability predicted words analogous topic word distribution matrix lda style topic models represents relevance word topic finally draw word reconstruct input leave details refer readers miao considering intermediate parameters encoded topical information use build topic representations follows represents group topic representations predefined dimension linear transformation relu activation weighted sum topic representation regarded overall topic representation document graph attention layer enrich sentence representation summarization approaches zheng narayan topical information learn topic fixed feature external model comparison latent topic model learned neural approach dynamically updated entire networks graph attention layer graph building let represent arbitrary graph represents node set represents edge set formally undirected graph defined stands sentence nodes stands topic nodes represents edge sentence topic indicating document graph bipartite graph propagation initialize vectors sentence nodes topic nodes learned document encoder learned ntm respectively update node representations graph attention network denoted node representation represents neighbor nodes represents heads concatenation model trainable parameters vanilla gat designed homogeneous graphs document graph heterogeneous sentence topic considered different semantic units need adaptation inspired consider convenient approach project topic sentence representations implicit common space calculate attention weight let sentence node topic node modify replacing shared matrix different projection functions shown follows nonlinear transformation functions project sentence topic nodes common vector space respectively graph attention layer build semantic relationships sentences topics example graph propagation sentences enrich representation topical information regarded global feature topics capture related sentences distil salient contents entire document different topical relevance topic nodes act intermediary help build inter sentence relationships high level semantic units sentences graph encoding obtain topic sensitive sentence representations concatenate overall topic representation capture topical relevance document choose single feed forward layer sentence predict final labels sigmoid function joint training jointly train ntm sentence classifier ntm objective function defined negative evidence lower bound shown follows term indicates kullback leibler divergence loss second term indicates reconstruction loss represent encoder decoder networks respectively binary cross entropy loss sentence classifier expressed final loss model linear combination parts loss hyperparameter balance weights experimental setup datasets conduct experiments datasets including document types news article scientific paper summarization news articles widely explored longer scientific papers challenging accurately encoding long texts summarization known challenge vaswani frermann klementiev conduct experiments scientific paper datasets verify generalization capability model long documents detailed statistics datasets summarized table datasets source cnn daily mail nyt arxiv pubmed news news news scientific paper scientific paper train docs val test avg tokens doc sum table statistics datasets split size average tokens document summary tried adding advanced classifiers cnn rnn gat layer performance shows substantial gain indicating model learned sufficient features cnn dailymail hermann widely standard dataset document summarization use standard splits preprocess data accordance previous works liu lapata wang liu nyt sandhaus popular summarization dataset collected new york times annotated corpus preprocess divide dataset according durrett arxiv pubmed cohan newly constructed datasets long document summarization collected arxiv org pubmed com respectively xiao carenini created oracle labels datasets use split cohan models comparison neusum zhou neural extractive model based framework attention mechanism banditsum dong regards sentence selection contextual bandit problem policy gradient methods train model jecs durrett compression based summarization model selects sentences compresses pruning dependency tree reduce redundancy bertsum liu lapta inserts multiple segmentation tokens document obtain sentence representation bert based extractive summarization model employ framework basic document encoder model hibert zhang modifies bert hierarchical structure design unsupervised method pre train discobert state art bert based extractive model encodes documents bert updates sentence representations graph encoder discobert builds document graph sentence units based discourse analysis model incorporates latent topics document graph produce heterogeneous bipartite graph implementation details hyperparameters document encoder use bert base uncased pre trained bert version fine tune experiments implement non bert version model replacing pre trained bert gru chung layer set hidden size compare baseline approaches pre trained language models fairly ntm set topic number dimension size topic representation set implement gnns dgl wang number gat layer set set number attention heads topic nodes sentence nodes hidden size dimension size node representations unchanged train model epochs nvidia cards batch size set pre trained bert encoder parameters randomly initialized optimized adam kingma set balance loss topic modeling sentence selection hyperparameters selected grid search validation set metric training strategy consider empirical training strategies similar cui model efficiently converge specifically pre train ntm epochs learning rate considering convergence speed slower general neural networks joint training ntm parameters trained learning rate learning rate parameters set ntm relatively stable result analysis section reports experimental results evaluate model criteria achieve state art results benefits latent topic contribute summarization end compare model state art approaches widely benchmark datasets cnn nyt evaluate model scientific paper datasets verify discovering latent topics help summarize long documents lastly present ablation case studies analysis overall performance table presents rouge results different models cnn nyt datasets section reports oracle second section reports approaches pre trained language models section reports bert based models section reports models results following observations removing pre trained language mode gru version model outperforms non bert baseline models obtains competitive results compared basic bert datasets model achieves state art results nyt dataset performance cnn dataset par discobert state art bert based extractive summarization model needs mention discobert relies external discourse analysis modeling long range dependencies model achieves highly competitive results external tools proves inherent superiority model oracle neusum zhou banditsum dong jecs durrett bert zhang bertsum liu lapata hibert zhang discobert topic graphsum gru topic graphsum cnn nyt table rouge results test set cnn nyt datasets results comparison models obtained respective papers represents corresponding result reported long document summarization long documents typically cover multiple topics xiao carenini hypothesize model capture important contents entire document discovering latent topics enhancing model sumbasic lexrank lsa cheng lapata attn pntr gen discourse aware topic graphsum gru topic graphsum arxiv pubmed table rouge results test set arxiv pubmed datasets results token cohan results token xiao carenini summarization performance verify hypothesis conduct additional experiments form documents table presents results model state art public summarization systems arxiv pubmed datasets section includes traditional approaches oracle second sections include abstractive extractive models respectively table model substantially outperforms baseline models large margin pre trained bert gaps increase combined bert note discourse aware model cohan slightly outperforms model pubmed dataset possible reason explicitly leverages section information introduction conclusion papers strong clues selecting summary sentences model achieves state art performance scientific paper datasets additional features indicating discovering latent topics help summarize long document consistent aforementioned analysis ablation study analyze relative contributions different modules summarizing documents compare model ablated variants ntm removes ntm module builds document graph fully connected sentence nodes regarded performing self attention calculation bert gat removes graph attention layer directly concatenates sentence representation overall topic vector sends sentence classifier lda version replaces ntm standard lda randomly initializes topic representation figure results model ablated variants datasets figure shows results different variants datasets following observations model outperforms variants datasets proves module necessary combining help model achieve best performance ntm module removed lda instead performance arxiv pubmed datasets declines dramatically cnn nyt datasets results competitive model possible reason lies news documents relatively short leads data sparsity problem reduces effect topic models similarly gat removed performance scientific paper datasets decreased significantly news datasets phenomenon indicates inter sentence relationships especially important summarizing long documents lda topic model boost performance gain lda fewer ntm long documents possible reason lda neural networks inevitably disconnected ntm jointly optimized document encoder graph networks mutually improve module wang analysis latent topics subsection conduct experiments better understand latent topics help summarize documents end define topical weight sentence weighted summation attention score topic sentence cnn model model model dmnytarxivpubmed figure visualized results sentence topical weight degree highlighting represents overall relevance sentence topics underlined sentences model selected summary left document pubmed dataset right document cnn dataset represents topical weight sentence topic distribution document learned ntm described section represents weight topic document attention score topic node sentence node figure shows examples visualized sentence topical weights ground truth summary sentences relatively high topical weights final selected sentences highly overlap topical sentences observation intuitive understanding model works model learns sentence representations discovers latent topics individually second graph attention layer builds semantic relationships sentences topics roughly selects important contents basis topical information finally model accurately selects summary sentences integrating features topical relevance document context information inter sentence relationships process explain model effective long documents latent topics help model preselect salient texts selection mainly focus fragments entire document conclusion future work paper systematically explore effects latent topics document summarization propose novel graph based extractive summarization model allows joint learning latent topics leverages enrich sentence representations heterogeneous graph neural network experimental results studied datasets demonstrate model achieves results par state art summarization models news article datasets significantly outperforms existing approaches scientific paper datasets indicating strong robustness document genres lengths explorations incorporating types semantic units keywords entities document graph enhancing performance summarization addressed future work acknowledgements work supported grant national natural science foundation china thank anonymous reviewers helpful comments aspects work reference alfred aho jeffrey ullman theory parsing translation compiling volume prentice hall englewood cliffs benjamin borschinger mark johnson particle filter algorithm bayesian word segmentation proceedings australasian language technology association workshop pages canberra australia peng cui yuanchao liu bingquan liu neural topic model based variational auto encoder aspect extraction opinion texts natural language processing chinese computing nlpcc lecture notes computer science vol arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang nazli goharian discourse aware attention model abstractive summarization long documents proceedings naacl hlt pages new orleans louisiana association computational linguistics ashok chandra dexter kozen larry stockmeyer alternation journal association computing machinery ziqiang cao furu wei wenjie sujian faithful original fact aware neural abstractive summarization aaai conference artificial intelligence junyoung chung caglar gulcehre kyunghyun cho yoshua bengio empirical evaluation gated recurrent neural networks sequence modeling nips greg durrett taylor berg kirkpatrick dan klein learning based single document summarization compression anaphoricity constraints arxiv preprint jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language understanding proceedings conference north american chapter association computational linguistics human language technologies volume long short papers pages minneapolis minnesota yue dong yikang shen eric crawford herke van hoof jackie chi kit cheung banditsum extractive summarization contextual bandit proceedings conference empirical methods natural language processing pages association computational linguistics gunes erkan dragomir radev lexrank graph based lexical centrality salience text summarization journal artificial intelligence research patrick fernandes miltiadis allamanis marc brockschmidt structured neural summarization arxiv preprint lea frermann alexandre klementiev inducing document structure aspect based summarization proceedings annual meeting association computational linguistics pages florence italy association computational linguistics dan gusfield algorithms strings trees sequences cambridge university press cambridge karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend cortes lawrence lee sugiyama garnett editors advances neural information processing systems pages curran associates inc linmei tianchi yang chuan shi houye xiaoli heterogeneous graph attention networks semi supervised short text classification proceedings conference empirical methods natural language processing international joint conference natural language processing pages hong kong china association computational linguistics diederik kingma jimmy adam method stochastic optimization proceedings international conference learning representations diederik kingma max welling autoencoding variational bayes arxiv preprint zhengyuan liu nancy chen exploiting discourse level segmentation extractive summarization proceedings workshop new frontiers summarization pages hong kong china association computational linguistics yang liu mirella lapata text summarization pretrained encoders proceedings conference empirical methods natural language processing international joint conference natural language processing emnlp ijcnlp pages hong kong china association computational linguistics yang liu mirella lapata hierarchical transformers multi document summarization proceedings annual meeting association computational linguistics pages florence italy association computational linguistics yishu miao edward grefenstette phil blunsom discovering discrete latent topics neural variational inference proceedings international conference machine learning icml sydney nsw australia pages rada mihalcea paul tarau textrank bringing order text proceedings conference empirical methods natural language processing pages shashi narayan shay cohen mirella lapata details summary topic aware convolutional neural networks extreme summarization proceedings conference empirical methods natural language processing pages brussels belgium association computational linguistics ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents aaai conference artificial intelligence mohammad sadegh rasooli joel tetreault yara parser fast accurate dependency parser computing research repository arxiv preprint version abigail peter liu christopher manning point summarization pointergenerator networks proceedings annual meeting association computational linguistics volume long papers pages vancouver canada akash srivastava charles sutton autoencoding variational inference topic models arxiv preprint ilya sutskever oriol vinyals quoc sequence sequence learning neural networks advances neural information processing systems pages evan sandhaus new york times annotated corpus linguistic data consortium philadelphia petar velikovi guillem cucurull arantxa casanova adriana romero pietro lio yoshua bengio graph attention networks arxiv preprint ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need guyon luxburg bengio wallach fergus vishwanathan garnett editors advances neural information processing systems pages curran associates inc yue wang jing hou pong chan irwin king michael lyu shuming shi topic aware neural keyphrase generation social media language proceedings annual meeting association computational linguistics pages florence italy association computational linguistics danqing wang pengfei liu yining zheng xipeng qiu xuanjing huang heterogeneous graph neural networks extractive document summarization arxiv preprint minjie wang lingfan zheng quan gan gai zihao mufei jinjing zhou huang chao ziyue huang qipeng guo hao zhang haibin lin junbo zhao jinyang alexander smola zheng zhang deep graph library efficient scalable deep learning graphs iclr workshop representation learning graphs manifolds yang wei document summarization method based heterogeneous graph international conference fuzzy systems knowledge discovery pages ieee wen xiao giuseppe carenini extractive summarization long documents combining global local context proceedings conference empirical methods natural language processing international joint conference natural language processing pages hong kong china association computational linguistics jiacheng greg durrett neural extractive text summarization syntactic compression arxiv preprint jiacheng zhe gan cheng jingjing liu discourse aware neural extractive model text summarization arxiv preprint michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan dragomir radev graph based neural multi document summarization proceedings conference computational natural language learning pages vancouver canada association computational linguistics xingxing zhang mirella lapata furu wei ming zhou neural latent extractive document summarization proceedings conference empirical methods natural language processing pages association computational linguistics xin zheng aixin sun jing karthik muthuswamy subtopic driven multi document summarization proceedings conference empirical methods natural language processing international joint conference natural language processing pages hong kong china association computational linguistics xingxing zhang furu wei ming zhou hibert document level pre training hierarchical bidirectional transformers document summarization proceedings annual meeting association computational linguistics pages florence italy association computational linguistics qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural document summarization jointly learning score select sentences proceedings annual meeting association computational linguistics volume long papers pages association computational linguistics
