different approaches for identifying important concepts in probabilistic biomedical text summarization milad moradi nasser department of electrical and computer engineering isfahan university of technology isfahan iran e mail abstractautomatic text summarization tools help users in biomedical domain to acquire their intended information from various textual resources more efficiently some of the biomedical text summarization systems put the basis of their sentence selection approach on the frequency of concepts extracted from the input text however it seems that exploring other measures rather than the frequency for identifying the valuable content of the input document and considering the correlations existing between concepts may be more useful for this type of summarization in this paper we describe a bayesian summarizer for biomedical text documents the bayesian summarizer initially maps the input text to the unified medical language system umls concepts then it selects the important ones to be used as classification features we introduce different feature selection approaches to identify the most important concepts of the text and to select the most informative content according to the distribution of these concepts we show that with the use of an appropriate feature selection approach the bayesian biomedical summarizer can improve the performance of summarization we perform extensive evaluations on a corpus of scientific papers in biomedical domain the results show that the bayesian summarizer outperforms the biomedical summarizers that rely on the frequency of concepts the domain independent and baseline methods based on the recall oriented understudy for gisting evaluation rouge metrics moreover the results suggest that using the meaningfulness measure and considering the correlations of concepts in the feature selection step lead to a significant increase in the performance of summarization sentence classification keywordsmedical text mining data mining bayesian classification feature selection umls concept corresponding author address department of electrical and computer engineering isfahan university of technology isfahan iran phone fax alternate email manuscript march introduction biomedical information available for researchers and clinicians is accessible from a variety of sources such as scientific literature databases electronic health record ehr systems web documents e mailed reports and multimedia documents the scientific literature provides a valuable source of information to researchers it is widely used as a rich source for assessing the newcomers in a particular field gathering information for constructing research hypotheses and collecting information for interpretation of experimental results it is interesting to know that the us national library of medicine has indexed over million citations from more than biomedical journals in its medline bibliographic however a large amount of data can not be effectively used to attain the desirable information in a limited time the required information should be accessed easily at the right time and in the most appropriate form for clinicians and researchers efficiently seeking useful information from the ever increasing body of knowledge and other resources is excessively time consuming managing this information overload is shown to be a difficult task without the help of automatic tools automatic text summarization is a promising approach to overcome the information overload problem reducing the amount of text that must be read it can be used to obtain the gist efficiently on a topic of interest it helps the clinicians and researchers to save their time and effort required to seek information five reasons have been identified for producing summaries from full text documents even when they provide abstracts the reasons include there are variants of an ideal summary in addition to the abstract some content of the full text may be missed in the abstract customized summaries are useful in question answering systems automatic summaries allow abstract services to scale the number of documents they can evaluate and assessing the quality of sentence selection methods can be helpful in development of document summarization systems in recent years various summarization methods have been proposed based on biomedical concepts they have improved the performance of biomedical text summarization focusing on concepts extracted from the source text rather than terms this concept level analysis of text is performed with the help of biomedical knowledge sources such as the unified medical language system umls it has been shown that constructing a frequency distribution model from the concepts of the original text and following this model to create the summary yields better performance compared to traditional term based methods another successful biomedical summarization approach relies on concept chaining identifying strong chains based on the frequency of their contained concepts and sentence scoring according to the presence of the relevant concepts from the strong chains regarding such summarization methods some questions should be taken into account for summarizing a document do similar approaches like probability distribution modeling of concepts yield a desirable summarization performance for concept based biomedical text summarization manuscript march should the summarizer consider all the concepts extracted from the source text are there any concepts which can be regarded as redundant and removed to increase the accuracy of the frequency or probability distribution model are there any criteria rather than the frequency to identify the important concepts can the model be more accurate by considering the correlations existing between concepts in the source text in this paper we address these questions describing a bayesian summarization method based on the probability distribution of concepts within the input document we also introduce and evaluate different feature selection approaches to select the relevant concepts of the text and to use them as the classification features the bayesian summarizer initially maps the input text to biomedical concepts contained in the umls a well known knowledge source in biomedical sciences maintained by the us national library of medicine then it identifies important concepts and selects them as classification features to this end we discuss five different feature selection strategies based on various criteria and methods the first strategy is the simplest one that considers all the extracted concepts the second method discards the concepts which seem to be potentially redundant and unnecessary the third method filters the concepts using the ranking method and according to the frequency of the concepts the fourth method utilizes a meaningfulness measure defined by the helmholtz principle to identify the important concepts the fifth method discovers the correlated concepts that represent the subtopics of the text by using frequent itemset mining a well known data mining technique after the feature selection step the summarizer represents each sentence as a vector of boolean features and specifies the value of features according to the occurrence of important concepts in the sentence afterwards in the classification stage it classifies the sentences into summary and non summary classes using the nave bayes classification method the classifier estimates the posterior probability of classifying a sentence using the prior and likelihood probabilities of concepts the summarizer selects the sentences that obtain the highest posterior odds ratio por values and puts them together to form the final summary to evaluate the performance of the proposed method we conduct a set of experiments on a of scientific papers from the biomedical domain and compare the results with other concept based biomedical summarizers we also evaluate the usefulness of the five different feature selection approaches to determine the competency of each one for this type of summarization the results demonstrate that when the bayesian summarizer uses the fourth and the fifth feature selection methods it performs significantly better than the other frequency based biomedical summarizers regarding the most commonly used recall oriented understudy for gisting evaluation rouge metrics the main contributions of this paper are using the nave bayes classifier in concept based biomedical text summarization for classifying the sentences of a document based on the probability distribution of important concepts within the source text manuscript march evaluating different feature selection approaches to identify the important concepts of a document and using them as classification features using different measure the meaningfulness rather than the frequency to determine the important concepts of a document and using them as classification features and discovering the correlated concepts of a document using itemset mining and using them as classification features the remainder of the paper is organized as follows section gives an overview of text summarization as well as a review of the related work in biomedical summarization in section we introduce our biomedical summarization method based on a bayesian classifier we also define different feature selection approaches that can be utilized in our summarization process then we describe the evaluation methodology in section the results of the preliminary experiments and the evaluations are presented in section and discussed in section finally section draws the conclusion and describes future lines of work background and related work text summarization text summarization methods can be divided into abstractive and extractive approaches an abstractive summarizer uses natural language processing nlp methods to process and analyze the input text then it infers and produces a new version on the other hand an extractive summarizer selects the most representative units paragraphs sentences or phrases from the original wording and puts them together into shorter form another classification of text summarization differentiates single document and multi document inputs a single document summarizer produces a summary which is the result of condensing only one document in contrast a multi document summarizer gets a cluster of inputs and provides a single summary another classification of summarization methods is based on the requirements of users generic oriented also known as query focused summarizers a general summary presents an overall implication of input without any specified preferences regarding content while a user oriented summary is biased towards a given query or some keywords to address a user s specific information requirement summarization systems can be supervised or unsupervised regarding whether they need training data a supervised system learns from labeled data to select the essential content of new documents while an unsupervised system generates summaries for new documents without relying on any training data in addition to the above categorizations there are other types of summaries including indicative informative multi lingual mono lingual cross lingual web based e mail based personalized sentiment based survey and update summaries the bayesian summarizer described in this paper is extractive single document generic and unsupervised manuscript march in the biomedical field various summarization methods have been proposed these methods have been reviewed in a survey of early work and in a systematic review of recently published research there have been some research works towards abstractive biomedical summarization they could be regarded as tools for providing decision support data from medline citations summarizing research related to the treatment of diseases helping in evidence based medical care summarizing drug information and multi document summarization of medline citations these methods mostly produce graphical summaries on the other hand the majority of extractive biomedical summarization systems focus on producing textual summaries extractive summarization methods have been widely studied in the biomedical domain for different tasks such as summarizing clinical notes developing clinical decision support tools for patient specific recommendation and treatment and the summarization of ehrs many biomedical summarizers utilize the umls knowledge source to map the input text to a wide range of biomedical and generic concepts this mapping helps the systems to be domain specific and act more accurately compared to traditional term based methods there are several knowledge sources such as mesh snomed go omim uwda and ncbi taxonomy widely used in knowledge intensive data and text processing tasks in the biomedical domain these knowledge sources along with over controlled vocabularies classification systems and additional information sources have been unified into the umls plaza performed an investigation on the impact of different knowledge sources on the performance of a summarization system the evaluations showed that the quality of generated summaries was improved significantly with the use of an appropriate knowledge source we make use of the umls concepts to incorporate the domain knowledge into the text modeling and the summarization process of our summarizer some of the biomedical summarization methods employed graph representation along with the umls concepts for semantic modeling of the input text plaza al proposed a graph based approach to biomedical summarization they used the umls concepts and the semantic relations between them to construct a semantic graph that is representative of the input document their system determined different topics within the text by applying a degree based clustering algorithm on the semantic graph another work performed the task of summarization based on a genetic graph based clustering algorithm using the continuity of concept relations rather than the centroid method it separated clusters and identified main topics menendez et al applied a combination of both genetic clustering and graph connectivity information to improve the performance of the previous semantic graph based summarization systems compared to these approaches our bayesian summarization method utilizes a simpler modelling representing the sentences of the input document as vectors of features the features are important concepts within the input text merging the domain knowledge and traditional methods some domain specific tools have been proposed for biomedical summarization one of the studies identified a set of medical cue terms and phrases and combined them with commonly used traditional features such as word frequency sentence position the similarity with the title of the article and sentence length the summarizer used the domain specific and manuscript march generic features for sentence scoring and summary generation sarkar al proposed a supervised summarization method based on bagging and decision tree as the base learner they utilized the key terms in mesh as a source of domain knowledge as well as other features including centroid overlap first sentence overlap sentence position sentence length and acronyms in a hybrid summarization system a classifier was utilized to learn and group sentences into six types of population intervention background outcome study and other the system also used a learning to identify important umls concepts commonly appearing in summaries relative sentence position and sentence length were other features used by the summarizer another study evaluated different positional approaches for sentence extraction in a semantic graph based method for biomedical literature summarization the study showed that sentences appearing in various sections of a biomedical article should be assigned different weights we do not use any positional information in our summarization method this allows the method to be applicable to input texts in which positional information may not be indicative of the importance and the informativeness of sentences some of the biomedical summarizers use the frequency of umls concepts extracted from the source text as the basis of their summarization approach biochain method pursued the lexical chaining idea creating chains and putting each group of semantically related terms into a chain biochain used concepts rather than terms and put concepts belonging to the same semantic type into a chain it computed the score of each chain using the frequency information of concepts identified strong chains and concepts and selected summary sentences according to the presence of strong concepts freqdist method performed the task of sentence selection based on the frequency distribution of concepts within the source text it initially created a frequency distribution model from the source text also a summary frequency distribution model using an iterative sentence selection procedure it selected a sentence that led to the closest alignment between the summary and original text frequency distributions in each iteration in contrast to biochain and freqdist our method does not merely make use of the concept frequency employing the nave bayes classifier it selects the sentences according to the probability distribution of concepts within the input text it still benefits from the frequency information in the form of two coefficients that provide the classifier with additional knowledge compared to freqdist our method does not consider the distribution of all the extracted concepts we evaluate different feature selection strategies to discard redundant and unnecessary concepts compared to biochain our method does not merely rely on the concept frequency to identify important concepts we use another metric namely the meaningfulness in the form of a feature selection method that yields better summarization performance moreover as one of the feature selection approaches we extract correlated concepts and use each set of them as a classification feature this correlation information provides the classifier with some additional knowledge to decide more accurately leading to an increase in the performance of the summarizer in domain independent text summarization some methods have been proposed based on bayesian approach one of the basic methods employed a set of features such as sentence length cut off manuscript march phrase paragraph thematic word and uppercase word to represent the sentences of a text document it trained a bayesian classifier on a training corpus and used the classifier to summarize new documents bayesum a supervised and query focused multi document summarizer built on bayesian inference and language modeling techniques represented documents and queries as probability distributions of words from a vocabulary estimating a sentence model for each sentence it ranked sentences based on the language model and the similarity of sentences to the query model wang et al proposed a bayesian sentence based topic model for multi document summarization they employed a variational bayesian to model the probability distribution of selecting sentences given topics and to estimate the model s parameters compared to these methods our biomedical summarizer does not use any complicated topic modeling approaches and does not need any training data it uses the prior probability of concepts to estimate the probability of selecting sentences for inclusion in the summary we give the nave bayes classifier some additional knowledge in the form of two coefficients and different feature selection approaches to the authors knowledge no biomedical summarization method has been proposed so far based on the nave bayes classifier and estimating the probability of summary sentences by following the distribution of concepts within the source text the rationale of our approach will be presented in section where it will be showed that for a corpus of biomedical papers the concepts within both the full text papers and the ideal summaries abstracts follow the same distribution classification our proposed summarization scheme consists of two main phases preparation and classification in the preparation phase we perform concept extraction feature selection and sentence representation in the classification phase we utilize a bayesian classifier to select sentences for the final summary in the following we give an overview of bayesian classification in general a bayesian classifier is based on the bayes theorem defined by eq below where c and x are random variables in classification tasks they refer to observing class c and instance x respectively x is a vector containing the values of features is the posterior probability of observing class c given instance in classification it could be interpreted as the probability of instance x being in class c and is what the classifier tries to determine is the likelihood which is the probability of observing instance x given class it is computed from the training data and are the prior probabilities of observing class c and instance x respectively they measure how frequent the class c and instance x are within the training data using eq the classifier can compute the probability of each class of target variable c given instance x and the most probable class the class that maximizes should be selected as the manuscript march result of classification this decision rule is known as maximum a posteriori map it is represented as follows arg is the jth class or value of target variable in eq the denominator is removed because it is where constant and does not depend on making a conditional independence assumption the nave bayes classifier reduces the number of probability values that must be estimated it assumes that the probability of each value of feature xi is independent of the value of any other features given the class variable cj therefore the nave bayes classifier finds the most probable class for the target variable by simplifying the joint probability calculation as follows arg max the posterior odds ratio por is a well known measure to assess the confidence of bayesian classification the por shows a measure of the strength of evidence for a particular classification compared to other class values it is calculated as follows is the posterior odds ratio that measures the strength of evidence in favor of classifying the against classifying the instance as class variable where instance as class variable the bayesian summarizer our bayesian summarization method consists of five steps including mapping text to biomedical concepts feature selection preparing sentences for classification sentence classification using nave bayes and summary generation fig illustrates the architecture of the bayesian summarizer in the following subsections we give a detailed description of each step manuscript march fig the architecture of our proposed bayesian biomedical text summarization method text to biomedical concepts firstly the summarizer maps the input text to the concepts of the umls metathesaurus the metathesaurus is a large multi lingual and multi purpose lexicon that contains millions of biomedical and health related concepts their relationships and their synonymous names in addition to the metathesaurus the umls includes two main components namely specialist lexicon and semantic network the specialist lexicon is a lexicographic information database intended to use in nlp systems it contains commonly occurring english words and biomedical vocabulary and records their syntactic morphological and orthographic information the semantic network consists of a set of broad subject categories known as semantic types these semantic types provide a categorization of all the concepts included in the metathesaurus it also contains a set of semantic relations between the semantic types for mapping biomedical text documents to the umls metathesaurus concepts the us national library of medicine has developed metamap program using a knowledge intensive approach based on nlp computational linguistic and symbolic techniques metamap identifies noun phrases in a text and extracts corresponding concepts metamap may return multiple concepts when a noun phrase is mapped to more than one concept in this situation the summarizer selects all the mappings returned by metamap it has been shown that the all mappings strategy can be useful in concept based biomedical text summarization metamap returns a semantic type along with each concept that determines the semantic category of the concept as noted manuscript march above the semantic types are included in the umls semantic network for the mapping step we use the version of metamap program and the umls release as the knowledge base fig shows a sample sentence and the concepts identified in the first step fig a sample sentence and its identified concepts from the umls metathesaurus selection in this step the summarizer identifies important concepts within the input document to this aim we introduce five feature selection approaches in the classification step the summarizer uses the important concepts as classification features we evaluate and discuss the impact of the feature selection approaches on the performance of the bayesian summarizer in section and in this subsection we use a sample to present some examples of the feature selection strategies the sample document is a scientific article about genetic overlap between autism schizophrenia and bipolar disorders it contains sentences first approach using all extracted concepts as features the simplest approach to feature selection for the bayesian summarizer is to consider all the distinct extracted concepts it means that the summarizer decides about the summary and non summary sentences considering the distribution of all the contained concepts regardless of whether they are important or not for example after concept extraction the sample document contains concepts of which are distinct concepts that are used as the classification features from these distinct concepts concepts appear only one time in the document s sentences and the three most frequent concepts appear in and sentences summarization method we use the first feature selection approach as a baseline to assess the amount of improvement obtained by the other strategies it also shows the impact of redundant and noisy features on the performance of the available at manuscript march second approach filtering out generic features there are some semantic types that their concepts can be discarded in the analysis of the input document they are generic and broad concepts and almost frequently appear in majority of documents either in general english and biomedical texts these semantic types have been identified empirically and include functional concept qualitative concept quantitative concept temporal concept spatial concept mental process language idea or concept and intellectual product in the second feature selection method we remove concepts that belong to these semantic types and use remaining concepts as classification features in this way we remove a set of potentially redundant and misleading features and we expect an improvement in the quality of produced summaries using this feature selection method the summarizer discards the following concepts from the sentence represented in fig widening analysis aspect further relationships and etiology aspects the sample document as an example of the second approach distinct concepts remain after removing generic concepts from third approach filtering features by ranking filtering is a category of feature selection methods in filtering methods features are scored based on some ranking criteria such as relevance correlation mutual information and the frequency of occurrence then the high ranked features are selected according to a threshold or a predefined number of features for the third feature selection strategy we filter features by the frequency of their corresponding concepts in the text after mapping the input text to the umls concepts and removing the potentially redundant features similar to the second approach the summarizer adds remaining features to a list named then it ranks the features based on the frequency of corresponding concepts such that a higher rank is assigned to the feature that its corresponding concept appears more frequently finally it filters the features of the using a threshold it removes a feature if its frequency is less than the threshold value in the following we introduce three possible thresholds where is the arithmetic mean of all concept frequencies in the and is the standard deviation of all concept frequencies in the in section we evaluate these three thresholds and use the best one as an optimum threshold for the third feature selection method manuscript march as an example when we use eq as the threshold value from the initial distinct concepts of the sample document concepts are selected as features when we use eq and eq as the threshold value and eight features are selected respectively fig shows the identified important concepts for the sample document along with their semantic types and frequencies in this example we use eq as the threshold value the and eight of which are selected as features fig are equal to and respectively there are concepts in the fig the identified important concepts by filtering ranking and selecting based on a given threshold for the sample document the semantic types are represented in brackets the features are sorted based on their frequencies fourth approach selecting meaningful features by the helmholtz principle the helmholtz principle from the gestalt theory of human perception defines a measure of meaningfulness for rapid change detection and keyword extraction in unstructured and textual data in data mining context the helmholtz principle states that essential features and interesting events are observed in large deviations from randomness in text mining research the helmholtz principle has been used for document processing and keyword extraction automatic text summarization and supervised and unsupervised feature selection the primary study dealt with words to extract the meaningful units of a text document but we deal with concepts instead in the fourth method the feature selection process is modeled as follows let d be the input document and p be a part of we consider p as a paragraph but it can be any structural unit of the input document such as a sentence or a page after mapping the input document d to the umls concepts we remove generic potentially redundant concepts similar to the second approach and add remaining concepts to the set for each concept c in the set c we start to compute the meaningfulness measure by calculating the number of false alarms nfa in each if the concept c appears m times in the p and k times in the whole document d then the nfa is calculated as follows where n is equal to that l is the total number of concepts in the document d and b is the total number of concepts in the paragraph in eq is a binomial coefficient computed as follows manuscript march to measure the meaningfulness value of concept c from d inside p the following formula is used log eventually we construct a set of meaningful concepts we add each concept c in c that its value is greater than to the where is the maximum of over all paragraphs p and is a parameter that determines the level of meaningfulness the summarizer selects the concepts included in the as classification features in section and we evaluate and discuss the optimum value of the parameter fig the identified meaningful concepts as the features by the helmholtz principle for the sample document the semantic types have been represented in brackets the features have been sorted based on the descending order of their meaning measures for example using this feature selection approach for the sample document when is equal to and the number of selected features is and respectively fig shows the meaningful concepts of the sample document identified by the helmholtz principle with a meaningfulness level of as can be seen there is no obvious relation between the meaning and frequency values the meaning values depend on the appearing pattern of the concepts in the paragraphs and the whole document fifth approach extracting correlated features by itemset mining in the four previous approaches features are representative of single concepts however some correlations may be exist among multiple concepts it means that some dependent concepts appear together in sentences manuscript march and point to one of the subtopics of the text in the fifth feature selection method we utilize frequent itemset mining to extract correlated concepts and use each set of dependent concepts as a classification feature frequent itemset mining is a data mining technique for finding items that appear together in a dataset it can be effectively used in our context to discover correlated concepts that frequently appear in the input text for the fifth method we utilize a well known itemset mining algorithm namely the apriori although the apriori algorithm is usually used for association rule mining we make use of its ability to extract frequent itemsets this algorithm works with datasets structured in a transactional format in a transactional dataset there are several transactions each one contains some items we can consider the input document as a transactional dataset such that the apriori algorithm deals with each sentence and its contained concepts as a transaction and its items therefore we perform itemset mining to discover frequent itemsets containing concepts that frequently appear in the input text each itemset has a property named itemset support calculated by dividing the number of sentences that contain the itemset by the total number of sentences in the document an itemset is said to be frequent if its support value is greater than or equal to a given minimum support threshold a k itemset is an itemset which contains k items if all the subsets of a k itemset are frequent the itemset is said to be a frequent k itemset given an input document its extracted concepts and a minimum support threshold we perform the apriori algorithm to discover correlated concepts each set of k correlated concepts discovered as a frequent k itemset demonstrates a subtopic of the document in section and we evaluate and discuss the optimum value of the parameter fig the frequent itemsets extracted as the classification features from the sample document minimum support threshold manuscript march in the fifth feature selection strategy after mapping the input document to the umls concepts we remove generic concepts similar to the second approach and apply the apriori algorithm the summarizer uses the extracted frequent itemsets as classification features for instance when the minimum support threshold is equal to and the number of extracted frequent itemsets is and respectively fig shows the extracted frequent itemsets extracted from the sample document in this example the value of minimum support threshold is the apriori algorithm returns a total number of frequent itemsets as can be seen seven itemsets contain more than one concept these itemsets convey the correlations that exist among some dependent concepts sentences for classification after concept extraction and feature selection the summarizer must represent the sentences of the input document in an appropriate format to be prepared for the classification step it considers all the selected features as boolean such that for a given sentence it sets the value of a feature to true if the corresponding concept appears in the sentence otherwise it sets the value to false some features contain more than one concept when the summarizer uses the fifth feature selection method for such features the summarizer sets the value to true if all the corresponding concepts appear in the sentence after assigning feature values the summarizer discards the sentences whose all feature values are false we consider these sentences as unimportant and do not use them in the subsequent steps for example let the summarizer uses the third feature selection strategy and the threshold for the sample document fig shows the selected features the sample document consists of sentences defined in eq to filter the features of which sentences do not contain any important concepts the summarizer considers these sentences as unimportant and discardeds them for the subsequent operations therefore sentences remain for preparation and classification note that we use this example hereafter to simplify the explanation of the remaining steps in case of using the other feature selection approaches the summarizer performs the preparation and classification stages likewise in this step the summarizer creates a vector of features for each remaining sentence it assigns a number to each vector according to the corresponding sentence number for example there are vectors for the sample document each one has eight features each feature corresponds to an important concept identified in the previous step the summarizer assigns the values of features in the ith vector according to the presence and absence of important concepts in the ith sentence for each vector there is a target class or a target variable named summary which is initially unknown in the classification step our bayesian summarizer determines the value of the target class as yes or no for all the vectors as an example in the sample document the sentence is therefore just as for deletions it is apparent that these large cnvs confer risk of a range of neurodevelopmental phenotypes including autism mental retardation and schizophrenia in this sentence four important concepts can be seen identified by manuscript march the threshold gene hence in the vector the values of these four features are true and the values of the other and represented in fig including autistic disorder schizophrenia deletion mutation and features are false fig shows the vector in the sample document the summarizer assigns the feature values for all the vectors just in the same way as the above example after this step we have a collection of vectors and their feature values their class variables are unknown and the summarizer must classify them as summary sentences yes or non summary sentences no every document has its particular set of concepts and the features of each text are different from others fig the sentence vector corresponding to the sentence in the sample document the summary class variable is initially unknown classification the compression rate is a parameter in summarization systems which determines the percentage of the input text that must be extracted as the final summary the summarizer does not initially know about the in eq the prior probability of class variable values however it knows what percentage of sentences that must be selected as summary and it can estimates the and for instance the total number of sentences in the sample document is suppose the compression rate is it means that of the text about sentences must be selected for the final summary in the preparation step the summarizer discarded the sentences that did not include any important concepts and vectors remained for the classification step the summarizer does not know which of these vectors has yes value for the summary class variable but it knows vectors must have the yes value hence for this example the is equal to and is equal to generally the is equal to the number of sentences that must be selected for the final summary according to the compression rate divided by the total number of remaining sentences for the classification step the bayesian summarizer follows an assumption about the in eq that simplifies the estimation of the likelihood probabilities the summarizer assumes that the summary can convey an manuscript march informative content if it follows the distribution of important concepts within the input document with regard to this assumption the summarizer can estimate the likelihood probabilities the probability of observing an important concept given class yes or no for example the concept schizophrenia appears in sentences within the sample document and its distribution within all the vectors is equal to therefore the probability of observing concept schizophrenia given class yes the would be equal to likewise the probability of not observing concept schizophrenia given class yes would be equal to likewise the summarizer estimates the likelihood probabilities of observing and not observing a concept given class value no in this step the summarizer can estimate the posterior probability of class values given a vector if the summarizer selects the value that maximizes the posterior probability of summary variable given ith vector similar to eq the number of sentences classified as yes may be less than the number of sentences that must be selected for the summary this comes true because in a vector the number of features having the true value is often less than the number of features having the false value therefore the summarizer should decide about the summary class values in a different way compared with eq the summarizer estimates the posterior probability of class values for each vector and assesses the strength of evidence for class value yes using the por measure we incorporate two coefficients into the estimation of posterior probabilities to discriminate between the presence and absence of more important and less important concepts in our context the presence of important concepts the true value for the features and their prior probabilities are contributing factors in the selection a sentence for the summary on the other hand when the suumarizer uses the bayes rule it does not discriminate between the presence and the absence of more important high frequent and less relevant frequent concepts in this case the summarizer decides based on the highest probable values of features as we show in section for the majority of documents even the high frequent concepts appear in less than of the sentences of a document this shows that for the majority of documents the most probable value for all features is false therefore for estimating the posterior probabilities of the class value yes and no the summarizer should take into account this issue we address this problem employing two coefficients in estimation of the posterior probability of class values the coefficients increase and decrease the impact of important concepts on the posterior probability of class values based on the frequency of concepts and whether they occur in a sentence or not we evaluate and discuss the impact of using these coefficients on the accuracy of the bayesian summarizer in section and the probability of inclusion in the summary to estimate the posterior probabilities firstly the summarizer estimates the posterior probability of class value yes given ith vector by rewriting eq as follows manuscript march where is the ith vector is the posterior probability of classifying the ith vector as yes given is the kth feature in the ith vector and is the likelihood probability the probability of observing or given class variable the value of the coefficient depends on whether the is true or false and is specified as follows k where is true or false the coefficient is the frequency of the concept corresponding to the affects the depending on whether the value of in two ways when the is true the frequency of corresponding concept is multiplied by the thus the values of the and increase consequently the presence of more frequent concepts increases the chance of selecting a sentence for the summary when the is false the inverted frequency of corresponding concept is multiplied by the and as a result the values of the and decrease in this case a higher frequency decreases the with a higher rate hence the absence of more frequent concepts decreases the chance of selecting a sentence for the summary the probability of exclusion from the summary after estimating the probability of inclusion in the summary the summarizer estimates the posterior probability of class value no given ith vector as follows where is the ith vector is the posterior probability of classifying the ith vector as no given is the kth feature in the ith vector and is the likelihood probability the probability of observing or given class variable the value of the coefficient depends on whether the is true or false and is specified as follows manuscript march o n m where the value of when the is the frequency of the concept corresponding to the is true or false the affects the in two ways similar to the depending on whether is true the inverted frequency of corresponding concept is multiplied by the and as a the and decrease in this case a higher frequency decreases the with a higher rate hence the presence of more frequent concepts decreases the probability of not selecting the values of result a sentence for the summary when the thus consequently the absence of more frequent concepts increases the probability of not selecting a sentence is false the frequency of corresponding concept is multiplied by the increase the values of the and for the summary after estimating the probability of classifying each vector as yes and no the summarizer needs to decide which sentences should be selected for inclusion in the final summary as mentioned earlier if the classifier selects for each vector the class value which maximizes the posterior probability of summary class variable the number of sentences classified as yes may be less than the number of sentences required for the summary therefore we employ the por measure early explained in section to classify the vectors for the ith vector the summarizer computes the value of the por by rewriting eq as follows is the posterior odds ratio of the ith vector the values of the and where are the posterior probabilities of classifying the ith vector as yes and no given which the summarizer estimated earlier the por demonstrates a measure of the strength of evidence for a particular class value therefore the greater pori for a vector the higher strength of evidence in favor of classifying the vector as yes after calculating the por value for all the vectors the summarizer can decide which sentences should be selected for the summary it sorts the vectors in descending order of their por values and assigns the ranked n vectors to the class yes where n is the number of sentences which must be selected to make the summary specified by the compression rate the summarizer assigns the remaining vectors to the class no manuscript march in the following we explain a redundancy reduction method that the summarizer can use to decrease redundant information in the summary the redundancy reduction method the problem of redundancy in text summarization concerns the same repeated information conveyed by multiple sentences in a summary compared to multi document summarization it is less probable to find redundant information in a summary produced for a single document however redundancy removal approaches can also be useful in single document summarization maximal marginal relevance mmr is a well known method for removing redundancy especially in query focused summarization it computes cosine similarities between sentences and a query also between sentences and already selected sentences then it assigns a marginal relevance to each sentence and adds the sentence with the maximum marginal relevance to the summary the mmr computes a linear combination of two functions relevance and novelty the relevance function needs a query to assess the relatedness of sentences since the bayesian summarizer does not use any query for summarization the mmr approach is not applicable to our method hence we propose a redundancy reduction method based on our context by gradually updating the probabilities to decrease the chance of high probable concepts and increase the chance of less probable concepts for inclusion in the summary we employ an iterative method aimed at reducing the redundancy that can emerge in the summary due to the large prior probability of high frequency concepts when the possible redundancy does not matter to the summarizer as explained earlier the summarizer estimates the prior likelihood and posterior probabilities then it computes the por values for all the sentences and ranks them based on their por values finally it assigns the top n sentences to the class value yes on the other hand when the summarizer employs the redundancy reduction method it performs the sentence selection process differently in this case when it computes the por values it assigns only the sentence having the highest por value to the class value yes next it estimates the prior likelihood and posterior probabilities again but it does not consider the previously selected sentences and their concepts in the subsequent estimations accordingly it reduces the probability of observing the high frequency concepts included in the sentences already selected moreover the summarizer increases the chance of observing the low frequency concepts in the summary in the subsequent iterations it computes the por values based on new probabilities it repeatedly estimates the probabilities without considering the sentences already selected for the summary and selects the sentence with the maximum por value until the number of summary sentences assigned to the class value yes reaches finally it assigns the remaining sentences to the class value no in section and we evaluate and discuss the efficiency of the redundancy reduction method manuscript march generation in the previous step the summarizer assigned the sentences to the class values yes and no in the summary generation step it adds the sentences of the class value yes to the summary it arranges the summary sentences in the same order as they appear in the primary document finally it adds the figures and tables in the main document referred to in the summary fig shows the summary of the sample document produced by the bayesian summarizer in this example for brevity reasons the compression rate is it means that the size of the summary must be of the input document fig the summary of the sample document generated by the bayesian summarizer compression evaluation method corpus the most common method of evaluating summaries generated by an automatic summarizer is to compare them against manually generated summaries known as model or reference summaries in such evaluation method we measure the similarity between the content of system and model summaries the more content shared between system and model summaries the better the performance of the summarization system obtaining manually generated summaries is a challenging and time consuming task because they have to be provided by human experts moreover human generated model summaries are highly subjective to the authors knowledge there is no of model summaries for single document biomedical text manuscript march summarization however most scientific papers have an abstract which can be considered as the model summary for evaluation to compare our bayesian biomedical summarization method against other summarizers we use a collection of biomedical scientific papers randomly selected from the biomed central s for text mining the size of evaluation corpus is large enough to allow the results of the assessment to be significant we use the abstracts of the papers as the model summaries to evaluate the performance of the generated summaries we perform our preliminary experiments using a separate development corpus containing papers randomly selected from the biomed central s corpus metrics a common feature which we assess in the performance evaluation of text summarization systems is the informativeness it is a feature for representing how much information from the original text is provided by the summary in this paper we use the rouge package to evaluate the performance of the bayesian summarizer in terms of the informative content of summaries the rouge package compares a generated summary with one or more model summaries estimates the shared content between them by calculating the proportion of shared n grams and produces different scores in terms of different metrics the rouge metrics produce a score between and the higher scores for a system summary the greater content overlap between the system and model summaries in our evaluations we use the following rouge metrics it computes the number of shared unigrams grams between the system and model summaries summaries it computes the number of shared bigrams grams between the system and model rouge r it computes the union of the longest common subsequences between the system and model summaries it takes into account the presence of consecutive matches rouge r it computes the overlap of skip bigrams pairs of words having intervening word gaps between the system and model summaries it allows a skip distance of four between bigrams in spite of their simplicity the rouge metrics have shown a high degree of correlation with human judges experiments and parameterization we introduce five feature selection approaches in section the first approach selects the classification features by simply considering all the extracted concepts the second approach tries to reduce the number of features by filtering out the generic features that seem to be potentially redundant in the third approach we manuscript march rank the features based on the frequency of corresponding concepts and use a threshold as a filtering criterion we introduce three possible threshold values in section based on the average and the standard deviation of the frequency of concepts in feature selection experiments we evaluate these three values to specify the optimum threshold for this type of filtering in the fourth feature selection approach we measure a meaningfulness value for each concept using the helmholtz principle we use a parameter that determines the level of meaningfulness for the concepts selected as classification features we perform a set of experiments to tune the parameter in the fifth approach we utilize an itemset mining method to extract correlated concepts in the form of frequent itemsets and use them as classification features the itemset mining extracts frequent itemsets according to a minimum support threshold we tune the parameter performing a set of preliminary experiments in the other set of preliminary experiments we assess the impact of the coefficients and introduced in section on the performance of our summarization method we evaluate the quality of the produced summaries in two situations the presence and the absence of the coefficients in section we introduce a redundancy reduction method to decrease the potential redundancy in the summary we conduct another set of experiments to investigate the impact of the redundancy reduction method on the performance of the bayesian summarizer it seems that the redundancy reduction strategy may achieve more percent of improvement under smaller compression rates we also assess the percentage of improvement under smaller and larger compression rates than with other summarization methods we compare the bayesian summarizer with six summarization methods and two baselines three methods of comparison systems are biomedical summarizers freqdist biochain and chainfreq we implement these three summarizers as explained in their original papers two comparison methods are independent and term based summa and swesum one of the methods microsoft autosummarize is a commercial application the two baseline methods are lead baseline and random baseline the size of the summaries generated by all the summarizers is of the original documents the choice of as the compression rate is based on a well accepted standard that says the size of a summary should be between and of the original text in the following we give a brief description of the competitor methods freqdist is a biomedical summarization method which uses concept frequency distribution to identify important sentences it initially maps the input text to the umls concepts then it creates an empty summary frequency distribution and a source text frequency distribution model counting the concepts afterwards using an iterative sentence selection process freqdist creates a candidate summary and compares the frequency distribution of the candidate summary with the distribution of the source text the method evaluates the sentences based on how much they align the frequency distribution of the summary to the original text it manuscript march selects a sentence in each iteration and adds it to the summary such that the two frequency distributions to be aligned as closely as possible the original study has compared five similarity functions to find the best one for evaluating the similarity of frequency distributions we implement and compare freqdist method with the dice s coefficient as it has reported the highest rouge scores in the original study biochain is a biomedical summarizer based on the lexical chaining idea it extracts the umls concepts from the input document considers the semantic types as the head of chains and puts the concepts of the same semntic type in the same chain biochain selects the strong chains based on the core concepts and their frequency identifies the strong concepts of each strong chain and uses the strong concepts to score the sentences finally it extracts the high scoring sentences and generates the final summary chainfreq is a hybrid summarizer which makes use of both freqdist and biochain methods it uses biochain to identify important sentences containing strong concepts then it sends the candidate sentences to freqdist to reduce the redundancy and to select the subset of sentences which aligns the summary frequency distribution to the source text in the original study two variants of biochain have been evaluated for chainfreq the first variant uses all the concepts of strong chains to score the sentences while the second one uses the most frequent concept of each strong chain in the evaluations the first method has obtained higher rouge scores accordingly we implement the first method as a part of chainfreq for our evaluations we also implement freqdist using the dice s coefficient in addition to the above biomedical summarizers we use three domain independent comparison methods in the evaluations to assess the performance of our method against traditional term based approaches we give a description of these methods in the following summa is a summarizer which uses generic and statistical features to score the sentences of an input document the features that we use for the evaluations include the frequency of terms within the sentence the position of the sentence within the document the similarity between the sentence and the first sentence of the document and the similarity between the sentence and the title swesum is an online and multi lingual summarizer based on generic features for the evaluations we use the following set of features presence of the sentence in the first line of the text presence of numerical values in the sentence and presence of keywords in the sentence we also set the type of text feature to academic microsoft autosummarize is a feature of the microsoft word this summarizer performs based on a word frequency algorithm it assigns a score to each sentence of a document according to the frequency of words contained in the sentence microsoft word microsoft corporation manuscript march our two baselines for the evaluations are lead baseline that returns the first n sentences of the input document as the summary and random baseline that randomly selects n sentences from the document and generates a summary results experiments experiment results feature selection in this subsection we first present the results of parameterization and the preliminary experiments which specifies the best settings for the feature selection approaches then we present the results of experiments conducted to assess the impact of the coefficients and the redundancy reduction method on the performance of the bayesian summarizer for brevity reasons we only report the and r scores for the preliminary as explained in section we conduct a set of preliminary experiments to tune the parameters and find the best settings of the feature selection methods we introduce three possible threshold values for the third method which uses a ranking and filtering strategy table shows the rouge scores obtained by the bayesian summarizer using the three thresholds the summarizer obtains the highest scores when it uses the threshold we use this threshold as the optimal value in the subsequent experiments table rouge scores obtained by the bayesian summarizer using the third feature selection approach and three threshold values the best result for each rouge score is shown in bold type rouge in the fourth feature selection approach we use the helmholtz principle to identify meaningful concepts in this method there is a parameter which specifies the meaningfulness threshold fig shows the rouge scores obtained by the bayesian summarizer using the fourth feature selection method and different values of the theshold between the threshold values of and report the best scores and r we set the optimal value of this parameter to in the subsequent experiments fig shows the average number of meaningful concepts selected as classification features for the different values of the threshold in the given range manuscript march rouge meaningfulness level fig rouge scores for the bayesian summarizer using the fourth feature selection method and the different values of the meaningfulness threshold r o c s e g u o r s e r u t a e r e b m u n e g a r e v a e h t meaningfulness level fig the average number of features for the different values of the meaningfulness level in the fourth feature selection method manuscript march in the fifth feature selection approach we use frequent itemset mining to extract correlated concepts frequent itemsets and to select them as classification features the itemset mining method employs a parameter as the minimum support threshold to discover frequent itemsets fig shows the rouge scores obtained by the bayesian summarizer using the fifth feature selection method and different values of the threshold between the threshold value of reports the best scores and r we choose this value as the optimal parameter for the subsequent experiments table presents the average number of frequent itemsets selected as features for different values of the threshold in the given range rouge r o c s e g u o r minimum support threshold fig rouge scores for the bayesian summarizer using the fifth feature selection method and the different values of the minimum support threshold the impact of the coefficients we mention in section that for the majority of documents even the high frequent concepts appear in less than of the sentences of a document we examine the most frequent concept of every document in the development corpus the results show that only in nine documents there are concepts that appear in more than of the sentences of the document this means there are only nine documents containing at least one feature with a most probable value of true for the other documents the most probable value of all the features is false in average the most frequent concept of a document in the development corpus appears in of sentences in the corresponding document we also investigate these statistics for the evaluation corpus the results show that only in documents there are concepts that appear in more than of the sentences of the document for the other documents the most frequent concept of each document appears in less manuscript march than of the sentences in average the most frequent concept of a document in the evaluation corpus appears in of sentences in the corresponding document regarding these observations as noted in section the coefficients help the summarizer to discriminate between the true and false values of features leading to a more accurate sentence classification table the average number of features for different values of the minimum support threshold in the fifth feature selection method extraction of correlated features by itemset mining the average number of features the average number of features we perform a set of experiments to assess the impact of the coefficients and on the quality of the produced summaries we evaluate the bayesian summarizer using the best settings of all the five feature selection methods with and without using the coefficients table shows the rouge scores for these experiments as can be seen the summarizer reports higher scores when it utilizes the coefficients table rouge scores obtained by the bayesian summarizer using the five different feature selection approaches with and without using the coefficients first approach second approach third approach fourth approach fifth approach rouge with the coefficients without the coefficients with the coefficients without the coefficients manuscript march the impact of the redundancy reduction method performing another set of preliminary experiments we assess the impact of the redundancy reduction method on the quality of the produced summaries table gives the rouge scores for the bayesian summarizer when the five different feature selection approaches are used with and without using the redundancy reduction method we compare the scores assigned to the bayesian summarizer in two cases of using and not using the redundancy reduction method under different compression rates table presents the percentage of improvement obtained using the redundancy reduction method for different compression rates as can be seen the percentage of improvement for the smaller compression rates is higher than for the greater rates table rouge scores obtained by the bayesian summarizer using the five different feature selection approaches with and without using the redundancy reduction method first approach second approach third approach fourth approach fifth approach rouge redundancy reduction yes redundancy reduction no redundancy reduction yes redundancy reduction no table the percentage of improvement achieved by the bayesian summarizer using the redundancy reduction method the percentages have been computed for scores using the five different feature selection methods under different compression rates of and compression rate first approach second approach third approach fourth approach fifth approach manuscript march table the average number of concepts covered in the summaries produced by the bayesian summarizer using the five different feature selection approaches with and without using the redundancy reduction method with redundancy reduction without redundancy reduction first approach second approach third approach fourth approach fifth approach we also assess the average number of concepts covered in the produced summaries by the bayesian summarizer with and without using the redundancy reduction method table shows these results when the summarizer uses the five different feature selection methods as the results show the summaries cover more concepts in average when the summarizer benefits from the redundancy reduction method results comparing the bayesian summarizer with the other methods we evaluate the performance of our method for biomedical text summarization table shows the rouge scores obtained by the bayesian summarizer and the comparison methods we evaluate the bayesian summarizer all the five feature selection approaches in order to test the statistical significance of the results we use a wilcoxon signed rank test with a confidence interval according to the results when the bayesian summarizer uses the fourth and fifth feature selection methods it significantly outperforms all the comparison methods in terms of all the reported rouge scores p using the third feature selection method the bayesian summarizer significantly performs better than biochain the domain independent and the baseline methods in terms of all the rouge scores p in comparison with chainfreq and freqdist the results report a significant improvement for all the scores except for the r p when we run the bayesian summarizer with the second feature selection method it significantly performs better than the domain independent and baseline summarizers p among the biomedical summarization methods its improvement is significant for all the rouge scores compared to biochain and its improvement is significant only for score with respect to freqdist p although it obtains better scores than chainfreq its improvement is not significant for all the scores p eventually the bayesian summarizer significantly performs better than the domain independent competitors and baselines in terms of all the scores when it uses the first feature selection method p compared to biochain it improves all the scores but the improvement is only significant for the manuscript march table gives the average minimum and maximum number of features selected by the five feature selection methods for documents in the evaluation corpus table rouge scores obtained by the bayesian summarizer and the comparison methods the best result for each rouge score is shown in bold type the summarizers are sorted based on the decreasing order of their scores bayesian fifth approach bayesian fourth approach rouge rouge bayesian third approach uv bayesian second approach bayesian first approach table the average minimum and maximum number of features for the documents of the evaluation corpus using the five different feature selection approaches average minimum maximum bayesian first approach bayesian second approach bayesian third approach uv bayesian fourth approach bayesian fifth approach chainfreq freqdist biochain summa swesum lead baseline autosummarize random baseline manuscript march discussion selection and parameterization as reported in table when the summarizer uses the third feature selection method with the threshold is the scores are higher than for the other two threshold values for a given document the value of the always less than for the other two thresholds and the value of the preliminary experiments evaluation corpus the average number of selected features for a document using is always greater than for the other ones for the thresholds the threshold value the number of selected features is relatively high in this case only some of the features is equal to and respectively this shows that when we use the and as indicate to essential concepts and the summarizer can be misled by numerous unimportant features on the other hand when we use the this reduction in the number of features helps the summarizer to decide more accurately considering the the number of selected features decreases to almost less than one third and features which point to important concepts indeed as fig shows when the summarizer uses the fourth feature selection method the best scores are reported for the both meaningfulness level of and in this case the average number of features is as showed in fig for the threshold values greater than zero the average number of features falls to nearly this rapid decrease happens because for the majority of concepts the meaningfulness value is zero as the number of features decreases rapidly the quality of summarization also decreases because many features which could help the summarizer to perform more accurately are no longer available the results show that the meaningfulness threshold of discards many features which can be considered as redundant ones when we assign threshold values smaller than redundant features decrease the performance of the summarizer there are other measures such as inverse document frequency idf inverse sentence frequency isf and inverse term frequency itf that are widely adopted in text mining research such measures may seem to be functionally similar to the meaningfulness measure defined by the helmholtz principle however by studying the theoretical and practical functions of the measures we found the meaningfulness more useful than others for selecting important concepts in the bayesian summarizer for several reasons first the idf isf and itf measures are generally defined for a corpus of documents but the bayesian summarizer analyzes one document at a time and the definition of such measures may not make sense for single documents second if we define the isf or itf weights in a document concepts appearing in fewer sentences are assigned more discriminative power such weighting scheme could not be useful in our context on the other hand the meaningfulness measure has been adopted successfully in single document text analysis furthermore as the example in fig shows the weights assigned by the helmholtz principle do not have any obvious relation to the frequency of concepts according to the formulas in section the meaningfulness value depends on manuscript march multiple factors such as the frequency of concepts within each paragraph and within the whole document the length of each paragraph and the length of the document as fig shows when the summarizer uses the fifth feature selection method the best scores are reported for the minimum support threshold of for this value the average number of features itemsets is for the smaller thresholds which produce more number of features the rouge scores decrease slightly this shows that more number of features could not significantly reduce the accuracy of the summarizer in this feature selection method although the numerous features mislead the summarizer to some extent it still benefits from the knowledge about the correlated concepts provided by the itemsets when the threshold tends to be greater than the average number of features decreases and the performance of the summarizer also decreases particularly when the threshold is greater than the average number of features drops to less than and the rouge scores are reduced considerably this shows that when the number of features decreases in fifth feature selection method the knowledge of the summarizer about important and correlated concepts is inadequate it decides according to a limited number of high supporting itemsets whereas there are a lot of useful itemsets discarded by an extreme threshold coefficients and as can be seen in table for all the five feature selection methods the bayesian summarizer obtains better rouge scores when it uses the coefficients since for the majority of documents even the most frequent features do not appear in more than of the sentences of a document the most probable value for almost every feature is false when the method does not use the coefficients it decides based on the most probable values of the features in this case the summarizer needs additional knowledge about the features and their importance to decide more accurately using the coefficients when the value of a feature is true in a sentence the probability of selecting the sentence for the summary increases in proportion to the occurrence of the feature moreover the probability of not selecting the sentence decreases on the other hand when the value of a feature is false in a sentence the probability of selecting the sentence for the summary decreases in proportion to the occurrence of the feature moreover the probability of not selecting the sentence increases adopting this strategy the summarizer can discriminate between the presence and absence of important concepts as a result the summarizer performs better and reports higher scores redundancy reduction method as table shows when the redundancy reduction method takes part in the summarization method the summarizer reports better scores with the help of the redundancy reduction method the summarizer gives sentences containing low frequency concepts a higher chance to be included in the summary therefore the summary can cover more number of concepts while it still conveys important concepts and subtopics when summaries cover more relevant information their informativeness increases as a results the summarizer obtains higher scores manuscript march table suggests that the percentage of improvement for the smaller compression rates is higher than for the greater rates this happens because for smaller compression rates fewer sentences must be selected for the summary hence when we use the redundancy reduction method and the summarizer selects new sentences containing new information the scores report a more impressive improvement on the other hand for greater compression rates the summarizer selects more sentences in this way the summary automatically includes new information the summary presents the new information along with some potentially redundant sentences which were selected earlier or would be selected later the redundancy reduction method may discard these redundant sentences and select new relevant information therefore the performance of the summarizer may be improved however this improvement for greater compression rates is less than for smaller rates because new relevant information is automatically included in the summary by the greater compression rates as table shows using the redundancy method the summaries cover more concepts in average in addition as can be seen in table the usage of the redundancy reduction method leads to an increase in the scores for all the feature selection methods however when we compare each pair of feature selection methods the greater average number of concepts covered in the produced summaries does not necessarily lead to better summarization performance for example in both cases of using and not using the redundancy reduction method the average number of concepts covered in the summaries for the fifth feature selection method is less than the average for the first second and fourth methods nevertheless the summarizer obtains the best rouge scores using the fifth method this suggests that with the use of an appropriate feature selection method the summaries convey more informative content even if they cover fewer concepts these results demonstrate that both the redundancy reduction method and an appropriate feature selection method are essential to enhance the performance of the summarizer the lack of each one has a negative impact on the quality of produced summaries with other summarizers as table shows when the bayesian summarizer utilizes the first feature selection method it performs better than the domain independent competitors this suggests that using concepts as classification features in our method can be a better approach compared to the summarizers which employ word frequency methods positional features and term similarity features our summarizer performs slightly worse than the two biomedical summarizers freqdist and chainfreq when it considers all extracted concepts as features in this case it seem that potentially redundant and unrelated concepts negatively affect the quality of produced summaries comparing the results of the first and second feature selection methods we observe that when generic and potentially redundant concepts are discarded the summarizer can decide more accurately and obtains higher scores looking at the number of features selected by the first and second methods in table it seems that almost a half of concepts extracted from a document can be considered as unnecessary removing redundant manuscript march concepts from classification features we observe a slight increase in the performance of the summarizer the results of the second feature selection method show that with respect to biochain considering the distribution of non generic concepts along with the redundancy reduction method improves the performance of biomedical summarization with respect to freqdist and chainfreq we still need to make more refinement in our feature selection strategy to increase the quality of produced summaries the third feature selection method employs a ranking and filtering strategy as the results show the use of all extracted concepts to constructing a frequency distribution model freqdist can be outperformed by the bayesian modeling in combination with an optimized feature selection based on the filtering method although chainfreq does not use all extracted concepts and utilizes biochain as a filtering method for its hybrid method the bayesian summarizer obtains relatively better scores using an appropriate threshold for filtering the features when we employ the third method for feature selection the maximum minimum and average number of features for a document in the evaluation is and respectively as the numbers in table show the third method considerably reduces the average number of features compared to the second method from to this reduction helps the summarizer to decide more accurately regarding the results of the second method this filtering strategy leads to a slight increase in the performance of summarization the results of the fourth feature selection method show that the meaningfulness is a better measure than the frequency for feature selection in the bayesian summarizer as table shows the fourth method produces a large number of features compared to the third method for the average number of features however the fourth method yields better summarization performance this suggests that the meaningfulness can be considered as a more efficient measure to remove unimportant concepts in the bayesian summarizer this measure provides the summarizer with a set of indeed important concepts to decide more optimally however some of concepts selected as features may not be considered as a frequent concept when the summarizer utilizes this measure it still makes use of the frequency of concepts in the form of the coefficients in fact the summarizer combines information about the meaningfulness measure and the frequency of concepts using this approach the bayesian summarizer obtains higher scores than all the biomedical competitors as table shows the bayesian summarizer reports the highest scores when it utilizes the fifth feature selection method and uses frequent itemsets as features using this method the summarizer implicitly takes into account correlations and appearing patterns existing among concepts the results show that this feature selection strategy and the bayesian modeling yield better summarization quality than the biomedical summarizers relying on the frequency of single concepts according to the results the fifth feature selection method performs slightly better than the fourth method this suggests that the bayesian summarizer can utilize either information about correlated concepts or the meaningfulness measure as two useful feature selection approaches to improve the performance of summarization comparing the results of different feature selection manuscript march methods we observe that frequent itemsets can be more useful than the frequency of single concepts in the bayesian summarizer according to the results our bayesian summarizer significantly outperforms the domain independent comparison methods summa swesum and autosummarize in biomedical text summarization these methods utilize statistical similarity based and word frequency features for sentence selection the results show that these term based methods can not be considered as useful summarizers for biomedical text on the other hand using domain knowledge and efficient feature selection methods the bayesian summarizer can perform more efficiently than the comparison methods fig the frequency distribution of concepts within the full text papers in the evaluation corpus fig the frequency distribution of concepts within the abstracts in the evaluation corpus manuscript march of the basic assumption as we explained in section the bayesian summarizer estimates the posterior probability of selecting and not selecting sentences for the summary based on the prior probability of concepts within the input document in other words the summarizer assumes that the distribution of important concepts within the summary should be similar to the original text this assumption has been justified by reeve et al they selected a corpus of biomedical full text papers and used the abstracts of the papers as the ideal summaries they constructed two frequency distribution models from the concepts of the full text papers and the abstracts and showed that these two frequency distribution models follow zipfian distribution regarding this observation they suggested that a full text paper and a version of its ideal summary abstract have the same frequency distribution form we perform a similar experiment using a larger corpus we extract concepts from the full texts and the abstracts of our evaluation corpus containing biomedical papers fig shows the frequency distribution of concepts within the full text papers and fig shows the frequency distribution of concepts within the abstracts as can be observed from fig and fig both the texts and the abstracts considered as the ideal summaries follow zipfian distribution this observation justifies the basic assumption of our proposed biomedical text summarization method that uses the distribution of concepts within the document to estimate the posterior probability of the sentences for inclusion in the summary conclusion in this paper we propose a biomedical text summarization method using a bayesian classification approach the method classifies the sentences of the input document as summary and non summary according to the distribution of important concepts within the text we introduce different feature selection approaches to identify the important concepts of the document and using them as classification features the summarizer uses two coefficients in probability estimation to discriminate between the presence and absence of important concepts it also employs a simple redundancy reduction method to reduce the potential redundancy in the summary conducting a set of preliminary experiments on a development corpus containing biomedical papers we tune the parameters of the system and assess the efficiency of the coefficients and the redundancy reduction method the results show that the coefficients and the redundancy reduction method have a positive impact on the quality of produced summaries leading to an improvement in the performance of the summarizer we evaluate the performance of the bayesian summarization method in comparison with the other biomedical summarizers relying on the frequency of concepts domain independent summarizers and baseline methods using an evaluation of biomedical papers the results show that when the bayesian summarizer utilizes the meaningfulness measure rather than the frequency of single concepts for selecting manuscript march features it outperforms the other summarizers moreover when the summarizer employs itemset mining and uses correlated concepts as classification features it significantly performs better than the comparison methods summing up the results we can draw the following conclusions that answer to the questions raised in section a bayesian classification method can be utilized for the probability distribution modeling of concept based biomedical text summarization an efficient feature selection method is required to enhance the accuracy the summarizer should not consider all the extracted concepts from the input document there many redundant concepts which may have a negative impact on the accuracy of the model and can be discarded of the classification method by the summarizer the meaningfulness measure defined by the helmholtz principle can be a useful criterion rather than the frequency to identify important concepts and use them as classification features using itemset mining to discover correlated concepts and incorporating these correlations into the feature selection phase provide the summarizer with a more accurate model leading to an increase in the obtained scores in our future research we intend to concentrate on extending our bayesian biomedical summarizer to deal with multi document and query focused summarization to do so a more complicated redundancy reduction method should be studied it seems that there is much more room for studying the helmholtz principle from the gestalt theory in the context of concept based summarization balinsky et al have modeled document sentences as a small world network using the helmholtz principle and investigated some applications such as text summarization future work can involve exploring this type of modeling for concept based biomedical text summarization the study of using other discriminative classifiers in this type of summarization can be considered as another potential topic for future research conflict of interest the authors declare that they have no conflict of interest mishra r bian j fiszman m weir cr jonnalagadda s mostafa j al text summarization in the biomedical domain a systematic review of recent research journal of biomedical informatics afantenos s karkaletsis v stamatopoulos summarization from medical documents a survey artificial intelligence in medicine fleuren ww alkema application of text mining in the biomedical domain methods reeve lh han h brooks ad the use of domain specific concepts in biomedical text summarization information processing management manuscript march references plaza l daz a gervs a semantic graph based approach to biomedical summarisation artificial intelligence in medicine chen p verma a query based medical information summarization system using ontology knowledge ieee symposium on computer based medical systems ieee plaza l carrillo de albornoz evaluating the use of different positional strategies for sentence selection in biomedical literature summarization bmc bioinformatics menndez hd plaza l camacho combining graph connectivity and genetic clustering to improve biomedical summarization ieee congress on evolutionary computation cec ieee reeve l han h brooks ad biochain lexical chaining methods for biomedical text summarization proceedings of the acm symposium on applied computing acm reeve lh han h nagori sv yang jc schwimmer ta brooks ad concept frequency distribution in biomedical text summarization proceedings of the acm international conference on information and knowledge management acm nelson sj powell t humphreys the unified medical language system umls project encyclopedia of library and information science balinsky aa balinsky hy simske sj on helmholtz principle for documents processing proceedings of the acm symposium on document engineering acm agrawal r imieliski t swami mining association rules between sets of items in large databases acm sigmod record mitchell generative and discriminative classifiers naive bayes and logistic regression manuscript available at cs cm tom newchapters html lin c rouge a package for automatic evaluation of summaries text summarization branches out proceedings of the gupta v lehal gs a survey of text summarization extractive techniques journal of emerging technologies in web intelligence alguliev rm aliguliyev rm hajirahimova ms mehdiyev ca mcmr maximum coverage and minimum redundant text summarization model expert systems with applications gambhir m gupta recent automatic text summarization techniques a survey artificial intelligence review workman te fiszman m hurdle jf text summarization as a decision support aid bmc medical informatics and decision making zhang h fiszman m shin d miller cm rosemblat g rindflesch tc degree centrality for semantic abstraction summarization of therapeutic studies journal of biomedical informatics fiszman m demner fushman d kilicoglu h rindflesch tc automatic summarization of medline citations for evidence based medical treatment a topic oriented evaluation journal of biomedical informatics kilicoglu summarizing drug information in medline citations fiszman m rindflesch tc kilicoglu abstraction summarization for managing the biomedical research literature proceedings of the hlt naacl workshop on computational lexical semantics association for computational linguistics zhang h fiszman m shin d wilkowski b rindflesch tc clustering cliques for graph based summarization of the biomedical research literature bmc bioinformatics moen h peltonen l m heimonen j airola a pahikkala t salakoski t al comparison of automatic summarisation methods for clinical free text notes artificial intelligence in medicine del fiol g mostafa j pu d medlin r slager s jonnalagadda sr al formative evaluation of a patient specific clinical knowledge summarization tool international journal of medical informatics morid ma fiszman m raja k jonnalagadda sr del fiol classification of clinically useful sentences in clinical evidence resources journal of biomedical informatics pivovarov r elhadad automated methods for the summarization of electronic health records journal of the american medical informatics association plaza comparing different knowledge sources for the automatic summarization of biomedical literature journal of biomedical informatics menndez hd plaza l camacho a genetic graph based clustering approach to biomedical summarization proceedings of the international conference on web intelligence mining and semantics acm sarkar using domain knowledge for text summarization in medical domain international journal of recent trends in engineering sarkar k nasipuri m ghose using machine learning for medical document summarization international journal of database theory and application sarker a moll d paris extractive summarisation of medical documents using domain knowledge and corpus statistics the australasian medical journal manuscript march barzilay r elhadad using lexical chains for text summarization advances in automatic text summarization kupiec j pedersen j chen a trainable document summarizer proceedings of the annual international acm sigir conference on research and development in information retrieval acm daum iii h marcu bayesian query focused summarization proceedings of the international conference on computational linguistics and the annual meeting of the association for computational linguistics association for computational linguistics wang d zhu s li t gong multi document summarization using sentence based topic models proceedings of the acl ijcnlp conference short papers association for computational linguistics larose dt larose cd data mining and predictive analytics john wiley sons national library of medicine umls metathesaurus fact sheet national library of medicine umls specialist lexicon fact sheet national library of medicine umls semantic network fact sheet aronson ar effective mapping of biomedical text to the umls metathesaurus the metamap program proceedings of the amia symposium american medical informatics association plaza l stevenson m daz resolving ambiguity in biomedical text to improve summarization information processing management chandrashekar g sahin a survey on feature selection methods computers electrical engineering forman an extensive empirical study of feature selection metrics for text classification journal of machine learning research balinsky a balinsky h simske on the helmholtz principle for data mining hewlett packard development company lp balinsky a balinsky h simske rapid change detection and text mining proceedings of the conference on mathematics in defence ima defence academy balinsky h balinsky a simske sj automatic text summarization and small world networks proceedings of the acm symposium on document engineering acm tutkan m ganiz mc akyoku helmholtz principle based supervised and unsupervised feature selection methods for text mining information processing management agrawal r mannila h srikant r toivonen h verkamo ai fast discovery of association rules advances in knowledge discovery and data mining ferreira r souza cabral l freitas f lins rd frana silva g simske sj al a multi document summarization system based on statistics and linguistic treatment expert systems with applications carbonell j goldstein the use of mmr diversity based reranking for reordering documents and producing summaries proceedings of the annual international acm sigir conference on research and development in information retrieval acm lin c looking for a few good metrics automatic summarization evaluation how many samples are enough mani summarization evaluation an overview saggion summa a robust and adaptable summarization tool traitement automatique langues swesum automatic text summarizer mitkov the oxford handbook of computational linguistics oxford university press blake a comparison of document sentence and term event spaces proceedings of the international conference on computational linguistics and the annual meeting of the association for computational linguistics association for computational linguistics balinsky h balinsky a simske document sentences as a small world systems man and cybernetics smc ieee international conference on ieee manuscript march
