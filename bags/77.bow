g u l c s c v v x r tsinghuauniversity beijing chinaabstractneuralmodelshaverecentlybeenusedintextsummarizationincludingheadlinegener ation themodelcanbetrainedusingasetofdocument headlinepairs themodeldoesnotexplicitlyconsidertopicalsimilaritiesanddifferencesofdocuments wesuggesttocategorizingdocumentsintovarioustopicssothatdocumentswithinthesametopicaresim ilarincontentandsharesimilarsummariza tionpatterns takingadvantageoftopicinfor mationofdocuments weproposetopicsen sitiveneuralheadlinegenerationmodel ourmodelcangeneratemoreaccuratesummariesguidedbydocumenttopics wetestourmodelonlcstsdataset andexperimentsshowthatourmethodoutperformsotherbaselinesoneachtopicandachievesthestate artperfor mance includingheadlinegeneration isanimportanttaskinnaturallanguageprocessing itistypicallychallengingtocapturethecoreinfor mationofadocumentandcreateaninformativebutbriefsummaryofthedocument mostexistingtextsummarizationapproachescanbedividedintotwocategories extractiveandgener ative umentandreorderthemintoacompactsummary duetothelimitationofvocabularyandsentenceindicatesequalcontribution edu edu cnstructure itisextremelydifcultforextractivemod elstogeneratecoherentandconcisesummaries generativemodels ontheotherhand aimatcom prehendingadocumentandgeneratingthesummarynotnecessarilyhavingappearedintheoriginaldoc ument recentyearshavewitnessedthedevelopmentofsequence andthengenerateanoutputsequenceaccordingly theadvantageofneu ralmodelsisthatthesemodelslearnasemanticmappingdirectlyaccordingtopairsofdocument headlinesequenceswithoutdesigninghand craftedfeatures becausethesemodelscanexiblymodeldocumentsemanticsfrominternalwordsequenceswithinthedocument muchexternalinformationaboutdocumentsmayalsoplayimpor tantrolesfortextsummarization forexample doc umentsusuallygroupintovarioustopics andthedocumentswithinacertaintopicmayexhibitspe cicsummarizationpatterns forexample adocu mentabouteconomyisusuallysummarizedinclud locationandplaceoftheevent inthispaper weproposetoincorporatetopicin formationofdocumentsintoneuralmodelsfortextsummarizationandproposetopic morespecically andintroducethetopiclabelsinneuralmodelstobuilduniqueencodersanddecodersforeachtopicrespec tively inthisway topicnhgcaneffectivelyiden tifythecorrespondingcrucialpartsinadocumentguidedbyitstopicinformation andareexpectedtogeneratewell focusedheadlines inthispaper weevaluateourmodelonalarge experimentresultsshowthatourmodelsignicantlyoutperformsotherbaselinesystems itconsistentlyperformsthebestoneachindividualtopic whichprovesthestatisticalsignicanceandrobustnessoftopicnhg basedonwhichagrudecodergeneratesaconciseheadline thet whencalculatinght itusesupdategateztandresetgaterttoimprovetheperformanceonlongsequences thegatesarecom putedaszt thenitcomputescandidateoutputhtandnaloutputhtasht decodernhgincludesanencodertoencodeinputtextxintoafeaturevectorvandadecodertogeneratehead lineybasedonv theattentionmechanismcanim itassignsdifferentfeaturevectorsvtfordifferentstepsofthedecoder fig themodelgeneratestheoutputsequencefollowingamarkovprocess nhgwithattentionmechanism t natedtogethasht thefeaturevectorvistheaverageofoutputh mally fort thunit attentionthefeaturevectorvremainsidenti theattentionmechanismdeter minesdifferentfeaturevtfort forshorttextx summaryyandtopiclabell l tionfeatureforadocument arethedirichletpriorsontheper documenttopicdistributionandper topicworddis tributionrespectively forsimplicity weassigneachinputsequencewithexactlyonetopicasl d inourmodel thetopiclabeloftheinputshorttextwillaf fectweightmatricesinencoder decoderandatten tionlayer forktopics kdifferentencoders decodersandattentionlay ers withkdifferentencoder wegeneratekrepre thenweselectarepresentationaccordingtothetopiclabelloftheinputsequence k thetrainingofourmodelistime consuming foreachtopic soweusetheparameterstrainedintheconven tionalnhgmodeltoinitializeourmodel specic usinginitializationalsomakesourmodelmoregeneralthateachpartofourmodelwillrstlybetrainedbyalargesetofgeneraltextsum mariesandlaterbetrainedtogeneratetopic specicones icnhg dia anteethequalityofdata iiandpart iiihasahuman labeledscore reectingtherelevanceofthesummary datainpart anddatainpart inourexperiment weusepart iasourtrainingdata andweusepart notethatwetakechinesecharactersasinputtoavoiderrorscausedbywordsegmentation wemanuallymarkthesetopicsasjob economy accident politicsandtechnology topickeywordsjobundergraduate graduate pho tographer researchereconomyrmb usd realty investor company manager ipoaccidentsuspect police court idcard bus taxi highwaypoliticsstatedepartment authority civilservants urbanizationtechnologyinternet consumer smartphone e business topicandkeywords topicpart ipart iipart samplesintopic ta fevaluationonthesemod els byintroducingtopics oneachtopic wecompareourmodelagainstthebaseline rouge rouge topicnhg fig inthisexample topic nhgperformsmuchbetterthanthebaseline thebaselineconsiderstherstsentenceoftheinputasitsmainpoint inmostcases thisassumptionisprobablytrue sothebaselinelearnsageneralreg ularityinsummarizationthattherstsentenceisimportant therstsentenceofaweiboconcerningpoliticsusuallytalksaboutaconference whilethecontentoftheconferenceinthefollowingsentencesismoreimportant weibo text afternoon meeting beijing s session national people s congress held press conference focusing people s house problem beijing s economical departments average square meter history solving waiting families new economical departments built human notation economical departments history year baseline meeting beijing s session national people s congress held press conference topicnhg economical department beijing history comparingtopicnhgwithbaseline weproposetopic sensitiveneuralheadlinegeneration theexperimentsprovethattopicisanimportantfeatureinheadlinegenerationtasks ourmodelisrelativelysimple itshighcostintrainingpreventsitfromhandlingmoretopics inthefuture dleplentyoftopics referencesdzmitrybahdanau kyunghyuncho andyoshuaben gio neuralmachinetranslationbyjointlylearningtoalignandtranslate fangzezhubaotianhu qingcaichen lcsts alargescalechineseshorttextsummarizationdataset davidmblei andrewyng andmichaelijordan latentdirichletallocation thejournalofma junyoungchung caglargulcehre kyunghyuncho andyoshuabengio empiricalevaluationofgatedrecurrentneuralnetworksonsequencemodel ing newmethodsinautomaticextracting jiataogu zhengdonglu hangli andvictorokli incorporatingcopyingmechanisminsequence sequencelearning quocvleandtomasmikolov distributedrepre sentationsofsentencesanddocuments tomasmikolov martinkaraat lukasburget jancer andsanjeevkhudanpur recurrentneuralnetworkbasedlanguagemodel ininter speech alexandermrush sumitchopra andjasonwe ston aneuralattentionmodelforab stractivesentencesummarization ilyasutskever oriolvinyals andquocvle sequencetosequencelearningwithneuralnetworks inadvancesinneuralinformationprocessingsystems
