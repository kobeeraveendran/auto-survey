autoencoder assistant supervisor improving text representation chinese social media text summarization shuming junyang houfeng key lab computational linguistics school eecs peking university learning lab beijing institute big data research peking university foreign languages peking university shumingma xusun linjunyang edu abstract current abstractive text marization models based sequence sequence model source content social media long noisy difcult learn accurate semantic representation compared source content notated summary short shares work ing source content supervise learning tation source content summary implementation regard summary autoencoder assistant pervisor following previous work evaluate model popular chinese social media dataset tal results model achieves state art performances benchmark dataset introduction text summarization produce brief summary main ideas text unlike extractive text summarization radev woodsend lapata cheng lapata lects words word phrases source texts summary abstractive text summarization learns semantic representation generate human like summaries recently models abstractive text summarization based sequence sequence model encodes source texts semantic representation encoder generates summaries representation decoder contents social media long contain errors come spelling mistakes informal expressions grammatical mistakes baldwin large errors contents cause great difculties text summarization rnn based difcult compress long sequence accurate representation gradient vanishing exploding problem compared source content easier encode representations summaries short manually selected source content summary share points possible supervise learning semantic representation source content summary paper regard summary coder assistant supervisor train autoencoder inputs structs summaries obtain better sentation generate summaries supervise internal representation autoencoder minimizing tance representations finally use adversarial learning enhance sion following previous work evaluate proposed model nese social media dataset experimental results model outperforms state art baseline models specically model outperforms baseline score rouge proposed model introduce proposed model detail section notation code available lancopku superae given summarization dataset consists data samples ith data sample training stage function measures distance tunable parameter balance loss supervision parts loss ber hidden unit limit magnitude distance function set based performance validation set distance representations written adversarial learning enhance supervision versarial learning approach shown use xed hyper parameter weight measure strength supervision toencoder case source content summary high relevance strength supervision higher source content summary low relevance strength lower order determine strength supervision namically introduce adversarial learning specically regard representation autoencoder gold representation sequence sequence fake resentation model trained discriminate gold fake representations called discriminator discriminator tries identify representations trary supervision minimizes tance representations makes ilar tries prevent discriminator ing correct predictions way criminator distinguish representations means source content summary low relevance strength supervision decreased discriminator fails distinguish strength supervision proved implementation adversarial learning discriminator objective function written log log probability criminator identies vector gold resentation probability vector identied fake sentation parameters test stage figure overview model model consists sequence sequence model autoencoder model training stage use autoencoder supervise sequence sequence model test stage use sequence sequence model generate maries tains source content summary number source words ber summary words training stage train model generate summary given source content test stage model decodes predicted summary given source content supervision autoencoder figure shows architecture model training stage source content encoder presses input contents internal sentation lstm encoder time summary encoder compresses ence summary representation fed lstm decoder erate summary finally semantic tation source content supervised summary implement supervision minimizing distance semantic representations term loss function written source content encodersummary encodersummary decodersupervisesource content encodersummary inator minimizing discriminator tive train parameters nator rest parameters remains changed supervision objective criminator written log log pairs pairs pairs iii text summary pairs iii manually annotated relevant scores ranged reserve pairs scores leaving pairs pairs iii following previous work use training set validation set iii test set minimizing supervision objective update parameters encoders evaluation metric loss function training parts objective functions optimize models rst cross entropy losses sequence sequence autoencoder lae second loss supervision written equation sarial learning equation tion sum parts nal loss function optimize use adam kingma timization method train model hyper parameters adam optimizer set learning rate momentum eters respectively clip gradients pascanu maximum norm experiments following previous work evaluate model popular chinese social media dataset rst introduce datasets evaluation metrics experimental tails compare model state art systems dataset large scale chinese social media text marization dataset lcsts constructed dataset consists text summary pairs constructed famous chinese social media website called sina weibo split parts com evaluation metric rouge score lin hovy popular tion evaluation metrics compare ically produced summary reference maries computing overlapping lexical units including unigram bigram trigram longest common subsequence lcs following previous work rush use unigram gram rouge lcs evaluation metrics reported experimental results experimental details vocabularies extracted training sets source contents summaries share vocabularies order alleviate risk word segmentation mistakes split chinese sentences characters prune vocabulary size covers common characters tune hyper parameters based rouge scores validation sets set word embedding size hidden size number lstm layers batch size use dropout srivastava dataset following ous work implement beam search set beam size baselines compare model following art baselines rnn rnn cont sequence sequence baseline gru encoder coder provided ference rnn context attention mechanism rnn rnn dist chen based neural model attention models rnn rnn cont rnn copynet rnn impl superae paper adversarial learning table comparison state art models lcsts test set note rouge spectively models sufx table word based rest models character based mechanism focuses different parts source content copynet incorporates copy mechanism allow parts erated summary copied source content srb sequence sequence based neural model improving semantic relevance input text output summary drgd deep recurrent generative decoder model combining coder variational autoencoder implementation sequence sequence model tion mechanism mental setting model fair son results purpose simplicity denote vision autoencoder model superae report rouge score model baseline models test sets table summarizes results superae model baselines rst compare model baseline shows models superae class class table accuracy sentiment tion amazon dataset train inputs internal representation provided sequence sequence model outputs predicted label compute class class accuracy predicted labels evaluate quality text representation superae model large improvement baseline rouge demonstrates ciency model compare model recent summarization systems evaluated training set test sets results directly reported referred articles shows superae outperforms models relative gain rouge perform ablation study removing adversarial learning component order contribution shows adversarial learning improves performance rouge summarization examples model shown table seqseq model captures wrong meaning source content produces summary china united airlines exploded airport superae model captures correct points erated summary close meaning ence summary analysis text representation want analyze internal text resentation improved superae model text representation abstractive hard evaluate translate representation sentiment score sentiment classier evaluate quality representation means sentiment accuracy perform experiments amazon fine foods reviews corpus mcauley leskovec amazon dataset contains users ing labels summary reviews making possible train classier predict sentiment labels model generate summaries train superae model source night people caught smoke ight china united airlines chendu beijing later ight porarily landed taiyuan airport sengers asked security check nied captain led collision tween crew passengers reference people smoked ight led collision crew passengers china united airlines exploded airport leaving people dead superae people smoked ight chendu beijing led collision crew passengers table summarization example model compared reference model text summary pairs convergence transfer encoders sentiment classier train classier xing parameters encoders classier simple feedforward neural network maps representation label tribution finally compute accuracy predicted class labels class labels shown table model achieves accuracy class class respectively superae model forms baselines large margin propose generator pointer model coder able generate words source texts solved issue rating copying mechanism allowing parts summaries copied source contents discuss problem incorporate pointer generator model coverage mechanism build large corpus chinese social media short text summarization benchmark datasets chen introduce tion based neural model forces tion mechanism focus difference parts source inputs propose neural model improve semantic relevance source contents summaries work related sequence sequence model cho toencoder model bengio liou sequence sequence model successful generative neural model widely applied machine translation sutskever jean luong text summarization rush chopra nallapati ral language processing tasks autoencoder gio articial neural network unsupervised learning efcient representation neural attention model rst proposed danau conclusion propose novel model coder supervisor sequence sequence model learn better internal representation abstractive summarization adversarial learning approach introduced improve supervision autoencoder tal results model outperforms sequence sequence baseline large margin achieves state art performances chinese social media dataset related work acknowledgements rush rst propose abstractive based summarization model uses tive cnn encoder compress texts neural network language model generate summaries chopra explore recurrent ture abstractive summarization deal vocabulary problem nallapati work supported national natural ence foundation china national high technology research development program china national gram thousand young talents program sun corresponding author paper references dzmitry bahdanau kyunghyun cho yoshua neural machine translation corr bengio jointly learning align translate timothy baldwin paul cook marco lui andrew mackinlay wang noisy cial media text diffrnt social media sources sixth international joint conference ral language processing ijcnlp nagoya japan october pages yoshua bengio learning deep architectures foundations trends machine learning qian chen xiaodan zhu zhenhua ling wei hui jiang distraction based neural networks modeling documents proceedings international joint conference articial gence ijcai new york aaai jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings annual meeting sociation computational linguistics acl august berlin germany volume long papers kyunghyun cho bart van merrienboer aglar gulcehre dzmitry bahdanau fethi bougares ger schwenk yoshua bengio learning phrase representations rnn encoder decoder proceedings statistical machine translation conference empirical methods natural language processing emnlp pages sumit chopra michael auli alexander rush abstractive sentence summarization naacl hlt tentive recurrent neural networks conference north american chapter association computational guistics human language technologies pages jiatao zhengdong hang victor incorporating copying mechanism proceedings sequence sequence learning annual meeting association putational linguistics acl baotian qingcai chen fangze zhu sts large scale chinese short text summarization dataset proceedings conference empirical methods natural language ing emnlp lisbon portugal september pages diederik kingma jimmy adam corr method stochastic optimization jiwei minh thang luong dan jurafsky hierarchical neural autoencoder paragraphs documents proceedings annual meeting association computational guistics international joint conference natural language processing asian eration natural language processing acl july beijing china volume long papers pages piji wai lam lidong bing zihao wang deep recurrent generative decoder stractive text summarization proceedings conference empirical methods natural language processing emnlp copenhagen denmark september pages chin yew lin eduard hovy matic evaluation summaries gram occurrence statistics human language ogy conference north american chapter association computational linguistics naacl cheng yuan liou wei chen cheng jiun wei liou daw ran liou autoencoder words neurocomputing cheng yuan liou jau chi huang wen chie yang modeling word perception elman network neurocomputing thang luong hieu pham christopher ning effective approaches attention based proceedings neural machine translation conference empirical methods natural language processing emnlp pages shuming sun wei sujian wenjie xuancheng ren query output erating words querying distributed word naacl sentations paraphrase generation shuming sun jingjing houfeng wang wenjie improving semantic relevance sequence sequence learning nese social media text summarization ings annual meeting association computational linguistics acl ver canada july august volume short papers pages sebastien jean kyunghyun cho roland memisevic yoshua bengio large vocabulary neural machine translation proceedings annual meeting sociation computational linguistics acl pages julian john mcauley jure leskovec amateurs connoisseurs modeling evolution user expertise online reviews ternational world wide web conference www rio janeiro brazil pages austin texas usa november pages kristian woodsend mirella lapata matic generation story highlights acl proceedings annual meeting sociation computational linguistics pages jingjing sun xuancheng ren junyang lin binzhen wei wei gan diversity promoting generative adversarial work generating informative diversied text corr jingjing sun zeng xiaodong zhang ancheng ren houfeng wang wenjie unpaired sentiment sentiment translation cled reinforcement learning approach acl ramesh nallapati bowen zhou ccero nogueira dos santos aglar gulcehre bing xiang abstractive text summarization sequence proceedings sequence rnns signll conference computational natural language learning conll berlin germany august pages razvan pascanu tomas mikolov yoshua bengio difculty training recurrent neural networks proceedings international conference machine learning icml lanta usa june pages dragomir radev timothy allison sasha goldensohn john blitzer arda elebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu jahna otterbacher hong horacio saggion simone teufel michael topper adam winkel zhu zhang mead platform multidocument multilingual text summarization proceedings fourth international ence language resources evaluation lrec alexander rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing emnlp lisbon portugal september pages abigail peter liu christopher manning point summarization generator networks proceedings nual meeting association computational linguistics acl pages nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan nov dropout simple way prevent neural journal machine networks overtting learning research sun xuancheng ren shuming houfeng wang meprop sparsied propagation accelerated deep learning reduced ting icml pages sun bingzhen wei xuancheng ren shuming label embedding network learning bel representation soft training deep networks corr ilya sutskever oriol vinyals quoc sequence sequence learning neural works advances neural information ing systems annual conference neural mation processing systems pages sho takase jun suzuki naoaki okazaki tsutomu rao masaaki nagata neural headline generation abstract meaning representation proceedings conference empirical methods natural language processing emnlp
