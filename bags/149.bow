autoencoder assistant supervisor improving text representation chinese social media text summarization shuming xu junyang houfeng key lab computational linguistics school eecs peking university learning lab beijing institute big data research peking university foreign languages peking university shumingma xusun linjunyang edu abstract current abstractive text marization models based sequence sequence model source content social media long noisy difcult learn accurate semantic representation compared source content notated summary short shares work ing source content supervise learning tation source content summary implementation regard summary autoencoder assistant pervisor following previous work evaluate model popular chinese social media dataset tal results model achieves state art performances benchmark dataset introduction text summarization produce brief summary main ideas text unlike extractive text summarization radev et al woodsend lapata cheng lapata lects words word phrases source texts summary abstractive text summarization learns semantic representation generate human like summaries recently models abstractive text summarization based sequence sequence model encodes source texts semantic representation encoder generates summaries representation decoder contents social media long contain errors come spelling mistakes informal expressions grammatical mistakes baldwin et al large errors contents cause great difculties text summarization rnn based difcult compress long sequence accurate representation li et al gradient vanishing exploding problem compared source content easier encode representations summaries short manually selected source content summary share points possible supervise learning semantic representation source content summary paper regard summary coder assistant supervisor train autoencoder inputs structs summaries obtain better sentation generate summaries supervise internal representation autoencoder minimizing tance representations finally use adversarial learning enhance sion following previous work ma et al evaluate proposed model nese social media dataset experimental results model outperforms state art baseline models specically model outperforms baseline score rouge l proposed model introduce proposed model detail section notation code available lancopku superae given summarization dataset consists n data samples ith data sample xi yi m l c s c v v x r training stage zs zs function measures distance zs zt tunable parameter balance loss supervision parts loss nh ber hidden unit limit magnitude distance function set based performance validation set distance representations written adversarial learning enhance supervision versarial learning approach shown eq use xed hyper parameter weight measure strength supervision toencoder case source content summary high relevance strength supervision higher source content summary low relevance strength lower order determine strength supervision namically introduce adversarial learning specically regard representation autoencoder gold representation sequence sequence fake resentation model trained discriminate gold fake representations called discriminator discriminator tries identify representations trary supervision minimizes tance representations makes ilar tries prevent discriminator ing correct predictions way criminator distinguish representations means source content summary low relevance strength supervision decreased discriminator fails distinguish strength supervision proved implementation adversarial learning discriminator objective function written log pd y log pd y pd y probability criminator identies vector z gold resentation pd y probability vector z identied fake sentation d parameters test stage figure overview model model consists sequence sequence model autoencoder model training stage use autoencoder supervise sequence sequence model test stage use sequence sequence model generate maries tains source content xi xm summary yi yl m number source words l ber summary words training stage train model generate summary y given source content test stage model decodes predicted summary given source content supervision autoencoder figure shows architecture model training stage source content encoder presses input contents internal sentation zt bi lstm encoder time summary encoder compresses ence summary y representation zt zs fed lstm decoder erate summary finally semantic tation source content supervised summary implement supervision minimizing distance semantic representations zt zs term loss function written ls zs nh source content encodersummary encodersummary decodersupervisesource content encodersummary inator minimizing discriminator tive train parameters nator rest parameters remains changed supervision objective criminator written log pd y log pd y pairs pairs ii pairs iii text summary pairs ii iii manually annotated relevant scores ranged reserve pairs scores leaving pairs ii pairs iii following previous work hu et al use training set ii validation set iii test set minimizing supervision objective update parameters encoders evaluation metric n n loss function training parts objective functions optimize models rst cross entropy losses sequence sequence autoencoder lae second loss supervision written equation sarial learning equation tion sum parts nal loss function optimize use adam kingma ba timization method train model hyper parameters adam optimizer set learning rate momentum eters respectively clip gradients pascanu et al maximum norm experiments following previous work ma et al evaluate model popular chinese social media dataset rst introduce datasets evaluation metrics experimental tails compare model state art systems dataset large scale chinese social media text marization dataset lcsts constructed hu et al dataset consists text summary pairs constructed famous chinese social media website called sina weibo split parts com evaluation metric rouge score lin hovy popular tion evaluation metrics compare ically produced summary reference maries computing overlapping lexical units including unigram bigram trigram longest common subsequence lcs following previous work rush et al hu et al use unigram bi gram rouge l lcs evaluation metrics reported experimental results experimental details vocabularies extracted training sets source contents summaries share vocabularies order alleviate risk word segmentation mistakes split chinese sentences characters prune vocabulary size covers common characters tune hyper parameters based rouge scores validation sets set word embedding size hidden size number lstm layers batch size use dropout srivastava et al dataset following ous work li et al implement beam search set beam size baselines compare model following art baselines rnn rnn cont sequence sequence baseline gru encoder coder provided hu et al ference rnn context attention mechanism rnn rnn dist chen et al based neural model attention models r l rnn et al et al rnn cont et al rnn et al et al copynet et al et al rnn et al et al impl superae paper adversarial learning table comparison state art models lcsts test set r l note rouge l spectively models sufx w table word based rest models character based mechanism focuses different parts source content copynet gu et al incorporates copy mechanism allow parts erated summary copied source content srb ma et al sequence sequence based neural model improving semantic relevance input text output summary drgd li et al deep recurrent generative decoder model combining coder variational autoencoder implementation sequence sequence model tion mechanism mental setting model fair son results purpose simplicity denote vision autoencoder model superae report rouge score model baseline models test sets table summarizes results superae model baselines rst compare model baseline shows models superae class class table accuracy sentiment tion amazon dataset train er inputs internal representation provided sequence sequence model outputs predicted label compute class class accuracy predicted labels evaluate quality text representation superae model large improvement baseline rouge l demonstrates ciency model compare model recent summarization systems evaluated training set test sets results directly reported referred articles shows superae outperforms models relative gain rouge l perform ablation study removing adversarial learning component order contribution shows adversarial learning improves performance rouge l summarization examples model shown table seqseq model captures wrong meaning source content produces summary china united airlines exploded airport superae model captures correct points erated summary close meaning ence summary analysis text representation want analyze internal text resentation improved superae model text representation abstractive hard evaluate translate representation sentiment score sentiment classier evaluate quality representation means sentiment accuracy perform experiments amazon fine foods reviews corpus mcauley leskovec amazon dataset contains users ing labels summary reviews making possible train classier predict sentiment labels model generate summaries train superae model source night people caught smoke ight china united airlines chendu beijing later ight porarily landed taiyuan airport sengers asked security check nied captain led collision tween crew passengers reference people smoked ight led collision crew passengers china united airlines exploded airport leaving people dead superae people smoked ight chendu beijing led collision crew passengers table summarization example model compared reference model text summary pairs convergence transfer encoders sentiment classier train classier xing parameters encoders classier simple feedforward neural network maps representation label tribution finally compute accuracy predicted class labels class labels shown table model achieves accuracy class class respectively superae model forms baselines large margin propose generator pointer model coder able generate words source texts gu et al solved issue rating copying mechanism allowing parts summaries copied source contents et al discuss problem incorporate pointer generator model coverage mechanism hu et al build large corpus chinese social media short text summarization benchmark datasets chen et al introduce tion based neural model forces tion mechanism focus difference parts source inputs ma et al propose neural model improve semantic relevance source contents summaries work related sequence sequence model cho et al toencoder model bengio liou et al sequence sequence model successful generative neural model widely applied machine translation sutskever et al jean et al luong et al text summarization rush et al chopra et al nallapati et al ral language processing tasks autoencoder gio articial neural network unsupervised learning efcient representation neural attention model rst proposed danau et al conclusion propose novel model coder supervisor sequence sequence model learn better internal representation abstractive summarization adversarial learning approach introduced improve supervision autoencoder tal results model outperforms sequence sequence baseline large margin achieves state art performances chinese social media dataset related work acknowledgements rush et al rst propose abstractive based summarization model uses tive cnn encoder compress texts neural network language model generate summaries chopra et al explore recurrent ture abstractive summarization deal vocabulary problem nallapati et al work supported national natural ence foundation china national high technology research development program china national gram thousand young talents program xu sun corresponding author paper references dzmitry bahdanau kyunghyun cho yoshua neural machine translation corr bengio jointly learning align translate timothy baldwin paul cook marco lui andrew mackinlay li wang noisy cial media text diffrnt social media sources sixth international joint conference ral language processing ijcnlp nagoya japan october pages yoshua bengio learning deep architectures ai foundations trends machine learning qian chen xiaodan zhu zhenhua ling si wei hui jiang distraction based neural networks modeling documents proceedings international joint conference articial gence ijcai new york ny aaai jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings annual meeting sociation computational linguistics acl august berlin germany volume long papers kyunghyun cho bart van merrienboer c aglar gulcehre dzmitry bahdanau fethi bougares ger schwenk yoshua bengio learning phrase representations rnn encoder decoder proceedings statistical machine translation conference empirical methods natural language processing emnlp pages sumit chopra michael auli alexander m rush abstractive sentence summarization naacl hlt tentive recurrent neural networks conference north american chapter association computational guistics human language technologies pages jiatao gu zhengdong lu hang li victor o k incorporating copying mechanism li proceedings sequence sequence learning annual meeting association putational linguistics acl baotian hu qingcai chen fangze zhu sts large scale chinese short text summarization dataset proceedings conference empirical methods natural language ing emnlp lisbon portugal september pages diederik p kingma jimmy ba adam corr method stochastic optimization jiwei li minh thang luong dan jurafsky hierarchical neural autoencoder paragraphs documents proceedings annual meeting association computational guistics international joint conference natural language processing asian eration natural language processing acl july beijing china volume long papers pages piji li wai lam lidong bing zihao wang deep recurrent generative decoder stractive text summarization proceedings conference empirical methods natural language processing emnlp copenhagen denmark september pages chin yew lin eduard h hovy matic evaluation summaries n gram occurrence statistics human language ogy conference north american chapter association computational linguistics naacl cheng yuan liou wei chen cheng jiun wei liou daw ran liou autoencoder words neurocomputing cheng yuan liou jau chi huang wen chie yang modeling word perception elman network neurocomputing thang luong hieu pham christopher d ning effective approaches attention based proceedings neural machine translation conference empirical methods natural language processing emnlp pages shuming ma xu sun wei li sujian li wenjie li xuancheng ren query output erating words querying distributed word naacl sentations paraphrase generation shuming ma xu sun jingjing xu houfeng wang wenjie li qi su improving semantic relevance sequence sequence learning nese social media text summarization ings annual meeting association computational linguistics acl ver canada july august volume short papers pages sebastien jean kyunghyun cho roland memisevic yoshua bengio large vocabulary neural machine translation proceedings annual meeting sociation computational linguistics acl pages julian john mcauley jure leskovec amateurs connoisseurs modeling evolution user expertise online reviews ternational world wide web conference www rio de janeiro brazil pages austin texas usa november pages kristian woodsend mirella lapata matic generation story highlights acl proceedings annual meeting sociation computational linguistics pages jingjing xu xu sun xuancheng ren junyang lin binzhen wei wei li gan diversity promoting generative adversarial work generating informative diversied text corr jingjing xu xu sun qi zeng xiaodong zhang ancheng ren houfeng wang wenjie li unpaired sentiment sentiment translation cled reinforcement learning approach acl ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre bing xiang abstractive text summarization sequence proceedings sequence rnns signll conference computational natural language learning conll berlin germany august pages razvan pascanu tomas mikolov yoshua bengio difculty training recurrent neural networks proceedings international conference machine learning icml lanta ga usa june pages dragomir r radev timothy allison sasha goldensohn john blitzer arda c elebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu jahna otterbacher hong qi horacio saggion simone teufel michael topper adam winkel zhu zhang mead platform multidocument multilingual text summarization proceedings fourth international ence language resources evaluation lrec alexander m rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing emnlp lisbon portugal september pages abigail peter j liu christopher d manning point summarization generator networks proceedings nual meeting association computational linguistics acl pages nitish srivastava geoffrey e hinton alex krizhevsky ilya sutskever ruslan nov dropout simple way prevent neural journal machine networks overtting learning research xu sun xuancheng ren shuming ma houfeng wang meprop sparsied propagation accelerated deep learning reduced ting icml pages xu sun bingzhen wei xuancheng ren shuming ma label embedding network learning bel representation soft training deep networks corr ilya sutskever oriol vinyals quoc v le sequence sequence learning neural works advances neural information ing systems annual conference neural mation processing systems pages sho takase jun suzuki naoaki okazaki tsutomu rao masaaki nagata neural headline generation abstract meaning representation proceedings conference empirical methods natural language processing emnlp
