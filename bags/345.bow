dissecting components factors neural text generation khyathi raghavi chandu alan w black language technologies institute carnegie mellon university kchandu cmu edu t c o l c s c v v x r abstract neural text generation metamorphosed critical natural language applications ranging text completion free form rative generation generating natural language fundamentally human attribute advent ubiquitous nlp applications virtual agents marks need impart skill machines colossal research effort frontiers neural text generation including machine translation summarization image captioning storytelling believe excellent ture retrospect directions eld specically paper surveys tal factors components relaying task nostic impacts generation tasks storytelling summarization tion specic present tion imperative techniques respect learning paradigms pretraining modeling approaches decoding key challenges hope deliver stop nation researchers eld facilitate perspective situate work impacts closely related tasks introduction text generation task producing written spoken narrative structured unstructured data overarching goal seamless machine communication presenting wealth data way comprehend spect modeling approaches main paradigms generating text based schema input output text text ii data text text table presents categorization different tasks based paradigm tasks deserve undivided attention accordingly ily dissected studied surveyed recent past instance independent exclusive generation paradigm text text data text text task dialog machine translation style transfer summarization image captioning visual storytelling speech recognition table text knowledge bases text knowledge bases language modeling input conversation history source language style text single multiple documents image images audio table null output response target language style text summary descriptive text descriptive text text text text sequence text table paradigms tasks text generation purposes compactness include text paradigm data text veys periodically conducted summarization lin ng allahyari et al nenkova mckeown tas kiyani knowledge text generation dblp conf inlg dblp conf naacl koncel machine translation chu wang dabre et al chand slocum dialog sponse generation liu et al montenegro et al ramesh et al chen et al storytelling narrative generation tong et al togelius et al image captioning hossain et al dig deeper task specic approaches foundational bleeding edge research tremely necessary focus techniques benecial tightly coupled tasks overlooked goal survey cus key components task agnostic improve ensemble tasks neural text generation studies conducted surveying text generation perera nand present detailed overview information ory based approaches iqbal qureshi primarily focus core modeling approaches pecially vaes kingma welling gans goodfellow et al gatt mer elaborated tasks captioning style trasfer primary focus data figure components factors approaches text generation components laid grey boxes factors laid left dotted lines text tasks controllability aspect explored prabhumoye et al workclosest lu et al perform empirical study core modeling approaches contrast paper focuses task nostic components factors capable pushing ensemble tasks forward figure presents components factors tant study neural text generation elaborated paper modeling approaches core modeling paradigms supervised learning generation proaches setting use maximum likelihood objective training sequence generation sequential multi label cross entropy log y t t inherent inconsistency posure ground truth text training inference stages teacher forcing training leads problem exposure bias ranzato et al training token current time step predicted conditioned ground truth prex correcting course tive word predicted model inference conditioned generated prex absence ground truth x problem severe ing length output solution address issue scheduled sampling bengio et al mixes teacher forced embeddings model predictions previous time step reinforcement learning main issue supervised learning approach text ation mismatch maximum hood objective optimized metrics text quality reinforcement learning addresses mismatch directly optimizing end metrics non differentiable typically policy ent algorithms optimize bleu score directly reinforce objective shown low sum log probabilities multiplied reward score reward score puted expected bleu score rt logp t x t computing bleu update computationally efcient incorporate training procedure problem inherent inefciency metric e bleu best measure evaluate text quality practice usually policy network usually trained maximum likelihood objective optimizing bleu score adversarial learning paradigm adversarial learning comprising competing jectives mismatch training inference stages addressed professor forcing lamb et al adversarial domain adaptation bring behavior training sampling close sharing parameters teacher forcing network evaluation humanautomaticemulatedfluency coverage length content planning speedinference decoding autoregressivenon autoregressivesamplingsearchsupervisedcore modeling encoder decoder adversarialvariationalreinforcementgenerative pretrainingsupervised free running network apart main components generator nator discriminator optimized correctly classify sequence belonging free running behavior teacher forced behavior tor goals maximize likelihood data fool discriminator options respect keeping xed ing closer rst respect teacher forcing network free running network empirically professor forcing plays role regularizer erative adversarial networks gan gained popularity respect recent times core idea gradient tor guides alter generated data margin order realistic slight change apparent continuous values comparison language discrete space variants adopted address cic problems seqgan assess partially generated sequence yu et al maskgan improve sample quality text lling fedus et al leakgan model long term pendencies leaking discriminator information generator guo et al main challenges researched area discrete sampling sampling step selecting argmax language non differentiable function solution replace continuous proximation adding gumbel noise ative log negative log sample uniform distribution known gumbel softmax mode collapse gans typically face issue sampling specic tokens cheat nator known mode collapse way subspace target distribution learnt erator dp gan addresses explicit diversity promoting reward xu et al power dynamics generator criminator problem arises criminator trained faster generator case gradient tor vanishes leading real update generator pre training recent couple years seen major surge interest pre training techniques primarily focused language understanding tasks work targeted training generation unilm unied pre trained language model dong et al proposed pre training mechanism natural language understanding natural guage generation tasks fundamentally viously widely elmo peters et al constitutes language model left right right left gpt radford et al autoregressive left right language model bert devlin et al bidirectional guage model unilm optimized jointly objectives additional new lm bidirectional encoding followed unidirectional decoding depending use case unilm adopted use directional lm left right bidirectional lm attention tokens lm attention tokens previous segment left context current segment similar goal mind mass song et al modied ing patterns input achieve bert xlnet yang et al pre train encoder gpt pretrains decoder framework introduced pretrain encoder attention decoder gether encoder masks sequence length decoder predicts sequence length token masked idea jointly training encoder attention decoder remains unilm interesting tribution way masking utilized bring following advantages kens masked decoder tokens masked encoder complementary ing encourages joint training encoder decoder ii encoder supports decoder extracting ful information masked fragments improves understanding nlu capabilities model sequence length decoded consecutively nlg capability proved note k model closer bert biased encoder k length sentence model closer gpt biased decoder similar unilm bart lewis et al tional encoder autoregressive decoder underlying model standard transformer vaswani et al based neural mt framework main difference bart mass tokens masked necessarily consecutive main idea second difference corrupt text arbitrary noise reconstruct original text input corrupted following transformations token masking token deletion ken inlling sentence permutation document rotation following raffel et al posed unifying framework ties nlp problems text generation tasks text text paradigm recently dathathri et al introduced plug play language models capable efciently training fewer parameters control huge underlying pretrained model finetuning vast models generative tasks studied style transformers sudhakar et al conversational agents dinan et al decoding strategies natural step pre training training decoding distinguishing characteristic generation absence dence time steps input output introducing crucial component decoding primarily categorized autoregressive non autoregressive autoregressive decoding traditional models strategy correspond true tributions words mainly comes specting conditional dependence property left right autoregressive techniques viewed sampling search techniques main disadvantage strategy throttling transformer based models fall short cating training advantages training non sequential inference holds sequential autoregressive decoding non autoregressive decoding line work primarily addresses problems associated autoregressive decoding denition conditional independence erty holds leads multimodality problem time step considers different variants respect entire sequence conditions compete ond main advantage reduction latency real time generation guo et al dressed problem context neural chine translation transformers copying source inputs decoder formly repeatedly based fertility counts address varying sequence lengths source target texts fertilities predicted dedicated neural network reduce unsupervised problem supervised enabling latent variable invariable replications based tilities lead duplication words closely followed van den oord et al took different approach introducing probability sity distillation modifying convolutional ral network pre trained teacher network score student network attempting minimize kl divergence teacher work works set trend latent variables capture interdependence different time steps decoder following work lee et al use iterative renement denoising latent variables ment steps idea iterative decoding inspired way avenues combining benets cloze style mask prediction objectives bert devlin et al include tion based techniques gu et al repeated masking regenerating ghazvininejad et al providing model predictions input ghazvininejad et al wang et al proposed alternative proach address repetition observed guo et al completeness regularization terms repetition handled izing similarity consecutive words pleteness addressed enabling reconstruction source sentence hidden states coder based duality translation tasks tween source target target source rently guo et al address issues improving inputs decoder additional phrase table information sentence level ment source target word embeddings sampling search techniques random sampling words sampled randomly based probability entire distribution pruning mass p yt t jv greedy decoding technique simply boils selecting argmax probability distribution selecting argmax problem limits diversity generation note result best output alternate hypothesis comprising path select probable word time step yt argmaxyt jv major disadvantage greedy decoding mechanism correct course mistake accumulates errors following time steps monotonous predictable texts alleviated niques beam search worked discrete settings gumbel greedy decoding gu et al variants studied zarrie schlangen beam search beam search introduces course correction mechanism approximation argmax selecting beam size number beams time step beam size greedy decoding beam size size vocabulary computationally expensive relatively ied task agnostic objectives wang et al instance including social media text wang ng error correction dahlmeier ng small beam sizes lead matical sentences grammatical increasing beam size similarly small beam sizes relevant respect content generic increasing beam size varieties beam search noisy parallel approximate decoding method cho introduces noise hidden state non deterministically slightly deviate argmax beam blocking repetition lems nlg technique paulus et al combats problem blocking repeated n grams essentially adjusts bility repeated n gram c iterative beam search order search diverse search space technique likov et al introduced iteratively perform beam search times current time step avoid partial potheses encountered time step previous iterations based soft hard decisions include exclude beams diverse beam search problem beam search times decoded sequence tends come highly signicant beams suppressing diversity moderation vijayakumar et al adds diversity penalty computed example hamming distance current hypothesis hypotheses groups readjust scores predicting word e clustered beam search goal prune unnecessary beams time step tam candidates embed averaged glove representations cluster ing k means k clusters pick b k candidates cluster b candidates total time step clustering post decoding proaches modify decoding step nique kriz et al clusters decoding sentence representations diversity promoting beam search variants tained clustered sentence high log likelihood selected cluster k sampling technique fan et al randomly samples ble candidates distribution means conning model select truncated probability mass p t p t v k p t ifyt v k k size vocabulary random sampling k greedy decoding high valued k results dicey words monotonous low valued k results safe puts monotonous problem k limited value scenarios p sampling aforementioned lem xed value k addressed p sampling known nucleus sampling holtzman et al instead getting rid unspecied probability mass k sampling importance shifted probability mass preserved addresses ios broader set reasonable options narrow set options achieved selecting dynamic k number words cumulative probability distribution words threshold probability value attained ytv p p t p key challenges challenges section provides list solutions pitfalls solutions described encouraging research address key challenges fluency couple detrimental factors affect uency text generation repetition coherence solution beam blocking blocking beams taining previously generated n grams quent generation combats repetition ages diversity multiple options form including cutting beam stream lect rest n grams klein et al paulus et al problem beams natural kind repetition instance order emphasize naturally humans blocked selecting number beams problem natural function word repeat solution problem massarelli et al tensively studied variants introducing beam blocking referred n gram ing applying delays beam search solution unlikelihood objective welleck et al argue fundamental aw objective likelihood main idea decrease probability unlikely negative candidates negative candidates selected previous contexts token sequence levels essentially n grams way simultaneously optimizing likelihood unlikelihood discouraging repetition previous outputs problem major issue selecting negative contexts tricky needs selection simple n gram sequences occurred previously solution coverage penalty discourages attention mechanism attend word repeatedly et al navigating time step source ferent time steps decoded output tion weights higher particular source timestep timestep covered coverage penalty coverage penalty attention probability mass source time step solution static dynamic planning addresses coherence terms layout tural organization text yao et al schema static dynamic plans form abstract ow text actual text realized problem underlying language models capable taking leading hallucinations compromising delity text length decoding factor guishes generation rest family tasks variability length ated output main problem length sequence increases sum log probability scores decrease means models prefer shorter hypotheses solutions combat problem following solution length normalization penalty generated output scored normalizing viding length wu et al explore different variation normalization constant pretty standard dataset high variance lengths solution probability boosting technique multiplies probability xed constant time step alleviates diminishing score problem solution bias incorporate bias model based empirical relations lengths source target sentences training data content selection certain tasks demand copying details input rare proper nouns instance news articles especially needed tasks like summarization demand combination extractive abstractive techniques solution copy mechanism copy mechanism forms pointing unknown words gulcehre et al based attention et al joint conditional copy mechanism gu et al puduppully et al maybe based attention copies segments input output problem technique boils combination extractive abstractive sort extractive system solution hierarchical modeling nique maintains global account content modeled hierarchical techniques dual stage models martin et al xu et al gehrmann et al rst stage pre selects relevant keywords generation following stage problem models possibly hit uency connecting dots selected content generation means good right words extracted decrease affects uency optimization objective similar servation earlier section inherent mismatch objective function maximum likelihood end metrics bleu rouge solution reinforcement learning common solution problem reinforcement learning optimize end metrics rouge combination mle rl objectives hu et al wang et al problem problem end metrics directly correlate human judgements optimizing bleu rouge ensure human quality text solution maximum mutual information idea incorporate pairwise information source target instead direction usually target given source li et al target probability subtracted target given source probability diminish probability generic sentences viable extension conditioning personality consistency solution distinguishability hallucinations abstractive generation unwanted byproducts optimizing log loss combat searchers explored optimizing minimized guishability human generated text hashimoto et al theis et al following similar path kang hashimoto proposed cating loss rid unwanted samples speed practical applications ating text real time time lag ing addition chasing state art sults model compression plays crucial demonstrating increase speed ation cheng et al exhaustively surveyed different techniques perform model sion techniques hardware certain modeling approaches handle problem gonzalvo et al work studied context real time interpretation speech fugen et al yarmohammadi et al grissom ii et al recently deng rush proposed cascaded decoding approach introducing markov transformers demonstrating high speed curacy quantization quantizing roy et al gray weights sharing weight value belong bin proved helpful improving speed facilitates computations gradients bin distillation performed teacher smaller student network tries replicate performance teacher fewer ters chen et al pruning technique thresholds prunes connections weights lesser predetermined threshold retrain network order adjust weights remaining connections real time gu et al trained agent learns decide actions reading discarding candidate writing accepting candidate policy network optimized combination quality evaluated bleu delay evaluated number consecutive words reading stage increases wait time caching trick cache previous computations avoid repetition evaluation similar generative modeling text ation faces crucial challenges evaluation reiter belz reiter van der lee et al present best practices evaluating automatically generated text main hindrance standardize evaluate nlg like standard tasks sub component tasks means input varied forms tables images text tain settings diverse image captioning need objects entities dialog need pronouns natural coherence instead repeating nouns desiderata text crucial dene tors contributing quality good text factors include relevant content appropriate structure terms coherence suitable surface forms addition uency grammaticality ability novelty scenarios crucial factors intrinsic extrinsic evaluation subjective scopes text generation performed intrinsically extrinsically intrinsic evaluation performed internally respect eration extrinsic evaluation typically performed metric evaluate stream task generation quality judged automatic metrics human evaluation automatic metrics outline broad categories metrics vantages disadvantages metrics classied following categories word overlap based metrics based extent word overlap means capture replication words problem measures focus semantics surface form words includes precision papineni et al improved weighting rare n grams nist doddington recall n grams rouge lin hovy equivalent n grams meteor banerjee lavie tf idf based cosine similarity n grams cider vedantam et al extension specic metrics evaluate content selection ing summarization content units pyramid nenkova passonneau parsed scene graphs objects relations spice derson et al stanojevic simaan proposed beer address ranking lem character n grams words language model based metrics includes perplexity brown et al metrics good commenting language model self sort gives average number choices random variable directly evaluate generation instance decrease perplexity imply decrease word error rate means cally lm good select right word corpus human likeness measured training model discriminate human machine generated text automatic turing test lowe et al cui et al hashimoto et al embedding based metrics tage able capture semantics meant lo lo et al putes structural similarity shallow semantic parses denitely discretionarily spectively word embeddings recently contextulaized embeddings extensively capture bertscore zhang et al bleurt sellam et al metrics based combination different embeddings proposed shimanaka et al ma et al problem correlating human judgements persists emulated automatic metrics rics check intended behavior generation based sub problem modeling approach addressing check correctness delity alty respect source document apply inference diversity evaluated computing corpus based distributions number distinct entities fan et al dong et al clark et al recently wang et al worked identifying factual cies generated summaries idea question posed source document summary result similar answers human evaluation broadly mechanisms conducting subjective evaluations challenging component text tion rst preference testing second scoring studies shown ence based testing prone variance pared absolute scoring important points mind conducting human expensive evaluation duct feasible check model repeated examination standard universally agreed guidelines setup tasks words conducting subjective ation subjective nature scores tend vary based nature scales judgements binary discrete integer values continuous observed human ences inconsistent biased sonal demographic conditions cases important measure inter annotator ment people lenient strict scaled people vi framing task unambiguous way elicit right information maintain reproducibility having critically discussed human evaluation best got absolutely crucial perform human evaluation nlg tasks problems need taken merely cautions develop rational systematic testing conditions comparisons automatic human evaluation systems belz reiter studied actively order bring human evaluation closer automatic metrics conclusion past decade witnessed text generation dribbling niche scenarios mainstream nlp applications urges need snapshot retrospect progress varied text tion tasks unison paper written goal presenting stop destination task agnostic components factors text ation researchers foraging situate work guage impact vast eld moving forward envision crucial directions focus impactful innovation text generation include generation real time non autoregressive decoding iii sistency situated contexts real virtual environments games iv consistency sonality opinions especially virtual agents v conditioning multiple modalities text data vi investigation ing nding better metrics evaluate nlg better correlated human judgements vii creative text generation believe right time extend advancements particular task tightly coupled tasks revamp improvements text generation holistic task references mehdi allahyari seyedamin pouriyeh mehdi asse saeid safaei elizabeth d trippe juan b text rez krys kochut arxiv preprint tion techniques brief survey peter anderson basura fernando mark johnson stephen gould spice semantic tional image caption evaluation computer vision eccv european conference dam netherlands october ceedings v volume lecture notes computer science pages springer satanjeev banerjee alon lavie meteor automatic metric mt evaluation proved correlation human judgments ceedings workshop intrinsic trinsic evaluation measures machine tion ann arbor michigan usa june pages ciation computational linguistics anja belz ehud reiter comparing matic human evaluation nlg systems eacl conference european ter association computational tics proceedings conference april trento italy association computer tics samy bengio oriol vinyals navdeep jaitly noam shazeer scheduled sampling quence prediction recurrent neural networks advances neural information processing tems annual conference neural tion processing systems december montreal quebec canada pages peter f brown stephen della pietra vincent j della pietra jennifer c lai robert l mercer estimate upper bound entropy english comput linguistics sunita chand empirical survey machine translation tools second international ference research computational intelligence communication networks icrcicn pages ieee hongshen chen xiaorui liu dawei yin jiliang tang survey dialogue systems recent advances new frontiers sigkdd explorations yen chun chen zhe gan yu cheng jingzhou liu jingjing liu distilling knowledge bert text generation corr yu cheng duo wang pan zhou tao zhang survey model compression eration deep neural networks arxiv preprint kyunghyun cho noisy parallel approximate decoding conditional recurrent language model corr international conference agents ing representations iclr new orleans la usa openreview net chenhui chu rui wang survey main adaptation neural machine translation proceedings international conference computational linguistics pages elizabeth clark yangfeng ji noah smith neural text generation stories entity proceedings sentations context conference north american chapter association computational linguistics human language technologies naacl hlt new orleans louisiana usa june volume long papers pages association computational linguistics yin cui guandao yang andreas veit xun huang serge j belongie learning ieee conference ate image captioning computer vision pattern recognition cvpr salt lake city ut usa june pages ieee computer society raj dabre chenhui chu anoop kunchukuttan survey multilingual neural machine translation arxiv preprint daniel dahlmeier hwee tou ng search decoder grammatical error correction proceedings joint conference ical methods natural language processing computational natural language learning pages association computational tics sumanth dathathri andrea madotto janice lan jane hung eric frank piero molino jason yosinski rosanne liu plug play language models simple approach controlled text generation international conference learning tations iclr addis ababa ethiopia april openreview net yuntian deng alexander m rush cascaded text generation markov transformers corr jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language proceedings conference standing north american chapter association computational linguistics human language technologies naacl hlt minneapolis mn usa june volume long short pers pages association tional linguistics emily dinan stephen roller kurt shuster angela fan michael auli jason weston wizard wikipedia knowledge powered conversational george doddington automatic evaluation machine translation quality n gram occurrence statistics proceedings second international conference human language nology research pages li dong nan yang wenhui wang furu wei aodong liu yu wang jianfeng gao ming zhou hsiao wuen hon unied language model pre training natural language ing generation advances neural tion processing systems pages ruo ping dong khyathi raghavi chandu alan w black induction reference entities visual story corr angela fan mike lewis yann dauphin proceedings erarchical neural story generation annual meeting association computational linguistics volume long papers pages angela fan mike lewis yann n dauphin strategies structuring story generation ceedings conference association computational linguistics acl florence italy july august volume long pers pages association tional linguistics william fedus ian j goodfellow andrew m dai maskgan better text generation lling international conference learning representations iclr vancouver bc canada april conference track proceedings openreview net christian fugen alex waibel muntsin kolss simultaneous translation lectures speeches machine translation albert gatt emiel krahmer survey state art natural language generation core tasks applications evaluation j artif intell res sebastian gehrmann yuntian deng alexander m rush abstractive summarization proceedings conference cal methods natural language processing pages marjan ghazvininejad omer levy yinhan liu luke zettlemoyer mask predict parallel decoding conditional masked language models proceedings conference cal methods natural language processing international joint conference natural language processing emnlp ijcnlp hong kong china november pages association computational linguistics marjan ghazvininejad omer levy luke semi autoregressive training arxiv preprint moyer proves mask predict decoding aaai symposium educational vances articial intelligence new leans louisiana usa february pages aaai press xavi gonzalvo siamak tazari chun chan markus becker alexander gutkin hanna silen recent advances google real time hmm driven unit selection synthesizer ian goodfellow jean pouget abadie mehdi mirza bing xu david warde farley sherjil ozair aaron courville yoshua bengio generative advances neural information versarial nets processing systems pages robert gray vector quantization ieee assp magazine alvin grissom ii jordan boyd graber john morgan hal daume iii nt nal verb wait reinforcement learning taneous machine translation proceedings conference empirical methods natural language processing emnlp pages jiatao gu daniel jiwoong m victor ok li neural machine translation gumbel greedy coding thirty second aaai conference cial intelligence jiatao gu qi liu kyunghyun cho insertion based decoding automatically ferred generation order trans assoc comput guistics jiatao gu zhengdong lu hang li victor ok incorporating copying mechanism li proceedings sequence sequence learning annual meeting association putational linguistics volume long papers pages jiatao gu graham neubig kyunghyun cho tor o k li learning translate time neural machine translation ings conference european ter association computational linguistics eacl valencia spain april ume long papers pages association computational linguistics caglar gulcehre sungjin ahn ramesh nallapati bowen zhou yoshua bengio pointing unknown words proceedings nual meeting association computational linguistics volume long papers pages jiaxian guo sidi lu han cai weinan zhang yong yu jun wang long text generation adversarial training leaked information ceedings thirty second aaai conference articial intelligence tive applications articial intelligence junliang guo xu tan di tao qin linli xu tie yan liu non autoregressive ral machine translation enhanced decoder thirty aaai conference cial intelligence aaai thirty vative applications articial intelligence ence iaai ninth aaai symposium ucational advances articial intelligence eaai honolulu hawaii usa january ary pages aaai press junliang guo xu tan linli xu tao qin enhong chen tie yan liu fine tuning riculum learning non autoregressive neural chine translation thirty fourth aaai ference articial intelligence aaai thirty second innovative applications articial intelligence conference iaai tenth aaai symposium educational advances articial telligence eaai new york ny usa ary pages aaai press tatsunori b hashimoto hugh zhang percy liang unifying human statistical evaluation natural language generation proceedings conference north american ter association computational linguistics human language technologies naacl hlt minneapolis mn usa june volume long short papers pages ation computational linguistics ari holtzman jan buys li du maxwell forbes yejin choi curious case neural text international conference degeneration learning representations iclr addis ababa ethiopia april openreview net md zakir hossain ferdous sohel mohd fairuz ratuddin hamid laga comprehensive survey deep learning image captioning corr junjie hu yu cheng zhe gan jingjing liu jianfeng gao graham neubig makes good story designing composite rewards visual storytelling thirty fourth aaai conference articial intelligence aaai second innovative applications articial gence conference iaai tenth aaai posium educational advances articial ligence eaai new york ny usa february pages aaai press touseef iqbal shaima qureshi survey text generation models deep learning journal king saud university computer information sciences daniel kang tatsunori hashimoto proved natural language generation loss tion corr diederik p kingma max welling international encoding variational bayes iclr conference learning representations banff ab canada april ference track proceedings guillaume klein yoon kim yuntian deng jean lart alexander m rush opennmt source toolkit neural machine translation proceedings acl system demonstrations pages reno kriz joao sedoc marianna apidianaki carolina zheng gaurav kumar eleni miltsakaki chris callison burch complexity weighted loss diverse reranking sentence simplication proceedings conference north american chapter association tional linguistics human language technologies volume long short papers pages ilia kulikov alexander miller kyunghyun cho jason weston importance search uation strategies neural dialogue modeling proceedings international conference natural language generation pages alex m lamb anirudh goyal alias parth goyal ying zhang saizheng zhang aaron c courville yoshua bengio professor forcing new algorithm training recurrent networks vances neural information processing systems pages chris van der lee albert gatt emiel van miltenburg sander wubben emiel krahmer best practices human evaluation automatically generated text proceedings tional conference natural language generation inlg tokyo japan october november pages association tional linguistics jason lee elman mansimov kyunghyun cho deterministic non autoregressive neural quence modeling iterative renement ceedings conference empirical ods natural language processing brussels gium october november pages association computational linguistics mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov luke zettlemoyer bart denoising sequence sequence pre training natural language generation translation comprehension arxiv preprint jiwei li michel galley chris brockett jianfeng gao bill dolan diversity promoting tive function neural conversation models ceedings conference north ican chapter association computational linguistics human language technologies pages chin yew lin eduard hovy manual tomatic evaluation summaries proceedings workshop automatic summarization pages hui lin vincent ng abstractive rization survey state art ceedings aaai conference articial ligence volume pages chia wei liu ryan lowe iulian vlad serban mike noseworthy laurent charlin joelle pineau evaluate dialogue system empirical study unsupervised evaluation rics dialogue response generation ings conference empirical methods natural language processing pages chi kiu lo meant accurate semantic mt evaluation output language ings second conference machine tion wmt copenhagen denmark september pages association tional linguistics chi kiu lo michel simard darlene stewart samuel larkin cyril goutte patrick littell accurate semantic textual similarity ing noisy parallel corpora semantic machine translation evaluation metric nrc supervised submissions parallel ltering task proceedings conference machine translation shared task papers wmt gium brussels october november pages association computational guistics ryan lowe michael noseworthy iulian vlad ban nicolas angelard gontier yoshua bengio joelle pineau automatic turing test learning evaluate dialogue responses proceedings annual meeting sociation computational linguistics acl vancouver canada july august volume long papers pages association computational linguistics sidi lu yaoming zhu weinan zhang jun wang yong yu neural text generation past present corr qingsong ma yvette graham shugen wang qun liu blend novel combined mt metric based direct assessment casict dcu sion metrics task proceedings second conference machine translation wmt copenhagen denmark september pages association computational guistics lara j martin prithviraj ammanabrolu xinyu wang william hancock shruti singh brent harrison mark o riedl event representations tomated story generation deep neural nets thirty second aaai conference articial gence luca massarelli fabio petroni aleksandra piktus myle ott tim rocktaschel vassilis plachouras fabrizio silvestri sebastian riedel decoding strategies affect veriability ated text arxiv preprint joao luis zeni montenegro cristiano andre da costa rodrigo da rosa righi survey sational agents health expert systems cations ani nenkova kathleen mckeown mining vey text summarization techniques text data pages springer ani nenkova rebecca j passonneau ating content selection summarization mid method human language technology ference north american chapter ciation computational linguistics hlt naacl boston massachusetts usa pages association computational linguistics van den driessche aaron van den oord yazhe li igor babuschkin karen simonyan oriol vinyals koray kavukcuoglu edward george hart luis c cobo florian stimberg norman casagrande dominik grewe seb noury sander dieleman erich elsen nal kalchbrenner heiga zen alex graves helen king tom walters dan belov demis hassabis parallel wavenet fast speech synthesis proceedings international conference machine learning icml stockholmsmassan holm sweden july volume proceedings machine learning research pages pmlr kishore papineni salim roukos todd ward jing zhu bleu method automatic uation machine translation proceedings annual meeting association tational linguistics july philadelphia pa usa pages acl romain paulus caiming xiong richard socher deep reinforced model abstractive international conference marization learning representations iclr vancouver bc canada april conference track proceedings openreview net rivindu perera parma nand recent vances natural language generation survey classication empirical literature comput formatics matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word sentations proceedings conference north american chapter association computational linguistics human language technologies volume long papers pages shrimai prabhumoye alan w black exploring arxiv preprint lan salakhutdinov lable text generation techniques ratish puduppully li dong mirella lapata data text generation content selection planning proceedings aaai conference articial intelligence volume pages alec radford karthik narasimhan tim salimans ilya sutskever improving language understanding generative pre training colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li peter j liu exploring limits transfer learning unied text text corr kiran ramesh surya ravishankaran abhishek joshi k chandrasekaran survey sign techniques conversational agents ternational conference information cation computing technology pages springer marcaurelio ranzato sumit chopra michael auli wojciech zaremba sequence level ing recurrent neural networks national conference learning representations iclr san juan puerto rico conference track proceedings ehud reiter structured review validity bleu comput linguistics ehud reiter anja belz investigation validity metrics automatically ating natural language generation systems tational linguistics aurko roy ashish vaswani arvind neelakantan niki parmar theory experiments tor quantized autoencoders corr abigail peter j liu christopher d manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers pages thibault sellam dipanjan das ankur p parikh bleurt learning robust metrics text eration corr hiroki shimanaka tomoyuki kajiwara mamoru komachi ruse regressor sentence embeddings automatic machine translation uation proceedings conference machine translation shared task papers wmt belgium brussels october november pages association tional linguistics jonathan slocum survey machine tion history current status future prospects computational linguistics kaitao song xu tan tao qin jianfeng lu yan liu mass masked sequence sequence pre training language generation arxiv preprint milos stanojevic khalil simaan beer proceedings better evaluation ranking ninth workshop statistical machine lation june maryland usa pages ation computer linguistics akhilesh sudhakar bhargav upadhyay arjun heswaran transforming delete retrieve generate approach controlled text style transfer proceedings conference cal methods natural language processing international joint conference natural language processing emnlp ijcnlp hong kong china november pages association computational linguistics yik cheung tam cluster based beam search pointer generator chatbot grounded knowledge computer speech language page oguzhan tas farzad kiyani survey matic text summarization pressacademia procedia lucas theis aaron van den oord matthias bethge note evaluation generative models international conference ing representations iclr san juan puerto rico conference track ings julian togelius georgios n yannakakis kenneth o stanley cameron browne search based procedural content generation taxonomy ieee trans comput intell ai games survey chao tong richard c roberts rita borgo sean p walton robert s laramee kodzo wegba aidong lu yun wang huamin qu qiong luo juan ma storytelling visualization extended survey information ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser illia polosukhin attention need advances neural information cessing systems annual conference neural information processing systems ber long beach usa pages ramakrishna vedantam c lawrence zitnick devi parikh cider consensus based image description evaluation ieee conference puter vision pattern recognition cvpr boston ma usa june pages ieee computer society ashwin k vijayakumar michael cogswell prasath r selvaraju qing sun stefan lee david crandall dhruv batra diverse beam search decoding diverse solutions neural quence models arxiv preprint alex wang kyunghyun cho mike lewis asking answering questions evaluate factual consistency summaries arxiv preprint pidong wang hwee tou ng beam search decoder normalization social media text application machine translation proceedings conference north american ter association computational linguistics human language technologies pages xin wang wenhu chen yuan fang wang william yang wang metrics perfect adversarial reward learning visual storytelling proceedings annual meeting association computational linguistics volume long papers pages xuancong wang hwee tou ng khe chai sim beam search decoder disuency proceedings coling tion international conference computational tics technical papers pages yiren wang fei tian di tao qin chengxiang zhai tie yan liu non autoregressive machine translation auxiliary regularization thirty aaai conference articial telligence aaai thirty innovative applications articial intelligence conference iaai ninth aaai symposium cational advances articial intelligence eaai honolulu hawaii usa january ary pages aaai press sean welleck ilia kulikov stephen roller emily nan kyunghyun cho jason weston ral text generation unlikelihood training international conference learning tations iclr addis ababa ethiopia april openreview net yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al google s neural machine translation system bridging gap arxiv preprint man machine translation jingjing xu xuancheng ren yi zhang qi zeng aoyan cai xu sun skeleton based model promoting coherence sentences proceedings narrative story generation conference empirical methods natural language processing pages jingjing xu xu sun xuancheng ren junyang lin bingzhen wei wei li gan diversity promoting generative adversarial network generating informative diversied text corr zhilin yang zihang dai yiming yang jaime bonell russ r salakhutdinov quoc v le xlnet generalized autoregressive pretraining language understanding advances neural formation processing systems pages lili yao nanyun peng ralph weischedel kevin knight dongyan zhao rui yan write better automatic storytelling proceedings aaai conference articial telligence volume pages mahsa yarmohammadi vivek kumar rangarajan har srinivas bangalore baskaran sankaran incremental segmentation decoding strategies simultaneous translation ings sixth international joint conference natural language processing pages lantao yu weinan zhang jun wang yong yu seqgan sequence generative adversarial proceedings nets policy gradient thirty aaai conference articial gence february san francisco nia usa pages aaai press sina zarrie david schlangen decoding strategies neural referring expression generation proceedings inlg tianyi zhang varsha kishore felix wu kilian q weinberger yoav artzi bertscore uating text generation bert national conference learning representations iclr addis ababa ethiopia april openreview net
