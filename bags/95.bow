a semantic qa based approach for text summarization evaluation ping chen fei wu tong wang wei ding university of massachusetts boston ping edu abstract many natural language processing and computational guistics applications involve the generation of new texts based on some existing texts such as summarization text simplification and machine translation however there has been a serious problem haunting these applications for ades that is how to automatically and accurately assess ity of these applications in this paper we will present some preliminary results on one especially useful and challenging problem in nlp system evaluation how to pinpoint content differences of two text passages especially for large passages such as articles and books our idea is intuitive and very ferent from existing approaches we treat one text passage as a small knowledge base and ask it a large number of tions to exhaustively identify all content points in it by paring the correctly answered questions from two text sages we will be able to compare their content precisely the experiment using duc summarization clearly shows promising results introduction technologies spawned from natural language processing nlp and computational linguistics cl have tally changed how we process share and access mation e search engines and questions answering tems however there has been a serious problem haunting many nlp applications that is how to automatically and accurately assess the quality of these applications in some case evaluation of a nlp task itself has become an active research area itself such as text summarization evaluation the main difficulty for developing such evaluation comes from the diversity of the nlp domain and our insufficient understanding of natural languages and human intelligence in general in this paper we focus on one especially useful and challenging area in nlp evaluation how to cally compare the content of two text passages e graphs articles or even large corpora pinpointing content differences among texts is critical to evaluation of many portant nlp applications such as summarization text gorization text simplification and machine translation not surprisingly many evaluation methods have been proposed but the quality of existing methods themselves is hard to sess in many cases human evaluation must be adopted which is often slow subjective and expensive in this paper we present an intuitive and innovative idea completely ferent from existing methods if we treat one text passage as a small knowledge base can we ask it a large number of questions to exhaustively identify all content points in it by comparing the correctly answered questions from two text passages we can compare their content precisely this idea may seem confusing as circling around the target stead of directly hitting the target however our question answering content evaluation is intuitive and supported by the following insights when we assess someone s understanding on a subject we do not ask him to write down all he knows about the subject instead a list of questions will be asked and rate and objective assessment can be achieved by counting the number of correct answers during this question ing process we can also identify which areas he needs to improve practical operability when assessing the similarity of two texts direct comparison may look natural however with current methods no matter supervised or rule based this direct approach becomes increasingly difficult as we move to larger text passages for example comparing two articles needs to answer the following questions how to align sentences how to semantically represent a sentence how to generate similarity scores without annotated samples or as few as possible to minimize cost how to interpret and evaluate these scores how to find the content differences of two texts easy to interpret many existing methods only generate a single score which illustrates little detail as how an ment measure is generated and offers no help for system provement on the other hand our qa based approach quires minimum manual efforts clearly shows how a ure is calculated and pinpoints exactly the content ences of two text passages in next section we will discuss some existing work tion will show the architecture for our qa based tion approach and experiment results will be presented in section we will provide some insights and findings when we design our evaluation system and conduct experiments in one discussion section we conclude in section related work human evaluations of nlp applications are expensive and slow a fast option is to use crowdsourcing such as amazon mechanical turk to quickly get a large amount of tion results from non expert annotators callison burch lasecki however besides the expense there is little control over the annotation quality automatic semantic evaluation has been studied for ades evaluation systems like rouge lin in marization and bleu papineni in machine tion have been widely adopted as measures yet these methods utilize only shallow features such as n grams and longest common subsequences which suffer from the inherent term specificity muhalcea moreover they usually require gold standard simplifications generated by human annotators as reference and which are subjective expensive and not always available to better represent mantics mikolov developed word embedding els e to semantically encode words and phrases more recent work moves to learn similarity of larger text pieces such as sentences le kiros one fundamental difficulty in embedding models is their high requirement of a large number of text samples as geted text pieces get larger secondly the quality of ity measures generated by these models is often vague and hard to assess usually authors handpick only a few samples or extrinsic evaluation has to be adopted for example mueller shows the following output the similarity of a boy is waving at some young ners from the ocean and a group of men is playing with a ball on the beach is according to the lstm model and according to a dependency tree based model just by looking at this sentence pair it is not clear why is a better similarity estimation than moreover current work on text similarity measurement focuses only on generation of such a single similarity score and largely ignores a more interesting and important issue what are the exact content differences between two text passages although our evaluation method applies to any tion where semantic comparison of texts is needed our rent experiments focus on text summarization evaluation in this section we will only discuss the existing work on marization evaluation automatic text summarization is the process to find the most important content from a document and create a mary in natural language how to automatically evaluate summaries remains a challenging problem jones jing steinberger any such process must be able to comprehend the full document extract the most salient and novel facts check if all main topics are covered in the mary and evaluate the quality of the content wang the problem of co selection measure is that it needs to count the common sentences between a machine summary and one of the human summaries which introduces a bias since they are based on a small number of assessors and a small change of sentences may affect the performance donaway introduced content based measure comparing the term frequency tf vectors of machine summary with the tf vectors of the full text or human summary the score is puted based on bag of words or tf idf model using sine similarity however it is likely the summary vector is sparse compared with the document vector and a summary may use terms that are not frequently used in the full ment an alternative is to use latent semantic indexing lsi to capture semantic topics based on singular value decomposition steinberger unfortunately lsi is expensive to compute and suffers from the polysemy lem louis proposed to use input summary similarity and pseudomodels to assess machine summary without a gold standard other content based measures include common subsequence unit overlap radev amid nenkova basic elements hovy and compression dissimilarity wang rouge recall oriented understudy for gisting ation is perhaps the most widely adopted automatic marization evaluation tool it determines the quality of a summary by comparing it with human summaries using grams word sequences and word pairs lin its put correlates very well with human judgements but rouge is unsuitable to evaluate abstractive tion or summaries with a significant amount of ing ng incorporates word embeddings learned from neural network to rouge there have been other efforts to improve automatic summarization evaluation measures such as the automatically evaluating summaries of peers aesop task in tac the major problem of these methods is the requirement of gold standard summaries and ally only a single score is generated which is hard to pret and does not provide any clue as how a summarization system can be improved after examining existing nlp evaluation methods ing from shallow analysis e n gram to deep semantics e deep neural network word embeddings there still main many major challenges high requirement of manual efforts in general supervised machine learning methods e deep learning or other classification methods need sufficient annotated samples for robust performance which become prohibitively expensive when evaluation moves from sentences to large text passages even with methods using only shallow features e rogue gold standard needs to be provided which is often created by highly trained personnel lack of details in evaluation most existing methods produce only a single score as the evaluation result in evaluation more information is certainly desirable and can significantly help researchers gain more insights and improve their work for example when simplifying a text passage it will be very helpful to pinpoint information differences between simplified text and original text so we will know whether what information is missing from the simplified version in this paper we will present a question answering based content evaluation method that can identify information ferences of different text passages without any manual forts our method can process various text sizes ranging from a sentence a paragraph a document to even a large corpus due to its fundamental nature our work can be plied anywhere comparison of two texts is required ing summarization evaluation text simplification tion and machine translation evaluation a qa based method for semantic comparison of texts our automated evaluation method will leverage two nlp fields question generation qg and question answering qa its architecture is illustrated in figure the main idea is first to generate a large number of questions from an original text passage to exhaustively cover its content and it is reasonable to assume that the original text contains formation to answer these questions to semantically assess the content of a newly generated text passage e a mary a simplified version or a translation to another guage a qa system will use the new passage as the only knowledge source to answer the questions generated from the original text if a question is correctly answered it means that this new text passage contains the same specific piece of information as in the original text although it may be pressed in a different way by examining all correct swers we can have an accurate measure of information tained in the new passage by comparing the questions that can be answered by original text passage but can not be swered by new passage we can pinpoint exactly the content differences between these two text passages question generation qg has been widely used in many fields in a document retrieval system a qg system can be used to construct well formed questions hasan rent qg methods are designed to generate questions with some focus e a query in an ir system main topic in a figure our qa based semantic evaluation system architecture figure customizing a typical qa system for our evaluation approach tutoring system le usually questions are formed by exploiting named entity information and predicate ment structures of sentences then a qg system ranks the questions in two aspects one is the question s relevance to the topic and the subtopics of the original passage and the other is the syntactic similarity of each question with the original passage as a result the system outputs questions with high relevance to the topic of original text passage in the qg step we generated factual questions by ing the grammatical structure labeling the lexical items with name entities or other high level semantic roles e son location time and performing syntactic mations such as subject auxiliary inversion and wh ment heilman we used a different ranking nent in our qg system so that we could generate not only the questions that closely relate to the topic but also the questions covering even minor content points such tions are highly related to the original text on the other hand these questions might not exclusively cover all the literal information or always be well structured due to the difficulty in extracting simplified statements from complicated structures in the original text hence using named entities and predefined templates to generate tions can be alternative way in this qg step we can first apply a named entity recognition method to identify named entities e person name time from a text passage for each identified named entity we will generate a set of questions according to predefined question templates for example there is one text passage born in hodgenville kentucky lincoln grew up on the western frontier in kentucky and indiana if lincoln is recognized as a person questions will be tomatically generated e who is lincoln when was lincoln born where was lincoln born when did lincoln die where did lincoln die these questions are generated without considering cific text passages so it is possible some answers can not be found in the original text passage in this case all questions are still asked to an original text passage and to a generated e summarized simplified translated text passage the difference of the two answer sets will show the content ference of two texts the advantage of this approach is that we do not have to always rely on the quality of questions from a qg system as long as predefined question template is carefully constructed we can obtain questions with good coverage over coverage does not matter and high quality after a large set of questions is generated from original text we need a qa system to check how many questions can be correctly answered using the content from a single text a typical qa system usually includes an figure the qa based summarization evaluation process using duc corpus retrieval component to return a large set of ranked ments that may contain the answer figure shows the chitecture of a customized qa system that will satisfy the needs of this evaluation project after the question cessing step our qa system component does nt pass lated queries to a passage retrieval component instead it uses the queries to search for relevant sentences within a document from which the system will extract answers the change in structure increases difficulties in the question cessing step and answer processing step of the qa system experiment to test our idea we have built a proof of concept system using some existing qg and qa systems for the question generation component we adapted heilman m qg system for the qa component we need our qa system component to be able to answer questions from a single ument instead of using an information retrieval system to return a large set of ranked documents that may contain the answer we take advantage of the open source qa work openephyra by replacing the passage retrieval ponent with a text searching component which searches within one document to test our prototype system we use the corpus from ument understanding conference duc the corpus contains sets of text passages the first set is the original documents divided into topics each topic consists of original documents the second set of texts is the summaries of each topic the summaries were generated by baseline summarization systems participating summarization systems and human summarizers thus in total there are summaries for each topic all of these summaries have been evaluated by human assessors and have been given scores on their content responsiveness and linguistic quality we hypothesized that the content quality of a summary can be measured by the number of questions answered by the qa system given this summary as the only knowledge the whole process of our experiment is shown in figure figure comparison of our evaluation scores and human evaluation scores in the question generation phase we use all the original documents of each topic as input to the qg component as output the qg component generates a large set of questions for each topic the number of generated questions ranges from a few hundred to a few thousand and varies from topic to topic depending on the document length all questions were limited to wh factoid questions that are shorter than a certain threshold in the qa phase we run the qa nent through each generated summarization from each topic for summarizations in each topic we use the set of tions corresponding to this topic as input to the qa system the goal of our automatic evaluation system is to determine the performance of different automatic summarization tems based on the content quality of the summarizations generated by them in this experiment due to time straints we chose to compare the performance of marization systems with system id and these systems are evenly distributed mance wise as evaluated by human assessors to make sure the answers generated by the qa system are mostly correct we set the confidence score of the answers to a very high value if the qa system is not highly positive about the swer to a question it will not answer that question as stated above for each topic we have a set of questions generated by qg component and these questions are used to evaluate summarization systems performance over this topic each summarization system s overall performance is measured by averaging its performance over the topics more specifically to evaluate summarization systems performance over a certain topic we ran the qa system times on this topic each time as input to the qa system we use the same set of questions generated from this topic s original documents but as knowledge source we use ent summaries generated by different summarization tems within each run to measure the performance of a summarization system we calculate the percentage of swered questions among the total questions for that topic given the this summarization system s summary as knowledge source the percentages are later normalized to the range of matching the scores given by the human assessors finally we average the scores of each zation system over the number of topics these average scores are the output of our automatic evaluation system which is the performance measure of each summarization system in the last step we compare the content scores given by human assessors and our system s output score by puting pearson s correlation between automatic evaluation system scores and human assigned mean content scores as shown in figure our performance scores and human score correlate very well to further evaluate the robustness of our approach we ied the parameters used in our system to check how these changes affect the system performance the confidence threshold was set to a very high value or as shown in table to ensure correctness of generated answers and to minimize possibility of generating false answers as shown in table in general our scores and human scores not able to find relations between entities however our current qg system may fail to generate deep and ligent inference questions and discover long distance dependencies our method currently focuses more on how much mation a summary contains rather than the importance of certain information within the original text by using topic words as a separate filter instead if integrating topic words into our qg system we can provide more personalized evaluation as certain information may be more important to a specific user the core focus of our idea is on how to semantically compare two texts which can be a summary and a ument a text and its simplified revised version a text and its translation in another language hence our approach can have broad applications in other nlp tasks such as text simplification and machine tion where evaluation is also very important and lenging although end to end deep neural network methods in nlp become popular due to their good performance our white box style approach has its unique appealing advantages in the evaluation field since humans are ten closely involved in this process and need assess the soundness of evaluation and find clues to improve their nlp system conclusion in this paper we present an innovative semantic evaluation method for various nlp applications by leveraging tion generation and question answering fields our method requires no manual efforts is easy to interpret and trates details about nlp systems being evaluated our periments on text summarization evaluation showed ising results since our focus is on a fundamental issue in nlp how to semantically compare two texts besides marization evaluation we expect that our idea will have broader applications on various nlp tasks such as text plification and machine translation acknowledgements this work is partially funded by nsf grant correlate very well the longer questions can generally more difficult for a qa system and sometimes results in lower performance which is why we limited the question length since summarization tends to keep most important mation not all questions are of the same importance tions covering important information should be given higher priority by question filtering we applied lda to identify topic words of documents and filtered out the questions that do not contain these topic words performance gets better since a summarization system will not be penalized by not covering unimportant information confidence question correlation length question filtering no no yes yes no no yes yes rouge rouge table experiment results discussion in this section we will provide some insights and findings as we design our qa based semantic evaluation system and analyze the experiment results we have manually examined some questions that we generated and found that they are highly related to the text both semantically and structurally the over erating approach helps us to obtain questions that can cover almost all the text content and literal information the average number of questions generated for a topic with documents is when setting the question length to be less than words the large amount of generated questions ensures our system s robustness when a small number of false answers exist in addition the generated questions are able to discover basic entity relations within sentences and between tences for example there are questions like who were assaulted by aryan nations guards who does richard cohen argue to who was co founder of the southern poverty law center what was morris dees the co founder of this fact also suggests the proposed qa based evaluation approach is potentially superior to rouge based measures since rouge is jun ping ng and viktoria abrecht better summarization tion with word rouge arxiv preprint embeddings for dragomir radev simone teufel horacio saggion wai lam john blitzer arda celebi hong qi elliott drabek and danyu liu evaluation of text summarization in a cross lingual information trieval framework center for language and speech processing johns hopkins university baltimore md tech rep kishore papineni salim roukos todd ward and wei jing zhu bleu a method for automatic evaluation of machine translation in proceedings of the annual meeting on association for putational linguistics pp association for computational linguistics josef steinberger karel jeek evaluation measures for text summarization in computing and informatics vol pp tong wang ping chen and dan simovici a new evaluation measure using compression dissimilarity on text summarization applied intelligence references chris callison burch fast cheap and creative evaluating lation quality using amazon mechanical turk in proceedings of the conference on empirical methods in natural language processing volume volume pp association for computational linguistics robert l donaway kevin w drummey and laura a mather a comparison of rankings produced by summarization evaluation measures in proceedings of the naacl anlpworkshop on automatic summarization volume pp association for computational linguistics hasan yllias chali sadid a towards automatic topical question generation proceedings of coling heilman michael automatic factual question generation from text diss carnegie mellon university eduard hovy chin yew lin liang zhou and junichi fukumoto automated summarization evaluation with basic elements in ceedings of the fifth conference on language resources and uation lrec pp genoa italy hongyan jing regina barzilay kathleen mckeown and michael elhadad summarization evaluation methods experiments and analysis in aaai symposium on intelligent summarization pp ks jones and julia r galliers evaluating natural language cessing systems an analysis and review vol springer ence business media kiros r zhu y salakhutdinov r zemel r s torralba a urtasun r and fidler s skip thought vectors nips walter s lasecki luz rello and jeffrey p bigham measuring text simplification with the crowd in proceedings of the web for all conference p acm le q and mikolov t distributed representations of sentences and documents icml chin yew lin rouge a package for automatic evaluation of maries in text summarization branches out proceedings of the workshop vol annie louis and ani nenkova automatically assessing machine summary content without a gold standard computational tics mikolov t sutskever i chen k corrado g and dean j distributed representations of words and phrases and their compositionality nips mihalcea r corley c and strapparava c corpusbased and knowledge based measures of text semantic similarity aaai conference on artificial intelligence mueller j and thyagarajan a siamese recurrent architectures for learning sentence similarity proceedings of the aaai conference on artificial intelligence aaai ani nenkova and rebecca j passonneau evaluating content lection in summarization the pyramid method in hlt naacl vol pp
