semantic qa based approach text summarization evaluation ping chen fei wu tong wang wei ding university massachusetts boston ping edu abstract natural language processing computational guistics applications involve generation new texts based existing texts summarization text simplification machine translation problem haunting applications ades automatically accurately assess ity applications paper present preliminary results especially useful challenging problem nlp system evaluation pinpoint content differences text passages especially large passages articles books idea intuitive ferent existing approaches treat text passage small knowledge base ask large number tions exhaustively identify content points paring correctly answered questions text sages able compare content precisely experiment duc summarization clearly shows promising results introduction technologies spawned natural language processing nlp computational linguistics cl tally changed process share access mation e search engines questions answering tems problem haunting nlp applications automatically accurately assess quality applications case evaluation nlp task active research area text summarization evaluation main difficulty developing evaluation comes diversity nlp domain insufficient understanding natural languages human intelligence general paper focus especially useful challenging area nlp evaluation cally compare content text passages e graphs articles large corpora pinpointing content differences texts critical evaluation portant nlp applications summarization text gorization text simplification machine translation surprisingly evaluation methods proposed quality existing methods hard sess cases human evaluation adopted slow subjective expensive paper present intuitive innovative idea completely ferent existing methods treat text passage small knowledge base ask large number questions exhaustively identify content points comparing correctly answered questions text passages compare content precisely idea confusing circling target stead directly hitting target question answering content evaluation intuitive supported following insights assess s understanding subject ask write knows subject instead list questions asked rate objective assessment achieved counting number correct answers question ing process identify areas needs improve practical operability assessing similarity texts direct comparison look natural current methods matter supervised rule based direct approach increasingly difficult larger text passages example comparing articles needs answer following questions align sentences semantically represent sentence generate similarity scores annotated samples possible minimize cost interpret evaluate scores find content differences texts easy interpret existing methods generate single score illustrates little detail ment measure generated offers help system provement hand qa based approach quires minimum manual efforts clearly shows ure calculated pinpoints exactly content ences text passages section discuss existing work tion architecture qa based tion approach experiment results presented section provide insights findings design evaluation system conduct experiments discussion section conclude section related work human evaluations nlp applications expensive slow fast option use crowdsourcing amazon mechanical turk quickly large tion results non expert annotators callison burch lasecki expense little control annotation quality automatic semantic evaluation studied ades evaluation systems like rouge lin marization bleu papineni machine tion widely adopted measures methods utilize shallow features n grams longest common subsequences suffer inherent term specificity muhalcea usually require gold standard simplifications generated human annotators reference subjective expensive available better represent mantics mikolov developed word embedding els e semantically encode words phrases recent work moves learn similarity larger text pieces sentences le kiros fundamental difficulty embedding models high requirement large number text samples geted text pieces larger secondly quality ity measures generated models vague hard assess usually authors handpick samples extrinsic evaluation adopted example mueller shows following output similarity boy waving young ners ocean group men playing ball beach according lstm model according dependency tree based model looking sentence pair clear better similarity estimation current work text similarity measurement focuses generation single similarity score largely ignores interesting important issue exact content differences text passages evaluation method applies tion semantic comparison texts needed rent experiments focus text summarization evaluation section discuss existing work marization evaluation automatic text summarization process find important content document create mary natural language automatically evaluate summaries remains challenging problem jones jing steinberger process able comprehend document extract salient novel facts check main topics covered mary evaluate quality content wang problem co selection measure needs count common sentences machine summary human summaries introduces bias based small number assessors small change sentences affect performance donaway introduced content based measure comparing term frequency tf vectors machine summary tf vectors text human summary score puted based bag words tf idf model sine similarity likely summary vector sparse compared document vector summary use terms frequently ment alternative use latent semantic indexing lsi capture semantic topics based singular value decomposition steinberger unfortunately lsi expensive compute suffers polysemy lem louis proposed use input summary similarity pseudomodels assess machine summary gold standard content based measures include common subsequence unit overlap radev amid nenkova basic elements hovy compression dissimilarity wang rouge recall oriented understudy gisting ation widely adopted automatic marization evaluation tool determines quality summary comparing human summaries grams word sequences word pairs lin correlates human judgements rouge unsuitable evaluate abstractive tion summaries significant ing ng incorporates word embeddings learned neural network rouge efforts improve automatic summarization evaluation measures automatically evaluating summaries peers aesop task tac major problem methods requirement gold standard summaries ally single score generated hard pret provide clue summarization system improved examining existing nlp evaluation methods ing shallow analysis e n gram deep semantics e deep neural network word embeddings main major challenges high requirement manual efforts general supervised machine learning methods e deep learning classification methods need sufficient annotated samples robust performance prohibitively expensive evaluation moves sentences large text passages methods shallow features e rogue gold standard needs provided created highly trained personnel lack details evaluation existing methods produce single score evaluation result evaluation information certainly desirable significantly help researchers gain insights improve work example simplifying text passage helpful pinpoint information differences simplified text original text know information missing simplified version paper present question answering based content evaluation method identify information ferences different text passages manual forts method process text sizes ranging sentence paragraph document large corpus fundamental nature work plied comparison texts required ing summarization evaluation text simplification tion machine translation evaluation qa based method semantic comparison texts automated evaluation method leverage nlp fields question generation qg question answering qa architecture illustrated figure main idea generate large number questions original text passage exhaustively cover content reasonable assume original text contains formation answer questions semantically assess content newly generated text passage e mary simplified version translation guage qa system use new passage knowledge source answer questions generated original text question correctly answered means new text passage contains specific piece information original text pressed different way examining correct swers accurate measure information tained new passage comparing questions answered original text passage swered new passage pinpoint exactly content differences text passages question generation qg widely fields document retrieval system qg system construct formed questions hasan rent qg methods designed generate questions focus e query ir system main topic figure qa based semantic evaluation system architecture figure customizing typical qa system evaluation approach tutoring system le usually questions formed exploiting named entity information predicate ment structures sentences qg system ranks questions aspects question s relevance topic subtopics original passage syntactic similarity question original passage result system outputs questions high relevance topic original text passage qg step generated factual questions ing grammatical structure labeling lexical items entities high level semantic roles e son location time performing syntactic mations subject auxiliary inversion wh ment heilman different ranking nent qg system generate questions closely relate topic questions covering minor content points tions highly related original text hand questions exclusively cover literal information structured difficulty extracting simplified statements complicated structures original text named entities predefined templates generate tions alternative way qg step apply named entity recognition method identify named entities e person time text passage identified named entity generate set questions according predefined question templates example text passage born hodgenville kentucky lincoln grew western frontier kentucky indiana lincoln recognized person questions tomatically generated e lincoln lincoln born lincoln born lincoln die lincoln die questions generated considering cific text passages possible answers found original text passage case questions asked original text passage generated e summarized simplified translated text passage difference answer sets content ference texts advantage approach rely quality questions qg system long predefined question template carefully constructed obtain questions good coverage coverage matter high quality large set questions generated original text need qa system check questions correctly answered content single text typical qa system usually includes figure qa based summarization evaluation process duc corpus retrieval component return large set ranked ments contain answer figure shows chitecture customized qa system satisfy needs evaluation project question cessing step qa system component nt pass lated queries passage retrieval component instead uses queries search relevant sentences document system extract answers change structure increases difficulties question cessing step answer processing step qa system experiment test idea built proof concept system existing qg qa systems question generation component adapted heilman m qg system qa component need qa system component able answer questions single ument instead information retrieval system return large set ranked documents contain answer advantage open source qa work openephyra replacing passage retrieval ponent text searching component searches document test prototype system use corpus ument understanding conference duc corpus contains sets text passages set original documents divided topics topic consists original documents second set texts summaries topic summaries generated baseline summarization systems participating summarization systems human summarizers total summaries topic summaries evaluated human assessors given scores content responsiveness linguistic quality hypothesized content quality summary measured number questions answered qa system given summary knowledge process experiment shown figure figure comparison evaluation scores human evaluation scores question generation phase use original documents topic input qg component output qg component generates large set questions topic number generated questions ranges thousand varies topic topic depending document length questions limited wh factoid questions shorter certain threshold qa phase run qa nent generated summarization topic summarizations topic use set tions corresponding topic input qa system goal automatic evaluation system determine performance different automatic summarization tems based content quality summarizations generated experiment time straints chose compare performance marization systems system id systems evenly distributed mance wise evaluated human assessors sure answers generated qa system correct set confidence score answers high value qa system highly positive swer question answer question stated topic set questions generated qg component questions evaluate summarization systems performance topic summarization system s overall performance measured averaging performance topics specifically evaluate summarization systems performance certain topic ran qa system times topic time input qa system use set questions generated topic s original documents knowledge source use ent summaries generated different summarization tems run measure performance summarization system calculate percentage swered questions total questions topic given summarization system s summary knowledge source percentages later normalized range matching scores given human assessors finally average scores zation system number topics average scores output automatic evaluation system performance measure summarization system step compare content scores given human assessors system s output score puting pearson s correlation automatic evaluation system scores human assigned mean content scores shown figure performance scores human score correlate evaluate robustness approach ied parameters system check changes affect system performance confidence threshold set high value shown table ensure correctness generated answers minimize possibility generating false answers shown table general scores human scores able find relations entities current qg system fail generate deep ligent inference questions discover long distance dependencies method currently focuses mation summary contains importance certain information original text topic words separate filter instead integrating topic words qg system provide personalized evaluation certain information important specific user core focus idea semantically compare texts summary ument text simplified revised version text translation language approach broad applications nlp tasks text simplification machine tion evaluation important lenging end end deep neural network methods nlp popular good performance white box style approach unique appealing advantages evaluation field humans closely involved process need assess soundness evaluation find clues improve nlp system conclusion paper present innovative semantic evaluation method nlp applications leveraging tion generation question answering fields method requires manual efforts easy interpret trates details nlp systems evaluated periments text summarization evaluation showed ising results focus fundamental issue nlp semantically compare texts marization evaluation expect idea broader applications nlp tasks text plification machine translation acknowledgements work partially funded nsf grant correlate longer questions generally difficult qa system results lower performance limited question length summarization tends important mation questions importance tions covering important information given higher priority question filtering applied lda identify topic words documents filtered questions contain topic words performance gets better summarization system penalized covering unimportant information confidence question correlation length question filtering yes yes yes yes rouge rouge table experiment results discussion section provide insights findings design qa based semantic evaluation system analyze experiment results manually examined questions generated found highly related text semantically structurally erating approach helps obtain questions cover text content literal information average number questions generated topic documents setting question length words large generated questions ensures system s robustness small number false answers exist addition generated questions able discover basic entity relations sentences tences example questions like assaulted aryan nations guards richard cohen argue co founder southern poverty law center morris dees co founder fact suggests proposed qa based evaluation approach potentially superior rouge based measures rouge jun ping ng viktoria abrecht better summarization tion word rouge arxiv preprint embeddings dragomir radev simone teufel horacio saggion wai lam john blitzer arda celebi hong qi elliott drabek danyu liu evaluation text summarization cross lingual information trieval framework center language speech processing johns hopkins university baltimore md tech rep kishore papineni salim roukos todd ward wei jing zhu bleu method automatic evaluation machine translation proceedings annual meeting association putational linguistics pp association computational linguistics josef steinberger karel jeek evaluation measures text summarization computing informatics vol pp tong wang ping chen dan simovici new evaluation measure compression dissimilarity text summarization applied intelligence references chris callison burch fast cheap creative evaluating lation quality amazon mechanical turk proceedings conference empirical methods natural language processing volume volume pp association computational linguistics robert l donaway kevin w drummey laura mather comparison rankings produced summarization evaluation measures proceedings naacl anlpworkshop automatic summarization volume pp association computational linguistics hasan yllias chali sadid automatic topical question generation proceedings coling heilman michael automatic factual question generation text diss carnegie mellon university eduard hovy chin yew lin liang zhou junichi fukumoto automated summarization evaluation basic elements ceedings fifth conference language resources uation lrec pp genoa italy hongyan jing regina barzilay kathleen mckeown michael elhadad summarization evaluation methods experiments analysis aaai symposium intelligent summarization pp ks jones julia r galliers evaluating natural language cessing systems analysis review vol springer ence business media kiros r zhu y salakhutdinov r zemel r s torralba urtasun r fidler s skip thought vectors nips walter s lasecki luz rello jeffrey p bigham measuring text simplification crowd proceedings web conference p acm le q mikolov t distributed representations sentences documents icml chin yew lin rouge package automatic evaluation maries text summarization branches proceedings workshop vol annie louis ani nenkova automatically assessing machine summary content gold standard computational tics mikolov t sutskever chen k corrado g dean j distributed representations words phrases compositionality nips mihalcea r corley c strapparava c corpusbased knowledge based measures text semantic similarity aaai conference artificial intelligence mueller j thyagarajan siamese recurrent architectures learning sentence similarity proceedings aaai conference artificial intelligence aaai ani nenkova rebecca j passonneau evaluating content lection summarization pyramid method hlt naacl vol pp
