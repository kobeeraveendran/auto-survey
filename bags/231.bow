efciency metrics data driven models text summarization case study erion ano institute formal applied linguistics charles university prague czech republic mff cuni ondrej bojar institute formal applied linguistics charles university prague czech republic mff cuni abstract data driven models solving text summarization similar tasks common years studies report basic accuracy scores known ability proposed models improve trained data paper dene pose data efciency metrics data score efciency data time deciency overall data efciency propose simple scheme uses metrics apply comprehensive evaluation ular methods text summarization title generation tasks task cess release huge collection million abstract title pairs scientic articles results reveal tested models transformer efcient tasks introduction text summarization process distilling noteworthy information document produce abridged version task earning considerable interest shorter sions long documents easier read save time basic ways marize texts extractive summarization selects relevant parts source document combines generate summary case summary contains exact copies words phrases picked source abstractive summarization hand paraphrases information required summary instead copying verbatim usually better complex harder achieve rapid progress ats stractive text summarization years vanilla encoder decoder bidirectional lstms hochreiter schmidhuber enhanced advanced mechanisms like tention bahdanau allows model focus widely embraced parts input ation phase successfully rush summarize news articles pointing copying mechanism helps leviate problem unknown words gulcehre coverage intra attention paulus proposed utilized avoid word repetitions producing readable summaries reinforcement learning concepts like policy gradient rennie recently bined encoder decoder architecture viating problems like train test inconsistency exposure bias paulus chen bansal developments helped boost ats rouge lin scores rush paulus increase roughly years studies evaluate methods datasets xed size ing tell expected models trained data training time rarely reported believe evaluation practice driven models incomplete data efciency metrics computed reported paper propose data efciency metrics data score efciency data time deciency overall data efciency rst represent output quality gain ing time delay model additional data samples ratio reects overall efciency models training data suggest simple scheme considers values metrics basic accuracy score use performance solely output quality time needed train model obtain output stead reporting proposed scheme metrics tailed evaluation supervised learning models examine recently text posed methods tasks tion popular cnndm cnn daily mail nallapati dataset title tion scientic articles oags novel dataset abstract title pairs processed released according results performing fastest methods datasets paulus chen bansal score time ciency transformer vaswani tinctly superior future examine transformer model data different rameter setups applying evaluation scheme related tasks machine translation benecial overall work brings following main contributions dene propose data efciency metrics simple evaluation scheme uses comprehensive tion data driven learning methods use scheme metrics benchmark recently proposed ats methods discuss training times rouge data efciency scores finally huge collection million scientic paper abstracts titles pared released community best knowledge largest data collection pared title generation experiments data efciency metrics related work training data efciency data driven ing models little considered literature early work lawrence investigate generalization ability neural works respect complexity proximation function size network degree noise training data case factor vary size ing data levels gaussian noise added data concluding ensemble techniques immune increased noise levels performance variations training data sizes considered jarrah review research erature focusing computational energy handle efciency data driven methods ticularly consider data intensive application areas big data computing sustainable data models help maximal learning curacy minimal computational cost cient processing large volumes data boom examine character level rnn recurrent neural network predict character text given previous characters assess evolution network performance terms perplexity train prediction scenarios function training time input training sequences cording results efciency model considerably inuenced chosen scenario similar experiment conducted riou explore reinforcement learning cepts task neural language generation compare different implementations reporting performance scores tion function cumulated learning cost training data size relevant work found hlynsson propose mental protocol comparing data efciency cnn convolution neural network higsfa hierarchical information preserving graph based slow feature analysis informal denition data efciency ering performance function training set size character recognition challenges dened methods trained increasing amounts data samples reporting corresponding accuracy scores proposed data efciency metrics despite experimental results insights bring studies task method specic computation schemes generic transferable formalization data efciency given section novel useful data efciency metrics suppose train data driven method dataset solve task test based performance score assume quality data samples different intervals homogeneous practice achieved shufing starting iments certain training data size takes seconds train model convergence gains observed training time score obtained testing standard independent test dataset xed size expect certain crease training samples fed quire extra time converge ing model attain extra score dene compute data score ciency score gain additional data samples method measure smartly effectively terprets extra data samples formance score scales training data larly data time deciency inverse data time efciency measures slowly lazily interprets additional samples given train test runs original enlarged datasets characterized measures training data training times achieved scores dene overall data efciency measure smartly quickly models utilizes data task practice absolute increments produce small values hard interpret work use training times depend computing conditions hardware setups result hardly reproducible different computing environments overcome limitations instead use relative increments computing corresponding relative data efciency metrics data time efciency fused training throughput dened popel bojar machine translation reects time required model update given additional data increase overall training time till convergence enlarged dataset comparison original relative metrics values cally easier interpret work transferable reproducible ferent computing setups important cross interpretation experimental results express values percent values ratio assorted remarks metrics presented ate different data driven methods compare eral parameter congurations basic method algorithm neural network help optimal sense generic task independent portant note represent sal global attributes method stead linear approximations local characterizations certain intervals words high values intervals necessarily assure decent generalization important confuse data ciency performance quality daily intuition tend consider highly cient machines techniques methods performing ones instead according denitions model perform poorly highly efcient training data happens performance scores increasing training data cuts low grow quickly assessment model yield high scores grow slowly increasing data sizes relatively small values case performing maybe best model data data efcient data efciency viewpoint best models obviously higher data score efciency lower data time deciency higher overall data efciency practice mance generally desired characteristic result data score efciency values important worthy port cases models trained relevant ertheless useful technical theoretical perspective paring different methods comparing different rameter congurations method trying run time optimizations comprehensive evaluation scheme sizes predictive models utilized datasets consistently growing comes difcult costly use human pertise evaluation typical approach test automatically means standard datasets scoring metrics popular ample case text summarization task common evaluations proposed methods set cnndm ble paulus table lin table believe shortcomings evaluation practice testing model method trained xed size data split reveal score trend fed data hard discern overall best method pared fair objective way especially achieved scores similar ing time rarely reported known time efciency models overcome limitations propose detailed evaluation scheme considers accuracy scores data efciency metrics dened section suppose dataset size homogeneous ing samples standard performance score methods want compare typical practice trains single models entire reports accuracy scores standard test set instead suggest split equal parts test set models size form intervals increasing sizes way train models compute scores equation compute scores models report score values relative data score efciency values conciseness limit biggest models given local nature efciency metrics sense report values dispersed data intervals like leftmost rightmost values middle figure illustration schema application probably relevant predicting score trend bigger training sizes pute report respective values values similar fashion equations section getting rst check distinctly higher comparing values sential real worth comes contrasting rightmost corresponding values signicant difference suggest reach higher scores bigger training set illustrate figure pothetical graphs approximate variations training size similar performance scores obviously grows faster till situation reversed expect actually happens example traditional practice computing verdict form instead scheme conclude form probably perform better trained data scheme evaluate data driven ods different scores different tasks section results obtained plying assess advanced ats methods text summarization datasets tendency data driven methods based neural networks encouraged experiments large text collections tasks case ats rst big datasets annotated english gigaword napoles rush million news ticles headlines processed corenlp split valid test valid test rec srcl tgtl voc table statistics datasets split shows number records rec average length source target texts tokens srcl tgtl total cabulary size voc number frequent words manning headline paired rst sentence corresponding cle create training base experiments evaluation baseline given small size sists document summary pairs curated human experts newsroom recent geneous bundle million news articles grusky cnndm popular dataset text summarization nallapati provides large set news articles responding multi sentence summaries unlike contain sentence summaries suitable training ing summarization models longer texts title generation task hand requires data samples shorter texts sentence tles collections abstracts titles tic articles suited exploring collection records scientic paper metadata title abstract keywords presented meng metadata belong ticles computer science acm digital brary sciencedirect web science demand data tivated initiatives research articles academic networks arnetminer system extracts researcher proles web integrates data unied network tang byproduct work oag open academic graph collection sinha produce big title generation dataset experiments started oag abstract nist gov title language elds extracted record available cases abstract language match language eld ignored language remove records english duplicates dropped texts ercased finally stanford corenlp tokenizer split title abstract texts ing dataset oags released paper tains million abstract title pairs title generation experiments quick look content oags observed papers medicine papers cial sciences psychology economics ing disciplines given huge size ical richness value oags twofold supplement existing datasets title generation tasks training data needed creating byproducts specic scientic disciplines domains text summarization evaluation section apply relative metrics section evaluation scheme section benchmark advanced methods text summarization news articles title tion scientic papers rst introduce methods parameters experimental data later present discuss achieved accuracy data efciency scores tested summarization methods ability recurrent neural networks resent process variable length sequences created tradition applying sequence tasks ats case ats goal process source text ducing target text shorter ingful easy read rush probably rst plement attention network dedicated ats model abs following uses coder learns soft alignment attention tween source target sequences ing context vector decoding phase uses beam search decoder dahlmeier window candidate words target position mensions hidden layer word embedding layer respectively authors reported state art results testing dataset proposed pointer generator pcov model implements based encoder producing context vector decoder extended pointing copying mechanism gulcehre step compute generation probability pgen context vector coder states decoder output step generation probability switch decide word predicted copied input extension coverage mechanism keeping track decoder outputs avoiding word repetitions mary chronic problem encoder decoder marizers method mented word embeddings hidden layer sizes respectively lin tried partial use lutions model globen avoid word repetitions semantic irrelevance maries couple encoder tional gated unit performs global encoding source context uses lter certain gram features rene output encoder time step globen big network parameters cnndm layers encoder coder dimensions taxonomy sequence methods added mechanisms found shi authors present detailed review problems proposed tions based network structures training gies generation algorithms furthermore develop release library nats plements combinations mechanisms like tion pointing coverage analyzing fects text summarization quality nats plemented network parameters pcov intra decoder attention weight sharing embeddings added decoder introduction transformer trans architecture removes recurrent lutional structures reduced computation cost training time vaswani totally based attention mechanism primarily designed transformer work text marization needs learn alignments input source texts output target summaries positional ing added word embeddings preserve order input output sequences trans biggest model tried layers encoder decoder dimensions layer including embedding layers ing steps warm steps observed problems encoder decoder framework exposure bias train test consistency keneshloo overcome ideas recently applied paulus use intra attention focus ent parts encoded sequence way likely model pgrl attend parts input different decoding steps fewer word repetitions appear summaries optimize rouge similar discrete evaluation metrics implement critical policy gradient training reward tion mechanism introduced rennie pgrl encoder decoder dimensions word embeddings dimensions aiming speed chen bansal veloped extractive abstractive text summarizer fastrl policy based reinforcement rst uses extractor agent pick salient sentences phrases instead encoding tire input sequence long uses encoder decoder abstractor rewrite press sentences parallel actor critic icy gradient reward function bahdanau joins extractor abstractor networks models fastrl uses dimensions recurrent layer word embeddings experiment pretraining word embeddings performed learned training model adam timizer kingma chose mini batches size cases globen trans avoid ory errors experiments conducted nvidia gtx gpus data cope limited computing resources records oags ments picked scheme section created splits samples authors rush shi lin vaswani chen paulus model cnndm oags table parameters rouge scores training times method splits datasets abs pcov nats globen trans fastrl pgrl figure score trends models method cnndm left oags right splits thirds cnndm statistics experimental data shown table vocabulary sizes ment shown column higher vocabulary sizes oags splits cause signicant difference parameters tween corresponding models method table transformer models grows cnndm oags difference sets experiments maximal number ing decoding steps words source target texts cnndm tively oags chose abstracts titles longer summarization results rouge scores training times seconds cnndm experiments shown dle table accurate models pgrl fastrl implement policy based training optimize rouge scores worst performer abs fall reaching similar scores score differences model rst usually small methods believe way rouge scores authors rush shi lin vaswani chen paulus models cnndm table data efciency scores models cnndm experiments computed based ing score similarly computed based authors rush vaswani paulus models oags table data efciency scores models oags experiments computed based corresponding score similarly computed based computed graphical representation trends method depicted figure left shown behave similarly results oags listed right table run models oags data extractive fastrl easily adapted perform word level extraction oags abstracts furthermore nats globen ran memory frequently remaining pgrl accurate trans follows abs weakest score trends shown figure right training speed cnndm fastrl absolutely best siderable difference second pgrl slowest globen training times higher fastrl fact took days train globen cnndm data oags training times lower cnndm ones oags data splits times ger number training samples happens oags source target samples ally shorter pcov fastest trans slowest efciency results equations section computed relative efciency metrics method values cnndm experiments shown table trans clearly efcient highest lowest highest scores grow quickly despite relatively low training times grow slowly despite high data intervals pcov globen manifest slowest racy score gains lowest globen comes second time efciency nats hand time inefcient highest lowest oags scores table reect similar tion trans leads pcov worst values models appear easy explain high score time efciency trans globen time cient score efcient biggest highest number parameters est layers networks tried sive feature trans lack recurrent structure globen use rnn certain phase hasty infer recurrent networks hinder score efciency attention boosts intuitive explanation fact general performance deeper networks scales better data capacity networks faster interpreting large additions training samples low fact layers bigger training datasets driven progress deep learning lutions application areas plan investigate issue future step run experiments bigger data sizes smaller data vals checking point accuracy scores growing transformer implementations varying number layers parameter tups examined investigating data efciency similar tions tasks like question answering reia standard datasets squad rajpurkar valuable conclusions paper dened data efciency rics better evaluation data driven learning models proposed simple scheme computing reporting addition basic accuracy scores text summarization tle generation tasks chosen case study insights proposed scheme rics reveal title generation cessed dataset million scientic titles abstracts released paper applied seven recent ats methods tasks according results ods mix concepts encoder decoder framework fastest accurate surprising result excellent efciency popular transformer model future work want perform similar studies analogous tasks like like investigate depth transformer model acknowledgments research work supported project national mobility researchers charles versity operational programme research development education project czech science dation elitr references jarrah paul yoo sami muhaidat george karagiannidis kamal taha efcient machine learning big data review corr dzmitry bahdanau philemon brakel kelvin anirudh goyal ryan lowe joelle pineau aaron courville yoshua bengio actor critic algorithm sequence prediction arxiv prints dzmitry bahdanau kyunghyun cho yoshua neural machine translation corr bengio jointly learning align translate cedric boom sam leroux steven bohez pieter simoens thomas demeester bart dhoedt efciency evaluation character level rnn training schedules corr yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proceedings annual ing association computational linguistics volume long papers pages tion computational linguistics alvaro henrique chaim correia jorge luiz eira silva thiago castro martins fbio gagliardi cozman fully based information retriever corr daniel dahlmeier hwee tou search decoder grammatical error correction proceedings joint conference empirical methods natural language ing computational natural language learning emnlp conll pages stroudsburg usa association computational tics max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies proceedings conference north american chapter association computational linguistics man language technologies pages ciation computational linguistics jiatao zhengdong hang victor incorporating copying mechanism proceedings sequence sequence learning annual meeting association putational linguistics volume long papers pages berlin germany association computational linguistics ramesh nallapati bowen zhou cicero dos santos caglar gulcehre bing xiang stractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning pages association computational linguistics caglar gulcehre sungjin ahn ramesh nallapati bowen zhou yoshua bengio pointing proceedings unknown words annual meeting association tional linguistics volume long papers pages berlin germany association tational linguistics hlynur dav hlynsson alberto escalante laurenz wiskott measuring data ciency deep learning methods ings international conference pattern recognition applications methods volume icpram pages insticc scitepress sepp hochreiter jrgen schmidhuber long short term memory neural computation yaser keneshloo tian shi naren ramakrishnan chandan reddy deep reinforcement learning sequence sequence models corr diederik kingma jimmy adam method stochastic optimization cite comment published ence paper international conference learning representations san diego steve lawrence lee giles chung tsoi size neural network gives optimal eralization convergence properties tion technical report chin yew lin rouge package automatic evaluation summaries proc acl workshop text summarization branches page junyang lin sun shuming global encoding abstractive tion proceedings annual meeting association computational linguistics ume short papers pages association computational linguistics christopher manning mihai surdeanu john bauer jenny finkel steven bethard david closky stanford corenlp natural guage processing toolkit association tational linguistics acl system demonstrations pages rui meng sanqiang zhao shuguang han daqing peter brusilovsky chi deep proceedings keyphrase generation annual meeting association tional linguistics pages association computational linguistics courtney napoles matthew gormley benjamin van durme annotated gigaword ceedings joint workshop automatic edge base construction web scale knowledge extraction akbc wekex pages stroudsburg usa association tional linguistics romain paulus caiming xiong richard socher deep reinforced model abstractive marization corr martin popel ond zej bojar training tips transformer model prague bulletin mathematical linguistics pranav rajpurkar robin jia percy liang know know unanswerable proceedings tions squad nual meeting association computational linguistics volume short papers pages melbourne australia association tational linguistics steven rennie etienne marcheret youssef mroueh jarret ross vaibhava goel self critical sequence training image captioning ieee conference computer vision pattern nition cvpr pages matthieu riou bassam jabaian stphane huet fabrice lefvre reinforcement adaptation attention based neural natural language tor spoken dialogue systems dialogue course alexander rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages association computational linguistics abigail peter liu christopher manning point summarization generator networks proceedings nual meeting association computational linguistics pages association putational linguistics tian shi yaser keneshloo naren ramakrishnan chandan reddy neural abstractive text summarization sequence sequence models corr arnab sinha zhihong shen yang song hao rin eide june paul hsu kuansan wang overview microsoft academic vice mas applications proceedings international conference world wide web www companion pages new york usa acm jie tang jing zhang limin yao juanzi zhang zhong arnetminer extraction mining academic social networks proceedings acm sigkdd international conference knowledge discovery data mining kdd pages new york usa acm zhaopeng zhengdong yang liu xiaohua liu hang modeling coverage neural machine translation proceedings nual meeting association computational linguistics pages association tional linguistics ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need guyon luxburg bengio wallach fergus vishwanathan nett editors advances neural information cessing systems pages curran sociates inc
