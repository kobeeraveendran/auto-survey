neural extractive text summarization with syntactic compression jiacheng xu and greg durrett department of computer science the university of texas at austin jcxu utexas edu p e s l c s c v v i x r a abstract recent neural network approaches to rization are largely either selection based traction or generation based abstraction in this work we present a neural model for single document summarization based on joint extraction and syntactic compression our model chooses sentences from the document identies possible compressions based on stituency parses and scores those sions with a neural model to produce the nal summary for learning we construct acle extractive compressive summaries then learn both of our components jointly with this supervision experimental results on the cnn daily mail and new york times datasets show that our model achieves strong performance comparable to state of the art systems as evaluated by rouge over our approach outperforms an off shelf compression module and human and manual evaluation shows that our model s put generally remains grammatical introduction neural network approaches to document marization have ranged from purely extractive cheng and lapata nallapati et al narayan et al to abstractive rush et al nallapati et al chopra et al tan et al gehrmann et al tractive systems are robust and straightforward to use abstractive systems are more exible for ied summarization situations grusky et al but can make factual errors cao et al li et al or fall back on extraction in practice see et al extractive and compressive tems berg kirkpatrick et al qian and liu durrett et al combine the strengths of both approaches however there has been tle work studying neural network models in this vein and the approaches that have been employed figure diagram of the proposed model extraction and compression are modularized but jointly trained with supervision derived from the reference summary typically use based sentence compression chen and bansal in this work we propose a model that can bine the high performance of neural extractive systems additional exibility from compression and interpretability given by having discrete pression options our model rst encodes the source document and its sentences and then quentially selects a set of sentences to further press each sentence has a set of compression tions available that are selected to preserve ing and grammaticality these are derived from syntactic constituency parses and represent an panded set of discrete options from prior work berg kirkpatrick et al wang et al the neural model additionally scores and chooses which compressions to apply given the context of the document the sentence and the decoder model s recurrent state a principal challenge of training an extractive and compressive model is constructing the cle summary for supervision we identify a set of high quality sentences from the document with beam search and derive oracle compression labels in each sentence through an additional renement process our model s training objective combines these extractive and compressive components and learns them jointly we conduct experiments on standard single document news summarization datasets cnn daily mail hermann et al and the new compression module extraction module input documentoutput summary ref compression rules we refer to the rules rived in li et al wang et al and durrett et al and design a concise set of syntactic rules including the removal of itive noun phrases relative clauses and bial clauses adjective phrases in noun phrases and adverbial phrases see figure dive verb phrases as part of noun phrases see ure prepositional phrases in certain rations like on monday content within these s and other parentheticals figure shows examples of several sion rules applied to a short snippet all binations of compressions maintain ity though some content is fairly important in this context the vp and pp and should not be deleted our model must learn not to delete these elements compressability summaries from different sources may feature various levels of sion at one extreme a summary could be fully sentence extractive at another extreme the editor may have compressed a lot of content in a tence in section we examine this question on our summarization datasets and use it to motivate our choice of evaluation datasets universal compression with rouge while we use syntax as a source of compression options we note that other ways of generating compression options are possible including using labeled pression data however supervising compression with rouge is critical to learn what information is important for this particular source and in any case labeled compression data is unavailable in in section we compare our many domains model to off the shelf sentence compression ule and nd that it substantially underperforms our approach model our model is a neural network model that encodes a source document chooses sentences from that document and selects discrete compression tions to apply the model architecture of sentence extraction module and text compression module are shown in figure and extractive sentence selection a single document consists of n sentences d sn the i th sentence is denoted as wim where wij is the figure text compression example in this case timate well known with their furry friends and featuring friends are deletable given compression rules york times annotated corpus sandhaus our model matches or exceeds the state of the art on all of these datasets and achieves the largest improvement on cnn rouge over our extractive baseline due to the more compressed nature of cnn summaries we show that our model s compression threshold is robust across a range of settings yet tunable to give length summaries finally we investigate the ency and grammaticality of our compressed tences the human evaluation shows that our system yields generally grammatical output with many remaining errors being attributed to the parser compression in summarization sentence compression is a long studied problem dealing with how to delete the least critical mation in a sentence to make it shorter knight and marcu martins and smith cohn and lapata wang et al li et al many of these approaches are syntax driven though end to end neural models have been proposed as well filippova et al wang et al past non neural work on summarization has used both syntax based kirkpatrick et al woodsend and lapata and discourse based carlson et al hirao et al li et al compressions our approach follows in the syntax driven vein our high level approach to summarization is shown in figure in section we describe the models for extraction and compression our pression depends on having a discrete set of valid compression options that maintain the cality of the underlying sentence which we now proceed to describe code are model jiacheng xu neu compression sum available at full model output and the pre trained featuringintimateportraitsnpnpnpnnsvbgjjnnswith their furry friendswell knownartistsjjppvp figure sentence extraction module of jecs words in input document sentences are encoded with bilstms two layers of cnns aggregate these into sentence representations hi and then the document representation vdoc this is fed into an attentive lstm decoder which selects sentences based on the decoder state d and the tations hi similar to a pointer network th word in the content selection module learns to pick up a subset of d denoted as d sk d where k sentences are selected sentence document encoder we rst use a bidirectional lstm to encode words in each sentence in the document separately and then we apply multiple convolution ers and max pooling layers to extract the resentation of every sentence specically him wim and hi him where hi is a resentation of the i th sentence in the document this process is shown in the left side of ure illustrated in purple blocks we then gregate these sentence representations into a ument representation vdoc with a similar bilstm and cnn combination shown in figure with ange blocks decoding the decoding stage selects a number of sentences given the document representation vdoc and sentences representations hi this cess is depicted in the right half of figure we use a sequential lstm decoder where at each time step we take the representation h of the last selected sentence the overall document tor vdoc and the recurrent state and produce a distribution over all of the remaining sentences excluding those already selected this approach resembles pointer network style approaches used in past work zhou et al formally we write this as dt hk vdoc scoret whhi hk vdoc hi i where hk is the representation of the sentence lected at time step t is the decoding figure text compression module a neural classier scores the compression option with their furry friends in the sentence and broader document context and cides whether or not to delete it den state from last time step wd wh wm and parameters in lstm are learned once a sentence is selected it can not be selected again at test time we use greedy decoding to identify the most likely sequence of sentences under our model text compression after selecting the sentences the text sion module evaluates our discrete compression options and decides whether to remove certain phrases or words in the selected sentences ure shows an example of this process for ing whether or not to delete a pp in this sentence this pp was marked as deletable based on rules described in section our network then encodes this sentence and the compression combines this information with the document context vdoc and decoding context hdec and uses a feedforward work to decide whether or not to delete the span our experiments we decode for a xed number of sentences tuned for each dataset as in prior extractive work narayan et al we experimented with dynamically choosing a number of sentences and found this to make little difference often cats can be overlooked in favour of their cuter canine counterparts but a new book artists and their cats is putting felines back on the map philadelphia based artist and journalist alison nastasi has collated a collection of intimate portraits featuring well known artists with their furry friends in this image spanish surrealist painter salvador dali poses with his cat babou a colombian wild cat blstm cnnblstm cnn blstm doccnnphiladelphia well known artists with their furry friends contextualized encoder compressionencsentence classicationlabel let ci cil denote the possible pression spans derived from the rules described in section let yi c be a binary variable equal to if we are deleting the cth option of the ith sentence our text compression module models st as described in the following section compression encoder we use a contextualized encoder elmo peters et al to compute contextualized word representations we then use cnns with max pooling to encode the sentence shown in blue in figure and the candidate pression shown in light green in figure the sentence representation vsent and the compression span representation vcomp are concatenated with the hidden state in sentence decoder hdec and the document representation vdoc compression classier we feed the catenated representation to a feedforward ral network to predict whether the sion span should be deleted or kept which is formulated as a binary classication lem this classier computes the nal probability st vdoc vcomp si the overall probability of a summary s y where s is the sentence oracle and y is the is the product of compression label tion and compression models s cci heuristic deduplication inspired by the gram avoidance trick proposed in paulus et al to reduce redundancy we take full tage of our linguistically motivated compression rules and the constituent parse tree and allow our model to compress deletable chunks with dant information we therefore take our model s output and apply a postprocessing stage where we remove any compression option whose unigrams are completely covered elsewhere in the summary we perform this compression after the model diction and compression training our model makes a series of sentence extraction decisions s and then compression decisions y to supervise it we need to derive gold standard bels for these decisions our oracle identication approach relies on rst identifying an oracle set of sentences and then the oracle compression reference artist and journalist alison nastasi put gether the portrait collection also features images of casso frida kahlo and john lennon reveals quaint sonality traits shared between artists and their felines document philadelphia based artist and ist alison nastasi has collated a collection of intimate portraits featuring well known artists with their furry friends compression rbf label ratio raf philadelphia based intimate well known featuring their furry friends del del del keep table oracle label computation for the text pression module rbf and raf are the rouge scores before and after compression the ratio is dened as raf rouge increases when words not appearing in rbf the reference are deleted rouge can decrease when terms appearing in the reference summary like ing are deleted tions oracle construction sentence extractive oracle we rst identify an oracle set of sentences to extract using a beam search procedure similar to maximal marginal relevance mmr carbonell and goldstein for each additional sentence we propose to add we compute a heuristic cost equal to the rouge score of a given sentence with respect to the reference summary when pruning states we calculate the rouge score of the combination of sentences currently selected and sort in ing order let the beam width be the time plexity of the approximate approach is where in practice n and n we set and n which means we only consider the rst sentences in the document the beam search procedure returns a beam of different sentence combinations in the nal beam we use the sentence extractive oracle for both the extraction only model and the joint compression model oracle compression labels to form our joint extractive and compressive oracle we need to give the compression decisions binary labels yi in each set of extracted sentences for simplicity and computational efciency we assign each sentence on which sentences are extracted different compression decisions may be optimal however re deriving these with a dynamic oracle goldberg and nivre is prohibitively expensive during training category cnn dm nyt bad weak positive strong positive table compressibility the oracle label distribution over three datasets compressions in the bad gory decrease rouge and are labeled as negative do not delete while weak positive less than rouge improvement and strong positive greater than both represent rouge improvements cnn features much more compression than the other datasets a single yi c independent of the context it occurs in for each compression option we assess the value of it by comparing the rouge score of the sentence with and without this phrase any tion that increases rouge is treated as a pression that should be applied when calculating this rouge value we remove stop words include stemming we run this procedure on each of our oracle tractive sentences the fraction of positive and negative labels assigned to compression options is shown for each of the three datasets in table cnn is the most compressable dataset among cnn dm and nyt ilp based oracle construction past work has derived oracles for extractive and compressive systems using integer linear programming ilp gillick and favre berg kirkpatrick et al following their approach we can directly optimize for rouge recall of an extractive or compressive summary in our framework if we specify a length limit however we evaluate on rouge as is standard when comparing to ral models that do nt produce xed length maries optimizing for rouge can not be mulated as an ilp since computing precision quires dividing by the number of selected words making the objective no longer linear we perimented with optimizing for rouge directly by nding optimal rouge recall maries at various settings of maximum summary length however these summaries frequently tained short sentences to ll up the budget and the collection of summaries returned tended to be less diverse than those found by beam search learning objective avoid committing to a single oracle summary for the learning process our procedure from tion can generate m extractive oracles s i let s i t denote the gold sentence for the i acle at timestep t past work narayan et al chen and bansal has employed icy gradient in this setting to optimize directly for rouge however because oracle summaries usually have very similar rouge scores we choose to simplify this objective as lsent put another m way we optimize the log likelihood averaged across m different oracles to ensure that each has high likelihood we use m oracles during training the oracle sentence indices are sorted cording to the individual salience rouge score rather than document order log i s the objective of the compression module is ned as lcomp i s where i c is the probability of the target sion for the th compression options of the i th sentence the joint loss function is l lsent lcomp we set in practice log experiments we evaluate our model on two axes first for tent selection we use rouge as is standard ond we evaluate the grammaticality of our model to ensure that it is not substantially damaged by compression experimental setup datasets we evaluate the proposed method on the three popular news summarization datasets new york times corpus sandhaus cnn and dailymail dm hermann et al as discussed in section compression will give different results on different datasets ing on how much compression is optimal from the standpoint of reproducing the reference maries which changes how measurable the impact of compression is in table we show the pressability of these three datasets how valuable various compression options seem to be from the standpoint of improving rouge we found that cnn has signicantly more positive compression options than the other two critically cnn also has the shortest references words on average often many oracle summaries achieve very to ilar rouge values we therefore want details about the experimental setup tion details and human evaluation are provided in the pendix model lead ours refresh narayan et al latsum zhang et al banditsum dong et al leaddedup leadcomp extraction extlstmdel jecs cnn r l table experimental results on the test sets of cnn indicates models evaluates with our own rouge rics our model outperforms our extractive model and lead based baselines as well as prior work compared to for daily mail see appendix in our experiments we rst focus on cnn and then evaluate on the other datasets models we present several variants of our model to show how extraction and compression work jointly in extractive summarization the lead baseline rst k sentences is a strong line due to how newswire articles are written leaddedup is a non learned baseline that uses our heuristic deduplication technique on the lead sentences leadcomp is a compression only model where compression is performed on the lead sentences this shows the effectiveness of the compression module in isolation rather than in the context of abstraction extraction is the tion only model jecs is the full joint extractive and compressive summarizer we compare our model with various abstractive and extractive summarization models neusum zhou et al uses a model to dict a sequence of sentences indices to be picked up from the document our extractive approach is most similar to this model refresh narayan et al banditsum dong et al and latsum zhang et al are extractive rization models for comparison we also compare with some abstractive models including cov see et al fars chen and bansal and cbdec jiang and bansal we also compare our joint model with a pipeline model with an off the shelf compression module we implement a deletion based bilstm model for sentence compression wang et al and run the model on top of our extraction output reimplemented the authors model following their specication and matched their accuracy for fair model cnndm lead ours refresh narayan et al neusum latsum zhang et al latsum compression banditsum cbdec jiang and bansal fars chen and bansal leaddedup leadcomp extraction jecs r l table experimental results on the test sets of ndm the portion of cnn is roughly one of tenth of dm gains are more pronounced on cnn because this dataset features shorter more compressed ence summaries the pipeline model is denoted as extlstmdel results on cnn table shows experiments results on cnn we list performance of the lead baseline and the formance of competitor models on these datasets starred models are evaluated according to our rouge metrics numbers very closely match the originally reported results our model achieves substantially higher formance than all baselines and past systems rouge compared to any of these on this dataset compression is substantially useful pression is somewhat effective in isolation as shown by the performance of leaddedup and leadcomp but compression in isolation still gives less benet on top of lead than when combined with the extractive model jecs in the joint framework furthermore our model beats the pipeline model extlstmdel which shows the necessity of training a joint model with rouge supervision results on combined cnndm and nyt we also report the results on the full cnndm and nyt although they are less compressable ble and table shows the experimental results on these datasets our models still yield strong performance pared to baselines and past work on the cnndm son we tuned the deletion threshold to match the sion rate of our model other choices did not lead to better rouge scores model lead leaddedup leadcomp extraction jecs r l table experimental results on the dataset and is reported jecs tially outperforms our lead based systems and our tractive model dataset the extraction model achieves parable results to past successful extractive proaches on cnndm and jecs improves on this across the datasets in some cases our model slightly underperforms on one sible reason is that we remove stop words when constructing our oracles which could mate the importance of bigrams containing words for evaluation finally we note that our compressive approach substantially outperforms the compression augmented latsum model that model used a separate model for ing which is potentially harder to learn than our compression model on nyt we see again that the inclusion of compression leads to improvements in both the lead setting as well as for our full jecs model grammaticality we evaluate grammaticality of our compressed summaries in three ways first we use zon mechanical turk to compare different pression techniques second to measure absolute grammaticality we use an automated out of box tool grammarly finally we conduct manual analysis human evaluation we rst conduct a human evaluation on the amazon mechanical turk form we ask turkers to rank different sion versions of a sentence in terms of icality we compare our full jecs model and the off the shelf pipeline model extlstmdel which have matched compression ratios we also propose another baseline extractdropout which randomly drops words in a sentence to match the compression ratio of the other two et al do not use the dataset so our results are not directly comparable to theirs durrett et al use a different evaluation setup with a hard limit on the summary length and evaluation on recall only model preference error ext extdrop extlstmdel jecs table human preference rouge and grammarly grammar checking results we asked turkers to rank the models output based on grammaticality error shows the number of grammar errors in sentences reported by grammarly our jecs model achieves the highest rouge and is preferred by humans while still making relatively few errors els the results are shown in table ers give roughly equal preference to our model and the extlstmdel model which was learned from supervised compression data however our jecs model achieves substantially higher rouge score indicating that it represents a more effective compression approach we found that absolute grammaticality ments were hard to achieve on mechanical turk turkers ratings of grammaticality were very noisy and they did not consistently rate true article sentences above obviously noised variants fore we turn to other methods as described in the next two paragraphs automatic grammar checking we use marly to check sentences sampled from the outputs of the three models mentioned above from cnn both extlstmdel and jecs make a small number of grammar errors not much higher than the purely extractive baseline one major source of errors for jecs is having the wrong article after the deletion of an adjective like an awesome style manual error analysis to get a better sense of our model s output we conduct a manual ysis of our applied compressions to get a sense of how many are valid we manually examined model summaries comparing the output with the raw sentences before compression and tied the following errors eight bad deletions due to parsing errors like a uk jj national from london eight inappropriate adjective deletions causing correctness issues with respect to the erence document like former president and clear weapon three other errors partial tion of slang inappropriate pp attachment tion and an unhandled grammatical construction reference summary prediction with compressions mullah omar the reclusive founder of the afghan taliban is still in charge a new biography claims an ex taliban insider says there have been rumors that the one eyed militant is dead cnn mullah mohammed omar is still the leader of the taliban s declared islamic emirate of afghanistan the taliban s cultural sion released the page document in several different translations on the movement s website ostensibly to commemorate the anniversary of an april meeting in afghanistan s kandahar province when an assembly of afghans swore allegiance to omar rebecca francis photo with a raffe was shared by ricky gervais francis was threatened on twitter for the picture francis a hunter said the giraffe was close to death and became food for locals cnn five years ago rebecca francis posed for a photo while lying next to a dead giraffe the trouble started monday when comedian ricky gervais tweeted the photo with a question francis who has appeared on the nbc sports network outdoor lifestyle show eye of the hunter and was the ject of an interview with hunting life in late march responded in a statement to huntinglife com on tuesday which was posted on its facebook page president barack frida ghitis obama is right to want a deal but this one gives iran too much she says the framework agreement starts lifting iran sanctions much too soon cnn president barack obama tied himself to the mast of a nuclear deal with iran even before he became the democratic candidate for president reaching a good solid agreement with iran is a worthy desirable goal but the process has unfolded under the destructive inuence of political considerations ening america s hand and strengthening iran table examples of applied compressions the top two are sampled from among the most compressed examples in the dataset our jecs model is able to delete both large chunks especially temporal pps giving dates of events as well as individual modiers that are nt determined to be relevant to the summary e the specication of the anniversary the last example features more modest compression students rst athletes second examples of output are shown in table the rst two examples are sampled from the top of the most compressed examples in the corpus we see a variety of compression options that are used in the rst two examples including removal of temporal pps large subordinate clauses tives and parentheticals the last example tures less compression only removing a handful of adjectives in a manner which slightly changes the meaning of the summary improving the parser and deriving a more semantically aware set of compression rules can help achieving better grammaticality and ity however we note that such errors are largely orthogonal to the core of our approach a more ned set of compression options could be dropped into our system and used without changing our fundamental model compression analysis compression threshold compression in our model is an imbalanced binary classication lem the trained model s natural classication threshold probability of del may not be optimal for downstream rouge we experiment with varying the classication threshold from no deletion only heuristic deduplication to all compressible pieces removed the results on cnn are shown in figure where we show the figure effect of changing the compression old on cnn the y axis shows the average of the of and the dotted line is the tive baseline the model outperforms the extractive model and achieves nearly optimal performance across a range of threshold values average rouge value at different compression thresholds the model achieves the best mance at but performs well in a wide range from to our compression is therefore robust yet also provides a controllable parameter to change the amount of compression in produced summaries compression type analysis we further break down the types of compressions used in the model table shows the compressions that our model ends up choosing at test time pps are often pressed by the deduplication mechanism because the compressible pps tend to be temporal and tion adjuncts which may be redundant across averageextraction node type len of comps comp acc dedup jj pp advp prn table the compressions used by our model on cnn average lengths and the fraction of that constituency type among compressions taken by our model comp acc indicates how frequently that compression was taken by the oracle note that error especially keeping constituents that we should nt may have minimal pact on summary quality dedup indicates the age of chosen compressions which arise from cation as opposed to model prediction proposed a deep generative model for text compression zhang et al explored the compression module after the extraction model but the separation of these two modules hurt the formance for this work we nd that relying on syntax gives us more easily understandable and controllable compression options contemporaneously with our work mendes et al explored an extractive and pressive approach using compression integrated into a sequential decoding process however their approach does not leverage explicit syntax and makes several different model design choices tences without the manual deduplication nism our model matches the ground truth around of the time however a low accuracy here may not actually cause a low nal rouge score as many compression choices only affect the nal rouge score by a small amount more details about compression options are in the tary material related work neural extractive summarization neural works have shown to be effective in extractive summarization past approaches have structured the decision either as binary classication over sentences cheng and lapata nallapati et al or classication followed by ranking narayan et al zhou et al used a seq to seq decoder instead for our model text compression forms a module largely orthogonal to the extraction module so additional improvements to extractive modeling might be expected to stack with our approach syntactic compression prior to the explosion of neural models for summarization syntactic compression martins and smith send and lapata was relatively more mon several systems explored the usage of stituency parses berg kirkpatrick et al wang et al li et al as well as based approaches hirao et al durrett et al our approach follows in this vein but could be combined with more sophisticated neural text compression methods as well neural text compression filippova et al presented an lstm approach to based sentence compression miao and blunsom conclusion in this work we presented a neural network work for extractive and compressive tion our model consists of a sentence extraction model joined with a compression classier that cides whether or not to delete syntax derived pression options for each sentence training the model involves nding an oracle set of extraction and compression decision with high score which we do through a combination of a beam search procedure and heuristics our model outperforms past work on the cnn daily mail corpus in terms of rouge achieves substantial gains over the extractive model and appears to have acceptable grammaticality according to human evaluations acknowledgments this work was partially supported by nsf grant a bloomberg data science grant and an equipment grant from nvidia the thors acknowledge the texas advanced ing center tacc at the university of texas at austin for providing hpc resources used to duct this research results presented in this paper were obtained using the chameleon testbed ported by the national science foundation hey et al thanks as well to the anonymous reviewers for their helpful comments references taylor berg kirkpatrick dan gillick and dan klein jointly learning to extract and compress in proceedings of the annual meeting of the ciation for computational linguistics human guage technologies pages association for computational linguistics ziqiang cao furu wei wenjie li and sujian li faithful to the original fact aware neural tive summarization in aaai conference on cial intelligence jaime carbonell and jade goldstein the use of mmr diversity based reranking for reordering documents and producing summaries in ings of the annual international acm sigir conference on research and development in mation retrieval sigir pages new york ny usa acm lynn carlson daniel marcu and mary ellen okurovsky building a discourse tagged corpus in the framework of rhetorical structure in proceedings of the second sigdial theory workshop on discourse and dialogue yen chun chen and mohit bansal fast tive summarization with reinforce selected tence rewriting in proceedings of the annual meeting of the association for computational guistics volume long papers pages association for computational linguistics jianpeng cheng and mirella lapata neural summarization by extracting sentences and words in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages association for putational linguistics sumit chopra michael auli and alexander m rush abstractive sentence summarization with in tentive recurrent neural networks ings of the conference of the north american chapter of the association for computational guistics human language technologies pages association for computational linguistics trevor cohn and mirella lapata sentence pression as tree transduction j artif int res yue dong yikang shen eric crawford herke van hoof and jackie chi kit cheung ditsum extractive summarization as a contextual bandit in proceedings of the conference on empirical methods in natural language ing pages association for tional linguistics greg durrett taylor berg kirkpatrick and dan klein learning based single document rization with compression and anaphoricity in proceedings of the annual straints ing of the association for computational linguistics volume long papers pages ciation for computational linguistics katja filippova enrique alfonseca carlos a menares lukasz kaiser and oriol vinyals sentence compression by deletion with lstms in proceedings of the conference on cal methods in natural language processing pages association for computational tics sebastian gehrmann yuntian deng and alexander rush bottom up abstractive in proceedings of the conference on tion empirical methods in natural language ing pages association for tional linguistics dan gillick and benoit favre a scalable global in proceedings of the model for summarization workshop on integer linear programming for ral language processing pages association for computational linguistics yoav goldberg and joakim nivre a dynamic oracle for arc eager dependency parsing in ceedings of coling pages the coling organizing committee max grusky mor naaman and yoav artzi newsroom a dataset of million summaries in with diverse extractive strategies ings of the conference of the north can chapter of the association for computational linguistics human language technologies ume long papers pages association for computational linguistics karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom teaching chines to read and comprehend in c cortes n d lawrence d d lee m sugiyama and r garnett editors advances in neural information processing systems pages curran associates inc tsutomu hirao yasuhisa yoshida masaaki nishino norihito yasuda and masaaki nagata single document summarization as a tree in proceedings of the sack problem ference on empirical methods in natural language processing pages association for putational linguistics yichen jiang and mohit bansal closed book training to improve summarization encoder ory in proceedings of the conference on pirical methods in natural language processing pages association for computational linguistics kate keahey pierre riteau dan stanzione tim erill joe mambretti paul rad and paul ruth chameleon a scalable production testbed for puter science research in contemporary high formance computing from petascale toward cale edition volume of chapman hall crc computational science chapter pages crc press boca raton fl diederik p kingma and jimmy ba adam a method for stochastic optimization arxiv preprint kevin knight and daniel marcu based summarization step one sentence in proceedings of the seventeenth pression tional conference on articial intelligence and twelfth conference on innovative applications of articial intelligence pages aaai press kevin knight and daniel marcu tion beyond sentence extraction a probabilistic approach to sentence compression artif intell chen li yang liu fei liu lin zhao and fuliang weng improving multi documents marization by sentence compression based on panded constituent parse trees in proceedings of the conference on empirical methods in ral language processing emnlp pages association for computational linguistics haoran li junnan zhu jiajun zhang and chengqing zong ensure the correctness of the mary incorporate entailment knowledge into in proceedings stractive sentence summarization of the international conference on tational linguistics pages association for computational linguistics junyi jessy li kapil thadani and amanda stent the role of discourse units in near extractive summarization in proceedings of the annual meeting of the special interest group on discourse and dialogue pages association for putational linguistics chin yew lin rouge a package for matic evaluation of summaries in text tion branches out christopher manning mihai surdeanu john bauer jenny finkel steven bethard and david mcclosky the stanford corenlp natural language processing toolkit in proceedings of annual meeting of the association for computational guistics system demonstrations pages sociation for computational linguistics andre martins and noah a smith tion with a joint model for sentence extraction and in proceedings of the workshop on compression integer linear programming for natural language processing pages association for tional linguistics afonso mendes shashi narayan sebastiao miranda zita marinho andre f t martins and shay b cohen jointly extracting and compressing documents with summary state representations in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies volume long and short papers yishu miao and phil blunsom language as a latent variable discrete generative models for sentence compression in proceedings of the conference on empirical methods in natural guage processing pages association for computational linguistics ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural work based sequence model for extractive marization of documents in aaai conference on articial intelligence ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang stractive text summarization using sequence sequence rnns and beyond in proceedings of the signll conference on computational natural language learning pages association for computational linguistics shashi narayan shay b cohen and mirella lapata ranking sentences for extractive tion with reinforcement learning in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long pers pages association for tional linguistics romain paulus caiming xiong and richard socher a deep reinforced model for abstractive in international conference on summarization learning representations matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer deep contextualized word resentations in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long papers pages association for computational linguistics xian qian and yang liu fast joint compression in and summarization via graph cuts ings of the conference on empirical methods in natural language processing pages association for computational linguistics alexander m rush sumit chopra and jason ston a neural attention model for tive sentence summarization in proceedings of the conference on empirical methods in natural language processing pages association for computational linguistics evan sandhaus the new york times annotated corpus linguistic data consortium philadelphia abigail see peter j liu and christopher d ning get to the point summarization with pointer generator networks in proceedings of the annual meeting of the association for tational linguistics volume long papers pages association for computational tics jiwei tan xiaojun wan and jianguo xiao stractive document summarization with a in proceedings based attentional neural model of the annual meeting of the association for computational linguistics volume long pers pages association for tional linguistics liangguo wang jing jiang hai leong chieu chen hui ong dandan song and lejian liao improving an can syntax help based sentence compression model for new in proceedings of the annual mains ing of the association for computational linguistics volume long papers pages canada association for computational guistics lu wang hema raghavan vittorio castelli radu rian and claire cardie a sentence pression based framework to query focused in proceedings of the document summarization annual meeting of the association for tational linguistics volume long papers pages association for computational tics kristian woodsend and mirella lapata ing to simplify sentences with quasi synchronous grammar and integer programming in proceedings of the conference on empirical methods in natural language processing pages sociation for computational linguistics xingxing zhang mirella lapata furu wei and ming zhou neural latent extractive document in proceedings of the summarization ference on empirical methods in natural language processing pages association for putational linguistics qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao neural ument summarization by jointly learning to score in proceedings of the and select sentences annual meeting of the association for tional linguistics volume long papers pages association for computational tics dataset train dev test ref len doc len cnn dm table statistics of the cnn daily mail and see text datasets cnn features the shortest reference summaries overall and this is where we nd sion is most effective node type len of comps oracle comp pp jj sbar advp table statistics of compression options in cnn we show the top four constituency types that are pressible along with the average length the fraction of available compressions it accounts for and how quently the oracle says to compress these constituents of the adjectives are compressable without hurting the rouge supplementary material a experimental setup data preprocessing we preprocess the datasets with the scripts provided by see et al which uses stanford corenlp tokenization ning et al we use the non anonymized version of the cnn dm as in previous rization work for the new york times corpus we lter out the examples with abstracts shorter than words following the criteria in durrett et al yielding the nyt dataset the tics of the datasets are listed in table ing sentence selection we always select tences for cnn dm and sentences for nyt which gave the best performance for our tic analysis all datasets are parsed with the stituency parser in stanford corenlp manning et al implementation details we use the same trained word embeddings used in narayan et al the size of the sentence and document representation vectors is for the sion module we use elmo as the ized encoder without ne tuning the parameter and project the vectors back to dimensions ter the elmo layer dropout is applied after word embedding layers and lstm layers at a rate of we use the adam optimizer kingma and ba with the initial learning rate at the model converges after epochs of training in tial experiments we also found elmo to be useful for sentence selection as well however to plify comparisons with past work and due to ing issues we use it for compression only we use rouge lin for evaluation during acle construction we use simplied unigram and bigram scores as a faster approximation to the full rouge b turk instructions figure shows the interface for amazon turk man evaluation c type analysis in table we show the statistics of the sion options in cnn pp attachment and adjectives are the top compression options and according to the oracle more than half of pp and almost all line parameters figure the interface for amazon turk human evaluation all of the examples are fully shufed
