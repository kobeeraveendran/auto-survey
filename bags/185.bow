neural extractive text summarization syntactic compression jiacheng greg durrett department computer science university texas austin jcxu utexas edu abstract recent neural network approaches rization largely selection based traction generation based abstraction work present neural model single document summarization based joint extraction syntactic compression model chooses sentences document identies possible compressions based stituency parses scores sions neural model produce nal summary learning construct acle extractive compressive summaries learn components jointly supervision experimental results cnn daily mail new york times datasets model achieves strong performance comparable state art systems evaluated rouge approach outperforms shelf compression module human manual evaluation shows model generally remains grammatical introduction neural network approaches document marization ranged purely extractive cheng lapata nallapati narayan abstractive rush nallapati chopra tan gehrmann tractive systems robust straightforward use abstractive systems exible ied summarization situations grusky factual errors cao fall extraction practice extractive compressive tems berg kirkpatrick qian liu durrett combine strengths approaches tle work studying neural network models vein approaches employed figure diagram proposed model extraction compression modularized jointly trained supervision derived reference summary typically use based sentence compression chen bansal work propose model bine high performance neural extractive systems additional exibility compression interpretability given having discrete pression options model rst encodes source document sentences quentially selects set sentences press sentence set compression tions available selected preserve ing grammaticality derived syntactic constituency parses represent panded set discrete options prior work berg kirkpatrick wang neural model additionally scores chooses compressions apply given context document sentence decoder model recurrent state principal challenge training extractive compressive model constructing cle summary supervision identify set high quality sentences document beam search derive oracle compression labels sentence additional renement process model training objective combines extractive compressive components learns jointly conduct experiments standard single document news summarization datasets cnn daily mail hermann new compression module extraction module input documentoutput summary ref compression rules refer rules rived wang durrett design concise set syntactic rules including removal itive noun phrases relative clauses bial clauses adjective phrases noun phrases adverbial phrases figure dive verb phrases noun phrases ure prepositional phrases certain rations like monday content parentheticals figure shows examples sion rules applied short snippet binations compressions maintain ity content fairly important context deleted model learn delete elements compressability summaries different sources feature levels sion extreme summary fully sentence extractive extreme editor compressed lot content tence section examine question summarization datasets use motivate choice evaluation datasets universal compression rouge use syntax source compression options note ways generating compression options possible including labeled pression data supervising compression rouge critical learn information important particular source case labeled compression data unavailable section compare domains model shelf sentence compression ule substantially underperforms approach model model neural network model encodes source document chooses sentences document selects discrete compression tions apply model architecture sentence extraction module text compression module shown figure extractive sentence selection single document consists sentences sentence denoted wim wij figure text compression example case timate known furry friends featuring friends deletable given compression rules york times annotated corpus sandhaus model matches exceeds state art datasets achieves largest improvement cnn rouge extractive baseline compressed nature cnn summaries model compression threshold robust range settings tunable length summaries finally investigate ency grammaticality compressed tences human evaluation shows system yields generally grammatical output remaining errors attributed parser compression summarization sentence compression long studied problem dealing delete critical mation sentence shorter knight marcu martins smith cohn lapata wang approaches syntax driven end end neural models proposed filippova wang past non neural work summarization syntax based kirkpatrick woodsend lapata discourse based carlson hirao compressions approach follows syntax driven vein high level approach summarization shown figure section describe models extraction compression pression depends having discrete set valid compression options maintain cality underlying sentence proceed describe code model jiacheng neu compression sum available model output pre trained featuringintimateportraitsnpnpnpnnsvbgjjnnswith furry friendswell knownartistsjjppvp figure sentence extraction module jecs words input document sentences encoded bilstms layers cnns aggregate sentence representations document representation vdoc fed attentive lstm decoder selects sentences based decoder state tations similar pointer network word content selection module learns pick subset denoted sentences selected sentence document encoder rst use bidirectional lstm encode words sentence document separately apply multiple convolution ers max pooling layers extract resentation sentence specically wim resentation sentence document process shown left ure illustrated purple blocks gregate sentence representations ument representation vdoc similar bilstm cnn combination shown figure ange blocks decoding decoding stage selects number sentences given document representation vdoc sentences representations cess depicted right half figure use sequential lstm decoder time step representation selected sentence overall document tor vdoc recurrent state produce distribution remaining sentences excluding selected approach resembles pointer network style approaches past work zhou formally write vdoc scoret whhi vdoc representation sentence lected time step decoding figure text compression module neural classier scores compression option furry friends sentence broader document context cides delete den state time step parameters lstm learned sentence selected selected test time use greedy decoding identify likely sequence sentences model text compression selecting sentences text sion module evaluates discrete compression options decides remove certain phrases words selected sentences ure shows example process ing delete sentence marked deletable based rules described section network encodes sentence compression combines information document context vdoc decoding context hdec uses feedforward work decide delete span experiments decode xed number sentences tuned dataset prior extractive work narayan experimented dynamically choosing number sentences found little difference cats overlooked favour cuter canine counterparts new book artists cats putting felines map philadelphia based artist journalist alison nastasi collated collection intimate portraits featuring known artists furry friends image spanish surrealist painter salvador dali poses cat babou colombian wild cat blstm cnnblstm cnn blstm doccnnphiladelphia known artists furry friends contextualized encoder compressionencsentence classicationlabel let cil denote possible pression spans derived rules described section let binary variable equal deleting cth option ith sentence text compression module models described following section compression encoder use contextualized encoder elmo peters compute contextualized word representations use cnns max pooling encode sentence shown blue figure candidate pression shown light green figure sentence representation vsent compression span representation vcomp concatenated hidden state sentence decoder hdec document representation vdoc compression classier feed catenated representation feedforward ral network predict sion span deleted kept formulated binary classication lem classier computes nal probability vdoc vcomp overall probability summary sentence oracle product compression label tion compression models cci heuristic deduplication inspired gram avoidance trick proposed paulus reduce redundancy tage linguistically motivated compression rules constituent parse tree allow model compress deletable chunks dant information model output apply postprocessing stage remove compression option unigrams completely covered summary perform compression model diction compression training model makes series sentence extraction decisions compression decisions supervise need derive gold standard bels decisions oracle identication approach relies rst identifying oracle set sentences oracle compression reference artist journalist alison nastasi gether portrait collection features images casso frida kahlo john lennon reveals quaint sonality traits shared artists felines document philadelphia based artist ist alison nastasi collated collection intimate portraits featuring known artists furry friends compression rbf label ratio raf philadelphia based intimate known featuring furry friends del del del table oracle label computation text pression module rbf raf rouge scores compression ratio dened raf rouge increases words appearing rbf reference deleted rouge decrease terms appearing reference summary like ing deleted tions oracle construction sentence extractive oracle rst identify oracle set sentences extract beam search procedure similar maximal marginal relevance mmr carbonell goldstein additional sentence propose add compute heuristic cost equal rouge score given sentence respect reference summary pruning states calculate rouge score combination sentences currently selected sort ing order let beam width time plexity approximate approach practice set means consider rst sentences document beam search procedure returns beam different sentence combinations nal beam use sentence extractive oracle extraction model joint compression model oracle compression labels form joint extractive compressive oracle need compression decisions binary labels set extracted sentences simplicity computational efciency assign sentence sentences extracted different compression decisions optimal deriving dynamic oracle goldberg nivre prohibitively expensive training category cnn nyt bad weak positive strong positive table compressibility oracle label distribution datasets compressions bad gory decrease rouge labeled negative delete weak positive rouge improvement strong positive greater represent rouge improvements cnn features compression datasets single independent context occurs compression option assess value comparing rouge score sentence phrase tion increases rouge treated pression applied calculating rouge value remove stop words include stemming run procedure oracle tractive sentences fraction positive negative labels assigned compression options shown datasets table cnn compressable dataset cnn nyt ilp based oracle construction past work derived oracles extractive compressive systems integer linear programming ilp gillick favre berg kirkpatrick following approach directly optimize rouge recall extractive compressive summary framework specify length limit evaluate rouge standard comparing ral models produce xed length maries optimizing rouge mulated ilp computing precision quires dividing number selected words making objective longer linear perimented optimizing rouge directly nding optimal rouge recall maries settings maximum summary length summaries frequently tained short sentences budget collection summaries returned tended diverse found beam search learning objective avoid committing single oracle summary learning process procedure tion generate extractive oracles let denote gold sentence acle timestep past work narayan chen bansal employed icy gradient setting optimize directly rouge oracle summaries usually similar rouge scores choose simplify objective lsent way optimize log likelihood averaged different oracles ensure high likelihood use oracles training oracle sentence indices sorted cording individual salience rouge score document order log objective compression module ned lcomp probability target sion compression options sentence joint loss function lsent lcomp set practice log experiments evaluate model axes tent selection use rouge standard ond evaluate grammaticality model ensure substantially damaged compression experimental setup datasets evaluate proposed method popular news summarization datasets new york times corpus sandhaus cnn dailymail hermann discussed section compression different results different datasets ing compression optimal standpoint reproducing reference maries changes measurable impact compression table pressability datasets valuable compression options standpoint improving rouge found cnn signicantly positive compression options critically cnn shortest references words average oracle summaries achieve ilar rouge values want details experimental setup tion details human evaluation provided pendix model lead refresh narayan latsum zhang banditsum dong leaddedup leadcomp extraction extlstmdel jecs cnn table experimental results test sets cnn indicates models evaluates rouge rics model outperforms extractive model lead based baselines prior work compared daily mail appendix experiments rst focus cnn evaluate datasets models present variants model extraction compression work jointly extractive summarization lead baseline rst sentences strong line newswire articles written leaddedup non learned baseline uses heuristic deduplication technique lead sentences leadcomp compression model compression performed lead sentences shows effectiveness compression module isolation context abstraction extraction tion model jecs joint extractive compressive summarizer compare model abstractive extractive summarization models neusum zhou uses model dict sequence sentences indices picked document extractive approach similar model refresh narayan banditsum dong latsum zhang extractive rization models comparison compare abstractive models including cov fars chen bansal cbdec jiang bansal compare joint model pipeline model shelf compression module implement deletion based bilstm model sentence compression wang run model extraction output reimplemented authors model following specication matched accuracy fair model cnndm lead refresh narayan neusum latsum zhang latsum compression banditsum cbdec jiang bansal fars chen bansal leaddedup leadcomp extraction jecs table experimental results test sets ndm portion cnn roughly tenth gains pronounced cnn dataset features shorter compressed ence summaries pipeline model denoted extlstmdel results cnn table shows experiments results cnn list performance lead baseline formance competitor models datasets starred models evaluated according rouge metrics numbers closely match originally reported results model achieves substantially higher formance baselines past systems rouge compared dataset compression substantially useful pression somewhat effective isolation shown performance leaddedup leadcomp compression isolation gives benet lead combined extractive model jecs joint framework furthermore model beats pipeline model extlstmdel shows necessity training joint model rouge supervision results combined cnndm nyt report results cnndm nyt compressable ble table shows experimental results datasets models yield strong performance pared baselines past work cnndm son tuned deletion threshold match sion rate model choices lead better rouge scores model lead leaddedup leadcomp extraction jecs table experimental results dataset reported jecs tially outperforms lead based systems tractive model dataset extraction model achieves parable results past successful extractive proaches cnndm jecs improves datasets cases model slightly underperforms sible reason remove stop words constructing oracles mate importance bigrams containing words evaluation finally note compressive approach substantially outperforms compression augmented latsum model model separate model ing potentially harder learn compression model nyt inclusion compression leads improvements lead setting jecs model grammaticality evaluate grammaticality compressed summaries ways use zon mechanical turk compare different pression techniques second measure absolute grammaticality use automated box tool grammarly finally conduct manual analysis human evaluation rst conduct human evaluation amazon mechanical turk form ask turkers rank different sion versions sentence terms icality compare jecs model shelf pipeline model extlstmdel matched compression ratios propose baseline extractdropout randomly drops words sentence match compression ratio use dataset results directly comparable theirs durrett use different evaluation setup hard limit summary length evaluation recall model preference error ext extdrop extlstmdel jecs table human preference rouge grammarly grammar checking results asked turkers rank models output based grammaticality error shows number grammar errors sentences reported grammarly jecs model achieves highest rouge preferred humans making relatively errors els results shown table ers roughly equal preference model extlstmdel model learned supervised compression data jecs model achieves substantially higher rouge score indicating represents effective compression approach found absolute grammaticality ments hard achieve mechanical turk turkers ratings grammaticality noisy consistently rate true article sentences obviously noised variants fore turn methods described paragraphs automatic grammar checking use marly check sentences sampled outputs models mentioned cnn extlstmdel jecs small number grammar errors higher purely extractive baseline major source errors jecs having wrong article deletion adjective like awesome style manual error analysis better sense model output conduct manual ysis applied compressions sense valid manually examined model summaries comparing output raw sentences compression tied following errors bad deletions parsing errors like national london inappropriate adjective deletions causing correctness issues respect erence document like president clear weapon errors partial tion slang inappropriate attachment tion unhandled grammatical construction reference summary prediction compressions mullah omar reclusive founder afghan taliban charge new biography claims taliban insider says rumors eyed militant dead cnn mullah mohammed omar leader taliban declared islamic emirate afghanistan taliban cultural sion released page document different translations movement website ostensibly commemorate anniversary april meeting afghanistan kandahar province assembly afghans swore allegiance omar rebecca francis photo raffe shared ricky gervais francis threatened twitter picture francis hunter said giraffe close death food locals cnn years ago rebecca francis posed photo lying dead giraffe trouble started monday comedian ricky gervais tweeted photo question francis appeared nbc sports network outdoor lifestyle eye hunter ject interview hunting life late march responded statement huntinglife com tuesday posted facebook page president barack frida ghitis obama right want deal gives iran says framework agreement starts lifting iran sanctions soon cnn president barack obama tied mast nuclear deal iran democratic candidate president reaching good solid agreement iran worthy desirable goal process unfolded destructive inuence political considerations ening america hand strengthening iran table examples applied compressions sampled compressed examples dataset jecs model able delete large chunks especially temporal pps giving dates events individual modiers determined relevant summary specication anniversary example features modest compression students rst athletes second examples output shown table rst examples sampled compressed examples corpus variety compression options rst examples including removal temporal pps large subordinate clauses tives parentheticals example tures compression removing handful adjectives manner slightly changes meaning summary improving parser deriving semantically aware set compression rules help achieving better grammaticality ity note errors largely orthogonal core approach ned set compression options dropped system changing fundamental model compression analysis compression threshold compression model imbalanced binary classication lem trained model natural classication threshold probability del optimal downstream rouge experiment varying classication threshold deletion heuristic deduplication compressible pieces removed results cnn shown figure figure effect changing compression old cnn axis shows average dotted line tive baseline model outperforms extractive model achieves nearly optimal performance range threshold values average rouge value different compression thresholds model achieves best mance performs wide range compression robust provides controllable parameter change compression produced summaries compression type analysis break types compressions model table shows compressions model ends choosing test time pps pressed deduplication mechanism compressible pps tend temporal tion adjuncts redundant averageextraction node type len comps comp acc dedup advp prn table compressions model cnn average lengths fraction constituency type compressions taken model comp acc indicates frequently compression taken oracle note error especially keeping constituents minimal pact summary quality dedup indicates age chosen compressions arise cation opposed model prediction proposed deep generative model text compression zhang explored compression module extraction model separation modules hurt formance work relying syntax gives easily understandable controllable compression options contemporaneously work mendes explored extractive pressive approach compression integrated sequential decoding process approach leverage explicit syntax makes different model design choices tences manual deduplication nism model matches ground truth time low accuracy actually cause low nal rouge score compression choices affect nal rouge score small details compression options tary material related work neural extractive summarization neural works shown effective extractive summarization past approaches structured decision binary classication sentences cheng lapata nallapati classication followed ranking narayan zhou seq seq decoder instead model text compression forms module largely orthogonal extraction module additional improvements extractive modeling expected stack approach syntactic compression prior explosion neural models summarization syntactic compression martins smith send lapata relatively mon systems explored usage stituency parses berg kirkpatrick wang based approaches hirao durrett approach follows vein combined sophisticated neural text compression methods neural text compression filippova presented lstm approach based sentence compression miao blunsom conclusion work presented neural network work extractive compressive tion model consists sentence extraction model joined compression classier cides delete syntax derived pression options sentence training model involves nding oracle set extraction compression decision high score combination beam search procedure heuristics model outperforms past work cnn daily mail corpus terms rouge achieves substantial gains extractive model appears acceptable grammaticality according human evaluations acknowledgments work partially supported nsf grant bloomberg data science grant equipment grant nvidia thors acknowledge texas advanced ing center tacc university texas austin providing hpc resources duct research results presented paper obtained chameleon testbed ported national science foundation hey thanks anonymous reviewers helpful comments references taylor berg kirkpatrick dan gillick dan klein jointly learning extract compress proceedings annual meeting ciation computational linguistics human guage technologies pages association computational linguistics ziqiang cao furu wei wenjie sujian faithful original fact aware neural tive summarization aaai conference cial intelligence jaime carbonell jade goldstein use mmr diversity based reranking reordering documents producing summaries ings annual international acm sigir conference research development mation retrieval sigir pages new york usa acm lynn carlson daniel marcu mary ellen okurovsky building discourse tagged corpus framework rhetorical structure proceedings second sigdial theory workshop discourse dialogue yen chun chen mohit bansal fast tive summarization reinforce selected tence rewriting proceedings annual meeting association computational guistics volume long papers pages association computational linguistics jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings annual meeting sociation computational linguistics volume long papers pages association putational linguistics sumit chopra michael auli alexander rush abstractive sentence summarization tentive recurrent neural networks ings conference north american chapter association computational guistics human language technologies pages association computational linguistics trevor cohn mirella lapata sentence pression tree transduction artif int res yue dong yikang shen eric crawford herke van hoof jackie chi kit cheung ditsum extractive summarization contextual bandit proceedings conference empirical methods natural language ing pages association tional linguistics greg durrett taylor berg kirkpatrick dan klein learning based single document rization compression anaphoricity proceedings annual straints ing association computational linguistics volume long papers pages ciation computational linguistics katja filippova enrique alfonseca carlos menares lukasz kaiser oriol vinyals sentence compression deletion lstms proceedings conference cal methods natural language processing pages association computational tics sebastian gehrmann yuntian deng alexander rush abstractive proceedings conference tion empirical methods natural language ing pages association tional linguistics dan gillick benoit favre scalable global proceedings model summarization workshop integer linear programming ral language processing pages association computational linguistics yoav goldberg joakim nivre dynamic oracle arc eager dependency parsing ceedings coling pages coling organizing committee max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies ings conference north chapter association computational linguistics human language technologies ume long papers pages association computational linguistics karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend cortes lawrence lee sugiyama garnett editors advances neural information processing systems pages curran associates inc tsutomu hirao yasuhisa yoshida masaaki nishino norihito yasuda masaaki nagata single document summarization tree proceedings sack problem ference empirical methods natural language processing pages association putational linguistics yichen jiang mohit bansal closed book training improve summarization encoder ory proceedings conference pirical methods natural language processing pages association computational linguistics kate keahey pierre riteau dan stanzione tim erill joe mambretti paul rad paul ruth chameleon scalable production testbed puter science research contemporary high formance computing petascale cale edition volume chapman hall crc computational science chapter pages crc press boca raton diederik kingma jimmy adam method stochastic optimization arxiv preprint kevin knight daniel marcu based summarization step sentence proceedings seventeenth pression tional conference articial intelligence twelfth conference innovative applications articial intelligence pages aaai press kevin knight daniel marcu tion sentence extraction probabilistic approach sentence compression artif intell chen yang liu fei liu lin zhao fuliang weng improving multi documents marization sentence compression based panded constituent parse trees proceedings conference empirical methods ral language processing emnlp pages association computational linguistics haoran junnan zhu jiajun zhang chengqing zong ensure correctness mary incorporate entailment knowledge proceedings stractive sentence summarization international conference tational linguistics pages association computational linguistics junyi jessy kapil thadani amanda stent role discourse units near extractive summarization proceedings annual meeting special interest group discourse dialogue pages association putational linguistics chin yew lin rouge package matic evaluation summaries text tion branches christopher manning mihai surdeanu john bauer jenny finkel steven bethard david mcclosky stanford corenlp natural language processing toolkit proceedings annual meeting association computational guistics system demonstrations pages sociation computational linguistics andre martins noah smith tion joint model sentence extraction proceedings workshop compression integer linear programming natural language processing pages association tional linguistics afonso mendes shashi narayan sebastiao miranda zita marinho andre martins shay cohen jointly extracting compressing documents summary state representations proceedings conference north american chapter association tional linguistics human language technologies volume long short papers yishu miao phil blunsom language latent variable discrete generative models sentence compression proceedings conference empirical methods natural guage processing pages association computational linguistics ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural work based sequence model extractive marization documents aaai conference articial intelligence ramesh nallapati bowen zhou cicero dos santos caglar gulcehre bing xiang stractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning pages association computational linguistics shashi narayan shay cohen mirella lapata ranking sentences extractive tion reinforcement learning proceedings conference north american ter association computational linguistics human language technologies volume long pers pages association tional linguistics romain paulus caiming xiong richard socher deep reinforced model abstractive international conference summarization learning representations matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word resentations proceedings conference north american chapter association computational linguistics human language technologies volume long papers pages association computational linguistics xian qian yang liu fast joint compression summarization graph cuts ings conference empirical methods natural language processing pages association computational linguistics alexander rush sumit chopra jason ston neural attention model tive sentence summarization proceedings conference empirical methods natural language processing pages association computational linguistics evan sandhaus new york times annotated corpus linguistic data consortium philadelphia abigail peter liu christopher ning point summarization pointer generator networks proceedings annual meeting association tational linguistics volume long papers pages association computational tics jiwei tan xiaojun wan jianguo xiao stractive document summarization proceedings based attentional neural model annual meeting association computational linguistics volume long pers pages association tional linguistics liangguo wang jing jiang hai leong chieu chen hui ong dandan song lejian liao improving syntax help based sentence compression model new proceedings annual mains ing association computational linguistics volume long papers pages canada association computational guistics wang hema raghavan vittorio castelli radu rian claire cardie sentence pression based framework query focused proceedings document summarization annual meeting association tational linguistics volume long papers pages association computational tics kristian woodsend mirella lapata ing simplify sentences quasi synchronous grammar integer programming proceedings conference empirical methods natural language processing pages sociation computational linguistics xingxing zhang mirella lapata furu wei ming zhou neural latent extractive document proceedings summarization ference empirical methods natural language processing pages association putational linguistics qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural ument summarization jointly learning score proceedings select sentences annual meeting association tional linguistics volume long papers pages association computational tics dataset train dev test ref len doc len cnn table statistics cnn daily mail text datasets cnn features shortest reference summaries overall sion effective node type len comps oracle comp sbar advp table statistics compression options cnn constituency types pressible average length fraction available compressions accounts quently oracle says compress constituents adjectives compressable hurting rouge supplementary material experimental setup data preprocessing preprocess datasets scripts provided uses stanford corenlp tokenization ning use non anonymized version cnn previous rization work new york times corpus lter examples abstracts shorter words following criteria durrett yielding nyt dataset tics datasets listed table ing sentence selection select tences cnn sentences nyt gave best performance tic analysis datasets parsed stituency parser stanford corenlp manning implementation details use trained word embeddings narayan size sentence document representation vectors sion module use elmo ized encoder tuning parameter project vectors dimensions ter elmo layer dropout applied word embedding layers lstm layers rate use adam optimizer kingma initial learning rate model converges epochs training tial experiments found elmo useful sentence selection plify comparisons past work ing issues use compression use rouge lin evaluation acle construction use simplied unigram bigram scores faster approximation rouge turk instructions figure shows interface amazon turk man evaluation type analysis table statistics sion options cnn attachment adjectives compression options according oracle half line parameters figure interface amazon turk human evaluation examples fully shufed
