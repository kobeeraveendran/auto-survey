text summarization pretrained encoders yang liu mirella lapata institute language cognition computation school informatics university edinburgh yang abstract bidirectional encoder representations transformers bert devlin resents latest incarnation pretrained guage models recently advanced wide range natural language processing tasks paper showcase bert usefully applied text tion propose general framework extractive abstractive models duce novel document level encoder based bert able express semantics document obtain representations sentences extractive model built encoder stacking sentence transformer layers abstractive summarization propose new tuning schedule adopts different optimizers encoder decoder means leviating mismatch pretrained demonstrate staged tuning approach boost quality generated summaries experiments datasets model achieves art results board tractive abstractive settings introduction language model pretraining advanced state art nlp tasks ranging sentiment analysis question answering ral language inference named entity recognition textual similarity state art pretrained models include elmo peters gpt radford recently tional encoder representations ers bert devlin bert combines word sentence representations single large transformer vaswani code available nlpyang presumm pretrained vast amounts text pervised objective masked language modeling sentence prediction tuned task specic objectives cases pretrained language models employed encoders paragraph level natural language understanding problems devlin involving classication tasks predicting sentences entailment relationship determining completion sentence alternative sentences paper amine inuence language model ing text summarization different ous tasks summarization requires wide coverage natural language understanding going meaning individual words sentences aim condense document shorter sion preserving meaning thermore abstractive modeling tions task requires language generation pabilities order create summaries containing novel words phrases featured source text extractive summarization ned binary classication task labels dicating text span typically sentence included summary explore potential bert text marization general framework passing extractive abstractive ing paradigms propose novel level encoder based bert able encode document obtain representations sentences extractive model built encoder stacking sentence transformer layers capture level features extracting sentences stractive model adopts encoder decoder tecture combining pretrained bert coder randomly initialized transformer coder vaswani design new training schedule separates optimizers encoder decoder order modate fact pretrained trained scratch finally motivated previous work showing bination extractive abstractive objectives help generate better summaries gehrmann present stage approach encoder tuned twice rst extractive objective subsequently stractive summarization task evaluate proposed approach single document news summarization datasets representative different writing conventions important information concentrated beginning document distributed evenly summary styles bose telegraphic extractive tive datasets experimentally proposed models achieve state art results extractive abstractive tings contributions work fold highlight importance document encoding summarization task variety recently proposed techniques aim enhance summarization performance copying nisms ati reinforcement learning narayan paulus dong multiple communicating encoders likyilmaz achieve better results minimum requirement model mechanisms showcase ways effectively employ pretrained language models summarization extractive tive settings expect improvements model pretraining translate better rization future proposed models stepping stone improve summarization performance baselines new proposals tested background pretrained language models pretrained language models peters radford devlin dong zhang recently emerged key technology achieving pressive gains wide variety natural guage tasks models extend idea word embeddings learning contextual sentations large scale corpora guage modeling objective bidirectional encoder representations transformers bert vlin new language representation model trained masked language modeling sentence prediction task corpus words general architecture bert shown left figure input text rst cessed inserting special tokens cls appended beginning text output representation token aggregate formation sequence sication tasks token sep inserted sentence indicator sentence aries modied text represented sequence tokens token assigned kinds embeddings token embeddings indicate meaning token segmentation embeddings criminate sentences sentence pair classication task position beddings indicate position token text sequence embeddings summed single input vector fed bidirectional transformer multiple layers input vectors layer normalization operation mhatt multi head attention operation vaswani superscript indicates depth stacked layer layer bert erate output vector token rich contextual information pretrained language models usually enhance performance language understanding tasks recently attempts apply pretrained models generation problems edunov rothe tuning specic task unlike elmo parameters usually xed parameters bert jointly tuned additional specic parameters extractive summarization extractive summarization systems create mary identifying subsequently nating important sentences ument neural models consider extractive figure architecture original bert model left bertsum right sequence input document followed summation kinds embeddings token summed vectors input embeddings bidirectional transformer layers generating contextual vectors token bertsum extends bert inserting multiple cls symbols learn sentence representations interval segmentation embeddings illustrated red green color distinguish multiple sentences marization sentence classication problem neural encoder creates sentence representations classier predicts sentences selected summaries summarunner lapati earliest neural approaches adopting encoder based rent neural networks refresh narayan reinforcement learning based system trained globally optimizing rouge metric recent work achieves higher performance sophisticated model structures tent zhang frames extractive marization latent variable inference problem instead maximizing likelihood gold standard labels latent model directly imizes likelihood human summaries given selected sentences sumo liu talizes notion structured attention duce multi root dependency tree representation document predicting output mary neusum zhou scores lects sentences jointly represents state art extractive summarization abstractive summarization neural approaches abstractive summarization conceptualize task sequence sequence problem encoder maps sequence tokens source document sequence continuous representations decoder generates target summary token token auto regressive manner modeling ditional probability rush nallapati rst apply neural decoder architecture text summarization enhance model generator network ptgen allows copy words source text coverage mechanism cov keeps track words summarized celikyilmaz propose abstractive system multiple agents encoders represent document hierarchical attention mechanism agents decoding deep ing agents dca model trained end end reinforcement learning paulus present deep reinforced model drm abstractive summarization handles erage problem intra attention mechanism decoder attends previously erated words gehrmann follow approach bottomup content lector rst determines phrases source document summary copy mechanism applied preselected phrases decoding narayan propose abstractive model larly suited extreme summarization single sentence summaries based convolutional ral networks additionally conditioned topic distributions fine tuning bert summarization summarization encoder bert tune ous nlp tasks application summarization layersinput documenttoken embeddingssegment embeddingsposition bertbert summarization straightforward bert trained masked language model output vectors grounded tokens instead sentences extractive summarization models nipulate sentence level representations segmentation embeddings represent different tences bert apply pair inputs summarization code manipulate multi sentential inputs ure illustrates proposed bert architecture summarization bertsum order represent individual sentences insert external cls tokens start sentence cls symbol collects features sentence preceding use terval segment embeddings distinguish ple sentences document senti assign segment embedding depending odd example document assign embeddings way document representations learned hierarchically lower transformer layers represent adjacent sentences higher ers combination self attention represent multi sentence discourse position embeddings original bert model maximum length come limitation adding position beddings initialized randomly tuned parameters encoder extractive summarization let denote document containing sentences sentm senti sentence document extractive tion dened task assigning label senti indicating sentence included summary assumed summary sentences represent important content document bertsum vector vector cls symbol layer representation senti inter sentence transformer layers stacked bert outputs capture document level features extracting summaries denotes tence vectors output bertsum tion posemb adds sinusoid positional dings vaswani indicating position sentence nal output layer sigmoid classier vector senti layer layer transformer experiments implemented transformers found transformer performed best model bertsumext loss model binary tion entropy prediction gold label inter sentence transformer layers jointly tuned bertsum use adam mizer ing rate schedule follows vaswani warming warmup min step step warmup abstractive summarization use standard encoder decoder framework abstractive summarization encoder pretrained bertsum coder layered transformer initialized domly conceivable match encoder decoder pretrained trained scratch tuning unstable example encoder overt data decoder underts vice versa circumvent design new tuning schedule separates optimizers coder decoder use adam optimizers encoder decoder spectively different warmup steps learning rates lre lre step lrd step lre warmupe encoder lrd decoder based assumption pretrained encoder tuned smaller learning rate smoother decay encoder trained accurate gradients decoder stable datasets docs train val test cnn dailymail nyt xsum words avg doc length sentences words avg summary length novel grams gold summary sentences table comparison summarization datasets size training validation test sets average document summary length terms words sentences proportion novel grams appear source documents appear gold summaries quanties corpus bias extractive methods addition propose stage tuning approach rst tune encoder extractive summarization task section tune abstractive tion task section previous work gehrmann suggests extractive objectives boost performance abstractive summarization notice stage approach conceptually ple model advantage information shared tasks mentally changing architecture default abstractive model bertsumabs stage tuned model bertsumextabs experimental setup section describe summarization datasets experiments discuss ous implementation details summarization datasets evaluated model benchmark datasets cnn dailymail news lights dataset hermann new york times annotated corpus nyt sandhaus xsum narayan datasets represent different summary styles ing highlights brief sentence summaries summaries vary respect type rewriting operations exemplify showcase cut paste tions genuinely abstractive ble presents statistics datasets test set example gold standard summaries provided supplementary material cnn dailymail contains news articles sociated highlights bullet points giving brief overview article dard splits hermann training validation testing cnn documents dailymail documents anonymize entities rst split sentences stanford corenlp toolkit manning pre processed dataset following input uments truncated tokens nyt contains articles abstractive summaries following durrett split training test ples based date publication test set contains articles published january onward examples training validation set followed ltering procedure documents summaries words removed dataset ltered test set includes amples sentences split stanford corenlp toolkit manning processed following durrett input documents truncated tokens xsum contains news articles nied sentence summary answering question article splits narayan training tion testing lowed pre processing introduced work input documents truncated tokens aside statistics datasets table reports proportion novel grams gold summaries measure abstractiveness expect els extractive biases perform better datasets extractive summaries abstractive models perform rewrite erations datasets abstractive summaries cnn dailymail nyt somewhat tive xsum highly abstractive implementation details extractive abstractive settings pytorch opennmt klein bert base version bert plement bertsum source target texts fhbjq tokenized bert subwords tokenizer extractive summarization extractive els trained steps gpus gtx gradient accumulation steps model checkpoints saved ated validation set steps selected checkpoints based ation loss validation set report eraged results test set greedy gorithm similar nallapati obtain oracle summary document train tractive models algorithm generates oracle consisting multiple sentences maximize score gold summary predicting summaries new ment rst use model obtain score sentence rank sentences scores highest lowest select sentences summary sentence selection use trigram blocking reduce redundancy paulus given summary candidate tence skip exists trigram lapping intuition lar maximal marginal relevance mmr bonell goldstein wish minimize similarity sentence ered sentences lected summary abstractive summarization abstractive models applied dropout probability linear layers label smoothing szegedy smoothing factor transformer decoder hidden units hidden size feed forward ers models trained steps gpus gtx gradient cumulation steps model checkpoints saved evaluated validation set ery steps selected checkpoints based evaluation loss validation set report averaged results test set decoding beam search size tuned length penalty validation set decode end sequence token emitted repeated trigrams blocked paulus worth noting decoder plies copy coverage mechanism despite popularity stractive summarization mainly model oracle extractive summarunner nallapati refresh narayan latent zhang neusum zhou sumo liu transformerext abstractive ptgen drm paulus bottomup gehrmann dca celikyilmaz transformerabs bert based bertsumext bertsumext interval embeddings bertsumext large bertsumabs bertsumextabs table rouge results cnn dailymail test set shorthands unigram bigram overlap longest common subsequence sults comparison systems taken thors respective papers obtained data ning publicly released software focus building minimum requirements model mechanisms introduce ditional hyper parameters tune thanks subwords tokenizer rarely observe sues vocabulary words trigram blocking produces diverse summaries managing reduce repetitions results automatic evaluation evaluated summarization quality cally rouge lin report unigram bigram overlap means assessing tiveness longest common subsequence rouge means assessing uency table summarizes results cnn dailymail dataset rst block ble includes results extractive oracle system upper bound present baseline simply selects rst sentences document second block table includes extractive models trained cnn dailymail dataset section overview model oracle extractive compress durrett sumo liu transformerext abstractive ptgen ptgen cov drm paulus transformerabs bert based bertsumext bertsumabs bertsumextabs model oracle lead abstractive ptgen narayan transformerabs bert based bertsumabs bertsumextabs table rouge results xsum test set results comparison systems taken thors respective papers obtained data ning publicly released software table rouge recall results nyt test set sults comparison systems taken thors respective papers obtained data ning publicly released software table cells lled results available comparison model mented non pretrained transformer baseline transformerext uses ture bertsumext fewer parameters randomly initialized trained summarization task transformerext ers hidden size feed forward lter size model trained settings vaswani block table highlights formance abstractive models cnn dailymail dataset section overview include abstractive baseline decoder abstractive bertsum els encoder layer transformer hidden size feed forward lter size fourth block reports results tuned bert models bertsumext ants interval embeddings large version bert abs bertsumextabs bert based els outperform baseline strawman cnn dailymail corpus superior extractive pati narayan zhou abstractive models bert models collectively outperform previously proposed extractive abstractive systems falling oracle upper bound bert variants bertsumext performs best entirely surprising cnn dailymail summaries somewhat tive abstractive models prone ing sentences source document trained dataset unsurprisingly observe larger versions bert lead performance improvements interval embeddings bring slight gains table presents results nyt dataset following evaluation protocol durrett use limited length rouge recall predicted summaries truncated length gold summaries report performance oracle upper bound baseline second block table contains previously proposed extractive models transformer baseline press durrett ilp based model combines compression anaphoricity constraints block includes abstractive models literature transformer baseline bert based models shown fourth block observe perform previously proposed approaches dataset abstractive bert models generally form better compared bertsumext approaching oracle performance table summarizes results xsum dataset recall summaries dataset highly abstractive table consisting gle sentence conveying gist document extractive models perform poorly orated low performance lead line simply selects leading sentence document oracle lects single best sentence document table result report results extractive models dataset second lrd lre table model perplexity cnn dailymail tion set different combinations encoder decoder learning rates block table presents results stractive models taken narayan includes abstractive transformer baseline block results bert summarizers rior previously reported models wide margin model analysis learning rates recall abstractive model uses separate optimizers encoder table examine decoder combination different learning rates lre lrd benecial specically port model perplexity cnn dailymail idation set varying encoder decoder learning rates model performs best lre lrd position extracted sentences addition evaluation based rouge lyzed detail summaries produced model extractive setting looked position source document tences selected appear mary figure shows proportion selected summary sentences appear source document positions analysis conducted cnn dailymail dataset oracle summaries produced sumext transformerext oracle summary sentences fairly smoothly distributed documents summaries created transformerext concentrate rst document sentences bertsumext puts similar oracle summaries cating pretrained encoder model relies shallow position features learns deeper document representations novel grams analyzed output abstractive systems calculating proportion novel grams appear summaries source texts results shown figure cnn dailymail dataset figure proportion extracted sentences according position original document portion novel grams automatically ated summaries lower compared ence summaries xsum gap smaller observe cnn dailymail bertextabs produces novel ngrams bertabs surprising bertextabs biased selecting sentences source document initially trained extractive model supplementary material includes examples system output additional ablation studies human evaluation addition automatic evaluation ated system output eliciting human judgments report experiments following answering paradigm clarke lapata narayan quanties degree summarization models retain key information document paradigm set questions created based gold summary assumption highlights important document content participants asked answer tions reading system summaries access article questions tem answer better summarizing document assessed overall ity summaries produced abstractive tems ability rewrite content produce disuent ungrammatical output specically followed best worst ing kiritchenko mohammad method participants presented output systems original document cnn dailymail dataset abstractive lead ptgen bottomup gold bertsum cnn rank nyt rank xsum rank table based ranking based evaluation models signicantly different sum paired student test table cells lled system output available gold setting lead rank evaluation results extractive abstractive systems shown tables respectively compared best performing bertsum model setting extractive abstractive state art systems output publicly available lead baseline gold standard upper bound shown tables participants overwhelmingly fer output model comparison systems datasets evaluation paradigms differences bertsum son models statistically signicant exception table xsum evaluation setting paper showcased pretrained bert usefully applied text summarization introduced novel document level encoder proposed general framework tive extractive summarization experimental results datasets model achieves state art results board automatic human based evaluation tocols mainly focused ment encoding summarization future like advantage capabilities bert language generation acknowledgments research supported google phd lowship rst author gratefully edge support european research cil lapata award number translating multiple modalities text like thank shashi narayan providing xsum dataset xsum dataset figure proportion novel grams model ated summaries extractive lead neusum sumo transformer bertsum cnn nyt table based evaluation models nicantly different bertsum paired dent test table cells lled system output available conclusions asked decide better according criteria informativeness fluency cinctness types evaluation conducted amazon mechanical turk platform cnn dailymail nyt datasets documents total questions previous work narayan liu xsum randomly selected documents questions release narayan elicited sponses hit regard evaluation adopted scoring mechanism clarke lapata correct answers marked score partially correct answers zero quality based evaluation rating system puted percentage times chosen better minus times selected worse ratings range worst best grams references jimmy lei jamie ryan kiros geoffrey arxiv preprint ton layer normalization sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference cal methods natural language processing pages brussels belgium jaime carbonell jade goldstein use mmr diversity based reranking ing documents producing summaries ceedings annual international acl gir conference research development information retrieval pages melbourne australia jiatao zhengdong hang victor incorporating copying mechanism proceedings sequence sequence learning annual meeting association putational linguistics volume long papers pages berlin germany association computational linguistics asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents proceedings abstractive summarization conference north american chapter association computational linguistics man language technologies volume long pers pages new orleans louisiana james clarke mirella lapata discourse constraints document compression tional linguistics jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language standing proceedings conference north american chapter association computational linguistics human language nologies volume long short papers pages minneapolis minnesota dong nan yang wenhui wang furu wei xiaodong liu wang jianfeng gao ming zhou hsiao wuen hon unied language model pre training natural language arxiv preprint understanding generation yue dong yikang shen eric crawford herke van hoof jackie chi kit cheung sum extractive summarization contextual dit proceedings conference pirical methods natural language processing pages brussels belgium greg durrett taylor berg kirkpatrick dan klein learning based single document tion compression anaphoricity constraints proceedings annual meeting sociation computational linguistics volume long papers pages berlin germany sergey edunov alexei baevski michael auli pre trained language model representations proceedings language generation conference north american chapter association computational linguistics man language technologies volume long short papers pages minneapolis nesota karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend advances ral information processing systems pages svetlana kiritchenko saif mohammad best worst scaling reliable rating scales case study sentiment intensity annotation proceedings annual meeting sociation computational linguistics volume short papers pages vancouver canada guillaume klein yoon kim yuntian deng jean senellart alexander rush opennmt open source toolkit neural machine translation proceedings acl system tions pages vancouver canada wei xinyan xiao yajuan lyu yuanzhuo wang improving neural abstractive ment summarization explicit information proceedings tion modeling ference empirical methods natural language processing pages brussels belgium chin yew lin rouge package matic evaluation summaries text tion branches pages barcelona spain yang liu ivan titov mirella lapata single document summarization tree induction ceedings conference north ican chapter association computational linguistics human language technologies ume long short papers pages minneapolis minnesota christopher manning mihai surdeanu john bauer jenny finkel steven bethard david mcclosky stanford corenlp natural language proceedings annual cessing toolkit meeting association computational guistics system demonstrations pages timore maryland ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization abigail peter liu christopher manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers pages vancouver canada christian szegedy vincent vanhoucke sergey ioffe jon shlens zbigniew wojna ing inception architecture computer vision proceedings ieee conference puter vision pattern recognition cvpr pages las vegas nevada ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need advances neural information cessing systems pages yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey google neural machine translation system bridging gap arxiv preprint man machine translation xingxing zhang mirella lapata furu wei ming zhou neural latent extractive document marization proceedings conference empirical methods natural language cessing pages brussels belgium xingxing zhang furu wei ming zhou bert document level pre training hierarchical bidirectional transformers document proceedings annual meeting tion association computational linguistics pages florence italy association computational linguistics qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural ment summarization jointly learning score select sentences proceedings annual meeting association computational guistics volume long papers pages melbourne australia documents proceedings aaai ference articial intelligence pages san francisco california ramesh nallapati bowen zhou cicero dos santos aglar gulcehre bing xiang stractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning pages berlin shashi narayan shay cohen mirella lapata details summary topic aware convolutional neural networks proceedings treme summarization conference empirical methods natural guage processing pages brussels gium shashi narayan shay cohen mirella lapata ranking sentences extractive rization reinforcement learning ings conference north american chapter association computational guistics human language technologies volume long papers pages new orleans louisiana romain paulus caiming xiong richard socher deep reinforced model abstractive marization proceedings international conference learning representations ver canada matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word proceedings resentations ence north american chapter ation computational linguistics human guage technologies volume long papers pages new orleans louisiana alec radford karthik narasimhan tim salimans ilya sutskever improving language corr derstanding generative pre training sascha rothe shashi narayan aliaksei leveraging pre trained checkpoints arxiv preprint eryn sequence generation tasks alexander rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages lisbon portugal evan sandhaus new york times annotated corpus linguistic data consortium philadelphia
