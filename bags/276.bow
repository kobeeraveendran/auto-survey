hooks in the headline learning to generate headlines with controlled styles di zhijing joey tianyi lisa peter mit web services singapore college edu zhijing hku hk a star edu sg edu a m l c s c v v i x r a abstract current summarization systems only produce plain factual headlines but do not meet the practical needs of creating memorable titles to increase exposure we propose a new task stylistic headline generation shg to enrich the headlines with three style options humor romance and clickbait in order to attract more readers with no style specic article headline pair only a standard headline summarization dataset and mono style corpora our method titlestylist generates style specic headlines by combining the summarization and struction tasks into a multitasking framework we also introduced a novel parameter sharing scheme to further disentangle the style from the text through both automatic and human evaluation we demonstrate that titlestylist can generate relevant uent headlines with three target styles humor romance and bait the attraction score of our model erated headlines surpasses that of the state the art summarization model by and even outperforms human written references introduction every good article needs a good title which should not only be able to condense the core meaning of the text but also sound appealing to the ers for more exposure and memorableness ever currently even the best headline generation hg system can only fulll the above requirement yet performs poorly on the latter for example in figure the plain headline by an hg model summ leopard frog found in new york city is less eye catching than the style carrying ones such as what s that chuckle you hear it may be the new frog from nyc corresponding author code is available at titlestylist figure given a news article current hg models can only generate plain factual headlines failing to learn from the original human reference it is also much less attractive than the headlines with humorous romantic and click baity styles to bridge the gap between the practical needs for attractive headlines and the plain hg by the current summarization systems we propose a new task of stylistic headline generation shg given an article it aims to generate a headline with a target style such as humorous romantic and click baity it has broad applications in reader adapted title generation slogan suggestion for online post headlines and many others shg is a highly skilled creative process and ally only possessed by expert writers one of the most famous headlines in american publications sticks nix hick pix could be such an example in contrast the current best summarization systems are at most comparable to novice writers who vide a plain descriptive representation of the text body as the title cao et al a lin et al song et al dong et al these systems usually use a language generation model that mixes styles with other linguistic patterns and inherently lacks a mechanism to control the style new frog species discovered in new york city area it has adistinctive croak scientists nd leopard frog yet have a name ribbit frog species found in new york city has a croakof its ownoriginal headline articlesumm leopard frog found innew york city hg model output what that chuckle you hear it may be thenew frog from nychumorous a new frog with a croak of its own awaitsits name in the roads of facts about the new frog with a croak ofits ownclick baity explicitly more fundamentally the training data comprise of a mixture of styles e the gigaword dataset rush et al obstructing the models from learning a distinct style in this paper we propose the new task shg to emphasize the explicit control of style in headline generation we present a novel headline generation model titlestylist to produce enticing titles with target styles including humorous romantic and click baity our model leverages a multitasking framework to train both a summarization model on headline article pairs and a denoising coder dae on a style corpus in particular based on the transformer architecture vaswani et al we use the style dependent layer ization and the style guided encoder attention to disentangle the language style factors from the text this design enables us to use the shared content to generate headlines that are more relevant to the articles as well as to control the style by plugging in a set of style specic parameters we validate the model on three tasks humorous romantic and click baity headline generation both automatic and human evaluations show that titlestylist can generate headlines with the desired styles that peal more to human readers as in figure the main contributions of our paper are listed below to the best of our knowledge it is the rst research on the generation of attractive news headlines with styles without any supervised style specic article headline paired data through both automatic and human tion we demonstrated that our proposed tlestylist can generate relevant uent lines with three styles humor romance and clickbait and they are even more attractive than human written ones our model can exibly incorporate multiple styles thus efciently and automatically viding humans with various creative headline options for references and inspiring them to think out of the box headline generation as summarization headline generation is a very popular area of search traditional headline generation methods mostly focus on the extractive strategies using guistic features and handcrafted rules luhn edmundson mathis et al salton et al jing and mckeown radev and eown dorr et al to enrich the versity of the extractive summarization abstractive models were then proposed with the help of ral networks rush et al proposed based summarization abs to make banko et al s framework of summarization more erful many recent works extended abs by ing additional features chopra et al takase et al nallapati et al shen et al tan et al guo et al other variants of the standard headline generation ting include headlines for community question swering higurashi et al multiple headline generation iwama and kano user specic generation using user embeddings in tion systems liu et al bilingual headline generation shen et al and question style headline generation zhang et al only a few works have recently started to cus on increasing the attractiveness of generated headlines fan et al xu et al fan et al focuses on controlling several features of the summary text such as text length and the style of two different news outlets cnn and lymail these controls serve as a way to boost the model performance and the and style control shows a negligible improvement xu et al utilized reinforcement learning to courage the headline generation system to generate more sensational headlines via using the readers comment rate as the reward which however can not explicitly control or manipulate the styles of lines shu et al proposed a style transfer approach to transfer a non clickbait headline into a clickbait one this method requires paired news articles headlines data for the target style however for many styles such as humor and romance there are no available headlines our model does not have this limitation thus enabling transferring to many more styles related work text style transfer our work is related to summarization and text style transfer our work is also related to text style transfer which aims to change the style attribute of the text while preserving its content first proposed by shen et al it has achieved great progress in recent years xu et al lample et al zhang et al fu et al jin et al yang et al jin et al however all these methods demand a text corpus for the target style however in our case it is expensive and technically challenging to collect news headlines with humor and romance styles which makes this category of methods not applicable to our problem methods problem formulation is trained on a source dataset s the model and target dataset t the source dataset s consists of pairs of a news article a and its plain headline we assume that the source corpus has a distribution p a h where and h a the target corpus t comprises of sentences written in a specic style e humor we assume that it conforms to the distribution p t note that the target corpus t only contains carrying sentences not necessarily headlines it can be just book text also no sentence t is paired with a news article overall our task is to learn the conditional distribution p t using only s and t this task is fully unsupervised because there is no sample from the joint distribution p a t model architecture for summarization we adopt a sequence sequence model based on the former architecture vaswani et al as in figure it consists of a layer encoder e e and a layer decoder g g with a hidden size of and a feed forward lter size of for better generation quality we initialize with the mass model song et al mass is pretrained by masking a sentence fragment in the encoder and then predicting it in the decoder on large scale english monolingual data this training is adopted in the current state of the art systems across various summarization benchmark tasks including hg figure the transformer based architecture of our model figure training scheme multitask training is adopted to combine the summarization and dae tasks supervised training for es and gs with the source domain dataset s based on the encoder decoder architecture we can learn the ditional distribution p by training zs and hs to solve the supervised learning task where zs is the learned tent representation in the source domain the loss function of this task is gs log es gs where es and gs are the set of model ters of the encoder and decoder in the source main and denotes the overall probability of generating an output sequence h given the input article a which can be further expanded as follows es gs zs gs l multitask training scheme where l is the sequence length to disentangle the latent style from the text we adopt a multitask learning framework luong et al training on summarization and dae taneously as shown in figure dae training for et and gt for the target style corpus t since we only have the sentence t without paired news articles we train zt et t and t gt zt by solving an unsupervised multi head self attentionlayer normmlplayer normembembembencoderdecodermulti head encoder attentionmlpmulti head self attentionstyle dependent layer normstyle dependent querytransformationstyle dependent layer normembembemb construction learning task where zt is the learned latent representation in the target domain and t is the corrupted version of t by randomly deleting or blanking some words and shufing the word orders to train the model we minimize the reconstruction error lt lt et gt ett log type style layer normalization inspired by previous work on image style transfer dumoulin et al we make the scaling and shifting rameters for layer normalization in the transformer architecture un shared for each style this style layer normalization approach aims to transform a layers activation into a normalized activation specic to the style s where et and gt are the set of model eters for the encoder and generator in the target domain we train the whole model by jointly imizing the supervised training loss ls and the unsupervised denoised auto encoding loss lt via multitask learning so the total loss becomes gs et gt gs et gt where is a hyper parameter parameter sharing scheme more constraints are necessary in the multitask training process we aim to infer the conditional distribution as p t gt however without samples from p a t this is a ing or even impossible task if es and et or gs and gt are completely independent of each other hence we need to add some constraints to the network by relating es and et and gs and gt the simplest design is to share all parameters tween es and et and apply the same strategy to gs and gt the intuition behind this design is that by exposing the model to both tion task and style carrying text reconstruction task the model would acquire some sense of the target style while summarizing the article however to encourage the model to better disentangle the tent and style of text and more explicitly learn the style contained in the target corpus t we share all parameters of the encoder between two domains i e between es and et whereas we divide the parameters of the decoder into two types independent parameters ind and style dependent parameters dep this means that only the independent parameters are shared between gs and gt while the style dependent parameters are not more specically the parameters of the layer normalization and encoder attention modules are made style dependent as detailed below s where and are the mean and standard deviation of the batch of x and s and s are style specic parameters learned from data specically for the transformer decoder tecture we use a style specic self attention layer normalization and nal layer normalization for the source and target domains on all six decoder layers type style guided encoder attention our model architecture contains the attention nism where the decoder infers the probability of the next word not only conditioned on the ous words but also on the encoded input hidden states the attention patterns should be different for the summarization and the reconstruction tasks due to their different inherent nature we insert this thinking into the model by introducing the style guided encoder attention into the multi head attention module which is dened as follows q query w s q k key wk v value wv k v softmax v qktr dmodel where query key and value denote the triple of inputs into the multi head attention module w s q wk and wv denote the scaled dot product matrix for afne transformation dmodel is the dimension of the hidden states we specialize the dot product matrix w s q of the query for different styles so that q can be different to induce diverse attention patterns experiments datasets we compile a rich source dataset by combining the new york times nyt and cnn as well as three target style corpora on humorous romantic and click baity text the average sentence length in the nyt cnn humor romance and clickbait datasets are and words respectively source dataset the source dataset contains news articles paired with corresponding headlines to enrich the ing corpus we combine two datasets the new york times k and cnn k after ing these two datasets we randomly selected pairs as the validation set and another pairs as the test set we rst extracted the archival abstracts and headlines from the new york times nyt pus sandhaus and treat the abstracts as the news articles following the standard processing procedures kedzie et al we ltered out advertisement related articles as they are very different from news reports resulting in news abstracts headlines pairs we then add into our source set the cnn marization dataset which is widely used for ing abstractive summarization models hermann et al we use the short summaries in the original dataset as the news abstracts and cally parsed the headlines for each news from the dumped news web and in total collected news abstract headline pairs three target style corpora humor and romance for the target style datasets we follow chen et al to use mor and romance novel collections in pus zhu et al as the humor and romance datasets we split the documents into sentences tokenized the text and collected k sentences as our datasets clickbait we also tried to learn the writing style from the click baity headlines since they have shown superior attraction to readers thus we used the examiner spamclickbait news dataset noted as the clickbait dataset we collected k headlines for our use com summarization datasets use cnn instead of the dailymail dataset since lymail headlines are very long and more like short summaries nyu smashwords kaggle com some examples from each style corpus are listed in table style examples humor romance clickbait the crowded beach like houses in the burbs and the line ups at walmart berthold stormed out of the brewing argument with his violin and bow and went for a walk with it to practice for the much more receptive polluted air i can face it joyously and with all my heart and soul she said with bright blue and green buttercream scales sparkling eyes and purple candy melt wings it sat majestically on a rocky ledge made from chocolate year old girl and year old boy cused of attempting to kill mother who is the adult chilly dry weather welcomes to south florida end segregation in alabama bryce hospital sale offers a golden opportunity table examples of three target style corpora humor romance and clickbait baselines we compared the proposed titlestylist against the following ve strong baseline approaches neural headline generation nhg we train the state of the art summarization model mass song et al on our collected news abstracts headlines paired data gigaword mass we test an off the shelf line generation model mass from song et al which is already trained on gigaword a large scale headline generation dataset with around million articles neural story teller nst it breaks down the task into two steps which rst generates headlines from the aforementioned nhg model then applies style shift techniques to generate style specic headlines kiros et al in brief this method uses the skip thought model to encode a sentence into a representation vector and then manipulates its style by a linear transformation afterward this transformed representation vector is used to ize a language model pretrained on a style specic so that a stylistic headline can be generated com examine the examiner sent summary more details of this method can refer to the ofcial website fine tuned we rst train the nhg model as mentioned above then further ne tuned it on the target style corpus via dae training multitask we share all parameters between es and et and between gs and gt and trained the model on both the summarization and dae tasks the model architecture is the same as nhg evaluation metrics to evaluate the performance of the proposed tlestylist in generating attractive headlines with styles we propose a comprehensive twofold egy of both automatic evaluation and human ation setup of human evaluation we randomly sampled news abstracts from the test set and asked three native speaker annotators for evaluation to score the generated headlines specically we conduct two tasks to evaluate on four criteria relevance attractiveness language uency and style strength for the rst task the human raters are asked to evaluate these outputs on the rst three aspects relevance attractiveness and language uency on a likert scale from to integer values for relevance human annotators are asked to evaluate how tically relevant the headline is to the news body for attractiveness annotators are asked how tractive the headlines are for uency we ask the annotators to evaluate how uent and readable the text is after the collection of human evaluation results we averaged the scores as the nal score in addition we have another independent human uation task about the style strength we present the generated headlines from titlestylist and lines to the human judges and let them choose the one that most conforms to the target style such as humor then we dene the style strength score as the proportion of choices setup of automatic evaluation apart from the comprehensive human evaluation we use automatic evaluation to measure the eration quality through two conventional aspects summarization quality and language uency note com neural storyteller that the purpose of this two way automatic uation is to conrm that the performance of our model is in an acceptable range good automatic evaluation performances are necessary proofs to compliment human evaluations on the model tiveness summarization quality we use the standard tomatic evaluation metrics for summarization with the original headlines as the reference bleu pineni et al meteor denkowski and lavie rouge lin and cider vedantam et al for rouge we used the toolkit and for other rics we used the pycocoeval toolkit language fluency we ne tuned the medium model radford et al on our lected headlines and then used it to measure the perplexity ppl on the generated outputs experimental details we used the fairseq code base ott et al during training we use adam optimizer with an initial learning rate of and the batch size is set as tokens for each gpu with the parameters update frequency set as for the dom corruption for dae training we follow the standard practice to randomly delete or blank the word with a uniform probability of and domly shufed the word order within tokens all datasets are lower cased is set as in ments for each iteration of training we randomly draw a batch of data either from the source dataset or from the target style corpus and the sampling strategy follows the uniform distribution with the probability being equal to results and discussion human evaluation results the human evaluation is to have a comprehensive measurement of the performances we conduct experiments on four criteria relevance attraction uency and style strength we summarize the man evaluation results on the rst three criteria in table and the last criteria in table note that through automatic evaluation the baselines nst fine tuned and gigaword mass perform poorer than other methods in section thereby we com pltrdy com maluuba nlg eval on the development set is removed them in human evaluation to save essary work for human raters settings relevance attraction fluency style none humor romance clickbait nhg human multitask titlestylist multitask titlestylist multitask titlestylist table human evaluation on three aspects relevance attraction and uency none represents the original headlines in the dataset relevance we rst look at the relevance scores in table it is interesting but not surprising that the pure summarization model nhg achieves the highest relevance score the outputs from nhg are usually like an organic reorganization of several keywords in the source context as shown in ble thus appearing most relevant it is thy that the generated headlines of our titlestylist for all three styles are close to the original written headlines in terms of relevance validating that our generation results are qualied in this pect another nding is that more attractive or more stylistic headlines would lose some relevance since they need to use more words outside the news body for improved creativity attraction in terms of attraction scores in ble we have three ndings the written headlines are more attractive than those from nhg which agrees with our observation in section our titlestylist can generate more attractive headlines over the nhg and multitask baselines for all three styles demonstrating that adapting the model to these styles could improve the attraction and specialization of some ters in the model for different styles can further hance the attraction adapting the model to the clickbait style could create the most attractive headlines even out weighting the original ones which agrees with the fact that click baity lines are better at drawing readers attention to be noted although we learned the clickbait style into our summarization system we still made sure that we are generating relevant headlines instead of too exaggerated ones which can be veried by our relevance scores fluency the human annotated uency scores in table veried that our titlestylist generated lines are comparable or superior to the written headlines in terms of readability style strength we also validated that our tlestylist can carry more styles compared with the multitask and nhg baselines by summarizing the percentage of choices by humans for the most morous or romantic headlines in table automatic evaluation results apart from the human evaluation of the overall eration quality on four criteria we also conducted a conventional automatic assessment to gauge only the summarization quality this evaluation does not take other measures such as the style strength into consideration but it serves as important plimentary proof to ensure that the model has an acceptable level of summarization ability table summarizes the automatic evaluation results of our proposed titlestylist model and all baselines we use the summarization related uation metrics i e bleu rouge cider and meteor to measure how relevant the generated headlines are to the news articles to some extent by comparing them to the original human written headlines in table the rst row nhg shows the performance of the current state of the art marization model on our data and table provides two examples of its generation output our mate goal is to generate more attractive headlines than these while maintaining relevance to the news body from table the baseline gigaword mass scored worse than nhg revealing that directly plying an off the shelf headline generation model to new in domain data is not feasible although this model has been trained on more than times larger dataset both nst and fine tuned baselines present very poor summarization performance and the reason could be that both of them cast the lem into two steps summarization and style fer and the latter step is absent of the tion task which prevents the model from ing its summarization capability in contrast the multitask baseline involves the summarization and style transfer via tion training processes at the same time and shows superior summarization performance even pared with nhg this reveals that the vised reconstruction task can indeed help improve news abstract turkey s bitter history with kurds is guring nently in its calculations over how to deal with bush administration s request to use turkey as the base for thousands of combat troops if there is a war with iraq recep tayyip erdogan leader of turkey s governing party says publicly for the rst time that future of iraq s kurdish area which abuts border region of turkey also heavily populated by kurds is weighing heavily on negotiations hints at what turkish ofcials have been saying privately for weeks if war comes to iraq riding turkish objective would be less helping icans topple saddam hussein but rather preventing kurds in iraq from forming their own state reunied berlin is commemorating anniversary of the start of construction of berlin wall almost years since germans jubilantly celebrated reopening between east and west and attacked hated structure with sledgehammers some germans are championing the preservation of wall at the time when little remains beyond few crumbling remnants to remind berliners of unhappy division that many have since worked hard to heal and put behind them what little remains of physical wall embodies era that germans have yet to resolve for themselves they routinely talk of wall in the mind to describe social and cultural differences that continue to divide easterners and westerners human nhg turkey assesses question of kurds turkey s bitter history with kurds what if there is a war with kurds humor romance what if the kurds say no to iraq clickbait for turkey a long hard road the wall berlin ca nt quite demolish construction of berlin wall is commemorated the berlin wall years later is still there the berlin wall from the past to the present east vs west berlin wall lives on table examples of style carrying headlines generated by titlestylist style nhg multitask titlestylist humor romance clickbait table percentage of choices for the most ous or romantic headlines among titlestylist and two baselines nhg and multitask the supervised summarization task more tantly we use two different types of corpora for the reconstruction task one consists of headlines that are similar to the news data for the summarization task and the other consists of text from novels that are entirely different from the news data however unsupervised reconstruction training on both types of data can contribute to the summarization task which throws light on the potential future work in summarization by incorporating unsupervised learning as augmentation we nd that in table titlestylist f achieves the best summarization performance this implicates that compared with the multitask baseline where the two tasks share all parameters specialization of layer normalization and encoder attention ters can make gs focus more on summarization it is noteworthy that the summarization scores for titlestylist are lower than titlestylist f but still comparable to nhg this agrees with the fact that the gt branch more focuses on bringing in tic linguistic patterns into the generated summaries thus the outputs would deviate from the pure marization to some degree however the relevance degree of them remains close to the baseline nhg which is the starting point we want to improve on later in the next section we will further validate that these headlines are faithful to the new article through human evaluation we also reported the perplexity ppl of the erated headlines to evaluate the language uency as shown in table all outputs from baselines nhg and multitask and our proposed titlestylist show similar ppl compared with the test set used in the ne tuning stage ppl indicating that they are all uent expressions for news headlines extension to multi style we progressively expand titlestylist to include all three target styles humor romance and clickbait to demonstrate the exibility of our model that is we simultaneously trained the summarization task on the headlines data and the dae task on the three target style corpora and we made the layer normalization and encoder attention ters specialized for these four styles fact humor romance and clickbait and shared the other rameters we compared this multi style version titlestylist versatile with the previously presented single style counterpart as shown in table from this table we see that the bleu and rouge l scores of titlestylist versatile are comparable to titlestylist for all three styles besides we ducted another human study to determine the better headline between the two models in terms of tion and we allow human annotators to choose both style corpus model bleu rouge l cider meteor ppl len ratio none humor romance clickbait nhg gigaword mass nst fine tuned multitask titlestylist titlestylist f nst fine tuned multitask titlestylist titlestylist f nst fine tuned multitask titlestylist titlestylist f table automatic evaluation results of our titlestylist and baselines the test set of each style is the same but the training set is different depending on the target style as shown in the style corpus column none means no style specic dataset and humor romance and clickbait corresponds to the datasets we introduced in section during the inference phase our titlestylist can generate two outputs one from gt and the other from gs outputs from gt are style carrying so we denote it as titlestylist outputs from gs are plain and factual thus denoted as titlestylist f the last column len ratio denotes the average ratio of abstract length to the generated headline length by the number of words model bleu rg l pref headlines than state of the art hg models style none humor romance clickbait titlestylist versatile titlestylist versatile titlestylist titlestylist versatile titlestylist titlestylist versatile titlestylist table comparison between titlestylist versatile and titlestylist rg l denotes rouge l and pref denotes preference options if they deem them as equivalent the result is presented in the last column of table which shows that the attraction of titlestylist versatile outputs is competitive to titlestylist versatile thus generates multiple headlines in ent styles altogether which is a novel and efcient feature conclusion we have proposed a new task of stylistic headline generation shg to emphasize explicit control of styles in headline generation for improved traction to this end we presented a multitask framework to induce styles into summarization and proposed the parameters sharing scheme to enhance both summarization and stylization bilities through experiments we validated our proposed titlestylist can generate more attractive acknowledgement we appreciate all the volunteer native speakers shreya karpoor lisa orii abhishek mohan paloma quiroga for the human evaluation of our study and thank the reviewers for their ing comments joey tianyi zhou is partially ported by the agency for science technology and research under its ame programmatic funding scheme project no references michele banko vibhu o mittal and michael j brock headline generation based on cal translation in proceedings of the annual meeting on association for computational tics pages association for computational linguistics ziqiang cao wenjie li sujian li and furu wei retrieve rerank and rewrite soft template based neural summarization in acl ziqiang cao furu wei wenjie li and sujian li faithful to the original fact aware neural abstractive summarization in thirty second aaai conference on articial intelligence cheng kuan chen zhu feng pan ming yu liu and min sun unsupervised stylish image scription generation via domain layer norm in the thirty third aaai conference on articial gence aaai the thirty first innovative plications of articial intelligence conference iaai the ninth aaai symposium on educational advances in articial intelligence eaai olulu hawaii usa january february pages aaai press sumit chopra michael auli and alexander m rush abstractive sentence summarization with tentive recurrent neural networks in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies pages michael denkowski and alon lavie meteor universal language specic translation evaluation for any target language in proceedings of the ninth workshop on statistical machine translation pages li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon unied language model pre training for natural language arxiv preprint understanding and generation bonnie dorr david zajic and richard schwartz hedge trimmer a parse and trim approach to headline generation in proceedings of the naacl on text summarization workshop volume pages association for computational guistics vincent dumoulin jonathon shlens and manjunath kudlur a learned representation for artistic style arxiv preprint hp edmundson problems in automatic ing communications of the acm angela fan david grangier and michael auli in controllable abstractive summarization ceedings of the workshop on neural machine translation and generation bourne australia july pages sociation for computational linguistics zhenxin fu xiaoye tan nanyun peng dongyan zhao and rui yan style transfer in text ration and evaluation in thirty second aaai ference on articial intelligence yidi guo heyan huang yang gao and chi lu conceptual multi layer neural network model for headline generation in chinese computational guistics and natural language processing based on naturally annotated big data pages springer karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend in advances in neural information processing systems pages tatsuru higurashi hayato kobayashi takeshi suyama and kazuma murao extractive line generation based on learning to rank for nity question answering in proceedings of the international conference on computational tics pages kango iwama and yoshinobu kano multiple news headlines generation using page metadata in proceedings of the international conference on natural language generation association for computational linguistics di jin zhijing jin joey tianyi zhou and peter szolovits unsupervised domain adaptation for neural machine translation with iterative back translation arxiv preprint zhijing jin di jin jonas mueller nicholas matthews and enrico santus unsupervised text tribute transfer via iterative matching and translation in ijcnlp hongyan jing and kathleen mckeown the composition of human written summary sentences chris kedzie kathleen mckeown and hal daume iii content selection in deep learning models of summarization arxiv preprint ryan kiros yukun zhu ruslan r salakhutdinov richard zemel raquel urtasun antonio torralba and sanja fidler skip thought vectors in advances in neural information processing systems pages guillaume lample eric michael marcaurelio ranzato multiple attribute text rewriting in iclr ludovic subramanian denoyer and y lan boureau sandeep smith chin yew lin rouge a package for automatic in text summarization evaluation of summaries branches out pages junyang lin xu sun shuming ma and qi su global encoding for abstractive summarization in acl tianshang liu haoran li junnan zhu jiajun zhang and chengqing zong review headline eration with user embedding in chinese tional linguistics and natural language processing based on naturally annotated big data china national conference ccl and national symposium nlp nabd changsha china october proceedings pages hans peter luhn the automatic creation of erature abstracts ibm journal of research and velopment minh thang luong quoc v le ilya sutskever oriol vinyals and lukasz kaiser corr task sequence to sequence learning betty a mathis james e rush and carol e young improvement of automatic abstracts by the use of structural analysis journal of the american society for information science ramesh nallapati bowen zhou caglar gulcehre bing xiang al abstractive text tion using sequence to sequence rnns and beyond arxiv preprint myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier and michael auli fairseq a fast ble toolkit for sequence modeling arxiv preprint kishore papineni salim roukos todd ward and jing zhu bleu a method for automatic uation of machine translation in proceedings of the annual meeting of the association for tational linguistics pages dragomir r radev and kathleen r mckeown generating natural language summaries from tiple on line sources computational linguistics alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever language models are unsupervised multitask learners alexander m rush sumit chopra and jason a neural attention model for arxiv preprint ston stractive sentence summarization alexander m rush seas harvard sumit chopra and jason weston a neural attention model for in aclweb proceedings sentence summarization of the conference on empirical methods in natural language processing gerard salton amit singhal mandar mitra and chris buckley automatic text structuring and marization information processing management evan sandhaus the new york times annotated corpus linguistic data consortium philadelphia shi qi shen yan kai lin cun chao tu yu zhao yuan liu mao song sun al recent vances on neural headline generation journal of computer science and technology shiqi shen yun chen cheng yang zhiyuan liu and maosong sun zero shot cross lingual ieee acm trans audio ral headline generation speech language processing shiqi shen yu zhao zhiyuan liu maosong neural headline generation arxiv preprint sun et al with sentence wise optimization tianxiao shen tao lei regina barzilay and tommi s jaakkola style transfer from non parallel in advances in neural text by cross alignment information processing systems annual ference on neural information processing systems december long beach ca usa pages kai shu suhang wang thai le dongwon lee and huan liu deep headline generation for bait detection ieee international conference on data mining icdm pages kaitao song xu tan tao qin jianfeng lu and yan liu mass masked sequence to sequence pre training for language generation arxiv preprint sho takase jun suzuki naoaki okazaki tsutomu hirao and masaaki nagata neural line generation on abstract meaning representation in proceedings of the conference on cal methods in natural language processing pages jiwei tan xiaojun wan and jianguo xiao from neural sentence summarization to headline in ijcai generation a coarse approach pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all in advances in neural information you need cessing systems pages ramakrishna vedantam c lawrence zitnick and devi parikh cider consensus based image in proceedings of the ieee scription evaluation conference on computer vision and pattern tion pages jingjing xu xu sun qi zeng xiaodong zhang ancheng ren houfeng wang and wenjie li unpaired sentiment to sentiment translation a cled reinforcement learning approach in acl peng xu chien sheng wu andrea madotto and cale fung clickbait sensational headline generation with auto tuned reinforcement learning arxiv zichao yang zhiting hu chris dyer eric p xing and taylor berg kirkpatrick unsupervised text style transfer using language models as in advances in neural information tors ing systems annual conference on neural mation processing systems neurips december montreal canada pages ruqing zhang jiafeng guo yixing fan yanyan lan jun xu huanhuan cao and xueqi cheng question headline generation for news articles in proceedings of the acm international ence on information and knowledge management cikm torino italy october pages zhirui zhang shuo ren shujie liu jianyong wang peng chen mu li ming zhou and enhong chen style transfer as unsupervised machine lation arxiv yukun zhu ryan kiros rich zemel ruslan dinov raquel urtasun antonio torralba and sanja fidler aligning books and movies towards story like visual explanations by watching movies and reading books in proceedings of the ieee national conference on computer vision pages
