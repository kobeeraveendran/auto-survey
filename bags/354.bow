readonce transformers reusable representations text transformers shih ting lin ashish sabharwal tushar khot university texas austin u s allen institute ai seattle u s edu ashishs org t c o l c s c v v x r abstract large scale language models tremely effective directly ne tuned end tasks models learn extract information solve task simultaneously end task supervision wasteful general problem gathering information document task independent need learned scratch time information captured computable representation examples ing faster training evaluation els present transformer based approach readonce transformers trained build information capturing tions text model compresses ument variable length task independent representation ferent examples tasks requiring document read ally extend standard text text models consume readonce representations text solve multiple downstream tasks task independent sentations multi hop qa stractive qa summarization observe speedups compared standard text text models able handle long documents normally exceed length limit current models introduction transformer based large scale language models lms radford et al devlin et al task independent models surprisingly effective directly ne tuned ent end tasks rajpurkar et al wang et al reduced need ing task specic system architectures author s work primarily ship allen institute ai figure readonce transformers learning extract information specic task use transformer based encoders build independent reusable document representations feed models trained end tasks approach relies heavily end task pervision learn solve sub problems multaneously extract information input document d solve end task e answer question d incentivizes lm based models learn extract task specic example specic information tuned end task example ment classication model learn tract sentiments d question swering qa model learn extract d information needed specic question hand strategy effective datasets inefcient requires models trained scratch end task sub problem gathering text textmodeltext textmodeltext task independentrepresentationstask specicmodelsreadoncetransformersreadingcomprehensionsummarizationabstractive qarepr textmodelrepr textmodel tion content input document d shared tasks second d read scratch context example e question ples share d computational dundancy undesirable slow inference quickly bottleneck deployed real time tems models billions parameters read d input query d longer document worse problem inspired humans ability read ment extract key information having know use case advance ask following question use based lms build compressed representations text task independent reusable extend text transformer architectures consume representations conjunction text prior representation learning approaches tempt capture meaning sentences continuous vector conneau et al kiros et al effective downstream classication tasks unclear capture information content entire paragraphs importantly proaches designed based hidden tations recurrent networks fed task specic classiers contrast goal build document representations ing transformer based lms combine example specic text inputs way consumed transformer architecture end propose approach vert encoder decoder based transformer lm bart lewis et al new chitecture termed readonce transformer reads documents key properties create compressed information capturing reusable representations refer representations consumes document representations example specic plain text e question compressing information document xed length vector lossy long uments wasteful short documents use variable length representation scales length input document key challenge nd compressed tation contains right level mation choose capture tion answer factoid questions d nore syntax low level semantics ingly use supervision factoid qa datasets squad rajpurkar et al supervisedqa lewis et al ing readonce representations finally solve end task modify text text transformer architecture consume readonce tions example specic text coder experiments qa model trained use readonce representations demonstrate representations effective ing information compared baseline approaches representations generalize tasks multi hop qa yang et al tive qa kocisky et al tion narayan et al readonce representations puted cached train infer models faster standard approaches marginal drop accuracy points qa rouge l points summarization speedup sion ratio parameter representations vides easy way trade computation time accuracy experiments simulating long document setting demonstrate readonce transformers virtue capturing compressed representations stantially outperform standard truncation based usage transformer models document text long t approach provides novel way use existing transformer models handle large documents ing recent line research designing novel transformer architectures specically long uments beltagy et al zaheer et al related work representation learning approaches monly extract xed length sentence beddings conneau et al kiros et al variable length text inputs xed length representations enabled development simpler downstream models deal variable lengths textual inputs representations mainly simple classication tasks short input texts bowman et al wang et al recent work reimers gurevych et al artetxe schwenk karpukhin et al tried building document embedding large scale language models xed length resentations built identify similar documents reimers gurevych karpukhin et al representations directly answer questions leaving unclear capture key pieces information document approach quase et al question answering supervision learn reusable sentence embeddings goal transfer supervision qa datasets tasks sentence embeddings learned source dataset hand focus compressing input document ing information content document artetxe schwenk learned lingual sentence embeddings based parallel corpora embeddings tence translate sentence representations able capture knowledge present sentence signed bilstms kiros extend idea transformer based language models limited sentences large scale especially designed handle long ments yang et al beltagy et al zaheer et al models propose novel architectures need pre trained large corpora present orthogonal approach scale existing language models longer uments requiring expensive model pre training language models deformer cao et al proposed nate approach scale language models forming local attentions lower layers aggregating higher layers task specic model usable representations readonce transformers goal work identify optimal architecture extract information capturing usable representations time need nd optimal architecture use representation conjunction text inputs high level shown fig need develop systems model compute representation document encoder general model tasks consume tor representations text model given recent success generality encoder decoder models radford et al raffel et al lewis et al focus developing models architecture present potential choices model nal model system indicated document encoder given encoder decoder model ent ways compute representations ment d tokens tn focus ing output representation generated coder represented hi token ti fixed length aggregation common approach extract single vector representation sequence tations kiros et al conneau et al compact representation document tends lossy cially dealing large documents result representations mainly classication conneau et al reimers gurevych retrieval karpukhin et al shown capture content document e g infersent conneau et al presented self attentive approach extract sentence embedding hidden representations r u function computes scalar tention hi reduce information loss extend models produce m representation vectors learning m sets parameters j j m e rj uj ej hi uj hi ej special token representations advent transformer models common approach adding special radford et al devlin et al s liu et al token context output representation special token inputs classiers stream models single representation lossy generate m representations serting multiple special tokens parameters learned depend length m output tion dynamically adjust number cial tokens based input length achieve compression ratio k special tokens use representations insert n consider different variations ing special tokens context add beginning context sufx add end context interleave add k tokens rst approaches ensure text loss continuity interleaving based approach directly incentivize model capture local context special token sliding window aggregation finally apply idea aggregating vector representations generate length representation apply aggregation function f sliding windows size w tokens capture local context window stride length s result tion vectors rj f hsj f corresponds pooling linear weighting described eqn max pooling respectively figure shows compute representations window size overlap e linear weighting function resulting readonce tions m vectors n number tokens input model present modication downstream task models use text generated readonce representations nlp tasks formulated text text lem radford et al raffel et al focus extending text text encoder decoder models text model figure sliding window aggregation approach tract meaning representations transformer based encoder linear weighted sum aggregate vectors nal output layer single vector resulting readonce representations vectors figure appending readonce representations lth layer encoder extend standard encoder decoder models handle inputs append encoder transformer block encoder dle input length layer possible approach append representations lth layer encoder allows model focus parsing input example text e question layers followed focusing answering question remaining layers model figure encoder processes q tokens question rst l layers m readonce tations added lth layer quent layers produce m q vectors attending representations text finally unmodied decoder produces output answer modify transformer block attention modifying input sider alternate approach modifying transformer block similar chines rashkin et al view sentation memory self attention block attend addition input text self attention block uses separate attention modules input types averages representationlinear weightinglinear weightinglinear weightinglinear vectors specically let hl enc matrix hidden states generated lth layer standard transformer hl enc enc enc enc k v attention module transformer takes q k v query key value matrix extra representations input instead compute hl enc hl enc attn enc enc r enc enc attn separate attention module include readonce representations r model weights initialized corresponding weights attn speed training decoder model pute hidden states layer eqn model attend extracted document information decoding cess training readonce qa given overall architecture system shown fig focus training model produce readonce representations capture information present ment prior representation learning models focused classication tasks stead use reading comprehension qa task ensure information capturing property model able use readonce sentations answer questions grounded document representations contain information needed answer questions key question qa datasets suitable training compact information capturing document tion low level semantic qa datasets qamr michael et al et al nt allow compression questions require representation capture ery word input sentence complex multi hop qa datasets hotpotqa yang et al appropriate cus learning reason learning capture information factoid qa tasks provide sweet spot extremes figure nal architecture readonce transformers model use aggregated sliding window representations shown fig ument encoder compute readonce tations append representations lth layer encoder model shown fig ne tune end end model qa tasks train document encoder extract information capturing representations freeze encoder ne tune model downstream tasks extracting key information sufcient swer questions use datasets squad rajpurkar et al unsupervised qa lewis et al datasets train els representations trained tract information document use solve varied tasks learning based reasoning downstream models downstream usage readonce verify generality readonce sentations train models perform multi hop reasoning abstractive qa summarization ing learned representations specically freeze document encoder model use generate representations documents ne tune model downstream task produce output label given readonce representations example specic input representation learning experiments rst evaluate different potential tural choices extracting document representations discussed tively main interest learning fective representations need train model model candidate representation order nd optimal tecture consume representation training setup train entire model factoid qa task ensure document representations capture factual knowledge primarily use squad reading comprehension dataset jpurkar et al containing crowd sourced factoid questions augment dataset based questions dataset lewis et al allows increasing size training dataset ing question diversity avoid cally generated questions overwhelming training ensure number questions selected datasets batch duplicating squad questions vein evaluate model based mance squad task mentioned use large model experiments optimize model cross entropy loss set learning rate weights initialized bart model randomly initialized newly added weights shown benecial peters et al parameters follow lewis et al architecture evaluation able evaluate representations need rst decide architecture model suming representations model explore different choices model model discussed suming representation generated ple document encoder model mean aggregation sliding window window size stride tokens results shown ble architecture append append append modifyatt design parameters squad em table comparison different bart based tectures jointly operating continuous tations text appending readonce tions early late encoder stack effective appending half way suspect ing early allow model focus understanding question ing late leave room attention question document resentations modifying transformer block attend representations results able score squad formed simple append architecture rest work stick simpler chitecture appending representation layer denoted document encoder given model model chitecture chosen explore potential document encoder architectures extract representations fair comparison ensure evaluated representations use average dataset number vectors represent documents table presents em scores squad chitectural choices discussed architecture design parameters squad slidingwindow slidingwindow f f slidingwindow f slidingwindow m sufx m interleave fixedlength em table comparison different architectures extracting continuous representations bart coder approach extracts representations length document length ther document average dataset rows explore sliding window architecture window size stride length e overlap dows different aggregation tions mentioned earlier mean learned weighted sum ble performance task outperform max pooling function evaluate pact increasing overlap windows scores uqa correlate scores experimented nt nd squad close models signicant gains increasing window size changing stride length keeps average number vectors constant learned weighted sum function results point drop score bly aggregation function having erate larger window evaluate approaches inspired prior work add special tokens use representations tokens bart model use newly added cls token special token table appending tokens end interleaving input results tations comparable sliding window based approaches finally x representation length vectors computed based average token length squad learned representations effective final readonce architecture based set experiments use sliding window chitecture document encoder learned weighted sum aggregation function pend representations layer nal task dependent model downstream task experiments evaluate quality tions downstream tasks different tasks readonce transformers trained results demonstrate tage representations faster ing inference finally benet representation scenario uments longer maximum token limit underlying lm experimental setup tasks consider end tasks extractive qa summarization abstractive qa uate system following datasets hotpotqa yang et al multi hop reasoning dataset requires models gate information paragraphs produce answer span input paragraphs focus distractor setting ditionally provide models distractor graphs efciency use output quark system groeneveld et al lects tokens including question special tokens prex similar scores sufx model input paragraphs use answer em scores metrics xsum narayan et al stractive news summarization dataset requires models generate summaries ply extracting key sentences use l summ commonly tion datasets computes union lcs longest common subsequences lcs pair reference hypothesis sentences contrast standard rouge l score computes lcs reference hypothesis ing sentence kocisky et al abstractive qa dataset answers extractive spans input document models need understand content ment generate answers use rouge l summ score summarization task baselines compare readonce formers bart based qa models use document text directly answer given tion models use text directly lossy compression score best viewed upper bound based bart model including train bart model generate answer given tire document question question word summary case xsum representations trained squad uqa dataset use baseline bartsquad uqa bart model rst ne tuned datasets readonce models freeze parameters document encoder generate tations documents datasets use representations model ne tuned end task evaluate impact training qa datasets compare model readonce architecture initialized bart model weights readonce document encoder trained dow size stride length results representations input length com google google research tree master rouge experiments notice tial difference simple rouge l metric summarization based metric model dle variable number representation vectors change compression ratio having train models cally use stride length k generate representations kth input length feed downstream model reducing value k reduce pression ratio improve model accuracy cost increased runtime interestingly discovered nt need train document encoder value k achieve performance comparable encoders trained individually value k document encoder trained k varying value k tuning step representation quality assess ability readonce resentations capture document information compared original document text shown table framework rouge l points bart model uses text directly demonstrates readonce representations capture relevant information document hotpotqa narr qa xsum architecture readonce readonce upper bounds bart bartsquad uqa r l r l table performance readonce transformers datasets vs standard text text transformer access document text competitive mance indicates document representation effective capturing information setting drops score shortly improves e reduces training inference time models fairness ne tuned bart model qa datasets squad uqa found ticeable difference baseline bart model lastly note readonce system simply uses bart model parameters rouge l points model learned representations shows model rely factoid questions learn extract meaningful representations model efciency key advantage readonce tions model needs read document reuse pre computed tions multiple examples multiple tasks pre computed compressed representations directly training downstream task model making training efcient reducing document level self attention cost representation level cost similarly reduce cost inference abling faster question answering static pus common application scenario general cost compute readonce representations moderate use encoder stack model example takes model seconds compute resentations documents hotpotqa narr qa documents longer average computing representations requires seconds documents training time shown figure model exhibits speedup training time compared standard bart model note includes cost reading readonce resentations disk caching tations memory speedup increased figure training time seconds batch readonce models document representations pre computed cached resulting speedup observe drops score use bart model parameters document encoder model generally shorter focus document level cost pay cost half layers set layer transformer certain computation costs affected reduction self attention cost found speedup relative baseline model plateaus inference time similarly observe speedup inference time figure plateaus figure accuracy models different mum window length assumptions readonce formers stay substantially accurate mum window length decreases line bart model drops signicantly model consistently performs baseline model exhibits lar drop score maximum token limit model tokens equation hand handle documents tokens requiring truncation examples extreme nario result point drop score t decreased simulations provide strong evidence ability readonce transformers handle long uments effectively standard transformer based models conclusion work introduced readonce transformers novel approach large scale based language models build sume reusable document representations akin humans ability read document tract useful information having know advance end use representations compact capturing document representations pre computed independent fashion results extractive qa summarization abstractive qa tasks demonstrate readonce representations lieu reading document text context example results substantially faster training ence modest cost accuracy figure inference time seconds batch readonce models document representations pre computed cached resulting speedup handling long documents compressing document representations downstream model enables model reason longer documents normally t maximum token length limit example model max limit t tokens truncate inputs length nd intelligent ways sub select text hand compression ratio k compute readonce tations k length t chunks input document concatenate t length representations produce sentation t max token limit chunk overlap o tokens previous chunk model able handle inputs lengths experiments set ensure partial sentence border chunk appear chunk evaluate impact compression tor k max token limit t simulate ent token limits narrativeqa datasets average number tokens documents dataset documents ing tokens results depicted figure reduce t underlying transformer model score token framework offers easy way trol trade speed accuracy compression ratio parameter enables use standard transformer architectures long documents normally model s token limit representations tasks better document retrieval summarizing long scientic documents han et al future steps acknowledgements thank dirk groeneveld providing quark system training model hotpotqa computations beaker org ported credits google cloud references m artetxe holger schwenk massively multilingual sentence embeddings zero shot cross lingual transfer tacl iz beltagy matthew e peters arman cohan longformer long document samuel r bowman gabor angeli christopher potts christopher d manning large tated corpus learning natural language inference emnlp qingqing cao h trivedi aruna balasubramanian niranjan balasubramanian deformer decomposing pre trained transformers faster question answering acl arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim w chang nazli goharian discourse aware attention model abstractive summarization long documents naacl hlt alexis conneau douwe kiela holger schwenk loc barrault antoine bordes supervised learning universal sentence representations natural language inference data emnlp jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language standing naacl dirk groeneveld tushar khot ashish sabharwal et al simple strong pipeline potqa emnlp hangfeng qiang ning dan roth quase question answer driven sentence encoding acl luheng m lewis luke zettlemoyer question answer driven semantic role labeling ing natural language annotate natural language emnlp v karpukhin barlas oguz sewon min patrick lewis ledell yu wu sergey edunov danqi chen wen tau yih dense passage retrieval open domain question answering emnlp jamnie kiros contextual lensing universal sentence representations arxiv ryan kiros yukun zhu russ r salakhutdinov richard zemel raquel urtasun antonio torralba sanja fidler skip thought vectors tomas kocisky jonathan schwarz p blunsom chris dyer k hermann gabor melis edward grefenstette narrativeqa reading prehension challenge tacl mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov luke zettlemoyer bart denoising sequence sequence training natural language generation translation comprehension acl patrick lewis ludovic denoyer sebastian riedel unsupervised question answering cloze translation acl yinhan liu myle ott naman goyal jingfei du dar joshi danqi chen omer levy mike lewis luke zettlemoyer veselin stoyanov roberta robustly optimized bert pretraining proach arxiv preprint julian michael gabriel stanovsky luheng gan luke zettlemoyer ing question answer meaning representations naacl s narayan shay b cohen mirella lapata nt details summary aware convolutional neural networks extreme summarization emnlp matthew e peters mark neumann robert l gan roy schwartz vidur joshi sameer singh noah smith knowledge enhanced tual word representations emnlp alec radford karthik narasimhan tim salimans ilya sutskever improving language standing unsupervised learning technical port openai colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li peter j liu exploring limits transfer learning unied text text jmlr pranav rajpurkar jian zhang konstantin lopyrev percy liang squad questions machine comprehension text emnlp hannah rashkin c elikyilmaz yejin choi feng gao plotmachines outline conditioned generation dynamic plot state tracking emnlp nils reimers iryna gurevych bert sentence embeddings siamese networks emnlp ijcnlp alex wang yada pruksachatkun nikita nangia amanpreet singh julian michael felix hill omer levy samuel bowman superglue stickier benchmark general purpose language understanding systems neurips alex wang amanpreet singh julian michael felix hill omer levy samuel r bowman glue multi task benchmark analysis form natural language understanding iclr zhilin yang zihang dai yiming yang jaime bonell russ r salakhutdinov quoc v le xlnet generalized autoregressive pretraining language understanding neurips zhilin yang peng qi saizheng zhang yoshua gio william w cohen ruslan salakhutdinov christopher d manning hotpotqa dataset diverse explainable multi hop question answering emnlp manzil zaheer guru guruganesh kumar avinava dubey joshua ainslie c alberti s ontanon philip pham anirudh ravula qifan wang l yang ahmed big bird transformers longer sequences arxiv
