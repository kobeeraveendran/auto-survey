r a m l c s c v v i x r a published as a conference paper at iclr bert fine tuning for arabic text summarization khalid n elmadani mukhtar elgezouli anas showk department of electrical and electronics engineering p o box khartoum sudan khalidnabigh com anas edu abstract fine tuning a pretrained bert model is the state of the art method for tive abstractive text summarization in this paper we showcase how this tuning method can be applied to the arabic language to both construct the rst documented model for abstractive arabic text summarization and show its mance in arabic extractive summarization our model works with multilingual bert as arabic language does not have a pretrained bert of its own we show its performance in english rst before applying it to arabic corpora in both extractive and abstractive tasks introduction arabic one of six ofcial languages of the united nations is the mother tongue of million ple and the ofcial language for countries nine of those are in africa hence arabic had a huge inuence in mother africa forming the culture and religious values in west africa consequently its safe to say arabic is the latin of africa the arabic script is an alphabet written from right to left there are two types of symbols in the arabic script letters and diacritics habash it has letters and each letters shape changes based on its position each character holds possible diacritics and the syntax of each word in the sentence depends on its last letters diacritic unfortunately the diacritics are usually absent in the texts of news articles and any online content the main challenge in arabic text summarization is in the ambiguity of the arabic language itself the meaning of a text depends heavily on the context text summarization is to extract and generate the key information in a brief expression from long documents generally there are two approaches for text summarization either extractive involves extracting the relevant phrases from the document then organizing them to form the summary or involves going through the hole document then try to write summary in your own abstractive words arabic text summarization works are few related to other languages and the research is focused on extractive approaches which are based on sentences scoring then selecting best ones as a summary three approaches are used for sentence scoring and selection al qassem et al symbolic based systems model the discourse structures of text numerical based systems assign numerical scores to words of sentences which reects their signicance and hybrid systems combine both symbolic based and numerical based methods recently jaafar bouzoubaa suggested hybrid approach to produce abstractive summaries based on extractive ones data sets english is the golden standard for text summarization strongly because of the vast number of well proposed benchmark data sets containing a huge capacity of summarized articles both in tive and abstractive schemes like cnn daily mail news highlights data set hermann et al contains k news articles and associated highlights another important data set in english is xsum narayan et al contains news articles accompanied with a one sentence these authors contributed equally code is available at com mukhtar algezoli arabic presumm published as a conference paper at iclr table rouge results on the cnn test set model rl bert m bert summary answering the questionwhat is this article about this type of rich corpus is what arabic language lacks in automatic text summarization the lack of arabic benchmark corpora makes evaluation for arabic summarization more difcult without unied benchmark corpus the results reported from existing model can only be a hint for overall performance comparison al qassem et al but recently there is a turnout to use some corpora like easc containing arabic articles and human generated extractive summaries of those articles and kalimat a multipurpose arabic corpus containing articles with their extractive summaries methodology we used the pretrained bert devlin et al for both abstractive and extractive tion the encoder bertsum liu lapata is pretrained bert expanded by adding several cls symbols for learning sentence representations and using interval segmentation embeddings to distinguish multiple sentences for abstractive summarization task the decoder is layered formers vaswani et al initialized randomly this mismatching between encoder and decoder encoder was pretrained while decoder is may lead to unstable training so liu lapata proposed a new ne tuning schedule which adopts different optimizers for the encoder and the decoder bertsumabs and for extractive summarization task a sigmoid classier was serted on top of each cls token in the encoder indicating whether the sentence should be included in the summary bertsumext this method of using pretrained bert is perfect for our condition because the pretrained model will compensate for the relatively small data set we are using but how could this model be applicable for arabic language since bert was trained on english documents the answer is multilingual bert m bert pires et al its similar to the normal bert but trained on languages we trained bertsumabs one time using bert and m bert another time on the cnn data set for steps to compare the impact of using m bert instead of bert sense m bert supports arabic finally we included a non pretrained transformer baseline for both extractive and abstractive tasks in order to measure the effect of using pretrained m bert both transformerabs and formerext encoders are layered transformers the rest of their architecture is the same as sumabs and bertsumext respectively results we ve automatically evaluated the quality of the summary using rouge lin och gram and bi gram overlap and are reported as a means of evaluating mativeness and the longest common subsequence rouge l as a means of evaluating uency table demonstrates the rst step towards arabic text summarization switching from monolingual bert to multilingual bert the results show very similar performance as compared to bert and m bert table presents our results on kalimat data set we conclude that pre trained m bert leads to huge improvements in performance for relatively small data sets in both extractive and abstractive summarization it also reveals that extractive models would have higher performance for extractive data sets than their corresponding abstractive ones published as a conference paper at iclr table rouge results on kalimat test set model rl bertsumext transformerext bertsumabs transformerabs in this paper we showed how multilingual bert could be applied to arabic text summarization and how effective it could be in low resource situations research in arabic nlp is still in its infancy compared to english abstractive text summarization was not attempted before at the time of this submission so there is no metrics output that we can evaluate against conclusion references lamees mahmoud al qassem di wang zaid al mahmoud hassan barada ahmad al rubaie and nawaf i almoosa automatic arabic summarization a survey of methodologies and systems procedia computer science jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language understanding arxiv preprint nizar y habash introduction to arabic natural language processing synthesis lectures on human language technologies karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa in advances in suleyman and phil blunsom teaching machines to read and comprehend neural information processing systems pp younes jaafar and karim bouzoubaa towards a new hybrid approach for abstractive tion procedia computer science chin yew lin and fj och looking for a few good metrics rouge and its evaluation in ntcir yang liu and mirella lapata text summarization with pretrained encoders arxiv preprint shashi narayan shay b cohen and mirella lapata do nt give me the details just the topic aware convolutional neural networks for extreme summarization arxiv preprint telmo pires eva schlinger and dan garrette how multilingual is multilingual bert arxiv preprint ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in advances in neural information processing systems pp workshop mary a data pre processing kalimat is a multipurpose arabic corpus used mainly for extractive summarization in this section we will show how we prepared it for bert in the raw data set each category culture economy local news international news religion and sport has its own le containing articles of each month in a txt le the text on those txt les are published as a conference paper at iclr in ironically does not support arabic so we converted it to and put it conveniently in a csv le we then made a standalone story le for each article and its summary with the summary formulated as highlights at the end of the le at this stage we used the preprocessing suggested by liu lapata but with some changes to make it work with arabic in the standford corenlp le we replaced the stanford with stanford models and used its path as the path for stanford corenlp tokenizer we then used the sentence splitting and tokenization as in the paper which split the articles and summaries into sentences list of vectors put into a json le lastly we tokenized the vectors using bert vocabulary multilingual bert model and formatted it to pytorch les at the end we got pytorch les each with entries each entry containing src txt and src those the original articles and their tokenized counterparts tokenized using tgt txt and tgt those the original summaries and their tokenized counterparts tokenized multilingual bert tokenizer using multilingual bert tokenizer
