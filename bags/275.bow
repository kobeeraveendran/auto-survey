pre training abstractive document summarization reinstating source text yanyan xingxing wei furu ming jd com microsoft research asia beijing china statnlp research group singapore university technology design zoe com xizhang fuwei com edu sg abstract abstractive document summarization ally modeled sequence sequence learning problem unfortunately training large based summarization models limited supervised summarization data challenging paper presents sequence sequence pre training hand step objectives allow pre train based abstractive marization model unlabeled text main idea given input text cially constructed document model pre trained reinstate original ment objectives include sentence ordering sentence generation masked document generation close tions abstractive document rization task experiments benchmark summarization datasets e cnn dailymail new york times jectives improve performance lines compared models pre trained large scale data gb method gb text pre training achieves comparable results demonstrates effectiveness code models lic available introduction automatic document summarization task condensing document shorter form important content preserved requires coverage understandings document specic words phrases task typically classied categories extractive abstractive document summarization tive summarization cheng lapata lapati et al narayan et al aims contribution internship microsoft research rst author works jd com extract important sentences input ment concatenates extracted sentences corresponding output summary ative orders selected sentences mary relative orders document differently abstractive tion nallapati et al et al paulus et al rewrites source text ates corresponding summary tain novel words phrases featured output summary closely related input document summary sentences phrased input abstractive rizers different relative order pared source text words contents original document reordered summary phenomena dened tent reordering section detailed nition statistically observed instances training split summarization dataset content reordering phenomena necessary design model capable reordering content far know relatively rare prior work studied abstractive summarization abstractive summarization usually framed sequence sequence learning lem nallapati et al et al paper adopt transformer vaswani et al strated state art modeling vaswani et al ott et al recent studies song et al dong et al lewis et al zhang et al fel et al proven effectiveness trained transformer models ural language generation tasks abstractive summarization based observations regard abstractive summarization work proposes t c o l c s c v v x r sequence sequence pre training hand step objectives train model unlabeled text sentence reordering sr sentence eration nsg masked document tion mdg objectives designed reinstate original source text sr learns recover document randomly shufed tences given rst segment document nsg generates segment original document mdg learns recover masked ument original form pre training model proposed unlabeled documents tune supervised summarization datasets e cnn dailymail new york times ments pre training documents training split summarization dataset method improve performance heavily tuned large transformer model includes strong pre trained coder large margin involving data gb pre training performance improved compared models pre trained data gb achieve comparable higher rouge scores related work extractive summarization task aims nd informative sentences document summary task usually viewed tence ranking problem kupiec et al roy oleary scores nary sequence classication model dicts sentence summary extractive neural models cheng ata nallapati et al narayan et al zhang et al employ hierarchical lstms cnns feature learning binary sequence classier largely perform discrete feature based models radev et al filatova hatzivassiloglou nenkova et al recently feature learning replaced pre trained transformer encoders zhang et al liu lapata lead huge formance gain extractive models limitations example extracted sentences long redundant sides manually written summaries nature abstractive focus abstractive summarization paper abstractive summarization task aims generate summary rewriting document learning problem attentive lstms hochreiter schmidhuber bahdanau et al employed nallapati et al extended copy mechanism gu et al coverage model et al reinforcement ing paulus et al liu lapata transformer model encoder initialized pre trained transformer encoder e bert devlin et al work proposes pre train decoder encoder initialize coder decoder summarization model pre trained transformer model line work bridges tive abstractive models attention nisms gehrmann et al hsu et al reinforcement learning chen bansal model simpler pre training pre training methods draw lot attentions recently peters et al ford et al pre trained lstm language modeling objectives leverage context directions bert vlin et al trained masked guage modeling sentence prediction jectives spanbert joshi et al applied masked language modeling objective masks contiguous random spans dom tokens xlnet yang et al proposed permutation language modeling objective moves independence assumption masked kens bert roberta liu et al extends bert training data better training strategies models focus pre training encoder decoder propose ods pre train model e coder decoder abstractive summarization dong et al unilm proposed unied language model ral language understanding generation tasks pre trained masked unidirectional language modeling objectives encoder decoder parameters shared contrast pre train transformer separate parameters encoder coder song et al mass proposed method pre train transformer masking span text predicting masked tokens pre training task similar mdg task apply different ing strategy predict original text song et al tested model sentence level tasks e machine translation sentence pression aim solve document level tasks e abstractive document summarization lewis et al bart adopted tion text inlling sentence permutation single objective transformer training differently propose objectives use individually specically mdg placed selected token masked token input sequence raffel et al ies different pre training objectives model tectures unlabeled datasets prophetnet yan et al predicts n tokens ously zhang et al pegasus proposed remove mask sentences input document learn generate removed masked tences pre training nsg predicts following sentences input sequence mdg masks randomly selected tokens proposed method sequence sequence learning work task abstractive document summarization modeled learning problem adopt transformer chitecture vaswani et al given ment x paired mary y aim learn model parameters estimate conditional probability p y t x y t stands tokens position t e y given training set x y model trained maximizing log likelihood training document summary pairs log p y x y x y y pre training objectives automatic abstractive summarization requires comprehensive understanding input ment rewrites source text shorter form summary closely related input retaining important contents ing document result content reordering dene content reordering follows document summary pair rst map sentence summary ing sentence document maximizing rouge score appendix details relative orders sentences summary different relative orders mapped sentences original document count content reordering according tics training split summarization dataset contents original documents reordered summaries cases proximately observations motivate propose sequence sequence pre training objectives capable pre training model ing abstractive summarization task sentence reordering sentence reordering sr rst divide unlabeled document multiple sentences based stops let change notation document slightly paragraph let x denote document si sentence m number sentences refers sentence tion sentence index order x resented o m shufe words document sentences items order o rearranged obtain shufed order os ai m aj m ai aj j m j concatenating tences following os obtain shufed ment xs model takes input shufed document xs pre trained reinstate original x demonstrated figure training tive calculated x log p xs xx rst pre train transformer model unlabeled text proposed pre training objectives section ne tune document summary dataset reasons design objective summary document ally consists multiple sentences expect model pre trained learn generate figure assume document contains sentences e sent sent sent transformer model pre trained proposed objective takes transformed document e shufed document rst segment document masked document input learns recover original document original document generation sr sentence reordering nsg sentence generation mdg masked document generation long coherent summaries sentences output objective e original ment contains multiple sentences second discussed earlier sentence reordering tent reordering necessary summarization abstractive summary requires reproducing factual details e named entities gures source document expect model learn copy tokens note document special case sentence reordering signicant partially ordered sentences believe simpler objective work consider general case sentence reordering sentence generation sentence eration nsg uses span text document predict span text leverages natural order text shown figure ically split document segments e note segment contain multiple sentences intuitively ument sentences highly correlated document randomly divided fragments x stops rotated document xr document rotation recovers x xr preceding sentences context dependent nature documents language intention learn generate multiple sentences learn focus input text ts ument summarization task ment summary usually includes multiple sentences closely related ing objective calculated x log p xx constraints split point position right stop bol ensures sentences ment instead split point position document lead plete sentences segments intend force model understand input text complete information similarly common wisdom abstractive summarization documents input truncated xed number tokens contain incomplete sentences ting allows reduce mismatches training ne tuning input masked document generation jective masked document generation mdg learns reinstate document masked span tokens figure document noted x randomly sample length span l discrete form distribution b distribution parameters span starting position discrete uniform distribution l m xk text span masked let xm denote ment application masking strategy training objective calculated x log p xm xx straightforward masking strategy place token residing m special mask token refrain following reasons usually mask tokens appear downstream tasks second similar sr avoiding replacing token mask helps model learn ability copying tokens input preserving ability generating novel kens sub sequence m token processed strategies placed mask token replaced random token remains unchanged inspired bert devlin et al selected tokens follow strategy cases employ strategy use strategy remaining cases pre training consider settings setting pre training model gle objective e sr nsg mdg resulting different pre trained models setting employing objectives training batch randomly choose objective objective training time taining model e section better reference model step e sequence sequence pre training denote model trained proposed fine tuning model pre trained tune model abstractive document rization datasets words continue train model document summary pairs experimental setup datasets cnndm cnndm dataset contains news articles associated highlights e maries collected cnn daily mail online articles collected starting april cnn june daily mail end april tion data march test data april hermann et al lowing previous work et al liu lapata use non anonymized sion cnndm specically preprocessed dataset publicly available vided et al obtained document summary pairs training validation test nyt nyt dataset sandhaus collection articles multi sentence summaries written library scientists following preprocessing procedures described durrett et al liu lapata test set constructed including articles published january later contains cles remaining articles split training set examples validation set examples following durrett et al removed articles summaries contain words test set resulting test set contains examples giga cm pre train model jectives introduced section following procedures zhang et al created giga cm dataset contains unlabeled documents training set giga cm posed documents sampled english gigaword training ments cnndm resulting gb text training documents validation split cnndm validation set note gigaword dataset overlaps nyt dataset excluded test set nyt training set giga cm table lists number document summary pairs cnndm nyt unlabeled uments giga cm cnndm nyt cnn com co com abisee cnn dailymail ldc upenn edu dataset training validation test cnndm nyt giga cm table number document summary pairs cnndm nyt unlabeled documents giga cm giga cm datasets segmented tokenized documents summaries giga cm contains documents stanford corenlp toolkit manning et al applied based bpe sennrich et al radford et al reduce vocabulary size common wisdom abstractive summarization documents summaries cnndm nyt usually truncated tokens spectively leverage unlabeled documents differently different pre training objectives rst split document token pieces contains tokens pieces documents tokens removed sr mdg use piece transformation predict original form set minimum maximum masked length mdg individually nsg piece predict tokens implementation details mentioned section adopt transformer model vaswani et al backbone architecture purpose releasing large pre trained models reuse community avoid high computational costs similar previous work liu ata encoder initialized trained model e robertalarge liu et al share chitecture specically encoder layer transformer layer attention heads hidden size feed forward lter size respectively decoder lower layers randomly initialized number total trainable model parameters m hidden size number attention head decoder identical encoder feed forward lter size use smaller lter size decoder tried robertabase obtained inferior results duce computational memory cost dropout rates layers encoder set dropout rates decoder set models optimized adam kingma ba optimization hyper parameters training ne tuning different training stage encoder initialized pre trained model decoder randomly initialized similar liu lapata separate optimizers encoder decoder peak learning rates encoder decoder set warmup steps respectively adopted learning rate schedule strategies vaswani et al smaller batch sizes datasets examples e giga cm cnndm nyt ensure epoch sufcient number model updates trained models convergence validation perplexities epochs giga cm epochs cnndm epochs nyt epoch giga cm takes hours nvidia tesla gpus time costs different pre training objectives close highlight parameters tuning stage different pre training stage remain learning rates encoder decoder set warmup steps encoder decoder pre trained trained models epochs cnndm epochs nyt respectively selected best model regard rouge score validation set decoding similar liu lapata dong et al applied beam search beam size ducted experiments validation set ndm different beam sizes e cording rouge l optimal detailed results different beam sizes cluded appendix b following paulus et al blocked repeated trigrams ing beam search tuned minimum summary length validation set range search range minimum summary length empirically set according summaries training split cnndm average medium minimum lengths step size quick feedback similar pre training process datasets instances ne tuned smaller batch sizes e nyt cnndm results automatic evaluation rouge lin measure quality different summarization model puts reported length based rouge l scores ndm limited length based l nyt rouge scores computed pl following durrett et al models comparison baseline simply takes rst sentences document summary bertext liu lapata extractive model ne tuned bert devlin et al outperforms extractive systems ptgen et al drm paulus et al dca celikyilmaz et al ing based models extended copy age mechanism reinforcement learning deep communicating agents individually tomup gehrmann et al assisted mary generation word prediction model bertabs liu lapata unilm dong et al pre training based models trained based bert devlin et al implemented stractive models baselines layer transformer dom initialization replaced coder transformer robertabase robertalarge liu et al obtain baselines robertabase respectively following liu et al train robertalarge ments training split cnndm epochs number epochs models indicated domain replaced coder transformer trained model resulting robertacont results cnndm results ndm listed table rst ond blocks results previous extractive com bheinzerling pyrouge git model r l bertext liu lapata extractive abstractive ptgen et al drm paulus et al bottomup gehrmann et al dca celikyilmaz et al bertabs liu lapata unilm dong et al transformer robertabase roberta robertacont step domain step giga cm sr nsg mdg sr nsg mdg table results test split cnndm length based rouge l r l indicates signicant improvements p measured rouge script compared models rst blocks abstractive models respectively results listed block outperforms transformer nearly rouge points roberta improves performance shows effectiveness pre trained encoders study effects different training objectives section rst train transformer model sizes model roberta identical unlabeled documents cnndm training split quick denoted step domain block table sentence ing sr sentence generation nsg masked document generation mdg improve robertabase roberta signicantly measured rouge terestingly merely use domain training split gb method signicantly outperforms unilm dong et al pre trained gb data compared step domain e pre training sr epoch takes hours cnndm nyt rouge script rouge means signicant difference p model r l model corpus size bertext liu lapata extractive abstractive ptgen et al drm paulus et al bertabs liu lapata transformer roberta step domain step giga cm sr nsg mdg sr nsg mdg table results test set nyt dataset limited length recall based rouge indicates nicant improvements p measured rouge script models rst blocks robertacont encoders models pre trained corpus epochs model achieves better performance shows mance gains mainly result proposed jectives pre training decoder encoder training roberta longer prove understanding tasks liu et al evidence shows longer training time roberta improve generation performance pre train model larger dataset e giga cm size gb indicated step giga cm sults improved method forms models comparison listed table results nyt table presents results nyt dataset following evaluation tocol durrett et al adopted limited length recall based rouge truncated predicted summaries length gold ones rst second blocks results previous extractive abstractive models respectively results models listed block similar trends cnndm method leads signicant mance gains p comparisons objectives pre training objectives sr works slightly pegasus pegasus hugenews bart prophetnet gb prophetnet gb unilm step gb gb gb gb gb gb gb gb r l table results cnndm test split models pre trained different corpora indicates signicant differences model better objectives e nsg mdg tried randomly use objectives training ity indicated interestingly served general outperforms objectives employing unlabeled documents training splits cnndm nyt limited number unlabeled uments training splits adding data e giag cm pre training sr sistently achieves highest cnndm nyt conclude sr effective pre training objective tive summarization sentence reordering jective ts content reordering requires prehensively understanding document wide coverage going individual words tences highly close essence stractive document summarization performance models validation splits cnndm nyt pendix b comparison models pre trained scale corpora worth noting models released recently trained corpora larger listed table raffel et al introduced gb training corpus pegasuslarge sions pre trained hugenews gb respectively bart lewis et al prophetnet gb yan et al pre trained gb corpus introduced liu et al compare best ing model step e pre training cm dataset sr objective els focus performance ndm known benchmark stractive summarization highlight est rouge scores table bold font use symbol indicate models form signicantly different step pegasus hugenews achieve signicantly higher scores model obtain higher rouge l scores hand consider els pre trained relatively small scale pus following bert devlin et al prophetnet gb yan et al unilm dong et al use gb text training listed table model signicantly outperforms models systems mr bertabs unilm roberta step gold table human evaluation results proportions tem rankings mr mean rank lower better input lose information residing following tokens qualitative analysis generated examples illustrated appendix c human evaluation conclusion summaries generated abstractive models produce disuent ungrammatical outputs evaluated abstractive systems eliciting human judgements compared best forming model e pre training cm dataset sr objective human ences denoted gold strong baselines system outputs available including roberta pre training based models e bertabs liu lapata unilm dong et al uments randomly sampled test split cnndm participants presented document list outputs generated ent abstractive summarization systems asked rank outputs systems best worst according informativeness summary capture informative document uency summary cal succinctness summary express document clearly words report proportions system rankings mean rank lower better table output step selected best cases obtained lower mean rank systems gold shows participants preference model converted ranking bers ratings e rank converted applied student t test ratings signicantly better systems cept gold comparison p lags human possible son system systems takes rst tokens long document proposed sequence sequence training objectives including sentence reordering sentence generation masked document generation objectives relations abstractive summarization task signed based reinstating source text model abstractive document marization pre trained tives ne tuned summarization dataset compared models pre training larger corpora gb method gb pre training achieve parable better performance ture like investigate objectives pre train models abstractive marization acknowledgments like thank anonymous reviewers thoughtful constructive comments yanyan zou wei lu supported gapore ministry education academic research fund acrf tier project references dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate proc iclr asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents abstractive summarization proc naacl bertabs unilm publicly able com nlpyang presumm com microsoft unilm yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proc acl jianpeng cheng mirella lapata neural summarization extracting sentences words proc acl john m conroy dianne p oleary text marization hidden markov models proc sigir jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language ing proc acl li dong nan yang wenhui wang furu wei aodong liu yu wang jianfeng gao ming zhou hsiao wuen hon unied language model pre training natural language ing generation proc nips greg durrett taylor berg kirkpatrick dan klein learning based single document tion compression anaphoricity constraints proc acl elena filatova vasileios hatzivassiloglou event based extractive summarization text marization branches sebastian gehrmann yuntian deng alexander rush abstractive summarization proc emnlp jiatao gu zhengdong lu hang li victor o k incorporating copying mechanism li sequence sequence learning proc acl karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend proc nips sepp hochreiter jurgen schmidhuber neural computation long short term memory wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun unied model extractive abstractive summarization inconsistency loss proc acl mandar joshi danqi chen yinhan liu daniel s weld luke zettlemoyer omer levy spanbert improving pre training representing ing spans transactions association putational linguistics diederik p kingma jimmy lei ba adam proc method stochastic optimization iclr julian kupiec jan pedersen francine chen proc trainable document summarizer gir mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov luke zettlemoyer bart denoising sequence sequence pre training natural language generation translation comprehension arxiv preprint chin yew lin rouge package proc matic evaluation summaries workshop yang liu mirella lapata text tion pretrained encoders proc emnlp yinhan liu myle ott naman goyal jingfei du dar joshi danqi chen omer levy mike lewis luke zettlemoyer veselin stoyanov roberta robustly optimized bert pretraining proach proc acl christopher manning mihai surdeanu john bauer jenny finkel steven bethard david mcclosky stanford corenlp natural language proc acl system cessing toolkit strations ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments proc aaai ramesh nallapati bowen zhou cicero dos santos caglar gulcehre bing xiang tive text summarization sequence sequence rnns proc signll shashi narayan shay b cohen mirella lapata nt details summary topic aware convolutional neural networks treme summarization proc emnlp shashi narayan shay b cohen mirella lapata ranking sentences extractive proc rization reinforcement learning naacl ani nenkova lucy vanderwende kathleen eown compositional context sensitive multi document summarizer exploring factors inuence summarization proc sigir myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier michael auli fairseq fast extensible toolkit sequence modeling proc naacl demonstrations romain paulus caiming xiong richard socher deep reinforced model abstractive marization proc iclr matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word resentations proc naacl dragomir radev timothy allison sasha goldensohn john blitzer arda c elebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu jahna otterbacher hong qi horacio saggion simone teufel michael topper adam winkel zhu zhang mead platform multidocument multilingual text summarization proc lrec alec radford jeffrey wu rewon child david luan dario amodei ilya sutskever language models unsupervised multitask learners openai blog colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li peter j liu exploring limits transfer learning unied text text arxiv preprint evan sandhaus new york times annotated corpus linguistic data consortium philadelphia abigail peter j liu christopher d manning point summarization generator networks proc acl rico sennrich barry haddow alexandra birch neural machine translation rare words word units proc acl kaitao song xu tan tao qin jianfeng lu yan liu mass masked sequence sequence proc pre training language generation icml ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention need proc nips yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang ming prophetnet predicting future zhou gram sequence sequence pre training arxiv preprint zhilin yang zihang dai yiming yang jaime bonell ruslan salakhutdinov quoc v le xlnet generalized autoregressive arxiv preprint ing language understanding jingqing zhang yao zhao mohammad saleh ter j liu pegasus pre training tracted gap sentences abstractive summarization arxiv preprint xingxing zhang mirella lapata furu wei ming zhou neural latent extractive document marization proc acl xingxing zhang furu wei ming zhou hibert document level pre training cal bidirectional transformers document rization proc acl qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural ment summarization jointly learning score select sentences proc acl additional setup details statistics content reordering recall unusual case human rewrites document summarize important mation track ordering information described ment phenomena dened content ordering follows document summary pair rst map sentence summary sentence document maximizing score relative orders tences summary different tive orders mapped sentences inal document count content ordering statistics cases training validation splits cnndm dataset specic borrow sentence annotations extractive summarization zhang et al zhou et al zhang et al sider extractive summarization sentence sication task sentences document maximize score lin human references labeled true sentences assigned false like previous extractive summarization systems zhang et al concatenate sentences label true document associated mary sentence summary search string closet sentence associated ument according count overlapped grams found instances relative orders sentences summary consistent relative orders closet sentences appearing document practice found instances training split instances validation set phenomenon b additional results results validation set performance proposed models validation splits cnndm nyt listed table spectively results validation set cnndm ferent beam sizes table lists rouge l beam size rouge l table rouge l results validation set cnndm different beam sizes model mnli sst qqp qnli sts rte mrpc cola roberta step encoder table results glue model r l step domain step giga cm sr nsg mdg sr nsg mdg table results validation split cnndm ing length based rouge l r l model r l step domain step giga cm sr nsg mdg sr nsg mdg table results validation set nyt dataset limited length recall based rouge results validation set cnndm ferent beam sizes beam search coding beam size gives highest l score use work results xsum different cnndm nyt xsum consists online news articles extracted british ing corporation bbc annotated short sentence news summary ing question article split ing validation testing preprocessing dures described work narayan et al adopted direct comparisons model r l extractive abstractive ptgen et al narayan et al bertabs liu lapata transformer roberta step table results test split xsum length based rouge l r l table lists results xsum report model pre trained sr objective domain pre training corpus indicated step pre training step performance gain ble reason summary xsum contains sentence sr objective helpful dataset results glue apply encoder best performing model glue tasks listed table compared roberta liu et al encoder best performing model consistently achieve higher results demonstrates improvements models abstractive summarization task come better encoder c examples system outputs table demonstrate output ples systems including bertabs liu lapata unilm dong et al gold standard summaries human references noted gold roberta baseline best performing model table shows ample outputs systems bertabs unilm copied sentence input article model generates summaries ing sentences table lists instance summary generated system unilm tains incomplete sentence article bertabs unilm cnn response reports big banks threatening withhold campaign funds senate democrats sen elizabeth warren week offered deant response bring warren said nt going slack calls breaking banks measures rein wall street hillary clinton prepares ofcially launch presidential campaign month need choice highlight issues relating economic inequality maryland gov martin omalley running democratic nomination trying steal clinton s thunder talking problems disproportionate wealth words signs democrats planning big issue economic inequality recent news likelihood new york s chuck schumer replace harry reid leader senate democrats means dreams economically leftward party crashing political reality schumer effective democrat skilled legislative leader wall street democrat spent time courting protecting powerful nancial interests run dominant industries state progressive moments president barack obama relied wall street donations campaigns despite talk conservatives left wing socialism white house nancial community willing open coffers democrats concern election democratic populism nt work current campaign nance system enormous pressures parties raise funds campaigns decades created pressure democrats despite political base court big donors california democrat tony coelho serving chairman democratic congressional campaign committee majority whip strong appeal savings loans executives crash industry catch republicans outanking raising money democrats continued losing traditional base campaign support organized labor central source campaign muscle providing money campaign assistance campaigns organized labor serve foundation pressure raising private funds increasing democrats concluded needed business democrats running president kind choices obama disappointed supporters rst president abandon post watergate public nance system campaigns altogether preferring raise money general campaign small donors enormously important victories business wall street executives height nancial crash public sentiment clearly turned wall street administration agreed nancial regulation bill dodd frank structured way powerful interests opportunity limit bite coming years wall street army counsel succeeded eroding impact legislation acceptance campaign nance system limit policy choices democrats greatly damages party s brand washington post reported scandal bring new jersey democratic sen robert menendez rst involving large scale super pac donations heart story physician salomon melgen gave senate majority pac possibly exchange favors simply sort accommodation democrats corporate system nt choice funds wo nt able compete election cycle independent campaign donors causing huge stir conservative circles koch brothers allies throwing enormous amounts money candidates support deregulatory agenda individual donors las vegas gambling magnate sheldon adelson causing ripples time candidates speak pressuring adjust agenda democrats found magnates political support tom steyer george soros campaign nance reform important congress changing fundamental dynamics wo nt room populism thrive democrats select like elizabeth warren candidate hillary clinton decides sharply left economic policy wo nt room reform time governance actually starts democratic party needs wall street needs stand wall street facts ground democrats want wall street tackle economic inequality rst bring reform campaign nance system campaigns publicly funded stringent limits independent expenditures wall street trouble achieving disproportionate inuence reform level playing eld campaign nance reform issue gets sidetracked little pro forma words support populist economic agenda revolved progressive taxation substantial public assistance strengthen middle class work different kind political system things stay democrats continue win elections turning corporate nancial base support julian zelizer sen elizabeth warren said nt going slack breaking banks says likelihood new york s chuck schumer replace harry reid leader senate democrats zelizer democratic populism nt work current campaign nance system julian zelizer democratic populism nt work current campaign nance system says pressure parties raise funds campaigns created pressure democrats court big donors says progressive moments president barack obama relied wall street donations campaigns says obama s decision abandon post watergate public nance system step roberta julian zelizer sen elizabeth warren nt going slack wall street zelizer democrats planning big issue economic inequality says democrats lost traditional base campaign support organized labor money campaigns julian zelizer democrats planning big issue economic inequality zelizer democratic populism nt work current campaign nance system says democrats lost traditional base campaign support julian zelizer elizabeth warren deant wall street hillary clinton likely wo nt zelizer democrats need wall street s campaign donations competitive gold table example article sampled test splitting cnndm paired list summaries generated different systems highlight bold sentences summaries copied article article bertabs unilm cnn hillary clinton nally announcing candidacy presidential election watched standing polls sag recent months likely boost days follow announcement democrats ample reason excited clinton s run presidency certainly strongest candidates decades brings table extensive political policy experience combination skills lacking roughest partisan wars emerged stronger keen sense nature modern news media use advantage survive scandal frenzies hardened tough partisan shy away republican attack americans positive memories clinton given booming economy late bill clinton s presidency hillary clinton puts effective campaign unbeatable democratic primaries general election buildup nal decision weaknesses exposed clinton nt want end like vice president al gore relatively nal election americans believing actually defeat george w bush nt generate energy campaign started touted perfect candidate ideal person job stiff inauthentic actually hit trail freeze television cameras rolling gore trouble connecting voters remake image constantly biggest asset ended viewed inevitable nominee actually stood clinton avoid following gore s path suffered fate primaries nt afford needs rest perception candidacy inevitable record experience important forth exciting vision stand white house voters thirst signs greatness pick presidents savvy understand reality polarized washington probably limit ability achieve bold change recent story washington post suggests advisers aware potential liability announcement going avoid big rallies events instead concentrate smaller events meet voters directly states iowa new hampshire clinton contend doubts authenticity rst day campaign trail sen rand paul immediately tapped concerns raising questions trusted question dogged clintons came national political scene late greatest virtue immense skills politicians come haunt bill clinton attacked slick willie members parties perception win hillary clinton faced similar criticism tried distance vote use force iraq democrats nt buy critique president george w bush s foreign policies went barack obama instead conducted listening tour new york running senate voters saw manufactured effort hide fact running ofce outsider explained recent stories use private email server state department email felt story relatively minor indicated nt telling nt hiding gives appearance months clinton connect party s base ongoing speculation sen elizabeth warren massachusetts suggested active democratic party enthused clinton s candidacy probably vote motivated nt trust stand democratic values need address concerns style agenda voters want hear talking issues tougher nancial regulation policies diminish economic inequality positions race policing need clear heard voters hawkish going war clear indications handle nuclear agreement iran clinton contend gender bias exists electorate large doubt subject questions comments appearance instance wo nt aimed male candidates candidacy effort break remaining vestiges political sexism struggle tough finally relates challenge clinton contend husband sure immense force campaign trail compelling democrats generation liability learned bill clinton easy control speaks mind dismissive comments obama s candidacy work fund raising records clinton foundation raise questions conict interest ongoing stories personal life case monica lewinsky returned media months ago emerge campaign trail fair point fair game modern campaign trail hillary clinton potential hugely successful presidential candidate campaign team need address multiple questions weaknesses clear recent months julian zelizer hillary clinton nally announcing candidacy presidential election zelizer roughest partisan wars emerged stronger says hardened tough partisan shy away republican attack julian zelizer hillary clinton nally announcing candidacy presidential election says extensive political policy experience combination skills lacking says clinton nt want end like vice president al gore nt generate energy campaign started clinton avoid following gore s path step roberta julian zelizer democrats plenty reason excited hillary clinton s run zelizer clinton puts effective campaign easily win general election says clinton needs forth stand white house julian zelizer democrats ample reason excited hillary clinton s run president zelizer clinton needs forth exciting vision stand julian zelizer hillary clinton immense political governmental experience says needs stronger connection party s base clinton needs convince voters authenticity zelizer says gold table example article sampled test splitting cnndm paired list summaries generated different systems incomplete sentence highlighted bold
