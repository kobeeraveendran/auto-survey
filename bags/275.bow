pre training for abstractive document summarization by reinstating source text yanyan xingxing wei furu and ming jd com microsoft research asia beijing china statnlp research group singapore university of technology and design zoe com xizhang fuwei com edu sg abstract abstractive document summarization is ally modeled as a sequence to sequence learning problem unfortunately training large based summarization models on limited supervised summarization data is challenging this paper presents three sequence to sequence pre training in hand step objectives which allow us to pre train a based abstractive marization model on unlabeled text the main idea is that given an input text cially constructed from a document a model is pre trained to reinstate the original ment these objectives include sentence ordering next sentence generation and masked document generation which have close tions with the abstractive document rization task experiments on two benchmark summarization datasets i e cnn dailymail and new york times show that all three jectives can improve performance upon lines compared to models pre trained on large scale data gb our method with only gb text for pre training achieves comparable results which demonstrates its effectiveness code and models are lic available at introduction automatic document summarization is the task of condensing a document into its shorter form with important content preserved which requires coverage understandings of the document rather than specic words or phrases this task can be typically classied into two categories extractive and abstractive document summarization tive summarization cheng and lapata lapati et al narayan et al aims to contribution during internship at microsoft research the rst author now works in jd com extract important sentences from the input ment and concatenates such extracted sentences as the corresponding output summary thus the ative orders of the selected sentences in the mary is the same as their relative orders in the put document differently abstractive tion nallapati et al see et al paulus et al rewrites the source text and ates the corresponding summary which may tain novel words and phrases not featured in the put the output summary is closely related to the input document also summary sentences phrased from the input by the abstractive rizers might have a different relative order pared to the source text in other words contents of the original document may be reordered in its summary such a phenomena is dened as tent reordering see section for detailed nition statistically we observed that around instances of the training split of our summarization dataset have this content reordering phenomena therefore it is necessary to design a model that is capable of reordering content however as far as we know relatively rare prior work has studied this for abstractive summarization abstractive summarization is usually framed as a sequence to sequence learning lem nallapati et al see et al in this paper we adopt the transformer vaswani et al which has been strated to be the state of the art for modeling vaswani et al ott et al recent studies song et al dong et al lewis et al zhang et al fel et al have proven effectiveness of trained transformer models on the ural language generation tasks such as abstractive summarization based on the above observations with regard to abstractive summarization this work proposes t c o l c s c v v i x r a three sequence to sequence pre training in hand step objectives which can be used to train a model on unlabeled text namely sentence reordering sr next sentence eration nsg and masked document tion mdg all three objectives are designed to reinstate the original source text sr learns to recover a document with randomly shufed tences given the rst segment of a document nsg generates the next segment of the original document mdg learns to recover a masked ument to its original form after pre training a model with our proposed on unlabeled documents we tune it on supervised summarization datasets i e cnn dailymail and new york times ments show that even pre training on documents from the training split of a summarization dataset our method can improve performance upon a heavily tuned large transformer model which already includes a strong pre trained coder by a large margin by involving more data gb for pre training the performance is further improved compared to models pre trained with much more data gb we can still achieve comparable or even higher rouge scores related work extractive summarization this task aims to nd the informative sentences in a document as its summary this task is usually viewed as a tence ranking problem kupiec et al roy and oleary using scores from a nary sequence classication model which dicts whether a sentence is in the summary or not extractive neural models cheng and ata nallapati et al narayan et al zhang et al employ hierarchical lstms cnns as the feature learning part of the binary sequence classier which largely perform discrete feature based models radev et al filatova and hatzivassiloglou nenkova et al very recently the feature learning part was replaced again with pre trained transformer encoders zhang et al liu and lapata that lead to another huge formance gain however extractive models have their own limitations for example the extracted sentences might be too long and redundant sides manually written summaries in their nature are abstractive therefore we focus on abstractive summarization in this paper abstractive summarization this task aims to generate a summary by rewriting a document which is a learning problem attentive lstms hochreiter and schmidhuber bahdanau et al are employed in nallapati et al that have been extended with copy mechanism gu et al coverage model see et al and reinforcement ing paulus et al liu and lapata used a transformer model with only its encoder initialized with a pre trained transformer encoder i e bert devlin et al this work proposes to pre train the decoder together with the encoder and then initialize both the coder and decoder of a summarization model with the pre trained transformer model there is also a line of work that bridges tive and abstractive models with attention nisms gehrmann et al hsu et al and reinforcement learning chen and bansal while our model is simpler pre training pre training methods draw a lot of attentions recently peters et al and ford et al pre trained lstm and former using language modeling objectives to leverage the context in both directions bert vlin et al is trained with the masked guage modeling and next sentence prediction jectives spanbert joshi et al applied only the masked language modeling objective that masks contiguous random spans rather than dom tokens xlnet yang et al proposed a permutation language modeling objective that moves the independence assumption of masked kens in bert roberta liu et al extends bert with more training data and better training strategies the above models focus on pre training an encoder or a decoder while we propose ods to pre train a model i e the coder together with the decoder for abstractive summarization dong et al unilm proposed a unied language model that can be used for both ral language understanding and generation tasks which is pre trained using masked unidirectional and language modeling objectives the encoder and decoder parameters are shared by contrast we pre train a transformer with separate parameters for the encoder and coder song et al mass proposed a method to pre train a transformer by masking a span of text and then predicting the masked tokens their pre training task is similar to our mdg task but we apply a different ing strategy and predict the original text song et al tested their model on sentence level tasks e machine translation and sentence pression while we aim to solve document level tasks e abstractive document summarization lewis et al bart adopted the tion of text inlling and sentence permutation as a single objective for transformer training differently we propose three objectives and use them individually specically mdg placed each selected token with a masked token in the input sequence raffel et al ies different pre training objectives model tectures and unlabeled datasets prophetnet yan et al predicts the next n tokens ously zhang et al pegasus proposed to remove mask sentences from an input document and learn to generate such removed masked tences for pre training while nsg predicts the following sentences of the input sequence and mdg masks randomly selected tokens proposed method sequence to sequence learning in this work the task of abstractive document summarization is modeled as a learning problem we adopt the transformer chitecture vaswani et al given a ment x paired with its mary y we aim to learn the model parameters and estimate the conditional probability p y t x where y t stands for all tokens before position t i e y given the whole training set x y this model can be trained by maximizing the log likelihood of the training document summary pairs log p y x y x y y pre training objectives automatic abstractive summarization requires comprehensive understanding of the input ment and rewrites the source text into its shorter form where the summary is closely related to the input retaining important contents also ing the document may result in content reordering now we dene content reordering as follows for each document summary pair we rst map each sentence in the summary to its ing sentence in the document by maximizing the rouge score see appendix a more details if the relative orders of sentences in the summary are different from the relative orders of their mapped sentences in the original document we count this as one content reordering according to the tics on the training split of our summarization dataset contents of the original documents are reordered in their summaries for of cases proximately the above observations motivate us to propose sequence to sequence pre training objectives that are capable of pre training a model ing the abstractive summarization task sentence reordering in sentence reordering sr we rst divide an unlabeled document into multiple sentences based on full stops let us change the notation of a document slightly in this paragraph let x denote a document where si is a sentence m is the number of sentences and refers to sentence tion the sentence index order in x can be resented as o m we then shufe in other words the the document by sentences items in the order o are rearranged and we obtain a shufed order os am where ai m aj m and ai aj for any i j m and i j concatenating tences following os we obtain a shufed ment xs a model takes as input the shufed document xs and is pre trained to reinstate the original one x as demonstrated in figure the training tive is calculated as x log p xs xx we rst pre train the transformer model on the unlabeled text using our proposed pre training objectives see section and then ne tune it on the document summary dataset there are several reasons why we design this objective first a summary of a document ally consists of multiple sentences we expect that the model is pre trained to learn to generate figure assume a document contains three sentences i e sent sent and sent a transformer model can be pre trained with our proposed objective it takes the transformed document i e a shufed document the rst segment of a document or a masked document as input and learns to recover the original document or part of the original document by generation sr sentence reordering nsg next sentence generation mdg masked document generation long and coherent summaries across sentences the output of the objective i e the original ment also contains multiple sentences second as we discussed earlier sentence reordering or tent reordering is necessary for summarization third abstractive summary requires reproducing factual details e named entities gures from the source document we also expect the model to learn to copy tokens note that document is a special case of sentence reordering with a signicant amount of partially ordered sentences which we believe is a simpler objective in this work we only consider the general case of sentence reordering next sentence generation next sentence eration nsg uses one span of text in a document to predict its next span of text which leverages the natural order of text as shown in figure ically we split a document into two segments i e and note that each segment might contain multiple sentences intuitively in a ument sentences are highly correlated with their document is randomly divided into two fragments x using full stops the rotated document is xr document rotation recovers x using xr preceding sentences due to the context dependent nature of documents or language our intention is to learn to generate multiple sentences and also learn to focus on input text which ts the ument summarization task since either a ment or its summary usually includes multiple sentences and they are closely related the ing objective is calculated as x log p xx we do not make constraints that the split point must be the position right after a full stop bol which ensures full sentences for each ment instead the split point can be at any position within the document which may lead to plete sentences in segments we intend to force the model to understand input text without complete information similarly as a common wisdom in abstractive summarization documents as input are truncated to a xed number of tokens which may also contain incomplete sentences this ting allows to reduce mismatches between the training and ne tuning input masked document generation the third jective is masked document generation mdg that learns to reinstate a document with a masked span of tokens see figure a document is noted as x we randomly sample the length of the span l from a discrete form distribution a and b are distribution parameters and the span starting position from another discrete uniform distribution l thus m xk is the text span to be masked let xm denote the ment after the application of our masking strategy the training objective is calculated as x log p xm xx one straightforward masking strategy is to place each token residing in m with a special mask token however we refrain from doing so because of the following two reasons usually mask tokens will not appear in downstream tasks second similar to sr avoiding replacing every token with mask also helps our model learn the ability of copying tokens from the input while preserving the ability of generating novel kens thus in the sub sequence m each token is processed with one of the three strategies placed with the mask token replaced with a random token remains unchanged inspired by bert devlin et al for of selected tokens we follow strategy in of cases we employ strategy and we use strategy for the remaining of cases during pre training we consider two settings setting one pre training a model with one gle objective i e sr nsg or mdg resulting in three different pre trained models setting two employing all three objectives for each training batch we randomly choose one objective and each objective is used for of the training time taining one model i e all see section for better reference we name our model as step i e sequence to sequence pre training that can be used to denote a model trained using our proposed fine tuning after a model is pre trained we tune the model on abstractive document rization datasets in other words we continue to train the model on the document summary pairs experimental setup datasets cnndm the cnndm dataset contains news articles and the associated highlights i e maries collected from the cnn and daily mail online articles were collected starting in april for cnn and june for daily mail both until the end of april the tion data is from march and the test data from april hermann et al lowing previous work see et al liu and lapata we use the non anonymized sion of cnndm specically we preprocessed the dataset with the publicly available vided by see et al and obtained document summary pairs for training for validation and for test nyt the nyt dataset sandhaus is a collection of articles along with multi sentence summaries written by library scientists following the preprocessing procedures described in durrett et al liu and lapata the test set is constructed by including all articles published on january or later which contains cles the remaining articles are split into a training set of examples and a validation set of examples following durrett et al we also removed articles whose summaries contain less than words from the test set and the resulting test set contains examples giga cm to pre train our model with the jectives introduced in section following the procedures in zhang et al we created the giga cm dataset which contains only unlabeled documents the training set of giga cm is posed of documents sampled from the english gigaword and the training ments in cnndm resulting in gb text for training we used the documents in the validation split of cnndm as the validation set note that the gigaword dataset overlaps with the nyt dataset and we therefore excluded the test set of nyt from the training set of giga cm table lists the number of document summary pairs for cnndm and nyt and unlabeled uments for giga cm for cnndm nyt and cnn com and co com abisee cnn dailymail ldc upenn edu dataset training validation test cnndm nyt giga cm table the number of document summary pairs for cnndm and nyt and unlabeled documents for giga cm giga cm datasets we segmented and tokenized documents summaries giga cm only contains documents using the stanford corenlp toolkit manning et al we further applied the based bpe sennrich et al radford et al to reduce the vocabulary size as a common wisdom in abstractive summarization documents and summaries in cnndm and nyt are usually truncated to and tokens spectively we leverage unlabeled documents differently for different pre training objectives we rst split each document into token pieces if it contains more than tokens pieces or documents with less than tokens are removed in sr and mdg we use the piece after transformation to predict its original form we set the minimum and maximum masked length a and in mdg individually in nsg each piece is used to predict its next tokens implementation details as mentioned in section we adopt the transformer model vaswani et al as our backbone architecture the purpose of releasing large pre trained models is to reuse so that the community can avoid high computational costs hence similar to previous work liu and ata our encoder is initialized with a trained model i e robertalarge liu et al and therefore they share the same chitecture specically the encoder is a layer transformer each layer has attention heads and its hidden size and feed forward lter size are and respectively the decoder is lower with layers and is randomly initialized the number of total trainable model parameters is m the hidden size and number of attention head of the decoder are identical to those of the encoder but the feed forward lter size is we use a smaller lter size in the decoder to tried robertabase and obtained inferior results duce the computational and memory cost the dropout rates of all layers in the encoder are set to and all dropout rates in the decoder are set to our models are optimized using adam kingma and ba with the other optimization hyper parameters for training and ne tuning are different in the training stage the encoder is initialized with a pre trained model while the decoder is randomly initialized therefore similar to liu and lapata we used two separate optimizers for the encoder and decoder the peak learning rates of the encoder and decoder are set to and with warmup steps respectively we also adopted the same learning rate schedule strategies as in vaswani et al we used smaller batch sizes for datasets with less examples i e for giga cm for cnndm and for nyt to ensure each epoch has sufcient number of model updates we trained our models until their convergence of validation perplexities around epochs on giga cm epochs on cnndm and epochs on nyt one epoch on giga cm takes around hours with nvidia tesla gpus the time costs for different pre training objectives are close we highlight the parameters used in the tuning stage that are different from the pre training stage others remain the same the learning rates for both the encoder and decoder are set to with warmup steps since both the encoder and decoder are already pre trained we trained our models for epochs on cnndm and epochs on nyt respectively we selected the best model with regard to rouge score on the validation set during decoding similar to liu and lapata dong et al we applied beam search with beam size of we also ducted experiments on the validation set of ndm with different beam sizes i e to cording to rouge l is indeed optimal detailed results with different beam sizes are cluded in the appendix b following paulus et al we also blocked repeated trigrams ing beam search and tuned the minimum summary length on the validation set in the range of the search range of minimum summary length was empirically set according to the summaries of training split of cnndm where the average and medium minimum lengths are both around we used step size of to get quick feedback similar to the pre training process the datasets with less instances were ne tuned with smaller batch sizes i e for nyt and for cnndm results automatic evaluation we used rouge lin to measure the quality of different summarization model puts we reported full length based and rouge l scores on ndm while we used the limited length call based and l on nyt the rouge scores are computed using the pl following durrett et al models in comparison is a baseline which simply takes the rst three sentences of a document as its summary bertext liu and lapata is an extractive model ne tuned on bert devlin et al that outperforms other extractive systems ptgen see et al drm paulus et al and dca celikyilmaz et al are ing based models extended with copy and age mechanism reinforcement learning as well as deep communicating agents individually tomup gehrmann et al assisted mary generation with a word prediction model bertabs liu and lapata and unilm dong et al are both pre training based models and are trained based on bert devlin et al we also implemented four stractive models as our baselines is a layer transformer with dom initialization when we replaced the coder of transformer with robertabase or robertalarge liu et al we obtain two baselines robertabase and respectively following liu et al we further train the robertalarge on the ments of training split of cnndm for epochs same as the number of epochs for our models indicated as in domain we replaced the coder of transformer with this further trained model resulting in robertacont results on cnndm the results on the ndm are listed in table the rst and ond blocks show results of previous extractive and com bheinzerling pyrouge git model r l bertext liu and lapata extractive abstractive ptgen see et al drm paulus et al bottomup gehrmann et al dca celikyilmaz et al bertabs liu and lapata unilm dong et al transformer robertabase roberta robertacont ours step in domain step giga cm sr nsg mdg all sr nsg mdg all table results on the test split of cnndm using length based and rouge l r l indicates signicant improvements p measured with the rouge script compared to models in the rst two blocks abstractive models respectively results of ours are all listed in the third block outperforms transformer by nearly rouge points roberta further improves the performance this shows the effectiveness of the pre trained encoders then we study the effects of different training objectives see section we rst train a transformer model the sizes of our model and roberta are identical on unlabeled documents of cnndm training split to get quick denoted as step domain from the top part of the third block in table we can see that sentence ing sr next sentence generation nsg and masked document generation mdg can all improve robertabase and roberta signicantly measured by the rouge terestingly even though we merely use the domain training split around gb our method still signicantly outperforms unilm dong et al that is pre trained on gb data compared to step in domain e pre training with sr epoch takes hours on cnndm and on nyt to the rouge script rouge almost always means a signicant difference with p model r l model corpus size bertext liu and lapata extractive abstractive ptgen see et al drm paulus et al bertabs liu and lapata transformer roberta ours step in domain step giga cm sr nsg mdg all sr nsg mdg all table results on the test set of nyt dataset using limited length recall based rouge indicates nicant improvements p measured with the rouge script to models in the rst two blocks with robertacont although the encoders of such two models are pre trained on the same corpus for the same epochs our model achieves better performance this shows that the mance gains mainly result from our proposed jectives for pre training the decoder together with the encoder training roberta longer may prove understanding tasks liu et al but no evidence shows longer training time for roberta may improve generation performance when we pre train the model on even larger dataset i e giga cm in the size of gb indicated as step giga cm the sults are further improved and our method forms all models under comparison as listed in the bottom part of table results on nyt table presents results on nyt dataset following the same evaluation tocol as durrett et al we adopted the limited length recall based rouge where we truncated the predicted summaries to the length of the gold ones again the rst and second blocks show results of previous extractive and abstractive models respectively results of our models are listed in the third block similar to the trends in cnndm our method leads to signicant mance gains with p comparisons among objectives among all three pre training objectives sr works slightly pegasus pegasus hugenews bart prophetnet gb prophetnet gb unilm step gb gb gb gb gb gb gb gb r l table results on the cnndm test split of models pre trained on different corpora indicates signicant differences from our model better than the other two objectives i e nsg and mdg we also tried to randomly use all the three objectives during training with ity each indicated as all interestingly we served that in general all outperforms all three objectives when employing unlabeled documents of training splits of cnndm or nyt which might be due to limited number of unlabeled uments of the training splits after adding more data i e giag cm for pre training sr sistently achieves the highest on both cnndm and nyt we conclude that sr is the most effective pre training objective for tive summarization since sentence reordering jective ts content reordering and it requires prehensively understanding a document in a wide coverage going beyond individual words and tences which is highly close to the essence of stractive document summarization we put the performance of our models on the validation splits of cnndm and nyt in the pendix b comparison to models pre trained with scale corpora it is worth noting that several models have been released recently which are trained using various corpora much larger than ours as listed in table top part raffel et al introduced gb as its training corpus pegasuslarge has two sions that are pre trained on and hugenews gb respectively both bart lewis et al and prophetnet gb yan et al are pre trained on a gb corpus introduced by liu et al we compare our best ing model step i e pre training on the cm dataset using sr objective with such els and focus on the performance on the ndm which is the well known benchmark for stractive summarization we highlight the est rouge scores in table using bold font and use the symbol to indicate the models that form signicantly different from step both and pegasus hugenews achieve signicantly higher scores than our model ever we obtain higher and rouge l scores on the other hand we also consider els pre trained on the relatively small scale pus following bert devlin et al both prophetnet gb yan et al and unilm dong et al use the same gb text for training as listed in table bottom part our model signicantly outperforms such two models systems mr bertabs unilm roberta step gold table human evaluation results proportions of tem rankings mr mean rank the lower the better as input and thus may lose information residing in the following tokens qualitative analysis with generated examples are illustrated in the appendix c human evaluation conclusion since summaries generated by abstractive models may produce disuent or ungrammatical outputs we also evaluated abstractive systems by eliciting human judgements we compared our best forming model i e pre training on the cm dataset using sr objective with human ences denoted as gold as well as several strong baselines whose system outputs are available to us including roberta and two pre training based models i e bertabs liu and lapata and unilm dong et al uments are randomly sampled from the test split of cnndm participants are presented with a document and a list of outputs generated by ent abstractive summarization systems then they are asked to rank the outputs of these systems from best to worst according to informativeness does the summary capture the informative part of the document uency is the summary cal and succinctness does the summary express the document clearly in a few words we report the proportions of system rankings and mean rank lower is better in table the output of step is selected as the best for the of cases and we obtained lower mean rank than all systems except for gold which shows the participants preference for our model we further converted ranking bers into ratings i e rank i is converted into and applied the student t test on the ratings ours is signicantly better than all other systems cept for gold in comparison with p ever it still lags behind human one possible son is that our system as well as other systems only takes the rst tokens of a long document we proposed three sequence to sequence training objectives including sentence reordering next sentence generation and masked document generation all those objectives have relations with abstractive summarization task and are signed based on reinstating the source text a model for abstractive document marization can be pre trained using such tives and then ne tuned on the summarization dataset compared to models pre training on the even larger corpora gb our method with only gb for pre training can still achieve parable and even better performance in the ture we would like to investigate other objectives to pre train models for abstractive marization acknowledgments we would like to thank the anonymous reviewers for their thoughtful and constructive comments yanyan zou and wei lu were supported by gapore ministry of education academic research fund acrf tier project references dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation by jointly learning to align and translate in proc of iclr asli celikyilmaz antoine bosselut xiaodong he and yejin choi deep communicating agents for abstractive summarization in proc of naacl of bertabs and unilm are publicly able at com nlpyang presumm and com microsoft unilm yen chun chen and mohit bansal fast tive summarization with reinforce selected sentence rewriting in proc of acl jianpeng cheng and mirella lapata neural summarization by extracting sentences and words in proc of acl john m conroy and dianne p oleary text marization via hidden markov models in proc of sigir jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language ing in proc of acl li dong nan yang wenhui wang furu wei aodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon unied language model pre training for natural language ing and generation in proc of nips greg durrett taylor berg kirkpatrick and dan klein learning based single document tion with compression and anaphoricity constraints in proc of acl elena filatova and vasileios hatzivassiloglou event based extractive summarization in text marization branches out sebastian gehrmann yuntian deng and alexander rush bottom up abstractive summarization in proc of emnlp jiatao gu zhengdong lu hang li and victor o k incorporating copying mechanism in li sequence to sequence learning in proc of acl karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa leyman and phil blunsom teaching chines to read and comprehend in proc of nips sepp hochreiter and jurgen schmidhuber neural computation long short term memory wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun a unied model for extractive and abstractive summarization using inconsistency loss in proc of acl mandar joshi danqi chen yinhan liu daniel s weld luke zettlemoyer and omer levy spanbert improving pre training by representing and ing spans transactions of the association for putational linguistics diederik p kingma and jimmy lei ba adam in proc of a method for stochastic optimization iclr julian kupiec jan pedersen and francine chen in proc of a trainable document summarizer gir mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer bart denoising sequence to sequence pre training for natural language generation translation and comprehension arxiv preprint chin yew lin rouge a package for in proc of matic evaluation of summaries workshop yang liu and mirella lapata text tion with pretrained encoders in proc of emnlp yinhan liu myle ott naman goyal jingfei du dar joshi danqi chen omer levy mike lewis luke zettlemoyer and veselin stoyanov roberta a robustly optimized bert pretraining proach in proc of acl christopher manning mihai surdeanu john bauer jenny finkel steven bethard and david mcclosky the stanford corenlp natural language in proc of acl system cessing toolkit strations ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based quence model for extractive summarization of ments in proc of aaai ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang tive text summarization using sequence to sequence rnns and beyond in proc of signll shashi narayan shay b cohen and mirella lapata do nt give me the details just the summary topic aware convolutional neural networks for treme summarization in proc of emnlp shashi narayan shay b cohen and mirella lapata ranking sentences for extractive in proc of rization with reinforcement learning naacl ani nenkova lucy vanderwende and kathleen eown a compositional context sensitive multi document summarizer exploring the factors that inuence summarization in proc of sigir myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier and michael auli fairseq a fast extensible toolkit for sequence modeling in proc of naacl demonstrations romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization in proc of iclr matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer deep contextualized word resentations in proc of naacl dragomir radev timothy allison sasha goldensohn john blitzer arda c elebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu jahna otterbacher hong qi horacio saggion simone teufel michael topper adam winkel and zhu zhang mead a platform for multidocument multilingual text summarization in proc of lrec alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever language models are unsupervised multitask learners openai blog colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu exploring the limits of transfer learning with a unied text to text former arxiv preprint evan sandhaus the new york times annotated corpus linguistic data consortium philadelphia abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proc of acl rico sennrich barry haddow and alexandra birch neural machine translation of rare words with word units in proc of acl kaitao song xu tan tao qin jianfeng lu and yan liu mass masked sequence to sequence in proc of pre training for language generation icml ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in proc of nips yu yan weizhen qi yeyun gong dayiheng liu nan duan jiusheng chen ruofei zhang and ming prophetnet predicting future zhou gram for sequence to sequence pre training arxiv preprint zhilin yang zihang dai yiming yang jaime bonell ruslan salakhutdinov and quoc v le xlnet generalized autoregressive arxiv preprint ing for language understanding jingqing zhang yao zhao mohammad saleh and ter j liu pegasus pre training with tracted gap sentences for abstractive summarization arxiv preprint xingxing zhang mirella lapata furu wei and ming zhou neural latent extractive document marization in proc of acl xingxing zhang furu wei and ming zhou hibert document level pre training of cal bidirectional transformers for document rization in proc of acl qingyu zhou nan yang furu wei shaohan huang ming zhou and tiejun zhao neural ment summarization by jointly learning to score and select sentences in proc of acl a additional setup details statistics for content reordering recall that it is not an unusual case that a human rewrites a document to summarize its most important mation yet does not track the ordering in which how such information is described in the ment this phenomena is dened as content ordering as follows for each document summary pair we rst map each sentence in the summary to one sentence in the document by maximizing the score if the relative orders of tences in the summary are different from the tive orders of their mapped sentences in the inal document we count this as one content ordering we did statistics of such cases over the training and validation splits of cnndm dataset to be specic we borrow the sentence annotations from extractive summarization zhang et al zhou et al zhang et al that sider extractive summarization as a sentence sication task the sentences in a document that maximize score lin against the human references are labeled as true while other sentences are assigned false like previous extractive summarization systems zhang et al we also concatenate sentences with label true in a document as its associated mary for each sentence in a summary we search for its string closet sentence in its associated ument according to the count of overlapped grams we found that for some instances the relative orders of sentences in the summary is not consistent with the relative orders of their closet sentences appearing in the document in practice we found that instances in the training split and instances in the validation set have this phenomenon b additional results results on validation set the performance of our proposed models on the validation splits of cnndm and nyt are listed in table and spectively results on validation set of cnndm with ferent beam sizes table lists the rouge l beam size rouge l table rouge l results on the validation set of cnndm with different beam sizes model mnli sst qqp qnli sts rte mrpc cola roberta step encoder table results on glue model r l step in domain step giga cm sr nsg mdg all sr nsg mdg all table results on the validation split of cnndm ing full length based and rouge l r l model r l step in domain step giga cm sr nsg mdg all sr nsg mdg all table results on the validation set of nyt dataset using limited length recall based rouge results on the validation set of cnndm with ferent beam sizes for the beam search during coding beam size of gives the highest l score thus we use in this work results on xsum different from cnndm and nyt xsum consists of online news articles extracted from british ing corporation bbc each annotated with a short one sentence news summary ing the question what is the article about the same split for ing validation testing and preprocessing dures described in the work of narayan et al are adopted to make direct comparisons model r l extractive abstractive ptgen see et al narayan et al bertabs liu and lapata transformer roberta step ours table results on the test split of xsum using length based and rouge l r l table lists results on the xsum here we report our model pre trained using sr objective on the in domain pre training corpus indicated as step as we can see that after pre training step does not give performance gain one ble reason is that the summary of xsum contains only one sentence the sr objective might not be helpful for this dataset results on glue we also apply the encoder of our best performing model to the glue tasks as listed in table compared to roberta liu et al the encoder of our best performing model does not consistently achieve higher results which demonstrates that the improvements of our models on the abstractive summarization task do not come from a better encoder c examples of system outputs table and demonstrate three output ples of various systems including bertabs liu and lapata unilm dong et al gold standard summaries human references noted as gold the roberta baseline and our best performing model table shows an ample that the outputs of systems bertabs and unilm copied a sentence from the input article while our model generates summaries by ing sentences table lists an instance where the summary generated by the system unilm tains an incomplete sentence article bertabs unilm cnn in response to reports of big banks threatening to withhold campaign funds from senate democrats sen elizabeth warren last week offered a deant response bring it on warren said she is nt going to slack off on her calls for breaking up banks and other measures to rein in wall street as hillary clinton prepares to ofcially launch her presidential campaign this month she will need to make a choice about how much to highlight issues relating to economic inequality former maryland gov martin omalley who is also running for the democratic nomination is trying to steal clinton s thunder by talking about the problems of disproportionate wealth in other words there are many signs that democrats are planning to take on the big issue of economic inequality but in other recent news the likelihood that new york s chuck schumer will replace harry reid as leader of the senate democrats means the dreams of a more economically leftward party are crashing into political reality while schumer has been a very effective democrat and skilled legislative leader he is also a wall street democrat who has spent much of his time courting and protecting powerful nancial interests who run one of the dominant industries in his state he is not alone even at his most progressive moments president barack obama relied on wall street donations for both of his campaigns despite all the talk from conservatives about left wing socialism in the white house the nancial community has been willing to open its coffers to democrats without much concern even in the election democratic populism ca nt really work within the current campaign nance system the enormous pressures for parties to raise funds in campaigns has for many decades created pressure on democrats despite their political base to court big donors during the california democrat tony coelho serving as the chairman of the democratic congressional campaign committee and then as majority whip made a strong appeal to savings and loans executives before the crash of the industry to catch up to republicans who had been outanking them in raising money the democrats were and have continued to losing their traditional base of campaign support organized labor which had been a central source of campaign muscle since the providing money and campaign assistance during campaigns without organized labor to serve as their foundation and with the pressure for raising private funds increasing many democrats concluded they needed business by their side democrats running for president have made the same kind of choices in obama disappointed many supporters upon becoming the rst president to abandon the post watergate public nance system for campaigns altogether preferring to raise money himself for the general campaign while small donors were enormously important to his victories so too were business and wall street executives at the height of the nancial crash when public sentiment had clearly turned against wall street the administration agreed to a nancial regulation bill dodd frank that was structured in such a way as to give powerful interests more than enough opportunity to limit the bite over the coming years wall street with an army of counsel succeeded in eroding the impact of the legislation not only does the acceptance of our campaign nance system limit the policy choices democrats can make but it also greatly damages the party s brand name as the washington post reported the scandal that might bring down new jersey democratic sen robert menendez is the rst involving large scale super pac donations at the heart of the story is almost that physician salomon melgen gave to senate majority pac possibly in exchange for favors this is not simply some sort of accommodation of democrats to the corporate system they do nt have much of a choice without these funds they wo nt be able to compete in this election cycle independent campaign donors are causing a huge stir in conservative circles the koch brothers and their allies are throwing around enormous amounts of money to candidates who will support their deregulatory agenda individual donors such as las vegas gambling magnate sheldon adelson are causing ripples every time candidates speak pressuring them to adjust their agenda democrats have found their own magnates for political support such as tom steyer and george soros this is why campaign nance reform is so important without congress changing the fundamental dynamics there wo nt be much room for populism to thrive even if democrats select someone like a elizabeth warren as their candidate or hillary clinton decides to move sharply to the left on economic policy there wo nt be much room for reform when the time of governance actually starts the democratic party needs wall street more than it needs to take a stand against wall street those are the facts on the ground if democrats really want to take on wall street and tackle economic inequality they rst have to bring about reform of the campaign nance system if campaigns were publicly funded or there were more stringent limits on independent expenditures wall street would have much more trouble achieving disproportionate inuence reform could level the playing eld more often than not campaign nance reform is an issue that gets sidetracked with little more than some pro forma words of support a more populist economic agenda that revolved around progressive taxation and substantial public assistance to strengthen the middle class can only work in a different kind of political system if things stay the same democrats can only continue to win elections by turning to their corporate and nancial base of support julian zelizer sen elizabeth warren said she is nt going to slack off on her call for breaking up banks he says the likelihood that new york s chuck schumer will replace harry reid as leader of the senate democrats zelizer democratic populism ca nt really work within the current campaign nance system julian zelizer democratic populism ca nt really work within the current campaign nance system he says the pressure for parties to raise funds in campaigns has created pressure on democrats to court big donors he says even at his most progressive moments president barack obama relied on wall street donations for both campaigns he says obama s decision to abandon the post watergate public nance system step roberta julian zelizer sen elizabeth warren is nt going to slack off on wall street zelizer democrats are planning to take on the big issue of economic inequality he says democrats have lost their traditional base of campaign support organized labor and money in their campaigns julian zelizer democrats are planning to take on the big issue of economic inequality zelizer democratic populism ca nt work within the current campaign nance system he says democrats have lost traditional base of campaign support julian zelizer elizabeth warren was deant about wall street but hillary clinton likely wo nt be zelizer the democrats need wall street s campaign donations to be competitive in gold table an example article sampled from the test splitting of cnndm paired with a list of summaries generated by different systems we highlight with bold the sentences in the summaries that are copied from the article article bertabs unilm cnn hillary clinton is nally announcing her candidacy for the presidential election although she has watched her standing in the polls sag in recent months there is likely to be a boost in the days that follow the announcement for democrats there is ample reason to be excited about clinton s run for the presidency she is certainly one of the strongest candidates in many decades she brings to the table extensive political and policy experience a combination of skills that is often lacking she has been through some of the roughest partisan wars and emerged stronger than ever before she has a keen sense about the nature of the modern news media how to use it to her advantage and how to survive scandal frenzies she is a hardened tough partisan who will not shy away from republican attack americans have many positive memories of clinton name given the booming economy of the late during bill clinton s presidency if hillary clinton puts together an effective campaign she could be unbeatable in the democratic primaries as well as in the general election however during the buildup to her nal decision some of her weaknesses have also been exposed clinton does nt want to end up like vice president al gore in although he did relatively well in the nal election with many americans believing that he did actually defeat george w bush he did nt generate much energy once the campaign started although he too was touted as a perfect candidate who was the ideal person for the job something seemed stiff and inauthentic when he actually hit the trail he seemed to freeze when the television cameras were rolling gore had trouble connecting with voters and he seemed to remake his image constantly his biggest asset ended up being that he was viewed as the inevitable nominee rather than what he actually stood for clinton must avoid following gore s path she suffered this fate in the primaries and ca nt afford to do so again she needs to do more than rest on the perception that her candidacy is inevitable and on her record of experience that is not enough more important is for her to put forth an exciting vision about what she would stand for in the white house voters thirst for signs of greatness when they pick their presidents even if they are savvy enough to understand that the reality of a polarized washington will probably limit her ability to achieve bold change a recent story in the washington post suggests that her advisers are aware of this potential liability after the announcement they are going to avoid big rallies and events and instead concentrate on smaller events where she will meet with voters directly in states such as iowa and new hampshire clinton also will have to contend with doubts about her authenticity in his rst day on the campaign trail sen rand paul immediately tapped into these concerns by raising questions about whether she could be trusted that question has dogged the clintons ever since they came onto the national political scene in the late their greatest virtue their immense skills as politicians has often come back to haunt them bill clinton was attacked as slick willie by members of both parties for the perception that he would say anything to win and hillary clinton has faced similar criticism when she tried to distance herself from her vote for the use of force in iraq many democrats did nt buy her critique of president george w bush s foreign policies and went for barack obama instead when she conducted her listening tour of new york before running for the senate many voters saw it as a manufactured effort to hide the fact she was running for ofce as an outsider when she explained that there was nothing to the recent stories about her use of a private email server rather than her state department email some felt that even if the story was relatively minor it indicated that she was nt always telling us what she was really about even if she is nt hiding anything she often gives that appearance during the next few months clinton will also have to connect with her party s base the ongoing speculation about sen elizabeth warren of massachusetts has suggested that the most active part of the democratic party is not that enthused with clinton s candidacy while they will probably vote for her they are not very motivated and do nt trust that she will stand for democratic values she will need to address these concerns not through her style but through her agenda voters will want to hear her talking about issues such as tougher nancial regulation and policies to diminish economic inequality as well as her positions on race and policing she will also need to make clear that she has heard voters on being too hawkish about going to war and give clear indications about how she would handle a nuclear agreement with iran clinton will also have to contend with the gender bias that still exists in the electorate at large without any doubt she will be subject to questions and comments about her appearance for instance that wo nt be aimed at male candidates part of her candidacy is itself an effort to break down these remaining vestiges of political sexism but the struggle will be tough finally and this relates to the last challenge clinton will have to contend with her husband to be sure he can be an immense force on the campaign trail one of the most compelling democrats of our generation but he can also be liability as she learned in bill clinton is not always easy to control when he speaks his mind as he did in dismissive comments about obama s candidacy it can often work against her the fund raising records of the clinton foundation will also raise questions about conict of interest and ongoing stories about his personal life as was the case when monica lewinsky returned to the media a few months ago could re emerge on the campaign trail whether that is fair or not is beside the point everything is fair game on the modern campaign trail hillary clinton has the potential to be a hugely successful presidential candidate but she and her campaign team will need to address the multiple questions and weaknesses that have become clear in recent months julian zelizer hillary clinton is nally announcing her candidacy for the presidential election zelizer she has been through some of the roughest partisan wars and emerged stronger than ever before he says she is a hardened tough partisan who will not shy away from republican attack julian zelizer hillary clinton is nally announcing her candidacy for the presidential election he says she has extensive political and policy experience a combination of skills often lacking he says clinton does nt want to end up like vice president al gore in he did nt generate much energy once the campaign started clinton must avoid following gore s path he step roberta julian zelizer for democrats there is plenty of reason to be excited about hillary clinton s run zelizer if clinton puts together an effective campaign she could easily win the general election he says clinton needs to put forth on what she would stand for in the white house julian zelizer for democrats there is ample reason to be excited about hillary clinton s run for president zelizer clinton needs to put forth an exciting vision about what she would stand for julian zelizer hillary clinton has immense political and governmental experience he says she needs to make stronger connection to her party s base clinton also needs to convince voters of her authenticity zelizer says gold table an example article sampled from the test splitting of cnndm paired with a list of summaries generated by different systems the incomplete sentence is highlighted with bold
