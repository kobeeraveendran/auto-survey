r m r s c v v x r automatic text summarization approaches speed topic model learning process mohamed juan manuel torres richard javier ramrez georges universite davignon pays de vaucluse france rstname avignon fr ecole polytechnique de montreal quebec canada autonoma metropolitana azcapotzalco mexico uam mx abstract number documents available internet moves day reason processing information eectively expressibly major concern companies scientists methods represent textual document topic representation widely information retrieval ir process big data wikipedia articles main diculty topic model huge data collection related material resources cpu time memory required model estimate deal issue propose build topic spaces summarized documents present study topic space representation context big data topic space representation behavior analyzed ent languages experiments topic spaces estimated text summaries relevant estimated complete ments real advantage approach processing time gain showed processing time drastically reduced summarized documents general study nally points dierences thematic representations documents depending targeted languages english latin languages introduction number documents available internet moves day ponential way reason processing information eectively expressibly major concern companies scientists portant information conveyed textual documents blogs micro blogs general advertise websites encyclopedic documents type textual data increases day new articles vey large heterogenous information famous collaborative internet encyclopedia wikipedia enriched worldwide volunteers visited website usa million users visiting preprint international journal computational linguistics applications site daily total reaching millions estimated million internet users massive number documents provided wikipedia mainly exploited natural language processing nlp scientists tasks word extraction document clustering automatic text summarization dierent classical representations document term frequency based tation proposed extract word level information large data limited time nonetheless straightforward tations obtain poor results nlp tasks respect abstract complex representations classical term frequency tion reveals little way inter document statistical structure allow capture possible unpredictable context dependencies reasons abstract representations based latent topics proposed known latent dirichlet allocation lda approach outperforms classical methods nlp tasks main drawback topic based representation time needed learn lda latent variables massive waste time occurs lda learning process mainly documents size number documents highly visible context big data wikipedia solution proposed article summarize documents contained big data corpus wikipedia learn lda topic space answer raised diculties reducing processing time lda learning process retaining intelligibility documents maintaining quality lda models summarization approach size documents drastically reduced intelligibility documents preserved assumption lda model quality conserved reasons classical term frequency document reduction considered paper extraction subset words represent document content allows reduce document size document structure intelligibility document main objective paper compare topic space representations complete documents summarized ones idea eectiveness document representation terms performance processing reduction summarized documents topic space representation behavior analyzed dierent languages english french spanish series proposed experiments topic models built complete summarized documents evaluated jensen shannon j s divergence measure perplexity measure best knowledge extensive set experiments interpreting ation topic spaces built complete summarized documents human models alexa com com rest paper organized following way section duces related work areas topic modeling automatic text rization evaluations section describes proposed approach including topic representation adopted work dierent summarization systems employed section presents topic space quality measures evaluation experiments carried results presented section discussion nally proposed section concluding section related work methods proposed information retrieval ir researchers process large corpus documents wikipedia encyclopedia methods consider documents bag words word order taken account rst methods proposed ir propose reduce ment discrete space words documents vector numeral values represented word counts number occurrences document named tf idf approach showed eectiveness dierent tasks precisely basic identication discriminative words document method weaknesses small tion description length weak intra statistical structure documents text corpus substantiate claims tf idf method ir researchers proposed dimensionality reductions latent semantic ysis lsa uses singular value decomposition svd reduce space dimension method improved proposed probabilistic lsa plsa plsa models word document sample mixture model mixture components multinomial random variables viewed representations topics method demonstrated performance tasks sentence keyword extraction spite eectiveness plsa approach method main drawbacks distribution topics plsa indexed training documents number parameters grows training document set size model prone overtting main issue ir task documents tering address shortcoming tempering heuristic smooth parameter plsa models acceptable predictive performance authors showed overtting occur tempering process overcome issues latent dirichlet allocation lda method proposed number lda parameters grow size training corpus lda candidate overtting section scribes precisely lda approach experimental study authors evaluated eectiveness jensen shannon j s theoretic measure predicting systems ranks summarization tasks query focused update summarization shown ranks produced pyramids produced j s measure correlate investigate eect measure summarization tasks generic multi document summarization duc task biographical summarization duc task opinion summarization tac os summarization languages english section describes proposed approach followed article ing topic space representation lda approach evaluation perplexity jensen shannon metrics overview proposed approach figure describes approach proposed paper evaluate quality topic model representation automatic text summarization systems latent dirichlet allocation lda approach described details section topic representation conjunction dierent state art summarization systems presented section fig overview proposed approach wikipediaenglish spanish frenchtraintestsummarysystemlatent dirichlet allocationfull textartex baseline baseline randomtopic spaces documents summarizedtopic spaces documents summarizedperplexitykl topic representation latent dirichlet allocation lda generative model considers document seen bag words mixture latent topics opposition multinomial mixture model lda considers theme associated occurrence word composing document associate topic complete document document change topics word word occurrences connected latent variable controls global respect distribution topics document latent topics terized distribution word probabilities associated plsa lda models shown generally outperform lsa ir tasks lda provides direct estimate relevance topic knowing word set figure shows lda formalism document d corpus d rst parameter drawn according dirichlet law parameter second parameter drawn according dirichlet law parameter generate word w document c latent topic z drawn multinomial distribution knowing topic z distribution words multinomial parameters parameter drawn documents prior parameter allows obtain parameter binding documents fig lda formalism techniques proposed estimate lda parameters variational methods expectation propagation gibbs sampling gibbs sampling special case markov chain monte carlo mcmc gives simple algorithm approximate inference high dimensional models lda overcomes diculty directly exactly estimate parameters maximize likelihood data collection dened knowing dirichlet parameters data collection w w rst use gibbs sampling estimating lda reported comprehensive description method found zwwordtopicndtopicdistributionworddistribution section describes income lda technique input lda method automatic summary document train corpus summaries built dierent systems automatic text summarization systems text summarization systems proposed years baseline systems artex summarization system reaches state art performance presented section baseline rst bf baseline leadbase selects n rst sentences documents n determined compression rate simple method strong baseline performance automatic summarization system old simple sentence weighting heuristic involve terms assigns highest weight rst sentences text texts genres news reports scientic papers specically designed heuristic e scientic paper tains ready summary beginning gives baseline proves hard beat texts worth noting document standing conference duc competitions ve systems performed baseline demerit systems baseline genre specic baseline random br baseline random randomly selects n tences documents n determined compression rate method classic baseline measuring performance automatic text summarization systems artex text artex algorithm simple extractive algorithm main idea represent text suitable space model vsm average document vector represents average global topic sentence vectors constructed time lexical weight sentence e number words sentence obtained angle average document sentence calculated narrow angles indicate sentences near global topic important extracted figure vsm words p vector sentences average global topic represented n dimensional space words angle sentence s global topic b processed follow b s b weight sentence calculated proximity global topic lexical weight figure lexical weight fig global topic vector space model n words represented vsm p sentences narrow angles indicate words closest lexical weight important finally summary generated concatenating sentences highest scores following order original document formally artex algorithm computes score tence calculating inner product sentence vector average pseudo sentence vector global topic average pseudo word weight pre processing complete matrix n words p sentences created let s vector sentence average pseudo word vector dened average number occurrences n words sentence s n j fig lexical weight vector space model p sentences topicssentencevsm average pseudo sentence vector occurrences word j p sentences bj average number bj p weight sentence s calculated follows b s np n bj p computed equation normalized interval calculation s b indicates proximity sentence s average pseudo sentence b weight proximity average pseudo word sentence s near b corresponding element high value s high score sentence s far main topic e s b near corresponding element amu low value e amu near s low score product s necessary divide scalar product constant angle element scale factor modify b s np n bj p term constant value equation equation equivalent summarization system outperforms cortex fresa measure artex evaluated corpus medecina clinica artex performance better cortex english spanish french targeted languages study evaluation lda model quality previous section described dierent summarization systems reduce size train corpus retain relevant information contained train documents section proposes set metrics evaluate quality topic spaces generated summaries train documents rst perplexity score popular propose study measure evaluate dispersion word given topic space measure called jensen shannon j s divergence perplexity perplexity standard measure evaluate topic spaces generally probabilistic model topic model z eective correctly predict unseen document test collection perplexity language modeling monotonically decreasing likelihood test data algebraically equivalent inverse geometric mean word likelihood lower perplexity score indicates better generalization performance exp log p w nb m nb nd m nb combined length m testing terms nd number words document d p w likelihood generative model assigned unseen word w document d test collection quantity inside exponent called entropy test collection logarithm enables interpret entropy terms bits information jensen shannon j s divergence perplexity evaluates performance topic space important information distribution words topic kullback leibler divergence kl estimates topic dierent n topics contained topic model distribution dened pi log wa pi pi p pj p probabilities word w generated topic zi symmetric kl divergence named jensen shannon j s divergence metric mid point measure zi j s dened equation mean divergences zi zi j pi log pj log pi pj pj pi wa j s divergence entire topic space dened divergence pair topics composing topic model z dened equation j j zj z ziz ziz zj z wa pi log pj log pi pj pj pi j log pj pi dening metrics evaluate quality model section describes experiment data sets experimental protocol experiments summarization systems compress retain relevant information train text collection language section presents experiments processed evaluate relevance eectiveness proposed system fast robust topic space building experimental protocol presented qualitative analysis obtained results performed evaluation metrics described section experimental protocol order train topic spaces large corpus documents required corpus corpus c particular language english spanish french composed training set testing set b corpus composed articles wikipedia languages set documents collected corpus summarized build topic spaces evaluate model need summarized table shows latin languages french spanish similar size dierence observed english bigger english text corpus times bigger french spanish corpus spite size dierence corpus number words sentences article note english vocabulary size roughly latin languages observations table presents statistics document level mean corpus section outcome fact seen perplexity evaluation topic spaces built english train text collection set topic spaces trained evaluate perplexity shannon j s scores language processing time marize compress documents train corpus following classical study lda topic spaces quality number topics model xed topic spaces built mallet toolkit table dataset statistics wikipedia corpus language words unique words sentences english spanish french table dataset statistics document wikipedia corpus language words unique words sentences english spanish french results experiments conducted paper topic based concern metric proposed section perplexity j s applied language english spanish french topic space size nally compression rate summarization process original size documents figures present results obtained varying number topics figure c percentage summary figure respectively perplexity jensen shannon j s measures results computed mean topic spaces size mean dierent reduced summaries size language study separately point dierences topic spaces quality depending language fig perplexity varying number topics corpus fig perplexity varying summary corpus discussions results reported figures allow point rst general remark observed section latin languages tendencies explained root languages latins figure shows spanish french corpus obtain perplexity number classes topic space varies observation languages topic spaces obtained summarized documents outperform ones obtained complete documents topics considered figures b best system languages ordered way systems ordered best worst manner artex bf fact explained noted j s measure curves figures br considerer number topics note topic spaces text documents e summarized english text corpus obtain better perplexity smaller documents processed summarization system particularly visible figures address shortcoming size english corpus times bigger latin languages number topics contained thematic space increased eectively disconnect words topics spite moving number topics perplexity topic spaces summarization systems random baseline rb perplexity obtained english corpus higher obtained spanish french corpus summarization systems reduce documents train corpus baseline bf obtains good results languages performance fact bf selects rst paragraph ment summary wikipedia content provider writes new article exposes main idea article rst sentences furthermore rest document relates dierent aspects article subject historical economical details useful compose relevant summary baseline hard outperform documents summarize encyclopedia wikipedia fig jensen shannon measure varying number topics fig jensen shannon measure varying summary corpus random baseline rb composes summary randomly selecting set sentences article kind system particularly relevant main ideas disseminated document blog website main reason baseline obtain good results j s divergence measure figures explained fact system selects sentences dierent places selects variable set words topic spaces documents contain variable vocabulary j s divergence evaluates word contained topic discriminative allows distinguish topic compose thematic representation figures jensen shannon j s divergence scores tween topics obtained similar performance order summarization systems languages corpus text documents outperform topic spaces representation languages summary rates reason text documents contain larger vocabulary j s divergence sensitive vocabulary size especially number topics equal summarized text documents observation pointed ures b means topic spaces summary rate text documents summarization systems points curves topic spaces high number topics estimated summaries outperform estimated text documents closer ones conrms original idea motivated work tables nally present processing time seconds varying number topics language corpus respectively text summarized documents processing time saved topic spaces learned summarized documents tables processing times follow exponential curve especially text context reason easily imagine processing time saved summaries instead complete documents inevitably contain non informative irrelevant terms general remark best summarization system artex account processing time topic space learning baseline bf best agreement want nd common ground low perplexity high j s divergence topics fast learning process bf method chosen table processing time seconds varying number topics corpus system text english spanish french language table processing time seconds varying number topics corpus system artex english spanish french br english spanish french bf english spanish french language language language system system conclusions paper qualitative study impact documents summarization topic space learning proposed basic idea learning topic spaces compressed documents time consuming learning topic spaces documents noted main advantage use text document text corpus build topic space semantic variability topic increase divergence ones experiments topic spaces topics size roughly divergence topic spaces large number topics e suitable knowing size corpus topics case lower perplexity better divergence topics time consuming lda learning process drawback topic spaces learned text corpus summarized documents disappear number topics comes suitable size corpus language considered references salton g automatic text processing transformation analysis retrieval blei d ng jordan m latent dirichlet allocation journal machine baeza yates r ribeiro neto b al modern information retrieval volume information computer learning research acm press new york salton g mcgill m j introduction modern information retrieval salton g yang c s specication term values automatic indexing journal documentation deerwester s dumais s furnas g landauer t harshman r indexing latent semantic analysis journal american society information science bellegarda j latent semantic analysis framework large span language eling fifth european conference speech communication technology hofmann t probabilistic latent semantic analysis proc uncertainty articial intelligence uai citeseer bellegarda j exploiting latent semantic information statistical language eling proceedings ieee suzuki y fukumoto f sekiguchi y keyword extraction term domain interdependence dictation radio news international conference computational linguistics volume acl popescul pennock d m lawrence s probabilistic models unied laborative content based recommendation sparse data environments proceedings seventeenth conference uncertainty articial intelligence morgan kaufmann publishers inc louis nenkova automatically evaluating content selection empirical methods natural language rization human models processing singapore lin j divergence measures based shannon entropy ieee transactions hofmann t unsupervised learning probabilistic latent semantic analysis information theory machine learning minka t laerty j expectation propagation generative aspect model proceedings eighteenth conference uncertainty articial gence morgan kaufmann publishers inc griths t l steyvers m finding scientic topics proceedings national academy sciences united states america geman s geman d stochastic relaxation gibbs distributions bayesian restoration images ieee transactions pattern analysis machine ligence heinrich g parameter estimation text analysis web arbylon net publications text est pdf torres moreno j m automatic text summarization wiley sons torres moreno j m artex text summarizer cs ir ledeneva y gelbukh garca hernandez r terms derived frequent sequences extractive text summarization computational linguistics intelligent text processing springer manning c d schutze h foundations statistical natural language ing mit press cambridge massachusetts duc document understanding conference torres moreno j m velazquez morales p meunier j g cortex un algorithme volume lyon pour la condensation automatique textes france torres moreno j m saggion h cunha sanjuan e velazquez morales p summary evaluation references polibits rosen zvi m griths t steyvers m smyth p author topic model authors documents proceedings conference uncertainty articial intelligence auai press mccallum k mallet machine learning language toolkit cs umass edu
