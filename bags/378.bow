scientic document summarization for laysumm and longsumm sayar ghosh roy nikhil pinnaparaju risubh jain manish gupta and vasudeva varma information retrieval and extraction lab international institute of information technology hyderabad india sayar ghosh nikhil iiit ac in risubh iiit ac in manish gupta ac in abstract text summarization has been automatic task in widely studied as an important traditionally natural language processing various feature engineering and machine learning based systems have been proposed for extractive as well as abstractive text summarization recently deep learning based specically transformer based systems have been immensely popular summarization is a cognitively challenging task extracting summary worthy sentences is laborious and expressing semantics in brief when doing abstractive summarization is complicated in this paper we specically look at the problem of summarizing scientic research papers from multiple domains we differentiate between two types of summaries namely laysumm a very short summary that captures the essence of the research paper in layman terms restricting overtly specic technical jargon and longsumm a much longer detailed summary aimed at providing specic insights into various ideas touched upon in the paper while leveraging latest transformer based models our systems are simple intuitive and based on how paper sections contribute to human maries of the two types described above evaluations against gold standard summaries using rouge lin metrics prove the effectiveness of our approach on blind test corpora our system ranks rst and third for the longsumm and laysumm tasks respectively introduction popularity of data science in recent years has led to a massive growth in the number of published papers online this has generated an epochal change in the way we retrieve analyze and consume tion from these papers also wider interest in data science implies even lay persons readers outside the author also works as a researcher at microsoft the data science community are signicantly ested in keeping up with the latest developments the readers have access to a huge amount of such research papers on the web for a human standing large documents and assimilating crucial information out of them is often a laborious and time consuming task motivation to make a concise representation of huge text while retaining the core meaning of the original text has led to the ment of various automated summarization systems these systems provide users ltered high quality and concise content to work with at an dented scale and speed summarization methods are mainly classied into two categories extractive and abstractive extractive methods aim to select salient phrases sentences or elements from the text while abstractive techniques focus on generating summaries from scratch without the constraint of reusing phrases from the original text scientic papers are large complex documents that tend to be geared towards a particular audience this is a very small percentage of the population while majority of individuals are unable to fully comprehend the contents of long scientic ments even among the people who are able to understand the material the length of such ments often spanning several pages demand a great deal of time and attention hence tasks like layman summarization laysumm and long form rization longsumm are of great importance in today s world typically scientic research papers are fairly structured documents containing standard sections like abstract introduction background related work experiments results discussion conclusion and acknowledgments thus summarization of such documents should be aware of such sectional structure an intuitive way is to pick a few tences from each of the sections to be a part of the summary but how do we decide how many sentences to pick from each section also which n a j l c s c v v i x r a sentences to pick can we rewrite sentences so as to obtain a concise abstractive summary we investigate answers to these questions in this paper multiple survey papers have provided a detailed overview of the automatic text summarization task tas and kiyani nenkova and eown allahyari et al most of the practically usable summarization systems are tractive in nature also most summarization ies have focused on summarization of news articles in this work we mainly focus on two interesting pects of text summarization summarization of scientic research papers and summarization for laymen cohan et al propose that section level processing of scientic documents is useful ther collins et al conclude that not all sections are equally useful also recent papers have observed that a hierarchical summarization of scientic documents is highly effective where at the rst level an extractive summary of each section is independently generated and at the ond level the sectional output is abstracted into a brief summary subramanian et al erera et al xiao and carenini observe that while summarizing local context is useful but global is not thus in our approach at the sectional level we use extracted information from only within the section text to obtain a section s extractive summary ignoring the remaining text of the entire paper for the laysumm task we observe that abstract is the most relevant section of a scientic paper from a layman perspective we therefore feed the abstract to a transformer based model and ate an abstractive summary for the laysumm task for the longsumm task we rst perform tive summarization for each section and choose a selected number of sentences from each section into the nal summary on blind test corpora of and papers for the laysumm and longsumm tasks our proposed system leads to a rouge of and respectively these results helped us bag the top positions on the leaderboards for the two tasks related work in this section we discuss related areas including text summarization and style transfer automatic text summarization text summarization focuses on summarizing a given document and obtaining its key information bits there are two types of text summarization methods extractive summarization and tive summarization extractive summarization extractive summarization deals with extracting pieces of text directly from the input document tractive summarization can also be seen as a text classication task where we try to predict whether a given sentence will be part of the summary or not liu most papers in this area focus on the summarization of news articles but several others focus on specic domains like tion of medical documents legal documents entic documents summarization can also be performed in a query sensitive manner or a user centric manner sentence scoring methods include graph based methods like lexrank erkan and radev or textrank mihalcea and rau machine learning or deep learning techniques and position based methods recently various deep learning architectures such as ert zhang et al bertsum liu and lapata summarunner nallapati et al csti singh et al and hybrid net singh et al have been proposed for extractive summarization abstractive summarization in abstractive summarization the model tries to generate the summary instead of extracting tences or keywords as compared to tive summarization this is more challenging and requires strong language modeling schemes to achieve good results traditionally abstractive summarization techniques have focused on erating short text such as headlines or titles but more recently there have been efforts on ation of longer summaries older methods have depended on tree transduction rules cohn and pata and quasi synchronous grammar proaches woodsend and lapata for tive abstractive summarization recently neural summarization approaches have been found to be more effective effective neural representative language models are very important for text eration tasks with the recent breakthrough of transformer based vaswani et al tectures like bert devlin et al raffel summary a set of research papers were vided as the blind test data the laysumm dataset comprises of full text papers with lay summaries in a variety of domains epilepsy archeology and materials engineering and from a number of nals elsevier made available a collection of lay summaries from a multidisciplinary collection of journals as well as their abstracts and full texts for a small sample dataset look at laysumm s ofcial github longsumm dataset the corpus for this task includes a training set that consists of extractive summaries and abstractive summaries of scientic papers in the domains of natural language processing and machine learning the extractive summaries are based on video talks from associated ences lev et al while the abstractive maries are blog posts created by nlp and ml searchers the average gold summary length was tokens the research papers were parsed ing the science library a collection of pdfs of research papers served as the blind test set the longsumm train and test datasets are publicly accessible on longsumm s ofcial github system overview in this section we present an overview of the posed systems for the laysumm and longsumm tasks system overview for laysumm we observed that the laysumm summaries in the train set were highly abstractive in nature with a length limit of words in fig we alyze how information from each paper section contributes to the nal lay summary by ing the rouge overlap between a paper section and the available gold summary this analysis is performed for the entire dataset fig shows that the abstract was the most signicant section followed by the conclusion moreover a tively high rouge l overlap indicates some gree of verbatim copying from the abstract onto com wing scisumm corpus blob master readme laysumm com figure and rouge l laps between paper sections and laysumm summary figure and rouge l laps between paper sections and longsumm summary al and bart lewis et al ing these types of models is crucial for obtaining good textual representations on the target side for neural abstractive summarization text style transfer neural text style transfer is yet another related area of work where the document in style a is converted to style b without any loss of content or tics syed et al vadapalli et al this work leverages transformer encoder decoder els the text encoder is used to obtain robust latent representations while the decoder generates text with a particular target style datasets we rst describe the datasets which were provided by the organizers of the workshop on scholarly document processing laysumm dataset a dataset of research papers and ing gold standard lay summaries were available for training tokens being the average length of a github io science parse sharedtasks html com guyfe longsumm scoresmean scoresmean scoresmean scoresmean scoresmean scoresmean rl the lay summary in addition to providing a high rouge overlap the conclusion section was atively shorter in length this indicates that the conclusion section contains a great degree of ful information in a more condensed fashion note that we picked the paper sections directly from the paper text without performing any orate conation on section headers conation in general should not hurt the performance of our models since a particular paper will contain only one form of the section heading e it will tain either materials and methods or methods however we plan to explore deeper section wise analysis using improved conation as part of future work we leveraged pretrained transformer models for conditional generation given a set of individual tions our results indicate that using abstract as the only sequence for conditional generation is a ter choice as compared to utilizing more sections therefore the problem at hand is one of ing salient information as one would expect from a summarization task with the additional avor of text style transfer figure system architecture for longsumm system overview for longsumm we performed a similar section contribution uation and considered section headings which peared in at least of all papers in our training set for the longsumm task fig shows that the introduction is the most important section when it comes to creating summaries followed by related work and results for our summary generation architecture we considered one section at a time without the global context as discussed earlier this was guided by isting scientic evidence from xiao and carenini which showed that not considering the global context and focusing purely on the section at hand is marginally better than doing otherwise based on section contribution evaluations we constructed a budget module to calculate how much weight to assign to a section for the purpose of combining tion summaries into the nal long summary fig illustrates the broad architecture of our proposed system for summarizing each individual section we used summarunner nallapati et al a ple neural extractive summarizer we pre trained summarunner on the pubmed cohan et al dataset to generate paper abstracts from ious paper sections we show results using tions in our budget module setting various cutoff thresholds for overlap in order for a section to be considered for the summary i e we ignore a section if overlap is less than the threshold value the system performance indicates that even with a fairly simple neural summarizer at the base our architecture is capable of achieving superior results on a blind test dataset experimental settings we used hugging implementation of and bart we experimented with various length settings for training and generation we found that minimum and maximum sequence lengths of and respectively for generation gave us the best results we used adam optimizer with an tial learning rate of with learn rate scheduling based on values calculated on the dation split in the hyper parameter tuning phase a repetition penalty of while generating lay maries provided the most optimal results we used hpzhao s implementation of with default hyper parameter values the topk parameter was dynamically adjusted to set the summary length of each section based on the section specic budget results our system with the team name summaformers ranks rst and third for the and tasks respectively further details on these tasks can be found in the shared tasks overview paper chandrasekaran et al in this section we present detailed results com hpzhao summarunner draco res ibm codalab abstractsummarized introductionsummarized conclusion long summary budget module method baseline base base base small r l recall recall r l recall table laysumm results best results are highlighted in bold method section cutoff at section cutoff at section cutoff at section cutoff at post proc r l recall recall r l recall table longsumm results best results are highlighted in bold lay summary generation we experimented with the bart large cnn model which is pre trained on the cnn dailymail rization dataset and with base in summarization mode we ne tuned the conditional generation architectures of these models using the available laysumm train corpus of documents which we split into training and validation splits in a ratio our initial results proved the superiority of large cnn bartl over we experimented with various generative sources such as abstract only abstract conclusion abstract conclusion introduction abstract conclusion introduction methods furthermore owing to the structure of the abstract itself we considered the rst second and nal paragraphs of the abstract also referred to as small abs as the source our results ble show that using the complete abstract as input to bartl is the best performing setting for laysumm since the papers in the dataset were published in various scientic journals the original abstracts contain highly domain specic technical the bartl model captures the salient points from the abstract in a short word budget while transferring the text style from scientic to a layman style after hyper parameter tuning on the generation end we achieved a score of on the blind test corpus our generated summaries are coherent in addition to being highly abstractive in nature for comparison we also present results for a nave baseline which outputs the rst tokens of the abstract as the summary as shown in table surprisingly this simple line leads to impressive results especially on recall metrics running summarunner on the abstract leads to results which are worse than the baseline long summary generation we used the summarunner nallapati et al neural extractive summarization system as our base section summarizer we pretrained this on the ing set of the publicly available pubmed dataset using glove pennington et al word embeddings to generate the paper abstract as closely as possible from any given section this grounds the network in a setting where it can easily capture salient points we plan to explore ing with other datasets as part of future work this was further netuned using the longsumm train set as follows the given longsumm training dataset was divided into train and validation splits in a ratio we used the same previous settings to netune on documents in the longsumm train split now the pretrained summarunner model was conditioned to extract sentences which mize the overlap with the provided gold standard long summaries finally based on our budget module we assign a weight to each available section and generate section summaries of computed lengths which are further concatenated to generate the nal summary we experiment with various settings in the weight assignment based on specied overlap cutoffs in the budget module as shown in table the best forming setting corresponds to selecting sections whose overlap with the long summary is greater than intuitively this prunes out irrelevant sections such as abbreviations and knowledgements the remaining sections were assigned weights based on the overlap with the provided long summary the generated long summaries are extractive and capture the most salient pieces of information from the given search papers the results improve slightly when we perform post processing using heuristics like removing paper citations within brackets moving non english unicode characters and ematical notation symbols case studies in the following we present two cases of lay maries generated by our system as we can see the generated summaries are highly abstractive and coherent they also capture the important aspects of the paper for the article at this the generated lay summary was as follows this paper proposes a novel approach to support the transformation of bioinformatics data into linked open data lod it denes competency questions that drive not only the denition of transformation rules but also the data transformation and exploration afterwards the paper also presents a support toolset and scribes the successful application of the proposed approach in the functional genomics domain cording to this approach a set of competency teria drive the transformation process this paper presents a framework for the development of an open data management system that can be easily adapted to different data types for the article at this the generated lay j engappai j engappai summary was as follows to foster interaction autonomous robots need to understand the ronment in which they operate one of the main challenges is semantic segmentation together with the recognition of important objects which can aid robots during exploration as well as when planning new actions and interacting with the environment in this study we extend a multi view semantic mentations system based on entangled forests by integrating and rening two object tectors mask r cnn and you only look once yolo with bayesian fusion and iterated graph cuts the new system takes the best of its ponents successfully exploiting both and data finally the following lay summary was ated by our model for this very paper in this paper we develop a novel system for summarizing scientic research papers from multiple domains we differentiate between two types of summaries namely laysumm a very short summary that captures the essence of the research paper in man terms restricting overtly specic technical gon and longsumm a much longer detailed summary aimed at providing specic insights into various ideas touched upon in the paper while leveraging latest transformer based models our systems are simple intuitive and based on how cic paper sections contribute to human summaries of the two types described above conclusions in this paper we studied two scientic document summarization tasks laysumm and longsumm we experimented with popular text neural models in a section aware manner our results indicate that modeling of the document structure with strong focus on which parts of a research paper to attend to while composing a summary gives a signicant boost to the quality of the resultant output on blind test corpora our system ranks rst and third for the longsumm and laysumm tasks respectively references mehdi allahyari seyedamin pouriyeh mehdi asse saeid safaei elizabeth d trippe juan b text rez and krys kochut arxiv preprint tion techniques a brief survey m k chandrasekaran g feigenblat hovy e and a ravichander m shmueli scheuer a de waard overview and insights from scientic document summarization shared tasks cl scisumm laysumm and longsumm in proceedings of the first workshop on scholarly document processing sdp page forthcoming arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and zli goharian a discourse aware attention model for abstractive summarization of long ments arxiv preprint trevor cohn and mirella lapata sentence in proceedings pression beyond word deletion of the international conference on tional linguistics coling pages ed collins isabelle augenstein and sebastian riedel a supervised approach to extractive arxiv preprint marisation of scientic papers jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language ing arxiv preprint shai erera michal shmueli scheuer guy feigenblat ora peled nakash odellia boni haggai roitman doron cohen bar weiner yosi mass or rivlin et al a summarization system for scientic documents arxiv preprint gunes erkan and dragomir r radev lexrank graph based lexical centrality as salience in text summarization journal of articial intelligence search guy lev michal shmueli scheuer jonathan herzig achiya jerbi and david konopnicki summ a dataset and scalable annotation method for scientic paper summarization based on conference talks arxiv preprint mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer bart denoising sequence to sequence pre training for natural language generation translation and comprehension arxiv preprint chin yew lin rouge a package for automatic evaluation of summaries page yang liu fine tune bert for extractive rization arxiv preprint yang liu and mirella lapata text in proceedings of tion with pretrained encoders the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages rada mihalcea and paul tarau textrank ing order into text in proceedings of the ference on empirical methods in natural language processing pages ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based quence model for extractive summarization of ments arxiv preprint ani nenkova and kathleen mckeown a in mining vey of text summarization techniques text data pages springer jeffrey pennington richard socher and christopher d manning glove global vectors for word resentation in proceedings of the conference on empirical methods in natural language ing emnlp pages colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu exploring the limits of transfer learning with a unied text to text former arxiv preprint abhishek kumar singh manish gupta and vasudeva varma hybrid memnet for extractive rization in proceedings of the acm on ence on information and knowledge management pages abhishek kumar singh manish gupta and vasudeva varma unity in diversity learning tributed heterogeneous sentence representation for extractive summarization in aaai sandeep subramanian raymond li jonathan lault and christopher pal on tive and abstractive neural document summarization with transformer language models arxiv preprint bakhtiyar syed gaurav verma balaji vasan vasan anandhavelu natarajan and vasudeva varma adapting language models for non parallel in aaai pages author stylized rewriting oguzhan tas and farzad kiyani a survey matic text summarization pressacademia procedia raghuram vadapalli bakhtiyar syed nishant prabhu balaji vasan srinivasan and vasudeva varma when science journalism meets articial in gence an interactive demonstration ings of the conference on empirical methods in natural language processing system strations pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all in advances in neural information you need cessing systems pages kristian woodsend and mirella lapata ing to simplify sentences with quasi synchronous grammar and integer programming in proceedings of the conference on empirical methods in natural language processing pages wen xiao and giuseppe carenini tive summarization of long documents by arxiv preprint bining global and local context xingxing zhang furu wei and ming zhou hibert document level pre training of cal bidirectional transformers for document in proceedings of the annual rization ing of the association for computational linguistics pages
