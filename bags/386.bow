mapgn masked pointer generator network sequence sequence pre training mana ihori naoki makishima tomohiro tanaka akihiko takashima shota orihashi ryo masumura ntt media intelligence laboratories ntt corporation japan abstract paper presents self supervised learning method generator networks improve spoken text normalization text normalization converts spoken style text style malized text important technology improving subsequent processing machine translation tion successful spoken text normalization method date sequence sequence mapping pointer generator networks possess copy mechanism input sequence models require large paired data spoken style text style normalized text difcult prepare volume data order construct spoken text normalization model limited paired data focus self supervised learning utilize unpaired text data prove models unfortunately conventional self supervised learning methods assume pointer generator networks utilized propose novel self supervised ing method masked pointer generator network mapgn proposed method effectively pre train pointer generator work learning masked tokens copy mechanism experiments demonstrate mapgn effective pointer generator networks conventional self supervised learning methods spoken text normalization tasks index terms sequence sequence pre training generator networks self supervised learning spoken text ization introduction rise automatic speech recognition asr cations smart speakers automatic dictation tems increasingly important accurately cess spoken style text transcribed text spoken ances spoken style text includes disuencies dant expressions minority spoken expressions alects asr systems convert speech text literal manner spoken style text adversely affects subsequent natural guage processing machine translation summarization technologies developed handle written style text text majority expressions disuencies dundant expressions required convert spoken style text including disuencies dialects style normalized text excludes disuencies dialects paper aim improve spoken text normalization spoken text normalization tasks considered monolingual translation regarded sequence sequence mapping text text recent studies fully neural based models shown effective performance ous monolingual translation tasks summarization phrase generation disuency detection ular models based pointer generator networks utilized recently pointer generator networks tive monolingual translation tasks contain copy mechanism copies tokens source text help generate infrequent tokens pointer generator networks reportedly performed attention based encoder decoder networks spoken text normalization task construct models spoken text normalization large paired data spoken style text style ized text necessary paired data need prepare manual transcriptions spoken utterances text style transcriptions needs normalized manually costly time consuming large paired data mitigate problem use self supervised ing gaining lot attention recent years supervised learning form unsupervised learning paired data employed designing supervised training tings natural language processing self supervised learning improving natural language generation natural language understanding unfortunately conventional self supervised learning methods models assume generator networks utilized practice tional methods insufcient pointer generator networks cause learn copy tokens source text explicitly paper propose novel self supervised learning method pointer generator networks proposed method masked pointer generator network mapgn extension masked sequence sequence pre training mass mass pre trains model predicting masked tokens taking masked sequence input contract mapgn pre train copy mechanism efciently learning choose copy generate tokens masked tokens experiments demonstrate proposed method effective pointer generator networks paired data spoken text normalization tasks dialect conversion task spoken written style conversion task pointer generator networks section denes spoken style normalization pointer generator networks dene spoken style text style normalized text tokens spoken style style normalized text respectively pointer generator network predicts generation probabilities written style text given spoken style text tion probability dened enc dec represents model parameter sets enc dec trainable parameter sets encoder decoder spectively computed encoder fig token span masking method text unpaired data model tuned paired data subsequent task paper use token span masking common mass corrupt original text basic self supervised learning strategy propose self supervised learning method pointer generator networks devising masking method token span masking basic self supervised learning strategy use token span masking mass basic self supervised learning strategy method given unpaired sentence fragment position masked number tokens sentence number tokens masked position length roughly position selected rst token token indicates sentence fragment addition selected masked token replaced mask token random token left unchanged describe details replacing method masked token section model pre trained predicting sentence fragment taking masked sequence input shown fig model parameter set optimized unpaired data loss function pre train model parameter set dened log fig network structure pointer generator network decoder copy mechanism fig shows network structure pointer generator network encoder converts input sequence hidden resentations hidden representations produced arbitrary network bidirectional rent neural networks rnns transformer encoder decoder computes copying tokens pointing ating tokens xed vocabulary based copy mechanism compute generation probabilities decoder converts tokens rst token token hidden vector hidden vector produced arbitrary network unidirectional rnns transformer decoder compute attention distribution function computes attention distribution tion distribution produces weighted sum encoder states generation probabilities token produced pgen log dec dec pgen dec dec tanh softmax sigmoid linear mational functions tanh softmax sigmoid activation copy mechanism enables switching probability pgen choose copy generate tokens pgen computes weighted sum generator distribution attention tribution produces prediction probability token model parameter set optimized paired data loss function optimize model parameter set dened log self supervised learning pointer generator networks section details self supervised learning pointer generator networks denoising auto encoder task widely self supervised learning task model learns reconstruct original text given corrupted original self supervised learning encoder encouraged derstand meaning unmasked tokens furthermore decoder encouraged extract useful information encoder masking decoder input tokens masked encoder decoder input tokens masked assumed decoder uses abundant information ceding tokens information encoder dition decoder learn effective language modeling predicting consecutive tokens decoder dicting discrete tokens masking methods propose self supervised learning method pointer generator networks token span masking selected masked token replaced mask token random token left unchanged vary percentage tokens replacing method develop suitable masking method pointer generator networks section describes conventional method mass methods varies percentage replaced tokens mass proposed method mapgn table summarizes masking method mass mass masked tokens encoder replaced mask tokens replaced random tokens unchanged random tokens introduced behalf table summary masking methods mask random unchanged mapgn select random tokens tokens tokens masking span mask token considering mask token pear tuning tokens randomly selected vocabulary paper masking method referred prepare methods varies age replaced tokens masked tokens encoder replaced mask kens replaced random tokens unchanged masked tokens encoder replaced mask tokens unchanged mapgn masked pointer generator network mapgn extension mass mapgn masked tokens encoder replaced mask tokens replaced random tokens unchanged key advance mapgn random tokens selected tokens tokens masking span example fig random tokens randomly selected reason use masking method detailed section key idea presume suitable pointer generator work pre training percentage unchanged tokens masking span encoder small model learn copy tokens explicitly assume pointer generator networks learn copy appropriate tokens input increasing percentage unchanged tokens assumed suitable pointer generator network unchanged tokens increased possibility copy mechanism overtting mapgn utilizes random tokens learn choose copy generate tokens effectively comparing mapgn verify increasing unchanged tokens pre train copy mechanism addition mapgn selects random tokens masking span words random tokens include tokens output sequence tokens information appropriate positions tokens aid copying tokens conceal position tokens copied role random tokens mapgn mask token encourage determine copy comparing mapgn validate selecting random tokens masking span experiments section describes experimental details pre training tuning spoken text normalization tasks particular chose dialect conversion spoken written style conversion tasks japanese dialect conversion task japanese dialect converted standard japanese spoken written style conversion task spoken style text produced automatic speech recognition system converted written style text correct punctuations disuencies datasets pre training prepared large scale japanese web text unpaired written style text data web text downloaded topic web pages home crawler loaded pages ltered way html tags javascript codes parts useful tasks cluded finally prepare million sentences pre training fine tuning dialect conversion task prepared paired data japanese dialect tohoku ben standard japanese crowd sourcing divided data training set validation set test set training set contained sentences divided training set sentences investigate difference performance different training data validation set sentences test set sentences fine tuning spoken written style conversion task parallel corpus japanese spoken written style sion cjsw cjsw domains data domain center dialogue training compare domain task domain ood task training set sentences divided increments sentences reason dialect conversion task validation set center dialogue containing sentences training domain test sets center dialogue test set task test sets ood task test sets divided accordance datasets paired data spoken style text manual transcriptions speech written style text created crowd sourcing setups pre trained attention based encoder decoder networks pointer generator networks pre training methods following congurations pre trained sional word embeddings continuous bag words encoder layer bidirectional long short term memory rnn lstm rnn units introduced decoder layer unidirectional lstm rnn units introduced additive attention mechanism output unit size corresponded number tokens training set word embeddings set train networks adam optimizer label smoothing smoothing rameter set mini batch size sentences dropout rate lstm rnn mini batch training truncated sentence tokens trainable parameters randomly initialized characters tokens pre trained networks tasks common tuning attention based encoder decoder work pointer generator network transferred trained model parameter sets constructed networks pre training baseline model congurations pre training model trainable parameters randomly initialized evaluation lated automatic evaluation scores metrics rouge meteor results table table experimental results dialect version task spoken written style conversion task tively fig shows score training data spoken written style conversion task table shows mapgn pointer generator networks outperformed masking methods evaluation metrics best mance encoder decoder networks performance pointer generator networks results sentences training formance mapgn improved sentences table results dialect conversion task sentences training data masking method baseline mapgn sentences sentences rouge meteor rouge meteor attention based encoder decoder network pointer generator network table results spoken written style conversion task sentences training data masking method baseline sentences rouge attention based encoder decoder network ood mapgn meteor ood pointer generator network ood ood sentences rouge ood meteor ood table shows mapgn pointer generator networks formed masking methods metrics fig shows encoder decoder networks masking methods improved line performance signicantly best formance training data pointer generator works mapgn yield best performance performance improved signicantly ood task task ood tasks performance difference masking method decreased paired training data creased results spoken text normalization tasks largely followed trend results encoder decoder networks outperformed methods words pre training method learned actively output tokens input sequence effective assumed method actively copy tokens effective spoken text normalization tasks encoder decoder networks table point pre training pointer generator networks results masking method performed differently indicates masking method important pointer generator network pre training example performance effective mapgn infer pointer generator network simply pre trains copy tokens actively copy mechanism learns copy tokens need copied assume networks pre trained copy effectively selecting random tokens tokens masking span tokens model finally results pointer generator networks mapgn outperformed masking methods task pointer generator networks suitable style normalization task baseline performance improved paired training data increases effectiveness pre training pointer generator networks decreases hand ood task pre training effective signicantly paired training data increases mapgn effective pre training method pointer generator networks paired training data small ood tasks fig score spoken written style conversion task conclusion paper proposed masked pointer generator network mapgn self supervised learning method pointer generator networks conventional self supervised learning methods port explicitly train copy mechanism pointer generator networks proposed method train copy mechanism ciently learning choose copy generate tokens masking span experiments demonstrated mapgn outperformed conventional methods spoken text malization tasks especially effective paired training data small ood tasks concluded mapgn suitable pre training pointer generator networks effective paired data set limited references tara sainath arun narayanan joe caroselli michiel bacchiani ananya misra izhak shafran hasim sak golan pundak kean chin khe chai sim ron weiss kevin wilson ehsan variani chanwoo kim olivier siohan mitchel weintraub erik mcdermott rick rose matt shannon acoustic modeling google home proc annual conference international speech communication ciation interspeech amanda purington jessie taft shruti sannon natalya natalie bazarova samuel taylor alexa new bff social roles user satisfaction personication amazon echo proc conference extended abstracts human factors computing tems chi guokan shang wensi ding zekun zhang antoine jean pierre tixier polykarpos meladianos michalis vazirgiannis jean pierre lorre unsupervised abstractive meeting summarization multi sentence compression budgeted submodular maximization proc annual meeting association computational linguistics acl manling lingyu zhang heng richard radke ing summaries topic abstractive multi modal meeting tion proc annual meeting association computational linguistics acl sander wubben antal van den bosch emiel krahmer phrase generation monolingual translation data evaluation proc international natural language generation conference inlg ilya sutskever oriol vinyals quoc sequence sequence learning neural networks proc international conference neural information processing systems nips romain paulus caiming xiong richard socher deep forced model abstractive summarization proc international conference learning representations iclr yen chun chen mohit bansal fast abstractive summarization reinforce selected sentence rewriting proc association computational linguistics acl aaditya prakash sadid hasan kathy lee vivek datla ashequl qadir joey liu oladimeji farri neural paraphrase generation stacked residual lstm networks proc international conference computational linguistics coling shuming sun wei sujian wenjie xuancheng ren query output generating words querying distributed word representations paraphrase generation proc conference north american chapter association computational linguistics human language technologies naacl hlt shaolei wang wanxiang che ting liu neural attention model disuency detection proc international conference putational linguistics coling qianqian dong feng wang zhen yang wei chen shuang adapting translation models transcript disuency tion proc thirty aaai conference articial intelligence aaai abigail peter liu christopher manning point summarization pointer generator networks proc nual meeting association computational linguistic acl zhengyuan liu angela sheldon lee nancy chen topic aware pointer generator networks summarizing spoken versations proc ieee automatic speech recognition standing workshop asru wenjun zhao meina song haihong summarization highway condition radom pointer generator network proc tional conference algorithms computing articial intelligence acai mana ihori akihiko takashima ryo masumura large context pointer generator networks spoken written style conversion proc international conference acoustics speech signal cessing icassp jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language derstanding arxiv preprint matthew peters mark neumann mohit iyyer matt gardner pher clark kenton lee luke zettlemoyer deep contextualized word representations proceedings conference north american chapter association computational tics human language technologies volume long papers alec radford jeffrey rewon child david luan dario amodei language models unsupervised multitask ilya sutskever learners openai blog kaitao song tan tao qin jianfeng tie yan liu mass masked sequence sequence pre training language generation proc international conference machine learning icml yinhan liu jiatao naman goyal xian sergey edunov jan ghazvininejad mike lewis luke zettlemoyer multilingual denoising pre training neural machine translation arxiv preprint liang wang wei zhao ruoyu jia sujian jingming liu noising based sequence sequence pre training text generation proc conference empirical methods natural language cessing international joint conference natural guage processing emnlp ijcnlp mike schuster kuldip paliwal bidirectional recurrent neural networks ieee transactions signal processing ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need proc advances neural information processing systems nips sepp hochreiter jurgen schmidhuber long short term memory neural computation mana ihori akihiko takashima ryo masumura parallel japanese spoken written style conversion proc language resources evaluation conference lrec thang luong hieu pham christopher manning effective approaches attention based neural machine translation proc conference empirical methods natural language processing emnlp tomas mikolov kai chen greg corrado jeffrey dean efcient estimation word representations vector space proc shop international conference learning representations iclr dzmitry bahdanau kyunghyun cho yoshua bengio neural chine translation jointly learning align translate proc international conference learning representations iclr kishore papineni salim roukos todd ward wei jing zhu bleu method automatic evaluation machine translation proc annual meeting association computational linguistics acl chin yew lin franz josef och automatic evaluation machine translation quality longest common subsequence skip bigram statistics proc annual meeting association computational linguistics acl satanjeev banerjee alon lavie meteor automatic metric evaluation improved correlation human judgments proc acl workshop intrinsic extrinsic evaluation sures machine translation summarization
