mapgn masked pointer generator network for sequence to sequence pre training mana ihori naoki makishima tomohiro tanaka akihiko takashima shota orihashi ryo masumura ntt media intelligence laboratories ntt corporation japan e f l c s c v v i x r a abstract this paper presents a self supervised learning method for generator networks to improve spoken text normalization text normalization that converts spoken style text into style malized text is becoming an important technology for improving subsequent processing such as machine translation and tion the most successful spoken text normalization method to date is sequence to sequence mapping using pointer generator networks that possess a copy mechanism from an input sequence however these models require a large amount of paired data of spoken style text and style normalized text and it is difcult to prepare such a volume of data in order to construct spoken text normalization model from the limited paired data we focus on self supervised learning which can utilize unpaired text data to prove models unfortunately conventional self supervised learning methods do not assume that pointer generator networks are utilized therefore we propose a novel self supervised ing method masked pointer generator network mapgn the proposed method can effectively pre train the pointer generator work by learning to ll masked tokens using the copy mechanism our experiments demonstrate that mapgn is more effective for pointer generator networks than the conventional self supervised learning methods in two spoken text normalization tasks index terms sequence to sequence pre training generator networks self supervised learning spoken text ization introduction with the rise of various automatic speech recognition asr cations such as smart speakers and automatic dictation tems it has become increasingly important to accurately cess spoken style text i e the transcribed text from spoken ances spoken style text often includes disuencies such as dant expressions and various minority spoken expressions e alects because asr systems convert speech into text in a literal manner spoken style text adversely affects subsequent natural guage processing e machine translation summarization because these technologies are often developed to handle written style text which is text with majority expressions and no disuencies or dundant expressions thus it is required to convert spoken style text including disuencies and dialects into style normalized text which excludes disuencies and dialects in this paper we aim to improve spoken text normalization spoken text normalization tasks are considered as monolingual translation that is regarded as sequence to sequence mapping from text to text in recent studies fully neural based models have shown effective performance in ous monolingual translation tasks such as summarization phrase generation and disuency detection in ular models based on pointer generator networks have been utilized recently pointer generator networks are tive for monolingual translation tasks because they contain a copy mechanism that copies tokens from a source text to help generate infrequent tokens pointer generator networks have reportedly performed attention based encoder decoder networks in spoken text normalization task to construct models for spoken text normalization a large amount of paired data of spoken style text and style ized text are necessary however to make these paired data we need to prepare manual transcriptions of spoken utterances and the text style of these transcriptions needs to be normalized manually thus it is costly and time consuming to make a large amount of paired data to mitigate this problem we use self supervised ing which has been gaining a lot of attention in recent years supervised learning is one form of unsupervised learning where paired data is only employed for designing supervised training tings in natural language processing self supervised learning has been improving in natural language generation and natural language understanding unfortunately conventional self supervised learning methods for models do not assume that generator networks are utilized in practice the tional methods are insufcient for pointer generator networks cause they can not learn to copy tokens from a source text explicitly in this paper we propose a novel self supervised learning method for pointer generator networks the proposed method masked pointer generator network mapgn is an extension of masked sequence to sequence pre training mass mass pre trains a model by predicting the masked tokens taking the masked sequence as input in contract mapgn can pre train a copy mechanism efciently by learning to choose whether to copy or generate tokens with masked tokens our experiments demonstrate that the proposed method is effective for pointer generator networks with less paired data in two spoken text normalization tasks dialect conversion task and spoken to written style conversion task pointer generator networks this section denes spoken style normalization with pointer generator networks we dene spoken style text as x xm and style normalized text as y yn where xm and yn are tokens in the spoken style and style normalized text respectively the pointer generator network predicts generation probabilities of a written style text y given a spoken style text x the tion probability of y is dened as p y p x n where enc dec represents model parameter sets enc and dec are trainable parameter sets with an encoder and a decoder spectively p x can be computed with the encoder fig token span masking method text by using unpaired data the model can be ne tuned with paired data for each subsequent task in this paper we use token span masking which is common with mass to corrupt the original text for the basic self supervised learning strategy we propose a self supervised learning method for pointer generator networks by devising a masking method for the token span masking basic self supervised learning strategy we use token span masking in mass as a basic self supervised learning strategy in this method given unpaired sentence y we get y a where its fragment from position a to b are masked here a n and n is the number of tokens of sentence y the number of tokens that are masked from position a to is and the length k is roughly of n position a is selected from between the rst token and the n token ya indicates the sentence fragment of y from a to b in addition each of the selected masked token is either replaced with a mask token a random token or left unchanged we describe details of this replacing method for the masked token in section the model is pre trained by predicting the sentence fragment ya taking the masked sequence y a as input as shown in fig the model parameter set can be optimized from unpaired data du y y a loss function to pre train the model parameter set is dened as l log p yd a y d fig the network structure of a pointer generator network and the decoder with a copy mechanism fig shows the network structure of the pointer generator network the encoder converts an input sequence x into the hidden resentations h hm these hidden representations are produced by an arbitrary network such as bidirectional rent neural networks rnns or a transformer encoder the decoder computes both copying tokens via pointing and ating tokens from a xed vocabulary based on a copy mechanism to compute the generation probabilities first the decoder converts tokens from the rst token to the n token into hidden vector vn the hidden vector is produced by an arbitrary network such as unidirectional rnns or a transformer decoder next we compute attention distribution m n from the function that computes the attention distribution with h and vn the tion distribution produces a weighted sum of the encoder states dn generation probabilities for the n token are produced with dn and vn by p x pgen m n m xm t a log p yd t y d t vn dec dec pgen t vn dec dec t t t t where tanh softmax and sigmoid are linear mational functions with a tanh softmax and sigmoid activation the copy mechanism enables switching probability pgen to choose whether to copy or generate tokens thus pgen computes the weighted sum with generator distribution and attention tribution and produces the prediction probability of the n token the model parameter set can be optimized from paired data dp x y x y a loss function to optimize the model parameter set is dened as l n log p yd x d self supervised learning for pointer generator networks this section details self supervised learning for pointer generator networks the denoising auto encoder task is widely used for self supervised learning in this task the model learns to reconstruct the original text given the corrupted original in this self supervised learning the encoder is encouraged to derstand the meaning of unmasked tokens furthermore the decoder is encouraged to extract more useful information from the encoder side by masking the decoder input tokens which are not masked in the encoder if the decoder input tokens are not masked at all it is assumed that the decoder uses abundant information from the ceding tokens rather than information from the encoder side in dition the decoder can learn more effective language modeling by predicting consecutive tokens in the decoder side rather than dicting discrete tokens masking methods we propose a self supervised learning method for pointer generator networks in token span masking each of the selected masked token is either replaced with a mask token a random token or left unchanged we vary the percentage of these tokens and the replacing method to develop a suitable masking method for pointer generator networks this section describes the conventional method mass two methods that varies the percentage of these replaced tokens in mass and our proposed method mapgn table summarizes each masking method mass in mass of the masked tokens in the encoder are replaced by mask tokens are replaced by random tokens and are unchanged the random tokens are introduced on behalf table summary of masking methods mask random unchanged mapgn select random tokens from all tokens all tokens masking span of the mask token considering that the mask token does not pear during ne tuning these tokens are randomly selected from the vocabulary in this paper this masking method is referred to as moreover we prepare two methods that varies the age of these replaced tokens and in of the masked tokens in the encoder are replaced by mask kens are replaced by random tokens and are unchanged in of the masked tokens in the encoder are replaced by mask tokens and are unchanged mapgn our masked pointer generator network mapgn is an extension of mass in mapgn of the masked tokens in the encoder are replaced by mask tokens are replaced by random tokens and are unchanged key advance of mapgn is that the random tokens are not selected from all tokens but from tokens in the masking span for example in fig the random tokens are randomly selected from a reason why we use this masking method is detailed in section key idea we presume that is not suitable for pointer generator work pre training because the percentage of unchanged tokens in the masking span of the encoder is small and the model can not learn to copy tokens explicitly we assume that pointer generator networks can learn to copy appropriate tokens from the input by increasing the percentage of unchanged tokens thus it is assumed that is suitable for pointer generator network however if only unchanged tokens are increased there is a possibility that the copy mechanism will be overtting thus mapgn utilizes random tokens to learn to choose to copy or generate tokens effectively by comparing with mapgn we can verify that increasing unchanged tokens is not enough to pre train the copy mechanism in addition mapgn selects random tokens from masking span in other words although random tokens include tokens in the output sequence these tokens do not have information on appropriate positions thus these tokens are used to aid for copying tokens and conceal the position of tokens that should be copied the role of the random tokens in mapgn is not only to mask the token but also encourage to determine whether to copy by comparing with mapgn we can validate the selecting of random tokens from the masking span experiments this section describes the experimental details of pre training and ne tuning on spoken text normalization tasks in particular we chose dialect conversion and spoken to written style conversion tasks in japanese in the dialect conversion task japanese dialect is converted into standard japanese in the spoken to written style conversion task spoken style text produced by an automatic speech recognition system is converted into written style text with correct punctuations and no disuencies datasets pre training we prepared a large scale japanese web text as the unpaired written style text data the web text was downloaded from various topic web pages using our home made crawler the loaded pages were ltered in such a way that html tags javascript codes and other parts that were not useful for these tasks were cluded finally we prepare one million sentences for pre training fine tuning on dialect conversion task we prepared paired data of a japanese dialect tohoku ben and standard japanese using crowd sourcing we divided the data into a training set validation set and test set the training set contained sentences and we divided the training set into and sentences to investigate the difference of performance with different amount of training data the validation set have sentences and the test set have sentences fine tuning on spoken to written style conversion task we used the parallel corpus for japanese spoken to written style sion cjsw although the cjsw has four domains we only used the data from one domain call center dialogue for training to compare the in domain id task and the out of domain ood task the training set has sentences which we divided into increments of between and sentences for the same reason as the dialect conversion task a validation set of call center dialogue containing sentences was used for training we used all domain test sets the call center dialogue test set for the id task and all other test sets for the ood task the test sets were divided in accordance with the datasets were paired data of spoken style text manual transcriptions of speech and written style text created by crowd sourcing setups we pre trained attention based encoder decoder networks and pointer generator networks with four pre training methods we used the following congurations we used pre trained sional word embeddings using continuous bag of words in the encoder a layer bidirectional long short term memory rnn lstm rnn with units was introduced in the decoder a layer unidirectional lstm rnn with units was introduced we used an additive attention mechanism the output unit size which corresponded to the number of tokens in the training set of word embeddings was set to to train these networks we used the adam optimizer and label smoothing with a smoothing rameter of we set the mini batch size to sentences and the dropout rate in each lstm rnn to for the mini batch training we truncated each sentence to tokens all trainable parameters were randomly initialized we used characters as tokens and these pre trained networks for two tasks in common in ne tuning we used the attention based encoder decoder work and the pointer generator network which are transferred trained model parameter sets we constructed these two networks without pre training as the baseline these model congurations were the same as that of the pre training model and all trainable parameters were randomly initialized in the evaluation we lated automatic evaluation scores in three metrics rouge l and meteor results table and table show the experimental results of the dialect version task and the spoken to written style conversion task tively fig shows the score with all training data in spoken to written style conversion task table shows that mapgn for pointer generator networks outperformed other masking methods in all evaluation metrics although was the best mance in encoder decoder networks it was the least performance in pointer generator networks the results were the same whether or sentences were used as training however the formance of mapgn improved when sentences were used table results of dialect conversion task with and sentences of training data masking method baseline mapgn sentences sentences rouge l meteor rouge l meteor attention based encoder decoder network pointer generator network table results of spoken to written style conversion task with and sentences of training data masking method baseline sentences rouge l id attention based encoder decoder network id ood mapgn meteor id ood pointer generator network ood id ood sentences rouge l id ood meteor id ood table shows that mapgn for pointer generator networks formed other masking methods in all metrics fig shows that in encoder decoder networks all masking methods improved the line performance signicantly moreover was the best formance for any amount of training data in pointer generator works mapgn yield the best performance and the performance was improved more signicantly in the ood task than in the id task in both id and ood tasks the performance difference between each masking method decreased as the amount of paired training data creased the results of the two spoken text normalization tasks largely followed the same trend the results of encoder decoder networks show that outperformed all other methods in other words the pre training method which learned to actively output the same tokens in the input sequence was more effective it is assumed that the method to actively copy tokens is effective for spoken text normalization tasks using encoder decoder networks next a table point of pre training in pointer generator networks results is that each masking method performed differently this indicates that the masking method is important for pointer generator network pre training for example was the least performance and was less effective than mapgn we can infer that even if the pointer generator network simply pre trains to copy tokens actively the copy mechanism learns to copy tokens that do not need to be copied moreover we assume that the networks can be pre trained to copy or not effectively by selecting random tokens from tokens in the masking span rather than from all tokens in the model finally the results of the pointer generator networks show that mapgn outperformed other masking methods however in the id task since pointer generator networks are suitable for style normalization task and the baseline performance is improved as the amount of paired training data increases the effectiveness of pre training for pointer generator networks decreases on the other hand in the ood task pre training is effective signicantly even if the amount of paired training data increases thus mapgn is an effective pre training method for pointer generator networks if the amount of paired training data is small or in ood tasks fig score of spoken to written style conversion task conclusion this paper proposed masked pointer generator network mapgn a self supervised learning method for pointer generator networks while conventional self supervised learning methods do not port to explicitly train a copy mechanism in the pointer generator networks the proposed method can train the copy mechanism ciently by learning to choose whether to copy or generate tokens against masking span experiments demonstrated that mapgn outperformed the conventional methods in two spoken text malization tasks and was especially effective if the amount of paired training data is small and in ood tasks we concluded that mapgn is suitable for pre training pointer generator networks and effective when paired data set is limited references bo li tara sainath arun narayanan joe caroselli michiel bacchiani ananya misra izhak shafran hasim sak golan pundak kean chin khe chai sim ron j weiss kevin wilson ehsan variani chanwoo kim olivier siohan mitchel weintraub erik mcdermott rick rose and matt shannon acoustic modeling for google home in proc annual conference of the international speech communication ciation interspeech amanda purington jessie taft shruti sannon natalya natalie bazarova and samuel taylor alexa is my new bff social roles user satisfaction and personication of the amazon echo in proc conference extended abstracts on human factors in computing tems chi pp guokan shang wensi ding zekun zhang antoine jean pierre tixier polykarpos meladianos michalis vazirgiannis and jean pierre lorre unsupervised abstractive meeting summarization with multi sentence compression and budgeted submodular maximization in proc annual meeting of the association for computational linguistics acl manling li lingyu zhang heng ji and richard j radke keep ing summaries on topic abstractive multi modal meeting tion in proc annual meeting of the association for computational linguistics acl pp sander wubben antal van den bosch and emiel krahmer phrase generation as monolingual translation data and evaluation in proc international natural language generation conference inlg pp ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural networks in proc international conference on neural information processing systems nips pp romain paulus caiming xiong and richard socher a deep forced model for abstractive summarization in proc international conference on learning representations iclr yen chun chen and mohit bansal fast abstractive summarization with reinforce selected sentence rewriting in proc association for computational linguistics acl pp aaditya prakash sadid a hasan kathy lee vivek datla ashequl qadir joey liu and oladimeji farri neural paraphrase generation with stacked residual lstm networks in proc international conference on computational linguistics coling pp shuming ma xu sun wei li sujian li wenjie li and xuancheng ren query and output generating words by querying distributed word representations for paraphrase generation in proc conference of the north american chapter of the association for computational linguistics human language technologies naacl hlt pp shaolei wang wanxiang che and ting liu a neural attention model for disuency detection in proc international conference on putational linguistics coling pp qianqian dong feng wang zhen yang wei chen shuang xu and bo xu adapting translation models for transcript disuency tion in proc thirty third aaai conference on articial intelligence aaai pp abigail see peter j liu and christopher d manning get to the point summarization with pointer generator networks in proc nual meeting of the association for computational linguistic acl pp zhengyuan liu angela ng sheldon lee ai ti aw and nancy f chen topic aware pointer generator networks for summarizing spoken versations in proc ieee automatic speech recognition and standing workshop asru wenjun zhao meina song and e haihong summarization with highway condition radom pointer generator network in proc tional conference on algorithms computing and articial intelligence acai pp mana ihori akihiko takashima and ryo masumura large context pointer generator networks for spoken to written style conversion in proc international conference on acoustics speech and signal cessing icassp pp jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language derstanding arxiv preprint matthew peters mark neumann mohit iyyer matt gardner pher clark kenton lee and luke zettlemoyer deep contextualized word representations in proceedings of the conference of the north american chapter of the association for computational tics human language technologies volume long papers pp alec radford jeffrey wu rewon child david luan dario amodei language models are unsupervised multitask and ilya sutskever learners openai blog p kaitao song xu tan tao qin jianfeng lu and tie yan liu mass masked sequence to sequence pre training for language generation in proc international conference on machine learning icml pp yinhan liu jiatao gu naman goyal xian li sergey edunov jan ghazvininejad mike lewis and luke zettlemoyer multilingual denoising pre training for neural machine translation arxiv preprint liang wang wei zhao ruoyu jia sujian li and jingming liu noising based sequence to sequence pre training for text generation in proc conference on empirical methods in natural language cessing and the international joint conference on natural guage processing emnlp ijcnlp pp mike schuster and kuldip k paliwal bidirectional recurrent neural networks ieee transactions on signal processing pp ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in proc advances in neural information processing systems nips pp sepp hochreiter and jurgen schmidhuber long short term memory neural computation pp mana ihori akihiko takashima and ryo masumura parallel for japanese spoken to written style conversion in proc language resources and evaluation conference lrec pp thang luong hieu pham and christopher d manning effective approaches to attention based neural machine translation in proc conference on empirical methods in natural language processing emnlp pp tomas mikolov kai chen greg corrado and jeffrey dean efcient estimation of word representations in vector space in proc shop at international conference on learning representations iclr dzmitry bahdanau kyunghyun cho and yoshua bengio neural chine translation by jointly learning to align and translate in proc international conference on learning representations iclr pp kishore papineni salim roukos todd ward and wei jing zhu bleu a method for automatic evaluation of machine translation in proc annual meeting on association for computational linguistics acl pp chin yew lin and franz josef och automatic evaluation of machine translation quality using longest common subsequence and skip bigram statistics in proc annual meeting on association for computational linguistics acl pp satanjeev banerjee and alon lavie meteor an automatic metric for mt evaluation with improved correlation with human judgments in proc the acl workshop on intrinsic and extrinsic evaluation sures for machine translation summarization pp
