c e d l c s c v v x r cross lingual approach abstractive summarization ales zagar marko sikonja university ljubljana faculty computer information science vecna pot ljubljana slovenia ales zagar marko uni lj abstract automatic text summarization extracts important information texts presents information form summary abstractive marization approaches progressed signicantly switching deep neural networks results satisfactory especially languages large training sets exist natural language processing tasks cross lingual model transfers successfully applied low resource guages summarization cross lingual model transfer far attempted non reusable decoder neural models work pretrained english summarization model based deep neural networks sequence sequence architecture summarize slovene news articles solved problem inadequate decoder additional language model target language evaluation developed models different proportions target language data ne tuning results assessed automatic evaluation measures small scale man evaluation results summaries cross lingual models ne tuned relatively small target language data useful similar quality abstractive summarizer trained data target language introduction summarization process extracting collecting important information texts presenting information form summary according output process broadly divided extractive abstractive summarization extractive approach non productive sense copies important sentences resulting summary include new words sentences abstractive approach creative produces summaries rephrase given content compressed sentences contain originally unused words abstractive neural summarization approaches use similar deep learning chitectures machine translation face additional problems input usually longer output short compared input compression lossy current abstractive summarization outputs repetitive absurd misrepresent facts deal poorly vocabulary words perform poorly content selection misleading ways produce good outputs summarization tools exist resource rich languages cross lingual embeddings present promising approach low resource languages enable model transfer resource rich resource poor languages adams et al artetxe schwenk typically training model resource rich language monolingual embeddings source guage applying resourced language input beddings target language mapped source language embeddings unfortunately standard procedure work cross lingual rization model trained output sentences grammar source language blindly applying procedure summarization model trained glish produce sentences english grammar target language e slovene proposed solution use pretrained english summarization model proposed chen bansal use english source language slovene target language cross lingual embeddings map slovene word embeddings english word vector space zero shot transfer ing expected satisfactory ne tune resulting model cross lingual models trained increasingly large portions able target language dataset post processing stage generated potheses selected best evaluation metrics including base neural language model target language summary approach direct cross lingual summarization model automatic metrics small scale human evaluation par summarization model trained scratch target language paper split ve sections section present related work section describe slovene datasets ne tune marization model build language model section outlines proposed cross lingual summarization model gives detail components report results section present conclusions ideas work section related work early summarization approaches use extractive approach suitable multi document summarization gambhir gupta lately deep neural networks learning sequence sequence transformations produced state art abstractive summaries rush et al nallapati et al encode source document internal numeric representation decode abstractive summary models work best short single document summaries e headline generation news summarization use attention mechanism ensures decoder focuses appropriate input words bahdanau et al additionally frequently use copy mechanism copies relevant words input present dictionary et al coverage mechanism avoids redundant contents tu et al cross lingual approach based monolingual model proposed chen bansal hybrid summarization model rst selects salient sentences paraphrases model comprised independently trained neural networks bridged policy based reinforcement learning use slovene target language report work summarization language recently zidarn built rst abstractive summarizer slovene language architecture deep neural networks best results produced layer lstm attention mechanism copy mechanism beam search author dataset approximately news showed large achieve results comparable english english languages abstractive marizers straka et al presented sumeczech large news summarization dataset czech million samples summarization compared vised methods textrank mihalcea returning rst sentences supervised methods logistic regression random forests hand crafted features fecht et al encoder decoder architecture german wikipedia articles samples summary rst section article subsequent text represents document hu et al created chinese summarization dataset million samples chinese ging website sina weibo recurrent neural network abstractive marization existing approaches cross lingual summarization combine rization translation steps zhu et al different translation schemes proposed early translation scheme rst translates original document target language generates summary late scheme rst ates summary translates target language general machine translation systems generate cross lingual dataset ouyang et al constructed summarization corpora new york times corpus haus low resource languages implemented neural network architecture proposed et al zhu et al corpus sina weibo hu et al cnn dailymail msmo zhu et al approach uses neural networks trains machine translation summarization end end manner chi et al outperformed machine translation based approaches training model monolingual cross lingual settings pre training procedure monolingual data ne tune model downstream nlg tasks idea word embeddings learn high dimensional vectors ture meaning words popular variants mikolov et al glove pennington et al fasttext grave et al elmo peters et al bert devlin et al important insight work relations words embedded space preserved languages mikolov et al cross lingual embeddings align monolingual embeddings common vector space ruder et al beginning niques required parallel corpora bilingual dictionary provide necessary information mapping word source target language recent approach train cross lingual embeddings unsupervised manner conneau et al major drawback classical word embeddings deal polysemy recent contextual embeddings bert devlin et al elmo peters et al learn polysemous representations words datasets shortly describe creation datasets summarization task language modeling datasets extracted gigada corpus written standard slovene consisting newspapers magazines web texts containing documents billion words summarization dataset contains news summaries rst paragraph sta slovenian press agency news web texts taken summary rest text news gigada corpus extracted sta news sentence segmented paragraph segmented designed heuristic extract rst paragraph started training samples kept texts characters texts discarded contained weather reports lists events world long total samples remained split train test validation set test validation set contain instances training set contains remaining news establish machine translation baseline google machine lation service rst translated test set slovene english generated english summaries pretrained monolingual english summarizer translated generated english summaries slovene language summarization model start trained english model cross lingual word mapping produce correct texts target guage grammar decoder remains source language train language model improvements intended cross lingual transfer purpose trained character level slovene language model following ndings bojanowski et al language models morphologically rich guages slovene improved character level information training set gigada tokenized sentence segmented punctuation special characters numbers preserved alphabetical characters lower cased total sentences extracted average sentence length characters sentences split train test validation set ratios architecture implementation cross lingual marizer section rst outline solution problem cross lingual marization provide descriptions components cross lingual word embeddings ne tuning pretrained english summarization model slovene generation evaluation best hypothesis evaluation scores cluding slovene language model architecture cross lingual summarizer scheme approach consists steps presented fig pretrained summarization engine step use pretrained marization models work english summarizer chen bansal adapt cross lingual setting rst replaced english word embeddings input slovene embeddings step match word semantics languages cross lingual procrustes alignment conneau et al mapped slovene word embeddings english word vector space allowed slovene text input summarization model step ne tune different amounts slovene text discussed section step trained model generate hypotheses step evaluated hypotheses uation independently trained slovene language model transformer architecture different metrics best hypothesis included mary figure outline proposed cross lingual summarization approach english summarization model summarization model pretrained summarization model posed chen bansal model uses customarily trained embeddings allows simple cross lingual mappings architecture model relatively complex belongs hybrid approaches text marization combine abstractive extractive elements high level consists extractive network selects salient sentences abstractive network rewrites paraphrases reinforcement learning rl step optimizes model end end extractor abstractor works learned independently rl step model updates extractor weights leaves abstractor model trained cnn dailymail dataset contains training pairs validation pairs test pairs details available original paper cross lingual word embeddings input neural network summarization model transform text numerical vectors want use model cross lingual setting use slovene input map english vector space slovene embeddings pretrained slovene fasttext embeddings grave et al trained mixture slovene wikipedia common crawl fasttext dings constructed cbow algorithm mikolov et al extended position weights subword information especially ful morphologically rich languages slovene transform slovene embeddings english vector space muse library conneau et al supervised setting transformation muse uses train dictionary size test dictionary size created internally addition transformation embeddings replaced english tionary slovene dictionary built common words slovene training dataset role dictionary map words embeddings fine tuning summarization model adapt target language input slovene source language glish summarization model use model summarization target language zero shot transfer target language summaries available improve summarization model additional training test required additional data created models presented table differ additional data ne tuning meng baseline zero shot transfer model means target language data english embeddings swapped aligned slovene embeddings models use target language training set ne tune english model trained extractor model reinforcement learning optimized extractor provided chen bansal neously updated weights pretrained abstractor nal step optimized models rl component mslo cross lingual model trained complete target language training set scratch note training set target language signicantly smaller training set source language mslo vs meng cc docs crawl vectors html slovene data size instances details model meng mslo slovene embeddings trained extractor trained abstractor transfer cross lingual mappings ne tuning zero shot transfer cross lingual mappings trained extractor ne tuned abstractor cross lingual mappings trained extractor ne tuned abstractor cross lingual mappings trained extractor ne tuned abstractor cross lingual mappings trained extractor ne tuned abstractor cross lingual mappings trained extractor ne tuned abstractor table produced models different amounts target language data slovene ne tuning original summarization model training slovene language model adapted ne tuned models produce summaries slovene quality adequate reason generated hypotheses cessed selected best according different evaluation approaches described section want optimize uency grammatical correctness output sentences evaluation approaches guage models purpose trained character based language model slovene current state art language models baevski auli dai et al trained datasets similar chelba et al use ants transformer architecture vaswani et al transformer decoder implemented library vaswani et al adam optimizer attention heads hidden layers position wise feed forward works hidden layer size relu activation function standard hyperparameters training single gpu increased mum size input approximately percentile sentence character length training corpus shorter sentences padded spaces longer cut dictionary contains characters total number learning parameters language model trained epochs parts limited computational resources epochs million sentences epoch million sentences batch size model evaluated test set size sentences training took approximately days nvidia titan x gb gpu postprocessing creation nal summary summarization model ne tuned produce slovene summaries outputs low quality example rization models produce repeating n grams eliminate rule based approach improve quality summaries extracted large number hypotheses abstractive network congured size beam search standard evaluate evaluation metrics use consist components try capture presence relevant contents readability hypotheses quality content assessed metrics standard metric summarization called rouge score lin hovy recently proposed bertscore zhang et al based pretrained gual language model bert devlin et al calculated rouge bertscore scores comparing generated hypotheses abstractor network sentences extracted extractive network assume contain information want preserve tive readability generated hypotheses assessed measures internal evaluation hypotheses loss function computed abstractive neural network slovene language model described section later expressed perplexity score computed average entropy character expressed bits approach got different evaluations generated pothesis rst evaluation metric select best hypothesis observed results considered combinations metrics example rst rouge scores narrow selection best potheses selected best language model scores constraint belong different categories content based combinations readability based combinations permitted evaluation section provide analysis results produced summarization models present automatic evaluation scores human evaluation generated summaries end compare results related works evaluation metrics rouge recall oriented understudy gisting evaluation metrics commonly evaluation automatically generated text summaries lin hovy measures quality summary number lapping units n grams sequences texts reference summaries created humans automatically generated summaries commonly metrics rouge n rouge l rst measures overlapping n grams e unigrams bigrams rouge l measures longest common subsequence found summaries results cross lingual ne tuning table shows results models listed table baseline monolingual models english monolingual model meng generates twice character models average sentences models generate numbers result learning corpora english summaries contains average longer summaries model shows little additional instances update number extracted sentences average generated evaluation scores model meng mslo mt baseline pg et al reference slovene reference english sentences characters rouge l perplexity table performance cross lingual models meng compared monolingual models mslo mt baseline pg rows represent statistics reference slovene english maries metrics rouge l similar relations tween compared models surprisingly zero shot transfer learning model meng scores higher rouge metrics reason extracts sentences generates longer summary sentences repeats tences analyzing results meng noticed model nish sentence properly e generates good content stop continues generate words speculate problem special tokens start sentence end sentence capture grammar source guage special tokens hidden problem cross lingual model transfer manually inspected returned summary assess readability signicant readability improvement meng shows improvement meng generates long sentences dant rare words inserts punctuation inappropriate places trary generates short sentences summaries missing words shows improvement sentence selection improvement readability meng sentences formulated meaning present models interesting properties considering duce scores close models trained larger training sets target language e mslo indicates utility cross lingual transfer produce useful models data observe pg model scores highest perplexity par highest rouge scores explained model constraint choosing content way hybrid extractive abstractive models end end abstractive model generates summaries higher readability mt baseline cross lingual models trained sufcient data cross lingual model mslo trained scratch best models slovene summarization models use amounts training data infeasible conclude model better terms manually inspected readability consistently shows better rouge scores improved rouge l suggests cross lingual approach works produces better summaries evaluation post processing table shows additional improvements achieve different approaches post processing described section recall creation nal summary set generated hypotheses extracted sentence use content based metrics rouge l bertscore ity metrics internal loss score perplexity language model baseline model best ne tuned model described section e lingual model uses internal loss score select nal output perplexity slovene transformer based lm rank date hypotheses model improved point point rouge l contents based metrics bertscore rouge l produce larger improvements close performance wise selection metrics baseline nn loss transformer lm multilingual bertscore rouge l rouge l bertscore rouge l table post processing improvements selection best hypothesis loss score baseline tested combinations tion metrics report best row initially hypothesized complementary metrics needed select best hypothesis content readability line table shows case best metric pair belong content selection group aware results biased nal evaluation rouge metrics content based manual comparison models complementary metrics models based metrics conrmed produced readable summaries lower content accuracy human evaluation automatic evaluation limited assessment actual user needs tations novikova et al reason organized small study human evaluation generated summaries text reference way evaluation candidate random order text procedure slightly modied enables comparison task referees assign accuracy readability summary table scales accuracy represents overlap given facts summarized information readability measures hensive summary study articles summaries text generated reference evaluated referees referees included females males aged different degrees education report averages standard deviations table surprising accuracy reference summaries lower accuracy generated score accuracy little lot readability incomprehensible poor acceptable good awless table evaluation scales accuracy readability summaries type reference generated accuracy readability table average standard deviation human assigned accuracy ability reference generated maries summaries identied reasons explain result erence summaries contain true facts information veried text misleading speculative generated summaries produce veriable content second evaluation method directly measure content quality summary following instructions ticipants assign high score summary contains true unimportant irrelevant information model hybrid model selects paraphrases sentences assume participants easily lured thinking greater overlap content text generated summary text reference summary finally study small standard deviation answers considerable considerable chance error anticipated readability score reference summaries higher system summaries comparison related research compare best summarization model related models english slovene languages table shows results reported authors addition standard rouge scores provide bertscore possible neural summarization model slovene built zidarn layer lstm neural network attention mechanism copy mechanism beam search approach training set sta news dataset extracted gigada corpus different train test idations splits model scored higher difference lower rouge l bertscore models tical given existing sources variation different subsets original data different splits problematic nature automatic summary evaluation metrics conclude models perform similarly human evaluation models produce acceptable readability scores terms accuracy model generates accurate content cross lingual monolingual slovene models compare english models terms performance english models usually trained million instances gigaword dataset appropriate headline generation cnn daily mail dataset similar larger slovene dataset english model experiments chen bansal achieves scores twice high english slovene results misleading usually represents facts information accurately lot inspected instances ability model omit unimportant dependent clauses present model rarely pegasus zhang et al currently best abstractive summarization model transformer based presents interesting novel insight ing objectives resemble downstream task better faster ne tuning follows authors propose pretraining objectives bert masked guage model known devlin et al gap sentence ation selects masks sentences documents concatenates gap sentences pseudo summary model pretrained large corpora dataset consists texts m web pages gb hugenews dataset larger articles tb model achieved state art performance summarization tasks model zidarn slovene lstm slovene cross lingual model english chen bansal english zhang et al rouge l bertscore table comparison related research conclusion work developed neural cross lingual model abstractive summarization lution based pretrained model resource rich language english outputs corrected trained language model target language slovene tested different amounts training data target language ne tuning affect model addition automatic evaluation human evaluation summaries additional contributions work rst slovene marization dataset consisting news publicly available character based transformer neural language model cross lingual approach generates useful summaries zero shot transfer mode gradually increasing target language data ne tuning performance gradually approaches performance model trained target language reasonably large data target language similar model directly trained target language ndings conrm quality size dataset dene range neural networks performance case evident considering diverse topics dataset topics better represented dataset better summarized represented ones human evaluation shows model generates summaries reasonably accurate content acceptable readability model improved ways quality cross lingual alignment case lower language pairs recently introduced contextual embeddings bert devlin et al elmo peters et al improved tasks applied ther necessary increase vocabulary size rich slovene morphology instead rouge reward rl step maximize bertscore ward readability measures assess readability generated summaries improve quality ne tuning dataset procuring news articles original summary text splits instead currently tics additionally denoise dataset calculating bertscore scores reference summary text retain best matching pairs future studies investigate improve metrics abstractive text summarization idea combine content based metrics rouge bertscore perplexity measure ensure accuracy readability metric interesting problem future work attain greater levels tion cross lingual model transfer research inuence special tokens studied acknowledgements research supported slovene research agency research core funding research nancially supported european social fund republic slovenia ministry education science sport projects quality slovene textbooks kauc development slovene digital environment rsdo paper supported european union s zon programme project embeddia cross lingual embeddings represented languages european news media grant results publication reects authors view commission sponsible use information contains references oliver adams adam makarucha graham neubig steven bird trevor cohn cross lingual word embeddings low resource language modeling proceedings conference european chapter association computational linguistics volume long papers pages mikel artetxe holger schwenk massively multilingual sentence embeddings zero shot cross lingual transfer transactions ation computational linguistics alexei baevski michael auli adaptive input representations neural guage modeling arxiv preprint dzmitry bahdanau kyunghyun cho yoshua bengio neural machine lation jointly learning align translate international conference learning representations piotr bojanowski edouard grave armand joulin tomas mikolov enriching word vectors subword information transactions association computational linguistics ciprian chelba tomas mikolov mike schuster qi ge thorsten brants phillipp koehn tony robinson billion word benchmark measuring progress statistical language modeling fifteenth annual conference tional speech communication association yen chun chen mohit bansal fast abstractive summarization selected sentence rewriting proceedings annual meeting association computational linguistics volume long papers pages zewen chi li dong furu wei wenhui wang xian ling mao heyan huang cross lingual natural language generation pre training arxiv pages arxiv alexis conneau guillaume lample marcaurelio ranzato ludovic denoyer arxiv preprint herve jegou word translation parallel data zihang dai zhilin yang yiming yang jaime g carbonell quoc le lan salakhutdinov transformer xl attentive language models length context proceedings annual meeting association computational linguistics pages jacob devlin ming wei chang kenton lee kristina toutanova bert training deep bidirectional transformers language understanding ceedings conference north american chapter ation computational linguistics human language technologies volume long short papers pages pascal fecht sebastian blank hans peter zorn sequential transfer learning nlp german text summarization proceedings edition swiss text analytics conference mahak gambhir vishal gupta recent automatic text summarization niques survey articial intelligence review gigada reference corpus slovenian cjvt gigada system cjvt si gigafida system retrieved url edouard grave piotr bojanowski prakhar gupta armand joulin tomas language resources mikolov learning word vectors languages evaluation conference baotian hu qingcai chen fangze zhu lcsts large scale chinese short text proceedings conference empirical summarization dataset methods natural language processing pages chin yew lin eduard hovy manual automatic evaluation summaries proceedings workshop automatic summarization volume pages rada mihalcea graph based ranking algorithms sentence extraction applied text summarization proceedings acl interactive poster stration sessions pages tomas mikolov kai chen greg corrado jeffrey dean efcient estimation word representations vector space tomas mikolov quoc v le ilya sutskever exploiting similarities languages machine translation ramesh nallapati bowen zhou cicero dos santos caglar gulcehre bing xiang abstractive text summarization sequence sequence rnns proceedings signll conference computational natural language learning pages jekaterina novikova ondrej dusek amanda cercas curry verena rieser need new evaluation metrics proceedings ence empirical methods natural language processing pages jessica ouyang boya song kathleen mckeown robust abstractive proceedings conference tem cross lingual summarization north american chapter association computational tics human language technologies volume long short papers pages jeffrey pennington richard socher christopher manning glove global tors word representation proceedings conference pirical methods natural language processing emnlp pages matthew e peters mark neumann mohit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word representations proceedings naacl hlt pages sebastian ruder ivan vulic anders sgaard survey cross lingual word embedding models journal articial intelligence research alexander m rush sumit chopra jason weston neural attention model abstractive sentence summarization proceedings conference empirical methods natural language processing pages evan sandhaus new york times annotated corpus linguistic data tium philadelphia abigail peter j liu christopher d manning point marization pointer generator networks proceedings annual meeting association computational linguistics volume long pers pages milan straka nikita mediankin tom kocmi zdenek vojtech hudecek jan hajic sumeczech large czech news based summarization dataset proceedings eleventh international conference language resources evaluation lrec zhaopeng tu zhengdong lu yang liu xiaohua liu hang li modeling proceedings annual coverage neural machine translation meeting association computational linguistics volume long pers pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention need proceedings international conference neural information cessing systems pages ashish vaswani samy bengio eugene brevdo francois chollet aidan n gomez stephan gouws llion jones ukasz kaiser nal kalchbrenner niki parmar ryan sepassi noam shazeer jakob uszkoreit neural machine translation corr url http org jingqing zhang yao zhao mohammad saleh peter j liu sus pre training extracted gap sentences abstractive summarization tianyi zhang varsha kishore felix wu kilian q weinberger yoav artzi bertscore evaluating text generation bert junnan zhu haoran li tianshang liu yu zhou jiajun zhang chengqing zong msmo multimodal summarization multimodal output ceedings conference empirical methods natural language processing pages junnan zhu qian wang yining wang yu zhou jiajun zhang shaonan wang chengqing zong ncls neural cross lingual summarization proceedings conference empirical methods natural language processing international joint conference natural language processing emnlp ijcnlp pages rok zidarn automatic text summarization slovene texts deep neural works university ljubljana faculty computer information science ljubljana msc thesis slovene
