c e d l c s c v v i x r a leveraging parsbert and pretrained for persian abstractive text summarization preprint compiled december mehrdad mohammad and mohammad of computer engineering islamic azad university north tehran branch tehran iran m tnb ac ir of electrical engineering and robotics queensland university of technology brisbane australia mohammad qut edu au of electrical and electronic engineering shahed univerisity tehran iran ac ir abstract text summarization is one of the most critical natural language processing nlp tasks more and more researches are conducted in this eld every day pre trained transformer based encoder decoder models have begun to gain popularity for these tasks this paper proposes two methods to address this task and introduces a novel dataset named pn summary for persian abstractive text summarization the models employed in this paper are and an encoder decoder version of the parsbert model i e a monolingual bert model for persian these models are ne tuned on the pn summary dataset the current work is the rst of its kind and by achieving promising results can serve as a baseline for any future work keywords text summarization abstractive summarization pre trained based bert introduction with the emergence of the digital age a vast amount of textual information has become digitally available dierent natural language processing nlp tasks focus on dierent aspects of this information automatic text summarization is one of these tasks and concerns about compressing texts into shorter formats such that the most important information of the content is served this is crucial in many applications since ing summaries by humans however precise can become quite a time consuming and cumbersome such applications include text retrieval systems used in search engines to display a marized version of the search results text summarization can be viewed from dierent perspectives including single document vs multi document and monolingual vs multi lingual however an important pect of this task is the approach which is either extractive or abstractive in extractive summarization a few sentences are selected from the context to represent the whole text these sentences are selected based on their scores or ranks these scores are determined by computing certain features such as the ordinal position of sentences concerning one another length of the sentence a ratio of nouns after sentences are ranked the top n sentences are selected to represent the whole text abstractive summarization techniques create a short version of the original text by generating new sentences with words that are not necessarily found in the original text compared to extractive summarization abstractive techniques are more daunting yet more attractive and exible therefore more and more attention is given to abstractive techniques in dierent languages however to the best of our knowledge too few works have been dedicated to text summarization in the persian language of which almost all are extractive this is partly due to the lack of proper persian text datasets available for this task this is the primary motivation behind the current work to ate an abstractive text summarization framework for the persian language and compose a new properly formatted dataset for this task there are dierent approaches towards abstractive text rization especially for the english language of which many are based on sequence to sequence structures as text summarization can be viewed as a task in a encoder decoder model in which a deep current generative decoder is used to improve the tion quality is presented the model presented in is an attentional encoder decoder recurrent neural network rnn used for abstractive text summarization in a new ing method is introduced that combines reinforcement learning with supervised word prediction an augmented version of a model is presented in similarly an extended sion of encoder decoder architecture that benets from an formation selection layer for abstractive summarization is preprint leveraging parsbert and pretrained for persian abstractive text summarization sequence to sequence parsbert parsbert is a monolingual version of bert language model for the persian language that adopts the base guration of the bert model i e hidden layers hidden size of with attention heads bert is a based language model with an encoder only architecture that is shown in gure in this architecture the input sequence xn is mapped to a contextualized encoded sequence by going through a series of bi directional attention blocks with two feed forward layers in each block the output sequence can then be mapped to a task specic put class by adding a classication layer to the last hidden layer n sented in many of the works mentioned above benet from pre trained language models as these models have started to gain dous popularity over the past few years this is because they simplify each nlp task to a lightweight ne tuning phase by employing transfer learning benets therefore an approach to pre train a structure for text summarization can be quite promising bert and are amongst widely used pre trained language modeling techniques bert uses a masked language model mlm and an encoder decoder stack to perform conditioning on the left and right context on the other hand is a unied framework that employs text text format to address nlp text based problems a multilingual variation of the model is called that covers dierent languages and is trained on a common crawl based dataset due to its multilingual property the model is a suitable option for languages other than english the bert model also has a multilingual version however there are numerous monolingual variations of this model that have shown to outperform the multilingual version on various nlp tasks for the persian language the parsbert model has shown state of the art on many persian nlp tasks such as named entity recognition ner and sentiment analysis although pre trained language models have been quite ful in terms of natural language understanding nlu tasks they have shown less eciency regarding tasks as a result in the current paper we seek to address the mentioned shortcomings for the persian language regarding text rization by making the following contributions introducing a novel dataset for the persian text marization task this dataset is publicly available for anyone who wishes to use it for any future work investigating two dierent approaches towards stractive text summarization for persian texts one is to use the parsbert model in a structure as presented in the other one is to use the model both models are ne tuned on the proposed dataset the rest of this paper is structured as follows section lines the parsbert encoder decoder model as well as in section an overview of the ne tuning and text generation congurations for both approaches is provided the composition of the dataset and its statistical features are duced in section this section also outlines the metrics used to measure the performance of the models section presents the results obtained from ne tuning the dataset mentioned in earlier models finally section concludes the paper models in this section an overview of sequence to sequence bert and architecture is provided com hooshvare pn summary figure the encoder only architecture of bert other tions of bert such as parsbert have the same architecture bert model achieves state of the art performance on nlu tasks by mapping input sequences to output sequences with a priori known output lengths however since the output quence dimension does not rely on the input it is impractical to use bert for text generation summarization in other words any bert based model corresponds to the architecture of only the encoder part of transformer based encoder decoder models which are mostly used for text generation on the other hand decoder only models such as preprint leveraging parsbert and pretrained for persian abstractive text summarization can be used as a means of text generation however it has been shown that encoder decoder structures can perform better for such a task as a result we used parsbert to warm start both encoder and decoder from an encoder only checkpoint as mentioned in to achieve a pre trained encoder decoder model or which can be ne tuned for text summarization using the dataset introduced in section in this architecture the encoder layer is the same as the bert transformer layers the decoder layers are also the same as that of parsbert with a few changes first cross attention layers are added between self attention and feed forward ers in order to condition the decoder on the contextualized coded sequence e the output of the parsbert model ond the bi directional self attention layers are changed into uni directional layers to be compatible with the auto regressive generation all in all while warm starting the decoder only the cross attention layer weights are initialized randomly and all other weights are parsbert s pre trained weights the proposed gure illustrates the building blocks of model warm started with the parsbert model along with an example text and its summarized version ated by the proposed model the persian language in various situations e forming plural in the example text shown in gure the word nouns is actually composed of three tokens noun pluralizing token where the token represents the half space token that connect the noun to the pluralizing token after that the text is fed into the encoder block the result of the encoder block is fed to the decoder block which in turn generates the output summary the half character tokens are then converted to actual half characters by the particular token decoder block stands for multilingual text to text transfer transformer multilingual and is a multilingual version of the model is an encoder decoder transformer architecture that closely reects the primary building block of the original transformer model and covers the following objectives language modeling to predict the next word de shuing to redene the original text corrupting spans to predict masked words network architecture inherits and transforms the previous unifying frameworks for down stream nlp tasks into a text text format in other words the architecture allows for employing the encoder decoder procedure to aggregate every possible nlp task into one network thus the same parameters and loss function are used for every task this is shown in gure figure architecture along with an ple persian text and its summarized version generated by the model in this gure the input text is rst fed to a special token encoder that handles half space character unicode and moves unwanted tokens half space character is widely used in figure as a unied framework for down stream nlp tasks the diagram shows each down stream task in a to text format including translation red linguistic ity blue sentence similarity yellow and text summarization green inherits all capabilities of the model was trained on an extended version of the dataset that contains more than and web page contents in languages including persian over monthly scrapes to date compared to other multilingual models like multilingual bert xlm r and multilingual bert no support for persian reaches state of the art on all the tasks especially on the summarization task gure illustrates the architecture after ne tuning along with an example text in this schema the hfs token represents preprint leveraging parsbert and pretrained for persian abstractive text summarization the half space character in persian and summarize serves as a text to text ag for summarization task sequences as compared to a greedy search one drawback is that beam search tends to generate sequences with some words repeated to overcome this issue we utilize n grams penalties this way if a next word causes the generation of an already seen n grams the probability of that word will be set to manually thus preventing that n gram from being repeated another parameter used in beam search is early stopping which can be either active or inactive if active text generation is stopped when all beam hypotheses reach the eos token the number of beams the n grams penalty sizes the length penalty and early stopping values used for and models in the current work are presented in table table beam search conguration for and models for auto regressive text summarization after ne tuning beams repetitive n gram size length penalty early stoping status active active evaluation for evaluating the performance of the two architectures duced in this paper we composed a new dataset by crawling numerous articles along with their summaries from dierent news agency websites hereafter denoted as pn summary both models are ne tuned on this dataset therefore this is the rst time this dataset is being proposed to be used as a benchmark for persian abstractive summarization this dataset includes a total of documents and covers a range of categories from economy to tourism the frequency distribution of the article categories and the number of articles from each news agency can be seen from gures and respectively it should be noted that the number of tokens in article maries is varying this can be viewed in gure as shown from this gure most of the articles summaries have a length of around tokens to determine the performance of the models we use oriented understudy for gisting evaluation rouge metric package this package is widely used for automatic marization and machine translation evaluation the metrics cluded in this package compare an automated summary against a reference summary for each document there are ve ent metrics included in this package we calculate the score for three of these metrics to show the overall performance of both models on the proposed dataset unigram scoring which computes the overlap of uni grams between the generated and the reference summaries bigram scoring which computes the overlap of bigrams between the generated and the erence summaries figure architecture solution and an example persian text and its summarized version generated by the model configurations fine tuning conguration to fine tune both models presented in section on the summery dataset introduced in section we have used the adam optimizer with warm up steps a batch size of and training epochs the learning rate for parsbert and are and respectively text generation conguration the text generation process refers to the decoding strategy for auto regressive language generation after the ne tuned model in essence the auto regressive generation is centered around the assumption that the probability distribution of any word quence can be decomposed into a product of conditional next word distributions as denoted by equation where is the initial context word and t is the length of the word sequence t the objective here is to maximize the sequence probability by choosing the optimal tokens words one method is greedy search in which the next word selected is simply the word with the highest probability this method however neglects words with high probabilities if they are hidden behind some low probability words to address this problem we use beam search method that keeps nbeams number of most likely sequences i e beams at each time step and eventually chooses the one with the highest overall probability beam search generates higher probability preprint leveraging parsbert and pretrained for persian abstractive text summarization rouge l scoring in which the scores are lated at sentence level in this metric new lines are ignored and longest common subsequence lcs is computed between two text pieces results and discussion this section presents the results obtained from ne tuned and parsbert based structure on the proposed pn summary dataset the scores on three dierent rouge metrics discussed in section are reported in table it can be seen that the parsbert structure achieves higher scores as compared to the model this could be due to the fact that encoder decoder weights i e parsbert weights in this architecture are concretely tuned on a massive persian corpus making it a tter architecture for persian only tasks table depicts rouge scores on the test set the jective of models and baselines is abstractive the two models are ne tuned on the persian news summarization dataset summary rouge model r l since no other pre trained abstractive summarization methods have been proposed for persian language and since this is the rst time the pn summary dataset is being introduced and leased it is impossible to compare the results of the present work with any other baseline as a result the outcomes sented in this work can serve as a baseline for any future stractive methods for the persian language that seeks to train their model on the proposed pn summary dataset presented and released with the current work to further illustrate these two models performance we have included two examples from the dataset in table the main text the actual summary and the summaries generated by the and models are shown in this table based on this table the summary given by the model in both examples is relatively closer to the actual summary in terms of both meaning and lexical choices conclusion limited work has been dedicated to text summarization for the persian language of which none are abstractive based on trained models in this paper we presented two pre trained methods and designed to address text summarization in sian with an abstract approach one is based on a multilingual model and the other is a warm started from the parsbert language model we have also composed and leased a new dataset called pn summary for text summarization since there is an apparent lack of such datasets for the persian language the results of ne tuning the proposed methods on the mentioned dataset are promising due to a lack of works in this area our work could not be compared to any earlier work figure the frequency of article categories in the proposed dataset figure the number of articles extracted from each of the news agency website figure token length distribution of articles summaries preprint leveraging parsbert and pretrained for persian abstractive text summarization table examples of highly abstractive reference summaries from persian news network using and models each example consists of the trim article the true summary and the generated summaries by both models references example and can now serve as a baseline for any future works in this eld ani nenkova and kathleen mckeown a survey of text summarization techniques in mining text data pages springer harold p edmundson new methods in automatic ing journal of the acm jacm andrew turpin yohannes tsegay david hawking and hugh e williams fast generation of result snippets in web search in proceedings of the annual tional acm sigir conference on research and ment in information retrieval pages aarti patil komal pharande dipali nale and roshani international agrawal automatic text summarization journal of computer applications janara christensen stephen soderland oren etzioni et al towards coherent multi document summarization in proceedings of the conference of the north ican chapter of the association for computational tics human language technologies pages ani nenkova lucy vanderwende and kathleen own a compositional context sensitive multi document summarizer exploring the factors that inuence rization in proceedings of the annual international acm sigir conference on research and development in information retrieval pages mahak gambhir and vishal gupta recent automatic text summarization techniques a survey articial gence review vishal gupta and gurpreet singh lehal a survey of text summarization extractive techniques journal of emerging technologies in web intelligence piji li wai lam lidong bing and z wang deep current generative decoder for abstractive text tion arxiv ramesh nallapati bowen zhou c d santos c aglar gulcehre and b xiang abstractive text summarization using sequence to sequence rnns and beyond in conll romain paulus caiming xiong and r socher a deep reinforced model for abstractive summarization arxiv a see peter j liu and christopher d manning get to the point summarization with pointer generator works arxiv wei li x xiao yajuan lyu and yuanzhuo wang proving neural abstractive document summarization with in emnlp explicit information selection modeling jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep tional transformers for language understanding arxiv preprint leveraging parsbert and pretrained for persian abstractive text summarization colin rael noam shazeer adam roberts katherine lee sharan narang m matena yanqi zhou w li and peter j liu exploring the limits of transfer learning with a unied text to text transformer j mach learn res linting xue noah constant a roberts mihir kale rami al rfou aditya siddhant a barua and colin fel a massively multilingual pre trained text to text transformer arxiv wissam antoun fady baly and hazem m hajj arabert transformer based model for arabic language ing arxiv louis martin benjamin muller pedro javier ortiz suarez yoann dupont laurent romary eric de la clergerie djame seddah and benot sagot camembert a tasty french language model arxiv mehrdad farahani mohammad gharachorloo marzieh farahani and m manthouri parsbert based model for persian language understanding arxiv sascha rothe shashi narayan and a severyn ing pre trained checkpoints for sequence generation tasks transactions of the association for computational guistics ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser and illia polosukhin attention is all you need arxiv a radford jerey wu r child david luan dario amodei and ilya sutskever language models are supervised multitask learners colin rael noam shazeer adam roberts katherine lee sharan narang m matena yanqi zhou w li and peter j liu exploring the limits of transfer learning with a unied text to text transformer j mach learn res ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser and illia polosukhin attention is all you need alexis conneau kartikay khandelwal naman goyal vishrav chaudhary guillaume wenzek francisco guzman edouard grave myle ott luke zettlemoyer and veselin stoyanov unsupervised cross lingual resentation learning at scale yinhan liu jiatao gu naman goyal xian li sergey edunov marjan ghazvininejad mike lewis and luke zettlemoyer multilingual denoising pre training for ral machine translation g klein yoon kim y deng jean senellart and der m rush opennmt open source toolkit for neural machine translation arxiv chin yew lin rouge a package for automatic tion of summaries in acl
