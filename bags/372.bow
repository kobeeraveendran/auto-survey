leveraging parsbert pretrained persian abstractive text summarization preprint compiled december mehrdad mohammad mohammad computer engineering islamic azad university north tehran branch tehran iran tnb electrical engineering robotics queensland university technology brisbane australia mohammad qut edu electrical electronic engineering shahed univerisity tehran iran abstract text summarization critical natural language processing nlp tasks researches conducted eld day pre trained transformer based encoder decoder models begun gain popularity tasks paper proposes methods address task introduces novel dataset named summary persian abstractive text summarization models employed paper encoder decoder version parsbert model monolingual bert model persian models tuned summary dataset current work rst kind achieving promising results serve baseline future work keywords text summarization abstractive summarization pre trained based bert introduction emergence digital age vast textual information digitally available dierent natural language processing nlp tasks focus dierent aspects information automatic text summarization tasks concerns compressing texts shorter formats important information content served crucial applications ing summaries humans precise time consuming cumbersome applications include text retrieval systems search engines display marized version search results text summarization viewed dierent perspectives including single document multi document monolingual multi lingual important pect task approach extractive abstractive extractive summarization sentences selected context represent text sentences selected based scores ranks scores determined computing certain features ordinal position sentences concerning length sentence ratio nouns sentences ranked sentences selected represent text abstractive summarization techniques create short version original text generating new sentences words necessarily found original text compared extractive summarization abstractive techniques daunting attractive exible attention given abstractive techniques dierent languages best knowledge works dedicated text summarization persian language extractive partly lack proper persian text datasets available task primary motivation current work ate abstractive text summarization framework persian language compose new properly formatted dataset task dierent approaches abstractive text rization especially english language based sequence sequence structures text summarization viewed task encoder decoder model deep current generative decoder improve tion quality presented model presented attentional encoder decoder recurrent neural network rnn abstractive text summarization new ing method introduced combines reinforcement learning supervised word prediction augmented version model presented similarly extended sion encoder decoder architecture benets formation selection layer abstractive summarization preprint leveraging parsbert pretrained persian abstractive text summarization sequence sequence parsbert parsbert monolingual version bert language model persian language adopts base guration bert model hidden layers hidden size attention heads bert based language model encoder architecture shown gure architecture input sequence mapped contextualized encoded sequence going series directional attention blocks feed forward layers block output sequence mapped task specic class adding classication layer hidden layer sented works mentioned benet pre trained language models models started gain dous popularity past years simplify nlp task lightweight tuning phase employing transfer learning benets approach pre train structure text summarization promising bert widely pre trained language modeling techniques bert uses masked language model mlm encoder decoder stack perform conditioning left right context hand unied framework employs text text format address nlp text based problems multilingual variation model called covers dierent languages trained common crawl based dataset multilingual property model suitable option languages english bert model multilingual version numerous monolingual variations model shown outperform multilingual version nlp tasks persian language parsbert model shown state art persian nlp tasks named entity recognition ner sentiment analysis pre trained language models ful terms natural language understanding nlu tasks shown eciency tasks result current paper seek address mentioned shortcomings persian language text rization making following contributions introducing novel dataset persian text marization task dataset publicly available wishes use future work investigating dierent approaches stractive text summarization persian texts use parsbert model structure presented use model models tuned proposed dataset rest paper structured follows section lines parsbert encoder decoder model section overview tuning text generation congurations approaches provided composition dataset statistical features duced section section outlines metrics measure performance models section presents results obtained tuning dataset mentioned earlier models finally section concludes paper models section overview sequence sequence bert architecture provided com hooshvare summary figure encoder architecture bert tions bert parsbert architecture bert model achieves state art performance nlu tasks mapping input sequences output sequences priori known output lengths output quence dimension rely input impractical use bert text generation summarization words bert based model corresponds architecture encoder transformer based encoder decoder models text generation hand decoder models preprint leveraging parsbert pretrained persian abstractive text summarization means text generation shown encoder decoder structures perform better task result parsbert warm start encoder decoder encoder checkpoint mentioned achieve pre trained encoder decoder model tuned text summarization dataset introduced section architecture encoder layer bert transformer layers decoder layers parsbert changes cross attention layers added self attention feed forward ers order condition decoder contextualized coded sequence output parsbert model ond directional self attention layers changed uni directional layers compatible auto regressive generation warm starting decoder cross attention layer weights initialized randomly weights parsbert pre trained weights proposed gure illustrates building blocks model warm started parsbert model example text summarized version ated proposed model persian language situations forming plural example text shown gure word nouns actually composed tokens noun pluralizing token token represents half space token connect noun pluralizing token text fed encoder block result encoder block fed decoder block turn generates output summary half character tokens converted actual half characters particular token decoder block stands multilingual text text transfer transformer multilingual multilingual version model encoder decoder transformer architecture closely reects primary building block original transformer model covers following objectives language modeling predict word shuing redene original text corrupting spans predict masked words network architecture inherits transforms previous unifying frameworks stream nlp tasks text text format words architecture allows employing encoder decoder procedure aggregate possible nlp task network parameters loss function task shown gure figure architecture ple persian text summarized version generated model gure input text rst fed special token encoder handles half space character unicode moves unwanted tokens half space character widely figure unied framework stream nlp tasks diagram shows stream task text format including translation red linguistic ity blue sentence similarity yellow text summarization green inherits capabilities model trained extended version dataset contains web page contents languages including persian monthly scrapes date compared multilingual models like multilingual bert xlm multilingual bert support persian reaches state art tasks especially summarization task gure illustrates architecture tuning example text schema hfs token represents preprint leveraging parsbert pretrained persian abstractive text summarization half space character persian summarize serves text text summarization task sequences compared greedy search drawback beam search tends generate sequences words repeated overcome issue utilize grams penalties way word causes generation seen grams probability word set manually preventing gram repeated parameter beam search early stopping active inactive active text generation stopped beam hypotheses reach eos token number beams grams penalty sizes length penalty early stopping values models current work presented table table beam search conguration models auto regressive text summarization tuning beams repetitive gram size length penalty early stoping status active active evaluation evaluating performance architectures duced paper composed new dataset crawling numerous articles summaries dierent news agency websites denoted summary models tuned dataset rst time dataset proposed benchmark persian abstractive summarization dataset includes total documents covers range categories economy tourism frequency distribution article categories number articles news agency seen gures respectively noted number tokens article maries varying viewed gure shown gure articles summaries length tokens determine performance models use oriented understudy gisting evaluation rouge metric package package widely automatic marization machine translation evaluation metrics cluded package compare automated summary reference summary document ent metrics included package calculate score metrics overall performance models proposed dataset unigram scoring computes overlap uni grams generated reference summaries bigram scoring computes overlap bigrams generated erence summaries figure architecture solution example persian text summarized version generated model configurations fine tuning conguration fine tune models presented section summery dataset introduced section adam optimizer warm steps batch size training epochs learning rate parsbert respectively text generation conguration text generation process refers decoding strategy auto regressive language generation tuned model essence auto regressive generation centered assumption probability distribution word quence decomposed product conditional word distributions denoted equation initial context word length word sequence objective maximize sequence probability choosing optimal tokens words method greedy search word selected simply word highest probability method neglects words high probabilities hidden low probability words address problem use beam search method keeps nbeams number likely sequences beams time step eventually chooses highest overall probability beam search generates higher probability preprint leveraging parsbert pretrained persian abstractive text summarization rouge scoring scores lated sentence level metric new lines ignored longest common subsequence lcs computed text pieces results discussion section presents results obtained tuned parsbert based structure proposed summary dataset scores dierent rouge metrics discussed section reported table seen parsbert structure achieves higher scores compared model fact encoder decoder weights parsbert weights architecture concretely tuned massive persian corpus making tter architecture persian tasks table depicts rouge scores test set jective models baselines abstractive models tuned persian news summarization dataset summary rouge model pre trained abstractive summarization methods proposed persian language rst time summary dataset introduced leased impossible compare results present work baseline result outcomes sented work serve baseline future stractive methods persian language seeks train model proposed summary dataset presented released current work illustrate models performance included examples dataset table main text actual summary summaries generated models shown table based table summary given model examples relatively closer actual summary terms meaning lexical choices conclusion limited work dedicated text summarization persian language abstractive based trained models paper presented pre trained methods designed address text summarization sian abstract approach based multilingual model warm started parsbert language model composed leased new dataset called summary text summarization apparent lack datasets persian language results tuning proposed methods mentioned dataset promising lack works area work compared earlier work figure frequency article categories proposed dataset figure number articles extracted news agency website figure token length distribution articles summaries preprint leveraging parsbert pretrained persian abstractive text summarization table examples highly abstractive reference summaries persian news network models example consists trim article true summary generated summaries models references example serve baseline future works eld ani nenkova kathleen mckeown survey text summarization techniques mining text data pages springer harold edmundson new methods automatic ing journal acm jacm andrew turpin yohannes tsegay david hawking hugh williams fast generation result snippets web search proceedings annual tional acm sigir conference research ment information retrieval pages aarti patil komal pharande dipali nale roshani international agrawal automatic text summarization journal computer applications janara christensen stephen soderland oren etzioni coherent multi document summarization proceedings conference north ican chapter association computational tics human language technologies pages ani nenkova lucy vanderwende kathleen compositional context sensitive multi document summarizer exploring factors inuence rization proceedings annual international acm sigir conference research development information retrieval pages mahak gambhir vishal gupta recent automatic text summarization techniques survey articial gence review vishal gupta gurpreet singh lehal survey text summarization extractive techniques journal emerging technologies web intelligence piji wai lam lidong bing wang deep current generative decoder abstractive text tion arxiv ramesh nallapati bowen zhou santos aglar gulcehre xiang abstractive text summarization sequence sequence rnns conll romain paulus caiming xiong socher deep reinforced model abstractive summarization arxiv peter liu christopher manning point summarization pointer generator works arxiv wei xiao yajuan lyu yuanzhuo wang proving neural abstractive document summarization emnlp explicit information selection modeling jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep tional transformers language understanding arxiv preprint leveraging parsbert pretrained persian abstractive text summarization colin rael noam shazeer adam roberts katherine lee sharan narang matena yanqi zhou peter liu exploring limits transfer learning unied text text transformer mach learn res linting xue noah constant roberts mihir kale rami rfou aditya siddhant barua colin fel massively multilingual pre trained text text transformer arxiv wissam antoun fady baly hazem hajj arabert transformer based model arabic language ing arxiv louis martin benjamin muller pedro javier ortiz suarez yoann dupont laurent romary eric clergerie djame seddah benot sagot camembert tasty french language model arxiv mehrdad farahani mohammad gharachorloo marzieh farahani manthouri parsbert based model persian language understanding arxiv sascha rothe shashi narayan severyn ing pre trained checkpoints sequence generation tasks transactions association computational guistics ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez lukasz kaiser illia polosukhin attention need arxiv radford jerey child david luan dario amodei ilya sutskever language models supervised multitask learners colin rael noam shazeer adam roberts katherine lee sharan narang matena yanqi zhou peter liu exploring limits transfer learning unied text text transformer mach learn res ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez lukasz kaiser illia polosukhin attention need alexis conneau kartikay khandelwal naman goyal vishrav chaudhary guillaume wenzek francisco guzman edouard grave myle ott luke zettlemoyer veselin stoyanov unsupervised cross lingual resentation learning scale yinhan liu jiatao naman goyal xian sergey edunov marjan ghazvininejad mike lewis luke zettlemoyer multilingual denoising pre training ral machine translation klein yoon kim deng jean senellart der rush opennmt open source toolkit neural machine translation arxiv chin yew lin rouge package automatic tion summaries acl
