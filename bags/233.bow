summae zero shot abstractive text summarization length agnostic auto encoders peter j liu google brain com yu chung mit csail edu jie ren google brain com t c o l c s c v v x r abstract propose end end neural model zero shot abstractive text summarization paragraphs introduce benchmark task rocsumm based rocstories subset collected human summaries task ve sentence stories paragraphs summarized sentence human summaries evaluation sults extractive human baselines demonstrate large abstractive gap formance model summae consists denoising auto encoder embeds tences paragraphs common space decoded summaries paragraphs generated decoding sentence paragraph representations nd traditional sequence sequence auto encoders fail produce good summaries describe specic architectural choices pre training techniques signicantly improve performance outperforming tive baselines data training tion code best model weights sourced story paragraph summarize jason listened weather heard going sunny thought kids like swimming gathered swimsuits towels sunscreen jason kids got truck drove beach spent hours playing splashing surf human summaries jason saw nice weather forecast went beach kids hours jason took kids swimming beach sunny day jason decided kids beach sunny day best extractive sentence jason listened weather heard going sunny unsupervised abstractive summary jason listened weather coolest fun day went beach got ready example story summarize figure validation set collected human maries human summaries independent raters c extractive sentence highest summary generated model seeing summaries introduction extractive summarization studied sively past decades gupta lehal ferreira et al humans typically summarize abstractively phrasing performing non trivial compression details difcult encode cal summarization algorithms recent progress neural language models sutskever et al jozefowicz et al radford et al enabled models generate near uent language mere regurgitations training data large datasets document summary pairs equal contribution work interning google brain primarily news domain abstractive marization approached supervised neural sequence transduction problem rush et al nallapati et al narayan et al fabbri et al outside news large parallel datasets rare cost prohibitive labeling process e reading long documents writing summaries commonly available large corpora documents summaries sirable models capable automatically summarizing documents abstractively little supervision contrast abstractive methods tractive approaches rely example maries inspired study extreme case exposure summaries training unsupervised zero shot abstractive tion uas recently far limited work uas multi document chu liu single document isonuma work focus et al cases uas paragraphs sentence haps basic form multi sentence document summarization particular marize ve sentence stories ries mostafazadeh et al non trivial abstractive gap man extractive performance making able benchmark measuring progress uas approach based training ing auto encoder vincent et al codes sentences paragraphs shared space decoder input pre pended special token signal decode sentence paragraph summarizing sentence erated decoding sentence encoded paragraph found traditional approaches training auto encoder sulted non overlapping sentence paragraph latent sub spaces segregation resulting long multi sentence summaries describe architectural modications supervised pre training objectives prevent regation improve performance signicantly sentence extractive baselines goal human performance far believe techniques presented major step direction summary contributions follows introduce new benchmark task summ measuring progress human performance uas propose novel end end differentiable neural model uas graphs describe novel self supervised training signicantly improve performance sentence extractive baselines denoising objectives conduct ablation experiments showing importance architectural choices model objectives new task unsupervised abstractive summarization new task purposes augments isting dataset rocstories mostafazadeh et al originally designed story cloze test sct model choose correct fth sentence candidates given rst stories self contained verse realistic non technical high quality coherent story arch human mance sct task close proposed uas task involves ing ve sentence training rocstories single sentence summaries ing e perform zero shot summarization found summaries independent human raters high similarity task suggesting dened relatively unambiguous trast summarization tasks sired length topic summary unclear simplicity task conducive iterating quickly making rapid progress uas ing ve sentences low bound total number words avoids engineering issues arise long sequences constraints simple calculate maximum sentence extractive summarization performance far human performance ble suggesting need abstractive models contrast unclear example human performance popular cnn dailymail supervised summarization task et al abstractive models provide benet extractive ones kryscinski et al collection reference summaries evaluation evaluate summarization models collected multiple summaries independent enced highly reputable amazon mechanical turk amt workers worker selection criteria amt template found pendix split original training stories train valid test amples collect human summaries validation examples test examples collecting multiple summaries allowed mate human performance treat multiple right answers fairly averaging metrics summaries given example ample story human summaries best tractive sentence model summaries found figure related work chu liu proposed model shot multi document abstractive summarization mean representations auto encoder input documents code summary isonuma et al proposed summarize product review describing discourse tree summary root child sentences explain parent baziotis et al performed sentence pression chaining sequence sequence models auto encoder gumbel softmax estimator jang et al sample output sequence rst encouraged guage pre trained language model loss encouraged related original sentence input second model trained reconstruct inal sentence fevry phang noising auto encoder sentence compression input sentence articially extended word shufed encouraging model learn exclude compress producing shorter tences wang lee trained cycle gan model zhu et al learn summary mapping given large datasets paired documents summaries model exposed summaries training zero shot summarization ther unlike original cycle gan model ages non differentiable nator distinguish real generated discrete language domain relies force williams radford et al trained large language model large web text dataset found model produce zero shot summaries prompted document followed considered rudimentary usable historically strong parallels development neural sequence transduction models translation summarization relying avor sequence sequence learning depart signicantly recent unsupervised figure model architecture summae backbone denoising auto encoder encoder enc maps paragraphs sentences common space decoded coder dec conditioning different sequence tokens training add noise text sequences mapping age enc learn robust text representations translation work lample et al artetxe et al models exposed source target sequences unpaired work models learn produce target quences e summarize having posed source sequences documents training model methods architecture summarization model summae depicted figure consists denoising encoder enc dec capable auto encoding text sequences t sentences particular encoder enc t graphs rzdim deterministic function parameterized e mapping text latent vector tion rzdim input sequence t add random noise t described tion t t consider encoder implementations bidirectional rnn schuster paliwal z derived rnn s nal hidden state h followed afne transformation h w h transformer stacked n cal transformer encoder blocks denoted trfenc vaswani et al z derived output sentation rst token devlin et al followed afne transformation x standard cross entropy loss input sequence reconstruction optimize gradient descent forcing williams t hl n h hn w h decoder parameterized g regressive generative model dening ity distribution t conditioned z condition decoder decode tence paragraph different sequence tokens indicated reconstructed input t obtained sampling token time special end sequence ken obtained t consider decoder implementations unidirectional rnn conditions z concatenating decoder input embeddings z time step transformer causal masked attention conditions adding z input embedding similar decoder vaswani et al decoder encoder attention cases avoid decoder encoder attention encourage semantic information sulated z dataset single example paragraph consisting sentences p auto encoder contributes tion loss terms sentences paragraph weighted s p e g si e g p e g approach summarize paragraphs sentence prompt decoder generate sentence conditioned latent vector z paragraph simply training rnn transformer auto encoder described generally fails section pothesize encoder learn map tences paragraphs separate regions tent space decoder recognize decode sentence paragraph tion based solely location z ignore nd result decoder ing paragraph prompted sentence phenomenon segregation ideally auto encoder learns higher level latent concept conveyed paragraphs tences disentangled original sion paragraphs sentences explicitly encourage investigated adding sarial discriminator critic d trained classify latent vector z encoded sentence paragraph words learns t sentence paragraph auto encoding loss augmented encourage fooling inator classifying paragraphs sentences similar approaches style fer hu et al shen et al romanov et al unsupervised machine tion lample et al stractive summarization details tation found appendix experiments found adding critic effective generating sentence short summaries decoder congurations critic found unnecessary harmed performance discuss section adding noise text use denoising auto encoder standard auto encoder reconstructing text quence noisy version denoising seen useful self supervised objective improving representations seen devlin et al serves form data tion effectively increasing number training examples nally discourages merely learning identity function having reduce information bottleneck zdim small value employ techniques adding noise randomly masking tokens similar devlin et al song et al randomly mask input sequence tokens training feeding encoder instead predicting masked tokens generate noised sequence apply masking following procedure select sequences mask probability ps sequences unpermuted test time selected sequences replace token mask token probability pm permuting order sentences graphs token masking observed failure mode latent representation paragraph overly focuses rst sentence paragraph memorizes best sentence memorize purpose constructing paragraph encourage learning structure coherence paragraphs rst sentence probability pperm permute order sentences paragraph train auto encoder recover original paragraph pre training encoder decoder motivated recent success self supervised language representation learning peters et al howard ruder radford et al devlin et al song et al zhang et al yang et al propose eral strategies pre train encoder coder optimizing jointly auto encoding objective equation gies applied jointly pre training phase adding corresponding losses adopt paradigm pre training followed ne tuning signicant past work differences work beled data available downstream tasks ne tuning supervised work pre training ne tuning supervised additionally previous work pre trained models extremely large pora different downstream datasets model learns encoder pre training corrupted graph prediction summarizing paragraph quires understanding sentences follow form coherent narrative age good paragraph representations propose novel pre training task classifying graph corrupted swapping order th sentence sentence mented adding logistic regression layer encoder paragraph output z minimizing cross entropy error paragraphs rupted unmodied refer task corrupted paragraph prediction cpp encoder pre training sentence diction propose pre training tive encouraging encoder understand sentences follow paragraph jective referred sentence paragraph nssp shares idea bert s sentence prediction nsp objective devlin et al classify sentences adjacent modied difcult bert sample sentence pairs b time b follows negatives negative pairs sampled paragraph instead corpus ing sentences paragraph usually similar topic served harder negative sampling leads ter downstream summarization performance implementation differs lowing way bert s nsp bert input sequence constructed concatenating b separated sep token better differentiate b bert employs segment dings contrast encode b pendently avoiding need separator token segment embeddings bert binary multi layer perceptron instead added pre training troducing extra parameters directly dot product sentence resentations followed sigmoid function b t tried pre training large corpora nt help restricts capacity er forcing relevant features z implementation simpler generic tied specic encoder e bert transformer architecture contrast cpp learns encode tence relationships paragraph representations nssp taught encode sentence relationships individual sentence representations decoder pre training auto regressive guage modeling pre train decoder standard auto regressive language modeling lm objective similar ramachandran et al implement setting z equation regardless input sequences pre training decoder receives tioning signal teacher forcing making process equivalent standard lm task experiments metrics found human summaries stractive unfairly punished rouge x precision variants report recall scores lin truncated generated summaries rst sentence word limit prevent ferring models overly long summaries ilar rush et al byte limit limitations rouge known particular favor extractive ods paraphrasing synonyms warded fairly time writing rouge remains dominant metric tion research kryscinski et al treated summaries evaluation example equally report average rouge scores test examples validation examples hyper parameter tuning observed little ference averaged validation test scores baselines extractive documents consist ve sentences example ve possible sentence extractive summaries computed performance selecting sentence summary denote extract computed imum achievable sentence extractive score lecting best performing sentence example evaluated based recall human summaries refer extract oracle method cheats looking test data actually achievable practice useful estimate ceiling extractive methods human estimated human performance computing maximum average rouge pairs human summaries evaluation example took mean examples meansum single designed document summarization adapted sum chu liu single document case treating sentence document similar isonuma et al called meansum single meansum erates summaries shape documents adaptation generates summaries training details hyper parameters text tokenization data driven word tokenizer sennrich et al cabulary size convert text integer id tokens followed dim embedding layer shared input output embedding layers press wolf architecture experimented base encoder decoder congurations summae rnn rnn trf trf trf rnn rnnenc trf stands transformer rnndec single layer grus chung et al hdim rnnenc bidirectional rnndec unidirectional trfenc stack transformer encoder ers consisting self attention layer attention heads point wise forward network hidden units trfdec hyper parameter setting trfenc composed transformer decoder layers zdim set model weights ing embeddings initialized randomly coding time step performed greedily arg max optimization performed gradient descent adam optimizer kingma ba learning rate batch size models trained early stopping ing maximum validation recall rnn rnn critic rnn rnn critic c trf rnn lm pre training critic figure d t sne visualizations summae latent space blue circle corresponds sentence red triangle corresponds paragraph best viewed color number pre training steps set pre training employed added model able generate sentence short summaries critic discriminator plemented multi layer perceptron hidden units token mask added set ps pm permuting tences paragraphs set pperm critic noise added tuning phase experimented parameters constructing summae found reported setting worked best empirically different results discussion table shows rouge scores summary lengths human extractive baselines summae enhancements described tion base encoder decoder tions rnn rnn trf trf trf rnn ken masking paragraph shufing added noise pre training critic added human extractive baseline performance best extractive sentences unsurprisingly rst sentences introducing subject revealing ending extractive acle denition best performance considerably higher xed index tence extract estimated human performance higher extractive oracle suggesting abstractive gap effectively restricts summary length critic base encoder decoder congurations tended generate overly long invalid summaries shown number words close sentences exceeding indicating equation likely ignored critic validate hypothesis segregation latent space underlying problem ing long summaries visualized latent space summae rnn rnn similar plots trf trf rnn d t sne maaten hinton critic figure critic sentence paragraph representations mapped completely separate regions figure adversarial feedback critic effectively merged clusters supporting sis effect lm pre training seen table lm pre training improved model performance keeping summaries short encoder decoder tions boosted scores tively qualitatively found models lm pre training generated uent summaries best encoder decoder conguration ingly observed trf rnn outperformed congurations use quential architecture encoder decoder similar results reported chen et al found hybrid model composed transformer encoder rnn decoder worked best machine translation possible reason decoder need attend long term dependencies transformers major advantages rnns surprisingly found trf rnn ant enhanced lm pre training quire critic generate sentence summaries table rouge recall scores generated summary lengths number words sentences estimated human performance extractive baselines meansum single variants summae default summae models incorporate token masking paragraph shufing critic pre training trf stands transformer numbers bold denote best performed models category based rouge numbers italics denote models qualied generating sentence summaries superscript letters b c d denote model pairs compared human evaluations table model rouge l num words num sentences e v t c r t e e v t c r t s b human average human maximumd extract extract extract extract extract extract oracle summae rnn rnn critica lm pre trainingb c summae trf trf critic lm pre training summae trf rnn critic lm pre training lm pre training token masking paragraph shufing cpp pre training nssp pre trainingc meansum singlea token masking prevented segregation decoder congurations figure visualizes corresponding latent space paragraph sentence representations mapped region suggesting lm pre training effect critic specic ration outperformed model critic lm pre training vs effect token masking paragraph ing default token masking graph shufing introduced section added noise base summae models auto encoding removing degraded performance score decreased token masking removed paragraph shufing moved experimentation found adding noise produced stable results different runs effect nssp cpp pre training summae trf rnn decoder pre training surpassed best extractive sentence tract encoder pre training ther nssp cpp observed ment respectively best summae model constructed transformer encoder rnn decoder pre trained nssp lm objectives lowed denoising auto encoding masked shufed input sequences achieved rouge l signicantly outperformed xed index extractive sentences comparable extract oracle looks test data human evaluation model summaries validate making progress rouge rics correlates making real progress judged humans conducted model comparisons covering range rouge scores low high end amazon chanical turk workers workers presented sentence story paragraph model summaries asked rate dimensions uency information vance minimize inter rater noise scores table human evaluation results comparing model summary pairs uency information relevance superscript letters b c d correspond models table denotes statistical signicance binomial tailed test null hypothesis models equally good model preference rnn rnn critic meansum single noise b rnn rnn critic lm rnn rnn critic c trf rnn lm nssp rnn rnn critic lm d human trf rnn lm nssp fluency information distinct workers collected example averaged aggregated results random examples test set results ing average preference workers dimensions presented table observed uency improved icantly meansum single model rnn models formation aspect continued improve best model trf rnn nssp language model pre training human formance far better dimensions compared additional model samples viewed appendix figures addition decoding sentence graph representation found informative look reconstructed paragraphs encoder included figure paragraph reconstructions coherence disuencies factual curacies common neural generative models summaries decoded latent vector reconstructions proving lead accurate maries conclusions introduce rocsumm new benchmark task zero shot unsupervised abstractive rization uas useful iterating measuring progress challenging lem rst works ing single document uas propose novel neural model based denoising auto encoder self supervised pre training techniques enhancing model performance far humans summae outperforms extractive baselines major step uas code data release provided code reproduce experimental setup google research google tree master summae include code process rocstories dataset rocsumm train test splits human summaries validation test evaluation amazon mechanical turk templates data collection evaluation include model training evaluation code model weights best model references mikel artetxe gorka labaka eneko agirre kyunghyun cho unsupervised neural chine translation iclr christos baziotis ion androutsopoulos ioannis stas alexandros potamianos seq differentiable sequence sequence sequence autoencoder unsupervised abstractive sentence compression naacl hlt mia xu chen orhan firat ankur bapna melvin johnson wolfgang macherey george foster llion jones mike schuster noam shazeer niki parmar ashish vaswani jakob uszkoreit lukasz kaiser zhifeng chen yonghui wu macduff hughes best worlds combining recent advances neural machine translation acl eric chu peter liu meansum neural model unsupervised multi document abstractive summarization icml junyoung chung caglar gulcehre kyunghyun cho yoshua bengio empirical evaluation gated recurrent neural networks sequence eling nips deep learning representation learning workshop jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language standing naacl hlt alexander fabbri irene li tianwei suyi li dragomir radev multi news large scale multi document summarization dataset tive hierarchical model proceedings annual meeting association tional linguistics pages florence italy association computational linguistics rafael luciano ferreira souza cabral rafael dueire lins gabriel pereira e silva fred freitas george cavalcanti rinaldo lima steven simske luciano favaro ing sentence scoring techniques extractive text summarization expert systems applications thibault fevry jason phang vised sentence compression denoising encoders conll ian goodfellow jean pouget abadie mehdi mirza bing xu david warde farley sherjil ozair aaron courville yoshua bengio generative versarial nets nips vishal gupta gurpreet singh lehal survey text summarization extractive techniques journal emerging technologies web gence jeremy howard sebastian ruder universal language model ne tuning text classication acl zhiting hu zichao yang xiaodan liang ruslan salakhutdinov eric xing trolled generation text icml masaru isonuma junichiro mori ichiro sakata unsupervised neural single document marization reviews learning latent discourse structure ranking acl eric jang shixiang gu ben poole gorical reparameterization gumbel softmax iclr rafal jozefowicz oriol vinyals mike schuster noam exploring arxiv preprint shazeer yonghui wu limits language modeling diederik kingma jimmy ba adam method stochastic optimization iclr wojciech kryscinski nitish shirish keskar mccann bryan caiming xiong richard socher neural text summarization critical evaluation emnlp guillaume ludovic denoyer lample marcaurelio ranzato unsupervised machine translation monolingual corpora iclr guillaume lample myle ott alexis conneau dovic denoyer al phrase based ral unsupervised machine translation ings conference empirical methods natural language processing pages chin yew lin rouge package automatic evaluation summaries workshop text marization branches laurens van der maaten geoffrey hinton visualizing data t sne journal machine learning research nasrin mostafazadeh nathanael chambers xiaodong devi parikh dhruv batra lucy vanderwende pushmeet kohli james allen corpus evaluation framework deeper understanding commonsense stories naacl hlt nasrin mostafazadeh michael roth annie louis nathanael chambers james allen sem shared task story cloze test eacl workshop linking models lexical sentential discourse level semantics ramesh nallapati bowen zhou cicero dos santos c glar bing xiang tive text summarization sequence sequence rnns conll shashi narayan shay cohen mirella lapata nt details summary topic aware convolutional neural networks treme summarization emnlp matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word resentations naacl hlt press lior wolf output bedding improve language models eacl alec radford karthik narasimhan tim salimans ilya sutskever improving language standing generative pre training technical port openai alec radford jeffrey wu rewon child david luan dario amodei ilya sutskever language models unsupervised multitask learners cal report openai prajit ramachandran peter liu quoc le unsupervised pretraining sequence sequence learning emnlp alexey romanov anna rumshisky anna rogers david donahue adversarial decomposition text representation naacl hlt alexander rush sumit chopra jason weston neural attention model abstractive tence summarization emnlp mike schuster kuldip paliwal bidirectional ieee transactions recurrent neural networks signal processing abigail peter liu christopher manning point summarization generator networks acl rico sennrich barry haddow alexandra birch neural machine translation rare words subword units acl tianxiao shen tao lei regina barzilay tommi jaakkola style transfer non parallel text cross alignment nips kaitao song xu tan tao qin jianfeng lu yan liu mass masked sequence quence pre training language generation icml ilya sutskever oriol vinyals quoc le quence sequence learning neural networks nips ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need nips pascal vincent hugo larochelle yoshua bengio pierre antoine manzagol extracting composing robust features denoising coders icml yaushian wang hung yi lee learning encode text human readable summaries generative adversarial networks emnlp ronald williams simple statistical following algorithms connectionist ment learning machine learning zhilin yang zihang dai yiming yang jaime bonell ruslan salakhutdinov quoc le xlnet generalized autoregressive pretraining language understanding neurips zhengyan zhang xu han zhiyuan liu xin jiang maosong sun qun liu ernie hanced language representation informative tities acl jun yan zhu taesung park phillip isola alexei efros unpaired image image tion cycle consistent adversarial networks iccv appendix amazon mechanical turk settings figure shows detailed instructions rocstories summarization task task user interface rewarded summary restricted worker requirements following hit approval rate location united states number hits approved worker requirements human evaluation experiment discussed section templates available code repository google research google tree master summae training adversarially regularized auto encoder model architecture augmented critic depicted figure critic implemented multi layer perceptron mlp den layer parameterized d trained standard classication loss d d t p randomly sampled sentence auto encoder loss augmented aging encoder fool d classifying graph vectors sentences e e g e g e generative adversarial networks fellow et al player max game practice alternate training autoencoder respect encoder coder parameters e g tor respect d unlike gans generate data training domain case textual sequences instead operate continuous latent space allowed maintain differentiability algorithm model training loop critic collection paragraphs p input p p step step step npretrain update e g gradient descent e g eq step update d gradient descent d eq update e g gradient descent e g eq end end output auto encoder parameters e g figure amt task instructions figure auto encoder model augmented critic story paragraph summarize ben decided walk stepped outside shivered weather turned cold overnight ben went sweater ready walk human different workers ben layers noticing cold went walk ben went outside realized cold sweater ben layers noticing cold went walk meansum single noise ben decided rnn rnn critic ben decided clean glass rnn rnn critic lm ben decided outside dark trf rnn nssp lm best model ben decided outside awhile ready ride shivering figure story summaries produced different models human evaluation table sample story bill bullied bully star athlete day bill pushed bully fell wrong broke ankle ruined athletic career summary bill bullied bully broke ankle broke reconstruction bill bullied day bully bullied bill beat opponent ball broke bill crushed ankle laced sample story antonio happy new pizza place great business pizza place edge freshest best ingredients pizza place long time summary antonio happy best pizza place great time place reconstruction antonio happy best pizza place favorite place great time best friend extra place place best pizza place place good time pizza sample story charles sure nt qualify auto loan years old nt car lunch break met nissan car salesman salesman invited charles apply car loan charles approved salesman sold nissan summary charles salesman told needed loan dealership buy car approved reconstruction charles salesman wanted quit charles applied job wanted car charles applied loan asked pay loan charles offered loan asked quit charles salesman told needed loan sample story sue nt feeling stayed bed day sue nt eaten day barely drank sue got dehydrated summary sue nt feeling bed barely drank reconstruction sue nt feeling drank night sue got cold sue felt refreshed sample story boss asked nd mean value data set told mean appropriate data set asked explain appropriate told data symmetrical thanked brilliant insight problem summary boss asked mistake data entry pleased nd reconstruction boss asked mistake data entry told expectation making special told wrong size told decision careful told perfect decision sample story bob got new puppy bob s cat like new puppy fought constantly eventually bob s pets familiar bob s new puppy cat eventually best friends summary bob got new puppy cat happy new puppy reconstruction bob got new puppy cute little girl got best cat d met kids eventually got new puppy person lonely sample story nathan wanted computer programmer code everyday school eventually got degree computer science hired microsoft help program new operating system nathan big contribution microsoft s development team summary nathan wanted great job company s computer able improve new role reconstruction nathan wanted programmer worked hard new company hired improve computer hired sponsor month able company able improve company sample story fred fever able work work boss angry got red summary fred fever able work work got red reconstruction fred fever able work got work mad able figure summaries generated best summae model reconstructed stories
