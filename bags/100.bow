n u j l c s c v v x r improving semantic relevance sequence sequence learning chinese social media text summarization shuming xu jingjing houfeng wenjie qi key laboratory computational linguistics peking university electronics engineering computer science peking university computing hong kong polytechnic university foreign languages peking university shumingma xusun jingjingxu wanghf edu cn polyu edu hk abstract current chinese social media text rization models based decoder framework ated summaries similar source texts literally low semantic relevance work goal improve mantic relevance source texts summaries chinese social media marization introduce semantic evance based neural model encourage high semantic similarity texts summaries model source text represented gated attention coder summary representation produced decoder similarity score tions maximized training experiments proposed model outperforms baseline systems social media corpus introduction text summarization produce brief mary main ideas text long normal documents extractive summarization achieves satisfying performance selecting sentences source texts radev et al woodsend lapata cheng lapata apply chinese social media text summarization texts comparatively short noise fore abstractive text summarization based encoder decoder framework better choice rush et al hu et al extractive summarization selected tences high semantic relevance text abstractive text tion current models tend produce grammatical text night people caught ke ight china united airlines chendu beijing later ight ly landed taiyuan airport ers asked security check denied captain led collision en crew passengers rnn china united airlines exploded airport leaving people dead gold people smoked ight led collision crew passengers figure example rnn generated high similarity text literally mary low semantic relevance coherent summaries regardless semantic relevance source texts figure shows summary generated current model rnn encoder decoder similar source text ally low semantic relevance work goal improve tic relevance source texts generated summaries chinese social media text rization achieve goal propose mantic relevance based neural model model similarity evaluation component duced measure relevance source texts generated summaries training mizes similarity score encourage high mantic relevance source texts maries representation source texts duced encoder summaries computed decoder introduce gated attention encoder better represent source text decoder generates summaries provide summary representation iments proposed model better performance baseline systems social media corpus background chinese abstractive text summarization current chinese social media text summarization model based encoder decoder framework encoder decoder model able compress source texts xn continuous tor representation encoder erate summary y ym coder previous work hu et al encoder bi directional gated recurrent neural network maps source texts sentence vector hn decoder directional recurrent neural network duces distribution output words yt vious hidden state word sof tmaxf f recurrent neural network output tion hidden state encoder hn attention mechanism introduced source ter texts bahdanau et al attention vector ct represented weighted sum encoder hidden states information capture context ct tihi n x ti hi n hj p hi relevant score coder hidden state st encoder hidden state hi predicting output word decoder takes account attention vector contains alignment information source texts summaries proposed model assumption source texts maries high semantic relevance posed model encourages high similarity y z x y attention cos b c figure semantic relevance based neural model consists decoder encoder cosine similarity function representations figure shows posed model model consists nents encoder decoder similarity function encoder compresses source texts tic vectors decoder generates summaries produces semantic vectors generated summaries finally similarity function uates relevance sematic vectors source texts generated summaries ing objective maximize similarity score generated summaries high semantic relevance source texts text representation methods represent text sentence mean pooling rnn output reserving state rnn model source text represented gated attention coder hahn keller upcoming word fed gated attention network measures importance gated attention work outputs important score ward network time step inputs word vector et previous context vector ht outputs score t word vector et multiplied score t fed rnn coder select output hn rnn coder semantic vector source text vt natural idea semantic vector summary feed encoder method wastes time encode sentence twice actually output sm contains information source text generated summaries simply pute semantic vector summary tracting hn sm vs sm hn previous work proved effective represent span words encoding wang chang semantic relevance goal compute semantic relevance source text generated summary given tic vector vt vs use cosine larity measure semantic relevance represented dot product magnitude vt vs vt kvskkvtk source text summary share language reasonable assume semantic vectors distributed space cosine similarity good way measure distance vectors space training given model parameter input text model produces corresponding summary y mantic vector vs vt objective mize loss function l vt conditional probability summaries given source texts computed encoder decoder model vt cosine similarity semantic vectors vs vt term tries maximize semantic relevance source input target output experiments section present evaluation model performance popular cial media corpus use case plain semantic relevance generated summary source text dataset million text summary pairs structed famous chinese social media site called sina split parts pairs pairs ii pairs iii summary pairs ii iii ually annotated relevant scores ranged reserve pairs scores following previous work use training set ii development set iii test set experiment setting alleviate risk word segmentation takes xu sun use chinese ter sequences source inputs target puts limit model vocabulary size covers common characters character represented random ized word embedding tune parameter development set model ding size hidden state size decoder size gated attention work use adam optimizer learn model parameters batch size set parameter encoder decoder based lstm unit ing previous work hu et al uation metric f score rouge rouge l lin hovy baseline systems rnn denote rnn basic sequence sequence model bi directional gru encoder uni directional gru decoder widely language generated framework portant baseline rnn context rnn context sequence sequence framework neural attention tion mechanism helps capture context tion source texts model stronger line system results discussions compare model baseline tems including rnn rnn context refer proposed semantic relevance based neural model srb srb gated tion encoder denoted attention table dataset large scale chinese short text summarization dataset lcsts structed hu et al dataset consists sina com model rnn w hu et al rnn c hu et al rnn context w hu et al rnn context c hu et al rnn context srb c attention c rouge l table results model baseline systems models achieve substantial improvement rouge scores baseline systems w word level c character level text bat ppspptv careful calculation ssful internet companies shanghai giant company like bat reason internet ies listed companies ing tax merged ay tudou pps pptv yihaodian satised segment market years gold shanghai comes giant company rnn context shanghai s giant company srb shanghai giant companies figure example rnn generated mary lcsts corpus shows results models baseline tems srb outperforms rnn rnn context f score rouge l concludes srb generates key words phrases gated attention encoder srb achieves better formance f score rouge l shows gated attention reduces noisy unimportant formation remaining information resents clear idea source text better representation encoder leads better model level rnn context word hu et al char word gu et al char char copynet work r l table results model state art systems copynet incorporates copying anism solve vocabulary problem higher rouge scores model incorporate mechanism currently ture work implement technic ther improve performance word word level char character level f score f score r l f score rouge l tic relevance evaluation similarity function srb gated attention encoder able generate summaries high semantic evance source text figure example semantic evance source text summary shows main idea source text reason shanghai giant pany rnn context produces shanghai s giant companies literally similar source text srb generates shanghai giant companies closer main idea semantics concludes srb produces maries higher semantic similarity texts table summarizes results model state art systems copynet est socres incorporates copying anism deals vocabulary word lem paper implement anism model future work try incorporates copying mechanism model solve vocabulary problem related work abstractive text summarization achieved cessful performance thanks sequence sequence model sutskever et al tention mechanism bahdanau et al rush et al rst attention based coder compress texts neural network guage decoder generate summaries ing work recurrent encoder introduced text summarization gained better mance lopyrev chopra et al wards chinese texts hu et al built large corpus chinese short text summarization deal unknown word problem nallapati et al proposed generator pointer model decoder able generate words source texts gu et al solved issue corporating copying mechanism ayana et al proposes minimum risk training method optimizes parameters target rouge scores neural attention model work related neural attention rst model posed bahdanau et al methods improve neural attention model jean et al luong et al accelerate training process sun conclusion work aims improving semantic relevance generated summaries source texts nese social media text summarization model able transform text summary dense vector encourage high similarity representation experiments model outperforms baseline systems erated summary higher semantic relevance acknowledgements work supported national high technology research development program china program national natural science foundation china xu sun corresponding thor paper references ayana shiqi shen zhiyuan liu maosong sun neural headline generation minimum risk training corr dzmitry bahdanau kyunghyun cho yoshua neural machine translation corr bengio jointly learning align translate jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings annual meeting sociation computational linguistics acl august berlin germany volume long papers sumit chopra michael auli alexander m rush abstractive sentence summarization tentive recurrent neural networks naacl hlt conference north american chapter association computational guistics human language technologies san diego california usa june pages jiatao gu zhengdong lu hang li victor o k incorporating copying mechanism li proceedings sequence sequence learning annual meeting association computational linguistics acl august berlin germany volume long papers michael hahn frank keller modeling man reading neural attention ings conference empirical ods natural language processing emnlp austin texas usa november pages baotian hu qingcai chen fangze zhu sts large scale chinese short text summarization dataset proceedings conference empirical methods natural language ing emnlp lisbon portugal september pages sebastien jean kyunghyun cho roland memisevic yoshua bengio large target vocabulary neural machine translation proceedings annual meeting association computational linguistics international joint conference natural guage processing asian federation natural language processing acl july beijing china volume long papers pages chin yew lin eduard h hovy matic evaluation summaries n gram occurrence statistics human language ogy conference north american chapter association computational linguistics naacl edmonton canada june konstantin lopyrev generating news corr lines recurrent neural networks thang luong hieu pham christopher d ning effective approaches attention based proceedings neural machine translation conference empirical methods natural language processing emnlp lisbon gal september pages ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre bing xiang abstractive text summarization sequence proceedings sequence rnns signll conference computational natural language learning conll berlin germany august pages dragomir r radev timothy allison sasha goldensohn john blitzer arda c elebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu jahna otterbacher hong qi horacio saggion simone teufel michael topper adam winkel zhu zhang mead platform multidocument multilingual text summarization proceedings fourth international ence language resources evaluation lrec lisbon portugal alexander m rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing emnlp lisbon portugal september pages xu sun asynchronous parallel learning ral networks structured models dense tures coling international ence computational linguistics proceedings conference technical papers december osaka japan pages ilya sutskever oriol vinyals quoc v le sequence sequence learning neural works advances neural information ing systems annual conference neural formation processing systems december montreal quebec canada pages wenhui wang baobao chang graph based dependency parsing bidirectional lstm proceedings annual meeting sociation computational linguistics acl august berlin germany volume long papers kristian woodsend mirella lapata matic generation story highlights acl proceedings annual meeting sociation computational linguistics july uppsala sweden pages jingjing xu xu sun dependency based gated recursive neural network chinese word mentation meeting association putational linguistics pages
