salience estimation multi attention learning abstractive text summarization piji li lidong zhongyu wei wai lam department systems engineering engineering management chinese university hong kong tencent ai lab shenzhen china damo academy school data science fudan university china cuhk edu hk inc com edu abstract attention mechanism plays dominant role sequence generation models improve performance machine translation abstractive text summarization different neural machine translation task text summarization salience mation words phrases sentences ical component output summary distillation input text cal attention mechanism conduct text ment selection input text conditioned decoder states gap duct direct effective salience detection bring direct salience estimation marization neural networks propose multi attention learning framework contains new attention learning nents salience estimation supervised tion learning unsupervised attention ing regard attention weights salience information means mantic units large attention value important context information tained based estimated salience porated typical attention mechanism decoder conduct summary tion extensive experiments mark datasets different languages strate effectiveness proposed work task abstractive summarization introduction sequence sequence framework attention mechanism achieved signicant provement eld neural machine translation bahdanau et al encouraged come researchers transplanted framework tackle problem abstractive text summarization rush et al chopra et al nallapati et al obtained encouraging results abstractive text summarization bloomed popular research task based works proposed example et al integrated copy operation gu et al vinyals et al coverage model tu et al typical attention based generate better summaries li et al designed recurrent generative decoder capture latent structures target summaries paulus et al employed deep reinforcement learning techniques enhance performance task frameworks improve quality generated abstractive summaries extent immerse designing dazzling complex tricks model tionally ignore important characteristics cic task text summarization way summarization research salience detectionnding important information words phrases sentences source text crucial essential component supervised ng et al wang et al unsupervised erkan radev mihalcea tarau ing methods proposed estimate salience score producing better summaries attention based framework straightforward gure conduct salience detection current attention nism summarization task natural effective tasks instance neural machine translation reasonable use current decoding state attend source sequence relevant information lating target word reading sion makes sense use question attend reading passage retrieve relevant information extracting answer text summarization difcult connect attention mechanism r l c s c v v x r salience estimation operation works tried strategies conduct salience detection exist tions example selective mechanism zhou et al implicitly performs salience tection graph based attention mechanism tan et al adopts unsupervised method capable exploit supervised nal training data paper propose global attention mechanisms based supervised learning unsupervised learning respectively salient formation detection supervised tion mechanism employ supervised learning method estimate probability word input text included generated summary normalized probability value garded supervised attention signal unsupervised attention mechanism inspired pagerank page et al based text tion methods lexrank erkan radev textrank mihalcea tarau graph based attention mechanism tan et al employ pagerank rithm estimate salience score input word regarded unsupervised tion signal types attention nals contain salience information terms source text examine efcacy obtained salience information integrate signals simple base model abstractive summarization e attention based model note employ ticated powerful models aim work verify bringing salience estimation neural abstractive summarization helpful improve performance ple base model allows conclusion biased modeling structures main contributions summarized lows investigate crucial element text summarization problem salience tion overlooked prior neural abstractive summarization approaches pose supervised attention mechanism directly estimate salience supervision signal provided state input text supervised attention mechanism employs graph algorithm estimate salience input word integrate types tention information base model propose unied neural network based framework named multi attention learning mal tackle task abstractive summarization experimental results benchmark datasets different languages demonstrate effectiveness posed attention learning methods salience mation framework overview proposed multi attention learning mal framework shown figure input variable length sequence x xm representing source text output ground truth sequence y denote generated summary sequence global salience mation add tailor attention learning mechanisms supervised attention learning supervised attention learning aim vised attention learning predict words input source text selected generated summaries e predicting bel word shown figure word embeddings e encoder recurrent neural work rnn hidden states h taken input information supervised attention learning modular design self attention model capture context information source text better feature representation learning output component regarded vised attention information unsupervised attention learning employ pagerank rithm estimate salience score input words unsupervised manner treat salience score unsupervised attention mation au types global attention information representing word salience bined hidden states h input source text obtain global attention context finally attention context information incorporated decoding procedure generate abstractive summaries supervised attention learning aim supervised attention learning timate probability words source text appear generated summaries sufcient training data regard problem supervised sequence labeling task employ straightforward method prepare ground figure multi attention learning mal framework abstractive summarization truth labels r source text words source text stopwords appear ground truth summaries annotated positive label words tions annotated negative label structure supervised attention learning framework illustrate computational logic rst states depicted left figure rst map input word xt vector xt rke retrieving embedding lookup table randomly initialized ne tuned training procedure word bedding sequence fed bi directional rnn capture context information compared lstm hochreiter schmidhuber gru cho et al comparable performance parameters efcient tion employ gru basic recurrent unit rt br zt bz gt bh ht zt zt gt rt reset gate zt update gate control mixture previous hidden gt current hidden ht rkh w b s learnable parameters denotes element wise multiplication tanh hyperbolic tangent activation function employ bidirectional gru network produce hidden states time step t gru xt gru xt overall hidden state encoder concatenation directions t ht order capture context information input sequence integrate self attention modeling component self attention weight time step t calculated based t source hidden states ship let ae j calculated j attention weight ae j ei j e j e hihe hjhe hi rkh ve rkh self attention context obtained weighted linear combination source hidden states hj ce t e t ae t e sequence length original den state t revised self attention context information ce t hhhe t chce t h attentionsupervised attentionrecurrent finally shown figure feed word t t self attention state t nal output layer prediction embedding vector xt hidden state attention context ce r t whr t wcrce r whrhe t r sigmoid function value r represents salience corresponding words source text order attention information attention context information rst add malization procedure predicted r e regard vector rt e attention information supervised based supervised attention information obtain type global attention context weighted linear combination source hidden states e cs finally cs incorporated decoder supervised attention context information summary generation unsupervised attention learning traditional text summarization research pagerank page et al based salience tion methods play crucial role identifying important information source text classical methods lexrank erkan radev textrank mihalcea rau proposed tackle problems text summarization keyphrase extraction applied practical tion applications products tan et al introduced graph based attention mechanism framework sentence salience estimation obtained encouraging results employ pagerank algorithm conduct unsupervised attention learning salience timation depicted middle upper figure difference conduct learning word level estimate salience input text sequence x length m xt rke representing embedding vector word xt build word based graph g nonstop words vertex set v relations words computed tion edge set e employ parameterized tensor method calculate weights edges assume adjacent matrix m rmm element calculated mi j wpxj wp rkeke neural parameter learned pagerank iterative algorithm closed form discussed tan et al p d diagonal matrix m damping factor q rm elements equal m vector rm estimated salience score m words add normalization procedure p au vector au rm regarded pervised attention information obtain second type global attention context weighted linear combination word dings au cu au cu incorporated framework unsupervised attention context information summary generation decoder mal framework gru based recurrent neural network improved tention modeling rst hidden state hd decoder initialized average source input hidden states hd m m layers grus designed conduct attention weights calculation decoder den states update rst gru layer den state calculated current input word embedding previous hidden state attention weights time step t calculated based relationship t source hidden states ad j e j ei j hhhe j ba attention context obtained weighted linear combination source hidden states t e cd output second gru layer jointly sidering word previous hidden state attention context cd t nal hidden state ad t t t cd t input sequence x ln ll log t x n nal objective loss function l ls ln ll framework trained multi task learning paradigm propagation method end end training style adadelta zeiler hyperparameters gradient based optimization traditional framework predict target word based t datasets experimental setup multi attention integration recall obtained supervised tion context cs section unsupervised attention context cu section grate attention context information straightforward manner t wd csacs wd t ha ha finally probability generating target word yt given follows cuacu bd yt hyha t bd hy hy rkykh bd hy rky wd softmax function prediction state use beam search algorithm koehn decoding generating best summary model training supervised attention learning use entropy objective function need minimized m ls ri ri ri ri ri prediction ground truth respectively summary generation employ tive log likelihood nll objective tion given ground truth summary y train evaluate framework ular benchmark datasets gigawords english sentence summarization dataset prepared based annotated extracting rst tence news report headline form source summary pair e rst tence headline directly download pared dataset rush et al roughly contains m training pairs k validation pairs test pairs test set identical comparative methods english dataset ing directly apply model trained gigawords contains documents document contains model summaries written experts length summary limited bytes lcsts large scale chinese short text summarization dataset consisting pairs short text summary collected sina hu et al training set ii development set iii test set score range beled human indicate relevance article summary use pairs scores parts contain m data points respectively experiments directly chinese character sequences input performing word segmentation ldc upenn edu nist gov weibo com evaluation metrics use rouge lin standard options evaluation metric idea rouge count number overlapping units tween generated summaries reference summaries overlapped n grams word quences word pairs f measures rouge l r l reported gigawords lcsts datasets rouge recalls reported duc dataset comparative methods compare mal bunch previous methods datasets standard extract results papers ported compared methods ent datasets slightly different topiary zajic et al best compressive text summarization combines system linguistic based transformations unsupervised topic detection algorithm compressive text summarization rush et al uses phrase based statistical machine translation system trained gigaword produce summaries abs rush et al neural network based models local attention modeling abstractive sentence rization rnn rnn context hu et al architectures rnn context grates attention mechanism model context copynet gu et al integrates copying mechanism framework distract chen et al uses new attention mechanism distracting historical attention decoding steps ras lstm ras elman chopra et al consider words word positions input use convolutional encoders handle source information attention based sequence decoding process ras elman lects elman rnn elman decoder ras lstm selects lstm architecture hochreiter schmidhuber lenemb kikuchi et al uses mechanism control summary length considering length embedding tor input miao blunsom uses generative model attention anism tackle sentence compression lem nallapati et al utilize trick control vocabulary size improve training efciency seass zhou et al integrates selective gated network system abs ras lstm ras elman asc seass sion drgd r l table rouge gigawords framework control mation ow encoder decoder drgd li et al proposes deep recurrent generative decoder enhance modeling ability latent structures target summaries experimental settings experiments english dataset words set dimension word embeddings dimension hidden states latent variables maximum length documents summaries tively maximum length summaries bytes dataset lcsts dimension word embeddings set dimension hidden states latent ables maximum length documents summaries chinese characters spectively damping factor d pagerank algorithm unsupervised attention learning set beam size decoder set neural network based framework mented theano theano development team results discussions rouge evaluation results english datasets gigawords shown table table respectively ablations version typical attention based framework implemented system topiary abs ras elman ras lstm lenemb seass sion drgd system rnn rnn context copynet rnn distract drgd sion drgd r l r l table rouge recall table rouge lcsts ablation method considering vised attention information considers unsupervised attention tion proposed framework experimental results mal framework performs better typical method strong parisons means multi attention text information improve mance typical summarization els worth noting methods utilize linguistic features parts speech tags named entity tags tf idf statistics words document representation generally useful features improve performance framework better strates effectiveness salience detection components results chinese dataset lcsts shown table mal achieves best performance copynet employs ing mechanism improve summary quality rnn distract considers attention information versity decoders drgd integrates recurrent variational auto encoder typical framework model better methods demonstrating effectiveness incorporation multi attention context information expectable integrating copying mechanism coverage diversity framework improve summarization performance highlight discussion note framework integrate attention information simple base model attention based model performance framework deed limited evaluation results good strong recent methods seass zhou et al pointer generator et al reinforced model paulus et al purpose work investigate performance applying ditional salience detection intuitions simple attention based framework ple base model allows conclusions biased modeling complications experimental analysis demonstrate effectiveness fore study paper reminds peer researchers crucial salience detection component summarization ine scope neural network based models presents practical approach solving problem types attention signals appropriately integrated recent models believe performance improved attention learning framework help revise design copy mechanism coverage ing strategy worthwhile directions investigate future works system suatt unatt giga duc lcsts table evaluation words tracted suatt unatt toyota rally world europe banned japan s toyota team europe banned world rally championship year friday crushing ruling world council international automobile tion golden toyota banned year suatt championship team ruling year unatt world council europe federation ternational japan ruling friday championship banned powerful bomb exploded outside navy base near sri lankan capital colombo tuesday seriously wounding person military ofcials said golden bomb attack outside srilanka navy base suatt sri bomb base navy colombo ankan powerful military wounding exploded unatt sri military capital tuesday bomb powerful navy exploded base ofcials palestinian prime minister ismail haniya insisted friday hamas led government continuing efforts secure release israeli soldier captured militants golden efforts underway secure dier s release hamas pm suatt palestinian release haniya hamas led soldier israeli government secure efforts ister unatt government prime palestinian friday israeli militants efforts continuing minister secure table words extracted suatt unatt receptively samples gigawords attention analysis regard supervised attention vised attention salience score words source text design experiments verify performance attention nisms nding important words input sequence au attention vectors obtained supervised attention learning supervised attention learning respectively ement value ai represents word salience score select k words input sequence according salience scores au intuitively extracted k words important large overlapping ground truth summary verify quantitatively regard words summaries conduct rouge evaluation order words ignored employ f measure score evaluation metric experimental results datasets given table set k results illustrate methods extract important words source text quality words tracted supervised attention e suatt better extracted unsupervised attention au e unatt adheres ition suatt method obtain stronger supervision signals unsupervised method unatt rouge results presented tables nd performance similar better phenomenon method suatt receive supervision signals guide training unatt unsupervised salience detection method nd complementary mation improve summarization formance order differences vividly present extracted words table words ranked based ing salience scores results know suatt unatt assign large salience scores important words instance suatt extract words toyoda banned year core elements golden summary toyota banned year result unatt diversied performance suatt unatt different integration performs quantitative evaluation experiments previous subsection different attention methods ture different aspects source text complement japan s toyota team europe banned world rally championship year friday crushing ruling world council international automobile tion golden toyota banned year toyota s world rally europe banned world rally championship mal toyota barred world rally pionship slovaks started voting day elections parliament centre right prime minister mikulas dzurinda ghting continue far reaching painful forms golden slovaks start voting legislative tions slovakia s parliament begins voting mal slovaks start voting early elections thai government set aside million baht million u s dollars support new eco tourism plans according report thai news agency tna tuesday golden tourism thailand support new eco tourism mal thailand support new eco tourism plans thai government support table examples generated summaries summary case analysis finally examples source texts golden summaries generated summaries typical attention based framework proposed mal framework shown table cases generated summaries mal generally better quality attention learning ponents salience detection framework ability assign small salience scores portant words uninformative symbols summary generated contains noisy symbols cases shown related works important content original text ument nenkova mckeown tional summarization methods classied categories extraction based methods erkan radev min et al based methods li et al wang et al li et al abstraction based methods barzilay mckeown bing et al recently researchers employ neural work based frameworks tackle abstractive summarization problem obtain encouraging performance rush et al proposed ral model local attention modeling trained gigaword corpus combined additional log linear extractive summarization model handcrafted features nallapati et al utilized trick control vocabulary size improve training efciency gu et al integrated copying mechanism framework improve quality generated summaries chen et al proposed new attention mechanism considers important source segments distracts decoding step order better grasp overall meaning input documents miao blunsom extended framework proposed generative model capture latent summary information zhou et al tegrated selective gated network framework control information ow encoder decoder li et al proposed deep recurrent generative decoder enhance modeling ability latent structures target summaries et al employed pointer networks converge mechanism improve quality generated summaries paulus et al proposed reinforcement learning based framework enhance performance rization chen et al proposes generative bridging network bridge module troduced assist training sequence diction model li et al employ actor critic training paradigm enhance quality generated summaries researchers combine traditional salience estimation methods frameworks order enhance marization performance tan et al automatic summarization process matically generating summary retains researchers regard compression approach special case extraction approach porated graph based attention information tained pagerank algorithm work hsu et al weighted attention mechanism sentence salience information calculated traditional supervised method contrast consider supervised salience information unsupervised salience information framework generate better summaries conclusions work investigate effect adding traditional salience detection text rization typical attention based framework abstractive summarization pose multi attention learning mal framework contains new attention learning nents supervised attention learning unsupervised attention learning salience mation salience information obtained based types attentions incorporated typical attention mechanism decoder conduct summary generation extensive periments benchmark datasets different languages demonstrate effectiveness posed framework task abstractive marization references dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly learning align translate iclr regina barzilay kathleen r mckeown tence fusion multidocument news tion computational linguistics lidong bing piji li yi liao wai lam weiwei guo rebecca passonneau abstractive document summarization phrase selection merging acl pages qian chen xiaodan zhu zhenhua ling si wei hui jiang distraction based neural networks document summarization ijcai pages wenhu chen guanlin li shuo ren shujie liu zhirui zhang mu li ming zhou tive bridging network neural sequence prediction naacl kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger learning schwenk yoshua bengio phrase representations rnn encoder decoder statistical machine translation emnlp pages sumit chopra michael auli alexander m rush seas harvard abstractive sentence marization attentive recurrent neural networks naacl hlt pages jeffrey l elman finding structure time nitive science gunes erkan dragomir r radev lexrank graph based lexical centrality salience text summarization journal articial intelligence search jiatao gu zhengdong lu hang li victor ok incorporating copying mechanism acl pages li sequence sequence learning sepp hochreiter jurgen schmidhuber neural computation long short term memory wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun ed model extractive abstractive arxiv preprint rization inconsistency loss baotian hu qingcai chen fangze zhu sts large scale chinese short text summarization dataset emnlp pages yuta kikuchi graham neubig ryohei sasano hiroya takamura manabu okumura ling output length neural encoder decoders emnlp pages philipp koehn pharaoh beam search coder phrase based statistical machine conference association tion models machine translation americas pages springer chen li fei liu fuliang weng yang liu document summarization guided sentence pression emnlp pages piji li lidong bing wai lam actor critic based training framework abstractive tion arxiv preprint piji li wai lam lidong bing weiwei guo hang li cascaded attention based unsupervised information distillation compressive tion emnlp piji li wai lam lidong bing zihao wang deep recurrent generative decoder emnlp pages stractive text summarization chin yew lin rouge package automatic text summarization evaluation summaries branches proceedings workshop volume yishu miao phil blunsom language latent variable discrete generative models tence compression emnlp pages david zajic bonnie dorr richard schwartz hlt naacl bbn umd topiary pages rada mihalcea paul tarau textrank ing order text emnlp matthew d zeiler adadelta adaptive ing rate method arxiv preprint qingyu zhou nan yang furu wei ming zhou selective encoding abstractive sentence summarization acl pages ziheng lin min yen kan chew lim tan exploiting category specic information document summarization coling pages ramesh nallapati bowen zhou caglar gulcehre bing xiang al abstractive text tion sequence sequence rnns arxiv preprint ani nenkova kathleen mckeown survey mining text text summarization techniques data pages springer jun ping ng praveen bysani ziheng lin min yen kan chew lim tan exploiting specic information multi document tion coling pages lawrence page sergey brin rajeev motwani terry winograd pagerank citation ing bringing order web technical report stanford infolab romain paulus caiming xiong richard socher deep reinforced model abstractive marization iclr alexander m rush sumit chopra jason weston neural attention model abstractive tence summarization emnlp pages abigail peter j liu christopher d manning point summarization generator networks acl volume pages jiwei tan xiaojun wan jianguo xiao abstractive document summarization based attentional neural model acl volume pages theano development team theano python framework fast computation mathematical pressions arxiv e prints zhaopeng tu zhengdong lu yang liu xiaohua liu hang li modeling coverage neural machine translation acl volume pages oriol vinyals meire fortunato navdeep jaitly pointer networks nips pages lu wang hema raghavan vittorio castelli radu rian claire cardie sentence pression based framework query focused acl pages document summarization
