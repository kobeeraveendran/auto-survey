salience estimation with multi attention learning for abstractive text summarization piji li lidong zhongyu wei wai lam department of systems engineering and engineering management the chinese university of hong kong tencent ai lab shenzhen china damo academy school of data science fudan university china cuhk edu hk inc com edu abstract attention mechanism plays a dominant role in the sequence generation models and has been used to improve the performance of machine translation and abstractive text summarization different from neural machine translation in the task of text summarization salience mation for words phrases or sentences is a ical component since the output summary is a distillation of the input text although the cal attention mechanism can conduct text ment selection from the input text conditioned on the decoder states there is still a gap to duct direct and effective salience detection to bring back direct salience estimation for marization with neural networks we propose a multi attention learning framework which contains two new attention learning nents for salience estimation supervised tion learning and unsupervised attention ing we regard the attention weights as the salience information which means that the mantic units with large attention value will be more important the context information tained based on the estimated salience is porated with the typical attention mechanism in the decoder to conduct summary tion extensive experiments on some mark datasets in different languages strate the effectiveness of the proposed work for the task of abstractive summarization introduction sequence to sequence framework with attention mechanism has achieved signicant provement in the eld of neural machine translation bahdanau et al encouraged by this come some researchers transplanted the framework to tackle the problem of abstractive text summarization rush et al chopra et al nallapati et al and also obtained some encouraging results since then abstractive text summarization has bloomed into a popular research task and quite a few based works have been proposed for example see et al integrated the copy operation gu et al vinyals et al and the coverage model tu et al into the typical attention based to generate better summaries li et al designed a recurrent generative decoder to capture the latent structures in the target summaries paulus et al employed deep reinforcement learning techniques to enhance the performance of this task the above frameworks can improve the quality of the generated abstractive summaries to some extent however when we immerse ourselves in designing such dazzling and complex tricks on top of the model we may tionally ignore some important characteristics cic to the task of text summarization along the whole way of summarization research salience detectionnding the most important information words phrases or sentences from the source put text has always been the most crucial and essential component some supervised ng et al wang et al or unsupervised erkan and radev mihalcea and tarau ing methods were proposed to estimate the salience score for producing better summaries however for the attention based framework it is not straightforward to gure out how to conduct salience detection the current attention nism for the summarization task is not as natural and effective as in some other tasks for instance in neural machine translation it is reasonable to use the current decoding state to attend the source sequence to get the relevant information for lating the next target word in reading sion it makes sense to use the question to attend the reading passage to retrieve relevant information for extracting the answer but for text summarization it is difcult to connect the attention mechanism r a l c s c v v i x r a with the salience estimation operation although several works have tried some strategies to conduct the salience detection there still exist some tions for example the selective mechanism zhou et al only implicitly performs salience tection the graph based attention mechanism tan et al only adopts an unsupervised method thus it is not capable to exploit the supervised nal in the training data in this paper we propose two global attention mechanisms based on supervised learning and unsupervised learning respectively for salient formation detection for the supervised tion mechanism we employ a supervised learning method to estimate the probability of each word in the input text to be included in the generated summary the normalized probability value is garded as the supervised attention signal for the unsupervised attention mechanism inspired by the pagerank page et al based text tion methods such as lexrank erkan and radev and textrank mihalcea and tarau as well as the graph based attention mechanism tan et al we employ the pagerank rithm to estimate the salience score of each input word which is regarded as the unsupervised tion signal thus these two types of attention nals contain the salience information of the terms in the source text to examine the efcacy of the obtained salience information we integrate these signals into a simple base model for abstractive summarization i e the attention based model note that we do not employ more ticated and powerful models because the aim of this work is to verify that bringing back salience estimation for neural abstractive summarization is helpful to improve the performance where a ple base model allows the conclusion not biased by other modeling structures our main contributions are summarized as lows we investigate a crucial element of text summarization problem namely salience tion which has been overlooked by the prior neural abstractive summarization approaches we pose a supervised attention mechanism to directly estimate the salience under the supervision signal provided by the state of the input text and an supervised attention mechanism which employs a graph algorithm to estimate the salience of each input word we integrate the two types of tention information into a base model and propose a unied neural network based framework named multi attention learning mal to tackle the task of abstractive summarization experimental results on some benchmark datasets in different languages demonstrate the effectiveness of the posed attention learning methods for salience mation our framework overview the proposed multi attention learning mal framework is shown in figure the input is a variable length sequence x xm representing the source text the output ground truth is also a sequence y we denote the generated summary sequence as for global salience mation we add two tailor made attention learning mechanisms supervised attention learning and supervised attention learning the aim of vised attention learning is to predict if words from the input source text should be selected into the generated summaries i e predicting a or bel for each word as shown in figure the word embeddings e and the encoder recurrent neural work rnn hidden states h are taken as the input information of this supervised attention learning modular we also design a self attention model to capture more context information from the source text for better feature representation learning the output of this component is regarded as the vised attention information as for unsupervised attention learning we employ the pagerank rithm to estimate the salience score of each input words in an unsupervised manner we treat the salience score as the unsupervised attention mation au then these two types of global attention information representing word salience are bined with the hidden states h of the input source text to obtain the global attention context finally the attention context information is incorporated in the decoding procedure to generate the abstractive summaries supervised attention learning the aim of supervised attention learning is to timate the probability of the words in the source text to appear in the generated summaries with sufcient training data we can regard this problem as a supervised sequence labeling task we employ a straightforward method to prepare the ground figure our multi attention learning mal framework for abstractive summarization truth labels r for the source text the words in the source text except the stopwords that appear in the ground truth summaries are annotated with the positive label all other words and the tions are annotated with the negative label the structure of the supervised attention learning framework we illustrate the computational logic with the rst two states is depicted on the left of figure we rst map each input word xt into a vector xt rke by retrieving an embedding lookup table which is randomly initialized and ne tuned in the training procedure the word bedding sequence is fed into a bi directional rnn to capture the context information compared with lstm hochreiter and schmidhuber gru cho et al has comparable performance but with less parameters and more efcient tion so we employ gru as the basic recurrent unit rt br zt bz gt bh ht zt zt gt where rt is the reset gate zt is the update gate to control the mixture of the previous hidden and gt to get the current hidden ht rkh and those w and b s are learnable parameters denotes the element wise multiplication and tanh is the hyperbolic tangent activation function we employ a bidirectional gru network to produce two hidden states at the time step t gru xt gru xt then the overall hidden state he encoder is a concatenation of both directions of the he t ht in order to capture more context information of the input sequence we integrate a self attention modeling component the self attention weight at the time step t is calculated based on the t and all the source hidden states he ship of he i let ae i and he j which can be calculated as i j be the attention weight between he ae i j ei j e j e hihe i we hjhe be a hi we where we a rkh and ve rkh then the self attention context is obtained by the weighted linear combination of all the source hidden states hj be ce t e t ae where t e is the sequence length the original den state he t can be revised using the self attention context information ce he t hhhe t we chce t be h attentionsupervised attentionrecurrent finally as shown in figure we feed the word t the t and the self attention state t into the nal output layer to get the prediction embedding vector xt the hidden state he attention context ce he r t whr t wcrce r whrhe he t be r where is the sigmoid function the value of r represents the salience of the corresponding words in the source text in order to get the attention information and the attention context information we rst add a malization procedure to the predicted r as i e we regard the vector as rt e attention information as the supervised based on the supervised attention information as we can obtain one type of global attention context by the weighted linear combination of the source hidden states e cs as finally cs is incorporated in the decoder as the supervised attention context information for the summary generation unsupervised attention learning in the traditional text summarization research the pagerank page et al based salience tion methods play a crucial role in identifying the most important information from the source text some classical methods such as lexrank erkan and radev and textrank mihalcea and rau were proposed to tackle the problems of text summarization and keyphrase extraction and have been applied into practical tion applications and products tan et al introduced the graph based attention mechanism into the framework for sentence salience estimation and obtained encouraging results here we also employ pagerank algorithm to conduct the unsupervised attention learning for salience timation as depicted in the middle upper part of figure the difference is that we conduct the learning on word level to estimate the salience for an input text sequence x with length m and xt rke representing the embedding vector for the word xt to build the word based graph g we take the nonstop words as the vertex set v and the relations between the words computed with tion as edge set e we employ a parameterized tensor method to calculate the weights of the edges assume that the adjacent matrix is m rmm then each element can be calculated as mi j i wpxj where wp rkeke is a neural parameter to be learned pagerank is a iterative algorithm but we can get the closed form as discussed in tan et al p where d is a diagonal matrix and i m i is the damping factor and q rm with all the elements equal to m then the vector rm is the estimated salience score for all the m words we also add a normalization procedure to p au i then the vector au rm is regarded as the pervised attention information we can also obtain the second type of global attention context by the weighted linear combination of the word dings using au cu au and cu will be incorporated with the framework as the unsupervised attention context information summary generation the decoder of our mal framework is still a gru based recurrent neural network with improved tention modeling the rst hidden state hd of the decoder is initialized using the average of all the source input hidden states hd he then m m the two layers of grus are designed to conduct the attention weights calculation and decoder den states update on the rst gru layer the den state is calculated only using the current input word embedding and the previous hidden state then the attention weights at the time step t are calculated based on the relationship of t and all the source hidden states he ad i j e j ei j i we hhhe j ba the attention context is obtained by the weighted linear combination of all the source hidden states t e cd is the output of the second gru layer jointly sidering the word the previous hidden state and the attention context cd t the nal hidden state ad t t t cd t for the input sequence x we have ln ll log t x n then the nal objective loss function is l ls ln ll the whole framework can be trained using the multi task learning paradigm with the propagation method in an end to end training style adadelta zeiler with hyperparameters and is used for gradient based optimization the traditional framework will predict the target word based on t datasets experimental setup multi attention integration recall that we have obtained the supervised tion context cs in section and the unsupervised attention context cu in section then we grate all the attention context information here in a straightforward manner t wd csacs wd t ha ha finally the probability of generating any target word yt is given as follows cuacu bd yt hyha t bd hy hy rkykh and bd hy rky is where wd the softmax function in the prediction state we use the beam search algorithm koehn for decoding and generating the best summary model training for supervised attention learning we use the entropy as the objective function which need to be minimized m i ls ri ri ri where ri and ri is the prediction and the ground truth respectively for summary generation we employ the tive log likelihood nll as the objective tion given the ground truth summary y we train and evaluate our framework on three ular benchmark datasets gigawords is an english sentence summarization dataset prepared based on annotated by extracting the rst tence from a news report together with the headline to form a source and summary pair i e the rst tence and headline we directly download the pared dataset used in rush et al it roughly contains m training pairs k validation pairs and test pairs the test set is identical to the one used in all the comparative methods is another english dataset only used for ing where we directly apply the model trained from gigawords it contains documents each document contains model summaries written by experts the length of the summary is limited to bytes lcsts is a large scale chinese short text summarization dataset consisting of pairs of short text and summary collected from sina hu et al we take part i as the training set part ii as the development set and part iii as the test set there is a score in the range of beled by human to indicate the relevance between an article and its summary we only make use of those pairs with scores no less than thus the three parts contain m and data points respectively in our experiments we directly take the chinese character sequences as input without performing word segmentation ldc upenn edu nist gov weibo com evaluation metrics we use rouge lin with standard options as our evaluation metric the idea of rouge is to count the number of overlapping units tween the generated summaries and the reference summaries such as overlapped n grams word quences and word pairs f measures of and rouge l r l are reported for gigawords and lcsts datasets rouge recalls are reported for the duc dataset comparative methods we compare our mal with a bunch of previous methods since the datasets are quite standard so we just extract the results from their papers if ported therefore the compared methods on ent datasets may be slightly different topiary zajic et al is the best on for compressive text summarization it combines a system using linguistic based transformations and an unsupervised topic detection algorithm for compressive text summarization rush et al uses a phrase based statistical machine translation system trained on gigaword to produce summaries abs and rush et al are both the neural network based models with local attention modeling for abstractive sentence rization rnn and rnn context hu et al are two architectures rnn context grates attention mechanism to model the context copynet gu et al integrates a copying mechanism into the framework distract chen et al uses a new attention mechanism by distracting the historical attention in the decoding steps ras lstm and ras elman chopra et al both consider words and word positions as input and use convolutional encoders to handle the source information for the attention based sequence decoding process ras elman lects elman rnn elman as its decoder and ras lstm selects lstm architecture hochreiter and schmidhuber lenemb kikuchi et al uses a mechanism to control the summary length by considering the length embedding tor as the input miao and blunsom uses a generative model with attention anism to tackle the sentence compression lem and nallapati et al utilize a trick to control the vocabulary size to improve the training efciency seass zhou et al integrates a selective gated network system abs ras lstm ras elman asc seass our sion drgd r l table rouge on gigawords into the framework to control the mation ow from encoder to decoder drgd li et al proposes a deep recurrent generative decoder to enhance the modeling ability of latent structures in the target summaries experimental settings for the experiments on the english dataset of words we set the dimension of word embeddings to and the dimension of hidden states and latent variables to the maximum length of documents and summaries is and tively for the maximum length of summaries is bytes for the dataset of lcsts the dimension of word embeddings is we also set the dimension of hidden states and latent ables to the maximum length of documents and summaries is and chinese characters spectively the damping factor d of the pagerank algorithm for the unsupervised attention learning is set to the beam size of the decoder is set to our neural network based framework is mented using theano theano development team results and discussions rouge evaluation the results on the english datasets of gigawords and are shown in table and table respectively among the ablations our version is the typical attention based framework implemented by us system topiary abs ras elman ras lstm lenemb seass our sion drgd system rnn rnn context copynet rnn distract drgd our sion drgd r l r l table rouge recall on table rouge on lcsts is the ablation method only considering vised attention information only considers unsupervised attention tion is our proposed framework from the experimental results we can see that our mal framework performs better than the typical method as well as some other strong parisons which means that the multi attention text information can indeed improve the mance of the typical summarization els it is worth noting that the methods and utilize linguistic features such as parts of speech tags named entity tags and tf and idf statistics of the words as part of the document representation generally more useful features can indeed improve the performance nevertheless our framework is still better than them which strates the effectiveness of our salience detection components the results on the chinese dataset lcsts are shown in table our mal also achieves the best performance although copynet employs a ing mechanism to improve the summary quality rnn distract considers attention information versity in their decoders and drgd integrates a recurrent variational auto encoder into the typical framework our model is still better than these methods demonstrating that the effectiveness of the incorporation of the multi attention context information it is expectable that integrating the copying mechanism and coverage diversity in our framework will further improve the summarization performance highlight discussion note that in our framework we integrate the attention information with a simple base model namely the attention based model thus the performance of the whole framework is deed limited and the evaluation results are not as good as some very strong recent methods such as seass zhou et al pointer generator see et al and the reinforced model paulus et al however the purpose of this work is to investigate the performance of applying the ditional salience detection intuitions in the simple attention based framework and such a ple base model allows the conclusions not biased by other modeling complications the experimental analysis can demonstrate its effectiveness fore our study in this paper not only reminds the peer researchers that the crucial salience detection component for summarization should be ine in the scope of neural network based models but also presents a practical approach to solving this problem if the two types of attention signals are appropriately integrated into the above recent models we believe that their performance can be improved as well moreover our attention learning framework can also help revise the design of the copy mechanism as well as the coverage ing strategy all these are worthwhile directions to investigate for the future works system suatt unatt giga duc lcsts table evaluation for the words tracted from suatt and unatt toyota rally world europe banned japan s toyota team europe were banned from the world rally championship for one year here on friday in a crushing ruling by the world council of the international automobile tion a golden toyota are banned for a year suatt championship team ruling year a unatt world council europe federation ternational japan ruling friday championship banned a powerful bomb exploded outside a navy base near the sri lankan capital colombo tuesday seriously wounding at least one person military ofcials said golden bomb attack outside srilanka navy base suatt sri bomb base navy colombo ankan powerful military wounding exploded unatt sri military capital tuesday bomb powerful navy exploded base ofcials palestinian prime minister ismail haniya insisted friday that his hamas led government was continuing efforts to secure the release of an israeli soldier captured by militants golden efforts still underway to secure dier s release hamas pm suatt palestinian release haniya hamas led soldier israeli government secure efforts ister unatt government prime palestinian friday israeli militants efforts continuing minister secure table words extracted from suatt and unatt receptively for samples in gigawords attention analysis we regard the supervised attention and the vised attention as the salience score for the words in the source text so we also design experiments to verify the performance of the two attention nisms for nding important words for each input sequence as and au are the two attention vectors obtained by supervised attention learning and supervised attention learning respectively the ement value ai a represents the word salience score therefore we can select the top k words from the input sequence according to the salience scores in as and au intuitively the extracted k words are very important and may have a large overlapping with the ground truth summary to verify it quantitatively we regard the top words as summaries and conduct rouge evaluation on them because the order of the top words is ignored so we employ the f measure score of as the evaluation metric the experimental results on those three datasets are given in table we set k to here the results illustrate that both methods can extract the important words from the source text and the quality of the top words tracted from the supervised attention as i e suatt is better than those extracted from the unsupervised attention au i e unatt this adheres to our ition because the suatt method can obtain stronger supervision signals than the unsupervised method unatt however from the rouge results presented in tables and we nd that the performance of is similar to or even better than this phenomenon may be because that both the method of and suatt can receive supervision signals to guide the training but unatt is an unsupervised salience detection method which may nd some complementary mation to further improve the summarization formance in order to show the differences vividly we present the extracted top words in table and all the words are ranked based on the ing salience scores from the results we know that suatt and unatt can indeed assign large salience scores to the important words for instance suatt can extract words of toyoda banned and year which are the core elements of the golden summary toyota are banned for a year the result of unatt is more diversied although the performance of suatt and unatt are different the integration of them performs well in the quantitative evaluation experiments in the previous subsection which may be because that different attention methods can ture different aspects of the source text and they can complement each other japan s toyota team europe were banned from the world rally championship for one year here on friday in a crushing ruling by the world council of the international automobile tion a golden toyota are banned for a year toyota s world rally europe banned from world rally championship mal toyota barred from world rally pionship slovaks started voting at am on day in elections to the parliament with centre right prime minister mikulas dzurinda ghting to continue far reaching but painful forms golden slovaks start voting in legislative tions slovakia s parliament begins voting mal slovaks start voting in early elections the thai government has set aside million baht about million u s dollars to support new eco tourism plans during according to a report of the thai news agency tna tuesday golden tourism thailand to support new eco tourism in mal thailand to support new eco tourism plans thai government to support table examples of the generated summaries summary case analysis finally some examples of the source texts golden summaries and the generated summaries by the typical attention based framework and our proposed mal framework are shown in table from these cases we can see that the generated summaries by mal generally have better quality moreover because of the attention learning ponents for salience detection our framework has the ability to assign small salience scores to portant words and uninformative symbols while the summary generated by contains more noisy symbols as the cases shown in related works most important content of the original text ument nenkova and mckeown tional summarization methods can be classied into three categories extraction based methods erkan and radev min et al based methods li et al wang et al li et al and abstraction based methods barzilay and mckeown bing et al recently some researchers employ neural work based frameworks to tackle the abstractive summarization problem and obtain encouraging performance rush et al proposed a ral model with local attention modeling which is trained on the gigaword corpus but combined with an additional log linear extractive summarization model with handcrafted features nallapati et al utilized a trick to control the vocabulary size to improve the training efciency gu et al integrated a copying mechanism into a framework to improve the quality of the generated summaries chen et al proposed a new attention mechanism that not only considers the important source segments but also distracts them in the decoding step in order to better grasp the overall meaning of input documents miao and blunsom extended the framework and proposed a generative model to capture the latent summary information zhou et al tegrated a selective gated network into the framework to control the information ow from encoder to decoder li et al proposed a deep recurrent generative decoder to enhance the modeling ability of latent structures in the target summaries see et al employed pointer networks and converge mechanism to improve the quality of the generated summaries paulus et al proposed a reinforcement learning based framework to enhance the performance of rization chen et al proposes a generative bridging network in which a bridge module is troduced to assist the training of the sequence diction model li et al employ actor critic training paradigm to enhance the quality of the generated summaries meanwhile some researchers also combine the traditional salience estimation methods into the frameworks in order to enhance the marization performance tan et al automatic summarization is the process of matically generating a summary that retains the researchers regard the compression approach as a special case of the extraction approach porated the graph based attention information tained by the pagerank algorithm into their work hsu et al weighted the attention mechanism using sentence salience information calculated by a traditional supervised method in contrast we consider both the supervised salience information and unsupervised salience information in our framework to generate better summaries conclusions in this work we investigate the effect of adding the traditional salience detection of text rization back to the typical attention based framework for abstractive summarization we pose a multi attention learning mal framework which contains two new attention learning nents namely supervised attention learning and unsupervised attention learning for salience mation the salience information obtained based on these two types of attentions is incorporated with the typical attention mechanism in the decoder to conduct the summary generation extensive periments on some benchmark datasets in different languages demonstrate the effectiveness of the posed framework for the task of abstractive marization references dzmitry bahdanau kyunghyun cho and yoshua gio neural machine translation by jointly learning to align and translate in iclr regina barzilay and kathleen r mckeown tence fusion for multidocument news tion computational linguistics lidong bing piji li yi liao wai lam weiwei guo and rebecca passonneau abstractive document summarization via phrase selection and merging in acl pages qian chen xiaodan zhu zhenhua ling si wei and hui jiang distraction based neural networks for document summarization in ijcai pages wenhu chen guanlin li shuo ren shujie liu zhirui zhang mu li and ming zhou tive bridging network in neural sequence prediction naacl kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger learning schwenk and yoshua bengio phrase representations using rnn encoder decoder for statistical machine translation in emnlp pages sumit chopra michael auli alexander m rush and seas harvard abstractive sentence marization with attentive recurrent neural networks naacl hlt pages jeffrey l elman finding structure in time nitive science gunes erkan and dragomir r radev lexrank graph based lexical centrality as salience in text summarization journal of articial intelligence search jiatao gu zhengdong lu hang li and victor ok incorporating copying mechanism in in acl pages li sequence to sequence learning sepp hochreiter and jurgen schmidhuber neural computation long short term memory wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun a ed model for extractive and abstractive arxiv preprint rization using inconsistency loss baotian hu qingcai chen and fangze zhu sts a large scale chinese short text summarization dataset in emnlp pages yuta kikuchi graham neubig ryohei sasano hiroya takamura and manabu okumura ling output length in neural encoder decoders in emnlp pages philipp koehn pharaoh a beam search coder for phrase based statistical machine in conference of the association for tion models machine translation in the americas pages springer chen li fei liu fuliang weng and yang liu document summarization via guided sentence pression in emnlp pages piji li lidong bing and wai lam actor critic based training framework for abstractive tion arxiv preprint piji li wai lam lidong bing weiwei guo and hang li cascaded attention based unsupervised information distillation for compressive tion in emnlp piji li wai lam lidong bing and zihao wang deep recurrent generative decoder for in emnlp pages stractive text summarization chin yew lin rouge a package for automatic in text summarization evaluation of summaries branches out proceedings of the workshop volume yishu miao and phil blunsom language as a latent variable discrete generative models for tence compression in emnlp pages david zajic bonnie dorr and richard schwartz in hlt naacl bbn umd at topiary pages rada mihalcea and paul tarau textrank ing order into text in emnlp matthew d zeiler adadelta an adaptive ing rate method arxiv preprint qingyu zhou nan yang furu wei and ming zhou selective encoding for abstractive sentence summarization in acl pages ziheng lin min yen kan chew and lim tan exploiting category specic information for document summarization coling pages ramesh nallapati bowen zhou caglar gulcehre bing xiang al abstractive text tion using sequence to sequence rnns and beyond arxiv preprint ani nenkova and kathleen mckeown a survey in mining text of text summarization techniques data pages springer jun ping ng praveen bysani ziheng lin min yen kan and chew lim tan exploiting specic information for multi document tion coling pages lawrence page sergey brin rajeev motwani and terry winograd the pagerank citation ing bringing order to the web technical report stanford infolab romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization iclr alexander m rush sumit chopra and jason weston a neural attention model for abstractive tence summarization in emnlp pages abigail see peter j liu and christopher d manning get to the point summarization with generator networks in acl volume pages jiwei tan xiaojun wan and jianguo xiao abstractive document summarization with a based attentional neural model in acl volume pages theano development team theano a python framework for fast computation of mathematical pressions arxiv e prints zhaopeng tu zhengdong lu yang liu xiaohua liu and hang li modeling coverage for neural machine translation in acl volume pages oriol vinyals meire fortunato and navdeep jaitly pointer networks in nips pages lu wang hema raghavan vittorio castelli radu rian and claire cardie a sentence pression based framework to query focused in acl pages document summarization
