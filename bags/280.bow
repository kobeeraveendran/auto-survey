query controllable video summarization jia hong huang university of amsterdam amsterdam netherlands j nl marcel worring university of amsterdam amsterdam netherlands m nl r a r i s c v v i x r a abstract when video collections become huge how to explore both within and across videos efficiently is challenging video summarization is one of the ways to tackle this issue traditional summarization approaches limit the effectiveness of video exploration because they only generate one fixed video summary for a given input video independent of the information need of the user in this work we introduce a method which takes a text based query as input and erates a video summary corresponding to it we do so by modeling video summarization as a supervised learning problem and propose an end to end deep learning based method for query controllable video summarization to generate a query dependent video mary our proposed method consists of a video summary controller video summary generator and video summary output module to foster the research of query controllable video summarization and conduct our experiments we introduce a dataset that contains frame based relevance score labels based on our experimental sult it shows that the text based query helps control the video mary it also shows the text based query improves our model formance com jhhuangkay query controllable video summarization ccs concepts computing methodologies artificial intelligence video summarization acm reference format jia hong huang and marcel worring query controllable video marization in proceedings of the international conference on multimedia retrieval icmr june dublin ireland acm new york ny usa pages introduction video data is now ubiquitous in our daily life most of the raw videos are too long and are containing redundant content as a consequence the amount of video data people have to watch is overwhelming this raises new challenges in efficiently exploring both within and across videos video summarization helps people explore a video efficiently by capturing the essence of the video learning what is essential depends on the information need of the user yet traditional video summarization methods such as generate one fixed video summary for a given input video hence they either create a video capturing all permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page copyrights for components of this work owned by others than the must be honored abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission a fee request permissions from org icmr june dublin ireland copyright held by the owner publication rights licensed to acm acm isbn figure this figure shows the main idea of the controllable video summarization a user gives a text based query which represents the desired video summary to trol a video summary generator to create a video summary based on the query note that the input of the proposed method is both query and video if two different queries are given with the same video input two different dependent video summaries will be generated it is different from the traditional video summarization which only has a video input for details of the video summary controller and video summary generator please refer to figure and possible information needs and therefore yield limited reduction in time or they lose essential information for specific needs having a fixed summary limits the effectiveness of video exploration to make the video exploration more effective and efficient we will need a new specialized method that steered by the information need of the user is capable of generating various video summaries for a given video we call this query controllable video rization there are two main features of query controllable video summarization which complicate this task when compared to the well studied domain of conventional video summarization one example is that query controllable video summarization has a based query input and a video input where the conventional video summarization only has a video input so in query controllable video summarization we need to model the implicit relations or teractions between the input query and video evaluating the ated video summaries is another challenge previously researchers usually conduct human expert evaluation based on predefined rules or showing human experts two different video summaries and ing them to select the better one human expert based evaluation methods for this task are problematic as these methods are expensive and time consuming because they rely on the ments of humans for each evaluation in this paper we prefer to conduct automatic evaluation which is more efficient there are different solutions to model the video summarization problem both supervised and pervised for the automatic manvideosummarycontrollervideosummarygeneratorvideo summaryoutput module evaluation in this paper we model video summarization as a pervised learning problem then we propose an end to end deep learning based model illustrated in figure to generate video maries depending on the text based input queries to the best of our knowledge we are the first proposing an end to end deep model for query controllable video summarization note that a non to end method for query controllable video summarization needs many preprocessing steps and in practice it will reduce the ciency of video exploration the model consists of a video summary controller a video summary generator and a video summary put module the controller uses text such as words phrases or sentences to describe the desired video summary then the erator creates a video summary based on the implicit relationship between the text based description and the input video the video summary output module then outputs a video summary based on a relevance score prediction vector to build a query controllable system for videos using multiple information sources requires dedicated datasets and evaluation methods we undertake such a study in this work starting from an existing dataset we establish a new video dataset on which to build our models with such a dataset in place it is the right time to research how to exploit deep learning based models with textual query inputs which are to steer the output as mentioned above our proposed model takes a text based query and video as input so how to effectively fuse the multi modal features with minimum loss of information is one of the technical problems it has been shown that in similar multi modal contexts the performance of models can decrease if feature fusion methods are improper how to solve this issue in general remains an open question so we not only conduct experiments to see how the input query affects our model performance but also conduct experiments to see how the commonly used feature fusion methods affect our model performance to foster the research community of video summarization we intend to publish the dataset and code we demonstrate experimental results of the proposed end to end deep model and show that our proposed method is capable of ating query dependent and controllable video summaries for given videos our experimental result shows that the text based input query helps control the video summary it also shows that the based input query improves the performance of our summarization model in the sense of accuracy contributions we propose a new end to end deep learning based model for a text visual embedding space for query controllable video summarization we conduct detailed experiments to show that the text based query not only helps control the video summary but also improves the performance of the proposed end to end model we introduce a query video pair based dataset based on the dataset proposed in for a query controllable video summarization task the dataset contains videos and corresponding frame based relevance score annotations related work in this section we discuss related work in terms of different ods and different datasets we first discuss the two main types of methods of video summarization i e supervised and unsupervised then we review the commonly used video summarization datasets unsupervised video summarization unsupervised approaches for video summarization usually exploit hand crafted heuristics to satisfy some properties such as interestingness ness and diversity in the authors propose a method based on color feature extraction from video frames and k means clustering the authors of observe that the key visual concepts usually pear repeatedly across videos with the same topic so they propose to create a summarized video by finding shots that co occur most frequently across videos they develop a maximal biclique finding mbf algorithm to find sparsely co occurring patterns in the authors introduce a space time video summarization method to extract the visually informative space time portions of input videos and analyze the distribution of the spatial and temporal tion in a video simultaneously the authors of present a video summarization method for egocentric camera data they develop region cues e the nearness to hands gaze and frequency of occurrence in egocentric video and learn a model to predict the relative importance of a new region based on these cues the thors of present a video summarization method to discover the story of a given egocentric video their proposed method is capable of selecting a short chain of sub shots of video which depicting the essential events in based on the modeling of the ers attention the authors propose a generic framework of video summarization the framework without the fully semantic content understanding of a given video eliminates the needs of complicated heuristic rules in video summarization task and takes advantage of computational attention models in the authors introduce a unified method for video summarization based on the analysis of structures and highlights of the video their approach emphasizes the content balance and perceptual quality of a video summary at the same time they also incorporate a normalized cut algorithm to partition a video into clusters and a motion attention model based on human perception to compute the perceptual quality of clusters and shots the authors of develop a new method to extract a video summary that captures important particularities arising in a given video and generalities identified from a set of given videos simultaneously the authors of introduce a method to learn a dictionary from the given video using group sparse coding a video summary is then generated by combining segments that can not be reconstructed sparsely using the dictionary in the authors propose a new formulation to perform video summarization from unpaired data their model aims to learn mapping from a set of raw videos to a set of video summaries such that the distribution of the generated video summary is similar to the distribution of the set of video summaries with the help of an adversarial objective also they enforce a diversity constraint on the mapping to ensure the generated video summaries are diverse enough visually in the authors introduce an end to end reinforcement learning based framework to train their video summarization model the work incorporates a new reward function to jointly account for diversity and representativeness of generated video summaries the design of the reward function makes it not rely on user interactions or labels at all the authors of propose a more based method with a specific objective for query adaptive video summarization since the method is not end to end it will need many preprocessing steps before generating a video summary so in practice it is inconvenient and not efficient for video exploration this motivates us to propose an end to end deep learning based method although many existing works model video tion task as an unsupervised problem and propose their methods to tackle it in general the performance of the unsupervised approach is worse than the supervised one supervised video summarization the other type of approach for video summarization task is supervised the learning of these ods is supervised by human expert labeled data i e ground truth video summaries the authors of treat video summarization as a supervised subset selection task they propose a probabilistic model for selecting a diverse sequential subset called the tial determinantal point process seqdpp note that the standard dpp treats video frames as randomly permutable elements the seqdpp heeds the inherent sequential structures in video data so it not only overcomes the deficiency of the standard dpp but also retains the power of modeling diverse subsets which is essential for video summarization the authors of introduce a new video summarization method and focus on user videos containing a set of interesting events the method starts by segmenting a given video based on a superframe segmentation tailored to raw videos then according to the estimation score of visual interestingness per frame by using a set of low level mid level and high level features the method picks an optimal subset of superframes to generate a video summary in the authors propose a new model to learn the importance score of global characteristics of a video summary the models jointly optimized for multiple objectives is capable of generating high quality video summaries in the authors duce a new probabilistic model built upon seqdpp to tackle video summarization problem the period of a video segment where the local diversity is imposed can be dynamically controlled by the model to get a well trained summarization model the authors develop a reinforcement learning algorithm to train the proposed model the authors of formulate video summarization as a sequence labeling problem they propose fully convolutional quence models to tackle the video summarization task first they establish a novel connection between video summarization and semantic segmentation second the adapted popular semantic mentation networks are used to generate video summaries in the authors propose an improved sequential determinantal point process seqdpp model in terms of modeling a new probabilistic distribution is designed to make when it is integrated into seqdpp the resulting model accepts the input of a user about the intended length of the video summary in terms of learning a large margin algorithm is proposed to address the problem of exposure bias in seqdpp in the authors propose a subset selection method that leverages supervision in the form of human created video maries to perform keyframe based video summarization the main idea of this method is nonparametrically transferring structures of summaries from annotated videos to unseen testing videos also the authors generalize the proposed method to sub shot based video summarization the authors of cast a video summarization task as a structured prediction problem on sequential data then they propose a new supervised learning technique incorporating long short term memory lstm to model the variable range dependencies entailed in the task also they exploit domain tation techniques based on the auxiliary annotated video datasets to improve the quality of the video summary in the authors propose a sequence to sequence learning model to tackle the video summarization problem to complement the discriminative losses with another loss such as measuring whether the generated video summary preserves the same information as the original video they propose to augment standard sequence learning models with a retrospective encoder that embeds the predicted video summary into an abstract semantic space then the embedding is compared to the original video s embedding in the same space the authors of introduce a novel dilated temporal relational generative adversarial network dtr gan to tackle the frame based video summarization dtr gan exploits an adversarial manner with a three player loss to learn a dilated temporal relational generator and discriminator the authors introduce a new dilated temporal relational unit to enhance the capturing of temporal representation and then the generator creates keyframes based on the unit the supervised methods are capable of learning useful cues which are hard to capture with hand crafted heuristics from ground truth video summaries so they usually outperform the unsupervised models that is the reason why we prefer to model video rization as a supervised learning problem in this paper video summarization dataset comparison in this section we shortly introduce a commonly used video summarization dataset and do some comparison with the dataset used in this paper to tackle the video summarization task the authors of propose a dataset named tvsum it contains videos with categories and the corresponding shot level importance scores obtained via crowdsourcing the categories are selected from the trecvid multimedia event detection med task and the videos five per category are collected from youtube by using the names of categories as search queries from the search results videos are chosen based on the following criteria i the selected video should contain more than a single shot the title of video is descriptive of the visual topic in the video iii under the creative commons license iv the duration of video is around and minutes the authors exploit amazon mechanical turk amt to collect responses per video and these responses are treated as gold standard labels a participant from amt is asked to read the title of video first simulating a typical scenario of online video browsing watch the whole video in a single take provide an importance score to each of uniform length shots for the whole video denoting from not important to very important the audio is muted to ensure the important scores are only based on visual stimuli according to the authors experience a two second shot length is ate for capturing local context with good visual coherence in the authors introduce another video summarization benchmark called summe consisting of videos covering holidays events and sports the length of video ranges from to minutes and each video is summarized by to different people the thors asked males and females to participate in making the dataset given a video participants are asked to produce a video summary containing most of the important content in the video they are allowed to watch cut and edit a video by using a simple table summary of commonly used video summarization datasets based on this table we find that the proposed dataset is much larger than the other datasets and it contains two types of input modalities including video and text the other two video summarization dataset only contains video data and the dataset size is not large so the proposed dataset is unique relevance scores in this work and important scores from tvsum are different referring to crowd sourced annotation subsection name of dataset summe tvsum ours based on annotation type interval based shot and frame level scores frame based important scores frame based relevance scores content user videos number of videos input modality video youtube videos youtube videos video video text interface the length of a video summary is required to range from to of the original video length that is to ensure the input video is indeed summarized rather than being shortened slightly the videos are shown randomly and the audio is muted to ensure the generated video summaries are only based on visual stimuli regarding the evaluation of video summarization approaches viously researchers conduct the human expert evaluation in one of the following ways based on a set of predefined criteria such as the degree of redundancy counting the inclusion of predefined important content summary duration and so on ii showing human experts two different video summaries and asking them to select the better one the authors of claim that the above human expert evaluation methods are problematic as these methods are expensive and time consuming because they rely on judges of human for each evaluation for example in the uation of the method requires one full week of human labor both of the human expert evaluation methods help to tell which video summary is better than another but fail to show what a good video summary should look like so the authors of do not exploit the above approaches instead they let a set of participants create their video summaries and collect multiple video summaries for each video the reason is that there is no true answer for correct video summarization but rather multiple possible ways with these man expert video summaries they can compare any summarization method which creates an automatic video summary in a repeatable and efficient way in such automatic versus human ison has already been used successfully for keyframes also the authors of show that comparing automatic keyframe based summaries to human keyframe based selections yields ratings that are comparable to letting humans directly judge the automatic video summaries both tvsum and summe datasets allow the automatic evaluation of video summarization approaches in this paper we also establish a dataset based on with automatic evaluation for our query controllable video summarization task the proposed dataset contains videos with frame level relevance score tations for convenience we summarize the above existing datasets and comparison with our dataset in table dataset introduction and analysis in this section we start to describe and analyze our proposed dataset for query controllable video summarization in terms of types of videos video labels and some statistics of the dataset note that although the dataset from partially matches our research pose and is publicly available we discover that the specification figure this figure shows the original number of frames of each video the axis denotes the video index and the y axis indicates the number of frames note that for venience we make all the videos have the same number of frames when we develop our proposed method such as annotations and amount of videos of the published dataset are different from the one mentioned in also some parts of the published dataset are not available anymore we base our uation on the dataset published in so we will first describe the process they have used to create the dataset and from there indicate what changes needed to make it suitable for our purpose setup since our dataset is based on the rules from for the dataset collection are similar the proposed dataset consists of videos and each video is retrieved based on a given text based query then according to the authors use amazon mechanical turk amt to annotate the video frames with the labels of text based query relevance scores the labels in this dataset are used similarly in the mediaeval diverse social images challenge the purpose of human expert annotated labels in the proposed dataset is to matically evaluate the methods for generating relevant and diverse video summaries in the proposed dataset based on the sentative samples of queries and videos are collected based on the following procedure the seed queries with different categories are selected from the top youtube queries between and typically since these queries are generic concepts and short the youtube auto complete function is exploited to obtain more tic and longer queries e ariana grande focus instrumental and ark survival evolved dragon for each query the top video result of frame label for each query video pair by merging the corresponding evance score annotations from amt workers then we base on the majority vote rule to evaluate the model performance for a relevance score prediction i e a predicted relevance score is correct if the majority of human annotators provided that exact score note that we map annotations very good to good to not good to and bad to note that referring to the relevance score in this work and the importance score from tvsum are different the relevance score in this work is to capture the relation between a given text based query and a video frame the important score is to capture the importance between a video frame and a final video summary of the video method overview in this section we start to describe the proposed controllable video summarization method the proposed method is composed of a video summary controller video summary generator and video summary output module the summary controller takes a text based query as input and outputs the vector representation of the query the summary generator takes the embedded query and a video as inputs and outputs the frame based relevance score prediction finally the video summary output module will use the score prediction to generate a video summary figure explains the above procedure video summary controller text based queries are meant to represent the expected video summary content while subtly alludes its semantic relationship therefore we use the following way to encode input queries and add their contribution to our proposed method in our paper we exploit the vector representation of the text based input query to control the generated video summary the main idea of our video summary controller is to generate a vector representation of an input query based on a dictionary in the beginning we form a dictionary based on a bag of words which are collected from all the unique words of the training queries then we encode an input query by exploiting the dictionary after the encoding we have a vector representation of the input query to represent the expected video summary content to make the procedure clearer we make a flowchart to explain the above referring to figure video summary generator the main idea of the video summary generator is to take a vector representation of an input text based query and a video to generate a frame based relevance score vector the summary generator is composed of a convolutional neural network cnn structure and a multi modality features fusion module note that the cnn structure will be trained on our training set before an input video goes to the cnn structure it is sampled at fps then in our case we use to extract the frame based features for each input video note that the feature used is the visual layer one layer below the classification layer after the features are extracted we exploit a feature fusion module to fuse the frames based features and the input text based query feature the fused feature vector will be sent to a fully connected layer for the frame based relevance score prediction the feature fusion module will be depicted in the following subsection please refer to figure for the flowchart of the above procedure note that we take cross entropy loss figure this figure conceptually depicts our video mary controller in the video summary controller we take all the queries from our training set to form a bag of words and create a dictionary then we base on this dictionary to embed an input query with a duration of to minutes is collected the following task is set up on amt by for video annotations all of the videos are sampled at one frame per second fps then an amt worker is asked to annotate every frame with its relevance with respect to the given text based query the answer candidates are very good good not good and bad where bad denotes the frame is not relevant and low quality such as bad contrast blurred and so on to reduce the subjectivity of labels every video in the proposed dataset is annotated by at least different amt workers additionally a qualification task is defined to ensure high quality annotations the results are manually reviewed to make sure the workers provide annotations with good quality after workers pass this task then they are allowed to take further assignments since our maximum number of frames of video is to make all the videos have the same number of frames we repeat frames starting from the first frame until reaching frames in total for each video similar to figure shows the original number of frames of each video crowd sourced annotation in this subsection we analyze the frame based relevance score annotations obtained through the above procedure also we explain how we merge these relevance score annotations for each video into one set of ground truth labels label distributions of relevance scores the distribution of evance score annotations is very good good not good and bad ground truth as mentioned in the video summarization dataset comparison subsection human based evaluation is problematic and time consuming so in this work based on the ing approach are used for evaluation for evaluation of the testing videos one way is to ask human experts to watch the full video instead of just video summaries and access the relevance of every single part of the video then their responses are considered as gold standard annotations the advantage of this approach is that once the annotations are obtained experiments can be carried out indefinitely this is desirable especially for a computer vision system involving multiple iterations and testing note that in the proposed dataset we create a single ground truth relevance score all of thetraining queriesword embeddingmodulequeryvectorrepresentation of the input querydictionarybag of words figure this figure conceptually depicts our video summary generator in the generator we take an input video and sample it at fps then we input the sampled frames to a cnn based structure for extracting frame based features the feature will be fused with a text based input query feature finally the fused feature will be pass to a prediction layer to generate the frame based relevance scores finally we base on the predicted relevance scores to output a query dependent video summary as our loss function referring to equation and adam as our optimizer for the optimizer parameters coefficients used for computing moving averages of gradient and its square are and respectively the term added to the denominator to improve numerical stability is and the learning rate class ln j where class denotes the ground truth class and indicates the prediction multi modality features fusion module one of the technical problems of our proposed method is fusing the query and based features with minimum loss of information it has been shown that in similar multi modal contexts the performance of models can decrease if models are poorly designed how to solve this issue in general remains an open question in this work we exploit difference commonly used approaches summation concatenation and element wise multiplication to fuse query and frame based features video summary output module after we get the frame based relevance score prediction vector from the video summary generator we pass this vector to our video summary output module the main idea of this module is to output a video summary based on the relevance score prediction vector in our case we map labels very good to good to good to andbad to if a predicted relevance score greater than or equal to then we consider the corresponding frame is relevant if a predicted relevance score is less than then we consider the corresponding frame is irrelevant finally we collect k relevant frames in time order as our video summary note that k is a user defined parameter for the length of a video summary experiments and analysis in this section we will evaluate our proposed end to end method for the query controllable video summarization task based on the setup of the proposed dataset we will also analyze the effectiveness of the query and methods of multi modal features fusion dataset preparation to validate our proposed query controllable video tion method we base on the following dataset setup to conduct our experiments we separate the whole dataset into i e for training validation testing respectively one video has one corresponding query the maximum number of words of the query is regarding the frame size input of cnn is by with channels i e red green and blue note that we malize each image channel by mean and std the maximum number of frames of video is similar to the video preprocessing method in we make all the videos have the same number of frames i e we show the original number of frames of each video in figure effectiveness analysis of query in this experiment we want to know whether the text based query will help generate a better video summary or not so we conduct the experiment based on two types of models query driven and non query driven according to figure we discover that the query driven model with the testing accuracy is better than the non query driven one with the testing accuracy also based on the validation accuracy versus the number of epochs we can see that the textual query is capable of guiding the query driven model to perform better the non query driven one however when vector representation of the input querycandidatefeature fusionmoduleframe basedfeaturesprediction summary figure the figure shows the model performance in two cases video only and in this figure the axis denotes the number of epochs the y axis denotes the model accuracy we test models after the epochs of ing note that the purple point denotes the video only testing accuracy and the black point indicates the testing accuracy we compare the worst model fusing features by summation in figure to the non query driven model we discover that the query driven model performs better this motivates us to conduct the other experiment about the comparison of multi modal feature fusion methods referring to the next subsection effectiveness analysis of different fusion methods in general multi modal features fusion is still an open question so we base on the commonly used methods summation concatenation and the element wise multiplication to conduct our experiment according to figure we can see that the model with element wise multiplication fusion method has the best formance if we compare the performance of the other feature fusion methods to the non query driven model we discover that the model with the concatenation fusion method is still better than the non query driven model however the model with the summation fusion method is worse than the non query driven model interaction between query and video based on the result of figure and figure it implies that using a proper multi modal feature fusion method is important the reason is that the fused feature embeds the implicit interaction between video i e frames and query if we use an improper fusion method such as summation the query will confuse the network in some sense this situation also happens in qualitative results and analysis in this subsection we show some qualitative results illustrated in figure note that because of the limited space we are only able to show some frames to represent the original video and the corresponding generated video summary in time order in figure the input is a video with the query civil war spiderman based on our video summary output module subsection we use green to indicate the relevant frame and black to indicate evant frames the second row in a represents the video frames with ground truth labels the third row in a represents the video frames with predicted labels the correct number of relevance score figure the figure shows the model performance under three different cases i e multi modal features fusion by summation concatenation and the element wise plication the model with element wise multiplication sion method has the best performance in this figure the axis denotes the case index the y axis denotes the model accuracy note that each model is well trained with a ent number of epochs prediction is out of note that the number of frames of the original input video is so we only show the video summary frames selected from to in order in figure the input is another video with the query movies we also use the same color to indicate the relevant and irrelevant frames the second row of b indicates the video frames with ground truth labels the third row in b represents the video frames with predicted labels the correct number of relevance score prediction is out of similar to since the number of frames of the original input video is so we only show the video summary frames selected from to in order based on figure it shows that our proposed method is capable of generating video summaries with content relevant to the input query conclusion and future work to sum up we treat a query controllable video summarization task as a supervised learning problem in this work to tackle this lem we propose an end to end deep learning based approach to generate a query dependent video summary the proposed method contains a video summary controller video summary generator and video summary output module to foster the query controllable video summarization research and conduct our experiments we propose a new dataset each video in the proposed dataset is notated by frame based relevance score labels our experimental results show that the text based query not only helps control video summary but also improves the model performance with in the sense of accuracy based on our experiment we know that the multi modal feature fusion method is crucial so developing a new fusion approach will be interesting future work of video queryvalidation video querytraining videovalidation query civil war spiderman correct number of relevance score prediction total number of frames query movies correct number of relevance score prediction total number of frames figure in this figure based on a similar qualitative result visualization method from we show two generated video summaries with the corresponding queries in a color coded by red indicates the original total number of frames of the input video in the first row we show some frames from the original video with frames to represent the input video the second row represents the original input video the third row represents the prediction in the fourth row we show e k frames to represent our generated video summary the numbers at the bottom indicate the frame index in the original video note that the video frame index is starting from in we show the second generated video summary result and the corresponding notations are similar to acknowledgments this project has received funding from the european unions horizon research and innovation programme under the marie skodowska curie grant agreement no references stanislaw antol aishwarya agrawal jiasen lu margaret mitchell dhruv batra c lawrence zitnick and devi parikh vqa visual question answering in proceedings of the ieee international conference on computer vision hedi ben younes rmi cadene matthieu cord and nicolas thome mutan multimodal tucker fusion for visual question answering in proceedings of the ieee international conference on computer vision wen sheng chu yale song and alejandro jaimes video co summarization video summarization by visual co occurrence in proceedings of the ieee ence on computer vision and pattern recognition sandra eliza fontes de avila ana paula brando lopes antonio da luz jr and arnaldo de albuquerque arajo vsumm a mechanism designed to produce static video summaries and a novel evaluation method pattern recognition letters akira fukui dong huk park daylen yang anna rohrbach trevor darrell and marcus rohrbach multimodal compact bilinear pooling for visual question answering and visual grounding arxiv preprint boqing gong wei lun chao kristen grauman and fei sha diverse sequential subset selection for supervised video summarization in advances in neural information processing systems michael gygli helmut grabner hayko riemenschneider and luc van gool creating summaries from user videos in european conference on computer vision springer michael gygli helmut grabner and luc van gool video summarization by learning submodular mixtures of objectives in proceedings of the ieee conference on computer vision and pattern recognition kaiming he xiangyu zhang shaoqing ren and jian sun deep residual learning for image recognition in proceedings of the ieee conference on computer vision and pattern recognition tao hu pascal mettes jia hong huang and cees gm snoek silco show a few images localize the common object in proceedings of the ieee international conference on computer vision jia hong huang robustness analysis of visual question answering models by basic questions king abdullah university of science and technology ms thesis jia hong huang modar alfadly and bernard ghanem vqabq visual question answering by basic questions cvpr vqa challenge workshop jia hong huang modar alfadly bernard ghanem and marcel worring arxiv preprint assessing the robustness of visual question answering jia hong huang cuong duc dao modar alfadly and bernard ghanem a novel framework for robustness analysis of visual qa models in proceedings of the aaai conference on artificial intelligence vol jia hong huang cuong duc dao modar alfadly c huck yang and bernard ghanem robustness analysis of visual qa models by basic questions cvpr vqa challenge and visual dialog workshop bogdan ionescu alexandru lucian gnsca bogdan boteanu adrian popescu mihai lupu and henning mller retrieving diverse social images at mediaeval challenge dataset and evaluation mediaeval hong wen kang yasuyuki matsushita xiaoou tang and xue quan chen space time video montage in ieee computer society conference on computer vision and pattern recognition vol ieee aditya khosla raffay hamid chih jen lin and neel sundaresan scale video summarization using web image priors in proceedings of the ieee conference on computer vision and pattern recognition diederik p kingma and jimmy ba adam a method for stochastic mization arxiv preprint yong jae lee joydeep ghosh and kristen grauman discovering important people and objects for egocentric video summarization in ieee conference on computer vision and pattern recognition ieee yandong li liqiang wang tianbao yang and boqing gong how local is the local diversity reinforcing sequential determinantal point processes with dynamic ground sets for supervised video summarization in proceedings of the european conference on computer vision eccv tiecheng liu and john r kender optimization algorithms for the selection of key frame sequences of variable length in european conference on computer vision springer yi chieh liu yung an hsieh min hung chen chao han huck yang jesper tegner and yi chang james tsai interpretable self attention temporal reasoning for driving behavior understanding arxiv preprint yi chieh liu hao hsiang yang c h huck yang jia hong huang meng tian hiromasa morikawa yi chang james tsai and jesper tegner synthesizing new retinal symptom images by multiple generative models in asian conference on computer vision springer zheng lu and kristen grauman story driven summarization for egocentric video in proceedings of the ieee conference on computer vision and pattern recognition yu fei ma lie lu hong jiang zhang and mingjing li a user attention model for video summarization in proceedings of the tenth acm international conference on multimedia acm chong wah ngo yu fei ma and hong jiang zhang automatic video marization by graph modeling in proceedings ninth ieee international conference on computer vision ieee paul over alan f smeaton and george awad the trecvid bbc rushes summarization evaluation in proceedings of the acm trecvid video summarization workshop acm rameswar panda and amit k roy chowdhury collaborative tion of topic related videos in proceedings of the ieee conference on computer vision and pattern recognition danila potapov matthijs douze zaid harchaoui and cordelia schmid category specific video summarization in european conference on computer vision springer mrigank rochan and yang wang video summarization by learning from unpaired data in proceedings of the ieee conference on computer vision and pattern recognition mrigank rochan linwei ye and yang wang video summarization using fully convolutional sequence networks in proceedings of the european conference on computer vision eccv aidean sharghi ali borji chengtao li tianbao yang and boqing gong improving sequential determinantal point processes for supervised video marization in proceedings of the european conference on computer vision eccv gunnar a sigurdsson santosh divvala ali farhadi and abhinav gupta asynchronous temporal fields for action recognition in proceedings of the ieee conference on computer vision and pattern recognition alan f smeaton paul over and wessel kraaij evaluation campaigns and trecvid in proceedings of the acm international workshop on multimedia information retrieval acm yale song jordi vallmitjana amanda stent and alejandro jaimes tvsum summarizing web videos using titles in proceedings of the ieee conference on computer vision and pattern recognition arun balajee vasudevan michael gygli anna volokitin and luc van gool query adaptive video summarization via quality aware relevance estimation in proceedings of the acm international conference on multimedia acm c h huck yang jia hong huang fangyu liu fang yi chiu mengya gao weifeng lyu jesper tegner al a novel hybrid machine learning model for auto classification of retinal diseases joint icml and ijcai workshop on computational biology c h huck yang fangyu liu jia hong huang meng tian md i hung lin yi chieh liu hiromasa morikawa hao hsiang yang and jesper tegner auto classification of retinal diseases in the limit of sparse data using a streams machine learning model in asian conference on computer vision springer chao han huck yang yi chieh liu pin yu chen xiaoli ma and yi chang james tsai when causal intervention meets adversarial examples and image masking for deep neural networks in ieee international conference on image processing icip ieee ke zhang wei lun chao fei sha and kristen grauman summary transfer exemplar based subset selection for video summarization in proceedings of the ieee conference on computer vision and pattern recognition ke zhang wei lun chao fei sha and kristen grauman video rization with long short term memory in european conference on computer vision springer ke zhang kristen grauman and fei sha retrospective encoders for video summarization in proceedings of the european conference on computer vision eccv yujia zhang michael kampffmeyer xiaodan liang min tan and eric p xing query conditioned three player adversarial network for video tion arxiv preprint yujia zhang michael kampffmeyer xiaoguang zhao and min tan gan dilated temporal relational adversarial network for video summarization in proceedings of the acm turing celebration conference china acm bin zhao and eric p xing quasi real time summarization for consumer videos in proceedings of the ieee conference on computer vision and pattern recognition kaiyang zhou yu qiao and tao xiang deep reinforcement learning for unsupervised video summarization with diversity representativeness reward in thirty second aaai conference on artificial intelligence
