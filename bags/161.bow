abstractive summarization sebastian gehrmann yuntian deng school engineering applied sciences harvard university gehrmann dengyuntian harvard edu alexander m rush t c o l c s c v v x r abstract neural network based methods tive summarization produce outputs uent techniques perform poorly content selection work poses simple technique addressing issue use data efcient content selector determine phrases source document summary use selector attention step constrain model likely phrases approach improves ability compress text generating uent summaries step process pler higher performing end end content selection models leading nicant improvements rouge cnn dm nyt corpus furthermore content selector trained little sentences making easy transfer trained summarizer new domain introduction text summarization systems aim generate ural language summaries compress mation longer text approaches ral networks shown promising results task end end models encode source document decode tive summary current state art neural stractive summarization models combine tive abstractive techniques generator style models copy words source document gu et al et al end end models produce uent abstractive summaries mixed success content selection e deciding summarize compared fully extractive models appeal end end models modeling perspective evidence summarizing people follow step source document german chancellor angela merkel look pleased weather annual easter holiday italy britain basks sunshine temperatures mrs merkel husband chemistry professor joachim sauer settle measly degrees chancellor spouse spending easter small island ischia near naples mediterranean decade sunny angela merkel husband chemistry professor joachim sauer spotted annual easter trip island ischia near naples couple traditionally spend holiday star miramare spa hotel south island comes private beach conies overlooking ocean reference angela merkel husband spotted italian island holiday baseline approach angela merkel husband chemistry professor joachim sauer spotted annual easter trip island ischia near naples summarization angela merkel husband spotted easter trip island ischia near naples figure example sentence summaries attention model low copying words erate words attention explicit sentence compression sentences copied verbatim approach rst selecting important phrases paraphrasing anderson hidi jing mckeown similar argument image captioning son et al develop state art model step approach rst pre computes bounding boxes segmented objects plies attention regions called attention inspired neuroscience search describing attention based properties herent stimulus buschman miller related work motivated approach consider attention neural abstractive marization approach rst selects selection mask source document constrains standard neural model mask approach better decide phrases model include summary sacricing uency advantages neural abstractive marizers furthermore requires fewer data train makes adaptable new domains model incorporates separate content selection system decide relevant aspects source document frame selection task sequence tagging problem tive identifying tokens document summary tent selection model builds contextual word embeddings peters et al identify rect tokens recall cision incorporate attention abstractive summarization models employ masking constrain copying words selected parts text produces grammatical outputs additionally experiment multiple methods incorporate similar straints training process plex end end abstractive summarization els multi task learning directly incorporating fully differentiable mask experiments compare attention state art abstractive tems compared baseline models et al attention leads provement rouge l score cnn daily mail cnn dm corpus simpler train comparable better results recent reinforcement learning based methods mle trained system thermore nd content selection model data efcient trained original training data vides opportunities domain transfer resource summarization rization model trained cnn dm ated nyt corpus improved points rouge l content selector trained domain sentences tension document summarization tween staying close source document allowing compressive abstractive modication non neural systems select press approach example dorr et al introduced system rst extracts noun verb phrases rst sentence news ticle uses iterative shortening algorithm compress recent systems durrett et al learn model select sentences compress contrast recent work neural network based data driven extractive summarization focused extracting ordering sentences cheng lapata dlikman lapati et al use classier determine include sentence selector ranks positively classied ones ods extract extraction word level requires maintaining grammatically correct output cheng lapata cult interestingly key phrase extraction grammatical matches closely content human generated summaries bui et al approach neural abstractive marization sequence sequence models sutskever et al bahdanau et al methods applied tasks headline generation rush et al article summarization nallapati et al chopra et al attention approaches specic summarization prove performance models gu et al rst copy mechanism troduced vinyals et al combine advantages extractive abstractive summarization copying words source et al rene pointer generator proach use additional coverage mechanism tu et al makes model aware attention history prevent repeated attention recently reinforcement learning rl proaches optimize objectives tion maximum likelihood shown improve performance tasks paulus et al li et al ilmaz et al paulus et al approach coverage problem intra attention decoder attention previously generated words rl based training difcult tune slow train method utilize rl training theory approach adapted rl methods papers explore multi pass extractive abstractive summarization pati et al create new source document comprised important sentences source train abstractive system liu et al describe extractive phase extracts paragraphs abstractive determines order finally zeng et al introduce mechanism reads source document passes uses information rst pass bias second method differs utilize completely abstractive model biased powerful content selector recent work explores alternative proaches content selection example cohan et al use hierarchical attention detect relevant sections document li et al generate set keywords guide summarization process pasunuru bansal develop loss function based salient keywords included summary approaches investigate content selection sentence level tan et al describe based attention attend sentence time chen bansal rst extract sentences document compress hsu et al modulate attention based likely sentence included summary background neural summarization paper consider set pairs texts x y x corresponds source tokens xn y y summary ym m n abstractive summaries generated word time time step model aware previously generated words problem learn function parametrized imizes probability generating correct sequences following previous work model abstractive summarization attentional sequence sequence model attention bution decoding step j culated neural network represents embedded soft distribution source tokens interpreted current focus model model additionally copy figure overview selection generation cesses described section nism vinyals et al copy words source copy models extend decoder predicting binary soft switch zj determines model copies generates copy distribution probability distribution source text joint distribution computed convex combination parts model zj zj parts represent copy generation distribution respectively following generator model et al reuse attention distribution copy tribution e copy probability token source w copy attention computed sum attention occurrences w training maximize marginal hood latent switch variable attention consider techniques incorporating content selection abstractive summarization illustrated figure content selection dene content selection problem level extractive summarization task signicant work custom extractive summarization related work plifying assumption treat sequence ging problem let tn denote binary tags source tokens e word copied target sequence supervised data task generate training data aligning maries document dene word xi sourcemasked sourcesummarycontent selectionbottom attention copied longest possible sequence tokens s xij integers k n s s y exists earlier sequence u s u use standard bidirectional lstm model trained maximum likelihood sequence labeling problem recent results shown better word representations lead cantly improved performance sequence tagging tasks peters et al rst map token wi embedding channels embedding represents static channel pre trained word embeddings e glove pennington et al contextual embeddings pretrained language model e elmo peters et al uses character aware token embedding kim et al followed bidirectional lstm ers contextual embeddings ne tuned learn task specic embedding linear combination states lstm layer token embedding sj trainable parameters embeddings add additional eters tagger remains data efcient despite high dimensional embedding space embeddings concatenated gle vector input bidirectional lstm computes representation hi word wi calculate probability word selected bs trainable parameters ws bs copy attention inspired work attention ages anderson et al restricts tion predetermined bounding boxes image use attention masks limit available selection pointer generator model shown figure common mistake neural copy models copying long quences sentences line model copied tokens copy sequences longer tokens number reference summaries attention modify source encoder tions found standard encoder text effective aggregation limit step attention masking concretely rst train pointer generator model dataset content selector dened inference time erate mask content selector computes lection probabilities n token source document selection probabilities modify copy attention distribution clude tokens identied selector let ai j note attention decoding step j encoder word given threshold selection plied hard mask qi ensure eq yields correct ity distribution rst multiply normalization parameter malize distribution resulting normalized distribution directly replace new copy probabilities end end alternatives step attention tage training simplicity theory dard copy attention able learn perform content selection end end training consider end end proaches incorporating content selection neural training method mask rst consider alignment proach help standard summarization tem inspired nallapati et al vestigate aligning summary source training xing gold copy tention pick correct source word cial think approach limiting set possible copies xed source word training changed mask test time method multi task investigate content selector trained abstractive system rst test pothesis posing summarization multi task problem training tagger tion model features setup use shared encoder abstractive marization content selection test time apply masking method tention method diffmask finally sider training system end end mask training jointly timize objectives use predicted tion probabilities softly mask copy attention leads fully differentiable model model soft mask test time inference authors noted longer form neural generation signicant issues rect length repeated words short form problems like translation proposed solutions clude modifying models extensions coverage mechanism tu et al et al intra sentence attention cheng et al paulus et al instead stick theme modifying inference modify scoring function include length penalty lp coverage penalty cp dened y log length encourage generation longer sequences apply length normalizations beam search use length penalty wu et al formulated tunable parameter increasing leads longer summaries additionally set minimum length based training data repeats copy models repeatedly attend source tokens generating phrase multiple times introduce new mary specic coverage penalty n max n aj m intuitively penalty increases decoder directs total attention sequence single encoded ken selecting sufciently high penalty blocks summaries lead repetitions additionally follow paulus et al restrict beam search repeat trigrams data experiments evaluate approach cnn dm pus hermann et al nallapati et al nyt corpus sandhaus standard corpora news summarization summaries cnn dm corpus let points articles shown respective websites nyt corpus contains maries written library scientists cnn dm summaries sentences average tokens bullet points nyt maries complete sentences shorter average tokens bullet points following et al use non anonymized version cnn dm corpus truncate source documents kens target summaries tokens training validation sets experiments nyt corpus use preprocessing scribed paulus et al additionally remove author information truncate source documents tokens instead changes lead average tokens cle decrease tokens token truncated articles target non copy lary limited tokens models content selection model uses pre trained glove embeddings size elmo size bi lstm layers den size dropout set model trained adagrad initial learning rate initial accumulator value limit number training examples corpus small impact performance jointly trained content selection models use uration abstractive model base model implemented pointer generator model described et al comparable number ters previous work use encoder hidden states directions layer lstm layer decoder embedding size set model trained adagrad conguration tent selector additionally learning rate halves epoch validation perplexity decrease epoch use dropout use gradient clipping mum norm found increasing model size transformer vaswani et al method r l pointer generator et al pointer generator coverage et al ml intra attention paulus et al ml rl paulus et al saliency entailment reward pasunuru bansal key information guide network li et al inconsistency loss hsu et al sentence rewriting chen bansal pointer generator implementation pointer generator coverage penalty copytransformer coverage penalty pointer generator mask pointer generator multi task pointer generator diffmask summarization summarization copytransformer table results abstractive summarizers cnn dm dataset rst section shows encoder decoder abstractive baselines trained cross entropy second section describes reinforcement learning based proaches section presents baselines attention masking methods described work lead slightly improved performance cost increased training time rameters report numbers transformer copy attention denote model randomly choose attention heads copy distribution follow parameters big vaswani et al inference parameters tuned ample subset validation set length penalty parameter copy mask differ els ranging ranging minimum length erated summary set cnn dm nyt pointer generator uses beam size improve larger beam found attention requires larger beam size coverage penalty parameter set copy attention tion parameter approaches use allennlp gardner et al tent selector opennmt py abstractive models klein et al reproduction instructions found com summary results compare non anonymized version corpus et al best results anonymized version results table shows main results dm corpus abstractive models shown attention methods rst observe age inference penalty scores coverage mechanism requiring tional model parameters model ne tuning results copytransformer coverage penalty indicate slight improvement scores observe signicant ence pointer generator attention found end end models lead improvements indicating cult apply masking training hurting training process mask model increased supervision copy mechanism performs similar task model hand tion leads major improvement scores expect better content lection primarily improve fact increase hints uency ing hurt specically cross entropy trained celikyilmaz et al compare dca model nyt corpus method ml dca point gen coverage pen summarization r l table results nyt corpus pare rl trained models marks models results paulus et al results celikyilmaz et al proach outperforms learning based approaches highest reported rouge l score chen bansal falls dence interval results table shows experiments tems nyt corpus point improvement compared baseline generator maximum likelihood approach carries dataset model outperforms rl based model paulus et al l comparable results celikyilmaz et al rouge l observed comparing ml pointer generator suspect difference summary lengths inference parameter choices leads difference access els summaries investigate claim shows approach achieves petitive results models trained summary specic objectives main benet summarization reduction mistakenly copied words best pointer generator models precision copied words compared reference precision creases drives increase independent samples t test shows improvement statistically signicant p observe decrease average sentence length summaries words adding content selection pared pointer generator holding inference parameters constant domain transfer end end training common benets step method content selector needs figure auc content selector trained cnn dm different training set sizes ranging data points solve binary tagging problem pretrained vectors performs limited training data shown figure sentences model achieves auc size auc model increases slightly increasing training data evaluate content selection consider application domain transfer experiment apply pointer generator trained cnn dm nyt corpus dition train content selectors thousand sentences nyt set use summarization results shown table demonstrates model trained smallest subset leads improvement points model attention improvement increases larger subsets points approach reach ble performance models trained directly nyt dataset represents signicant crease augmented cnn dm model produces summaries readable example summaries appendix technique low resource mains problems limited data ability analysis discussion extractive summary content selection given content selector effective junction abstractive model ing know learned effective extractive summarization system ble shows experiments comparing content tion extractive baselines baseline commonly baseline news tion extracts rst sentences increasing training auc r l data novel verb noun adj cnndm reference vanilla pointer generator attention table results domain transfer ment auc numbers shown content selectors rouge scores represent abstractive model trained cnn dm evaluated nyt additional copy constraints trained training ples nyt corpus method neusum zhou et al sents cont select oracle phrase selector content selector r l table results extractive approaches cnn dm dataset rst section shows extractive scores second section rst shows oracle score content selector selected rect words according matching heuristic finally results content selector extracts phrases selection probability threshold article shows performance extract sentences average copy probability selector interestingly method sentences rst reinforcing strength baseline naive sentence extractor performs slightly worse highest reported extractive score zhou et al specically trained score nations sentences nal entry shows performance words threshold extracted resulting summaries approximately length reference summaries oracle score represents results model perfect accuracy shows tent selector yielding competitive results room improvements future work result shows model tive nding important words effective chaining similar paulus et al nd decrease indicates lack uency grammaticality generated summaries table novel shows percentage words summary source document columns speech tag distribution novel words generated summaries typical example looks like man food rst hamburger fully years michael hanline convicted murder ing truck driver jt mcgarry judge charges particular ungrammatical example highlights benet combined approach predictions chained uently abstractive system note abstractive system requires access source document distillation experiments tried use output selection training input abstractive models showed drastic decrease model performance analysis copying pointer generator models ability abstract summary use copy mechanism causes summaries extractive table shows copying percentage generated words source document decreases reference summaries abstractive novel words attention leads reduction half percent generated summaries typically longer words ference abstractive system attention novel word summary shows benet abstractive models ity produce better paraphrasing ability create uent summaries extractive process table shows speech tags novel generated words observe interesting effect application tion leads sharp decrease novel adjectives data pointer generator length penalty coverage penalty trigram repeat r l table results cnn dm adding ence penalty time conclusion work presents simple accurate tent selection model summarization ties phrases document likely cluded summary showed tent selector tion restricts ability abstractive marizers copy words source combined summarization system leads improvements rouge scores points cnn dm nyt corpora comparison end end trained methods showed particular problem easily solved single model instead requires tuned inference restrictions finally showed technique data efciency adjust trained model data points making easy transfer new main preliminary work investigates similar approaches domains quire content selection grammar tion data text generation shown promise investigated future work acknowledgements like thank barbara j grosz ful discussions feedback early stages work thank anonymous reviewers work supported sung research award yd funded bloomberg research award sg funded nih grant references peter anderson xiaodong chris buehler damien teney mark johnson stephen gould lei zhang attention arxiv preprint image captioning vqa valerie anderson suzanne hidi figure copied words tion length copied phrases black lines indicate reference summaries bars summaries tention nouns fraction novel words verbs sharply increases looking novel verbs generated notice high percentage tense number changes indicated variation word example said says novel nouns morphological variants words source figure shows length phrases copied copied phrases reference summaries groups words pointer generator copies long quences sentences words content selection mask interrupts long copy sequences model ate unselected words tion probability use different word instead observed cases frequently generated summaries fraction long copied phrases decreases attention distribution length copied phrases different reference inference penalty analysis analyze effect inference time loss functions ble presents marginal improvements simple pointer generator adding penalty time observe ties improve scores added indicates unmodied pointer generator model ready learned appropriate representation abstractive summarization problem limited ineffective content selection inference methods generatorbottom attentioncopy actions different ing students summarize educational leadership dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly arxiv preprint learning align translate duy duc bui guilherme del fiol john f hurdle siddhartha jonnalagadda extractive text summarization system aid data extraction text systematic review development journal biomedical informatics timothy j buschman earl k miller control attention science prefrontal posterior parietal cortices asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents proceedings abstractive summarization conference north american chapter association computational linguistics man language technologies volume long pers volume pages yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting arxiv preprint jianpeng cheng li dong mirella lapata long short term memory networks machine reading arxiv preprint greg durrett taylor berg kirkpatrick dan klein learning based single document tion compression anaphoricity constraints arxiv preprint matt gardner joel grus mark neumann oyvind tafjord pradeep dasigi nelson liu matthew ters michael schmitz luke zettlemoyer allennlp deep semantic natural language cessing platform arxiv preprint jiatao gu zhengdong lu hang li victor ok incorporating copying mechanism arxiv preprint li sequence sequence learning karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend advances ral information processing systems pages wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun ed model extractive abstractive arxiv preprint rization inconsistency loss hongyan jing kathleen r mckeown decomposition human written summary tences proceedings annual tional acm sigir conference research velopment information retrieval pages jianpeng cheng mirella lapata neural summarization extracting sentences words arxiv preprint yoon kim yacine jernite david sontag der m rush character aware neural language models aaai pages sumit chopra michael auli alexander m rush abstractive sentence summarization tentive recurrent neural networks proceedings conference north american ter association computational linguistics human language technologies pages arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang nazli goharian discourse aware attention model abstractive summarization long documents proceedings conference north american chapter association tional linguistics human language technologies volume short papers volume pages alexander dlikman mark machine learning methods linguistic features single document extractive summarization pkdd ecml pages bonnie dorr david zajic richard schwartz hedge trimmer parse trim approach headline generation proceedings naacl text summarization workshop volume pages association computational guistics guillaume klein yoon kim yuntian deng jean senellart alexander m rush opennmt open source toolkit neural machine translation arxiv preprint chenliang li weiran xu si li sheng gao guiding generation abstractive text tion based key information guide network proceedings conference north american chapter association tional linguistics human language technologies volume short papers volume pages piji li lidong bing wai lam critic based training framework abstractive marization arxiv preprint peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser noam shazeer generating wikipedia arxiv preprint summarizing long sequences ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments aaai pages ramesh nallapati bowen zhou caglar gulcehre bing xiang al abstractive text marization sequence sequence rnns yond arxiv preprint zhaopeng tu zhengdong lu yang liu xiaohua liu hang li modeling coverage arxiv preprint neural machine translation ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention need advances neural information cessing systems pages oriol vinyals meire fortunato navdeep jaitly pointer networks advances neural formation processing systems pages yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al google s neural chine translation system bridging gap arxiv preprint human machine translation wenyuan zeng wenjie luo sanja fidler raquel efcient summarization arxiv preprint urtasun read copy mechanism qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural ment summarization jointly learning score select sentences proceedings annual meeting association computational guistics volume long papers volume pages ramesh nallapati bowen zhou mingbo ma classify select neural architectures extractive document summarization arxiv preprint ramakanth pasunuru mohit bansal reward reinforced summarization saliency entailment proceedings conference north american chapter association computational linguistics human language technologies volume short papers volume pages romain paulus caiming xiong richard socher deep reinforced model abstractive marization arxiv preprint jeffrey pennington richard socher christopher manning glove global vectors word representation proceedings ence empirical methods natural language cessing emnlp pages matthew e peters waleed ammar chandra ula russell power semi supervised quence tagging bidirectional language models arxiv preprint matthew e peters mark neumann mohit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word resentations arxiv preprint alexander m rush sumit chopra jason neural attention model arxiv preprint ston stractive sentence summarization evan sandhaus new york times annotated corpus linguistic data consortium philadelphia abigail peter j liu christopher d point summarization arxiv preprint ning pointer generator networks ilya sutskever oriol vinyals quoc v le sequence sequence learning neural works advances neural information ing systems pages jiwei tan xiaojun wan jianguo xiao abstractive document summarization proceedings based attentional neural model annual meeting association putational linguistics volume long papers volume pages examples reference content selection reference content selection generated summary green bay packers successful season largely quarterback brett favre ahman green rushed yards victory giants true dorsey levens good start teams green backup contributed kickoff returns yards playoff bound green bay packers beat giants victory packers won games paul byers pioneer visual anthropology dies age paul byers early practitioner mead died dec home manhattan enlisted navy trained cryptanalyst stationed australia paul byers early practitioner anthropology pioneered garet mead table domain transfer examples domain transfer examples present generated summaries cnn dm nyt domain transfer experiment table refers pointer generator coverage penalty trained cnn dm scores rouge l nyt dataset content selection improves rouge l ne tuning model
