hybrid word character model abstractive summarization chieh teng chang chi chia huang chih yuan yang jane yung jen hsu department computer science information engineering national taiwan university taipei taiwan com yangchihyuan ntu edu tw abstract automatic abstractive text tion important challenging search topic natural language ing widely languages chinese language special erty chinese character contains rich information comparable word isting chinese text summarization ods adopt totally character based word based representations fail fully exploit information carried representations accurately capture essence articles propose hybrid word character approach hwc preserves advantages based character based representations evaluate advantage proposed hwc approach applying ing methods discover generates state art performance gin rouge points widely dataset lcsts addition nd sue contained lcsts dataset offer script remove overlapping pairs summary short text create clean dataset community posed hwc approach generates best performance new clean sts dataset introduction text summarization aims create short uent effective summary long text document rapid growth information stored textual form digital documents summaries greatly help address text data able online searching useful information consuming relevant articles text summarization covers types general summarization proves effectiveness indexing reduces bias caused humans query focused marization takes preferences consideration satisfy individual needs information document summarization aims generate maries multiple documents topic extractive summarization combines group informative pieces text source changing abstractive summarization generates entirely new sentences absorbing information contained source text moreno gambhir gupta recently rapid development neural works brings signicant advances text marization especially family tional sequence sequence tures sutskever et al bahdanau et al generates promising performance abstractive summarization rush et al nallapati et al learn internal guage representation large number amples generate summaries similar ones humans language critical factor text summarization studies chinese large number users tinuous use long history widespread inuence east asia chinese signicantly different european languages especially character representation word tation old chinese characters oped thousands years ago monosyllabic structure words derived modern chinese inherits bank characters numbering tens thousands composing words ther single multiple characters tuations widely modern chinese arate sentences delimiter sentence isolate words word segmentation error prone process largely affects result automatic summarization ayana et al ma et al chinese p e s l c s c v v x r ters semantic meanings polysemous existing studies use character based sentation simplify effort prevent certainty segmentation hu et al chen et al li et al ayana et al ma et al li et al existing methods test based representation hu et al gu et al performances slightly proved worse use word based resentation source target text doubtful approach relies site sufcient training samples terms curred words lengths source target text text summarization sample initely asymmetric summaries targets surely shorter source articles questionable dataset large provide satisfactory size target text issue memory limitation chinese word composed single multiple chinese characters representing given chinese text dataset words instead acters means signicantly increasing lary size text summarization algorithm plemented encoder decoder framework running gpu fast execution size vocabulary restricted gpu s ory capacity best knowledge gpu platforms expand overall memory ity training text summarization models developed existing methods use word based representation decoder use selected subset complete vocabulary bank extracted target text usually high frequency words size vocabulary subset large low frequency words text replaced unknown token lose messages results incomplete summaries low rouge scores address problem propose hybrid word character hwc approach uses brid embedding units encoder decoder preserve advantages word based character based representations encoder contain softmax layer computational load memory requirement far decoder jean et al feasible apply word based representation coder use large vocabulary bank mental results approach works encoder decoder summarization methods generates state art performance widely chinese text summarization dataset related work extractive abstractive summarization merous automatic summarization methods proposed literature formats generated summaries categorize existing ods classes extractive abstractive extractive summarization selects keywords sentences original text arranges form summary luhn erkan radev mihalcea tarau cheng lapata abstractive summarization erates brief version original text serve information content ansd overall ing extractive summarization developed lier highly simplies text summarization text partition selection abstractive summarization heavily studied recently challenge practicability early studies abstractive approach include statistical machine translation techniques banko et al knight marcu deletion compression methods cohn lapata filippova et al rapid spread neural works recent studies build models encoder decoder framework especially tentional model rush et al lapati et al promising performance chinese text summarization datasaet tion english rst language studied text summarization large number nese users motivates studies explore language features rst compiled chinese text dataset available text summarization chinese gigaword graff chen contains comprehensive archive newswire text data quired chinese news sources tral news agency taiwan xinhua news agency beijing years corpus impressive richness ther free charge thoroughly categorized lack human evaluation quality summaries titles news reports contrary lcsts large scale chinese short text summarization dataset hu et al created academic research text sources news reports titles collected nese microblogging website croblogging articles length restricted der short text s limit collected ticles consistent dataset provides predened training test subsets manually labeled ity indexes test summaries free assess long term maintenance open source implementation rapid opment signicant advances puter science elds motivated trend publicly available algorithm libraries opencv bradski computer vision openmnt klein et al neural machine translation platforms provide great nience test new ideas reproduce results optimize performance libraries maintained new methods soon available easily called unied interface searchers benet reducing fort implementation preventing errors misunderstanding saving time ing experiments considerable merits validate proposed hwc approach openmnt library language translation text summarization language translation distinct problems family natural language processing share certain similar properties representation sequential data sion text tokens problems highly close possible apply models veloped problem e ing source target languages tion problem source target text inspired summarization problem nt breakthrough translation problem wu et al gehring et al gehring et al vaswani et al apply posed hwc approach state art lation model nd integrated method generates leading edge performance proposed method illustrated figure proposed hwc proach encoder decoder framework input articles represented words output summaries characters verify effectiveness proposed hwc approach baseline method tional state art vaswani et al figure input output formats proposed hwc approach encoder decoder summarization method different existing methods use xed type embedding units coder decoder shown table posed method uses word embedding represent input articles character embedding output summaries design motivated observation chinese language words precise provide information ters chinese characters highly mous characters shorter exible words hand input data ambiguous represent articles words characters lcsts dataset use experimental validation words mere characters word bedding units expands limit vocabulary bank terms size train effective encoder capture meaning contained word lationship hand output targets summarized sentences highly trated common chinese shorten long phrases convenience e international olympic committee abbreviated term olympic game reduced committee characters exible words resent output sentences nitely shorter length experimental setup experiments conducted machine equipped core cpu g ory high performance gpu nvidia ti use open source implementation code released original authors duce results transformer vocabulary size word based copynet character based copynet dgrd ac abs distraction wean encoder usage decoder usage table vocabulary sizes available sts dataset different representation units existing methods low portion vocabulary word based tion tested existing methods caused limitation gpu memory words mented documents jeiba utilities methods respectively rouge scores rnn rnn context methods lcsts dataset original authors train transformer methods low existing methods rnn copynet split set distinct training idation sets original authors port random splitting explicit ting mechanism use long standing random number generator ve seeds select articles validation sets use remaining training sets report mean rouge scores dataset conduct experiments lcsts dataset hu et al evaluate posed method dataset contains large number short chinese news articles headlines short summaries collected sina chinese microblogging site dataset composed parts shown table contains large number pairs articles headlines annotation parts ii iii contain text data human labeled scores measuring quality summaries terms relevance source articles difference parts ii iii numbers annotators create scores ii iii relevance scores range larger relevant fair comparison follow split setting existing methods hu et al gu et al chen et al li et al ayana et al ma et al li et al weibo ii iii version clean clean articles scores n table statistics lcsts datasets different versions version updated version replacing articles appear iii newly collected articles number articles change version clean rened strict criterion remove articles highly similar articles iii sharing summaries use training set iii s relevance subset scores equivalent greater testing set lcsts examining rst released sion lcsts dataset found iii contains high ratio articles repeated reported problem authors received response released correct dataset failed lter common articles parts iii deal problem released dataset actually experiments replaced overlapping articles newly collected ones assigned new version lcsts clean scrutinizing lcsts nd cleanup complete items lcsts s items iii terms exactly summaries highly similar articles differing characters end articles source newspaper example shown table source newspaper shanghai morning post contribute message carried article sue likely weaken dataset remove highly repeated items amended dataset lcsts clean order evaluate proposed method dataset remove highly ping items split lcsts lcsts clean script ate lcsts clean github repository reproducing experimental results searchers summary article table example highly overlapping items lcsts dataset item split differs item iii split presence term shanghai morning post end cle evaluation metrics adopt rouge lin metrics uation widely tive summarization measure quality summaries computing overlap generated reference ones fair ison report gram bigrams rouge l longest common sequence scores compared methods compared methods rnn rnn context hu et al similar rnn based methods context generator included simpler architecture context generator decoder uses rnn encoder s state input data complexer architecture context generator nected gated recurrent units hidden states uses generated context generate summaries copynet gu et al integrates copying mechanism attentional model order combine selected subsequences input sequence generate output sequence distraction chen et al framework distracts document ent regions content order better grasp overall meaning input document drgn li et al attentional model equipped latent structure modeling component mrt ayana et al employs minimum risk training strategy attentional model wean ma et al based tional model generates summaries querying distributed word representations attention mechanism decoder ac abs li et al employs actor critic approach originally developed reinforcement learning attentional model method simplest tentional model additional component use implementation available openmnt system klein et al evaluate baseline transformer vaswani et al newly veloped encoder decoder method uses tention mechanisms complex recurrent convolutional neural networks adopt model chinese abstractive summarization rst report performance preprocessing hyperparameters adopt approach rnn context copynet segment input articles words ing jieba segmentation hyperparameter vocabulary size iments numbers shown table ranges rnn context net large numbers assess extent numbers words training sets occurrence quency greater eters method cally set numbers embedding dimension hidden layers use adagrad duchi et al optimizer set initial learning rate dropout rate set beam size decoders periments transformer method use default parameters implementation able opennmt py results discussion evaluating proposed model sts dataset nd dataset s original version contains overlapping items rm issue authors experiments fair comparisons ing methods original version new versions clean fairly evaluate performance rouge scores datasets shown tables respectively original version method generates best score signicant margin existing ods method lcsts python org pypi cc encoder decoder measure vocab size rouge l method copynet gu et al hu et al base char char word char distraction chen et al char dgrd li et al char mrt ayana et al char wean ma et al char ac abs li et al char char max vocab size word word max vocab size word char transformer char transformer max vocab size word vocab size base char char word char char char char char char char char char char char char char n cases maximal table scores rouge measures lcsts dataset vocabulary size report numbers vocabularies available training splits overall dataset training plus validation numbers slightly smaller ones reported table score method given authors rouge scores unavailable encoder decoder measure vocab size rouge l method rnn hu et al max vocab size base char word char word char char word vocab size base char word char word char char char table scores rouge measures lcsts dataset report scores rnn methods table authors addendum explained section encoder decoder measure vocab size rouge l method base char char max vocab size word max vocab size word char transformer char transformer max vocab size word vocab size base char char char char char char char table scores rouge measures lcsts clean dataset n clean dataset rouge scores decline ably shows importance evaluate ods dataset vocabulary size shown tables larger vocabulary banks lead higher rouge scores lctst dataset char based method generates higher rouge scores existing methods merely ing vocabulary bank lctst clean datasets char based method benets large vocabulary banks adopting hwc approach method word based encoder uses larger vocabulary banks generates better rouge scores lcsts datasets transformer method generates mance hwc approach large vocabulary bank effects hwc approach datasets lcsts clean method generates higher rouge scores method improvements observed transformer method lcsts clean dataset table outlines enhancement shows overlapping data result larger creases rouge scores indicates model aggressive learner effectively adapts hidden states training data generates high rouge scores test data overlap training ones rouge l lcsts transformer lcsts clean transformer table rouge scores improved ing hwc approach encoder decoder methods analysis generated summaries order reason success caused hwc approach example summaries erated methods transformer table including source articles man references examples method better catches major messages tained source articles lars article private trusts article executive meetings li keqiang article xian ge qing xiaomi compare article contrast character based transformer method tends copy sentence source article miss point ample copies love football family nancial status supported article misses key term dollars highest level rich article misses key term private trust combining ternet boxes xiaomi routers compare misses key term xian ge qing proposed hwc approach helps detect key terms leads higher rouge scores training time proposed hwc approach advantages improving formance increasing training ciency speedup chart ure experiments shown table ducted lctst clean dataset cabulary sizes maxima method memory limitation methods use fewer epoches generate high scores applying hwc proach note unit horizontal axis minute epoch means actual execution time taken ation method hwc proach increases mean training time epoch minutes transformer method reduces time utes result caused tension factors word based representation ens length input sequences reduces level training difculty increases load updating vocabulary embedding encoder larger vocabulary size figure training time performance ison evaluated methods proposed hwc approach conclusion future study paper propose hybrid representation approach improve performance text marization methods encoder decoder work experimental results demonstrate kick ball opening eyes fans ecstatic standing asian cup blindfolded eyes china won place world brazil situation different blind footed veteran lin jinbiao training camp time covers accommodation training time salary dollars day love football family nancial status supported reference china blind footed player won place world salary dollars day transformer lin jinbiao love football family nancial status supported football veteran china training camp salary dollars highest level rich money share controlled fact biggest characteristic private trusts cross life cycles help customers manage personal wealth life inherit distribute wealth generations unique privacy secrecy stability wealthy individuals family business pursuit china reference private trusts create new bridge wealth inheritance transformer highest level rich private trusts cross life cycles visiting participating important events premier li keqiang hosts state council executive meeting wednesday executive meetings times wednesday months subject meetings strung red line shows trajectory changes major policies policies governance policy current government reference analyze li keqiang s way governance executive meetings transformer state council executive meetings strung red line analyze li keqiang s executive meetings months xian ge qing rename zhongke cloud network meng kai lled expectation work big data concept believe upgrading radio tv network equivalent combining radio tv nuclear weapons combining internet boxes xiaomi routers compare reference xian ge qing works cable tv combining xiaomi s products compare transformer meng kai combining internet routers compare chairman xian ge qing talks upgrading radio tv networks xiaomi routers compare table example summaries generated clean dataset proposed approach clearly generates state addition nd art performance rors widely dataset provide script polish improved performance better representation ing speech tagging proposed proach interested applicability proposed approach natural language processing problems dialogue generation machine translation references ayana et al ayana shiqi shen zhiyuan liu maosong sun neural headline ation minimum risk training arxiv preprint bahdanau et al dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate iclr banko et al michele banko vibhu o mittal michael j witbrock headline tion based statistical translation acl pages gary bradski opencv brary dr dobb s journal software tools chen et al qian chen xiaodan zhu zhenhua ling si wei hui jiang based neural networks modeling documents ijcai pages cheng jianpeng cheng mirella lapata neural summarization extracting sentences words acl pages cohn trevor cohn mirella ata sentence compression word tion coling pages duchi et al john duchi elad hazan yoram singer adaptive subgradient methods line learning stochastic optimization journal machine learning research erkan gunes erkan dragomir r radev lexrank graph based lexical trality salience text summarization journal articial intelligence research filippova et al katja filippova enrique seca carlos colmenares lukasz kaiser oriol vinyals sentence compression tion lstms emnlp pages gambhir mahak gambhir vishal gupta recent automatic text summarization techniques survey articial intelligence review gehring et al jonas gehring michael auli david grangier yann n dauphin convolutional encoder model neural machine translation acl gehring et al jonas gehring michael auli david grangier denis yarats yann n dauphin convolutional sequence sequence ing icml graff david graff ke chen chinese gigaword linguistic data consortium gu et al jiatao gu zhengdong lu hang li victor o k li incorporating copying acl anism sequence sequence learning pages et al baotian hu qingcai chen fangze zhu lcsts large scale chinese short text emnlp pages summarization dataset jean et al sebastien jean kyunghyun cho roland memisevic yoshua bengio large target vocabulary neural chine translation arxiv preprint klein et al guillaume klein yoon kim tian deng jean senellart alexander m rush open source toolkit neural machine translation acl pages knight kevin knight daniel marcu statistics based summarization step aaai iaai pages sentence compression et al piji li wai lam lidong bing hao wang deep recurrent generative decoder emnlp abstractive text summarization pages et al piji li lidong bing wai lam actor critic based training framework arxiv preprint abstractive summarization chin yew lin rouge package automatic evaluation summaries ings workshop pages hans peter luhn automatic creation literature abstracts ibm journal search development et al shuming ma xu sun wei li sujian li wenjie li xuancheng ren word embedding attention network generating words querying distributed word representations phrase generation naacl hlt mihalcea rada mihalcea paul rau textrank bringing order text emnlp nallapati et al ramesh nallapati bowen zhou caglar gulcehre bing xiang al tive text summarization sequence sequence rnns signll pages rush et al alexander m rush sumit chopra jason weston neural attention model abstractive sentence summarization emnlp pages sutskever et al ilya sutskever oriol vinyals quoc v le sequence sequence nips pages ing neural networks torres juan manuel torres moreno automatic text summarization john wiley sons inc vaswani et al ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin nips pages attention need wu et al yonghui wu mike schuster zhifeng chen quoc v le mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey et al google s neural chine translation system bridging gap arxiv preprint human machine translation
