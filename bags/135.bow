hybrid word character model abstractive summarization chieh teng chang chi chia huang chih yuan yang jane yung jen hsu department computer science information engineering national taiwan university taipei taiwan com yangchihyuan ntu edu abstract automatic abstractive text tion important challenging search topic natural language ing widely languages chinese language special erty chinese character contains rich information comparable word isting chinese text summarization ods adopt totally character based word based representations fail fully exploit information carried representations accurately capture essence articles propose hybrid word character approach hwc preserves advantages based character based representations evaluate advantage proposed hwc approach applying ing methods discover generates state art performance gin rouge points widely dataset lcsts addition sue contained lcsts dataset offer script remove overlapping pairs summary short text create clean dataset community posed hwc approach generates best performance new clean sts dataset introduction text summarization aims create short uent effective summary long text document rapid growth information stored textual form digital documents summaries greatly help address text data able online searching useful information consuming relevant articles text summarization covers types general summarization proves effectiveness indexing reduces bias caused humans query focused marization takes preferences consideration satisfy individual needs information document summarization aims generate maries multiple documents topic extractive summarization combines group informative pieces text source changing abstractive summarization generates entirely new sentences absorbing information contained source text moreno gambhir gupta recently rapid development neural works brings signicant advances text marization especially family tional sequence sequence tures sutskever bahdanau generates promising performance abstractive summarization rush nallapati learn internal guage representation large number amples generate summaries similar ones humans language critical factor text summarization studies chinese large number users tinuous use long history widespread inuence east asia chinese signicantly different european languages especially character representation word tation old chinese characters oped thousands years ago monosyllabic structure words derived modern chinese inherits bank characters numbering tens thousands composing words ther single multiple characters tuations widely modern chinese arate sentences delimiter sentence isolate words word segmentation error prone process largely affects result automatic summarization ayana chinese ters semantic meanings polysemous existing studies use character based sentation simplify effort prevent certainty segmentation chen ayana existing methods test based representation performances slightly proved worse use word based resentation source target text doubtful approach relies site sufcient training samples terms curred words lengths source target text text summarization sample initely asymmetric summaries targets surely shorter source articles questionable dataset large provide satisfactory size target text issue memory limitation chinese word composed single multiple chinese characters representing given chinese text dataset words instead acters means signicantly increasing lary size text summarization algorithm plemented encoder decoder framework running gpu fast execution size vocabulary restricted gpu ory capacity best knowledge gpu platforms expand overall memory ity training text summarization models developed existing methods use word based representation decoder use selected subset complete vocabulary bank extracted target text usually high frequency words size vocabulary subset large low frequency words text replaced unknown token lose messages results incomplete summaries low rouge scores address problem propose hybrid word character hwc approach uses brid embedding units encoder decoder preserve advantages word based character based representations encoder contain softmax layer computational load memory requirement far decoder jean feasible apply word based representation coder use large vocabulary bank mental results approach works encoder decoder summarization methods generates state art performance widely chinese text summarization dataset related work extractive abstractive summarization merous automatic summarization methods proposed literature formats generated summaries categorize existing ods classes extractive abstractive extractive summarization selects keywords sentences original text arranges form summary luhn erkan radev mihalcea tarau cheng lapata abstractive summarization erates brief version original text serve information content ansd overall ing extractive summarization developed lier highly simplies text summarization text partition selection abstractive summarization heavily studied recently challenge practicability early studies abstractive approach include statistical machine translation techniques banko knight marcu deletion compression methods cohn lapata filippova rapid spread neural works recent studies build models encoder decoder framework especially tentional model rush lapati promising performance chinese text summarization datasaet tion english rst language studied text summarization large number nese users motivates studies explore language features rst compiled chinese text dataset available text summarization chinese gigaword graff chen contains comprehensive archive newswire text data quired chinese news sources tral news agency taiwan xinhua news agency beijing years corpus impressive richness ther free charge thoroughly categorized lack human evaluation quality summaries titles news reports contrary lcsts large scale chinese short text summarization dataset created academic research text sources news reports titles collected nese microblogging website croblogging articles length restricted der short text limit collected ticles consistent dataset provides predened training test subsets manually labeled ity indexes test summaries free assess long term maintenance open source implementation rapid opment signicant advances puter science elds motivated trend publicly available algorithm libraries opencv bradski computer vision openmnt klein neural machine translation platforms provide great nience test new ideas reproduce results optimize performance libraries maintained new methods soon available easily called unied interface searchers benet reducing fort implementation preventing errors misunderstanding saving time ing experiments considerable merits validate proposed hwc approach openmnt library language translation text summarization language translation distinct problems family natural language processing share certain similar properties representation sequential data sion text tokens problems highly close possible apply models veloped problem ing source target languages tion problem source target text inspired summarization problem breakthrough translation problem gehring gehring vaswani apply posed hwc approach state art lation model integrated method generates leading edge performance proposed method illustrated figure proposed hwc proach encoder decoder framework input articles represented words output summaries characters verify effectiveness proposed hwc approach baseline method tional state art vaswani figure input output formats proposed hwc approach encoder decoder summarization method different existing methods use xed type embedding units coder decoder shown table posed method uses word embedding represent input articles character embedding output summaries design motivated observation chinese language words precise provide information ters chinese characters highly mous characters shorter exible words hand input data ambiguous represent articles words characters lcsts dataset use experimental validation words mere characters word bedding units expands limit vocabulary bank terms size train effective encoder capture meaning contained word lationship hand output targets summarized sentences highly trated common chinese shorten long phrases convenience international olympic committee abbreviated term olympic game reduced committee characters exible words resent output sentences nitely shorter length experimental setup experiments conducted machine equipped core cpu ory high performance gpu nvidia use open source implementation code released original authors duce results transformer vocabulary size word based copynet character based copynet dgrd abs distraction wean encoder usage decoder usage table vocabulary sizes available sts dataset different representation units existing methods low portion vocabulary word based tion tested existing methods caused limitation gpu memory words mented documents jeiba utilities methods respectively rouge scores rnn rnn context methods lcsts dataset original authors train transformer methods low existing methods rnn copynet split set distinct training idation sets original authors port random splitting explicit ting mechanism use long standing random number generator seeds select articles validation sets use remaining training sets report mean rouge scores dataset conduct experiments lcsts dataset evaluate posed method dataset contains large number short chinese news articles headlines short summaries collected sina chinese microblogging site dataset composed parts shown table contains large number pairs articles headlines annotation parts iii contain text data human labeled scores measuring quality summaries terms relevance source articles difference parts iii numbers annotators create scores iii relevance scores range larger relevant fair comparison follow split setting existing methods chen ayana weibo iii version clean clean articles scores table statistics lcsts datasets different versions version updated version replacing articles appear iii newly collected articles number articles change version clean rened strict criterion remove articles highly similar articles iii sharing summaries use training set iii relevance subset scores equivalent greater testing set lcsts examining rst released sion lcsts dataset found iii contains high ratio articles repeated reported problem authors received response released correct dataset failed lter common articles parts iii deal problem released dataset actually experiments replaced overlapping articles newly collected ones assigned new version lcsts clean scrutinizing lcsts cleanup complete items lcsts items iii terms exactly summaries highly similar articles differing characters end articles source newspaper example shown table source newspaper shanghai morning post contribute message carried article sue likely weaken dataset remove highly repeated items amended dataset lcsts clean order evaluate proposed method dataset remove highly ping items split lcsts lcsts clean script ate lcsts clean github repository reproducing experimental results searchers summary article table example highly overlapping items lcsts dataset item split differs item iii split presence term shanghai morning post end cle evaluation metrics adopt rouge lin metrics uation widely tive summarization measure quality summaries computing overlap generated reference ones fair ison report gram bigrams rouge longest common sequence scores compared methods compared methods rnn rnn context similar rnn based methods context generator included simpler architecture context generator decoder uses rnn encoder state input data complexer architecture context generator nected gated recurrent units hidden states uses generated context generate summaries copynet integrates copying mechanism attentional model order combine selected subsequences input sequence generate output sequence distraction chen framework distracts document ent regions content order better grasp overall meaning input document drgn attentional model equipped latent structure modeling component mrt ayana employs minimum risk training strategy attentional model wean based tional model generates summaries querying distributed word representations attention mechanism decoder abs employs actor critic approach originally developed reinforcement learning attentional model method simplest tentional model additional component use implementation available openmnt system klein evaluate baseline transformer vaswani newly veloped encoder decoder method uses tention mechanisms complex recurrent convolutional neural networks adopt model chinese abstractive summarization rst report performance preprocessing hyperparameters adopt approach rnn context copynet segment input articles words ing jieba segmentation hyperparameter vocabulary size iments numbers shown table ranges rnn context net large numbers assess extent numbers words training sets occurrence quency greater eters method cally set numbers embedding dimension hidden layers use adagrad duchi optimizer set initial learning rate dropout rate set beam size decoders periments transformer method use default parameters implementation able opennmt results discussion evaluating proposed model sts dataset dataset original version contains overlapping items issue authors experiments fair comparisons ing methods original version new versions clean fairly evaluate performance rouge scores datasets shown tables respectively original version method generates best score signicant margin existing ods method lcsts python org pypi encoder decoder measure vocab size rouge method copynet base char char word char distraction chen char dgrd char mrt ayana char wean char abs char char max vocab size word word max vocab size word char transformer char transformer max vocab size word vocab size base char char word char char char char char char char char char char char char char cases maximal table scores rouge measures lcsts dataset vocabulary size report numbers vocabularies available training splits overall dataset training plus validation numbers slightly smaller ones reported table score method given authors rouge scores unavailable encoder decoder measure vocab size rouge method rnn max vocab size base char word char word char char word vocab size base char word char word char char char table scores rouge measures lcsts dataset report scores rnn methods table authors addendum explained section encoder decoder measure vocab size rouge method base char char max vocab size word max vocab size word char transformer char transformer max vocab size word vocab size base char char char char char char char table scores rouge measures lcsts clean dataset clean dataset rouge scores decline ably shows importance evaluate ods dataset vocabulary size shown tables larger vocabulary banks lead higher rouge scores lctst dataset char based method generates higher rouge scores existing methods merely ing vocabulary bank lctst clean datasets char based method benets large vocabulary banks adopting hwc approach method word based encoder uses larger vocabulary banks generates better rouge scores lcsts datasets transformer method generates mance hwc approach large vocabulary bank effects hwc approach datasets lcsts clean method generates higher rouge scores method improvements observed transformer method lcsts clean dataset table outlines enhancement shows overlapping data result larger creases rouge scores indicates model aggressive learner effectively adapts hidden states training data generates high rouge scores test data overlap training ones rouge lcsts transformer lcsts clean transformer table rouge scores improved ing hwc approach encoder decoder methods analysis generated summaries order reason success caused hwc approach example summaries erated methods transformer table including source articles man references examples method better catches major messages tained source articles lars article private trusts article executive meetings keqiang article xian qing xiaomi compare article contrast character based transformer method tends copy sentence source article miss point ample copies love football family nancial status supported article misses key term dollars highest level rich article misses key term private trust combining ternet boxes xiaomi routers compare misses key term xian qing proposed hwc approach helps detect key terms leads higher rouge scores training time proposed hwc approach advantages improving formance increasing training ciency speedup chart ure experiments shown table ducted lctst clean dataset cabulary sizes maxima method memory limitation methods use fewer epoches generate high scores applying hwc proach note unit horizontal axis minute epoch means actual execution time taken ation method hwc proach increases mean training time epoch minutes transformer method reduces time utes result caused tension factors word based representation ens length input sequences reduces level training difculty increases load updating vocabulary embedding encoder larger vocabulary size figure training time performance ison evaluated methods proposed hwc approach conclusion future study paper propose hybrid representation approach improve performance text marization methods encoder decoder work experimental results demonstrate kick ball opening eyes fans ecstatic standing asian cup blindfolded eyes china won place world brazil situation different blind footed veteran lin jinbiao training camp time covers accommodation training time salary dollars day love football family nancial status supported reference china blind footed player won place world salary dollars day transformer lin jinbiao love football family nancial status supported football veteran china training camp salary dollars highest level rich money share controlled fact biggest characteristic private trusts cross life cycles help customers manage personal wealth life inherit distribute wealth generations unique privacy secrecy stability wealthy individuals family business pursuit china reference private trusts create new bridge wealth inheritance transformer highest level rich private trusts cross life cycles visiting participating important events premier keqiang hosts state council executive meeting wednesday executive meetings times wednesday months subject meetings strung red line shows trajectory changes major policies policies governance policy current government reference analyze keqiang way governance executive meetings transformer state council executive meetings strung red line analyze keqiang executive meetings months xian qing rename zhongke cloud network meng kai lled expectation work big data concept believe upgrading radio network equivalent combining radio nuclear weapons combining internet boxes xiaomi routers compare reference xian qing works cable combining xiaomi products compare transformer meng kai combining internet routers compare chairman xian qing talks upgrading radio networks xiaomi routers compare table example summaries generated clean dataset proposed approach clearly generates state addition art performance rors widely dataset provide script polish improved performance better representation ing speech tagging proposed proach interested applicability proposed approach natural language processing problems dialogue generation machine translation references ayana ayana shiqi shen zhiyuan liu maosong sun neural headline ation minimum risk training arxiv preprint bahdanau dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate iclr banko michele banko vibhu mittal michael witbrock headline tion based statistical translation acl pages gary bradski opencv brary dobb journal software tools chen qian chen xiaodan zhu zhenhua ling wei hui jiang based neural networks modeling documents ijcai pages cheng jianpeng cheng mirella lapata neural summarization extracting sentences words acl pages cohn trevor cohn mirella ata sentence compression word tion coling pages duchi john duchi elad hazan yoram singer adaptive subgradient methods line learning stochastic optimization journal machine learning research erkan gunes erkan dragomir radev lexrank graph based lexical trality salience text summarization journal articial intelligence research filippova katja filippova enrique seca carlos colmenares lukasz kaiser oriol vinyals sentence compression tion lstms emnlp pages gambhir mahak gambhir vishal gupta recent automatic text summarization techniques survey articial intelligence review gehring jonas gehring michael auli david grangier yann dauphin convolutional encoder model neural machine translation acl gehring jonas gehring michael auli david grangier denis yarats yann dauphin convolutional sequence sequence ing icml graff david graff chen chinese gigaword linguistic data consortium jiatao zhengdong hang victor incorporating copying acl anism sequence sequence learning pages baotian qingcai chen fangze zhu lcsts large scale chinese short text emnlp pages summarization dataset jean sebastien jean kyunghyun cho roland memisevic yoshua bengio large target vocabulary neural chine translation arxiv preprint klein guillaume klein yoon kim tian deng jean senellart alexander rush open source toolkit neural machine translation acl pages knight kevin knight daniel marcu statistics based summarization step aaai iaai pages sentence compression piji wai lam lidong bing hao wang deep recurrent generative decoder emnlp abstractive text summarization pages piji lidong bing wai lam actor critic based training framework arxiv preprint abstractive summarization chin yew lin rouge package automatic evaluation summaries ings workshop pages hans peter luhn automatic creation literature abstracts ibm journal search development shuming sun wei sujian wenjie xuancheng ren word embedding attention network generating words querying distributed word representations phrase generation naacl hlt mihalcea rada mihalcea paul rau textrank bringing order text emnlp nallapati ramesh nallapati bowen zhou caglar gulcehre bing xiang tive text summarization sequence sequence rnns signll pages rush alexander rush sumit chopra jason weston neural attention model abstractive sentence summarization emnlp pages sutskever ilya sutskever oriol vinyals quoc sequence sequence nips pages ing neural networks torres juan manuel torres moreno automatic text summarization john wiley sons inc vaswani ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin nips pages attention need yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey google neural chine translation system bridging gap arxiv preprint human machine translation
