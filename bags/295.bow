mlsum the multilingual summarization corpus thomas paul alexis sylvain lamprier benjamin jacopo cnrs france sorbonne universite cnrs paris france recital paris france thomas jacopo paul ai sylvain lamprier benjamin fr r a l c s c v v i x r a abstract we present mlsum the rst large scale tilingual summarization dataset obtained from online newspapers it contains ticle summary pairs in ve different languages namely french german spanish russian turkish together with english newspapers from the popular cnn daily mail dataset the collected data form a large scale multilingual dataset which can enable new research tions for the text summarization community we report cross lingual comparative analyses based on state of the art systems these light existing biases which motivate the use of a multi lingual dataset introduction the document summarization task requires eral complex language abilities understanding a long document discriminating what is relevant and writing a short synthesis over the last few years advances in deep learning applied to nlp have contributed to the rising popularity of this task among the research community see et al kryscinski et al scialom et al as with other nlp tasks the great majority of available datasets for summarization are in english and thus most research efforts focus on the glish language the lack of multilingual data is partially countered by the application of transfer learning techniques enabled by the availability of pre trained multilingual language models this proach has recently established itself as the facto paradigm in nlp guzman et al under this paradigm for encoder decoder tasks a language model can rst be pre trained on a large corpus of texts in multiple languages then the model is ne tuned in one or more pivot languages for which the task specic data are available at inference it can still be applied to the different guages seen during the pre training because of the dominance of english for large scale corpora english naturally established itself as a pivot for other languages the availability of multilingual pre trained models such as bert multilingual bert allows to build models for target languages different from training data however previous works reported a signicant performance gap tween english and the target language e for classication conneau et al and question answering lewis et al tasks a similar approach has been recently proposed for rization chi et al obtaining again a lower performance than for english for specic nlp tasks recent research efforts have produced evaluation datasets in several target languages allowing to evaluate the progress of the eld in zero shot scenarios nonetheless those proaches are still bound to using training data in a pivot language for which a large amount of tated data is available usually english this vents investigating for instance whether a given model is as tted for a specic language as for any other answers to such research questions sent valuable information to improve model mance for low resource languages in this work we aim to ll this gap for the matic summarization task by proposing a scale multilingual summarization mlsum dataset the dataset is built from online news lets and contains over m article summary pairs in languages french german spanish sian and turkish which complement an already established summarization dataset in english the contributions of this paper can be rized as follows we release the rst large scale multilingual summarization dataset we provide strong baselines from multilingual abstractive text generation models we report a comparative cross lingual ysis of the results obtained by different proaches related work multilingual text summarization over the last two decades several research works have focused on multilingual text summarization radev et al developed mead a document summarizer that works for both english and chinese litvak et al proposed to improve multilingual summarization using a netic algorithm a community driven initiative multiling giannakopoulos et al marked summarization systems on multilingual data while the multiling benchmark covers languages it provides relatively few examples in the release most proposed approaches so far have used an extractive approach given the lack of a multilingual corpus to train abstractive models duan et al more recently with the rapid progress in matic translation and text generation abstractive methods for multilingual summarization have been developed ouyang et al proposed to learn summarization models for three low resource guages somali swahili and tagalog by using an automated translation of the new york times dataset although this showed only slight ments over a baseline which considers translated outputs of an english summarizer results remain still far from human performance summarization models from translated data usually under perform as translation biases add to the difculty of rization following the recent trend of using multi lingual pre trained models for nlp tasks such as gual bert m bert pires et al or xlm lample and conneau chi et al posed to ne tune the models for summarization on english training data the assumption is that the summarization skills learned from english data can transfer to other languages on which the model has been pre trained however a signicant mance gap between english and the target language is observed following this process this sizes the crucial need of multilingual training data for summarization existing multilingual datasets the research community has produced several tilingual datasets for tasks other than tion we report two recent efforts below noting that both rely on human translations and only provide evaluation data the cross lingual nli corpus the snli pus bowman et al is a large scale dataset for natural language inference nli it is posed of a collection of human written glish sentence pairs associated with their label entailment contradiction or neutral the genre natural language inference multinli pus is an extension of snli comparable in size but including a more diverse range of text conneau et al introduced the cross lingual nli pus xnli to evaluate transfer learning from glish to other languages based on multinli a collection of test and dev pairs were translated by humans in languages mlqa given a paragraph and a question the question answering qa task consists in ing the correct answer large scale datasets such as rajpurkar et al choi et al trischler et al have driven fast progress however these datasets are only in english to assess how well models perform on other languages lewis et al recently proposed mlqa an ation dataset for cross lingual extractive qa posed of k qa instances in languages xtreme the cross lingual transfer tion of multilingual encoders benchmark covers languages over tasks the summarization task is not included in the benchmark xglue in order to train and evaluate their formance across a diverse set of cross lingual tasks liang et al recently released xglue ering both natural language understanding and generation scenarios while no summarization task is included it comprises a news title ation task the data is crawled from a commercial news website and provided in form of article title pairs for languages german english french spanish and russian com google instance see the squad leaderboard rajpurkar bert blob master multilingual github io squad existing summarization datasets we describe here the main available corpora for text summarization document understanding conference several small and high quality summarization datasets in english harman and over dang have been produced in the context of the ment understanding conference duc they are built by associating newswire articles with sponding human summaries a distinctive feature of the duc datasets is the availability of ple reference summaries this is a valuable acteristic since as found by rankel et al the correlation between qualitative and automatic metrics such as rouge lin decreases signicantly when only a single reference is given however due to the small number of training data available duc datasets are often used in a domain adaptation setup for models rst trained on larger datasets such as gigaword cnn dm nallapati et al see et al or with unsupervised methods dorr et al mihalcea and tarau barrios al gigaword again using newswire as source data the english gigaword napoles et al rush et al chopra et al corpus is acterized by its large size and the high diversity in terms of sources since the samples are not sociated with human summaries prior works on summarization have trained models to generate the headlines of an article given its incipit which duces various biases for learning models new york times corpus this large corpus for summarization consists of hundreds of thousand of articles from the new york spanning over years the articles are paired with summaries written by library scientists although grusky et al found indications of bias towards extractive approaches several search efforts have used this dataset for tion hong and nenkova durrett et al paulus et al cnn daily mail one of the most commonly used dataset for summarization nallapati et al see et al paulus et al dong et al although originally built for tion answering tasks hermann et al it consists of english articles from the cnn and the nist daily mail associated with bullet point highlights from the article when used for summarization the bullet points are typically concatenated into a single summary newsroom composed of m articles grusky et al and featuring high diversity in terms of publishers the summaries associated with english news articles were extracted from the web pages metadata they were originally written to be used in search engines and social media bigpatent sharma et al collected lion u s patent documents across several nological areas using the google patents public datasets the patents abstracts are used as target summaries lcsts the large scale chinese short text summarization dataset hu et al is built from million short texts from the sina weibo microblogging platform they are paired with summaries given by the author of each text the dataset includes summaries which were ally scored by human for their relevance mlsum as described above the vast majority of rization datasets are in english for arabic there exist the essex arabic summaries corpus easc el haj et al and kalimat el haj and koulali those comprise circa and samples respectively pontes et al posed a corpus of few hundred samples for spanish portuguese and french summaries to our edge the only large scale non english tion dataset is the chinese lcsts hu et al with the increasing interest for cross lingual els the nlp community have recently released multilingual evaluation datasets targeting cation xnli and qa lewis et al tasks as described in though still no large scale dataset is avaulable for document summarization to ll this gap we introduce mlsum the rst large scale multilingual summarization corpus our corpus provides more than millions articles in french fr german de spanish es turkish tr and russian ru being similarly built from news articles and providing a similar amount of training samples per language except for russian as the previously mentioned cnn daily mail it can effectively serve as a multilingual extension of the cnn daily mail dataset in the following we rst describe the ology used to build the corpus we then report the corpus statistics and nally interpret the mances of baselines and state of the art models collecting the corpus the cnn daily mail cnn dm dataset see tion is arguably the most used large scale dataset for summarization following the same methodology we consider news articles as the text input and their paired highlights description as the summary for each language we selected an online newspaper which met the following requirements being a generalist newspaper ensuring that a broad range of topics is represented for each language allows to minimize the risk of ing topic specic models a fact which would hinder comparative cross lingual analyses of the models having a large number of articles in their lic online archive providing written human lights summaries that for can be extracted from the html code of the web page articles the after a careful preliminary exploration we lected the online version of the following pers le french german el spanish moskovskij russian internet turkish for each outlet we crawled archived articles from to we applied one simple ter all the articles shorter than words or maries shorter than words are discarded so as to avoid articles containing mostly audiovisual tent each article was archived on the wayback allowing interested research to re build lemonde fr sueddeutsche elpais com mk ru internethaber com archive org or extend mlsum we distribute the dataset as a list of immutable snapshot urls of the articles along with the accompanying corpus construction allowing to replicate the parsing and processing procedures we employed this is due to legal reasons the content of the articles is righted and redistribution might be seen as ing of publishing rights nonetheless we make available upon request an exact copy of the dataset used in this work a similar approach has been adopted for several dataset releases in the recent past such as question answering corpus mann et al or xsum narayan et al we provide further recommended train validation test splits following a ical ordering based on the articles publication dates in our experiments below we train evaluate the models on the training test splits obtained in this manner specically we use data from to included for training data for of the dataset for validation up to may and test may december while this choice is arguably more challenging due to the possible emergence of new topics over time we consider it as the realistic scenario a successful summarization system should be able to deal with incidentally this also bring the advantage of excluding most cases of leakage across languages it prevents a model for instance from seeing a training sample describing an important event in one language and then being submitted for inference a similar article in another language published around the same time and dealing with the same event dataset statistics we report statistics for each language in sum in table including those computed on the cnn daily mail dataset english for quick parison mlsum provides a comparable amount of data for all languages with the exception of sian with ten times less training samples important characteristics for summarization datasets are the length of articles and summaries the vocabulary size and a proxy for abstractiveness namely the percentage of novel n grams between the article and its human summary from table we observe that russian summaries are the shortest as well as the most abstractive com agude wayback machine archiver com recitalai mlsum using dataset size training set size mean article length mean summary length compression ratio novelty gram total vocabulary size occurring times fr de es ru tr en table statistics for the different languages en refers to cnn daily mail and is reported for comparison purposes article and summary lengths are computed in words compression ratio is computed as the ratio between article and summary length novelty is the percentage of words in the summary that were not in the paired article total vocabulary is the total number of different words and occurring the total number of words occurring times coupled with the signicantly lower amount of articles available from its online source the task can be seen as more challenging for russian than for the other languages in mlsum conversely similar characteristics are shared among other guages for instance french and german topic shift with the exception of turkish the article urls in mlsum allow to identify a category for a given article in figure we show the shift over gories among time in particular we plot the most frequent categories per language models we experimented on mlsum with the established models and baselines described below those clude supervised and unsupervised methods tractive and abstractive models for all the iments we train models on a per language basis we used the recommended hyperparameters for all languages in order to facilitate assessing the robustness of the models we also tried to train one model with all the languages mixed together but we did not see any signicant difference of performance extractive summarization models random in order to elaborate and compare the performances of the different models across guages it is useful to include an unbiased model as a point of reference to that purpose we dene a simple random extractive model that randomly extracts n words from the source document with n xed as the average length of the summary simply selects the three rst sentences from the input text sharma et al among others showed that this is a robust baseline for several summarization datasets such as cnn dm nyt and bigpatent textrank an unsupervised algorithm proposed by mihalcea and tarau it consists in puting the co similarities between all the sentences in the input text then the most central to the ment are extracted and considered as the summary we used the implementation provided by barrios et al abstractive summarization models most of the models for abstractive summarization are neural sequence to sequence models sutskever et al composed of an encoder that encodes the input text and a decoder that generates the mary oracle extracts the sentences within the input text that maximise a given metric in our ments rouge l given the reference summary it is an indication of the maximum one could in this achieve with extractive summarization work we rely on the implementation of narayan et al pointer generator see et al proposed the addition of the copy mechanism vinyals et al on top of a sequence to sequence lstm model this mechanism allows to efciently copy out of vocabulary tokens leveraging tion bahdanau et al over the input we used the publicly available opennmt figure distribution of topics for german top left spanish top right french bottom left and russian bottom right grouped per year the shaded area for highlights validation and test data with the default hyper parameters ever to avoid biases we limited the preprocessing as much as possible and did not use any sentence separators as recommended for cnn dm this plains the relatively lower reported rouge pared to the model with the full preprocessing m bert encoder decoder transformer tectures are a very popular choice for text ation recent research efforts have adapted large pretrained self attention based models for text eration peters et al radford et al devlin et al in particular liu and lapata added a domly initialized decoder on top of bert ing the use of a decoder dong et al posed to instead add a decoder like mask during the pre training to unify the language models for both encoding and generating both these approaches achieved sota results for summarization in this paper we only report results obtained following dong et al as in preliminary experiments we observed that a simple multilingual bert bert with no modication obtained comparable performance on the summarization task net opennmt py summarization html evaluation metrics rouge arguably the most often reported set of metrics in summarization tasks the oriented understudy for gisting evaluation lin computes the number of n grams similar between the evaluated summary and the human reference summary meteor the metric for evaluation of lation with explicit ordering banerjee and lavie was designed for the evaluation of machine it is based on the harmonic translation output mean of unigram precision and recall with recall weighted higher than precision meteor is often reported in summarization papers see et al dong et al in addition to rouge novelty because of their use of copy nisms some abstractive models have been reported to rely too much on extraction see et al kryscinski et al hence it became a mon practice to report the percentage of novel grams produced within the generated summaries neural metrics several approaches based on neural models have been recently proposed recent works eyal et al scialom et al have proposed to evaluate summaries with qa based methods the rationale is that a good summary should answer the most relevant questions about the oracle random textrank lead pointer generator m bert oracle random textrank lead pointer generator m bert fr fr de de es es ru ru tr tr en en table rouge l top and meteor bottom results obtainedby the models described in on the different proposed datasets article further kryscinski et al proposed a discriminator trained to measure the factualness of the summary while bohm et al learned a metric from human annotation all these models were only trained on english datasets preventing us to report them in this paper the availability of mlsum will enable future works to build such metrics in a multilingual fashion results and discussion the results presented below allow us to compare the models across languages and investigate or pothesize where their performance variations may come from we can distinguish the following tors to explain differences in the results differences in the data independently from the language such as the structure of the cle the abstractiveness of the summaries or the quantity of data differences due to the language itself either due to metric biases e due to a different morphological type or to biases inherent to the model while the rst fold of differences have more to do with domain adaptation the second fold motivates further the development of multilingual datasets since they are the only mean to study such phenomenon turning to the observed results we report in ble the rouge l and meteor scores obtained by each model for all languages we note that the overall order of systems for each language is preserved when using either metric modulo some swaps between lead and pointer generator but with relatively close scores russian the low resource language in mlsum for all experimental setups the performance on russian is comparatively low this can be explained by at least two factors first the corpus is the most abstractive see table limiting the performance gures obtained for the extractive models random and oracle second one order of magnitude less training data is available for russian than for the other mlsum languages a fact which can explain the impressive improvement of performance in terms of rouge l see table between a not pretrained model pointer generator and a pretrained model m bert how abstractive are the models we report the novelty i e the percentage of novel words in the summary in figure as previous works reported see et al pointer generator networks are poorly abstractive relying too much on their copy mechanism it is particularly true for russian the lack of data probably makes it easier to learn to copy than to cope with natural language generation as expected pretrained guage models such as m bert are consistently more abstractive and by a large margin since they are exposed to other texts during pretraining figure percentage of novel n grams for different abstractive models neural and human for the datasets model biases toward languages consistency among rouge scores the dom model obtains comparable rouge l scores across all the languages except for russian this can be explained by the aforementioned russian corpus characteristics highest novelty shortest summaries and longest input documents see ble thus in the following for pair wise based comparisons we focus only on scores tained by the different models on french german spanish and turkish since we can not draw ingful interpretations over russian as compared to other languages abstractiveness of the datasets the oracle formance can be considered as the upper limit for an extractive model since it extracts the sentences that provide the best rouge l we can observe that while being similar for english and german and to some extent turkish the oracle performance is lower for french or spanish however as described in gure the percentage of novel words are similar for german french and spanish this may indicate that the relevant information to extract from the article is more spread among sentences for spanish and french than for german this is conrmed with the results of german and english have a much higher rouge l and than french or spanish and the case of textrank the textrank mance varies widely across the different languages fr de es ru tr cnn dm en cnn dm en full preprocessing duc en newsroom en t p b p table ratios of rouge l t p is the ratio of trank to pointer generator and b p is the ratio of bert to pointer generator the results for cnn full preprocessing duc and newsroom datasets are those reported in table of grusky et al pointer c in their paper is our pointer generator regardless oracle it is particularly surprising to see the low performance on german whereas for this language has a comparatively higher performance on the other hand the performance on english is remarkably high the rouge l is higher than for turkish higher than for french and higher than for spanish we pect that the textrank parameters might actually overt english in table we report the performance ratio tween textrank and pointer generator on our pus as well as on cnn dm and two other english corpora duc and newsroom textrank has a performance close to the pointer generator on glish corpora ratio between to but not in other languages ratio between to that are novelfrpointer generatorbert that are noveldepointer generatorbert that are novelespointer generatorbert that are novelrupointer generatorbert that are noveltrpointer generatorbert that are novelenpointer generatorbert mhuman we thus hypothesise that self attention plays an portant role for german but has a limited impact for french this could nd an explanation in the morphology of the two languages in statistical parsing tsarfaty et al considered german to be very sensitive to word order due to its rich morphology as opposed to french among other reasons the exibility of its syntactic ordering is mentioned this corroborates the hypothesis that self attention might help preserving information for languages with higher degrees of word order freedom possible derivative usages of mlsum multilingual question answering originally cnn dm was a question answering dataset mann et al the hypothesis is that the information in the summary is also contained in the pair article hence questions can be ated from the summary sentences by masking the named entities contained therein the masked entities represent the answers and thus a masked question should be answerable given the source article so far no multilingual training dataset has been proposed for question answering this methodology could be thus applied on sum as a rst step toward a large scale gual question answering corpus incidentally this would also allow progressing towards multilingual question generation a crucial component to ploy the neural summarization metrics mentioned in section news title generation while the release of mlsum hereby described covers only summary pairs the archived news articles also clude the corresponding titles the accompanying code for parsing the articles allows to easily trieve the titles and thus use them for news title generation topic detection a topic category can be ciated with each article summary pair by simply parsing the corresponding url a natural tion of this data for summarization would be for template based summarization perez beltrachini et al using it as additional features ever it can also be a useful multilingual resource for topic detection figure improvement rates from textrank to oracle in abscissa against rates from pointer generator to bert in ordinate this suggests that this model despite its generic and unsupervised nature might be highly biased towards english the benets of pretraining we hypothesize that the closer an unsupervised model performance to its maximum limit the less improvement would come from pretraining in figure we plot the improvement rate from textrank to oracle against that of pointer generator to m bert looking at the correlation emerging from the plot the hypothesis appears to hold true for all guages including russian not plotted for scaling reasons y with the exception of english this exception is probably due to the aforementioned bias of textrank towards the glish language pointer generator and m bert finally we observe in our results that m bert always forms the pointer generator however the ratio is not homogeneous across the different languages as reported in table in particular the improvement for german is much more important than the one for french interestingly this observation is in line with the results reported for machine translation the transformer vaswani et al outperforms signicantly gehring et al for english to german but obtains comparable results for english to french see table in vaswani et al neither model is pretrained nor based on lstm hochreiter and schmidhuber and they both use bpe tokenization shibata et al fore the main difference is represented by the attention mechanism introduced in the transformer while used only source to target attention textrank pointer generator bert conclusion we presented mlsum the rst large scale lingual summarization dataset comprising over m article summary pairs in french german russian spanish and turkish we detailed its construction and its complementary nature to the cnn dm summarization dataset for english we reported extensive preliminary experiments lighting biases observed in existing summarization models as well as analyzing and investigating the relative performances across languages of state the art approaches in future work we plan to add other languages including arabic and hindi and to investigate the adaptation of neural metrics to multilingual summarization references dzmitry bahdanau kyunghyun cho and yoshua gio neural machine translation by jointly arxiv preprint learning to align and translate satanjeev banerjee and alon lavie meteor an automatic metric for mt evaluation with improved correlation with human judgments in proceedings of the acl workshop on intrinsic and extrinsic ation measures for machine translation marization pages federico barrios federico lopez luis argerich and rosa wachenchauzer variations of the larity function of textrank for automated tion arxiv preprint federico barrios federico lopez luis argerich and rosa wachenchauzer variations of the larity function of textrank for automated tion corr florian bohm yang gao christian m meyer ori shapira ido dagan and iryna gurevych ter rewards yield better summaries learning to in proceedings of the marise without references conference on empirical methods in ral language processing and the international joint conference on natural language processing emnlp ijcnlp pages hong kong china association for computational linguistics samuel r bowman gabor angeli christopher potts and christopher d manning a large tated corpus for learning natural language inference arxiv preprint zewen chi li dong furu wei wenhui wang ling mao and heyan huang cross lingual natural language generation via pre training arxiv preprint eunsol choi he he mohit iyyer mark yatskar tau yih yejin choi percy liang and luke moyer quac question answering in context arxiv preprint sumit chopra michael auli and alexander m rush abstractive sentence summarization with tentive recurrent neural networks in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies pages alexis conneau ruty rinott guillaume lample ina williams samuel r bowman holger schwenk and veselin stoyanov xnli evaluating lingual sentence representations in proceedings of the conference on empirical methods in ral language processing association for tional linguistics hoa trang dang duc evaluation of in question focused summarization systems ceedings of the workshop on task focused rization and question answering pages sociation for computational linguistics jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language standing in proceedings of the conference of the north american chapter of the association for computational linguistics human language nologies volume long and short papers pages li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao ming zhou and hsiao wuen hon unied language model pre training for natural language arxiv preprint understanding and generation bonnie dorr david zajic and richard schwartz hedge trimmer a parse and trim approach to headline generation in proceedings of the naacl on text summarization workshop volume pages association for computational guistics xiangyu duan mingming yin min zhang boxing chen and weihua luo zero shot lingual abstractive sentence summarization through teaching generation and attention in proceedings of the annual meeting of the association for putational linguistics pages greg durrett taylor berg kirkpatrick and dan klein learning based single document tion with compression and anaphoricity constraints in proceedings of the annual meeting of the association for computational linguistics volume long papers pages m el haj u kruschwitz and c fox using chanical turk to create a corpus of arabic summaries mahmoud el haj and rim koulali kalimat a multipurpose arabic corpus in second workshop on arabic corpus linguistics pages matan eyal tal baumel and michael elhadad question answering as an automatic evaluation in ric for news article summarization ings of the conference of the north american chapter of the association for computational guistics human language technologies volume long and short papers pages jonas gehring michael auli david grangier denis yarats and yann n dauphin convolutional in proceedings sequence to sequence learning of the international conference on machine learning volume pages jmlr org george giannakopoulos jeff kubina john conroy josef steinberger benoit favre mijail kabadjov udo kruschwitz and massimo poesio ling multilingual summarization of single and multi documents on line fora and call center versations in proceedings of the annual ing of the special interest group on discourse and dialogue pages max grusky mor naaman and yoav artzi newsroom a dataset of million summaries with diverse extractive strategies in proceedings of the conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages francisco guzman peng jen chen myle ott juan pino guillaume lample philipp koehn vishrav chaudhary and marcaurelio ranzato the ores evaluation datasets for low resource machine translation nepali english and sinhala english in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural guage processing emnlp ijcnlp pages donna harman and paul over the effects of man variation in duc summarization evaluation in text summarization branches out pages karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend in advances in neural mation processing systems pages karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend in advances in neural mation processing systems nips kai hong and ani nenkova improving the estimation of word importance for news in proceedings of the document summarization conference of the european chapter of the sociation for computational linguistics pages baotian hu qingcai chen and fangze zhu sts a large scale chinese short text summarization dataset in proceedings of the conference on empirical methods in natural language processing pages wojciech kryscinski bryan mccann caiming xiong and richard socher evaluating the factual consistency of abstractive text summarization arxiv preprint wojciech kryscinski romain paulus caiming xiong and richard socher improving abstraction in text summarization in proceedings of the ference on empirical methods in natural language processing pages guillaume lample and alexis conneau lingual language model pretraining arxiv preprint patrick lewis barlas oguz ruty rinott sebastian riedel and holger schwenk mlqa uating cross lingual extractive question answering arxiv preprint yaobo liang nan duan yeyun gong ning wu fei guo weizhen qi ming gong linjun shou daxin jiang guihong cao al xglue a new benchmark dataset for cross lingual arxiv training understanding and generation preprint chin yew lin rouge a package for automatic in text summarization evaluation of summaries branches out pages marina litvak mark last and menahem friedman a new approach to improving multilingual in summarization using a genetic algorithm ceedings of the annual meeting of the ation for computational linguistics pages association for computational linguistics yang liu and mirella lapata text in proceedings of tion with pretrained encoders the conference on empirical methods in ural language processing and the international joint conference on natural language processing emnlp ijcnlp pages rada mihalcea and paul tarau textrank ing order into text in proceedings of the ference on empirical methods in natural language processing pages sepp hochreiter and jurgen schmidhuber neural computation long short term memory ramesh nallapati bowen zhou cicero dos santos caglar gulcehre and bing xiang tive text summarization using sequence to sequence in proceedings of the rnns and beyond signll conference on computational natural guage learning pages courtney napoles matthew gormley and benjamin in van durme annotated gigaword ceedings of the joint workshop on automatic edge base construction and web scale knowledge extraction pages association for tional linguistics shashi narayan shay b cohen and mirella lapata do nt give me the details just the summary topic aware convolutional neural networks for in proceedings of the treme summarization conference on empirical methods in natural guage processing brussels belgium shashi narayan shay b cohen and mirella lapata ranking sentences for extractive tion with reinforcement learning in proceedings of the conference of the north american ter of the association for computational linguistics human language technologies volume long pers pages new orleans louisiana association for computational linguistics jessica ouyang boya song and kathleen mckeown a robust abstractive system for cross lingual summarization in proceedings of the ence of the north american chapter of the ation for computational linguistics human guage technologies volume long and short pers pages romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization arxiv preprint laura perez beltrachini yang liu and mirella ata generating summaries with topic plates and structured convolutional decoders arxiv preprint matthew e peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer deep contextualized word resentations in proceedings of naacl hlt pages telmo pires eva schlinger and dan garrette arxiv how multilingual is multilingual bert preprint elvys linhares pontes juan manuel torres moreno stephane huet and andrea carneiro linhares a new annotated portuguese spanish corpus for the multi sentence compression task in proceedings of the eleventh international conference on language resources and evaluation lrec dragomir radev simone teufel horacio saggion wai lam john blitzer arda celebi hong qi liott drabek and danyu liu evaluation of text summarization in a cross lingual information trieval framework center for language and speech processing johns hopkins university baltimore md tech rep alec radford karthik narasimhan tim salimans and ilya sutskever improving language understanding by generative pre training url com us assets researchcovers languageunsupervised language understanding paper pdf amazonaws pranav rajpurkar jian zhang konstantin lopyrev and squad questions percy liang for machine comprehension of text arxiv preprint peter a rankel john m conroy hoa trang dang and ani nenkova a decade of automatic tent evaluation of news summaries reassessing the state of the art in proceedings of the annual meeting of the association for computational guistics volume short papers pages soa bulgaria association for computational guistics alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of the tence summarization conference on empirical methods in natural guage processing pages evan sandhaus the new york times annotated corpus linguistic data consortium philadelphia thomas scialom sylvain lamprier benjamin wowarski and jacopo staiano answers unite unsupervised metrics for reinforced in proceedings of the rization models ference on empirical methods in natural language processing and the international joint ence on natural language processing ijcnlp pages hong kong china sociation for computational linguistics abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proceedings of the nual meeting of the association for computational linguistics volume long papers pages eva sharma chen li and lu wang bigpatent a large scale dataset for abstractive and coherent summarization arxiv preprint yusuxke shibata takuya kida shuichi fukamachi masayuki takeda ayumi shinohara takeshi hara and setsuo arikawa byte pair encoding a text compression scheme that accelerates pattern matching technical report technical report department of informatics kyushu sity i sutskever o vinyals and qv le sequence to sequence learning with neural networks advances in nips adam trischler tong wang xingdi yuan justin ris alessandro sordoni philip bachman and heer suleman newsqa a machine hension dataset arxiv preprint reut tsarfaty seddah yoav goldberg sandra kubler marie candito jennifer foster yannick sley ines rehbein and lamia tounsi tistical parsing of morphologically rich languages spmrl what how and whither in proceedings of the naacl hlt first workshop on statistical parsing of morphologically rich languages pages association for computational linguistics ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all in advances in neural information you need cessing systems pages oriol vinyals meire fortunato and navdeep jaitly pointer networks in advances in neural formation processing systems pages a samples french summary terre dorigine du clan karza ville mridionale de kandahar un bastion historique talibans mollah omar a vcu et conserv profondes racines cest sur cette terre pachtoune plus qu kaboul que lavenir long terme du pays pourrait dcider body lorsque lon parle lafghanistan yeux monde sont rivs sur capitale kaboul cest l que concentrent pouvoir et dtermine principe son avenir cest aussi que sont runis les commandements forces civiles militaires internationales envoyes sur sol afghan pour lutter contre linsurrection et aider pays reconstruire mais y regarder plus prs kaboul nest quune faade face un etat inexistant une structure pouvoir afghan encore clanique tribus restes puissantes face une dmocratie articielle importe vraie lgitimit ne vient pas kaboul la gographie pouvoir afghan aujourdhui oblige dire quune bonne partie cls la population afghane trouve au sud pachtoune dans une cit hostile aux trangers foyer talibans kandahar kandahar terre dorigine du clan karza tribu popalza hamid karza prsident afghan tient son pouvoir poids de son clan dans la rgion mi novembre dans la grande son frre wali kandahar pressaient chefs de venus de tout lafghanistan piliers de son rseau lobjet rencontre bilan post lectoral aprs la rlection conteste son frre la pays parfois dcri pour ses liens supposs avec cia traquants wali karza joue un rle politique mconnu il a organis campagne son frre jour l kandahar jouait sous sa houlette qui avaient soutenu ou au contraire refus leur soutien hamid chef dorchestre charg du clan prsident wali personnalit forte du pays les karza adossent leur inuence celle de kandahar dans lhistoire lorsque ahmad shah fondateur du pays en conquit en capitale jusquen lors linvasion sovitique kandahar a incarn afghan les kandaharis considrent quils ont un droit divin diriger pays rsume mariam abou zahab experte du monde pachtoune kandahar cest lafghanistan explique ceux qui linterrogent tooryala wesa la province la politique sy fait et encore aujourdhui la politique sera dicte par les vnements qui sy drouleront cette emprise de kandahar svalue aux places prises au sein du gouvernement par ceux sud la composition du nouveau gouvernement dcembre na pas chang la donne dautant moins que karza dans le sud ou ailleurs nont pas russi renforcer au cours du dernier mandat du prsident lautre terre pachtoune grand paktia dans est pays frontire avec pakistan qui a fourni tant rois ne dispose plus de ses relais dans la capitale kandahar pse aussi pays car sy trouve linsurrection qui menace place lotan de depuis huit ans na cess du terrain dans le sud insurgs contrlent des zones entires les provinces du helmand et de kandahar sont les zones les plus meurtrires pour la coalition et lotan semble dpourvue cohrente kandahar natale talibans ils sont ns dans campagnes du helmand et de kandahar mouvement taliban sest constitu dans la ville kandahar o vivait leur chef spirituel mollah omar et o il a conserv profondes racines la pression sur vie quotidienne afghans est croissante les talibans supplent mme gouvernement dans domaines tels que la justice quotidienne ceux qui collaborent avec trangers sont stigmatiss menacs voire tus en guise de premier avertissement les talibans collent nuit lettres sur portes collabos la progression talibane fait dans le sud relate alex strick van linschoten unique spcialiste occidental la rgion mouvement taliban vivre kandahar sans protection linscurit labsence travail poussent vers kaboul ceux qui ont peu dducation comptence seuls restent pauvres et ceux qui veulent largent en raction cette dtrioration les amricains ont dcid sans lassumer ouvertement reprendre contrle situations cones ofciellement par lotan aux britanniques dans le helmand et aux canadiens dans la province de kandahar le mouvement a t progressif mais depuis un an etats unis nont cess denvoyer renforts amricains au point dexercer aujourdhui direction oprations dans cette rgion une tendance qui se renforcera encore avec troupes supplmentaires promises par barack obama lhistoire a montr que pour gagner en afghanistan il fallait tenir campagnes de kandahar les britanniques lont expriment faon cuisante lors la seconde guerre anglo afghane la xixe sicle sovitiques nen sont jamais venus bout on sait comment cela sest termin pour eux on essayer dviter les mmes erreurs observait mi novembre optimiste un ofcier suprieur amricain german summary die wurzeln des elends liegen in der vergangenheit haiti bezahlt immer noch fr seine befreiung vor jahren auch damals nahmen die wichtigen der welt den insel staat nicht ernst body das portrait von zeigt haitis nationalhelden franois dominique toussaint louverture er war einer der anfhrer der revolution in haiti und autor der ersten verfassung die wurzeln des elends liegen in der vergangenheit haiti bezahlt immer noch fr seine befreiung vor jahren auch damals nahmen die wichtigen der welt den insel staat nicht ernst am vergangenen wochenende schickte der britische architekt und grnder der organisation architecture for humanity eine atemlose verzweifelte e mail an seine freunde und untersttzer nicht erdbeben sondern gebude tten menschen schrieb er in die betreffzeile damit brachte er auf den punkt was auch der geologe und autor simon winchester oder der urbanist mike davis immer wieder geschrieben haben es gibt keine naturkatastrophen es gibt nur gewaltige naturereignisse die folgen haben die konsequenz aus dieser schlussfolgerung ist die schuldfrage einfach lsst sie sich beantworten gier und korruption sind fast immer die auslser einer katastrophe in haiti aber liegen die wurzeln der tragdie tief in der geschichte des landes diese begann nach europischer rechnung i m jahre als christopher kolumbus auf der insel landete die ihre ureinwohner ayt nannten kolumbus benannte die insel in hispaniola um und grndete mit den trmmern der gestrandeten santa maria die erste spanische kolonie in der neuen welt ende jahrhunderts besetzten franzsische siedler den westen der insel den frankreich zur franzsischen kolonie sainte domingue erklrte ideale der franzsischen revolution gut hundert jahre whrte die herrschaft der beiden kolonialherren ber die geteilte insel saint domingue war die reichste europische kolonie in den amerikas schrieb der historiker hans schmidt kam fast die hlfte weltweit produzierten zuckers aus der franzsischen kolonie die auch in der produktion von kaffee baumwolle und indigo weltmarktfhrer war sklaven arbeiteten auf den plantagen und sie erfuhren bald vom neuen geist ihrer herren die franzsische revolution brachte die ideale von freiheit gleichheit und brderlichkeit in die karibik i m august war so weit der voodoo priester dutty boukman rief whrend einer messe zum aufstand einer der erfolgreichsten kommandeure der rebellion war der ehemalige sklave franois dominique toussaint louverture nach dem heute der flughafen von port au prince benannt ist gab toussaint dem land seine erste verfassung die gleichzeitig eine unabhngigkeitserklrung war fr napoleon sollte haiti eine schmach bleiben daraufhin sandte napoleon bonaparte kriegsschiffe und soldaten toussaint wurde verhaftet und nach frankreich gebracht wo er i m kerker starb doch als napoleon i m jahr darauf die sklaverei wieder einfhren wollte kam erneut zum aufstand verzweifelt baten die franzsischen truppen i m sommer um verstrkung da aber hatte napoleon schon das interesse an der neuen welt verloren i m april hatte er seine kolonie louisiana an die nordamerikaner verkauft ein gebiet das rund ein viertel des staatsgebietes der heutigen usa umfasste fr napoleon sollte haiti eine schmach bleiben am januar erklrte der rebellenfhrer jean jacques dessalines die ehemalige kolonie heie nun haiti und sei eine freie republik der erste und bis zur abschaffung der sklaverei einzige erfolgreiche sklavenaufstand der neuen welt war ein schock fr die gromchte der kolonialra die ihren reichtum auf der sklaverei gegrndet hatten ein handel der die geschichte haitis bis heute bestimmt die freiheit hatte ihren preis ein groteil der plantagen war zerstrt ein drittel der bevlkerung haitis den kmpfen zum opfer gefallen vor allem aber wollte keine kolonialmacht die junge republik anerkennen i m gegenteil meisten lnder untersttzten das embargo der insel und die forderungen franzsischer sklavenherren nach reparationszahlungen in der hoffnung als freie nation zugang zu den weltmrkten zu erhalten lie sich die neue machtelite haitis auf einen handel ein der die geschichte der insel bis heute bestimmt mehr als zwei jahrzehnte nach dem sieg der rebellen entsandte knig karl x seine kriegsschiffe nach haiti ein emissr stellte die regierung vor die wahl haiti sollte fr die anerkennung als staat millionen francs bezahlen sonst wrde man einmarschieren und die bevlkerung erneut versklaven haiti nahm schulden auf und bezahlte bis zum jahre lhmte die schuldenlast die haitianische wirtschaft und legte den grundstein fr armut und korruption lie der damalige haitianische prsident jean bertrand aristide errechnen was diese reparationszahlungen fr haiti bedeuteten rund milliarden amerikanische dollar rckzahlung forderten seine anwlte damals von der franzsischen regierung vergebens lesen sie auf der nchsten seite wie haiti von den akteuren der weltbhne geschnitten wurde spanish summary el aeropuerto ha estado hasta las con slo dos pistas por ausencia controladores areos varias aerolneas han denunciado hasta minutos los pasajeros embarcados body el espacio har un repaso cronolgico vida esteban desde momento en el que una completa desconocida comenz a aparecer medios en como jesuln de ubrique hasta llegar a hoy en convertida la princesa del pueblo en concreto del popular madrileo distrito de san blas donde vive tal y como algunos han calicado y protagonista revistas diarios y portales web aparecer incluso entre los personajes ms populares google junto a mara teresa campos estarn en el plat patricia prez presentadora del programa matinal sbados en telecinco vulveme loca quien ha conducido las campanadas en cuatro ocasiones y los comentaristas maribel escalona emilio pineda y jos manuel parada los vuelos han venido registrando este viernes importantes retrasos en barajas a de que desde las el aeropuerto opera con las cuatro pistas segn han informado fuentes de aena mientras las compaas han denunciado por parte controladores hasta minutos los pasajeros embarcados segn los datos facilitados por aena ausencia de controladores que estaban programados en el turno torre control de barajas oblig a cerrar dos pistas del aeropuerto lo que gener retrasos medios de minutos turkish summary atamas yaplmayan retmenler miting yapt retmen adaylarna muharrem nce ve tekel iileri destek verdi body yetersiz alan kadrolar nedeniyle atamas yaplamayan retmen adaylar ankarada miting yapt tekel iilerinin de destek verdii retmen adaylarnn mitinginde retmen kkenli chp milletvekili muharrem nce de hazr bulundu trkiyenin eitli illerinden gelen atamas yaplmayan retmenler platformu yesi szlemeli retmenler saatlerinde abdi peki parknda topland milletvekillii iin kpss getirilsin kadrolu retmen cretli retmen ve cretli kle olmayacaz yazl dvizler tayan ve ayn ierikli sloganlar atan retmenlerin dzenledii mitinge baz siyasi parti sivil toplum kuruluu temsilcileri ve tekel destek verdi chp yalova milletvekili muharrem nce okullarda derslerin bo getiini ne srerek okullar retmensiz retmenler ise isiz dedi hkmetin bu genlerin sesini gerektiini belirten nce bu lkenin bin eitim fakltesi mezunu genci i bekliyorsa bu hkmetin ve lkenin aybdr eitim sorununu zememi bir hkmet bu lkenin hibir sorununu zememi demektir bu kadar nemli bir soruna kulaklarn tkayamaz diye konutu ankarann gbeinde derslerin bo getiini ileri sren nce bu lkede zik ve matematik retmeni atanmyor ama bunlarn kat din dersi retmeni atanyor dedi platform adna yaplan aklamada trkiyede her yl niversite bitirerek diplomasn alan retmenlerin eitim alanndaki yetersizlik dolaysyla isizler kervanna katld ifade edildi talep edilen haklarn insancl ve makul olduu belirtilen aklamada retmenlerin haklarn vermeyenlerin niyetli olduu ne srld aklamada hkmetin eitim politikas eletirilerek szlemeli retmenlerin kadrolu atamalarnn yaplmas retmen yetitiren fakltelere retmen ihtiyac kadar retmen aday alnmas ve kpss yerine daha effaf bir atama sistemi getirilmesi istendi lm orucu balatacaklar eitli sivil toplum kuruluu mitingde kadrolu atamalar yaplmad takdirde i brakma eylemi ve lm orucu yaplaca duyuruldu
