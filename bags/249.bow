n j g l s c v v x r journal ieee transactions data engineering vol xx xx xxxx attentive representation learning adversarial training short text clustering wei zhang member ieee chao dong jianhua yin jianyong wang fellow ieee abstract short text clustering far reaching effects semantic analysis showing importance multiple applications summarization information retrieval inevitably encounters severe sparsity short text representations making previous clustering approaches far satisfactory paper present novel attentive representation learning model shot text clustering cluster level attention proposed capture correlations text representations cluster representations relying representation learning clustering short texts seamlessly integrated unied model ensure robust model training short texts apply adversarial training unsupervised clustering setting injecting perturbations cluster representations model parameters perturbations optimized alternately minimax game extensive experiments real world short text datasets demonstrate superiority proposed model strong competitors verifying robust adversarial training yields substantial performance gains index terms short text clustering representation learning attention mechanisms robust adversarial training introduction r ecent years witnessed fast growing trade short text data kinds social media example twitter instagram sina weibo consequence short text clustering task automatically grouping multiple unlabeled texts number clusters increasingly important benet multiple content centric downstream applications event exploration trend detection online user clustering cluster based retrieval compared general text clustering short text clustering challenging text representations original lexical space usually sparse issue amplied short texts key success short text clustering learn effective short text representation scheme suitable clustering basis classical general clustering algorithms k means current developments short text clustering fall branches bayesian topic models e latent let allocation lda deep learning approaches realizes probabilistic text clustering assuming document associated distribution topics topic distribution words way topic usually regarded cluster model short text rate topic models presented changing text generation process major limitation remains topic models input representation short text commonly based bag words assumption hot w zhang corresponding author c dong school computer science technology east china normal sity shanghai china e mail zhangwei com ecnu edu cn j yin department computer science technology shandong university qingdao shandong china e mail edu cn j wang department computer science technology tsinghua university beijing china jiangsu collaborative innovation center language ability jiangsu normal university xuzhou china e mail edu cn encoding sparse lack expressive ability aiming leverage power representation learning short text clustering studies utilize word dings deep convolutional neural networks build multi stage framework better text representations rstly learned fed conventional k means algorithm improving clustering performance optimization process partitioned separate stages incapable guiding text representation learning clustering objective deep representation learning approaches general clustering problems trained end end fashion tailored textual data require additional step obtain short text tions specically hand crafted text representations term frequency inverse document frequency tf idf commonly taken model input overlook usage word level distributional representations important grasp text semantic relatedness paper concentrate bridging gap short text representation learning short text clustering fusing unied model inspired roaring success tion mechanisms natural language processing nlp devise novel attentive representation learning arl model tailored short text clustering task rst leverages dimensional word embeddings build dense representations simple effective mean pooling technique short texts cluster level attention developed capture correlations short text representations cluster representations shares similar spirit gsdmm tailored short text modeling assumes words short text belong cluster topic derived attention weights evidence determine cluster assignments enable unsupervised learning word cluster tions propose reconstruct short text representations weighted combination cluster representations objective function combining pairwise ranking loss point wise loss employed measure reconstruction gap journal ieee transactions data engineering vol xx xx xxxx improve effectiveness learning short text clustering incorporate robust adversarial training nal model naming arl adv robust adversarial ing requires feed original real examples intentional adversarial examples model training cess obtained impressive performance supervised semi supervised classication tasks concretely arl adv associates continuous cluster representations adversarial turbations form additional adversarial objective function supplement original objective function adversarial perturbations play role adaptive regularization providing optimization process robustness benecial short texts contain noise model parameters perturbations optimized minimax game model parameters minimizing aforementioned reconstruction gap contrast perturbations maximizing reconstruction gap sum contributions follows present novel attentive representation learning approach arl couple short text representation learning tering unied model proposing reconstruct text representations cluster level attention model optimized unsupervised manner cluster assignments learned enhance robustness learning cluster label ments short texts introduce robust adversarial training customizing adversarial examples arl adv best knowledge rst study apply robust adversarial training eld unsupervised clustering setting conduct extensive experiments real world short text datasets publicly available remaining constructed following recent work comparing arl adv strong baselines demonstrate signicant improvements verify adversarial training boosts performance ensure reproducibility paper event related dataset released evaluation section related work section briey reviews recent achievements text clustering methods especially short texts generative adversarial text representation schemes networks clustering attention mechanisms adversarial training discussed text clustering methods general text clustering extensively studied literature short text clustering receives attention recent years ourish user generated content online social media currently main categories approaches literature rst category topic modeling approaches ing ideas lda uncover latent topic distributions implicitly modeling word co occurrence patterns unlike lda assumes generate topic word document gsdmm supposes words short text share topic naturally regarded cluster text advanced model btm explicitly models co occurrence patterns bigrams dened pairs unordered words given document cluster determination btm depends topic proportions bigrams target short text recently topic models developed clustering streaming short texts incrementally paper primarily concentrate general clustering setting leave adapt model streaming short texts future inspired success deep learning category promotes development clustering eld relevant text clustering study stcc designed short trained end end fashion leaving room performance improvement gaussian lda incorporates word embeddings generation process topic models compatible short texts lda recently deep learning approaches proposed general clustering tasks crafted texts dec utilizes idea self training measure gap distribution soft cluster assignments corresponding auxiliary distribution kullback leibler kl divergence stc adopts training smoothed inverse frequency sif introduced weight word embeddings document stc obtains text representations separate step optimize word embeddings learning text clustering vade extends variational auto encoder integrate gaussian mixture model generate latent vectors instead gaussian distribution models address importance representation learning short texts making clustering performance satised situation generative adversarial networks clustering beneting generative adversarial networks gans aim generate data matches target distribution approaches utilize gans clustering tasks specically premachandran et al employed gans extract features later fed k clustering algorithm learned end end fashion clustering dac leverages discriminator push forward tion data representations generated trainable encoder gaussian mixture distribution ganmm revisits expectation maximization based clustering algorithm replaces expectation step maximization step classication models gans mrabah et al extended dec adding regularization term gans constrain reconstructed data points similar realistic data points clustergan consists generator discriminator encoder learn mappings discrete class labels continuous image pixels innovation lies design discrete latent space z providing sufcient signals generator samples belonging specic class beneting clustering study shares similar spirit clustergan devising special latent space ease clustering self paced learning adopted distinguish easy hard examples achieving reliable training process summary gan based methods welcomed clustering images ability handle continuous values nontrivial applying gan based methods clustering discrete textual data difculty optimization surrogate way represent text xed continuous vectors e tf idf pre trained word embeddings gan based models inevitably limit power learning text representations journal ieee transactions data engineering vol xx xx xxxx text representation schemes traditionally single word represented hot vector xed dimensional space space size specied vocabulary accompanied vector space models tf idf commonly utilized denote short text representations alleviate issue sparse exact word overlap similarity computation low dimensional techniques principal component analysis pca probabilistic topic models e lda applied text domain particular word embedding techniques gained lots attention recent years improved ability grasp semantic relatedness consequently progress served respect including contextualized word embeddings like bert beneting advantage word embeddings pre training ne tuning paradigm state art text classication nlp tasks embedding based text clustering approaches ne tune ne grained word level representations guidance clustering tives leaving room improving text representations motivates proposed models arl arl adv mitigate issue jointly learning word embeddings text clustering degraded performance ne tuning word embeddings table attention mechanisms attention mechanisms growing popularity learning representations image text different modalities given context query typically culates prominence representation key sub layers form integrated representation value literature text modeling attention mechanisms allow dependency modeling regardless distance words compared recurrent neural networks recent study develops header self attention mechanism representing input output sequences approach entirely composed attention blocks outperforms recurrence convolution based models machine translation tasks based self attention mechanism large scale pre trained language models e bert proposed beneted nlp tasks cic text classication task ma et al introduced mutual tention mechanism assign importance weights semantic related long distance words local semantic features aspect works consider incorporating user factors attention computation process words sentences work adopts word level attention extract related words de emphasize irrelevant words different studies perform cluster level attention associate representation learning automatic determination cluster assignments unied model achieve state art short text clustering performance robust adversarial training pioneering work proposes investigate benet supervised learning intentional adversarial examples age classication robust adversarial training continuous progress different elds example miyato et al applied virtual adversarial training variant adversarial training semi supervised learning text classication task et al learned pairwise ranking based matrix factorization model achieving better recommendation performance adversarial manner nutshell ties robust adversarial training proactively inject small intentional perturbations original data train models behave enhancing robustness model training best knowledge rst study leverage robust adversarial training literature unsupervised clustering worth noting robust adversarial training able difference gans following reasons unlike gans aiming generating new data match specied target distribution robust adversarial training targets ensuring robust model training process secondly need robust adversarial training tailored discriminator like gans target distinguish real synthetic data studies couple gans attention mechanisms share technological thought let proposed clustering tasks wei et al adopted adversarial attacks clustering approach largely different proposed arl adv relies labeled data clustering procedure grid based clustering adversarial attack separated different stages adversarial involve model parameter learning computational model model overview figure depicts overall architecture proposed model arl adv given short text examples perspective model contains ve computational modules short text representation cluster level attention short text reconstruction objective function robust adversarial training taking given short text e text illustrative example rst fed short text representation module section words e tax text associated trainable word embeddings constitute text representation cluster level attention section conducted compute probability relevance weights text representation different cluster representations e trainable weights consequently adopted combine cluster representations reconstructing short text representation section build pairwise ranking based clustering objective function section short text j sampled given corpus pseudo negative text eventually adversarial training module section injects perturbations e c cluster representations constitute short text reconstruction ranking providing ability robust training arl adv trained cluster label short text determined sorting corresponding probability weights returning cluster index e maximal weight e pi k short text representation text clustering problem corpus d short containing multiple short texts e size d taking short text di d illustration associated matrix based representation di li length text xi t column vector hot encoding position t size journal ieee transactions data engineering vol xx xx xxxx fig architecture proposed arl adv types marks different colors label different information ow dotted lines middle left gure denote cluster label short text derived model trained correspond normal objective function equation adversarial objective function equation respectively equal number words pre specied vocabulary v worth noting di original text representation strategies tf idf regarded hand crafted feature engineering texts convert hot sparse representation xi t t li low dimensional dense embedding rst adopt look encoding procedure exi wi e trainable word embedding matrix k embedding size given dense word embeddings simple mean pooling technique empirically adopted construct informative representation short text li wi t motivation operation short texts commonly insufcient contextual information contain noisy word information plex contextual text representation methods including pre trained contextual word representations e bert attention based document level representations recurrent sequential sentations convolution based representations exhibit signicant improvement worse mean pooling local tests attempted vlawe text representations suitable word embeddings hard ne tuned learning short text clustering separate procedure word embedding clustering consequently deem pooling suitable simultaneous learning short text representations clustering cluster level attention represent presumed m clusters specic d trainable cluster representation matrix c cm rkm established di contained useful global semantic information short text leveraging select suitable cluster feasible primary motivation level attention general attention function takes query set key value pairs input attention score characterizes compatibility corresponding key query later weighted combination values let key value correspond representations clusters regard short text representations queries intuitive perform selection based probability distribution cluster m m formally distribution dened follows zm mdi computational manner realization level attention captures text cluster interaction idea conforms successful experience probabilistic topic models short text modeling specically lda like models topic word long text separately determined sampled topics contrast gsdmm assumes words short text belong topic exhibits improved performance short text modeling arl adv consistent gsdmm latent topic zm short text shown equation denote pi zm short referring probability values pi zm different m sufcient identify information negative textinformation positive textinformation cluster embeddingsperturbation cluster embeddingscluster level attentioncluster embedding adversarial positive textlookup meanword embeddingnegative textlookup meanword text america afford highest corporate tax rate farmers market bonsall big nice surprises short text sample journal ieee transactions data engineering vol xx xx xxxx relevant cluster given di probability value employed rescale representations corresponding clusters yielding ci m ci m pi zm cm short text reconstruction short text clustering unsupervised learning problem leverage cluster representations reconstruct short text representation later utilized guide training model assuming reconstructed representation th text calculate based linear combination dependent cluster matrix relevance score original representation negative samples given margin supplement adopt relevance maximization consistent optimization direction dene following loss function e c regarded pointwise loss function aiming predict similar possible given combination cluster representations fusing ensures larger relevance pseudo negative text pairs large value taking short texts d consideration dene overall objective loss function follows ci m m beneting different contributions clusters rebuild expressive representation text specic text derivation cluster level attention equation encourages clusters similar text representation contribute linear combination equation derived reconstruction driven closer text representation cluster level attention gradually favor single cluster training goes visualization figure partly explain consequence experiments nd average probabilities inferred maximum cluster results mean text highly concentrated single cluster arbitrary value objective function given obtained short text representation structed representation cluster representations dene relevance score cosine similarity representations deem relevance score gets larger result reconstruction better achieve propose hybrid objective function consisting parts pairwise ranking relevance maximization rst model seeks minimize margin based pairwise ranking loss specically short text sample texts corpus regard pseudo negative text set ni text dj ni obtain text similarity short texts e dj based cosine similarity text representations culated equation embedding dj obtained procedure shown equation formulate pairwise ranking loss short text follows e c dj jni denotes number pseudo negative text ically setting margin positive negative ones empirically setting minimizing loss reconstructed larger c e e perspective arl adv contains word embedding matrix e cluster representation matrix c parameters short text directly encoded low dimensional space word embeddings extraordinary parameters required adversarial training clustering propose apply robust adversarial training facilitate representation learning process arl adv essence robust adversarial training provides new objective function based adversarial perturbations complement original optimization procedure objective requires model perform original samples adversarial samples beneting model training robustness especially short texts typical adversarial perturbations continuous values added real valued vectors like images directly applied discrete tokens like words following study add continuous adversarial perturbations cluster representations worth noting add versarial perturbations word embedding matrix reasons lie aspects e efciency effectiveness number words extends far clusters adding perturbations words instead clusters entails parameters perturbations optimized reduce model training efciency second empirically validate applying perturbations words effective adding perturbations cluster representations results shown table formally dene adversarial cluster level perturbations arl adv c rkm consequently propose new objective function formulated follows c c e c e c new cluster representations c c accompanied corresponding probability distribution zm m m leveraged constitute reconstructed short text di optimization representation objective function viewed aspects learning c optimal condition follows c arg max c c c journal ieee transactions data engineering vol xx xx xxxx contrary optimization target e c minimize typically norm based constraint adopted restrict scale c integrating objective functions e nal optimization target given c c c c c e c c arg min e c c c max c controls relative strength optimization equation involves playing minimax game step worst case perturbations cluster representations rst identied increasing value possible e c optimized robust intentional perturbations retaining satisfactory performance way text representation correlations normal reconstruction adversarial reconstruction simultaneously considered ensures robustness model training conforms short length noisy situation short texts algorithm summarizes optimization process usual stochastic gradient descent algorithms adam leveraged learn word clustering embeddings adv calculates gradients e c equation propagation updates parameters accordingly learning adversarial perturbations c follows mated linearizing methodology leading fast solutions analytic forms adding norm column c e m m easily derive following updating rule perturbation cluster representation c m gc m cm gc m nishing training alr adv cluster ments determined based probability weights learned cluster embedding matrix c adversarial perturbations c ignored experiments section conduct experiments answer following pivotal questions proposed arl adv outperform standard vanced text clustering models short text datasets main components proposed model including robust adversarial training benecial performance short text clustering good learned short text embeddings cluster representations compared baselines datasets utilize real world short text datasets brief introduction dataset common text preprocessing procedures provided following dataset text retrieval conference tweet tracks organized sponding queries evaluated relevance levels com mstream algorithm robust adversarial training arl adv input short text corpus d cluster number m adversarial strength adversarial norm hyper parameters embedding size k output word embeddings e cluster representations c cluster level attention weights m begin initialize word embeddings e cluster representations c iter max iter sample mini batch dbatch d dbatch constructing text pairs j j dbatch learning adversarial perturbations c equation updating word embeddings e cluster representations c gradient descent equation retain tweets labeled relevant highly relevant queries ensure quality labels dataset composed groups news titles snippets clawed google manual observation conrmed favorable grouping quality event following study extract event related tweets shelf tweet dataset crawled prior knowledge events including time window relevant entities keywords fetched wikipedia created based questions posted stack overow require selected questions associated tag tags regarded ground truth cluster labels questions created dataset substantially larger datasets hoping provide convincing empirical study simplicity use denote dataset preprocessing utilize typical procedures text cessing consists converting words lowercase moving stop words irregular words applying porter ming addition words frequencies discarded detailed statistics datasets found table cluster text refer actual number clusters texts dataset respectively vocabulary number remaining word tokens preprocessing avg len stands average length texts counted words methods comparisons baselines selected representative baselines categorized tional text clustering methods deep learning based general tering models tailored methods short text clustering iii details baselines given clarity google com org download stackexchange journal ieee transactions data engineering vol xx xx xxxx table detailed statistics dataset table default settings arl adv dataset trec googlenews event stackoverow cluster text vocabulary avg len k k means classical simple clustering approach relying hand crafted features text clustering ture types adopted experiments e tf idf low dimensional representations obtained pca note k means running different features k idf k respectively tried number primary components turns suitable choice report results based setting euclidean distance utilized internal similarity metric run random centroids initialized hieclu similar k means hierarchical clustering hieclu simple baseline clustering results taking textual features pca reported better mance tf idf lda lda classical standard generative cal model learns topic cluster distribution document assign text topic largest probability value inferred lda following set k number topics parameter settings tried observe signicant improvement performance btm btm regards bi grams representations short texts generates conditioned different topics similar lda tune set based performance number training iterations set btm reach convergence gsdmm gsdmm tailored short text clustering assumes text generated topic fundamentally different lda generates word topic set parameters train model iterations dec deep embedded clustering model leverages autoencoder tf idf features input map documents low dimensional embeddings ping function cluster representations rened based idea self training similar vade dec specied transforming parameter settings text data perform pre training iterations training iterations ensure convergence sgd optimizer pre training training phases momentum set batch size set empirical results setting dimension hidden layer dec help consistent robust performance stcc model mainly consists separate steps rst trains convolutional neural network help autoencoders trained model ployed text embeddings fed k means nal clustering set kernel width convolutional com xiaohuiyan btm com gsdmm com piiswrong dec com jacoxu layers k k max pooling number feature maps rst second convolutional layer respectively adam batch based optimization learning rate batch size vade vade extends canonical variational auto encoding approaches support clustering tasks utilizing idea gaussian mixture model partially follow parameter settings text data implementation vade procedure tf idf based feature transformation setting pre training iterations training iterations results convergence vade empirical basis phases use adam optimizer learning rate default conguration adam adopted e adjust size hidden representations batch size general stable outcomes clustergan clustergan recently proposed age gans encode continuous data discrete cluster labels considering requirement continuous data representations tried tf idf average word embedding denote short text reported better performance stc stc adopts self training inspired dec uses smoothed inverse frequency sif compute weighted average pre trained word embeddings stage independent optimizing clustering model word embeddings initialized arl adv size tuned dimension autoencoder set parameters kept original settings models tuned experimental datasets ensure statistical signicance results experiments averaged runs tried introduced section results gaussianlda obtain competitive clustering mance baselines training ganmm incurs heavy computational burden especially cluster number small omit details paper variants proposed model verify effectiveness main components proposed model consider variants introduced arl adv version proposed model adopts optimization target equation enables adversarial training arl employed train model ing arl model arl adv verify effectiveness robust adversarial training unsupervised clustering task arl train w method optimizes parameters word embedding matrix comparison com vade com clustergan com hadifar clustering com rajarshd gaussian lda com eyounx ganmm journal ieee transactions data engineering vol xx xx xxxx table performance comparison methods type ii iii dataset metrics lda k idf k hieclu clustergan dec vade stcc btm gsdmm stc arl arl adv nmi trec ari acc googlenews ari acc nmi nmi event ari acc stackoverow ari acc nmi necessity learning word level semantic representations arl train c cluster representation matrix learned model hoping partially verify benet combining representation learning clustering arl adv pairwise ranking loss removed arl adv short texts interact negative samples arl adv means arl adv consider supplementary pointwise loss arl adv arl adv perform adversarial training arl random variant takes random noise bations scale level intentional adversarial perturbations arl shares similar spirit adv adversarial perturbations added word embeddings instead cluster representations arl adv variants word embeddings trained dataset iterations initialize centroids clusters performing k means short text embeddings study loss generality default hyper parameters set ones shown table k dimension embeddings size batch stated results reported setting evaluation metrics e normalized mutual commonly metrics text clustering adopt performance evaluation information nmi adjusted rand index ari clustering curacy acc suppose denotes number short texts th true topic represents number short texts j th inferred cluster use ij denote number short texts simultaneously appearing clusters addition m denotes set possible mappings generated clusters real topics efcient search best mapping hungarian algorithm adopted formally nmi ari acc formulated follows nmi ij ij log j log log ari ij j j j acc max mm higher values evaluated nmi ari acc indicate better clustering quality equal perfect match achieved cluster assignments corpus nmi ari penalize unnecessary splits texts true cluster inferred clusters ari penalizes undesirable merging texts different true clusters inferred cluster making rigorous metric software conguration conduct experiments server cpus gpus baselines software ments congured according specic requirements extensively literature scikit learn adopted implementations k means hieclu lda visualization tool t sne metrics nmi ari acc models arl arl adv built based python tensorow cuda enable usage gpus release event described section publication paper source code prepared relevant studies model comparison report evaluation results table arl arl adv baselines cluster numbers set table perspective methods perform w t metrics example k idf gains better results lda w t nmi acc exhibits worse performance w t ari phenomenon shows necessity adopting metrics focus different properties clustering results provide comprehensive comparisons distinct perspectives expected models belonging conventional gory competitive performance types baselines cases tailored short texts fully leverage power representation learning com gensim models html com codeupload e arl adv journal ieee transactions data engineering vol xx xx xxxx table ablation study proposed model dataset metrics arl train w arl train c arl adv arl adv arl arl random arl arl adv nmi trec ari acc googlenews ari acc nmi nmi event ari acc stackoverow ari acc nmi specically lda works poorly assigning different clusters words short text aggravate sparsity corresponding results demonstrate pca converts sparse representation tf idf continuous low dimensional space k shows improvements k idf result indicates introducing representation independent model training process easily work classical simple model hieclu adopts mechanism e agglomerative clustering performance level k means dec vade deep learning based approaches ii learn low dimensional representations targets original representations pixel values images tf idf based representations texts proposed general clustering tasks limited texts shown table yield better results cases category conventional methods investigating results clustergan nd exhibit satised clustering performance attributed reason model incapable learning representations discrete text tokens gan based design baselines originally proposed short text tering iii stcc performs comparison shows separating representation learning clustering different stages tends sub optimal reason representation learning process lacks proper guidance feedback clustering learned compared stcc gsdmm btm perform better cases particular gsdmm outperforms general deep learning based clustering models cases attribute phenomenon proper assumption gsdmm short texts generated topic proper approximation posterior distribution stc exhibits good performance trec stackoverow worse adv good results datasets makes sense stc optimize word embeddings training clustering method sum model arl adv superior improvements baselines statistically signicant datasets veried t test attributed following reasons arl adv utilizes word embeddings obtain short text embeddings learns word embeddings optimization model learning separately arl adv combines representation learning short text clustering end end learning fashion robust adversarial training adopted arl adv effectively improve clustering performance table shows adv improves arl consistently datasets especially googlenews event stackoverow verifying reason section empirically demonstrate benet learning word embeddings cluster representations ablation study model analysis proposed model table shows results ablation study tigate role critical components proposed model rst comparing arl train w arl train c arl adv performance drops signicantly phenomenon shows learning word representations cluster representations accompanied automatic selection clusters indispensable arl train c behaves better arl train w indicating training word embeddings critical model second table shows contributions pairwise ranking loss pointwise loss trec losses play similar roles removing obviously damage performance contrary pairwise ranking loss crucial clustering performance datasets adding pointwise loss strengthen short text clustering cases improvement observed stackoverow actually add parameter equation control relative inuence pointwise loss specic dataset table explores effect versarial cluster level perturbations comparing level random perturbations arl random adversarial level perturbations arl based results observe arl random obtains slightly worse results arl reveals simply adding random perturbations learning bring useful information model arl performs marginally better arl cases arl adv especially googlenews event stackoverow example compared arl arl adv gains relative improvements evaluated acc datasets respectively qualitative studies explore representations generated alr adv baselines use t sne visualize short text dings figure color denotes cluster nd arl adv arl generate highly separable semantic clusters compared high dimensional sparse tf idf based features low dimensional embeddings vade dec sualization explains outstanding scores arl adv evaluated metrics dec shows better clustering visualization baselines central areas trec journal ieee transactions data engineering vol xx xx xxxx trec tf idf trec vade c trec dec trec arl e trec arl adv googlenews tf idf g googlenews vade googlenews dec googlenews arl j googlenews arl adv event tf idf l event vade m event dec event arl o event arl adv p tf idf q vade r dec arl t arl adv fig visualization short text representations dimensional space t sne quality tf idf features short text embeddings vade dec proposed models investigated table representative topical words trec table representative topical words event topic arl adv card consolid credit debt loan unsecur advic settlement counsel creditor bailout gifford gabriell buildup uid congresswoman rehabilit icu recoveri brain hospit doctor drone uav deliveri airspac southwest amazon chair requir commerci propos slice gsdmm debt credit consolid card loan settlement relief bad negoti post unsecur combin lower gifford gabriell rehab doctor recoveri brain news congresswoman uid buildup drone amazon propos airspac deliveri commerci zone sky uav high spe plan topic arl adv daallo plane somalia land somali beachfront shabaab au hablod mogadishu airplan plow plough bastilleday nice revel speed promenad driven franceattack truck lahoreblast pakistanbomb lahor peshawar prayforlahor peshawarblast guess gsdmm somalia plane land emerg explos forc daallo airlin injur passeng hole blast truck crowd attack prayfornic innoc heart shatter tie enjoy parad plow leader lahoreblast islamabad pakistan pm armi punjab govt oper blast lahor minist googlenews fuzzy clusters event stackoverow separated humans clusters provided arl adv arl easy identify exhibiting potential basis initialization downstream applications require insights data manifold exist non negligible differences visualizations arl adv arl showing robust adversarial training affects cluster representations example embeddings provided arl adv tend compact provided arl apparent event stackoverow evaluate cluster level quality comparing keywords obtained arl adv gsdmm presented table way topic detected judge held short texts belonging cluster align clusters different methods according detected topics stcc vade dec afford journal ieee transactions data engineering vol xx xx xxxx fig inuence model performance fig inuence model performance information directly model correlations words clusters gsdmm chooses words based learned word distributions different topics adv selects representative words larger cosine similarity values learned cluster representations words identiable meanings discarded listed words remaining chosen arl adv gsdmm share lots keywords mon showing detect similar topics reach consensus certain degree words better reect topics taking topic trec example words like brain luid recoveri imply case cured illness related entity gifford gabriell gsdmm identies doctor arl adv points hospit icu words ranked higher word lists gsdmm highly correlated topic event observe similar phenomenon overlapped keywords exist methods conrming capability providing overview cluster revealed topic event succeed identify dallo somalia words arl adv goes extracts highly correlated entities like beachf ront shabaab evidences consolidate viewpoint arl adv capture highly correlated keywords tend neglected conventional topic models like welcomed better semantic gsdmm model quality cluster level hyper parameter analysis effect described equation adjusts strength constrains contribution adversarial perturbations nal loss function figure shows clustering performance arl adv alters selection helps understand role loss note set default value shown table gures wide range provide signicant gain cases datasets metrics selecting achieves better results values trec googlenews stackoverow reaching balance loss function c adversarial version c c preferable choice arl adv x experiments summary arl adv relies completely robust adversarial learning showing appropriately effectively takes advantage perturbations effect figure presents performance arl adv varying dened equation constrains norm adversarial perturbations cluster representations larger larger adversarial perturbations training process setting discussed select values range affects arl arl arl arl arl arl arl arl arl arl arl arl arl arl arl arl arl arl arl arl arl arl arl arl arl journal ieee transactions data engineering vol xx xx xxxx fig inuence cluster number ratio intended cluster number original topic number fig performance iterations effect iterations table training time costs time unit data trec clustergan dec vade stc arl arl adv situations large margin attributed tend assignment strategies cluster counts sufcient arl adv effective adaptive robust determine proper mapping texts facing options performance proposed arl adv tions figure curves averaged runs choose points interval clarity effect adversarial perturbations arl adv experiences relatively moderate growth performance iterations arl adv starts attain stable state metrics sharing good convergence property time complexity analysis section compare training time efciencies adopted representation learning approaches single gpu probabilistic topic models like gsdmm included gibbs sampling based optimization implementations leverage computational power gpu report efciencies k means algorithms demand multiple processors speed solution assigning short text closest center fair comparisons set batch size uniformity testing time costs methods compared table shows time costs needed training iteration average following key tions computational efciencies proposed models comparable existing approaches second adopting robust adversarial training additionally incur heavy burden approximate linear increase computational complexity arl arl adv observed time cost data size comparisons trec stackoverow arl arl adv require training iterations usually shown figure convergence compared approaches computational experiments conclude complexities good level results meet expectation clustering performance infer gures positive impact achieved locally falls range cases noticeably increasing based chosen range shows growing tendency event apply datasets alter larger values reect universal property arl adv varying helpful list results limited space consequently summarize restriction imposed norm c lie proper scope reasonable results effect cluster number figure shows arl adv adapts change cluster number rst datasets w t nmi ratio predetermined cluster number original topic number convenience initialization cluster representations described section choose compare arl adv gsdmm competitive baseline k offers insights perspective spectral clustering considered methods dramatic increase found ratio goes phenomenon intuitive models forced merge texts different topics clusters inevitably lead worse clustering results w t nmi evaluation scores continue grow gradually saturated k fails performance ratio gets larger spectral based clustering algorithms lack exibility context short text clustering turn heavily rely human expertise datasets hand increasing ratio help arl adv gsdmm arl adv outperforms gsdmm advgsdmmk advgsdmmk advgsdmmk journal ieee transactions data engineering vol xx xx xxxx table results cluster based retrieval noting denotes original language model retrieval clusters clustering method event stackoverow k gsdmm dec arl adv proposed models simple architectures model parameters involve word embeddings cluster representations adversarial perturbations clusters application cluster based retrieval verify effect short text clustering models stream applications choose cluster based retrieval task specic scenario testing task assumes document relevant given query document cluster belongs related query specically select cbdm variant language models cluster smoothing retrieval model validate contribution text clusters retrieval performance clusters choose produced arl adv representative baselines including k gsdmm dec table presents retrieval results event overow datasets evaluated observe clusters generated model adv boosts performance downstream application e cluster based retrieval gains brought arl adv largest chosen clustering methods conclusion paper developed novel clustering model adv fuses short text representation learning clustering unied model proposed cluster level attention adversarial perturbations added cluster tions enhancing robustness effectiveness model training minimax game extensive studies real life datasets arl adv achieve superior performance compared state art methods short text tering recently developed deep learning based clustering models analysis arl adv performed explain contributions components references t b brown b mann n ryder m subbiah j kaplan p dhariwal neelakantan p shyam g sastry askell al language models shot learners arxiv c carpineto g romano consensus clustering based new ieee probabilistic rand index application subtopic retrieval tpami h chen m sun c tu y lin z liu neural sentiment classication user product attention emnlp pages r das m zaheer c dyer gaussian lda topic models word embeddings acl pages j devlin m chang k lee k toutanova bert pre training deep bidirectional transformers language understanding naacl pages w feng c zhang w zhang j han j wang c c aggarwal j huang streamcube hierarchical spatio temporal hashtag clustering event exploration twitter stream icde pages t fu s tai h chen attentive adversarial learning video summarization wacv pages l gao x li j song h t shen hierarchical lstms adaptive attention visual captioning ieee tpami k ghasedi x wang c deng h huang balanced self paced learning generative adversarial clustering network cvpr pages goodfellow j pouget abadie m mirza b xu d warde farley s ozair courville y bengio generative adversarial nets nips pages j goodfellow j shlens c szegedy explaining harnessing adversarial examples hadifar l sterckx t demeester c develder self training approach short text clustering pages w harchaoui p mattei c bouveyron deep adversarial gaussian mixture auto encoder clustering iclr workshop r w s lee h t ng d dahlmeier unsupervised neural attention model aspect extraction acl pages x z x du t chua adversarial personalized ranking recommendation sigir pages s hochreiter j schmidhuber long short term memory neural computation r huang g yu z wang j zhang l shi dirichlet process ieee mixture model document clustering feature partition tkde l hubert p arabie comparing partitions journal classication r t ionescu m butnaru vector locally aggregated word embeddings vlawe novel document level representation naacl pages k jain data clustering years k means pattern recognition letters z jiang y zheng h tan b tang h zhou variational deep embedding unsupervised generative approach clustering ijcai pages n kalchbrenner e grefenstette p blunsom convolutional neural network modelling sentences acl pages y kim convolutional neural networks sentence classication emnlp pages d p kingma j ba adam method stochastic optimization d p kingma m welling auto encoding variational bayes iclr iclr h w kuhn hungarian method assignment problem naval research logistics y lecun y bengio g hinton deep learning nature c c aggarwal c zhai survey text clustering algorithms mining text data pages springer y li c luo s m chung text clustering feature selection m ailem f role m nadif sparse poisson latent block model statistical data ieee tkde document clustering ieee tkde s liang e yilmaz e kanoulas dynamic clustering streaming d bahdanau k cho y bengio neural machine translation jointly learning align translate arxiv preprint s banerjee k ramanathan gupta clustering short texts wikipedia sigir pages short documents sigkdd pages s liang e yilmaz e kanoulas collaboratively tracking interests user clustering streams short texts ieee tkde x liu w b croft cluster based retrieval language models y bengio learning deep architectures ai foundations trends sigir pages machine learning d m blei y ng m jordan latent dirichlet allocation jmlr q ma l yu s tian e chen w w y ng global local mutual attention model text classication ieee acm taslp journal ieee transactions data engineering vol xx xx xxxx l v d maaten g hinton visualizing data t sne jmlr h zhang j goodfellow d n metaxas odena self attention generative adversarial networks icml pages c d manning h p raghavan introduction information retrieval cambridge university press m mathioudakis n koudas twittermonitor trend detection twitter stream sigmod pages t mikolov sutskever k chen g s corrado j dean distributed representations words phrases compositionality nips pages t miyato m dai goodfellow adversarial training methods semi supervised text classication iclr t miyato s maeda m koyama k nakae s ishii distributional smoothing virtual adversarial training iclr n mrabah m bouguessa r ksantini adversarial deep embedded clustering better trade feature randomness feature drift arxiv s mukherjee h asnani e lin s kannan clustergan latent aaai pages space clustering generative adversarial networks v premachandran l yuille unsupervised learning generative adversarial training clustering arxiv rangrej s kulkarni v tendulkar comparative study www pages clustering techniques short text documents ritter e wright w casey t mitchell weakly supervised www pages extraction computer security events twitter m rosen zvi t l grifths m steyvers p smyth topic model authors documents uai pages r socher karpathy q v le c d manning y ng grounded compositional semantics nding describing images sentences tacl strehl j ghosh cluster ensembles knowledge reuse work combining multiple partitions jmlr d tang b qin t liu document modeling gated recurrent emnlp pages neural network sentiment classication vaswani n shazeer n parmar j uszkoreit l jones n gomez kaiser polosukhin attention need nips pages w wang w zhang j wang j yan h zha learning sequential correlation user generated textual content popularity prediction ijcai pages w wei b xi m kantarcioglu adversarial clustering grid based clustering algorithm active adversaries arxiv s xiao j yan m farajtabar l song x yang h zha learning time series associated event sequences recurrent point process networks ieee tnnls j xie r girshick farhadi unsupervised deep embedding clustering analysis icml pages q xipeng s tianxiang x yige s yunfan d ning h xuanjing pre trained models natural language processing survey science china technological sciences j xu p wang g tian b xu j zhao f wang h hao short text clustering convolutional neural networks naacl pages j xu b xu p wang s zheng g tian j zhao self taught convolutional neural networks short text clustering neural networks x yan j guo y lan x cheng biterm topic model short texts www pages z yan y guo c zhang deep defense training dnns improved adversarial robustness neurips pages z yang d yang c dyer x j smola e h hovy hierarchical attention networks document classication naacl pages s yeung o russakovsky n jin m andriluka g mori l fei moment counts dense detailed labeling actions complex videos ijcv j yin d chao z liu w zhang x yu j wang model based clustering short text streams sigkdd pages j yin j wang dirichlet multinomial mixture model based approach short text clustering sigkdd pages l yu w zhang j wang y yu seqgan sequence generative adversarial nets policy gradient aaai pages ijcai pages y yu w zhou mixture gans clustering
