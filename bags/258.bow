n a j l c s c v v i x r a length controllable abstractive summarization by guiding with summary prototype itsumi kyosuke kosuke atsushi hisako junji hiroyuki yuji ntt media intelligence laboratories ntt nara institute of science and riken center for advanced intelligence itsumi saito ntt co jp abstract we propose a new length controllable abstractive tion model recent state of the art abstractive tion models based on encoder decoder models generate only one summary per source text however controllable rization especially of the length is an important aspect for practical applications previous studies on length controllable abstractive summarization incorporate length embeddings in the decoder module for controlling the summary length though the length embeddings can control where to stop coding they do not decide which information should be cluded in the summary within the length constraint unlike the previous models our length controllable abstractive marization model incorporates a word level extractive ule in the encoder decoder model instead of length dings our model generates a summary in two steps first our word level extractor extracts a sequence of important words we call it the prototype text from the source text ing to the word level importance scores and the length straint second the prototype text is used as additional input to the encoder decoder model which generates a summary by jointly encoding and copying words from both the type text and source text since the prototype text is a guide to both the content and length of the summary our model can generate an informative and length controlled summary periments with the cnn daily mail dataset and the room dataset show that our model outperformed previous models in length controlled settings introduction neural summarization has made great progress in recent years it has two main approaches extractive and tive extractive methods generate summaries by selecting important sentences zhang et al zhou et al they produce grammatically correct summaries however they do not give much exibility to the summarization cause they only extract sentences from the source text by contrast abstractive summarization enables more exible summarization and it is expected to generate more uent and readable summaries than extractive models the most commonly used abstractive summarization model is the pointer generator see liu and manning which erates a summary word by word while copying words from source text various types of renewable energy such as solar and wind are often touted as being the solution to the world s growing energy crisis but one researcher has come up with a novel idea that could trump them all a biological solar panel that works around the clock by harnessing the electrons generated by plants such as moss he said he can create useful energy that could be used at home or elsewhere a university of cambridge scientist has revealed his green source of energy by using just moss he is able to generate enough power to run a clock shown he said panels of plant material could power appliances in our homes and the technology could help farmers grow crops where electricity is scarce reference summary university of cambridge scientist has revealed his green source of energy by using just moss he is able to generate enough power to run a clock he said panels of plant material could power appliances in our homes and the tech could help farmers grow crops where electricity is scarce outputs extracted prototype he said panels of plant material could power in our abstractive summary panels of plant material could power appliances outputs extracted prototype university of cambridge scientist has vealed his he said panels of plant material could power ances in our homes and the technology could help farmers grow crops where is scarce abstractive summary university of cambridge scientist has revealed his green source of energy he said panels of plant terial could power appliances in our homes figure output examples of our model our model extracts the top k important words which are colored red k and blue k as a prototype from the source text it generates an abstractive summary based on the prototype and source texts the length of the generated summary is controlled in accordance with the length of the prototype text the source text and generating words from a pre dened cabulary set this model can generate an accurate summary by combining word level extraction and generation although the idea of controlling the length of the mary was mostly neglected in the past it was recently pointed out that it is actually an important aspect of tive summarization liu luo and zhu fan ier and auli in practical applications the summary length should be controllable in order for it to t the device that displays it however there have only been a few ies on controlling the summary length kikuchi et al proposed a length controllable model that uses length beddings in the length embedding approach the summary length is encoded either as an embedding that represents the remaining length at each decoding step or as an initial bedding to the decoder that represents the desired length liu luo and zhu proposed a model that uses the desired length as an input to the initial state of the decoder these previous models control the length in the decoding module by using length embeddings however length beddings only add length information on the decoder side consequently they may miss important information because it is difcult to take into account which content should be cluded in the summary for certain length constraints we propose a new length controllable abstractive rization that is guided by the prototype text our idea is to use a word level extractive module instead of length dings to control the summary length figure compares the previous length controllable models and the proposed one the yellow blocks are the modules responsible for length control since the word level extractor controls which tents are to be included in the summary when a length straint is given it is possible to generate a summary ing the important contents our model consists of two steps first the word level extractor predicts the word level portance of the source text and extracts important words according to the importance scores and the desired length the extracted word sequence is used as a prototype of the summary we call it the prototype text second we use the prototype text as an additional input of the encoder decoder model the length of the summary is kept close to that of the prototype text because the summary is generated by ring to the prototype text figure shows examples of put generated by our model our abstractive summaries are similar to the extracted prototypes the extractive module produces a rough overview of the summary and the decoder module produces a uent summary based on the tracted prototype our idea is inspired by extractive and abstractive rization extractive and abstractive summarization rates an extractive model in an abstractive encoder decoder model while in the simple encoder decoder model one model identies the important contents and generates ent summaries the extractive and abstractive model has an encoder decoder part that generates uent summaries and a separate part that extracts important contents several ies have shown that separating the problem of nding the important content and the problem of generating uent maries improves the accuracy of the summary gehrmann deng and rush chen and bansal our model can be regarded as an extension of models that work in this previous length controllable models proposed model source summary source enc dec model summary enc dec model length embeddings desired length k prototype k words extractive model desired length k figure comparison of previous length controllable els and proposed model our model controls the summary length in accordance with the length of the prototype text way however this is the rst to extend the extractive ule such that it can control the summary length ours is the rst method that controls the summary length using an extractive module and that achieves both high racy and length controllability in abstractive summarization our contributions are summarized as follows we propose a new length controllable prototype guided abstractive summarization model called lpas controllable prototype guided abstractive tion our model effectively guides the abstractive marization using a summary prototype our model trols the summary length by controlling the number of words in the prototype text our model achieved state of the art rouge scores in length controlled abstractive summarization settings on the cnn dm and newsroom datasets task denition our study denes length controllable abstractive rization as two pipelined tasks prototype extraction and prototype guided abstractive summarization the problem formulations of each task are described below task prototype extraction given a source text x c xc with l words x xc l and a desired summary length k the model estimates importance scores p l and extracts the top k important words x p pext pext xp xp k as a prototype text on the basis of p ext the desired summary length k can be set to an arbitrary value note that the original word order is preserved in x p x p is not bag of words task prototype guided abstractive summarization given the source text and the extracted prototype text x p the model generates a length controlled abstractive mary y yt the length of summary t is trolled in accordance with the prototype length k proposed model overview our model consists of three modules the prototype tor joint encoder and summary decoder figure the last two modules comprise task the prototype guided tive summarization the prototype extractor uses bert summary pointer generator dual encoder block source dual encoder block prototype decoder block source shared encoder block shared encoder block decoder block prototype positional encoding positional encoding glove source text glove prototype text positional encoding glove joint encoder sec summaization decoder sec prototype text top k words sigmoid linear bert source text prototype extractor sec figure architecture of proposed model while the joint encoder and summary decoder use the former architecture vaswani et al prototype extractor the prototype extractor tracts the top k important words from the source text joint encoder the joint encoder encodes both the source text and the prototype text summary decoder the summary decoder is based on the pointer generator model and generates an abstractive summary by using the output of the joint encoder prototype extractor since our model extracts the prototype at the word level the prototype extractor estimates an importance score pext of each word xc l x c bert has achieved sota on many classication tasks so it is a natural choice for the prototype extractor our model uses bert and a task specic forward network on top of bert we tokenize the source text using the bert and ne tune the bert model the importance score pext l l pext l is dened as c l where bert is the last hidden state of the pre trained bert rdbert and are learnable parameters is a sigmoid function dbert is the dimension of the last hidden state of the pre trained bert to extract a more uent prototype than when using only the word level importance we dene a new weighted tance score pextw that incorporates a sentence level tance score as a weight for the word level importance score l pextw l pext l pext sj pext sj nsj xl xlsj pext l where nsj is the number of words in the j th sentence sj x c our model extracts the top k important words as a prototype from the source text on the basis of pextw it controls the length of the summary in accordance with the number of words in the prototype text k l com google research joint encoder embedding layer this layer projects each of the one hot vectors of words xc of size v into a dword dimensional l vector space with a pre trained weight matrix w e rdwordv such as glove pennington socher and ning then the word embeddings are mapped to dmodel dimensional vectors by using the fully connected layer and the mapped embeddings are passed to a relu function this layer also adds positional encoding to the word embedding vaswani et al transformer encoder blocks the encoder encodes the embedded source and prototype texts with a stack of former blocks vaswani et al our model encodes the two texts with the encoder stack independently we denote s rdmodelk s rdmodell and ep these outputs as ec respectively transformer dual encoder blocks this block calculates the interactive alignment between the encoded source and prototype texts specically it encodes the source and totype texts and then performs multi head attention on the other output of the encoder stack i e ec s we denote the outputs of the dual encoder stack of the source text and prototype text by m c rdmodell and m p rdmodelk respectively s and ep summary decoder embedding layer the decoder receives a sequence of words in an abstractive summary y which is generated through an auto regressive process at each decoding step t this layer projects each of the one hot vectors of the words yt in the same way as the embedding layer in the joint encoder transformer decoder blocks the decoder uses a stack of decoder transformer blocks vaswani et al that form multi head attention on the encoded representations of the prototype m p it uses another stack of decoder former blocks that perform multi head attention on those of the source text m c on top of the rst stack the rst stack rewrites the prototype text and the second one complements the rewritten prototype with the original source information the subsequent mask is used in the stacks since this nent is used in a step by step manner at test time the output of the stacks is m s rdmodelt copying mechanism our pointer generator model copies the words from the source and prototype texts on the basis of the copy distributions for efcient reuse copy distributions the copy distributions of the source and prototype words are described as follows p tk tl xk xl xc l where p tl are respectively the rst attention heads of the last block in the rst and second stacks of the decoder tk and c final vocabulary distribution the nal vocabulary tribution is described as follows joint encoder and summary decoder the main loss for the encoder decoder is the cross entropy loss g s cc tl m c l t t cc t cp tkm p p cp t bv xl s xk bg lmain gen n t n t log x c x p moreover we add the attention guide loss of the summary decoder this loss is designed to guide the estimated tion distribution to the reference attention where w v bv w g rdmodelv and bg rv are learnable parameters training our model is not trained in an end to end manner the totype extractor is trained rst and then the encoder and coder are trained generating training data x c rl is if xc l l and label rl pairs xc prototype extractor since there are no supervised data for the prototype extractor we created pseudo training data like in gehrmann deng and rush the training data consists of word xc l rl for all xc is included in the summary otherwise it is to construct the paired data automatically we rst extract oracle source sentences soracle that mize the rouge r score in the same way as in hsu et al then we calculate the word by word alignment tween the reference summary and soracle using a dynamic programming algorithm to consider the word order finally we label all aligned words with and other words including the words that are not in the oracle sentence with joint encoder and summary decoder we have to create triple data of x c x p y consisting of the source text the gold prototype text and the target text for training our encoder and decoder we use the top k words in terms of pextw eq in the oracle sentences soracle as the gold l totype text to extract a prototype closer to the reference mary and improve the quality of the encoder decoder ing k is decided using the reference summary length t to obtain a natural summary close to the desired length we quantize the length t into discrete bins where each bin resents a size range we set the size range to in this study that is the value nearest to the summary length t among multiples of is selected for k loss function prototype extractor we use the binary cross entropy loss because the extractor estimates the importance score of each word eq which is a binary classication task lext n l n l rl log pext l rl pext l where n is the number of training examples lsum attn log c t lproto attn log proto t n t n t n t n t proto t is the rst attention head of the last block in the joint encoder stack for the prototype denotes the absolute sition in the source text corresponding to the t th word in the sequence of summary words the overall loss of the tion model is a linear combination of these three losses lgen lmain gen attn attn and were set to in the experiments inference during the inference period we use a beam search and ranking chen and bansal we keep all nbeam mary candidates provided by the beam search where nbeam is the size of the beam and generate the nbeam best maries the summaries are then re ranked by the number of repeated n grams the smaller the better the beam search and this re ranking improve the rouge score of the output as they eliminate candidates that contain repetitions for the length controlled setting we set the value of k to the sired length for the standard setting we set it to the average length of the reference summary in the validation data experiments datasets and settings dataset we used the cnn dm dataset hermann et al a standard corpus for news summarization the maries are bullet points for the articles shown on their spective websites following see liu and manning we used the non anonymized version of the corpus and truncated the source documents to tokens and the get summaries to tokens the dataset includes training pairs validation pairs and test pairs we also used the newsroom dataset grusky man and artzi newsroom contains various news sources different news sites we used pairs of data for training we sampled pairs for validation data and the number of the test pairs was to uate the length controlled setting for newsroom dataset we randomly sampled samples from the test set length model avg lenemb lpas lc lenemb lpas lc lenemb lpas lc lenemb lpas lc lenemb lpas lc lenemb lpas r l table rouge scores of abstractive summarization models with different lengths on the cnn dm dataset words avg indicates the average rouge score for the ve different lengths luo and zhu model congurations we used the same congurations for the two datasets the extractor used the pre trained bertlarge model devlin et al we ne tuned bert for two epochs with the default settings our encoder and coder used pre trained dimensional glove embeddings the encoder and decoder transformer have four blocks the number of heads was and the number of dimensions of ffn was dmodel was set to we used the adam optimizer kingma and ba with a scheduled learning rate vaswani et al we set the size of the input cabulary to and the output vocabulary to evaluation metrics we used the rouge scores including and rouge l r l as the evaluation metrics lin we used the toolkit for culating the rouge results does our model improve the rouge score in the length controlled setting we used two types of controllable models as baselines the rst one is a based length controllable model lc that uses the desired length as an input to the initial state of the cnn based coder liu luo and zhu the second one lenemb embeds the remaining length and adds them to each decoder step kikuchi et al since there are no previous results on applying lenemb to the cnn dm dataset we mented it as a transformer based encoder decoder model specically we simply added the embeddings of the ing length to the word embeddings at each decoding step com t g n e l t u t u o desired length figure results in the length controlled setting on cnn dm a rouge l recall precision and f scores for different lengths left output length distribution right table shows that our model achieved high rouge scores for different lengths and outperformed the previous length controllable models in most cases our model was about points more accurate on average than lenemb our model selected the most important words from the source text in accordance with the desired length it was thus tive at keeping the important information even in the controlled setting figure shows the precision recall and f score of rouge for different lengths our model tained a high f score around the average length around words this indicates that it can select important information and generate stable results with different lengths does our model generate a summary with the desired length figure shows the relationship between the sired length and the output length the axis indicates the desired length and the y axis indicates the average length and standard deviation of the length controlled output mary the results show that our model properly controls the summary length this controllable nature comes from the training procedure when training our encoder decoder we set the number of words k in the prototype text according to the length of the reference summary therefore the model learns to generate a summary that has a similar length to the prototype text how good is the quality of the prototype text to uate the quality of the prototype we evaluated the rouge scores of the extracted prototype text table shows the sults in the table lpas ext sents means the three sentences were extracted using pext interestingly sj and scores of the lpas ext top k words were higher than those of the sentence level tive models this indicates that word level lpas ext is fective at nding not only important words but also important phrases also we can see from table that whole lpas improved the rouge l score of lpas ext this indicates that our joint encoder and mary decoder generate more uent summaries with the help of the prototype text does our abstractive model improve if the quality of the prototype is improved we evaluated our model in the following two settings in order to analyze the relationship between the quality of the abstractive summary and that of the prototype in the gold length setting we only gave the gold length k to the prototype extractor in the gold bottom up bottom up lpas ext sents top k words r l table rouge scores of our prototype extractor lpas ext on cnn dm deng and rush et al wei and zhou average length gold length gold sentences gold length r l table rouge scores of abstractive summarization models with gold settings on the cnn dm dataset tences the gold length setting we gave the gold sentences soracle and gold length see table shows the results these results indicate that selecting the correct number of words in the prototype improves the rouge scores in this study we simply selected the average length when ing the prototype for all examples in the standard setting however there will be an improvement if we adaptively lect the number of words in the prototype for each source text moreover the rouge score largely improved in the gold sentence and gold length settings this indicates that the quality of the generated summary will signicantly prove by increasing the accuracy of the extractive model is our model effective on other datasets to verify the effectiveness of our model on various other summary styles we evaluated it on a large and varied news summary dataset newsroom table and figure show the results in the length controlled setting for newsroom our model achieved higher rouge scores than those of lenemb from figure we can see that the f value of the rouge score was highest around words this is because the erage word number is about words moreover figure shows that our model also acquired a length control ity for a dataset with various styles how well does our model perform in the standard ting table shows that our model achieved the rouge scores comparable to previous models that do not consider the length constraint on the cnn dm dataset we note that the current state of the art models use pre trained decoder models while the encoder and decoder of our model except for prototype extractor were not pre trained we also examined the results of generating a summary from only the prototype lpas source or the source length model avg lenemb lpas lenemb lpas lenemb lpas lenemb lpas lenemb lpas lenemb lpas r l table rouge scores of abstractive summarization models with different lengths on the newsroom dataset t g n e l t u t u o desired length figure results in the length controlled setting on room a rouge l recall precision and f scores for ferent lengths left output length distribution right lpas prototype here using only the prototype turned out to have the same accuracy as using only the source but the model using the source and the prototype simultaneously had higher accuracy these results indicate that our prototype extraction and joint encoder effectively incorporated the source text and prototype information and contributed to improving the accuracy the results for the newsroom dataset under dard settings are shown in table to consider differences in summary length between news domains we evaluated our model in the average length and domain level average length denoted as domain length settings the results cate that our model had signicantly higher rouge scores compared with the ofcial baselines and outperformed our baseline lpas prototype they also indicate that our model is effective on datasets containing text in ous styles moreover we found that considering the domain length has positive effects on the rouge scores this dicates that our model can easily reect differences in mary length among various styles related work and discussion length control for summarization kikuchi et al were the rst to propose using length embedding for controlled abstractive summarization fan grangier and auli also used length embeddings at the beginning of the decoder module for length control liu luo and zhu proposed a cnn based length controllable marization model that uses the desired length as an model pre trained encoder decoder model pointer pointer generator key information guide unied sentence bottom exconsumm etads lpas prototype source pre trained encoder decoder model r l table rouge scores of abstractive tion models on cnn dm liu and manning et al et al and bansal deng and rush et al et al et al et al et al et al lpas prototype denotes a simple transformer based generator which is our model without the prototype tor and the joint encoder lpas source denotes a model that generates a summary only from the prototype text pointer generator lpas k average length k domain length lpas prototype r l table rouge scores of proposed models on room dataset naaman and artzi put to the initial state of the decoder takase and okazaki introduced positional encoding that represents the remaining length at each decoder step of the based encoder decoder model it is almost equivalent to the model lenemb we implemented these previous models use length embeddings for controlling the length in the coding module whereas we use the prototype extractor for controlling the summary length and to include important formation in the summary neural extractive and abstractive summarization hsu et al gehrmann deng and rush and you et al incorporated a and word level extractive model in the pointer generator model their models weight the copy probability for the source text by using an extractive model and guide the pointer generator model to copy portant words li et al proposed a keyword guided abstractive summarization model chen and bansal proposed a sentence extraction and re writing model that trains in an end to end manner by using reinforcement ing cao et al proposed a search and rewrite model mendes et al proposed a combination of level extraction and compression the idea behind these models is word level weighting for the entire source text or sentence level re writing on the other hand our model guides the summarization with a length controllable type text by using the prototype extractor and joint encoder utilizing extractive results to control the length of the mary is a new idea large scale pre trained language model bert vlin et al is a new pre trained language model that uses bidirectional encoder representations from former bert has performed well in many natural language understanding tasks such as the glue benchmarks wang et al and natural language inference williams gia and bowman liu used bert for their sentence level extractive summarization model zhang wei and zhou trained a new pre trained model that siders document level information for sentence level tive summarization we used bert for the word level totype extractor and veried the effectiveness of using it in the word level extractive module several researchers have published pre trained encoder decoder models very cently wang et al lewis et al raffel et al wang et al pre trained a transformer based pointer generator model lewis et al pre trained a normal transformer based encoder decoder model using large unlabeled data and achieved state of the art results dong et al extended the bert structure to handle sequence to sequence tasks reinforcement learning for summarization ment learning rl is a key summarization technique rl can be used to optimize non differential metrics or ple non differential networks narayan cohen and lapata and dong et al used rl for extractive marization for abstractive summarization paulus xiong and socher used rl to mitigate the exposure bias of abstractive summarization chen and bansal used rl to combine sentence extraction and pointer generator els our model achieved high rouge scores without rl in future we may incorporate rl in our models to get a further improvement conclusion we proposed a new length controllable abstractive rization model our model consists of a word level prototype extractor and a prototype guided abstractive summarization model the prototype extractor identies the important part of the source text within the length constraint and the stractive model is guided with the prototype text this acteristic enabled it to achieve a high rouge score in dard summarization tasks moreover our prototype tor ensures the summary will have the desired length periments with the cnn dm dataset and the newsroom dataset show that our model outperformed previous els in standard and length controlled settings in future we mendes a narayan s miranda s marinho z martins a f t and cohen s b jointly extracting and pressing documents with summary state representations in naacl narayan s cohen s b and lapata m ranking sentences for extractive summarization with reinforcement learning in naacl paulus r xiong c and socher r a deep reinforced model for abstractive summarization corr pennington j socher r and manning c d glove global vectors for word representation in emnlp raffel c shazeer n roberts a lee k narang s matena m zhou y li w and liu p j ing the limits of transfer learning with a unied text to text transformer arxiv e prints see a liu p j and manning c d get to the point summarization with pointer generator networks in acl takase s and okazaki n positional encoding to control output sequence length in naacl vaswani a shazeer n parmar n uszkoreit j jones l gomez a n kaiser l u and polosukhin i attention is all you need in nips wang a singh a michael j hill f levy o and bowman s r glue a multi task benchmark and analysis platform for natural language understanding in wang l zhao w jia r li s and liu j ing based sequence to sequence pre training for text ation in emnlp to appear williams a nangia n and bowman s r a broad coverage challenge corpus for sentence understanding through inference in naacl hlt you y jia w liu t and yang w improving stractive document summarization with salient information modeling in acl zhang x lapata m wei f and zhou m neural latent extractive document summarization in emnlp association for computational linguistics zhang x wei f and zhou m hibert document level pre training of hierarchical bidirectional transformers for document summarization in acl zhou q yang n wei f huang s zhou m and zhao t neural document summarization by jointly ing to score and select sentences in acl would like to incorporate a pre trained language model in the abstractive model to build a higher quality tion model references cao z li w li s and wei f retrieve rerank and rewrite soft template based neural summarization in acl chen y and bansal m fast abstractive rization with reinforce selected sentence rewriting in acl devlin j chang m lee k and toutanova k bert pre training of deep bidirectional transformers for guage understanding corr dong y shen y crawford e van hoof h and cheung j c k banditsum extractive summarization as a contextual bandit in emnlp dong l yang n wang w wei f liu x wang y gao j zhou m and hon h unied language model pre training for natural language understanding and generation in advances in neural information processing systems fan a grangier d and auli m controllable abstractive summarization in gehrmann s deng y and rush a bottom up abstractive summarization in emnlp grusky m naaman m and artzi y newsroom a dataset of million summaries with diverse extractive strategies in acl hermann k m kocisky t grefenstette e espeholt l kay w suleyman m and blunsom p teaching machines to read and comprehend in nips hsu w lin c lee m min k tang j and sun m a unied model for extractive and abstractive summarization using inconsistency loss in acl kikuchi y neubig g sasano r takamura h and okumura m controlling output length in neural encoder decoders in emnlp kingma d p and ba j adam a method for stochastic optimization in iclr lewis m liu y goyal n ghazvininejad m hamed a levy o stoyanov v and zettlemoyer l bart denoising sequence to sequence pre training for natural language generation translation and sion arxiv e prints li c xu w li s and gao s guiding generation for abstractive text summarization based on key information guide network in acl lin c rouge a package for automatic evaluation of summaries in acl liu y luo z and zhu k controlling length in abstractive summarization using a convolutional neural work in emnlp liu y fine tune bert for extractive summarization corr
