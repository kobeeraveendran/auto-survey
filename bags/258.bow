length controllable abstractive summarization guiding summary prototype itsumi kyosuke kosuke atsushi hisako junji hiroyuki yuji ntt media intelligence laboratories ntt nara institute science riken center advanced intelligence itsumi saito ntt abstract propose new length controllable abstractive tion model recent state art abstractive tion models based encoder decoder models generate summary source text controllable rization especially length important aspect practical applications previous studies length controllable abstractive summarization incorporate length embeddings decoder module controlling summary length length embeddings control stop coding decide information cluded summary length constraint unlike previous models length controllable abstractive marization model incorporates word level extractive ule encoder decoder model instead length dings model generates summary steps word level extractor extracts sequence important words prototype text source text ing word level importance scores length straint second prototype text additional input encoder decoder model generates summary jointly encoding copying words type text source text prototype text guide content length summary model generate informative length controlled summary periments cnn daily mail dataset room dataset model outperformed previous models length controlled settings introduction neural summarization great progress recent years main approaches extractive tive extractive methods generate summaries selecting important sentences zhang zhou produce grammatically correct summaries exibility summarization cause extract sentences source text contrast abstractive summarization enables exible summarization expected generate uent readable summaries extractive models commonly abstractive summarization model pointer generator liu manning erates summary word word copying words source text types renewable energy solar wind touted solution world growing energy crisis researcher come novel idea trump biological solar panel works clock harnessing electrons generated plants moss said create useful energy home university cambridge scientist revealed green source energy moss able generate power run clock shown said panels plant material power appliances homes technology help farmers grow crops electricity scarce reference summary university cambridge scientist revealed green source energy moss able generate power run clock said panels plant material power appliances homes tech help farmers grow crops electricity scarce outputs extracted prototype said panels plant material power abstractive summary panels plant material power appliances outputs extracted prototype university cambridge scientist vealed said panels plant material power ances homes technology help farmers grow crops scarce abstractive summary university cambridge scientist revealed green source energy said panels plant terial power appliances homes figure output examples model model extracts important words colored red blue prototype source text generates abstractive summary based prototype source texts length generated summary controlled accordance length prototype text source text generating words pre dened cabulary set model generate accurate summary combining word level extraction generation idea controlling length mary neglected past recently pointed actually important aspect tive summarization liu luo zhu fan ier auli practical applications summary length controllable order device displays ies controlling summary length kikuchi proposed length controllable model uses length beddings length embedding approach summary length encoded embedding represents remaining length decoding step initial bedding decoder represents desired length liu luo zhu proposed model uses desired length input initial state decoder previous models control length decoding module length embeddings length beddings add length information decoder consequently miss important information difcult account content cluded summary certain length constraints propose new length controllable abstractive rization guided prototype text idea use word level extractive module instead length dings control summary length figure compares previous length controllable models proposed yellow blocks modules responsible length control word level extractor controls tents included summary length straint given possible generate summary ing important contents model consists steps word level extractor predicts word level portance source text extracts important words according importance scores desired length extracted word sequence prototype summary prototype text second use prototype text additional input encoder decoder model length summary kept close prototype text summary generated ring prototype text figure shows examples generated model abstractive summaries similar extracted prototypes extractive module produces rough overview summary decoder module produces uent summary based tracted prototype idea inspired extractive abstractive rization extractive abstractive summarization rates extractive model abstractive encoder decoder model simple encoder decoder model model identies important contents generates ent summaries extractive abstractive model encoder decoder generates uent summaries separate extracts important contents ies shown separating problem nding important content problem generating uent maries improves accuracy summary gehrmann deng rush chen bansal model regarded extension models work previous length controllable models proposed model source summary source enc dec model summary enc dec model length embeddings desired length prototype words extractive model desired length figure comparison previous length controllable els proposed model model controls summary length accordance length prototype text way rst extend extractive ule control summary length rst method controls summary length extractive module achieves high racy length controllability abstractive summarization contributions summarized follows propose new length controllable prototype guided abstractive summarization model called lpas controllable prototype guided abstractive tion model effectively guides abstractive marization summary prototype model trols summary length controlling number words prototype text model achieved state art rouge scores length controlled abstractive summarization settings cnn newsroom datasets task denition study denes length controllable abstractive rization pipelined tasks prototype extraction prototype guided abstractive summarization problem formulations task described task prototype extraction given source text words desired summary length model estimates importance scores extracts important words pext pext prototype text basis ext desired summary length set arbitrary value note original word order preserved bag words task prototype guided abstractive summarization given source text extracted prototype text model generates length controlled abstractive mary length summary trolled accordance prototype length proposed model overview model consists modules prototype tor joint encoder summary decoder figure modules comprise task prototype guided tive summarization prototype extractor uses bert summary pointer generator dual encoder block source dual encoder block prototype decoder block source shared encoder block shared encoder block decoder block prototype positional encoding positional encoding glove source text glove prototype text positional encoding glove joint encoder sec summaization decoder sec prototype text words sigmoid linear bert source text prototype extractor sec figure architecture proposed model joint encoder summary decoder use architecture vaswani prototype extractor prototype extractor tracts important words source text joint encoder joint encoder encodes source text prototype text summary decoder summary decoder based pointer generator model generates abstractive summary output joint encoder prototype extractor model extracts prototype word level prototype extractor estimates importance score pext word bert achieved sota classication tasks natural choice prototype extractor model uses bert task specic forward network bert tokenize source text bert tune bert model importance score pext pext dened bert hidden state pre trained bert rdbert learnable parameters sigmoid function dbert dimension hidden state pre trained bert extract uent prototype word level importance dene new weighted tance score pextw incorporates sentence level tance score weight word level importance score pextw pext pext pext nsj xlsj pext nsj number words sentence model extracts important words prototype source text basis pextw controls length summary accordance number words prototype text com google research joint encoder embedding layer layer projects hot vectors words size dword dimensional vector space pre trained weight matrix rdwordv glove pennington socher ning word embeddings mapped dmodel dimensional vectors fully connected layer mapped embeddings passed relu function layer adds positional encoding word embedding vaswani transformer encoder blocks encoder encodes embedded source prototype texts stack blocks vaswani model encodes texts encoder stack independently denote rdmodelk rdmodell outputs respectively transformer dual encoder blocks block calculates interactive alignment encoded source prototype texts specically encodes source totype texts performs multi head attention output encoder stack denote outputs dual encoder stack source text prototype text rdmodell rdmodelk respectively summary decoder embedding layer decoder receives sequence words abstractive summary generated auto regressive process decoding step layer projects hot vectors words way embedding layer joint encoder transformer decoder blocks decoder uses stack decoder transformer blocks vaswani form multi head attention encoded representations prototype uses stack decoder blocks perform multi head attention source text rst stack rst stack rewrites prototype text second complements rewritten prototype original source information subsequent mask stacks nent step step manner test time output stacks rdmodelt copying mechanism pointer generator model copies words source prototype texts basis copy distributions efcient reuse copy distributions copy distributions source prototype words described follows respectively rst attention heads block rst second stacks decoder final vocabulary distribution nal vocabulary tribution described follows joint encoder summary decoder main loss encoder decoder cross entropy loss tkm lmain gen log add attention guide loss summary decoder loss designed guide estimated tion distribution reference attention rdmodelv learnable parameters training model trained end end manner totype extractor trained rst encoder coder trained generating training data label pairs prototype extractor supervised data prototype extractor created pseudo training data like gehrmann deng rush training data consists word included summary construct paired data automatically rst extract oracle source sentences soracle mize rouge score way hsu calculate word word alignment tween reference summary soracle dynamic programming algorithm consider word order finally label aligned words words including words oracle sentence joint encoder summary decoder create triple data consisting source text gold prototype text target text training encoder decoder use words terms pextw oracle sentences soracle gold totype text extract prototype closer reference mary improve quality encoder decoder ing decided reference summary length obtain natural summary close desired length quantize length discrete bins bin resents size range set size range study value nearest summary length multiples selected loss function prototype extractor use binary cross entropy loss extractor estimates importance score word binary classication task lext log pext pext number training examples lsum attn log lproto attn log proto proto rst attention head block joint encoder stack prototype denotes absolute sition source text corresponding word sequence summary words overall loss tion model linear combination losses lgen lmain gen attn attn set experiments inference inference period use beam search ranking chen bansal nbeam mary candidates provided beam search nbeam size beam generate nbeam best maries summaries ranked number repeated grams smaller better beam search ranking improve rouge score output eliminate candidates contain repetitions length controlled setting set value sired length standard setting set average length reference summary validation data experiments datasets settings dataset cnn dataset hermann standard corpus news summarization maries bullet points articles shown spective websites following liu manning non anonymized version corpus truncated source documents tokens summaries tokens dataset includes training pairs validation pairs test pairs newsroom dataset grusky man artzi newsroom contains news sources different news sites pairs data training sampled pairs validation data number test pairs uate length controlled setting newsroom dataset randomly sampled samples test set length model avg lenemb lpas lenemb lpas lenemb lpas lenemb lpas lenemb lpas lenemb lpas table rouge scores abstractive summarization models different lengths cnn dataset words avg indicates average rouge score different lengths luo zhu model congurations congurations datasets extractor pre trained bertlarge model devlin tuned bert epochs default settings encoder coder pre trained dimensional glove embeddings encoder decoder transformer blocks number heads number dimensions ffn dmodel set adam optimizer kingma scheduled learning rate vaswani set size input cabulary output vocabulary evaluation metrics rouge scores including rouge evaluation metrics lin toolkit culating rouge results model improve rouge score length controlled setting types controllable models baselines rst based length controllable model uses desired length input initial state cnn based coder liu luo zhu second lenemb embeds remaining length adds decoder step kikuchi previous results applying lenemb cnn dataset mented transformer based encoder decoder model specically simply added embeddings ing length word embeddings decoding step com desired length figure results length controlled setting cnn rouge recall precision scores different lengths left output length distribution right table shows model achieved high rouge scores different lengths outperformed previous length controllable models cases model points accurate average lenemb model selected important words source text accordance desired length tive keeping important information controlled setting figure shows precision recall score rouge different lengths model tained high score average length words indicates select important information generate stable results different lengths model generate summary desired length figure shows relationship sired length output length axis indicates desired length axis indicates average length standard deviation length controlled output mary results model properly controls summary length controllable nature comes training procedure training encoder decoder set number words prototype text according length reference summary model learns generate summary similar length prototype text good quality prototype text uate quality prototype evaluated rouge scores extracted prototype text table shows sults table lpas ext sents means sentences extracted pext interestingly scores lpas ext words higher sentence level tive models indicates word level lpas ext fective nding important words important phrases table lpas improved rouge score lpas ext indicates joint encoder mary decoder generate uent summaries help prototype text abstractive model improve quality prototype improved evaluated model following settings order analyze relationship quality abstractive summary prototype gold length setting gave gold length prototype extractor gold lpas ext sents words table rouge scores prototype extractor lpas ext cnn deng rush wei zhou average length gold length gold sentences gold length table rouge scores abstractive summarization models gold settings cnn dataset tences gold length setting gave gold sentences soracle gold length table shows results results indicate selecting correct number words prototype improves rouge scores study simply selected average length ing prototype examples standard setting improvement adaptively lect number words prototype source text rouge score largely improved gold sentence gold length settings indicates quality generated summary signicantly prove increasing accuracy extractive model model effective datasets verify effectiveness model summary styles evaluated large varied news summary dataset newsroom table figure results length controlled setting newsroom model achieved higher rouge scores lenemb figure value rouge score highest words erage word number words figure shows model acquired length control ity dataset styles model perform standard ting table shows model achieved rouge scores comparable previous models consider length constraint cnn dataset note current state art models use pre trained decoder models encoder decoder model prototype extractor pre trained examined results generating summary prototype lpas source source length model avg lenemb lpas lenemb lpas lenemb lpas lenemb lpas lenemb lpas lenemb lpas table rouge scores abstractive summarization models different lengths newsroom dataset desired length figure results length controlled setting room rouge recall precision scores ferent lengths left output length distribution right lpas prototype prototype turned accuracy source model source prototype simultaneously higher accuracy results indicate prototype extraction joint encoder effectively incorporated source text prototype information contributed improving accuracy results newsroom dataset dard settings shown table consider differences summary length news domains evaluated model average length domain level average length denoted domain length settings results cate model signicantly higher rouge scores compared ofcial baselines outperformed baseline lpas prototype indicate model effective datasets containing text ous styles found considering domain length positive effects rouge scores dicates model easily reect differences mary length styles related work discussion length control summarization kikuchi rst propose length embedding controlled abstractive summarization fan grangier auli length embeddings beginning decoder module length control liu luo zhu proposed cnn based length controllable marization model uses desired length model pre trained encoder decoder model pointer pointer generator key information guide unied sentence exconsumm etads lpas prototype source pre trained encoder decoder model table rouge scores abstractive tion models cnn liu manning bansal deng rush lpas prototype denotes simple transformer based generator model prototype tor joint encoder lpas source denotes model generates summary prototype text pointer generator lpas average length domain length lpas prototype table rouge scores proposed models room dataset naaman artzi initial state decoder takase okazaki introduced positional encoding represents remaining length decoder step based encoder decoder model equivalent model lenemb implemented previous models use length embeddings controlling length coding module use prototype extractor controlling summary length include important formation summary neural extractive abstractive summarization hsu gehrmann deng rush incorporated word level extractive model pointer generator model models weight copy probability source text extractive model guide pointer generator model copy portant words proposed keyword guided abstractive summarization model chen bansal proposed sentence extraction writing model trains end end manner reinforcement ing cao proposed search rewrite model mendes proposed combination level extraction compression idea models word level weighting entire source text sentence level writing hand model guides summarization length controllable type text prototype extractor joint encoder utilizing extractive results control length mary new idea large scale pre trained language model bert vlin new pre trained language model uses bidirectional encoder representations bert performed natural language understanding tasks glue benchmarks wang natural language inference williams gia bowman liu bert sentence level extractive summarization model zhang wei zhou trained new pre trained model siders document level information sentence level tive summarization bert word level totype extractor veried effectiveness word level extractive module researchers published pre trained encoder decoder models cently wang lewis raffel wang pre trained transformer based pointer generator model lewis pre trained normal transformer based encoder decoder model large unlabeled data achieved state art results dong extended bert structure handle sequence sequence tasks reinforcement learning summarization ment learning key summarization technique optimize non differential metrics ple non differential networks narayan cohen lapata dong extractive marization abstractive summarization paulus xiong socher mitigate exposure bias abstractive summarization chen bansal combine sentence extraction pointer generator els model achieved high rouge scores future incorporate models improvement conclusion proposed new length controllable abstractive rization model model consists word level prototype extractor prototype guided abstractive summarization model prototype extractor identies important source text length constraint stractive model guided prototype text acteristic enabled achieve high rouge score dard summarization tasks prototype tor ensures summary desired length periments cnn dataset newsroom dataset model outperformed previous els standard length controlled settings future mendes narayan miranda marinho martins cohen jointly extracting pressing documents summary state representations naacl narayan cohen lapata ranking sentences extractive summarization reinforcement learning naacl paulus xiong socher deep reinforced model abstractive summarization corr pennington socher manning glove global vectors word representation emnlp raffel shazeer roberts lee narang matena zhou liu ing limits transfer learning unied text text transformer arxiv prints liu manning point summarization pointer generator networks acl takase okazaki positional encoding control output sequence length naacl vaswani shazeer parmar uszkoreit jones gomez kaiser polosukhin attention need nips wang singh michael hill levy bowman glue multi task benchmark analysis platform natural language understanding wang zhao jia liu ing based sequence sequence pre training text ation emnlp appear williams nangia bowman broad coverage challenge corpus sentence understanding inference naacl hlt jia liu yang improving stractive document summarization salient information modeling acl zhang lapata wei zhou neural latent extractive document summarization emnlp association computational linguistics zhang wei zhou hibert document level pre training hierarchical bidirectional transformers document summarization acl zhou yang wei huang zhou zhao neural document summarization jointly ing score select sentences acl like incorporate pre trained language model abstractive model build higher quality tion model references cao wei retrieve rerank rewrite soft template based neural summarization acl chen bansal fast abstractive rization reinforce selected sentence rewriting acl devlin chang lee toutanova bert pre training deep bidirectional transformers guage understanding corr dong shen crawford van hoof cheung banditsum extractive summarization contextual bandit emnlp dong yang wang wei liu wang gao zhou hon unied language model pre training natural language understanding generation advances neural information processing systems fan grangier auli controllable abstractive summarization gehrmann deng rush abstractive summarization emnlp grusky naaman artzi newsroom dataset million summaries diverse extractive strategies acl hermann kocisky grefenstette espeholt kay suleyman blunsom teaching machines read comprehend nips hsu lin lee min tang sun unied model extractive abstractive summarization inconsistency loss acl kikuchi neubig sasano takamura okumura controlling output length neural encoder decoders emnlp kingma adam method stochastic optimization iclr lewis liu goyal ghazvininejad hamed levy stoyanov zettlemoyer bart denoising sequence sequence pre training natural language generation translation sion arxiv prints gao guiding generation abstractive text summarization based key information guide network acl lin rouge package automatic evaluation summaries acl liu luo zhu controlling length abstractive summarization convolutional neural work emnlp liu fine tune bert extractive summarization corr
