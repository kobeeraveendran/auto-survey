n j l c s c v v x r length controllable abstractive summarization guiding summary prototype itsumi kyosuke kosuke atsushi hisako junji hiroyuki yuji ntt media intelligence laboratories ntt nara institute science riken center advanced intelligence itsumi saito ntt co jp abstract propose new length controllable abstractive tion model recent state art abstractive tion models based encoder decoder models generate summary source text controllable rization especially length important aspect practical applications previous studies length controllable abstractive summarization incorporate length embeddings decoder module controlling summary length length embeddings control stop coding decide information cluded summary length constraint unlike previous models length controllable abstractive marization model incorporates word level extractive ule encoder decoder model instead length dings model generates summary steps word level extractor extracts sequence important words prototype text source text ing word level importance scores length straint second prototype text additional input encoder decoder model generates summary jointly encoding copying words type text source text prototype text guide content length summary model generate informative length controlled summary periments cnn daily mail dataset room dataset model outperformed previous models length controlled settings introduction neural summarization great progress recent years main approaches extractive tive extractive methods generate summaries selecting important sentences zhang et al zhou et al produce grammatically correct summaries exibility summarization cause extract sentences source text contrast abstractive summarization enables exible summarization expected generate uent readable summaries extractive models commonly abstractive summarization model pointer generator liu manning erates summary word word copying words source text types renewable energy solar wind touted solution world s growing energy crisis researcher come novel idea trump biological solar panel works clock harnessing electrons generated plants moss said create useful energy home university cambridge scientist revealed green source energy moss able generate power run clock shown said panels plant material power appliances homes technology help farmers grow crops electricity scarce reference summary university cambridge scientist revealed green source energy moss able generate power run clock said panels plant material power appliances homes tech help farmers grow crops electricity scarce outputs extracted prototype said panels plant material power abstractive summary panels plant material power appliances outputs extracted prototype university cambridge scientist vealed said panels plant material power ances homes technology help farmers grow crops scarce abstractive summary university cambridge scientist revealed green source energy said panels plant terial power appliances homes figure output examples model model extracts k important words colored red k blue k prototype source text generates abstractive summary based prototype source texts length generated summary controlled accordance length prototype text source text generating words pre dened cabulary set model generate accurate summary combining word level extraction generation idea controlling length mary neglected past recently pointed actually important aspect tive summarization liu luo zhu fan ier auli practical applications summary length controllable order t device displays ies controlling summary length kikuchi et al proposed length controllable model uses length beddings length embedding approach summary length encoded embedding represents remaining length decoding step initial bedding decoder represents desired length liu luo zhu proposed model uses desired length input initial state decoder previous models control length decoding module length embeddings length beddings add length information decoder consequently miss important information difcult account content cluded summary certain length constraints propose new length controllable abstractive rization guided prototype text idea use word level extractive module instead length dings control summary length figure compares previous length controllable models proposed yellow blocks modules responsible length control word level extractor controls tents included summary length straint given possible generate summary ing important contents model consists steps word level extractor predicts word level portance source text extracts important words according importance scores desired length extracted word sequence prototype summary prototype text second use prototype text additional input encoder decoder model length summary kept close prototype text summary generated ring prototype text figure shows examples generated model abstractive summaries similar extracted prototypes extractive module produces rough overview summary decoder module produces uent summary based tracted prototype idea inspired extractive abstractive rization extractive abstractive summarization rates extractive model abstractive encoder decoder model simple encoder decoder model model identies important contents generates ent summaries extractive abstractive model encoder decoder generates uent summaries separate extracts important contents ies shown separating problem nding important content problem generating uent maries improves accuracy summary gehrmann deng rush chen bansal model regarded extension models work previous length controllable models proposed model source summary source enc dec model summary enc dec model length embeddings desired length k prototype k words extractive model desired length k figure comparison previous length controllable els proposed model model controls summary length accordance length prototype text way rst extend extractive ule control summary length rst method controls summary length extractive module achieves high racy length controllability abstractive summarization contributions summarized follows propose new length controllable prototype guided abstractive summarization model called lpas controllable prototype guided abstractive tion model effectively guides abstractive marization summary prototype model trols summary length controlling number words prototype text model achieved state art rouge scores length controlled abstractive summarization settings cnn dm newsroom datasets task denition study denes length controllable abstractive rization pipelined tasks prototype extraction prototype guided abstractive summarization problem formulations task described task prototype extraction given source text x c xc l words x xc l desired summary length k model estimates importance scores p l extracts k important words x p pext pext xp xp k prototype text basis p ext desired summary length k set arbitrary value note original word order preserved x p x p bag words task prototype guided abstractive summarization given source text extracted prototype text x p model generates length controlled abstractive mary y yt length summary t trolled accordance prototype length k proposed model overview model consists modules prototype tor joint encoder summary decoder figure modules comprise task prototype guided tive summarization prototype extractor uses bert summary pointer generator dual encoder block source dual encoder block prototype decoder block source shared encoder block shared encoder block decoder block prototype positional encoding positional encoding glove source text glove prototype text positional encoding glove joint encoder sec summaization decoder sec prototype text k words sigmoid linear bert source text prototype extractor sec figure architecture proposed model joint encoder summary decoder use architecture vaswani et al prototype extractor prototype extractor tracts k important words source text joint encoder joint encoder encodes source text prototype text summary decoder summary decoder based pointer generator model generates abstractive summary output joint encoder prototype extractor model extracts prototype word level prototype extractor estimates importance score pext word xc l x c bert achieved sota classication tasks natural choice prototype extractor model uses bert task specic forward network bert tokenize source text bert ne tune bert model importance score pext l l pext l dened c l bert hidden state pre trained bert rdbert learnable parameters sigmoid function dbert dimension hidden state pre trained bert extract uent prototype word level importance dene new weighted tance score pextw incorporates sentence level tance score weight word level importance score l pextw l pext l pext sj pext sj nsj xl xlsj pext l nsj number words j th sentence sj x c model extracts k important words prototype source text basis pextw controls length summary accordance number words prototype text k l com google research joint encoder embedding layer layer projects hot vectors words xc size v dword dimensional l vector space pre trained weight matrix w e rdwordv glove pennington socher ning word embeddings mapped dmodel dimensional vectors fully connected layer mapped embeddings passed relu function layer adds positional encoding word embedding vaswani et al transformer encoder blocks encoder encodes embedded source prototype texts stack blocks vaswani et al model encodes texts encoder stack independently denote s rdmodelk s rdmodell ep outputs ec respectively transformer dual encoder blocks block calculates interactive alignment encoded source prototype texts specically encodes source totype texts performs multi head attention output encoder stack e ec s denote outputs dual encoder stack source text prototype text m c rdmodell m p rdmodelk respectively s ep summary decoder embedding layer decoder receives sequence words abstractive summary y generated auto regressive process decoding step t layer projects hot vectors words yt way embedding layer joint encoder transformer decoder blocks decoder uses stack decoder transformer blocks vaswani et al form multi head attention encoded representations prototype m p uses stack decoder blocks perform multi head attention source text m c rst stack rst stack rewrites prototype text second complements rewritten prototype original source information subsequent mask stacks nent step step manner test time output stacks m s rdmodelt copying mechanism pointer generator model copies words source prototype texts basis copy distributions efcient reuse copy distributions copy distributions source prototype words described follows p tk tl xk xl xc l p tl respectively rst attention heads block rst second stacks decoder tk c final vocabulary distribution nal vocabulary tribution described follows joint encoder summary decoder main loss encoder decoder cross entropy loss g s cc tl m c l t t cc t cp tkm p p cp t bv xl s xk bg lmain gen n t n t log x c x p add attention guide loss summary decoder loss designed guide estimated tion distribution reference attention w v bv w g rdmodelv bg rv learnable parameters training model trained end end manner totype extractor trained rst encoder coder trained generating training data x c rl xc l l label rl pairs xc prototype extractor supervised data prototype extractor created pseudo training data like gehrmann deng rush training data consists word xc l rl xc included summary construct paired data automatically rst extract oracle source sentences soracle mize rouge r score way hsu et al calculate word word alignment tween reference summary soracle dynamic programming algorithm consider word order finally label aligned words words including words oracle sentence joint encoder summary decoder create triple data x c x p y consisting source text gold prototype text target text training encoder decoder use k words terms pextw eq oracle sentences soracle gold l totype text extract prototype closer reference mary improve quality encoder decoder ing k decided reference summary length t obtain natural summary close desired length quantize length t discrete bins bin resents size range set size range study value nearest summary length t multiples selected k loss function prototype extractor use binary cross entropy loss extractor estimates importance score word eq binary classication task lext n l n l rl log pext l rl pext l n number training examples lsum attn log c t lproto attn log proto t n t n t n t n t proto t rst attention head block joint encoder stack prototype denotes absolute sition source text corresponding t th word sequence summary words overall loss tion model linear combination losses lgen lmain gen attn attn set experiments inference inference period use beam search ranking chen bansal nbeam mary candidates provided beam search nbeam size beam generate nbeam best maries summaries ranked number repeated n grams smaller better beam search ranking improve rouge score output eliminate candidates contain repetitions length controlled setting set value k sired length standard setting set average length reference summary validation data experiments datasets settings dataset cnn dm dataset hermann et al standard corpus news summarization maries bullet points articles shown spective websites following liu manning non anonymized version corpus truncated source documents tokens summaries tokens dataset includes training pairs validation pairs test pairs newsroom dataset grusky man artzi newsroom contains news sources different news sites pairs data training sampled pairs validation data number test pairs uate length controlled setting newsroom dataset randomly sampled samples test set length model avg lenemb lpas lc lenemb lpas lc lenemb lpas lc lenemb lpas lc lenemb lpas lc lenemb lpas r l table rouge scores abstractive summarization models different lengths cnn dm dataset words avg indicates average rouge score ve different lengths luo zhu model congurations congurations datasets extractor pre trained bertlarge model devlin et al ne tuned bert epochs default settings encoder coder pre trained dimensional glove embeddings encoder decoder transformer blocks number heads number dimensions ffn dmodel set adam optimizer kingma ba scheduled learning rate vaswani et al set size input cabulary output vocabulary evaluation metrics rouge scores including rouge l r l evaluation metrics lin toolkit culating rouge results model improve rouge score length controlled setting types controllable models baselines rst based length controllable model lc uses desired length input initial state cnn based coder liu luo zhu second lenemb embeds remaining length adds decoder step kikuchi et al previous results applying lenemb cnn dm dataset mented transformer based encoder decoder model specically simply added embeddings ing length word embeddings decoding step com t g n e l t u t u o desired length figure results length controlled setting cnn dm rouge l recall precision f scores different lengths left output length distribution right table shows model achieved high rouge scores different lengths outperformed previous length controllable models cases model points accurate average lenemb model selected important words source text accordance desired length tive keeping important information controlled setting figure shows precision recall f score rouge different lengths model tained high f score average length words indicates select important information generate stable results different lengths model generate summary desired length figure shows relationship sired length output length axis indicates desired length y axis indicates average length standard deviation length controlled output mary results model properly controls summary length controllable nature comes training procedure training encoder decoder set number words k prototype text according length reference summary model learns generate summary similar length prototype text good quality prototype text uate quality prototype evaluated rouge scores extracted prototype text table shows sults table lpas ext sents means sentences extracted pext interestingly sj scores lpas ext k words higher sentence level tive models indicates word level lpas ext fective nding important words important phrases table lpas improved rouge l score lpas ext indicates joint encoder mary decoder generate uent summaries help prototype text abstractive model improve quality prototype improved evaluated model following settings order analyze relationship quality abstractive summary prototype gold length setting gave gold length k prototype extractor gold lpas ext sents k words r l table rouge scores prototype extractor lpas ext cnn dm deng rush et al wei zhou average length gold length gold sentences gold length r l table rouge scores abstractive summarization models gold settings cnn dm dataset tences gold length setting gave gold sentences soracle gold length table shows results results indicate selecting correct number words prototype improves rouge scores study simply selected average length ing prototype examples standard setting improvement adaptively lect number words prototype source text rouge score largely improved gold sentence gold length settings indicates quality generated summary signicantly prove increasing accuracy extractive model model effective datasets verify effectiveness model summary styles evaluated large varied news summary dataset newsroom table figure results length controlled setting newsroom model achieved higher rouge scores lenemb figure f value rouge score highest words erage word number words figure shows model acquired length control ity dataset styles model perform standard ting table shows model achieved rouge scores comparable previous models consider length constraint cnn dm dataset note current state art models use pre trained decoder models encoder decoder model prototype extractor pre trained examined results generating summary prototype lpas source source length model avg lenemb lpas lenemb lpas lenemb lpas lenemb lpas lenemb lpas lenemb lpas r l table rouge scores abstractive summarization models different lengths newsroom dataset t g n e l t u t u o desired length figure results length controlled setting room rouge l recall precision f scores ferent lengths left output length distribution right lpas prototype prototype turned accuracy source model source prototype simultaneously higher accuracy results indicate prototype extraction joint encoder effectively incorporated source text prototype information contributed improving accuracy results newsroom dataset dard settings shown table consider differences summary length news domains evaluated model average length domain level average length denoted domain length settings results cate model signicantly higher rouge scores compared ofcial baselines outperformed baseline lpas prototype indicate model effective datasets containing text ous styles found considering domain length positive effects rouge scores dicates model easily reect differences mary length styles related work discussion length control summarization kikuchi et al rst propose length embedding controlled abstractive summarization fan grangier auli length embeddings beginning decoder module length control liu luo zhu proposed cnn based length controllable marization model uses desired length model pre trained encoder decoder model pointer pointer generator key information guide unied sentence exconsumm etads lpas prototype source pre trained encoder decoder model r l table rouge scores abstractive tion models cnn dm liu manning et al et al bansal deng rush et al et al et al et al et al et al lpas prototype denotes simple transformer based generator model prototype tor joint encoder lpas source denotes model generates summary prototype text pointer generator lpas k average length k domain length lpas prototype r l table rouge scores proposed models room dataset naaman artzi initial state decoder takase okazaki introduced positional encoding represents remaining length decoder step based encoder decoder model equivalent model lenemb implemented previous models use length embeddings controlling length coding module use prototype extractor controlling summary length include important formation summary neural extractive abstractive summarization hsu et al gehrmann deng rush et al incorporated word level extractive model pointer generator model models weight copy probability source text extractive model guide pointer generator model copy portant words li et al proposed keyword guided abstractive summarization model chen bansal proposed sentence extraction writing model trains end end manner reinforcement ing cao et al proposed search rewrite model mendes et al proposed combination level extraction compression idea models word level weighting entire source text sentence level writing hand model guides summarization length controllable type text prototype extractor joint encoder utilizing extractive results control length mary new idea large scale pre trained language model bert vlin et al new pre trained language model uses bidirectional encoder representations bert performed natural language understanding tasks glue benchmarks wang et al natural language inference williams gia bowman liu bert sentence level extractive summarization model zhang wei zhou trained new pre trained model siders document level information sentence level tive summarization bert word level totype extractor veried effectiveness word level extractive module researchers published pre trained encoder decoder models cently wang et al lewis et al raffel et al wang et al pre trained transformer based pointer generator model lewis et al pre trained normal transformer based encoder decoder model large unlabeled data achieved state art results dong et al extended bert structure handle sequence sequence tasks reinforcement learning summarization ment learning rl key summarization technique rl optimize non differential metrics ple non differential networks narayan cohen lapata dong et al rl extractive marization abstractive summarization paulus xiong socher rl mitigate exposure bias abstractive summarization chen bansal rl combine sentence extraction pointer generator els model achieved high rouge scores rl future incorporate rl models improvement conclusion proposed new length controllable abstractive rization model model consists word level prototype extractor prototype guided abstractive summarization model prototype extractor identies important source text length constraint stractive model guided prototype text acteristic enabled achieve high rouge score dard summarization tasks prototype tor ensures summary desired length periments cnn dm dataset newsroom dataset model outperformed previous els standard length controlled settings future mendes narayan s miranda s marinho z martins f t cohen s b jointly extracting pressing documents summary state representations naacl narayan s cohen s b lapata m ranking sentences extractive summarization reinforcement learning naacl paulus r xiong c socher r deep reinforced model abstractive summarization corr pennington j socher r manning c d glove global vectors word representation emnlp raffel c shazeer n roberts lee k narang s matena m zhou y li w liu p j ing limits transfer learning unied text text transformer arxiv e prints liu p j manning c d point summarization pointer generator networks acl takase s okazaki n positional encoding control output sequence length naacl vaswani shazeer n parmar n uszkoreit j jones l gomez n kaiser l u polosukhin attention need nips wang singh michael j hill f levy o bowman s r glue multi task benchmark analysis platform natural language understanding wang l zhao w jia r li s liu j ing based sequence sequence pre training text ation emnlp appear williams nangia n bowman s r broad coverage challenge corpus sentence understanding inference naacl hlt y jia w liu t yang w improving stractive document summarization salient information modeling acl zhang x lapata m wei f zhou m neural latent extractive document summarization emnlp association computational linguistics zhang x wei f zhou m hibert document level pre training hierarchical bidirectional transformers document summarization acl zhou q yang n wei f huang s zhou m zhao t neural document summarization jointly ing score select sentences acl like incorporate pre trained language model abstractive model build higher quality tion model references cao z li w li s wei f retrieve rerank rewrite soft template based neural summarization acl chen y bansal m fast abstractive rization reinforce selected sentence rewriting acl devlin j chang m lee k toutanova k bert pre training deep bidirectional transformers guage understanding corr dong y shen y crawford e van hoof h cheung j c k banditsum extractive summarization contextual bandit emnlp dong l yang n wang w wei f liu x wang y gao j zhou m hon h unied language model pre training natural language understanding generation advances neural information processing systems fan grangier d auli m controllable abstractive summarization gehrmann s deng y rush abstractive summarization emnlp grusky m naaman m artzi y newsroom dataset million summaries diverse extractive strategies acl hermann k m kocisky t grefenstette e espeholt l kay w suleyman m blunsom p teaching machines read comprehend nips hsu w lin c lee m min k tang j sun m unied model extractive abstractive summarization inconsistency loss acl kikuchi y neubig g sasano r takamura h okumura m controlling output length neural encoder decoders emnlp kingma d p ba j adam method stochastic optimization iclr lewis m liu y goyal n ghazvininejad m hamed levy o stoyanov v zettlemoyer l bart denoising sequence sequence pre training natural language generation translation sion arxiv e prints li c xu w li s gao s guiding generation abstractive text summarization based key information guide network acl lin c rouge package automatic evaluation summaries acl liu y luo z zhu k controlling length abstractive summarization convolutional neural work emnlp liu y fine tune bert extractive summarization corr
