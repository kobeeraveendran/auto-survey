conditional neural generation sub aspect functions extractive news summarization zhengyuan liu shi nancy chen institute infocomm research singapore liu zhengyuan shi star edu abstract progress text marization fueled neural architectures ing large scale training corpora news domain neural models easily leveraging position related features prevalence inverted pyramid ing style addition unmet need generate variety summaries paper propose ent users ral framework exibly control mary generation introducing set aspect functions importance diversity sition sub aspect functions lated set control codes decide sub aspect focus summary eration demonstrate extracted maries minimal position bias rable generated standard els advantage position preference news summaries generated focus diversity preferred human raters results suggest exible neural summarization work providing control options desirable tailoring different user ences useful tical articulate preferences different applications priori introduction text summarization targets automatically erate shorter version source content retaining important information straightforward effective method extractive summarization creates summary selecting subsequently concatenating salient semantic units document recently neural approaches trained end end ner achieved favorable improvements ious large scale benchmarks nallapati narayan liu lapata despite renewed interest avid development extractive summarization standing unresolved challenges major lem position bias especially common news domain majority research summarization studied news articles sentences appearing earlier tend tant summarization tasks hong nenkova preference reected reference summaries public datasets tendency common classic book writing style inverted pyramid lan news articles presented ous ways journalism writing styles include anecdotal lead question answer format chronological organization stovall fore salient information scattered entire article instead trated rst sentences depending chosen writing style journalist inverted pyramid style widespread news articles kryscinski neural models easily overt position related tures extractive summarization tasks data driven learning setup tags tures correlate output result models select sentences beginning document best didates regardless considering context resulting sub optimal models fancy neural architectures generalize domains kedzie additionally according nenkova content selection deterministic process salton marcu mani ferent people choose different sentences include summary person select different sentences different times rath observations lead concerns advisability single human model observations suggest individuals differ considers key information different circumstances reects need generate application specic summaries ing establishing appropriate expectations knowledge targeted readers prior model development ground truth construction publicly available datasets provide associated reference summary document explicit instructions targeted tions user preferences ground truth construction summarization constrained assignment kryscinski challenging end end models generate alternative summaries proper anchoring reference summaries making harder models reach potential work propose exible neural marization framework able provide explicit control options automatically erating summaries figure rization regarded combination aspect functions information layout bonell goldstein lin bilmes follow spirit sub aspect theory adopt control codes sub aspects condition summary generation advantages fold provides systematic approach investigate analyze minimize position bias extractive news summarization neural modeling previous work like jung kryscinski focus lyzing degree prevalence position bias work step propose research methodology direction disentangle position bias important non redundant summary content text summarization needs domain application specic cult articulate priori user preferences requiring potential iterations adapt rene human ground truth construction summarization time consuming intensive exible summary eration framework minimize manual labor generate useful summaries efciently ideal set sub aspect control codes characterize different aspects summarization comprehensive manner time delineate relatively clear boundary minimize set size higgins achieve adopt aspects dened jung importance diversity position assess terization capability cnn daily mail news figure proposed conditional generation framework exploiting sub aspect functions hermann quantitative yses unsupervised clustering utilize trol codes based sub aspect functions label training data implement tional generation approach neural selector model empirical results given different control codes model generate output maries alternative styles maintaining formance comparable state art model modulation semantic sub aspects reduce systemic bias learned news corpus prove potential generality domains relation work text summarization benchmark datasets focus news domain nyt haus cnn daily mail hermann human written summaries abstractive extractive paradigms gehrmann improve mance extractive summarization non neural approaches explore linguistic cal features lexical characteristics kupiec latent topic information ying lang chang chien discourse analysis rao liu chen based modeling erkan radev mihalcea tarau contrast neural approaches learn features data driven manner based recurrent neural networks summarunner earliest neural models nallapati development extractive rization reinforcement learning narayan jointly learning scoring ranking zhou deep tual language models liu lapata despite development recent neural proaches challenges bias resulting prevalent inverted pyramid journalism writing style lin hovy system bias jung stemming sition preference ground truth date analysis work ize position bias problem ramications inability generalize corpora domains kedzie kryscinski attempted resolve long standing problem position bias ral approaches work rst stab introduce sub aspect functions conditional extractive summarization explore bility disentangling sub aspects commonly characterize summarization position choosing sentences position importance choosing relevant repeating content document diversity ensuring minimal redundancy summary sentences jung summary generation process particular use sub aspects control codes conditional ing best knowledge rst work applying auxiliary conditional codes extractive summary generation nlp tasks topic information conditional signals applied dialogue sponse generation xing training large scale language models keskar sentiment polarity text style transfer john image style transfer codes specifying color texture train conditional generative models mirza osindero higgins extractive oracle construction similarity metric semantic afnity lexical overlap benchmark corpora widely adopted cnn daily mail hermann golden abstractive summaries written humans corresponding extractive oracle summaries convert human written abstracts extractive oracle summaries previous work rouge score lin counts contiguous gram overlap similarity teria rank select sentences source content rouge scores conduct figure cumulative position distribution oracles built rogue blue bertscore orange axis ratio article length axis tive percentage summary sentences cal matching word overlapping algorithms salient sentences source content phrased human editors overlooked rouge scores low sentences high count common words inated rouge score kryscinski tackle drawback rouge propose apply semantic similarity metric bertscore zhang rank candidate sentences bertscore performed better rouge bleu sentence level semantic similarity ment zhang bertscore includes recall measures reference candidate sequences suitable metric distance based similarity measures wieting reimers gurevych rization related tasks rical relationship reference generated text oracle construction evaluation build oracles semantic similarity rst segment sentences source documents written gold convert text semantically rich distributed vector space sentence gold summary use bertscore calculate semantic similarity candidates source content sentence highest recall score chosen candidates score lower excluded streamline selection process observed oracle summaries ated semantic similarity differ chosen gram overlap positional butions schemes different early sentence bias signicant bertscore scheme figure evaluate fectiveness oracle construction approach details corpus appendix score score rouge oracle bertscore oracle similarity evaluation gold summaries rouge candidates bertscore candidates paradigm evaluation entity event questions gold summaries rouge candidates bertscore candidates extended questions gold summaries rouge candidates bertscore candidates score accuracy table rouge human evaluation scores cle summaries built bertscore rouge conducted assessments rouge scores computed gold summaries table shows oracle summaries derived bertscore comparable slightly lower rouge unexpected given bertscore mismatched rouge metric conducted human evaluations ranked candidate summary pairs news samples based similarity human written gold summaries narayan guistic analyzers asked consider pects informativeness coherence radev evaluation score represents hood higher ranking normalized adopted question answering paradigm liu lapata evaluate selected ples sentence gold summary tions constructed based key information events named entities questions answer obtained comprehending summary included human tators asked answer questions given oracle summary extractive summaries structed bertscore signicantly higher human evaluations table sub aspect control codes sub aspect features news summarization conditional generation uses control codes auxiliary vector adjust pre dened style tures classic examples include sentiment polarity style transfer john physical tributes color image generation higgins figure sample level distribution sub aspect tions bertscore oracle values percentage categorized samples adds cnn daily mail training set remaining belong sub aspects summarization lenging pinpoint intuitive dened features writing style vary according genre topic editor preference work adopt position importance diversity set sub function features terize extractive news summarization jung considerations include inverted mid writing style common news articles making layout position salient sub aspect summarization importance sub aspect cates assumption repeatedly occurring tent source document contains tant information diversity sub aspect suggests selected salient sentences maximize semantic volume distributed semantic space lin bilmes yogatama summary level quantitative analysis apply methods evaluate ity effectiveness sub aspects choose extractive news summarization duct quantitative analysis cnn daily mail corpus based assumption writing style variability summaries characterized different combinations sub aspects lin bilmes source document converted tences vector representations pre trained contextual language model bert devlin sentence averaged hidden states tokens sentence embedding ilar jung obtain subset tences correspond importance sub aspect com google research bert figure autoencoder adversarial training egy unsupervised clustering sentence level bution sub aspect functions figure sentence level clustering result labeled sub aspect features axis cluster index axis proportion sub aspect features cluster adopted nearest method calculates averaged pearson correlation tence rest source sentence vectors collected rst candidates highest scores equals oracle summary length tain subset corresponds diversity sub aspect quickhull algorithm barber vertices regarded sentences maximize volume size projected semantic space subset corresponds tion sub aspect rst sentences source document chosen sets sub aspects quantied distribution different sub aspects tive oracle constructed section oracle summary mapped importance aspect sentences summary subset importance sub aspect oracle summaries shorter tences occupying oracle sentence determine sub aspect mapped note mapping summary mapped sub aspect figure displays distribution sub aspect functions oracle summaries position occupies largest area visualization shows sub aspects represent distinct linguistic attributes overlap sentence level unsupervised analysis according mapping algorithm previous section summaries mapped aspect nding motivated investigate distribution sub aspect functions sentence level conducted unsupervised clustering qhull assuming samples cluster similar represented dominant feature shown figure use autoencoder architecture adversarial training model correlation document summary tences semantic space encoding nent receives source document representation summary sentence representation input compresses latent feature vector latent vector document vector nated fed decoding component struct sentence vector obtain compact effective latent vector representing tion source summary adopt adversarial training strategy john specically adversarial decoder include aims reconstruct sentence vector directly latent vector training process update parameters autoencoder adversarial penalty appendix implementation details training coder conduct means clustering latent representation vectors analyze clustering output sentence level labels sub aspect functions dened section shown figure sentences position aspect distributed relatively equally cluster importance diversity dominate respectively different clusters based tering results assign sub aspect function dominant unmapped sentences cluster instance diversity assigned unmapped sentences cluster importance assigned cluster reduce unmapped sentences reduce unmapped maries criteria section conditional neural generation section construct set control codes specify sub aspect features described section label oracle summaries structed section propose neural extractive model conditional learning egy exible summary generation control code specication scheme control codes constructed form importance diversity position specify aspect features exibly indicate state sub aspect switching corresponding value enabling disentanglement sub aspect function instance control code tell model focus importance sentence scoring selection focus diversity position switching position code help model obtain minimal position bias note mean rst sentences selected overlap position importance diversity shown figure control codes specication scheme pect code design provide model sub aspect conditions generating summaries neural extractive selector given document containing number tences content selector assigns score sentence indicating probability included summary neural model trained extractive lector text summarization tasks contextually modeling source content implemented adapted neural extractive selector sequence labeling manner kedzie shown figure model consists components contextual encoding component selection modeling ponent output component bert contextual encoding component obtain feature rich sentence level representations training process concatenated sentence embeddings pre calculated control code vector fed layer models contextual hidden states conditional signals linear layer sigmoid function receives hidden states produces scores segment figure overview neural selector architecture figure position distribution generated summaries strong baseline model bertext tional summarization model position code set implementations axis position ratio axis sentence level proportion probability extractive selection architecture straightforward shown competitive combined state art contextual representation liu lapata setting sentences processed word tokenizer dings initialized dimension uncased bert devlin xed training lengthy source documents truncated selection modeling nent applied multi layer directional lstm schuster paliwal transformer network vaswani cally shown layer lstm performed best appendix implementation details testing sentences selection probability extracted output mary trigram blocking strategy paulus reduce redundancy experimental results analysis quantitative analysis test possibility reducing position bias conditioning summary generation switched position code compared position figure sub aspect mapping generated summary importance focus code left panel sentence summary belongs importance aspect right panel sentences summary long importance sub aspect contour lines denote number generated summaries figure sub aspect mapping generated summary diversity focus code left panel tence summary belongs diversity sub aspect right panel sentences summary belong diversity sub aspect contour lines denote number generated summaries selected sentences summaries generated model state art baseline bertext based tuning bert liu lapata results bertext chance choosing rst sentences ument proposed framework stronger tendency choose sentences rst sentences position distribution attened compared bertext respectively switched importance sity codes categorized generated maries subset sub aspect function section shown figure maries subset importance diversity weigh higher corresponding control codes results demonstrate sibility proposed framework erate output summaries alternative styles given different control codes automatic evaluation calculated rouge scores generated summaries control codes compared bertscore oracle section baseline selecting sentences summary competitive extractive els summarunner nallapati formerext liu lapata table observe summary erated code similar dynamically learn positional features limited rst sentences isolating diversity importance features ing importance sub aspect leads worst performance performance improved considering sub aspects focusing diversity sub aspect code generate results comparable strong baselines oracle bertscore summarunner transformerext bertext code code code code code code code code table rouge score evaluation trol codes form importance diversity tion denotes results corresponding paper human evaluation addition automatic evaluation human evaluation conducted experienced tic analysts best worst scaling louviere analysts given news articles randomly chosen cnn daily mail test set corresponding summaries tems oracle bertext codes disabling sub aspect position code enabling tion asked decide best worst summaries document terms informativeness coherence radev narayan collected judgments human evaluators comparison evaluator documents randomized differently order summaries ument shufed differently uator score model calculated percentage times labeled best minus percentage times labeled worst ranging differences come pairs sum evaluation scores summary types adds zero observed oracle bertext code code code code evaluation score recall oracle baseline bertext code code code table human evaluation samples baselines model control codes form portance diversity position table inference scores ami corpus lines model control codes form importance diversity position denotes results kedzie bertext code code code table inference scores samples shufed tences control codes form importance diversity position values brackets absolute crease scores original order samples summaries diversity code favored importance tion produce better results table ndings resonate matic evaluation suggesting ation metric lexical overlap rouge human judgement diversity sub aspect plays salient role importance tomatic human evaluations rizing semantic related sub aspect condition codes achieves reasonable summaries examples appendix generated summaries position biased preserve key information source content inference samples shufed sentences assess decoupling aspect signals positional information learned model conducted experiment ples shufed sentences similar document shufe kedzie setting introduce shufe process model ference phase shufed sentences test samples section applied trained model generate predicted summaries shown table outputs position sub aspect bertext suffer icant drop performance shufe sentence order comparison far decrease shufed order ples diversity importance control code demonstrating latent features semantic related sub aspects rely tion information suggesting applying semantic sub aspects training process reduce temic bias learned model corpus strong position preference inference ami meeting corpus conducted inference experiment position biased corpus ami corpus letta collection meetings tated text transcriptions human written summaries different news summarization meeting summaries abstractive extracted keywords unlike previous comparison work kedzie train model scratch ami training set instead applied pre trained model tuning section summarization ence test set meeting transcript summary pairs table shows summaries importance code obtain highest scores better best reported model kedzie surprisingly summaries position code perform position bias ami ndings suggest models semantic related trol codes generalize domains conclusion proposed neural framework conditional extractive news summarization particular aspect functions importance diversity sition condition summary generation framework enables reduce position bias long standing problem news summarization generated summaries preserving comparable performance standard models results suggest conditional ing summaries efciently tailored different user preferences application needs acknowledgments research supported funding institute infocomm research ares singapore thank bin chen shen tat goh ridong jiang jung jae kim ping ong zeng zeng sightful discussions thank anonymous reviewers precious feedback help prove extend piece work references bradford barber david dobkin david dobkin hannu huhdanpaa quickhull rithm convex hulls acm transactions ematical software toms jaime carbonell jade goldstein use mmr diversity based reranking reordering ments producing summaries proceedings annual international acm sigir ence research development information retrieval sigir pages new york usa acm jean carletta simone ashby sebastien bourban mike flynn mael guillemot thomas hain jaroslav kadlec vasilis karaiskos wessel kraaij melissa kronenthal ami meeting corpus pre announcement international workshop machine learning multimodal interaction pages springer jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language proceedings conference standing north american chapter association computational linguistics human language technologies volume long short papers pages minneapolis minnesota ation computational linguistics gunes erkan dragomir radev lexrank graph based lexical centrality salience text summarization journal articial intelligence search sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference pirical methods natural language processing pages brussels belgium association computational linguistics karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend advances neural information processing systems pages irina higgins loic matthey arka pal christopher burgess xavier glorot matthew botvinick shakir mohamed alexander lerchner beta vae learning basic visual concepts constrained variational framework iclr tsutomu hirao masaaki nishino yasuhisa yoshida jun suzuki norihito yasuda masaaki nagata summarizing document trimming ieee acm trans audio speech discourse tree lang proc kai hong ani nenkova improving estimation word importance news proceedings document summarization conference european chapter sociation computational linguistics pages gothenburg sweden association tational linguistics vineet john lili mou hareesh bahuleyan olga vechtomova disentangled representation learning non parallel text style transfer ceedings annual meeting tion computational linguistics pages florence italy association computational guistics taehee jung dongyeop kang lucas mentch uard hovy earlier better aspect analysis corpus system biases marization proceedings conference empirical methods natural language ing international joint conference natural language processing emnlp ijcnlp pages hong kong china association computational linguistics chris kedzie kathleen mckeown hal daume iii content selection deep learning models proceedings summarization ference empirical methods natural language processing pages brussels belgium association computational linguistics nitish shirish keskar bryan mccann lav varshney caiming xiong richard socher ctrl conditional transformer language model lable generation arxiv preprint diederik kingma jimmy adam method stochastic optimization proceedings international conference learning representations wojciech kryscinski nitish shirish keskar bryan cann caiming xiong richard socher neural text summarization critical evaluation proceedings conference empirical methods natural language processing international joint conference natural guage processing emnlp ijcnlp pages hong kong china association tional linguistics julian kupiec jan pedersen francine chen trainable document summarizer proceedings annual international acm sigir ference research development tion retrieval sigir page new york usa association computing machinery chin yew lin rouge package text matic evaluation summaries rization branches proceedings workshop pages barcelona spain tion computational linguistics chin yew lin eduard hovy identifying ics position fifth conference applied ral language processing pages ton usa association computational guistics hui lin jeff bilmes learning mixtures submodular shells application document summarization eighth conference uncertainty articial telligence pages arlington ginia united states auai press proceedings yang liu mirella lapata text proceedings tion pretrained encoders conference empirical methods ural language processing international joint conference natural language processing emnlp ijcnlp pages hong kong china association computational linguistics zhengyuan liu nancy chen exploiting discourse level segmentation extractive proceedings workshop rization new frontiers summarization pages hong kong china association computational linguistics jordan louviere terry flynn anthony fred john marley best worst scaling ory methods applications cambridge sity press mani overview shop summarization evaluation acl summarization daniel marcu discourse structures text summaries intelligent scalable text tion rada mihalcea paul tarau textrank bringing order text proceedings conference empirical methods natural guage processing pages barcelona spain association computational linguistics mehdi mirza simon osindero tional generative adversarial nets arxiv preprint ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments thirty aaai conference articial intelligence shashi narayan shay cohen mirella lapata details summary topic aware convolutional neural networks proceedings treme summarization conference empirical methods natural guage processing pages brussels gium association computational linguistics shashi narayan shay cohen mirella lapata ranking sentences extractive tion reinforcement learning proceedings conference north american ter association computational linguistics human language technologies volume long pers pages new orleans louisiana association computational linguistics ani nenkova rebecca passonneau kathleen mckeown pyramid method ing human content selection variation tion evaluation acm trans speech lang process romain paulus caiming xiong richard socher deep reinforced model abstractive marization proceedings international conference learning representations iclr dragomir radev eduard hovy kathleen introduction special issue marization computational linguistics rath resnick savage tion abstracts selection sentences sentence selection men machines american documentation nils reimers iryna gurevych bert sentence embeddings siamese networks proceedings conference empirical methods natural language processing international joint conference ral language processing emnlp ijcnlp pages hong kong china association computational linguistics gerard salton amit singhal mandar mitra chris buckley automatic text structuring marization information processing management evan sandhaus new york times annotated corpus linguistic data consortium philadelphia christopher scanlan reporting writing sics century oxford university press qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural ument summarization jointly learning score select sentences proceedings nual meeting association computational linguistics volume long papers pages melbourne australia association tational linguistics mike schuster kuldip paliwal tional recurrent neural networks ieee transactions signal processing abigail peter liu christopher manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers pages nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov dropout simple way prevent neural networks overtting journal machine learning research james glen stovall writing mass media prentice hall ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention advances neural information need cessing systems pages john wieting kevin gimpel graham neubig lor berg kirkpatrick simple effective paraphrastic similarity parallel translations proceedings annual meeting ciation computational linguistics pages florence italy association computational linguistics yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey google neural machine translation system bridging gap arxiv preprint man machine translation chen xing wei jie liu yalou huang ming zhou wei ying topic aware thirty aaai neural response generation conference articial intelligence ying lang chang chien latent let learning document summarization ieee international conference acoustics speech signal processing pages dani yogatama fei liu noah smith extractive summarization maximizing semantic volume proceedings conference empirical methods natural language processing pages lisbon portugal association computational linguistics tianyi zhang varsha kishore felix kilian weinberger yoav artzi bertscore proceedings uating text generation bert eighth international conference learning representations iclr details corpus cnn daily mail corpus hermann contains english news articles ated human written summaries ular large scale benchmark news summarization pre processed dataset training pairs validation pairs test pairs replace entities anonymised ers sentence segmentation documents summaries liu lapata obtain word embedding tion tokenized sentences sub word algorithm bert devlin implementation details unsupervised analysis model section provide implementation details model section autoencoder adversarial training strategy encoding component given document sentation vector vdoc sentence representation vector vsen input encoding component linear layers compresses lower dimension latent feature vector vlatent setting hidden dimensions vdoc vsen vlatent respectively henc hidden vector dened henc vsen vlatent trainable parameters layer denotes concatenation operation decoding component given latent feature resentation vector vlatent document tation vdoc input decoding component linear layers targeted reconstruct sentence representation vsen hdec vlatent sdec whdec hdec sdec hidden state struction output respectively adversarial decoding component given tent feature representation vector vlatent input adversarial decoding component linear layer targeted reconstruct sentence sentation vsen sadv wvlatent sadv reconstruction output training procedure setting training batch step parameter date update adversarial decoder mean square error mse loss sadv vsen lossadv vsen update autoencoder mse loss sadv vsen combined penalty adversarial mse reduce unnecessary mation leaked vsen encoding nent adversarial loss dened lossadv vsen vsen training setting adam optimizer kingma learning rate weight decay batch size set drop vastava rate applied linear layer bert parameters xed training trainable parameter size tesla memory training pytorch tional selector model section provide implementation details model section neural sentence lector extractive summarization bert encoding component given document containing number sentences input encoding component produces sentence representation list tokens use average token level hidden states layer bert wbertrep selection modeling component given cic control code vctrl sentence vectors com pytorch pytorch lantent feature vector dened implementation details neural input component use layer directional lstm model contextual information sub aspect conditioning ward backward hidden states concatenated output orward lstmf vctrl ubackward vctrl orward ubackward input embedding dimension hidden size control code dimension spectively denotes concatenation operation output component linear layer duce output sentence probability included generated summary training setting binary cross entropy bce measure loss prediction ground truth time steps loss adam optimizer kingma learning rate weight decay batch size set drop vastava rate applied modeling layer output linear layer bert parameters xed training lengthy documents truncated trainable rameter size excluding pre trained language model trained model epochs hours tesla gpu reported models selected based best validation performance according ection point loss value generated summary examples summary examples golden groundtruth oracle baseline bertext proposed conditional neural summarizer news articles figure figure news article examples oracle summaries underlined summaries baseline model highlighted blue summaries model specied control codes orange overlaps purple
