n a j v c s c v v i x r a how good is a video summary a new benchmarking dataset and evaluation framework towards realistic video summarization vishal suraj anshul rishabh and ganesh of computer science indian institute of technology bombay of computer science university of texas at dallas abstract automatic video summarization has attracted a lot of interest however it is still an unsolved problem due to several challenges we take steps towards making automatic video tion more realistic by addressing the following challenges the currently available datasets either have very short videos or have few long videos of only a particular type we introduce a new benchmarking video dataset called visiocity video summarization based on continuity intent and diversity which comprises of longer videos across six dierent categories with dense concept annotations capable of supporting dierent avors of video summarization and other vision problems secondly for long videos human reference summaries necessary for supervised video summarization techniques are dicult to obtain we explore strategies to automatically generate multiple reference summaries from indirect ground truth present in visiocity we show that these summaries are at par with human summaries we also present a study of ent desired characteristics of a good summary and demonstrate how especially in long videos it is quite possible and frequent to have two good summaries with dierent characteristics thus we argue that evaluating a summary against one or more human summaries and using a single measure has its shortcomings we propose an evaluation framework for better quantitative assessment of summary quality which is closer to human judgment lastly we present insights into how a model can be enhanced to yield better summaries sepcically when multiple diverse ground truth summaries can exist learning from them individually and using a combination of loss functions measuring dierent characteristics is better than learning from a single combined oracle ground truth summary using a single loss function we demonstrate the eectiveness of doing so as compared to some of the representative state of the art techniques tested on visiocity we release visiocity as a benchmarking dataset and invite researchers to test the eectiveness of their video summarization algorithms on visiocity introduction and motivation the unprecedented rise in the amount of video data has also made it dicult to consume them this has given rise to the need for automatic video summarization techniques which aim at producing much shorter videos without signicantly compromising on the key information contained in them consequently there has been a lot of work pushing the state of the art for newer algorithms and model architectures and datasets however the literature also talks of a few fundamental challenges that need to be addressed before we have a more realistic video summarization that works in practice in this work we take steps towards addressing the following challenges dataset for a true comparison between dierent techniques a benchmark dataset is critical almost all recent techniques have reported their results on tvsum and summe which have emerged as benchmarking datasets of sorts however since the average video length in these datasets is of the order of only minutes they are far from being eective in real world settings characterized by long videos while there have been several attempts at creating datasets for video summarization they either a have very short videos or they have very few long videos and often of only a particular type a large dataset with a lot of dierent types of full length videos with rich annotations to be able to support dierent techniques was one of the recommendations in is still not a reality and is clearly a need of the hour we introduce visiocity to address this need visiocity is a diverse collection of long videos spanning across six dierent categories with dense concept annotations furthermore dierent avors of video summarization for example query focused video summarization are often treated dierently and on dierent datasets with its rich annotations visiocity can lend itself well to other avors of video summarization and also other computer vision video analysis tasks like captioning or action recognition also since the videos span across dierent well dened domains visiocity is suitable for more in depth domain specic studies on video summarization reference summaries for supervised learning supervised techniques tend to work better than unsupervised techniques because of learning directly from human summaries video summaries are highly context dependent depends on the purpose behind getting a video summary subjective even for the same purpose preferences of two persons do nt match and depends on high level semantics of the video two visually dierent scenes could capture the same semantics or visually similar looking scenes could capture dierent semantics as an example of context one may want to summarize a surveillance video either to see a gist of what all happened or to quickly spot any abnormal activity as an example of personal preferences or subjectivity while summarizing a friends video a popular tv series two users may have dierent opinion on what is important or interesting similar example for higher level semantics is that closeup of a player in soccer can be considered important if it is immediately followed by a goal while not so important when it occurs elsewhere even though both look visually the same thus there is no single right answer and two human summaries could be quite dierent in their selections in a race to achieve better performance most state of the art techniques are based on deep architectures and are thus data hungry the larger the dataset and more the number of reference summaries to learn from the better unfortunately for long videos getting human summaries is very time consuming it becomes increasingly expensive and beyond a point infeasible to get these reference summaries from humans also this is not scalable to experiments where reference summaries of dierent lengths are desired to alleviate this problem in this work we explore strategies to automatically generate ground truth reference summaries which can be used to train a model further most supervised learning approaches are trained using a combined ground truth mary either in form of combined scores from multiple ground truth summaries or scores or in form a set of ground truth selections as in dpplstm however combining them into one misses out on the separate avors captured by each of them combining many into one set of scores also runs the risk of giving more emphasis to importance over and above other desirable characteristics of a summary like continuity diversity this is also noted by where they argue that supervised learning approaches which rely on the use of a combined truth summary can not fully explore the learning potential of such architectures the necessity to deal with dierent kind of summaries in dierent ways was also observed by use this argument to advocate the use of unsupervised approaches we leverage visiocity to demonstrate that better results can be achieved when a supervised model learns from individual ground truth summaries using multiple loss functions each measuring deviation from dierent desired characteristics of summaries evaluation a video summary is typically evaluated by comparing it against human summaries for example using score dened as harmonic mean of precision ratio of temporal overlap between candidate and reference summary to duration of summary and recall ratio of temporal overlap between candidate and reference summary to video duration to accommodate multiple human summaries either average or max is reported however a good candidate may get a low score just because it was not fortunate to have a matching human summary a likely scenario in case of long videos furthermore score has some limitations due to the segmentation used as a post processing step in typical video summarization pipeline even random summaries can get good scores also is not designed to measure aspects like continuity and diversity of a summary two summaries may have same score and yet one may be more continuous and hence visually more pleasurable to watch than another we propose an evaluation framework where a summary is assessed on its own merit using the rich annotations in visiocity as against comparing it against available human summaries using a suite of measures to capture various aspects of a summary like continuity diversity redundancy importance as against over dependence on one measure related work datasets currently available datasets for video summarization either have very short videos or have few long videos of only a particular type table compares visiocity with other existing datasets for video summarization med summaries dataset consists of annotated videos of length minutes with event categories like birthday wedding feeding the annotation comprises of segments and their importance scores tvsum consists of videos average length minutes from categories with importance scores provided by annotators for each second snippet the videos correspond to very short events like changing vehicle tires making sandwich though number of categories in tvsum and medsummaries appear to be large the notion of categories there is of events like making a sandwich or attempting bike tricks quite dierent from dierent domains in visiocity with an intent of studying the characteristics of summaries of dierent types of videos like sports or tv shows the ut egocentric dataset consists of long and annotated videos captured from head mounted cameras however though each video is very long there are only videos and they are of one type i e egocentric summe consists of videos with an average length of about min the annotation is in form of user summaries of length between to each video has summaries the vsumm dataset consists of two datasets youtube consists of videos min and ovp consists of videos of about min from the open video project each video has user summaries in the form of set of key frames consists of videos with a total duration of hours and is designed primarily for multi video summarization it is a collection of videos of a tourist place the average duration of each video is about mins tv episodes dataset consists of tv show videos each of mins the total duration is hours a recent dataset oers videos with user generated summaries each lol consists of online esports videos from the league of legends it consists of videos with each video being between mins the associated summary videos are mins long while this dataset is signicantly larger compared to the other datasets it is limited only to a single domain i e esports have extended the ute dataset to videos and have provided concept annotations but they are limited to only egocentric videos and do not support any concept hierarchy the scores annotations as in tvsum are richer annotations but are limited only to importance scores visiocity on the other hand comes with dense concept annotations for each snippet to the best of our knowledge visiocity is one of its kind large dataset with many long videos spanning across multiple categories and annotated with rich concept annotations for each snippet total duration cat name summe tvsum med summaries ut egocentric youtube youtube tv episodes lol visiocity ours videos duration of videos avg min secs avg min sec dur min avg min avg min sec dur min avg min sec dur min avg min sec avg min avg min dur min dur mins avg mins hours hours hours hours hours hours hours hours hours table comparison of visiocity with other datasets in literature means the corresponding information was not available dur stands for duration and cat is of event categories available in a dataset techniques for automatic video summarization a number of techniques have been proposed to further the state of the art in automatic video summarization most video summarization algorithms try to optimize several criteria such as diversity coverage importance and representation some techniques do this through lar functions some use lstms some use determinantal point processes dpps some use reinforcement learning and some use attention attempts to address video summarization via attention aware and adversarial training els evaluation evaluation of video summaries is challenging task owing to the multiple denitions of success early approaches involved user studies but with the obvious demerit of cost and reproducibility with a move to automatic evaluation every new technique of video summarization came with its own evaluation criteria making it dicult to compare results dierent techniques some of the early approaches included viper which addresses the problem by dening a specic ground truth format which makes it easy to evaluate a candidate summary and superseiv which is an unsupervised technique to evaluate video summarization algorithms that perform frame ranking vert on the other hand was inspired by bleu in machine translation and rouge in text summarization other techniques include pixel level distance between keyframes objects of interest as an indicator of similarity and precision recall scores over key frames selected by human annotators it is not surprising thus that observed that researchers should at least reach a consensus on what are the best procedures and metrics for evaluating video abstracts they concluded that a detailed research that focuses exclusively on the evaluation of existing techniques would also be a valuable addition to the eld this is one of the aims of this work more recently computing overlap between reference and generated summaries has become the standard framework for video summary evaluation however all these methods which require comparison with ground truth summaries suer from the challenges highlghted earlier yeung et al observed that visual need not mean semantic and hence proposed a text based approach of evaluation called videoset the candidate summary is converted to text and its similarity is computed with a ground truth textual summary that text is better equipped at capturing higher level semantics has been acknowledged in the literature and form the motivation behind our proposed evaluation measures however our measures are dierent in the sense that a summary is not converted to text domain before evaluating rather how important its selections are or how diverse its selections are is computed from the rich textual annotations in visiocity this is similar in spirit to but there it has been done only for egocentric videos as noted by the limited number of evaluation videos and annotations further magnify this ambiguity problem our visiocity framework precisely hits the nail by not only oering a larger dataset but also in proposing a richer evaluation framework better equipped at dealing with this ambiguity visiocity dataset videos visiocity is a diverse collection of videos spanning across six dierent categories tv shows friends sports soccer surveillance education tech talks birthday videos and wedding videos the videos have an average duration of about mins summary statistics for visiocity are presented in table publicly available soccer friends techtalk birthday and wedding videos were downloaded from internet tv shows contains videos from a popular tv series friends they are typically more aesthetic in nature and professionally shot and edited in sports category visiocity contains soccer videos these videos typically have well dened events of interest like goals or penalty kicks and are very similar to each other in terms of the visual features under surveillance category visiocity covers diverse settings like indoor outdoor classroom oce and lobby the videos were recorded using our own surveillance cameras these videos are in general very long and are mostly from static continuously recording cameras under educational category visiocity has tech talk videos with static views or inset views or dynamic views in personal videos category visiocity has birthdays and wedding videos these videos are typically long and unedited the videos are available to see and download from the project website at github domain videos duration total duration sports soccer tvshows friends surveillance educational personal videos birthday personal videos wedding all hours hours hours hours hours hours table key statistics of visiocity third column is in minutes min max avg annotations the ground truth in visiocity is not direct in form of the user summaries but indirect in form of concepts marked for each snippet being at a higher level indirect ground truth can be seen as a generator of ground truth summaries and thus allows for multiple solutions reference summaries of dierent lengths with dierent desired characteristics and is easy to scale it also makes the annotation process more objective and easier than asking the users to directly produce reference ground truth summaries concepts are carefully selected list of verbs and nouns based on the type of the video and are given importance ratings based on the knowledge of the particular domain the concepts are organized in categories instead of a long at list example categories include actor entity action scene number of people categories provide a natural structuring to make the annotation process easier and also support for at least one level hierarchy of concepts for driven summarization in addition to concepts we ask annotators to group those consecutive snippets as mega events which together constitute a cohesive event for example a few snippets preceeding a goal in a soccer video the goal snippet and a few snippets after the goal snippet together would constitute a mega event a model trained to learn importance scores only would do well to pick up the goal snippet however such a summary will not be very pleasing to watch because what is required in a summary in this case is not just the ball entering the goal post but the build up to this event and probably a few snippets as a followup thus this notion of mega events helps us to model the notion of continuity textual annotations vs ratings or scores as indirect ground truth while past work has made use of other forms of indirect ground truth like asking annotators to give a score or a rating to each shot using textual concept annotations oers several advantages first cially for long videos it is easier and more accurate for annotators to mark all keywords applicable to a shot snippet than for them to tax their brain and give a rating especially when it is quite subjective and requires going back and forth over the video for considering what is more important or less important second when annotators are asked to provide ratings they often suer from chronological bias one work addresses this by showing the snippets to the annotators in random order but it does nt work for long videos because an annotator can not remember all of these to be able to decide the relative importance of each third the semantic content of a snippet is better captured through text this is relevant from an importance perspective as well as diversity perspective as noted earlier two snippets may look visually dierent but could be semantically same and vice versa text captures the right level of semantics desired by video summarization also when two snippets have same rating it is not clear if they are semantically same or they are semantically dierent but equally important textual annotations brings out such similarities and dissimilarities more eectively fourth as already noted textual annotations make it easy to adapt visiocity to a wide variety of problems annotation protocol a group of professional annotators were tasked to annotate videos without listening to the audio by marking all applicable keywords on a snippet shot through a python gui application developed by us for this task it allows an annotator to go over the video unit by unit shot snippet and select the applicable keywords using a simple and intuitive gui figure it provides convenience features like copying the annotation from previous snippet which comes in handy where there are are a lot of consecutive identical snippets for example in surveillance videos figure annotation and visualization tool developed by us used in visiocity framework special caution was exercised to ensure high quality annotations specically the guidelines and protocols were made as objective as possible the annotators were trained through sample annotation tasks and the annotation round was followed by two verication rounds where both precision how accurate the annotations were and recall whether all events of interest and continuity information has been captured in the annotations were veried by another set of tators whatever inconsistencies or inaccuracies were found and could be automatically detected were included in our automatic sanity checks which were run on all annotations proposed evaluation framework literature talks about certain desirable good characteristics of a video summary for example a good video summary is supposed to be diverse non redundant continuous or visually pleasing without abrupt shot transitions representative of the original video and contain important or interesting snippets from the video in what follows we dive deeper into these teristics and propose measures to assess the candidate summaries on those characteristics diversity a summary which does good on diversity is non redundant it contains segments quite dierent from one another dierent could mean dierent things in terms of content alone i e one does nt want two similar looking snippets in a summary or in terms of content and time i e one does nt want visually similar consecutive snippets but does want visually similar snippets that are separated in time or in terms of the concepts covered one does not want too many snippets covering the same concept and would rather want a few of all concepts in surveillance videos for example one would like to have a summary which does nt have too many visually similar consecutive and hence redundant snippets but does have visually similar snippets that are separated in time for instance consider a video showing a person entering her oce at three dierent times of the day though all three look similar and will have identical concept annotations as well all are desired in the summary with regards to the quantitative formulation we dene the rst avor of diversity as where x is a subset of snippets dij is iou measure between snippets i and j based on their concept vectors for the other two avors of diversity we dene diversity clustered max min i jx dij max jxci rj where c are the clusters which can be dened over time divtime all consecutive similar snippets form a cluster or concepts divconcept all snippets covering a concept belong to a cluster and rj is the importance rating of a snippet j when optimized this function leads to the selection of the best snippet from each cluster this can be easily extended to select a nite number of snippets from each cluster instead of the best one megaeventcontinuity element of continuity makes a summary pleasurable to watch since only a small number of snippets are to be included in a summary some discontinuity in the summary is expected however the less the discontinuity at a semantic level more pleasing is the summary to watch there is a thin line between modelling redundancy and continuity when it comes to visual cues of frames some snippets might be redundant but are important to include in the summary from a continuity perspective to model the continuity visiocity has the notion of mega events as dened earlier to ensure no redundancy within a mega event the mega event annotations are as tight as possible meaning they contain bare minimum snippets just enough to indicate the event a non mega event snippet is continuous enough to exist in the summary on its own and a mega event snippet needs other adjacent snippets to be included in the summary for semantic continuity we measure mega event continuity as follows m e where e is the number of mega events in the video annotation is the rating of the mega event mi and is equal to maxsmi a is the annotation of video v that is a set of snippets such that each snippet s has a set of keywords k s and information about mega event m is a set of all mega events such that each mega event mi i e is a set of snippets that constitute the mega event mi importance interestingness this is the most obvious characteristic of a good summary for some domains like sports there is a distinct importance of some snippets over other snippets for eg score changing events this however is not applicable for some other domains like tech talks where there are few or no distinctly important events with respect to the annotations available in visiocity importance of a shot or snippet is dened by the ratings of the keywords of a snippet these ratings come from a mapping function which maps keywords to ratings for a domain the ratings are dened from to with rated keyword being the most important and indicated an undesirable snippet we assign ratings to keywords based on their importance to the domain and average frequency of occurence given the ratings of each keyword rating of a snippet is dened as otherwise here k s is the set of keywords of a snippet s rs if i and rks i thus importance function can be dened as and rs maxi rks is the rating of a particular keyword k s i i i note that when both importance and mega event continuity is measured we dene the importance only on the snippets which are non mega events since the mega event continuity term above already takes care of the importance of the mega event snippets as discussed earlier since there are mutliple right answers with varying characteristics we hypothesize that these are orthogonal characteristics and vary across dierent human summaries for example one human good summary could contain more important but less diverse segments while another human good summary could contain more diverse and less important segments depending on the intent behind summarization or user subjectivity also in assessing summaries one measure could be more relevant than another depending on the type of the video for example in sports videos because of well dened events of interest importance is more relevant in evaluating a summary we verify our hypotheses experimentally motivated by this we propose using a suite of measures as dened above instead of overly depending on any one of them the measures are computed using the annotations present in visiocity we summarize them in table we propose that a true and wholesome assessment of a candidate summary can only be done when this suite of measures including the existing measures like f score are used results and observations from our extensive experiments corroborate this fact measure diversitysim concept mega event continuity importance expression max mini jx dij maxjxci rj table some of the proposed measures in visiocity x is the candidate summary cis are clusters of consecutive similar snippets or concepts denotes rating and m denotes mega events ground truth summaries for supervised learning in practice it is dicult to acquire many human summaries with diverse characteristics especially for long videos we explore strategies to automatically generate the reference ground truth summaries of desired lengths using the annotations present in visiocity specifcially we use the above posed assessment measures as scoring functions and maximize them to get the desired ground truth summaries we note that maximizing a particular scoring function would yield a summary rich in that particular characteristic but it may fall short on other characteristics for example a mary maximizing importance will capture the goals in a soccer video but some snippets preceeding the goal and following the goal will not be in the summary and the summary will not be visually pleasing example illustration at github hence a weighted mixture of such measures need to be maximized to arrive at optimal yet diverse reference summaries this ite scoring function weighted mixture takes an annotated video keywords and mega events dened over snippets shots and generates a set of candidate ground truth summaries which supervised or semi supervised summarization algorithms can use mathematically given x a set of snippets of a video v let be dened as m this scoring function is parameterized on and is approximately optimized via a greedy rithm to arrive at the ground truth summaries dierent conguration of generates dierent summaries we explore two dierent strategies of identifying the right s for producing desired diverse reference summaries pareto optimality which is based on brute force search and proportional fairness for which there is a known ecient greedy algorithm with a provable imation guarantee for fairness pareto optimality pareto optimality is a situation that can not be modied so as to make any one individual or preference criterion better o without making at least one individual or preference criterion worse o beginning with a random element a possible conguration of the lambdas in the pareto optimal set we iterate over remaining elements to decide whether a new element should be added or old should be removed or new element should be discarded this is decided on the basis of the performance on various measures a conguration is better than another when it is better on all measures otherwise it is not proportional fairness consider each conguration as an allocation to some agents the ent scoring terms such that an allocation yields dierent performance on dierent measures the value seen by the agents specically for a performance measure p an tion conguration i yields value this setting allows us to use the notion of fair public decision making applying nash social welfare equation where the best allocation is one which is proportionally fair to all agents we borrow from the approach in which studies a fairness erty called core that generalizes the notion of proportional fairness and pareto optimality here the problem is reduced to maximizing the equation f for every congurations in the conguration space thus computing the top t congurations will take time n k we verify experimentally that the automatic ground truth summaries so generated are at par with the human summaries both qualitatively and quantitatively we use them in training the models tested on visiocity towards a new state of the art following we formulate the problem of automatic video summarization as a subset selection problem where a weighted mixture of set functions is maximized to produce an optimal summary for a desired budget specically given a video v as a set of snippets yv the problem reduces to picking y yv which maximizes our objective such that k k being the budget y is the predicted summary xv the feature representation of the video snippets and y is the weighted mixture of components y argmax y y xv y our mixture model comprises of a submodular facility location term and modular importance terms the facility location function is dened as ff vv maxxx where v is an element from the ground set v and measures the similarity between element v and element facility location thus models representativeness the importance scores are taken from the vasnet model and the vslstm model trained on visioicty the weights of the model are learnt ing the large margin framework as described in using many automatic ground truth summaries and a margin loss which combines the feedback from dierent evaluation measures specically given n pairs of a video and an automatic reference summary v ygt we learn the weight vector w by optimizing the following large margin formulation min n n where is the generalized hinge loss of training example n and w is the weight vector max yy n v wt xn v wt xn v gt this objective is chosen so that each ground truth summary scores higher than any other summary by some margin for training example n the margin we chose is denoted by and is a linear combination of the normalized losses reported by our proposed measures we call our proposed method visiocity sum we show that a simple model like this out performs the current niques state of the art on tvsum and summe on visiocity dataset because of learning from multiple ground truth summaries and learning from mutliple loss functions each capturing dierent characteristics of a summary experiments and results implementation details for analysis of and comparison with human summaries we generated automatic summaries per video of about the same length as the human summaries score of any candidate summary is computed with respect to the human ground truth summaries following we report both avg and max to calculate scores of human summaries with respect to human summary we compute max and avg in a leave one out fashion for analysis of and comparison of dierent techniques on the visiocity dataset we report their scores computed against the automatically generated summaries as a proxy for human summaries we generate automatic summaries for each video all target summaries are generated such that their lengths are to of the video length we test the performance of three dierent representative state of the art techniques on the visiocity benchmark vslstm is a supervised technique that uses bilstm to learn the variable length context in predicting important scores it learns from a combined ground truth in terms of aggregated scores vasnet is a supervised technique based on a simple attention based network without computationally intensive lstms and it learns from a combined ground truth in terms of aggregated scores and outputs a bilstms predicted score for each frame in the video dr dsn is an unsupervised deep reinforcement learning based model which learns from a combined diversity and representativeness reward on scores predicted by a bilstm decoder it outputs predicted score for every frame of a video to generate a candidate machine generated summary from the importance scores predicted by vslstm vasnet and dr dsn we follow to convert them into machine generated summary of desired length max of original video our proposed model visiocity sum learns from multiple ground truth summaries and outputs a machine generated summary as a subset of snippets in all tables refers to avg score refers to max score nearest neighbor score imp mc dt dc and dsi refer to the importance score mega event continuity score time score diversity concept score and diversity similarity score respectively as calculated by the proposed measures all gures are in percentages figure dierent human summaries of same video perform dierently on dierent measures dierent human summaries have dierent characteristics we asked a set of users dierent from the annotators to create human summaries for two randomly sampled videos of each domain the users were asked to look at the video without the audio and mark segments they feel should be included in the summary such that the length of the summary remains between to of the original video the procedure followed was similar to that of summe we assess these human summaries qualitatively and quantitatively using the proposed set of performance measures and make the following observations the human summaries are consistent with each other in as much as there are important scenes in the video for example goals in soccer videos in the absence of such clear interesting events the human summaries exhibit more inconsistency with each other a representative plot for the scores of human summaries of friends video is presented in figure we note the following proposed measures get good values on the human summaries as compared to uniform and random summaries thus ascertaining their utility b a human summary could score low on one measure and high on another measure the desired characteristics dier slightly across dierent domains for example importance seems to be more important for soccer videos than diversity automatically generated reference summaries are at par with human summaries in our experiments we search for fair congurations using both pareto optimality and proportional fairness using the ecient greedy algorithm table shows the average scores of both kinds of automatic ground truth summaries as compared to human summaries uniform summaries and random summaries we see that both approaches yield comparable performance we use the automatic summaries generated using pareto optimality in the rest of our experiments we compare automatically generated reference summaries with human summaries on our posed measures and present the quantitative results in table we see that automatic and human summaries are much better than random on all the evaluation criteria next we see that both the human and the automatic summaries are close to each other in terms of the metric the automatic summaries have the highest importance continuity and diversity scores this is not surprising as they are obtained at the rst place by optimizing a combination of these criteria ure shows a representative plot for min mean max of dierent measures for dierent summaries domain human uniform random auto pareto auto prop fri soc we d sur tec bir table performance of human and auto summaries on videos across all the domains pareto and auto prop stand for automatic summaries generated using pareto optimality and tional fairness respectively figure behavior of dierent measures for dierent types of summaries for soccer videos cont summaries are visually continuous summaries assembled by picking a set of continuous snippets of soccer videos we also compare the human and automatic summaries qualitatively as an example figure compares the selections by human summaries left and automatic ground truth summaries right for friends video we see a considerable similarity in selections though a perfect match of selections is neither possible nor expected keeping with the spirit of multiple correct answers some human mary videos and automatic ground truth summary videos are reported at github we see that it is very hard to distinguish the automatic summaries from human summaries and they form very good visual summaries in themselves visiocity benchmark performance of dierent models on siocity we test some state of the art models and our simple enhancement visiocity sum on visiocity and report the numbers in table we make the following observations dr dsn tries to generate a summary which is diverse as we can see in the results it almost always gets high score on the diversity term please note that the way we have dened these diversity measures diversity concept dc and diversity time domain soccer friends surveillance techtalk birthday wedding technique human uniform random auto human uniform random auto human uniform random auto human uniform random auto human uniform random auto human uniform random auto imp mc dt dc dsi table performance of human and auto summaries for dierent domains techtalk videos do not have megaevents dt have an element of importance in them also on the other hand diversity sim dsi is a pure diversity term where dr dsn almost always excels due to this nature of dr dsn when it comes to videos where the interestingness stands out and importance clearly plays a more important role dr dsn does nt perform well in such scenarios vslstm is seen to perform better closely followed by vasnet it is also interesting to note that while two techniques may yield similar scores on one measure for example vslstm and vasnet for soccer videos table one of them in this case vslstm does better on mega event continuity and produces a desirable characteristic in the summary this further strengthens our claim of having a set of measures evaluating a technique or a summary rather than over dependence on one which may not fully capture all desirable characteristics of good summaries we also note that even though dr dsn is an unsupervised technique it is a state of the art technique when tested on tiny datasets like tvsum or summe but when it comes to a large dataset like visiocity with more challenging videos it does nt do well especially on those domains where there are clearly identiable important events for example in soccer goal save penalty and birthday videos cake cutting in such cases models like vslstm and vasnet perform better as they are geared towards learning importance in contrast since the interstingness level in videos like surveillance and friends is more spread out dr dsn does relatively well even without any supervision visiocity sum does better than all techniques on account of learning from individual ground truth summaries and a figure shot numbers selected by some human summaries left and by some automatic ground truth summaries right for the friends video combination of loss functions conclusion in order to improve the objectivity and consistency in the design of video summarization benchmark datasets as well as their use in evaluating video summarization models we present visiocity a large benchmarking dataset and demonstrated its eectiveness in real world setting to the best of our knowledge it is the rst of its kind in the scale diversity and rich concept annotations we introduce a recipe to automatically create ground truth summaries typically needed by the supervised techniques motivated by the fact that dierent good summaries have dierent teristics and are not necessarily better or worse than the other we propose an evaluation framework better geared at modeling human judgment through a suite of measures than having to overly depend on one measure finally we report the strengths and weaknesses of some representative state of the art techniques when tested on this new benchmark and demonstrate the eectiveness of our simple extension to a mixture model making use of individual ground truth summaries and a combination of loss functions we hope our attempt to address the multiple issues currently rounding video summarization as highlighted in this work will help the community advance the state of the art in video summarization we make visiocity available through the project page at github and invite the researchers to test their algorithms on visiocity benchmark acknowledgements this work is supported in part by the ekal fellowship www ekal org and national center of lence in technology for internal security iit bombay ncetis iitb ac in domain soccer friends surveillance techtalk birthday wedding technique auto dr dsn vasnet vslstm ours random auto dr dsn vasnet vslstm ours random auto dr dsn vasnet vslstm ours random auto dr dsn vasnet vslstm ours random auto dr dsn vasnet vslstm ours random auto dr dsn vasnet vslstm ours random imp mc dt dc dsi table comparison of dierent techniques on visiocity techtalk videos do not have megaevents references e apostolidis e adamantidou a i metsai v mezaris and i patras unsupervised video summarization via attention driven adversarial learning in international conference on timedia modeling pages springer s e f de avila a p b lopes a da luz jr and a de albuquerque araujo vsumm a mechanism designed to produce static video summaries and a novel evaluation method pattern recognition letters d doermann and d mihalcik tools and techniques for video performance evaluation in icpr page ieee b fain k munagala and n shah fair allocation of indivisible public goods corr j fajtl h s sokeh v argyriou d monekosso and p remagnino summarizing videos with attention in asian conference on computer vision pages springer c fu j lee m bansal and a c berg video highlight prediction using audience chat reactions arxiv preprint t fu s tai and h chen attentive and adversarial learning for video summarization in ieee winter conference on applications of computer vision wacv pages ieee b gong w chao k grauman and f sha diverse sequential subset selection for pervised video summarization in advances in neural information processing systems pages m gygli h grabner h riemenschneider and l van gool creating summaries from user videos in european conference on computer vision pages springer m gygli h grabner h riemenschneider and l van gool creating summaries from user videos in eccv m gygli h grabner and l van gool video summarization by learning submodular tures of objectives in proceedings of the ieee conference on computer vision and pattern recognition pages m huang a b mahajan and d f dementhon automatic performance evaluation for video summarization technical report maryland univ college park inst for advanced computer studies z ji k xiong y pang and x li video summarization with attention based encoder decoder networks ieee transactions on circuits and systems for video technology s kannappan y liu and b tiddeman human consistency evaluation of static video maries multimedia tools and applications v kaushal r iyer k doctor a sahoo p dubal s kothawade r mahadev k dargan and g ramakrishnan demystifying multi faceted video summarization tradeo between diversity representation coverage and importance in ieee winter conference on applications of computer vision wacv pages ieee v kaushal s subramanian s kothawade r iyer and g ramakrishnan a framework towards domain specic video summarization in ieee winter conference on applications of computer vision wacv pages ieee a khosla r hamid c lin and n sundaresan large scale video summarization using in proceedings of the ieee conference on computer vision and pattern web image priors recognition pages a kulesza b taskar et al determinantal point processes for machine learning foundations and trends in machine learning s lan r panda q zhu and a k roy chowdhury ffnet video fast forwarding via forcement learning in proceedings of the ieee conference on computer vision and pattern recognition pages y j lee j ghosh and k grauman discovering important people and objects for tric video summarization in computer vision and pattern recognition cvpr ieee conference on pages ieee z lei c zhang q zhang and g qiu framerank a text processing approach to video summarization arxiv preprint y li and b merialdo vert automatic evaluation of video summaries in proceedings of the acm international conference on multimedia pages acm y li l wang t yang and b gong how local is the local diversity reinforcing sequential determinantal point processes with dynamic ground sets for supervised video summarization in proceedings of the european conference on computer vision eccv pages z lu and k grauman story driven summarization for egocentric video in proceedings of the ieee conference on computer vision and pattern recognition pages y ma l lu h zhang and m li a user attention model for video summarization in proceedings of the tenth acm international conference on multimedia pages acm m minoux accelerated greedy algorithms for maximizing submodular set functions in mization techniques pages springer m otani y nakashima e rahtu and j heikkila rethinking the evaluation of video maries in proceedings of the ieee conference on computer vision and pattern recognition pages r panda n c mithun and a k roy chowdhury diversity aware multi video tion ieee transactions on image processing b a plummer m brown and s lazebnik enhancing video summarization via language embedding in computer vision and pattern recognition volume d potapov m douze z harchaoui and c schmid category specic video summarization in european conference on computer vision pages springer a sharghi a borji c li t yang and b gong improving sequential determinantal point processes for supervised video summarization in proceedings of the european conference on computer vision eccv pages a sharghi b gong and m shah query focused extractive video summarization in european conference on computer vision pages springer a sharghi j s laurel and b gong query focused video summarization dataset evaluation and a memory network based approach in the ieee conference on computer vision and pattern recognition cvpr pages y song j vallmitjana a stent and a jaimes tvsum summarizing web videos using titles in proceedings of the ieee conference on computer vision and pattern recognition pages y song j vallmitjana a stent and a jaimes tvsum summarizing web videos using titles in cvpr pages ieee computer society b taskar v chatalbashev d koller and c guestrin learning structured prediction models in proceedings of the international conference on machine a large margin approach learning pages acm b t truong and s venkatesh video abstraction a systematic review and classication acm transactions on multimedia computing communications and applications tomm a b vasudevan m gygli a volokitin and l van gool query adaptive video summarization via quality aware relevance estimation in proceedings of the acm international conference on multimedia pages acm s xiao z zhao z zhang x yan and m yang convolutional hierarchical attention network for query focused video summarization arxiv preprint b xiong y kalantidis d ghadiyaram and k grauman less is more learning highlight detection from video duration in proceedings of the ieee conference on computer vision and pattern recognition pages s yeung a fathi and l fei fei videoset video summary evaluation through text arxiv preprint l yuan f e tay p li l zhou and j feng cycle sum cycle consistent adversarial lstm networks for unsupervised video summarization arxiv preprint k zhang w chao f sha and k grauman summary transfer exemplar based subset selection for video summarization in proceedings of the ieee conference on computer vision and pattern recognition pages k zhang w chao f sha and k grauman video summarization with long short term memory in european conference on computer vision pages springer b zhou a lapedriza j xiao a torralba and a oliva learning deep features for scene recognition using places database in advances in neural information processing systems pages k zhou y qiao and t xiang deep reinforcement learning for unsupervised video in thirty second aaai conference on marization with diversity representativeness reward articial intelligence
