abstractive summarization attentive neural techniques jacob krantz gonzaga university spokane gonzaga edu jugal kalita university colorado colorado springs colorado springs edu abstract world proliferating data ity rapidly summarize text ing importance automatic tion text thought quence sequence problem area natural language processing solves sequence sequence problem chine translation rapidly ing development based encoder decoder networks work applies modern techniques abstractive summarization perform analysis attention mechanisms summarization goal oping approach architecture aimed improving state art ticular modify optimize lation model self attention ating abstractive sentence summaries effectiveness base model attention variants compared lyzed context standardized uation sets test metrics metrics limited ability effectively score abstractive summaries propose new approach based intuition abstractive model requires abstractive evaluation introduction goal summarization textual document distill concise form preserving important information meaning end approaches historically taken extractive abstractive extractive summarization selects tant words given document combines rearranges form nal summarization nallapati approach restricted words directly source document unable paraphrase abstractive rithms generate summary attempt derstand document meaning allowing phrasing like human abstractive approaches difcult develop tractive ones intermediate tion knowledge required dominant techniques summarization extractive nature wide ranging solutions utilizing statistical topic based graph based machine learning approaches gambhir gupta potential generating coherent insightful summaries abstractive approaches gaining popularity fueled novel deep learning techniques stractive summarization process includes ing words respective embeddings puting document representation ing output words neural networks recently shown perform step dong deep learning models attention allows coder focus different segments input stepping output regions lated sequence sequence task machine lation attention introduced existing encoder decoder model bahdanau resulted large improvements past tems ability consider larger window context output generation ing vaswani showed multi headed self attention replace recurrence convolutions entirely areas machine translation abstractive summarization lated structurally semantically opments machine translation inform rection research abstractive summarization paper apply advancements develop pursuit sentence marization attempt summarization resulting text condensed original task generated summaries constrained xed maximum length tested models learn decide formation reproduced related work successful sentence summarization approaches classically statistical methods iary zajic detected salient ics guided sentence compression ing linguistic transformations moses tical machine translation system performed directly summarization koehn attention mechanisms shown improve results abstractive marization rush improved sic statistical results neural language model minimal contextual attention coder primary model training tractive tuning step performed cent dataset related extension convolutional attentive encoder experimented replacing decoder language model rnn variants lstm cells rnn elman showed improved rouge scores chopra attentive encoder decoder ployed zeng rnn chitecture weight improve context input sequence decoder tention copy mechanism differentiated vocabulary words based usage input nallapati tinued progress encoder decoder architectures employing bidirectional gru rnn encoder unidirectional gru rnn decoder posing dynamic vocabulary restrictions proved results reducing dimensionality softmax output layer pointer generator works encode bidirectional lstm code attention restriction coverage tor limits attention words previously tended maintained recently summarization progress paragraph level reinforcement learning recurrent abstractive summarization model teacher forcing similarity metric pared generated summary target mary paulus architecture figure transformer based network ture multi headed attention mechanisms tain recall options similar pand vaswani tained directional lstm intra attention actor critic reinforcement learning produce highest scores sentence summarization important eration optimizing purely test ric overall recall improved higher rouge scores necessarily correlate readability summaries models encoder decoder architectures provide able structure development systems solve sequence sequence problems coder maps input sequence latent vector representation decoder takes tion called context vector generates output sequence models variants follow structured select base architecture provides strong foundation analyze effect self attention ants transformer transformer architecture proposed vaswani notable performing state art machine translation efcient train past systems orders magnitude possible replacing sequence aligned recurrence parallel attention sequence order preserved self attention modules including positional beddings instead incremental values tional embeddings determined position sinusoidal time series curve masking decoder self attention performed ing output token dependent generated multi headed self attention encoder coder mechanisms map query vector key value vector pair results output vector tying encoder decoder multi headed attention mechanism query comes self attentive output decoder keys values attentive output encoder work vaswani attention heads scaled dot product attention tionally efcient multiple query key value vectors implemented combined matrix scaled dot product attention denes ture self attention mechanisms present attention sof tmax qkt attention mechanisms exist base dot product attention analyze performance mechanisms context abstractive summarization changing way query key value vectors interact allows attention mechanism learn different ships sequence elements relative dot product attention uses scaled product attention instead absolute sitional encodings uses relative positional coding relative encodings learn relate elements query elements keys values gehring codings distance limited context dow vector sequences local attention divides key value vectors localized blocks liu query strided corresponding block given lter size blocks contain positions prior following given position masking element based absolute position self attention performed block lation local masked attention adds mask blocks local attention blocks future quential position masked query elements block remain visible given intuitively masking future query position tions forces mechanism attend current past positions important tion attention distribution local block masked attention masks ous blocks future blocks query position future positions individual blocks masked dilated attention divides key value tors blocks introduces gap block query position limited context window specied number blocks preceding following memory tion dilated masked attention performs operations dilated attention masks future memory positions block evaluation standard test metric automatic summary generation rouge recall oriented study gisting evaluation lin fore rouge metrics introduced man judges summary evaluation human judges provide ideal evaluation impractical regular use rouge allows automatic comparison generated summaries target summaries target summaries human generated limited length recall monly reported rouge compare unigram bigram overlap respectively generalizes rouge gram overlap rouge determines longest common quence lcs evaluation quality tion models directly compared ous work metrics reported past models rush zeng nallapati metrics allow reasonably rate comparison summary generation models inherent problems exist critical limitation rouge consider equivalent phrasing synonymous concepts rouge works word level meaning tured compared binary manner word appears generated summary rouge proposed alleviate lem remove expectation erated summaries need identical summary ganesan pointed rush best human tor scored target endeavour astronauts join segments international space station endeavour astronauts join sections international space station endeavour astronauts remove segments international space station endeavour astronauts join segments international space station sentence rouge cos sim wmd vert table highlighted differences rouge vert scoring notice incorrect word placement scores reasonable word replacement rouge vert discounts score accordingly included perfect scores identical summary dataset illustrates idea maries need order high quality ate approach summary comparison evaluate semantic similarity erated target summaries instead lated word counts rouge captures tic similarity synonym dictionary evaluating grams lcs dresses word level shortcoming nal rouge metrics similarity xed discrete list acceptable alternatives fully capture phrase substitution provement evaluate semantic larity entities continuous scale vert metric improve quality summary evaluation introduce vert evaluation tool scores quality generated hypothesis summary compared reference target mary vert stands versatile evaluation duced texts vert compares summaries underlying semantics word count tios calculate vert score summary pair similarity sub score dissimilarity score calculated functionally combined naturally higher similarity score lower dissimilarity score leads higher better vert score similarity sub score considers mantics summary taken document level sentence embedding vector sized generated target summaries cosine similarity vectors vert implementation publicly com able vertmetric provides similarity score sentence beddings generated infersent source neural encoder trained natural language inference tasks conneau infersent chosen shown ize use problems requiring tence representations dissimilarity sub score operates individual word level sentence level aggregate euclidean tance calculated words erated summary words target mary word mover tance wmd algorithm measure far document travel match document word vector space kusner stop words discarded prior distance culation effect distance documents negligible sub score motivations consideration use sub scores independent tions infersent cosine ity wmd robust ence score wmd unaffected word ordering encoder infersent maintains sequential input illustrate suppose target sentence right left generated sentence switches order ing left right wmd gives perfect distance infersent ity accurately discounts score hand longer summaries compared infersent embeddings begin lose effect individual words word beddings replaced singular embedding problem wmd finally similarity sub score uses glove trained common crawl dissimilarity sub score uses trained google news dataset different word embeddings provides resistance potential learned tation biases formula specication dened similarity sub score dissimilarity sub score dened maximum dissimilarity value default distance generated words vocabulary default maries words compare innite distance strongly inuence vert score averages resulting sub score values range seek combine scores nal vert score treated percentage ert given equal weight nal vert score satisfy criteria present vert equation ert dissimilarity normalized outer linearity multiplied shifts range choice observe empirical distance ceiling table incorporating ing gives sub scores equal precedence removing necessity nonlinearity normalization hyperbolic tangent hyperparameters baseline similarity sub score uses pre trained fersent encoder reproducibility needs hyperparameter adjustments dissimilarity requires hyperparameter specify maximum threshold wmd stay default value value normalize dissimilarity vert ward use single hyperparameter stanford edu wmd summary count table wmd human summaries article human mary held target compare human summaries resulting parisons metric rouge vert pearson value table pearson correlation coefcient automatic metrics human evaluation sponsiveness provide scoring reference test man summary vert holdout process table erage similarity sub score average dissimilarity sub score combined average vert score comparison human evaluation evaluate effectiveness vert culate correlation vert scores scores given human judges generated tence summaries relative dot product attention model summaries generated dataset evaluated vert metric averaging vert scores tween target summaries duct experiment human uators score generated summaries based duc responsiveness primary consideration responsiveness information summary relates original sentence evaluators score level responsiveness point likert scale best possible table shows vert correlates human judgment google com archive nist gov responsiveness assessment instructions siveness stronger standard rouge metrics experiments experiment setup environment evaluation models strictly follows precedent set rush training testing extract sentence summary pairs news articles rst sentence article treated tence summarized headline article acts target summary datasets training data comes gigaword dataset collection million news articles graff necessary discard certain article headline pairs news articles open sentence poorly relates headline question preprocessing tasks includes ltering ptb tokenization casing replacing digit characters placing low frequency words unk uation hyperparameter tuning performed testing summaries capped length bytes article target maries compared processing gaword data provided rush duc datasets processed according tasks specied certain sentence summary pairs duc poorly relate fact generated summaries context entire duc article decide adequate summary shortcoming present els attempting sentence summarization duc effort remove difcult ings test set base implementation hyperparemeter specication models attention heads dimension dense feed forward layers cross entropy loss function optimization performed adam optimizer able learning rate encourage nal convergence nist gov tasks html dataset gigaword articles sent len sum len table comparison general dataset details sentence summary lengths reported average word count gigaword noticeably shorter target summaries duc dataset counteract models generating short summaries augment beam search ing probabilities encourage longer summaries training required approximately epochs promising feature attention based chitecture models ble trained approximately hours single gpu recent state art rent summarization models mentioned days rush mented models brary backed tensorflow strong local imum exists training closely relates extracting rst words input text bytes trivial approach produces atively high rouge scores simply ural similarity target summaries sentences diversity attention couraged varying learning rate ing attention mechanism ing step beam search beam size results rouge scores higher simple greedy inference decoding xed length bytes align easily word level decoding tation approximate cutoff limiting summary sequence words results attention comparisons attention mechanisms described performed scale analysis performance training model gigaword dataset evaluating experiment foundational ture held constant modied coder self attention decoder self attention perform specied given attention table model scaled anism com nist gov table comparison attention mechanisms represents rouge recall vert infersent cosine similarity sub score vert average wmd sub score mechanism dot prod rel dot prod local local mask local blk mask dilated dilated mask vert vert vert model topiary zajic abs rush ras lstm chopra koehn ras elman chopra rush lstm zeng words nallapati att rel abs vert table rouge recall scores compared models sorted score abs abs vert scores calculated summaries provided respective authors dot product attention acted baseline prod highest performing mechanism relative scaled dot product attention showing relative positional encodings ful absolute encodings demonstrates token generation rely heavily relationships surrounding words relationships global sequential level cal masked attention attained marginally higher rouge scores scaled product attention scaled dot product tention scored noticeably higher vert marily similarity sub score gests scaled dot product model better local mask model considering mary semantics entire sequence local dilated attention mechanisms repeated words regardless input sentence masked counterparts problem found high dependence batch size ing training process models verge batch sizes kens batch batch size train models tokens dilated attention dilated mask attention models trained lower batch sizes higher memory ments negatively effected results model comparisons compare best model past work comparing published rouge scores slight ances present reported metrics potential differences data preprocessing tines table compare best model published results relative product self attention model att rel beats rouge scores abs lower abs tuned tive routine att rel comparable lower certain els comes scores longer subsequence comparisons rouge att rel performs attributed ability self attention mechanisms retain strong ory past elements input decoded sequences actor critic method abs beats att rel tested categories qualitative discussion summaries generated best model strongly abstractive illustrated example figure example showcases ity utilize long range recall tive phrase model determined hariri prime minister lebanon adjusted morphology country succinctness model determined hariri resigning based words bowing occasionally tion heads misdirected attend words phrases contain primary ing occurred example correctly modied inclusion generated summaries exhibit information directly input sentence example correctly identies premier romano italian greatly improves informedness summary primary strength self attentive model incorporating abstract information segments input sentence gested long subsequence rouge scores seen clearly qualitative analysis assessment linguistic formed alongside duc responsiveness sessment followed procedure tailed section questions pertained maticality non redundancy referential clarity structure coherence grammaticality scored non redundancy scored referential ity scored structure coherence scored scores averaged good good non redundancy nearly perfect likely summaries short redundancy issue referential ity scored high associated performance self attention words decoded conclusion effect modern attention mechanisms plied sentence summarization tested analyzed shown self attentive encoder decoder perform sentence marization task use recurrence convolutions primary mechanisms state art summarization approaches day inherent limitation existing tems computational cost training nist gov quality questions txt figure examples generated summaries relative dot product self attention model ated recurrence models presented trained gigaword dataset hours single gpu relative dot product self attention model generated highest ity summaries tested models played ability abstracting reducing plex dependencies shown gram evaluation rouge metrics falls short judging quality abstractive summaries vert metric proposed native evaluate future automatic summarization based premise abstractive summary judged abstractive manner acknowledgments material based work supported national science foundation grant university colorado colorado springs reu site peter liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser noam shazeer generating wikipedia ing long sequences international conference learning representations ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments aaai pages ramesh nallapati bowen zhou ccero nogueira dos santos aglar gulcehre bing xiang abstractive text summarization sequence proceedings sequence rnns signll conference computational natural language learning conll berlin germany august pages romain paulus caiming xiong richard socher deep reinforced model abstractive marization arxiv preprint alexander rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages abigail peter liu christopher manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers pages association computational linguistics ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need advances neural information cessing systems pages david zajic bonnie dorr richard schwartz proceedings bbn umd topiary hlt naacl document understanding workshop boston pages wenyuan zeng wenjie luo sanja fidler raquel efcient summarization arxiv preprint urtasun read copy mechanism references dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly arxiv preprint learning align translate sumit chopra michael auli alexander rush abstractive sentence summarization tentive recurrent neural networks proceedings conference north american ter association computational linguistics human language technologies pages alexis conneau douwe kiela holger schwenk loic barrault antoine bordes supervised learning universal sentence representations arxiv preprint natural language inference data yue dong based summarization methods survey neural arxiv preprint mahak gambhir vishal gupta recent matic text summarization techniques survey ticial intelligence review kavita ganesan rouge updated proved measures evaluation summarization tasks arxiv preprint jonas gehring michael auli david grangier nis yarats yann dauphin lutional sequence sequence learning national conference machine learning pages david graff junbo kong chen kazuaki maeda english gigaword linguistic data consortium philadelphia philipp koehn hieu hoang alexandra birch chris callison burch marcello federico nicola bertoldi brooke cowan wade shen christine moran richard zens moses open source toolkit statistical machine translation ceedings annual meeting acl interactive poster demonstration sessions pages association computational tics matt kusner sun nicholas kolkin kilian weinberger word embeddings ment distances international conference chine learning pages piji lidong bing wai lam critic based training framework abstractive marization arxiv preprint chin yew lin rouge package automatic evaluation summaries proceedings acl workshop text summarization branches
