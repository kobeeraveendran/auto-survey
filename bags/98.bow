contextualizing citations for scientific summarization using word embeddings and domain knowledge arman cohan information retrieval lab dept of computer science georgetown university nazli goharian information retrieval lab dept of computer science georgetown university a m l c s c v v i x r a abstract citation texts are sometimes not very informative or in some cases inaccurate by themselves they need the appropriate context from the referenced paper to reflect its exact contributions to address this problem we propose an unsupervised model that uses tributed representation of words as well as domain knowledge to extract the appropriate context from the reference paper tion results show the effectiveness of our model by significantly outperforming the state of the art we furthermore demonstrate how an effective contextualization method results in improving citation based summarization of the scientific articles keywords text summarization scientific text information retrieval introduction in scientific literature related work is often referenced along with a short textual description regarding that work which we call citation text citation texts usually highlight certain contributions of the referenced paper and a set of citation texts to a reference paper can provide useful information about that paper therefore citation texts have been previously used to enhance many downstream tasks in ir nlp such as search and summarization while useful citation texts might lack the appropriate context from the reference article for example details of the ods assumptions or conditions for the obtained results are often not mentioned furthermore in many cases the citing author might misunderstand or misquote the referenced paper and ascribe butions to it that are not intended in that form hence sometimes the citation text is not sufficiently informative or in other cases even inaccurate this problem is more serious in life sciences where accurate dissemination of knowledge has direct impact on human lives we present an approach for addressing such concerns by adding the appropriate context from the reference article to the citation texts enriching the citation texts with relevant context from the reference paper helps the reader to better understand the context for the ideas methods or findings stated in the citation text permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page copyrights for components of this work owned by others than acm must be honored abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission a fee request permissions from sigir shinjuku tokyo japan acm doi a challenge in citation contextualization is the discourse and terminology variations between the citing and the referenced thors hence traditional ir models that rely on term matching for finding the relevant information are ineffective we propose to address this challenge by a model that utilizes word embeddings and domain specific knowledge specifically our approach is a retrieval model for finding the appropriate context of citations aimed at capturing terminology variations and ing between the citation text and its relevant reference context we perform two sets of experiments to evaluate the performance of our system first we evaluate the relevance of extracted contexts intrinsically then we evaluate the effect of citation tion on the application of scientific summarization experimental sults on tac benchmark show that our approach significantly outperforms several strong baselines in extracting the relevant contexts we furthermore demonstrate that our contextualization models can enhance summarizing scientific articles contextualizing citations given a citation text our goal is to extract the most relevant context to it in the reference article these contexts are essentially certain textual spans within the reference article throughout colloquially we refer to the citation text as query and reference spans in the reference article as documents our approach extends language models for ir lm by incorporating word embeddings and domain ontology to address shortcomings of lm for this research purpose the goal in lm is to rank a document d according to the tional probability qi q where qi shows the tokens in the query estimating is often achieved by maximum likelihood estimate from term frequencies with some sort of smoothing using dirichlet smoothing we have d w v w d where d shows the frequency of term qi in document d c is the entire collection v is the vocabulary and the dirichlet smoothing parameter in the citation contextualization problem the target reference sentences are short documents and there exist terminology variations between the citing author and the referenced author hence the citation terms usually do not appear in the documents and relying only on the frequencies of citation terms in the documents d for estimating yields an almost uniform smoothed distribution that is unable to decisively distinguish between the documents embeddings distributed representation embedding of a word w in a field f is a mapping w fn where words semantically similar to w will be ideally located close to it given a query q we rank the documents d according to the following scoring function knowledge can further help the embedding based retrieval model we incorporate it in our model in the following ways retrofitting faruqui et al proposed a model that uses the constraints on wordnet lexicon to modify the word vectors and pull synonymous words closer to each other to inject the domain knowledge in the embeddings we apply this model on two domain specific ontologies namely mesh and protein ontologies we chose these two biomedical domain ontologies because they are in the same domain as the articles in the tac dataset mesh is a broad ontology that consists of biomedical terms and po is a more focused ontology related to biology of proteins and genes interpolating in the lm we also directly incorporate the main knowledge in the retrieval model we modify the lm into the following interpolated lm with parameter where is estimated using eq and is similar to except that we replace fsem with the function font which considers domain ontology in calculating similarities font d dj dj if if qi dj where is a parameter and qi dj shows that there is an is synonym relation in ontology between qi and experiments data we use the tac biomedical summarization this dataset contains scientific biomedical journal articles and total citation texts where the relevant contexts for each citation text are annotated by experts baselines to our knowledge the only published results on tac is where the authors utilized query reformulation qr based on umls ontology in addition to we also implement eral other strong baselines to better evaluate the effectiveness of our model vsm vector space model that was used in desm dual embedding space model which is a recent embedding based retrieval model and lmd lda language modeling with lda smoothing which is a recent extension of the lmd to also account for the latent topics all the baseline parameters are tuned for the best performance and the same preprocessing is applied to all the baselines and our methods our methods we first report results based on training the beddings on wikipedia wewiki since tac dataset is in ical domain many of the biomedical terms might be either of vocabulary or not captured in the correct context using eral embeddings therefore we also train biomedical embeddings in addition we report results for biomedical embeddings with retrofitting as well as interpolating domain knowledge intrinsic evaluation first we analyze the effectiveness of our proposed approach for contextualization intrinsically that is we evaluate the quality of the values of the parameters and were selected empirically by grid search train biomedical embeddings on trec genomics and collections both wikipedia and genomics embeddings were trained using gensim implementation of negative sampling window size of and dimensions figure dot product of embeddings and its logit for a ple word and its top most similar words top and which leverages this property fsem d w v fsem w d where fsem is a function that measures semantic relatedness of the query term qi to the document and is defined as fsem d d dj where dj s are document terms and dj is the relatedness between the the query term and document term which is calculated by applying a similarity function to the distributed representations of qi and we use a transformation of dot products between the unit vectors and corresponding to the embeddings of the terms qi and dj for the similarity function dj if otherwise we first explain the role of and then the reason for considering the function instead of raw dot product is a parameter that controls the noise introduced by less similar words many unrelated word vectors have non zero similarity scores and adding them up introduces noise to the model and reduces the performance s function is to set the similarity between unrelated words to zero instead of a positive number to identify an appropriate value for we select a random set of words from the embedding model and calculate the average and standard deviation of pointwise absolute value of similarities between terms from these two samples we then select the threshold to be two standard deviations larger than the average to only consider very high similarity values this choice was empirically justified examining term similarity values between words shows that there are many terms with high similarities associated with each term and these values are not highly discriminative we apply a transfer function to the dot product to dampen the effect of less similar words in other words we only want highly related words to have high similarity values and similarity should quickly drop as we move to less related words we use the logit function for to achieve this dampening effect log figure shows this effect the purple line is the normalized dot product of a sample word with the most similar words in the model as illustrated the similarity score differences among top words is not very discriminative however applying the logit function green line causes the less similar words to have lower similarity values to the target word domain knowledge successful word embedding methods have previously shown to be effective in capturing syntactic and semantic relatedness between terms these co occurrence based models are data driven on the other hand domain ontologies and lexicons that are built by experts include some information that might not be captured by embedding methods therefore using domain dot productnormalized dot productnormalized logit table results on tac dataset p r c f acter offset precision recall and scores rg rouge character offset precision at shows statistical nificant improvement over the best baseline performance two tailed t test values are percentages method vsm desm lmd lda qr wewiki webio r f ndcg c c p table top relevant words to the word expression ing to embeddings trained on wikipedia genomics general wikipedia interpretation sense emotion function intension manifestation expressive biomedical genomics upregulation mrna induction protein abundance gene downregulation extracted citation contexts using our contextualization methods in terms of how accurate they are with respect to human annotations evaluation we consider the following evaluation metrics for assessing the quality of the retrieved contexts for each citation from multiple aspects character offset overlaps of the retrieved contexts with human annotations in terms of precision c p recall r and f score c f these are the recommended metrics for the task per ndcg we treat any partial overlaps with the gold standard as a correct context and then calculate the ndcg scores rouge n scores to also consider the content similarity of the retrieved contexts with the gold standard we calculate the rouge scores between them character precision at k since we are usually interested in the top retrieved spans we consider character offset precision only for the top k spans and we denote it with c results the results of intrinsic evaluation of contextualization are presented in table our models last rows of table achieve significant improvements over the baselines consistently across most of the metrics this shows the effectiveness of our models viewed from different aspects in comparison with the baselines the best baseline performance is the query reformulation qr method by which improves over other baselines wiki we observe that using general domain embeddings does not vide much advantage in comparison with the best baseline compare we and qr in the table however using the domain specific embeddings webio results in c f improvement over the best baseline this is expected since word relations in the biomedical context are better captured with biomedical embeddings in table an illustrative word expression gives better intuition why is that the case as shown using general embeddings left column in the table the most similar words to expression are those related to table breakdown of our best model s character f score f by quartiles of human performance measured by c quartiles p f of our model mean stdev the general meaning of it however many of these related words are not relevant in the biomedical context in the biomedical context expression refers to the appearance in a phenotype attributed to a particular gene as shown on the right column the domain specific embeddings bio trained on genomics data are able to capture this meaning this further confirms the inferior performance of the out of domain word embeddings in capturing correct word level semantics last two rows in table show incorporating the domain knowledge in the model which results in significant provement over the best baseline in terms of most metrics and f improvements this shows that domain ontologies provide additional information that the domain trained embeddings may not contain while both our methods of incorporating domain ontologies prove to be effective interpolating domain knowledge directly webio dmn has the edge over retrofitting webio rtrft this is likely due to the direct effect of ontology on the interpolated language model whereas in retrofitting the ontology first affects the embeddings and then the context extraction model to analyze the performance of our system more closely we took the context identified by annotator as the candidate and the other as gold standard and evaluated the precision to obtain an estimate of human performance on each citation we then divided the tions based on human performance to groups by quartiles table shows our system s performance on each of these groups we serve that when human precision is higher upper quartiles in the table our system also performs better and with more confidence lower std therefore the system errors correlate well with human disagreement on the correct context for the citations averaged over the annotators for each citation the mean precision was note that this translates to our c metric in table we observe that our best method c of is comparable with average human precision score c of which further demonstrates the effectiveness of our model external evaluation citation based summarization can effectively capture various tributions and aspects of the paper by utilizing citation texts however as argued in section citation texts do not always curately reflect the original paper we show how adding context from the original paper can address this concern while keeping the benefits of citation based summarization specifically we compare how using no contextualization versus various proposed alization approaches affect the quality of summarization we apply the following well known summarization algorithms on the set of citation texts and the retrieved citation contexts lexrank based sumbasic and kl divergence for space constraints we will not explain these approaches here refer to for details we then compare the effect of our proposed contextualization methods using the standard rouge n summarization evaluation metrics results the results of external evaluation are illustrated in ble the first row no context shows the performance of each table effect of contextualization on summarization columns are summarization algorithms and rows show tation contextualization approaches no context uses only citations without any contextualization evaluation metrics are rouge rg scores shows statistically significant provement over the best baseline performance method no context vsm desm lmd lda qr wewiki webio lsa klsum lexrank sumbasic summarization approach solely on the citations without any textualization the next rows show the baselines and last rows are our proposed contextualization methods as shown effective contextualization positively impacts the generated summaries for example our best method is webio dmn which significantly proves the quality of generated summaries in terms of rouge over the ones without any context we observe that two low performing baseline methods for contextualization according to table vsm and also do not result in any improvements for tion therefore the intrinsic quality of citation contextualization has direct impact on the quality of generated summaries these sults further demonstrate that effective contextualization is helpful for scientific citation based summarization related work related work has mostly focused on extracting the citation text in the citing article in this work given the citation texts we focus on extracting its relevant context from the reference paper related work have also shown that citation texts can be used in different applications such as summarization our proposed model utilizes word embeddings and the domain edge embeddings have been recently used in general information retrieval models vuli and moens proposed an architecture for learning word embeddings in multilingual settings and used them in document and query representation mitra et al proposed dual embedded space model that predicts document aboutness by comparing the centroid of word vectors to query terms ganguly et al used embeddings to transform term weights in a tion model for retrieval their model uses embeddings to expand documents and use co occurrences for estimation unlike these works we directly use embeddings in estimating the likelihood of query given documents we furthermore incorporate ways to utilize domain specific knowledge in our model the most relevant prior work to ours is where the authors approached the problem using a vector space model similarity ranking and query reformulations conclusions citation texts are textual spans in a citing article that explain tain contributions of a reference paper we presented an effective model for contextualizing citation texts associating them with the appropriate context from the reference paper we obtained cally significant improvements in multiple evaluation metrics over several strong baseline and we matched the human annotators precision we showed that incorporating embeddings and domain knowledge in the language modeling based retrieval is effective for situations where there are high terminology variations between the source and the target such as citations and their reference context citation contextualization not only can help the readers to better understand the citation texts but also as we demonstrated they can improve other downstream applications such as scientific document summarization overall our results show that citation contextualization enables us to take advantage of the benefits of citation texts while ensuring accurate dissemination of the claims ideas and findings of the original referenced paper acknowledgements we thank the three anonymous reviewers for their helpful ments and suggestions this work was partially supported by tional science foundation nsf through grant references amjad abu jbara and dragomir radev reference scope identification in citing sentences in naacl hlt acl arman cohan and nazli goharian scientific article summarization using citation context and article s discourse structure in emnlp arman cohan and nazli goharian scientific document summarization via citation contextualization and scientific discourse international journal on digital libraries arman cohan luca soldaini and nazli goharian matching citation text and cited spans in biomedical literature a search oriented approach in naacl hlt anita waard and henk pander maat epistemic modality and knowledge attribution in scientific discourse a taxonomy of types and overview of features in workshop on detecting structure in scholarly discourse acl manaal faruqui jesse dodge kumar sujay jauhar chris dyer eduard hovy and noah smith retrofitting word vectors to semantic lexicons in naacl hlt association for computational linguistics debasis ganguly dwaipayan roy mandar mitra and gareth jones word embedding based generalized language model for information retrieval in sigir acm felix hill roi reichart and anna korhonen evaluating semantic models with similarity estimation computational linguistics kokil jaidka muthu kumar chandrasekaran sajal rustagi and min yen overview of the cl scisumm shared task in jcdl fanghong jian jimmy xiangji huang jiashu zhao tingting he and po hu a simple enhancement for ad hoc information retrieval via topic modelling in sigir acm qiaozhu mei and chengxiang zhai generating impact based summaries for scientific literature in acl vol bhaskar mitra eric nalisnick nick craswell and rich caruana a dual embedding space model for document ranking corr ramesh nallapati bowen zhou and mingbo ma classify or select neural architectures for extractive document summarization ani nenkova and kathleen mckeown a survey of text summarization techniques in mining text data springer vahed qazvinian and dragomir radev scientific paper summarization using citation summary networks coling anna ritchie stephen robertson and simone teufel comparing citation contexts for information retrieval in cikm acm gnes sndor and anita de waard identifying claimed knowledge updates in biomedical research articles in proceedings of the workshop on detecting structure in scholarly discourse acl simone teufel and marc moens summarizing scientific articles ments with relevance and rhetorical status computational linguistics ivan vuli and marie francine moens monolingual and cross lingual information retrieval models based on bilingual word embeddings in sigir stephen wan ccile paris and robert dale whetting the appetite of scientists producing summaries tailored to the citation context in jcdl chengxiang zhai and john lafferty a study of smoothing methods for language models applied to information retrieval tois
