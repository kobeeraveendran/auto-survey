sequence generation sides middle long jiajun chengqing heng chinese academy sciences beijing china laboratory pattern recognition casia beijing china center excellence brain science intelligence technology shanghai china intelligence technology lab alibaba group long zhou jjzhang ia ac cn yuheng inc com n u j l c s c v v x r abstract encoder decoder framework achieved promising process sequence generation tasks neural machine translation text summarization framework usually ates sequence token token left right autoregressive decoding procedure time consuming output sentence longer lacks guidance future text crucial avoid translation alleviate issues propose synchronous bidirectional sequence generation sbsg model predicts outputs sides middle simultaneously sbsg model able left right right left generation help interact leveraging interactive bidirectional attention work experiments neural machine translation ende chen enro text rization tasks proposed model nicantly speeds decoding improving generation quality compared autoregressive transformer introduction neural encoder decoder framework widely adopted sequence generation tasks including neural chine translation nmt sutskever et al bahdanau et al vaswani et al text summarization rush et al zhou et al li et al image tioning xu et al vinyals et al work encoder models semantics input sentence transforms context vector representation fed decoder generate output sequence token token left right manner framework obtained great success sequence sequence model suffers decoding ciency problem gu et al models use autoregressive decoders operate step time slow generating long sequences computationally intensive neural network predict token recently proposed models avoid rence training time leveraging convolutions gehring et figure illustration synchronous bidirectional decoding sequence sequence model bidirectional decoder predicting outputs left right right left simultaneously interactively produce tokens time step al self attention vaswani et al parallelizable alternatives recurrent neural networks decoding process share speed strength allelization autoregressive generation schema decoder importantly left right decoding advantage future contexts generated right left decoding zhang et al avoid autoregressive property gu et al proposed non autoregressive model speed machine translation directly generating target words ing previous predictions oord et al modied convolutional network non autoregressive modeling speech synthesis lee et al introduced conditional non autoregressive neural sequence model based iterative renement spite improvement ing speed non autoregressive models typically suffer substantial drop generation quality paper propose synchronous bidirectional quence generation sbsg model achieve better ment generation quality decoding speed instead producing output sentences token token predicting outputs totally parallel manner sbsg model ates tokens time shown figure tional decoder generate output sentences sides middle left right right left directions furthermore introduce interactive bidirectional attention network bridge puts specically moment generation target tokens rely previously ated outputs history information depends ously predicted tokens generation direction ture information specically contributions paper right decoding left rized folds propose novel sbsg model employs coder predict outputs sides middle simultaneously interactively best knowledge rst work perform sequence generation ends middle extensively evaluate proposed model cal sequence generation tasks neural machine translation text summarization case chine translation obtain approximately speedup decoding autoregressive transformer beam search greedy search improvement bleu points translation quality ende nist chen enro tively signicantly outperforms previous non autoregressive models gu et al lee et al kaiser et al text summarization proposed model able decode approximately faster achieving better generation quality relative autoregressive counterparts related work autoregressive decoding recent approaches sequence sequence learning typically leverage recurrence sutskever et al convolution gehring et al attention vaswani et al basic building blocks particularly relying entirely attention mechanism transformer introduced vaswani et al improve training speed model performance ate autoregressive architecture mi et al introduced sentence level vocabulary able reduce computing time memory usage devlin focused fast accurate neural machine translation decoding cpu zhang et al proposed average attention network aan alternative self attention network decoder transformer despite remarkable success difcult parallelize unidirectional decoding work limits potential liu et al non autoregressive decoding terms speeding decoding neural transformer gu et al modied autoregressive architecture speed machine translation directly generating target words parallel main drawback work need tensive policy gradient ne turning techniques issue method works machine translation applied sequence generation tasks allel gu et al oord et al presented cessful non autoregressive sequence model speech form kaiser et al rst auto encoded sequence shorter sequence discrete latent ables decoded output sentence shorter latent sequence parallel lee et al introduced conditional non autoregressive neural sequence model based iterative renement concurrently work wang et al presented semi autoregressive transformer faster translation changing autoregressive erty global approaches improved allelizability signicantly reduced generation quality bidirectional decoding liu et al posed agreement model encourage agreement tween pair target directional lstms generated balanced targets similarly work attempted target bidirectional decoding smt nmt watanabe sumita finch sumita liu et al sennrich et al liu et al recently zhang et al zhou et al proposed asynchronous synchronous bidirectional decoding nmt tively serdyuk et al presented twin networks encourage hidden state forward network close backward network predict ken studies speed decoding procedure sacrice speed exchange quality improvement work differs ducing novel sequence generation model aims ing advantage left right right left ing accelerate improve sequence generation framework goal work achieve better improvement generation quality decoding speed introduce novel method decoding left right left manners simultaneously interactively unied model demonstrated figure proposed model consists encoder bidirectional decoder special labels beginning output sentence utilized guide sequence generation left right right left bidirectional decoder reads encoder representation generates output tokens time step interactive bidirectional attention networks detail individual components introduce algorithm training inference neural encoder given input sentence xm new leverages encoder induce input semantic dependencies enable decoder recover encoded information output sentence encoder composed stack n identical layers sub layers hl superscript l indicates layer depth ln layer malization ffn means feed forward networks mhatt denotes multi head attention mechanism follows scaled dot product attention attention function described mapping query set key value pairs output output computed weighted sum values weight assigned value computed compatibility function query ing key scaled dot product attention operates query q key k value v k v softmax qk t dk v figure new transformer architecture proposed rectional multi head attention network instead producing output sentence token token predicting outputs totally parallel proposed model generates tokens left right right left time indicated special labels dk dimension key multi head attention use multi head version h heads obtains h different representations q k v computes scaled dot product attention tion concatenates results projects concatenation feed forward layer k v o v w v headi q w o parameter matrices kw k w k w v w q bidirectional decoder bidirectional decoder performs decoding left right right left manners guidance ously generated forward backward outputs apply bidirectional attention network replace self attention network decoder illustrate overall tecture figure present tional attention models integrate decoder transformer k v q q keys k values bidirectional scaled dot product attention figure left shows particular attention input v sists queries concatenated forward states h j backward ward states new forward states h j obtained bidirectional dot product scaled states h j calculated attention new forward states k t j dk k t j dk v j softmax v j softmax j att j att k j k j v j v j q j q j h f q j q j h b j obtained conventional scaled dot product j contains tention introduced equation tentional future information decoding h b h figure left bidirectional scaled dot product attention operates forward backward queries q keys k values v right bidirectional multi head intra attention consists attention layers parallel h f use linear interpolation method integrate forward formation h j integration j backward information h f h j hyper parameter decided performance development set h b j j h b j h b j decoding similar calculation forward h j h j backward hidden states hidden states puted follows att q j q j h f h b h j integration j att k j k j h j v j v j h b j integration introduced equation refer procedure formulated equation bsdpa h j h j bsdpa worth noting calculated parallel q j q j h j v j k j k j h j improve v j bidirectional multi head intra attention different mask multi head attention equation obtain new forward backward hidden states simultaneously shown figure right tention head j th target token computed bsdpa headi j h j k j w v h bsdpa k k v j q q q j v v w k w q parameter matrices standard multi head attention introduced tion contrast bidirectional multi head inter attention composed standard multi head attention models interact integrating bidirectional attention decoder use bidirectional attention network replace multi head attention decoder demonstrated figure layer bidirectional decoder rst sub layer bidirectional multi head intra attention multi head intra attentionn ffnffnffnffnbidirectional multi head intra attentionffnffnffnffnbidirectional multi head inter right decoding attentionright left decoding predictor bidirectional scaled dot product attentionjvjvjkjkjqjqjkjkjvjvjfhfjhbjhbjhjhjhhhvvkkqq biattintra capable combining history future information s l d s l s s s s s sl denotes l layer hidden states embedding tors subscript d denotes decoder informed intra attention representation second sub layer bidirectional multi head inter attention biattinter integrates tion corresponding source sentence performing right right left decoding attention respectively shown figure e s l d s l e denotes encoder informed inter attention sentation hn source hidden state layer l d hn hn hn hn s l sub layer position wise fully connected forward neural network s l s l e s l e finally employ linear transformation softmax tivation compute probability j th tokens based j s n n sn nal hidden states forward backward decoding s n j j n j y j n w denotes weight matrix shared rameters decoding j w j w training inference training given parallel sentence pair y design smart strategy enable synchronous bidirectional ation decoder rst divide output sentence y halves reverse second half second separately add special labels beginning half sentence y y guide erating tokens left right right left finally propose smoothing model better connect rectional generational results shown figure output length odd add additional tag fore forward backward sentence randomly words model capable generating null word necessary following previous work gu et al wang et al use knowledge distillation niques kim rush train model given set training examples training algorithm aims nd model parameters maximize follow vaswani et al vaswani et al use residual connection layer normalization decoder layer omitted presentation simplicity figure smoothing model introduced connect results smoothly output sentence odd tokens domly insert means null word removed postprocessing figure bidirectional beam search process proposed model produces tokens left right right left multaneously guidance special labels bidirectional attention model left right left decoding help interact hood training data z z log z j z j y z j log z j z j y z j inference proposed model trained employ simple bidirectional beam search algorithm predict output sequence illustrated figure special start tokens optimized training process let half beam decoding left right allow half beam decode right left blue blocks denote ongoing expansion esis decoding terminates end sentence ag predicted importantly bidirectional head intra attention decoding manners help interact beam search process tively use greedy search model application neural machine translation use bleu papineni et al evaluate proposed model translation tasks setup verify model translation datasets different sizes english ende nist chen english enro training sets consist m m m sentence pairs respectively tokenize corpora script moses koehn et al segment word subword units bpe sennrich et al use k k shared bpe tokens ende enro respectively ende use dation set test set chen utilize bpe encode chinese english respectively limit source target vocabularies frequent k tokens use nist validation set nist statmt org translation task html corpora include statmt org translation task html decoding decoding attention system architecture english german chinese english english romanian quality speed quality speed gu et al lee et al kaiser et al wang et al beam search wang et al greedy search work beam search work greedy search speed n n n quality existing nmt systems nmt systems nat nat d nat d nat adaptive lt lt sat sat sat sat transformer transformer model transformer transformer model table translation quality bleu speed ofcial test sets translation speed measured translated sentences second comparison list results reported gu et al lee et al kaiser et al wang et al note sat use different size corpus different preprocessing methods chinese english translation autoregressive semi autoregressive nmt models greater potential speedup decoding major drawback translation quality degradation making use history information future information sbsg model signicant bleu improvement p autoregressive semi autoregressive non autoregressive models test sets enro use development test sets implement proposed model based toolkit bidirectional transformer model employ adam optimizer use warmup decay strategy learning rate vaswani et al warmup steps training employ label smoothing value use gpus train ende gpu language pairs evaluation use beam search beam size length penalty sides use encoder decoder layers hidden size attention heads feed forward inner layer dimensions results analysis parameters nat gu et al adopts encoder decoder architecture additional fertility predictor model nat lee et al decoders needs parameters conventional transformer bidirectional nmt model uses single encoder decoder model predict target tokens left right right left manners simultaneously sbsg model increase parameters hyper parameter pared standard transformer inference speed shown table proposed sbsg model capable decoding approximately faster autoregressive transformer beam search lation tasks model obtains ende chen enro speedup greedy search compromise solution com tensorow tween autoregressive non autoregressive models speed model relatively slower nat d nat lt kaiser et al proposed model capable obtaining comparable translation speed compared sat wang et al translation quality table shows translation mance ende chen enro translation tasks proposed model behaves better nat d nat lt test datasets particular model beam search signicantly outperforms nat d nat lt bleu points large scale english german lation respectively sat faster decoding speed sbsg model k bigger fers translation quality degradation relative toregressive nmt compared autoregressive transformer proposed model beam search able behave better terms decoding speed translation quality thermore model greedy search perform autoregressive transformer bleu points translation quality ende chen enro respectively signicantly speedups coding conventional transformer length analysis follow bahdanau et al group sentences similar lengths compute bleu score averaged length translations group figure shows performance transformer transformer drops rapidly length input sentence increases sbsg model alleviates problem generating sequence sides dle general encourages model produce accurate long sentences source reference sbsg logo westbound sailboat braving wind waves churning arc shaped spindrifts light blue color words zheng s anniversary west logo composed sailboats break wind break waves rolling light blue water wave owers zheng anniversary west logo composed sailing boat wind waves rolling light blue shaped owers words west table chinese english translation example baselines proposed model model alleviate unbalanced output problems liu et al generating sentence sides middle abs selective enc transformer sbsg beam sbsg greedy rg l speed table rouge recall evaluation results duc test set comparison list results reported rush et al lapati et al zhou et al results mark taken corresponding papers proposed sbsg model icant outperforms conventional transformer model terms decoding speed generation quality results analysis table report rouge score speed duc test set experiments generation ity proposed model par state art text summarization models observe approximately faster decoding autoregressive transformer achieving better generation quality specially model beam search greedy search capable decoding faster conventional transformer english gaword test set conclusions work propose novel sbsg model performs bidirectional decoding simultaneously interactively stead producing output sentence token token posed model makes decoding parallelizable generates tokens time step extensively uate proposed sbsg model neural machine tion ende chen enro text tion english gigaword tasks different previous autoregressive models gu et al lee et al kaiser et al suffer quality dation sbsg model achieves signicant improvement generation quality decoding speed compared state art autoregressive transformer acknowledgments research work described paper funded national key research development program china grant natural science foundation china grant work supported grants nvidia nvail program figure length analysis performance generated lations respect lengths source sentences proposed sbsg model alleviate translation producing longer translation long sentences case study table present translation example nist chinese english liu et al model produces translation good prex omits second half sentence model usually generates generation better sufxes sults conrm ndings proposed sbsg model alleviate errors generating sequences sides middle application text summarization verify effectiveness proposed sbsg model text summarization real world plication encoder decoder framework succeeds rush et al setup abstractive sentence summarization aims provide like summary long sentence conduct text rization experiments english gigaword allel corpus produced pairing rst sentence headline news article heuristic rules tracted corpus contains m sentence summary pairs training set k examples development set employ shared vocabulary k word types use duc rush et al test set model structure neural machine translation employ rouge evaluation metric widely adopted evaluation metric text summarization com harvardnlp sent summary source source references bahdanau et al dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate iclr devlin jacob devlin sharp models dull ware fast accurate neural machine translation ing cpu emnlp pages finch sumita andrew finch eiichiro sumita bidirectional phrase based statistical machine translation emnlp pages gehring et al jonas gehring michael auli david grangier denis yarats yann dauphin convolutional sequence sequence learning icml gu et al jiatao gu james bradbury caiming xiong victor ok li richard socher autoregressive neural machine translation arxiv preprint kaiser et al ukasz kaiser aurko roy ashish vaswani niki pamar samy bengio jakob uszkoreit noam shazeer fast decoding sequence models discrete latent variables arxiv preprint kim rush yoon kim alexander m rush sequence level knowledge distillation emnlp koehn et al philipp koehn hieu hoang alexandra birch chris callison burch marcello federico nicola bertoldi brooke cowan wade shen christine moran richard zens chris dyer ondrej bojar alexandra stantin evan herbst moses open source toolkit statistical machine translation acl lee et al jason lee kyunghyun cho deterministic non autoregressive neural emnlp sequence modeling iterative renement pages elman mansimov et al haoran li junnan zhu jiajun zhang chengqing zong ensure correctness mary incorporate entailment knowledge abstractive sentence summarization coling liu et al lemao liu andrew m finch masao utiyama eiichiro sumita agreement bidirectional lstms sequence sequence learning aaai liu et al lemao liu masao utiyama andrew agreement finch eiichiro sumita bidirectional neural machine translation naacl liu et al yuchen liu long zhou yining wang yang zhao jiajun zhang chengqing zong rable study model averaging ensembling reranking nmt nlpcc pages mi et al haitao mi zhiguo wang abe cheriah vocabulary manipulation neural machine translation acl pages oord et al aaron van den oord yazhe li igor babuschkin karen simonyan oriol vinyals koray kavukcuoglu george van den driessche edward hart luis c cobo florian stimberg al lel wavenet fast speech synthesis arxiv preprint papineni et al kishore papineni salim roukos todd ward weijing zhu bleu methof matic evaluation machine translation acl rush et al alexander m rush sumit chopra jason weston neural attention model abstractive sentence summarization emnlp sennrich et al rico sennrich barry haddow alexandra birch edinburgh neural machine translation systems wmt wmt sennrich et al rico sennrich barry haddow alexandra birch neural machine translation rare words subword units acl pages serdyuk et al dmitriy serdyuk nan rosemary ke alessandro sordoni adam trischler chris pal yoshua bengio twin networks matching future sequence generation iclr sutskever et al ilya sutskever oriol vinyals quoc vv le sequence sequence learning ral networks nips pages vaswani et al ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention need nips pages vinyals et al oriol vinyals alexander toshev samy bengio dumitru erhan tell ral image caption generator cvpr wang et al chunqi wang ji zhang haiqing chen semi autoregressive neural machine translation arxiv preprint watanabe sumita taro watanabe eiichiro sumita bidirectional decoding statistical machine translation coling xu et al kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan salakhutdinov richard zemel yoshua bengio attend tell neural image caption generation visual tion computer science pages zhang et al biao zhang deyi xiong jinsong su accelerating neural transformer average tion network acl pages zhang et al xiangwen zhang jinsong su yue qin yang liu rongrong ji hongji wang chronous bidirectional decoding neural machine lation aaai et al qingyu zhou nan yang furu wei ming zhou selective encoding abstractive sentence summarization acl pages et al long zhou chengqing zong machine translation tacl pages synchronous bidirectional neural jiajun zhang
