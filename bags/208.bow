sequence generation sides middle long jiajun chengqing heng chinese academy sciences beijing china laboratory pattern recognition casia beijing china center excellence brain science intelligence technology shanghai china intelligence technology lab alibaba group long zhou jjzhang yuheng inc com abstract encoder decoder framework achieved promising process sequence generation tasks neural machine translation text summarization framework usually ates sequence token token left right autoregressive decoding procedure time consuming output sentence longer lacks guidance future text crucial avoid translation alleviate issues propose synchronous bidirectional sequence generation sbsg model predicts outputs sides middle simultaneously sbsg model able left right right left generation help interact leveraging interactive bidirectional attention work experiments neural machine translation ende chen enro text rization tasks proposed model nicantly speeds decoding improving generation quality compared autoregressive transformer introduction neural encoder decoder framework widely adopted sequence generation tasks including neural chine translation nmt sutskever bahdanau vaswani text summarization rush zhou image tioning vinyals work encoder models semantics input sentence transforms context vector representation fed decoder generate output sequence token token left right manner framework obtained great success sequence sequence model suffers decoding ciency problem models use autoregressive decoders operate step time slow generating long sequences computationally intensive neural network predict token recently proposed models avoid rence training time leveraging convolutions gehring figure illustration synchronous bidirectional decoding sequence sequence model bidirectional decoder predicting outputs left right right left simultaneously interactively produce tokens time step self attention vaswani parallelizable alternatives recurrent neural networks decoding process share speed strength allelization autoregressive generation schema decoder importantly left right decoding advantage future contexts generated right left decoding zhang avoid autoregressive property proposed non autoregressive model speed machine translation directly generating target words ing previous predictions oord modied convolutional network non autoregressive modeling speech synthesis lee introduced conditional non autoregressive neural sequence model based iterative renement spite improvement ing speed non autoregressive models typically suffer substantial drop generation quality paper propose synchronous bidirectional quence generation sbsg model achieve better ment generation quality decoding speed instead producing output sentences token token predicting outputs totally parallel manner sbsg model ates tokens time shown figure tional decoder generate output sentences sides middle left right right left directions furthermore introduce interactive bidirectional attention network bridge puts specically moment generation target tokens rely previously ated outputs history information depends ously predicted tokens generation direction ture information specically contributions paper right decoding left rized folds propose novel sbsg model employs coder predict outputs sides middle simultaneously interactively best knowledge rst work perform sequence generation ends middle extensively evaluate proposed model cal sequence generation tasks neural machine translation text summarization case chine translation obtain approximately speedup decoding autoregressive transformer beam search greedy search improvement bleu points translation quality ende nist chen enro tively signicantly outperforms previous non autoregressive models lee kaiser text summarization proposed model able decode approximately faster achieving better generation quality relative autoregressive counterparts related work autoregressive decoding recent approaches sequence sequence learning typically leverage recurrence sutskever convolution gehring attention vaswani basic building blocks particularly relying entirely attention mechanism transformer introduced vaswani improve training speed model performance ate autoregressive architecture introduced sentence level vocabulary able reduce computing time memory usage devlin focused fast accurate neural machine translation decoding cpu zhang proposed average attention network aan alternative self attention network decoder transformer despite remarkable success difcult parallelize unidirectional decoding work limits potential liu non autoregressive decoding terms speeding decoding neural transformer modied autoregressive architecture speed machine translation directly generating target words parallel main drawback work need tensive policy gradient turning techniques issue method works machine translation applied sequence generation tasks allel oord presented cessful non autoregressive sequence model speech form kaiser rst auto encoded sequence shorter sequence discrete latent ables decoded output sentence shorter latent sequence parallel lee introduced conditional non autoregressive neural sequence model based iterative renement concurrently work wang presented semi autoregressive transformer faster translation changing autoregressive erty global approaches improved allelizability signicantly reduced generation quality bidirectional decoding liu posed agreement model encourage agreement tween pair target directional lstms generated balanced targets similarly work attempted target bidirectional decoding smt nmt watanabe sumita finch sumita liu sennrich liu recently zhang zhou proposed asynchronous synchronous bidirectional decoding nmt tively serdyuk presented twin networks encourage hidden state forward network close backward network predict ken studies speed decoding procedure sacrice speed exchange quality improvement work differs ducing novel sequence generation model aims ing advantage left right right left ing accelerate improve sequence generation framework goal work achieve better improvement generation quality decoding speed introduce novel method decoding left right left manners simultaneously interactively unied model demonstrated figure proposed model consists encoder bidirectional decoder special labels beginning output sentence utilized guide sequence generation left right right left bidirectional decoder reads encoder representation generates output tokens time step interactive bidirectional attention networks detail individual components introduce algorithm training inference neural encoder given input sentence new leverages encoder induce input semantic dependencies enable decoder recover encoded information output sentence encoder composed stack identical layers sub layers superscript indicates layer depth layer malization ffn means feed forward networks mhatt denotes multi head attention mechanism follows scaled dot product attention attention function described mapping query set key value pairs output output computed weighted sum values weight assigned value computed compatibility function query ing key scaled dot product attention operates query key value softmax figure new transformer architecture proposed rectional multi head attention network instead producing output sentence token token predicting outputs totally parallel proposed model generates tokens left right right left time indicated special labels dimension key multi head attention use multi head version heads obtains different representations computes scaled dot product attention tion concatenates results projects concatenation feed forward layer headi parameter matrices bidirectional decoder bidirectional decoder performs decoding left right right left manners guidance ously generated forward backward outputs apply bidirectional attention network replace self attention network decoder illustrate overall tecture figure present tional attention models integrate decoder transformer keys values bidirectional scaled dot product attention figure left shows particular attention input sists queries concatenated forward states backward ward states new forward states obtained bidirectional dot product scaled states calculated attention new forward states softmax softmax att att obtained conventional scaled dot product contains tention introduced equation tentional future information decoding figure left bidirectional scaled dot product attention operates forward backward queries keys values right bidirectional multi head intra attention consists attention layers parallel use linear interpolation method integrate forward formation integration backward information hyper parameter decided performance development set decoding similar calculation forward backward hidden states hidden states puted follows att integration att integration introduced equation refer procedure formulated equation bsdpa bsdpa worth noting calculated parallel improve bidirectional multi head intra attention different mask multi head attention equation obtain new forward backward hidden states simultaneously shown figure right tention head target token computed bsdpa headi bsdpa parameter matrices standard multi head attention introduced tion contrast bidirectional multi head inter attention composed standard multi head attention models interact integrating bidirectional attention decoder use bidirectional attention network replace multi head attention decoder demonstrated figure layer bidirectional decoder rst sub layer bidirectional multi head intra attention multi head intra attentionn ffnffnffnffnbidirectional multi head intra attentionffnffnffnffnbidirectional multi head inter right decoding attentionright left decoding predictor bidirectional scaled dot product attentionjvjvjkjkjqjqjkjkjvjvjfhfjhbjhbjhjhjhhhvvkkqq biattintra capable combining history future information denotes layer hidden states embedding tors subscript denotes decoder informed intra attention representation second sub layer bidirectional multi head inter attention biattinter integrates tion corresponding source sentence performing right right left decoding attention respectively shown figure denotes encoder informed inter attention sentation source hidden state layer sub layer position wise fully connected forward neural network finally employ linear transformation softmax tivation compute probability tokens based nal hidden states forward backward decoding denotes weight matrix shared rameters decoding training inference training given parallel sentence pair design smart strategy enable synchronous bidirectional ation decoder rst divide output sentence halves reverse second half second separately add special labels beginning half sentence guide erating tokens left right right left finally propose smoothing model better connect rectional generational results shown figure output length odd add additional tag fore forward backward sentence randomly words model capable generating null word necessary following previous work wang use knowledge distillation niques kim rush train model given set training examples training algorithm aims model parameters maximize follow vaswani vaswani use residual connection layer normalization decoder layer omitted presentation simplicity figure smoothing model introduced connect results smoothly output sentence odd tokens domly insert means null word removed postprocessing figure bidirectional beam search process proposed model produces tokens left right right left multaneously guidance special labels bidirectional attention model left right left decoding help interact hood training data log log inference proposed model trained employ simple bidirectional beam search algorithm predict output sequence illustrated figure special start tokens optimized training process let half beam decoding left right allow half beam decode right left blue blocks denote ongoing expansion esis decoding terminates end sentence predicted importantly bidirectional head intra attention decoding manners help interact beam search process tively use greedy search model application neural machine translation use bleu papineni evaluate proposed model translation tasks setup verify model translation datasets different sizes english ende nist chen english enro training sets consist sentence pairs respectively tokenize corpora script moses koehn segment word subword units bpe sennrich use shared bpe tokens ende enro respectively ende use dation set test set chen utilize bpe encode chinese english respectively limit source target vocabularies frequent tokens use nist validation set nist statmt org translation task html corpora include statmt org translation task html decoding decoding attention system architecture english german chinese english english romanian quality speed quality speed lee kaiser wang beam search wang greedy search work beam search work greedy search speed quality existing nmt systems nmt systems nat nat nat nat adaptive sat sat sat sat transformer transformer model transformer transformer model table translation quality bleu speed ofcial test sets translation speed measured translated sentences second comparison list results reported lee kaiser wang note sat use different size corpus different preprocessing methods chinese english translation autoregressive semi autoregressive nmt models greater potential speedup decoding major drawback translation quality degradation making use history information future information sbsg model signicant bleu improvement autoregressive semi autoregressive non autoregressive models test sets enro use development test sets implement proposed model based toolkit bidirectional transformer model employ adam optimizer use warmup decay strategy learning rate vaswani warmup steps training employ label smoothing value use gpus train ende gpu language pairs evaluation use beam search beam size length penalty sides use encoder decoder layers hidden size attention heads feed forward inner layer dimensions results analysis parameters nat adopts encoder decoder architecture additional fertility predictor model nat lee decoders needs parameters conventional transformer bidirectional nmt model uses single encoder decoder model predict target tokens left right right left manners simultaneously sbsg model increase parameters hyper parameter pared standard transformer inference speed shown table proposed sbsg model capable decoding approximately faster autoregressive transformer beam search lation tasks model obtains ende chen enro speedup greedy search compromise solution com tensorow tween autoregressive non autoregressive models speed model relatively slower nat nat kaiser proposed model capable obtaining comparable translation speed compared sat wang translation quality table shows translation mance ende chen enro translation tasks proposed model behaves better nat nat test datasets particular model beam search signicantly outperforms nat nat bleu points large scale english german lation respectively sat faster decoding speed sbsg model bigger fers translation quality degradation relative toregressive nmt compared autoregressive transformer proposed model beam search able behave better terms decoding speed translation quality thermore model greedy search perform autoregressive transformer bleu points translation quality ende chen enro respectively signicantly speedups coding conventional transformer length analysis follow bahdanau group sentences similar lengths compute bleu score averaged length translations group figure shows performance transformer transformer drops rapidly length input sentence increases sbsg model alleviates problem generating sequence sides dle general encourages model produce accurate long sentences source reference sbsg logo westbound sailboat braving wind waves churning arc shaped spindrifts light blue color words zheng anniversary west logo composed sailboats break wind break waves rolling light blue water wave owers zheng anniversary west logo composed sailing boat wind waves rolling light blue shaped owers words west table chinese english translation example baselines proposed model model alleviate unbalanced output problems liu generating sentence sides middle abs selective enc transformer sbsg beam sbsg greedy speed table rouge recall evaluation results duc test set comparison list results reported rush lapati zhou results mark taken corresponding papers proposed sbsg model icant outperforms conventional transformer model terms decoding speed generation quality results analysis table report rouge score speed duc test set experiments generation ity proposed model par state art text summarization models observe approximately faster decoding autoregressive transformer achieving better generation quality specially model beam search greedy search capable decoding faster conventional transformer english gaword test set conclusions work propose novel sbsg model performs bidirectional decoding simultaneously interactively stead producing output sentence token token posed model makes decoding parallelizable generates tokens time step extensively uate proposed sbsg model neural machine tion ende chen enro text tion english gigaword tasks different previous autoregressive models lee kaiser suffer quality dation sbsg model achieves signicant improvement generation quality decoding speed compared state art autoregressive transformer acknowledgments research work described paper funded national key research development program china grant natural science foundation china grant work supported grants nvidia nvail program figure length analysis performance generated lations respect lengths source sentences proposed sbsg model alleviate translation producing longer translation long sentences case study table present translation example nist chinese english liu model produces translation good prex omits second half sentence model usually generates generation better sufxes sults conrm ndings proposed sbsg model alleviate errors generating sequences sides middle application text summarization verify effectiveness proposed sbsg model text summarization real world plication encoder decoder framework succeeds rush setup abstractive sentence summarization aims provide like summary long sentence conduct text rization experiments english gigaword allel corpus produced pairing rst sentence headline news article heuristic rules tracted corpus contains sentence summary pairs training set examples development set employ shared vocabulary word types use duc rush test set model structure neural machine translation employ rouge evaluation metric widely adopted evaluation metric text summarization com harvardnlp sent summary source source references bahdanau dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate iclr devlin jacob devlin sharp models dull ware fast accurate neural machine translation ing cpu emnlp pages finch sumita andrew finch eiichiro sumita bidirectional phrase based statistical machine translation emnlp pages gehring jonas gehring michael auli david grangier denis yarats yann dauphin convolutional sequence sequence learning icml jiatao james bradbury caiming xiong victor richard socher autoregressive neural machine translation arxiv preprint kaiser ukasz kaiser aurko roy ashish vaswani niki pamar samy bengio jakob uszkoreit noam shazeer fast decoding sequence models discrete latent variables arxiv preprint kim rush yoon kim alexander rush sequence level knowledge distillation emnlp koehn philipp koehn hieu hoang alexandra birch chris callison burch marcello federico nicola bertoldi brooke cowan wade shen christine moran richard zens chris dyer ondrej bojar alexandra stantin evan herbst moses open source toolkit statistical machine translation acl lee jason lee kyunghyun cho deterministic non autoregressive neural emnlp sequence modeling iterative renement pages elman mansimov haoran junnan zhu jiajun zhang chengqing zong ensure correctness mary incorporate entailment knowledge abstractive sentence summarization coling liu lemao liu andrew finch masao utiyama eiichiro sumita agreement bidirectional lstms sequence sequence learning aaai liu lemao liu masao utiyama andrew agreement finch eiichiro sumita bidirectional neural machine translation naacl liu yuchen liu long zhou yining wang yang zhao jiajun zhang chengqing zong rable study model averaging ensembling reranking nmt nlpcc pages haitao zhiguo wang abe cheriah vocabulary manipulation neural machine translation acl pages oord aaron van den oord yazhe igor babuschkin karen simonyan oriol vinyals koray kavukcuoglu george van den driessche edward hart luis cobo florian stimberg lel wavenet fast speech synthesis arxiv preprint papineni kishore papineni salim roukos todd ward weijing zhu bleu methof matic evaluation machine translation acl rush alexander rush sumit chopra jason weston neural attention model abstractive sentence summarization emnlp sennrich rico sennrich barry haddow alexandra birch edinburgh neural machine translation systems wmt wmt sennrich rico sennrich barry haddow alexandra birch neural machine translation rare words subword units acl pages serdyuk dmitriy serdyuk nan rosemary alessandro sordoni adam trischler chris pal yoshua bengio twin networks matching future sequence generation iclr sutskever ilya sutskever oriol vinyals quoc sequence sequence learning ral networks nips pages vaswani ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need nips pages vinyals oriol vinyals alexander toshev samy bengio dumitru erhan tell ral image caption generator cvpr wang chunqi wang zhang haiqing chen semi autoregressive neural machine translation arxiv preprint watanabe sumita taro watanabe eiichiro sumita bidirectional decoding statistical machine translation coling kelvin jimmy ryan kiros kyunghyun cho aaron courville ruslan salakhutdinov richard zemel yoshua bengio attend tell neural image caption generation visual tion computer science pages zhang biao zhang deyi xiong jinsong accelerating neural transformer average tion network acl pages zhang xiangwen zhang jinsong yue qin yang liu rongrong hongji wang chronous bidirectional decoding neural machine lation aaai qingyu zhou nan yang furu wei ming zhou selective encoding abstractive sentence summarization acl pages long zhou chengqing zong machine translation tacl pages synchronous bidirectional neural jiajun zhang
