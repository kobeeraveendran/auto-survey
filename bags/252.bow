improving abstractive text summarization history aggregation pengcheng chuang xiaojun xiaofei c e d l c s c v v x r abstract recent neural sequence sequence models vided feasible solutions abstractive summarization models hard tackle long text dependency summarization task high quality summarization system usually depends strong encoder rene important information long input texts decoder generate salient maries encoders memory paper propose gregation mechanism based transformer model address challenge long text representation model review history information encoder hold memory capacity cally apply aggregation mechanism transformer model experiment cnn dailymail dataset achieve higher ity summaries compared strong baseline models rouge metrics introduction task text summarization automatically compressing long text shorter version keeping salient information divided approaches extractive abstractive rization extractive approach usually selects sentences phrases source text directly contrary abstractive approach rst understands semantic information source text erates novel words appeared source text extractive rization easier abstractive summarization like way humans process text paper focuses abstractive approach unlike sequence generation tasks language processing machine translation lengths input output text close summarization task ists severe imbalance lengths means summarization task model long distance text dependencies rnns ability tackle time sequence text variants sequence sequence based emerged large scale generate promising results solve distance text dependencies bahdnau et al rst propose tion mechanism allows decoder step refer hidden states rush et al rst incorporate attention anism summarization task attention based models ease problem long input texts summarization task like bahdnau hierarchical based simple celikyilmaz et al segment text encode segmented text independently cast encoding systems promising school cyber security university chinese academy sciences institute information engineering chinese academy sciences jing china jing china corresponding author chuang zhang ac cn exhibit undesirable behaviors producing inaccurate tual details repeating hard decide attend ignore pass encoder modeling effective encoder representing long text challenge committed solving long text dependency problems aggregation mechanism key idea tion mechanism collect history information computes tion encoder nal hidden states history information distribute encoder nal states suggests encoder read long input texts times understand text clearly build model reconstructing transformer incorporating novel aggregation mechanism empirically rst analyze features summarization translation dataset experiment different encoder decoder layers results reveal ability encoder layer portant decoder layer implies cus encoder finally experiment cnn dailymail dataset model generates higher quality summaries compared strong baselines pointer generator transformer models rouge metrics human evaluations main contributions paper follows forward novel aggregation mechanism redistribute context states text collected history information equip transformer model aggregation mechanism model outperforms rouge l scores cnn dailymail dataset rouge l scores build chinese news dataset compared transformer baseline model related work section rst introduce extractive summarization troduce abstractive summarization extractive summarization extractive summarization aims select salient sentences source documents directly method modeled sentence ranking problem selecting sentences high sequence label integer ear models leverage manual gineered features replaced neural network extract features automatically cheng et al sentence sentation convolutional neural document representation recurrent neural select sentences words hierarchical extractor nallapati et al treat summarization sequence labeling task sentence document representations rnns classication layer sentence label indicates tence selected zhou et al present model tive summarization jointly learning score select sentences zhang et al forward latent variable model tackle problem sentence label bias abstractive summarization abstractive summarization aims rewrite source texts standable semantic meaning methods task based sequence sequence models rush et al rst incorporate attention mechanism abstractive summarization achieve state art scores gigaword datasets chopra et al improve model performance rnn decoder nallapati et al adopt hierarchical network process long source text hierarchical structure gu et al rst copy mechanism advantage extractive tive summarization copying words source text extractive summarization generating original words abstractive rization et al incorporate copy coverage mechanisms avoid generating inaccurate repeated words celikyilmaz et al split text paragraph apply encoder paragraph broadcast paragraph encoding recently vaswani et al new view sequence sequence model employs self attention replace rnn sequence sequence model uses multi head attention capture different semantic information lately researchers focus combine abstractive extractive summarization hsu et al build unied model inconsistency loss gehrmann et al rst train selector select mask salient information train tive model pointer generator generate abstractive tion model section rst describe attention mechanism transformer baseline model introduce pointer bpe mechanism novel aggregation mechanism described code model available online notation pairs texts x y x long text y y summary corresponding lengths d y ld ly respectively text d composed sequence words w embed word w vector e represent document d embedding vector eld representation y attention mechanism attention mechanism widely text summarization els produce word signicance distribution source text disparate decode steps bahdanau et al rst propose attention mechanism attention weight distribution calculated whhi bt et attentiont sof com pc liao hi encoder hidden states ith word st decoder hidden states time step t vector v ws wh scalar bt able parameters attentiont probability distribution sents importance different source words decoder time step t transformer redenes attention mechanism concisely practice compute attention function set queries multaneously packed matrix q keys values packed matrices k v k v sof tmax qk dk v transpose function q rndk k rmdk v rmdv r real eld n m lengths query key value sequences dk dv dimensions key value summarization model assume k v self attention dened basic attention q k v multi head attention concatenates multiple basic attentions different eters formulate multi head attention m k v hdi wq kwk wmh learnable parameters wv wk v wv vector transformer baseline model baseline model corresponds transformer model nmt tasks model different previous sequence sequence models applies attention replace rnn transformer model divide encoder decoder discuss respectively input attention dened transformer bag model add extra position information input position encodes heuristic sine cosine tion p dmodel p dmodel pos position word text dimension dex embedding dimension model dmodel network u equal source text word embeddings ew eld added position embeddings ep pld encoder goal encoder extracting features input text map vector representation encoder stacks n encoder layer layer consists multi head self attention position wise feed forward sublayers employ residual tion sublayers followed layer tion multi head attention sublayer extract different semantic information compute encoder layer s nal hidden states position wise feed forward lth encoder layer formulated s el n f f s k l s s s s s v l s s s connection n orm layer normalization function multi head self attention output residual el means figure aggregation transformer model overview compared transformer baseline model apply aggregation layer encoder decoder aggregation layer collect history information redistribute encoder s nal hidden states s v l s k l output encoder layer l s u l s learnable parameters p f f position wise feed forward sublayer sublayer described convolution erations kernel size s k l vector s v l s s scalar s s el decoder decoder generating salient uent text encoder hidden states decoder stacks n decoder ers layer consists masked multi head self attention head attention feed forward sublayers similar encoder employ residual connections sublayers lowed layer normalization lth decoder layer ample use masked multi head attention encode summary vector ms ms ms v l ms k l ms ms n h ms v l ms k l ms ms v l ms k l layers ms egw egp rst layer output l decoder layer egw egp word embeddings position embeddings generated words respectively m h masked multi head self attention mask similar transformer decoder execute multi head attention encoder decoder dl dl kd vd d ms hidden states decoder masked multi head attention kd vd hn el encoder layer output states finally use position wise feed forward layer normalization sublayers compute nal states dl d d d dl n f f d d d vector d learnable parameters projecting decoder nal hidden states vocab size vocabulary probability distribution pvocab scalar d pointer bpe mechanism generation tasks deal generated text problem tackle problem contains limited vocabulary words replaces oovs unk things worse summarization task specic place low frequency key mation summary vocabulary built k words frequent occurrence specic nouns occur vocabulary pointer byte pair encoder bpe mechanism tackle oov problem original bpe mechanism simple data compression technique replaces frequent bytes pair unused byte sennrich et al rst use technique word segmentation merging characters instead bytes xed vocabulary load subwords alleviate problem oov pointer mechanism allows copying words source text generating words xed vocabulary pointer mechanism decoder time step generation probability pgen calculated pgen dl bgen vector wdl scalar bgen learnable parameter hn dl decoder output states compute nal word distribution pointer network sof bcopy pcopy zi ld pf inal pgen pvocabpgen u representation input zi hot indicator vector wi pcopy probability distribution source words pf inal nal probability distribution aggregation mechanism overview model figure enhance memory ity add aggregation mechanism encoder coder collecting history information aggregation nism reconstructs encoder s nal hidden states reviewing tory information forward primitive aggregation proaches proved effective task historyhidden stateattnencoder layerencoder pos pos embeddingoutputshift rightaggregationencoderword embeddingword embedding figure overview projection aggregation mechanism coder layers rst approach connected networks collect torical approach rst goes normal encoder layers outputs layer lect middle l layers outputs concatenate input connected networks obtain history information h hh finally compute multi head attention history state h output encoder layer process formulated hh l el el bh vector wh scalar bh learnable parameters l parameter explored add multi head attention layer encoder layer output hn el history information hh output attention nal states encoder ha m k p v p qp history information hp k p v p hn el ly t n l n index selected encoder layers previous history state k v encoder output hl el iteratively calculating history information selected encoder layer nal history hidden states ha states nal states encoder finally dene objective function given golden mary y yly input text x minimize negative log likelihood target word sequence training objective function described log model parameter n number source summary text pairs training set loss sample added loss generated word yt time step t log x x calculated decoder t time step t total decoding steps experiments section rst dene setup experiment analyze results experiments experimental setup dataset conduct experiments cnn dailymail widely long document marization tasks corpus constructed collecting online news articles human generated summaries cnn daily mail site choose non anonymized placing named entity unique identier dataset contains pairs articles summaries details dataset tion training details conduct experiments nvidia tesla training testing time truncate source text words build shared vocabulary encoder decoder small vocabulary size pointer bpe mechanism word embeddings learned training time use adam optimizer initial learning rate parameter training phase adapt learning rate according loss validation set half learning rate validation set loss going epochs use regulation dropout training process converges steps model generation phase use beam search algorithm duce multiple summary candidates parallel better summaries add repeated words blacklist processing search avoid duplication fear favoring shorter generated summaries utilize length penalty detail set beam size repeated n gram size length penalty parameter constrain maximum minimum length generated mary respectively figure overview attention aggregation mechanism encoder layers second approach attention mechanism collect tory figure select middle l encoder layers outputs iteratively compute multi head attention current encoder layer output previous history information lth history information calculated follows m k v com abisee cnn dailymail encoder layer layer layer layer layer layer layer layer table comparison different model results cnn daliymail test dataset scores rouge l condence interval rst previous abstractive baseline models second transformer baseline model transformer model aggregation mechanism best scores bolded model temp att pointer generator coverage pointer generator coverage cbdec inconsistency loss rnn ext abs rl rerank transformer aggregation rouge l evaluate system f measures rouge l metrics respectively represent overlap gram longest common sequence golden mary system summary scores computed python package experiment explorations explore inuence different experiment hyper parameters setup model s performance includes different experiment settings firstly explore number transformer encoder decoder layers table secondly dig different aggregation methods gregation layer table exploration includes baseline transformer model add tion aggregation attention aggregation thirdly explore different performance different number aggregation layers table groups experiments different number aggregation layers adding transformer projection aggregation method transformer attention aggregation method models exploration encoder decoder layers use encoder decoder layers human evaluation rouge scores widely automatic evaluation summarization great limitations semantic syntax information case use manual ation ensure performance models perform small scale human evaluations randomly select ated summaries generator aggregation transformer randomly shufe order summaries anonymize model identities let mous volunteers excellent english literacy skills score random summaries models range score means high quality summary average score mary nal score evaluation criteria follows salient summaries important point source text ency summaries consistent human reading habits grammatical errors non repeated summaries contain redundancy word results dataset analysis demonstrate difference rization translation tasks compare dataset tasks table summarization dataset cnn dailymail contains org project table comparison translation summarization datasets sentence tags source text split sentences blank count maximal average length token dataset dataset cnn max token avg token abs max token avg token de max token en avg token en en max token avg token train valid test training pairs validation pairs test pairs translation dataset training pairs validation pairs test pairs respectively nd characteristics different tasks comparison summarization source text include words average length source text times longer target text translation task contains words average length source text target text need strong encoder memory ability decide attend ignore quantitative analysis experimental results given ble overall model improves articles scores model gets lower rouge l score rl reinforcement learning celikyilmaz et al rouge l score correlated summary quality model generates novel words compared baselines novelty experiment novel words harmful l scores result account models abstractive figure shows ground truth summary generated maries transformer baseline model aggregation transformer attention aggregation method source text main fragment truncated text compared gation transformer summary generated transformer line model problems firstly summary baseline model lack salient information marked red source text secondly contains unnecessary information marked blue source text hold opinion transformer baseline model weak memory ability compared model remind source national grid revealed uk s rst new pylon nearly years called t pylon artist s illustration shown shorter old lattice pylons able carry power volts designed obtrusive clean energy purposes national grid building training line obtrusive t pylons eakring training academy nottinghamshire britain s rst pylon erected july near edinburgh designed chitectural luminary sir reginald blomeld inspired greek root word pylon meaning gateway egyptian temple campaign unloved run rudyard kipling john nard keynes hilaire belloc ve years later biggest peacetime construction project seen britain connection power stations miles cable completed marked birth national grid major ing nation s industrial engine vital asset second world war ground truth national grid revealed uk s rst new pylon nearly years called t pylon shorter old lattice pylons able carry power volts designed obtrusive clean energy transformer baseline t pylon artist shown shorter old lattice pylons able carry power volts designed obtrusive clean energy purposes model national grid revealed uk s rst new lon nearly years called t pylon shorter old lattice pylons able carry power volts designed obtrusive clean energy purposes figure comparison ground truth summary generated maries abstractive summarization models cnn dailymail dataset red represents missed information blue means unnecessary tion green signify appropriate information information far current states lead missing salient information remember irrelevant information lead unnecessary words generated summaries model uses aggregation mechanism review primitive information enhance model memory capacity aggregation mechanism makes model generate salient repetitive words summaries table compare different layers report results cnn dailymail test dataset precision recall scores rouge e d r r rouge r encoder decoder layers analysis rst exploration iment consists transformer models different encoder decoder layers experiment number coder decoder layers tried encoder decoder layers notable difference encoder decoder layers increasing lot parameters taking time converge transformer baseline model encoder decoder layers decrease layers encoder decoder respectively results shown table concluded comparison model results lower precision higher recall score encoder layers decreasing opposite results decoder layers decreasing experiments higher score lower l scores model decreasing decoder layer compared decreasing encoder layer conclude encoder captures features source text decoder makes summaries consistently table aggregation mechanism experiments experiments use aggregation methods different aggregation layers model layer layer layer layer layer layer rouge l aggregation mechanism analysis second exploration periment consists baseline aggregation transformer model different aggregation table use baseline model adding l result scores decrease tion simply adding l distribute encoder nal states history states average portance weights layers maybe things worse compared baseline model result scores gation boosting compute attention encoder nal value distribute nal states encoder obtains ability fusing history information different importance exploration contains groups experiments add projection attention aggregation transformer models use different gation layers experiment model groups aggregation layers extraordinary low rouge scores models rouge l roughly incorporate output rst encoder layer semantic information harmful distributing encoder nal states compare models explicitly add aggregation group increase added layers rouge scores add layers nal state distributions tend uniform distribution makes decoder confused key ideas source text reason worse scores add layers projection aggregation group increase aggregation layers rouge scores rise aggregate layers history states contain information lead performance improvement lose lot tion aggregation layers increasing achieve best result aggregation layers attention aggregation group best score aggregation layer rouge scores decline increase aggregation layers need layer attention focus history states attention layers sive dependency history states encoder nal distribution focus shallow layers introduced lot useless mation harmful encoder capture salient features figure statistics novel n grams sentences model generate far novel n grams sentences pointer generator transformer baseline abstractive analysis figure shows model copy sentences source texts copy rate close reference summaries huge gap n grams generation main area improvement particular pointer generator model tends examples novel words summaries lower rate novel words generation transformer baseline model generate novel summaries model great improvement novelty improvement n compared transformer baseline model model reviews history states distribute encoder nal states accurate semantic representation proves gation mechanism improve memory capability encoder table human evaluation models compare average score salient uency non repeated best scores bolded model pointer generator pointer generator coverage transformer transformer aggregation salient fluency non repeated human evaluation conduct human evaluation setup section results table compared models salient uency non repeated criteria model gets highest score criteria uency criterion models scores means hard understand semantic information models pointer generator baseline abstractive summarization approach lowest scores pointer generator uses coverage mechanism avoid generating overlap words summaries uent repetitive transformer new abstractive tion based attention mechanism better performance pointer generator model equip transformer model aggregation mechanism great improvement criteria chinese experiments build chinese summarization dataset crawling news process raw web page contents character based texts details dataset table dataset similar average length source texts summaries compared cnn dm dataset temporary dataset contains pairs text totally adding data dataset table experiments chinese dataset experiment baseline models evaluate results rouge f metrics best scores bolded model pointer generator pointer generator coverage transformer transformer aggregation rouge l experiment chinese dataset evaluate result metrics model gets highest score pointer generator model gets high rouge scores table dataset contain novel words suitable pointer generator model dataset contains novel novel sentences comparison novel n gram sentences frequency cnn dm figure tively pointer generator model generates summaries taining novel words sentences leads high scores chinese dataset finally compare model baseline model results improve rouge l scores conclusions paper propose new aggregation mechanism transformer model enhance encoder memory ability addition aggregation mechanism obtains best performance compared transformer baseline model pointer tor cnn dailymail dataset terms rouge scores plore different aggregation methods add projection attention methods attention method performs best explore performance different aggregation layers improve best score build chinese dataset summarization task statistics table proposed method achieves best performance chinese dataset future explore memory network collect history information try directly send history information coding processing improve performance summarization task aggregation mechanism transferred eration tasks acknowledgment work supported national natural science tion china grant thepaper com tensorflow master utils rouge py gramsentencenovelty pointer generatortransformerour modelground truth references dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate arxiv preprint asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents abstractive summarization arxiv preprint yenchun chen mohit bansal fast abstractive summarization reinforce selected sentence rewriting arxiv computation guage jianpeng cheng mirella lapata neural summarization ing sentences words arxiv preprint sumit chopra michael auli alexander m rush abstractive tence summarization attentive recurrent neural networks ceedings conference north american chapter association computational linguistics human language nologies pp john m conroy dianne p oleary text summarization hidden markov models xiangyu duan mingming yin min zhang boxing chen hua luo zero shot cross lingual abstractive sentence summarization teaching generation attention proceedings annual meeting association computational linguistics pp jonas gehring michael auli david grangier denis yarats yann n dauphin convolutional sequence sequence learning international conference machine proceedings learning volume pp jmlr org sebastian gehrmann yuntian deng alexander m rush abstractive summarization arxiv preprint jiatao gu zhengdong lu hang li victor o k li incorporating copying mechanism sequence sequence learning caglar gulcehre sungjin ahn ramesh nallapati bowen zhou yoshua bengio pointing unknown words arxiv computation language han guo ramakanth pasunuru mohit bansal soft layer specic multi task summarization entailment question generation arxiv computation language karl moritz hermann tomas kocisk edward grefenstette lasse peholt kay mustafa suleyman phil blunsom teaching chines read comprehend arxiv computation language wanting hsu chiehkai lin mingying lee kerui min jing tang min sun unied model extractive abstractive summarization inconsistency loss yichen jiang mohit bansal closed book training improve marization encoder memory panagiotis kouris georgios alexandridis andreas stafylopatis abstractive text summarization based deep learning semantic content generalization julian m kupiec jan o pedersen francine r chen trainable document summarizer logan lebanoff kaiqiang song franck dernoncourt doo soon kim seokhwan kim walter chang fei liu scoring sentence tons pairs abstractive summarization arxiv computation language chin yew lin rouge package automatic evaluation maries hui lin vincent ng abstractive summarization survey state art junyang lin xu sun shuming ma qi su global encoding abstractive summarization arxiv computation language linqing liu yao lu min yang qiang qu jia zhu hongyan li generative adversarial network abstractive text summarization arxiv computation language yang liu mirella lapata hierarchical transformers document summarization yang liu mirella lapata text summarization pretrained coders arxiv computation language konstantin lopyrev generating news headlines recurrent neural networks arxiv computation language takuya makino tomoya iwakura hiroya takamura manabu mura global optimization length constraint neural text marization edward moroshko guy feigenblat haggai roitman david konopnicki editorial network enhanced document tion arxiv computation language ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive rization documents arxiv computation language ramesh nallapati bowen zhou mingbo ma classify select neural architectures extractive document summarization arxiv computation language ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre bing xiang abstractive text summarization sequence sequence rnns shashi narayan shay b cohen mirella lapata ranking tences extractive summarization reinforcement learning myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier michael auli fairseq fast sible toolkit sequence modeling tatsuro oya yashar mehdad giuseppe carenini raymond t ng template based abstractive meeting summarization leveraging summary source text relationships alexander m rush sumit chopra jason weston neural tion model abstractive sentence summarization arxiv tion language abigail peter j liu christopher d manning point summarization pointer generator networks rico sennrich barry haddow alexandra birch neural machine translation rare words subword units arxiv computation language ilya sutskever oriol vinyals quoc v le sequence sequence learning neural networks advances neural information cessing systems pp sho takase jun suzuki naoaki okazaki tsutomu hirao masaaki nagata neural headline generation abstract meaning tion jiwei tan xiaojun wan jianguo xiao abstractive document summarization graph based attentional neural model ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez lukasz kaiser illia polosukhin attention need arxiv computation language oriol vinyals meire fortunato navdeep jaitly pointer networks kristian woodsend mirella lapata automatic generation story highlights kaichun yao libo zhang tiejian luo yanjun wu deep inforcement learning extractive document summarization computing yongjian weijia jia tianyi liu wenmian yang improving abstractive document summarization salient information ing wenyuan zeng wenjie luo sanja fidler raquel urtasun cient summarization read copy mechanism arxiv computation language haoyu zhang yeyun gong yu yan nan duan jianjun xu ji wang ming gong ming zhou pretraining based natural language eration text summarization arxiv computation language xingxing zhang mirella lapata furu wei ming zhou neural latent extractive document summarization xingxing zhang furu wei ming zhou hibert document level pre training hierarchical bidirectional transformers document summarization qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural document summarization jointly learning score select sentences
