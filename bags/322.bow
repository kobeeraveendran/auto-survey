noname manuscript no will be inserted by the editor image conditioned keyframe based video summarization using object detection neeraj baghel suresh c raikwar charul bhatnagar p e s v c s c v v i x r a received date accepted date abstract video summarization plays an important role in selecting keyframe for understanding a video tionally it aims to nd the most representative and diverse contents or frames in a video for short maries recently query conditioned video tion has been introduced which considers user queries to learn more user oriented summaries and its ence however there are obstacles in text queries for user subjectivity and nding similarity between the user query and input frames in this work i image is troduced as a query for user preference a ematical model is proposed to minimize redundancy based on the loss function summary variance and iii the similarity score between the query image and input video to obtain the summarized video more the object based query image oqi dataset has been introduced which contains the query images the proposed method has been validated using ut tric ute dataset the proposed model successfully resolved the issues of i user preference ii recognize important frames and selecting that keyframe in daily life videos with dierent illumination conditions the proposed method achieved average score for ute dataset and outperforms the existing state of art by the process time is times faster than actual time of video experiments on a recently proposed ute dataset show the eciency of the posed method keywords image key frame object detection query video summarization neeraj baghel suresh c raikwar charul bhatnagar department of computer engineering and applications gla university mathura india e mail com introduction the video capable mobile devices becoming increasingly ubiquitous there is an analogous increase in the amount of video data that is captured and stored additionally as the diculty of capturing video the cost of storage decreases and tends to see a corresponding increase in the quality of captured videos as a result of this it becomes very dicult to watch or discover interesting video clips among the vast amount of data one tion to this problem is to lies in the development of a video summarization system which can automatically locate these interesting clips and generate a nal rated video summary in current years interest in video summarization is increased due to a large amount of video data tainly professionals and consumers both have access to video retrieval nowadays the image contains a large amount of data whereas the video is a collection of ages which contains huge information and knowledge thus it is very hard for users to watch or discover the incidents in it quick video summarization methods able us to speedily scan a long video by removing evant and redundant frames the video summarization is a tool for creating a compact summary of a video it can either be an der of stable images keyframes or motion pictures video skims the video can be summarized using i keyframes and video skims the keyframes are used to represent important formation contained in video these are also named as r frames representative frames still image synopses and a collection of prominent images obtained from the video data the challenges in selecting keyframes are as follows i redundant frames are selected as a keyframe dicult to make a cluster when content is neeraj baghel et al dened the loss function for obtaining a selection score for frames in a video the object based query image oqi dataset is prepared for the selection of a query image an adaptive method to compute threshold is ned for the selection of keyframes using standard deviation in addition to this primary contribution parts of the proposed work also serve as relevant contributions in isolation these include perform video summarization on commodity hardware the remaining structure of this paperwork is as lows provides the related work of video marization describes the datasets used and its setting describes the proposed ical model of the problem and its solution by calculating loss function also flowchart and its working consist of video segmentation feature extraction frame scoring and summary generation describes the plementation details evaluation metrics experimental results obtained by the proposed model cuss about gui tool for summary generation in the last discussion about the conclusion of the proposed model related work although video summarization has been an active search topic in computer vision nowadays the high computational capabilities of modern hardware allow us to process a video in a fraction of the time if quired which when combined with the evolution of modern vision techniques such as deep neural networks has resulted in a signicant increase in the breadth of techniques which are viable to apply to the topic of video summarization combined with the vast quantity of prior work involving video summarization many teresting research prospects are available to pursue although the primary focus is video summarization some of the steps performed during the proposed work need a signicant amount of prior research among these query focused video summarizing which simply deals with taking a video and dividing it up into a ber of segments and image and video feature extraction related to the query which deals with extracting vant and useful features from images and videos in author has proposed a new technique on video summarization which is query focused that includes user perspective based on text queries related to video into the summarization method this is a promising way to personalize video summary the author lects user annotations and meets the issues of good fig video summarization workow video and image are selected from their respective datasets further various techniques are applied to generate the selection matrix on behalf of the selection matrix summarised video is formed non identical the video skims represents moving story borad of a video the video is split into many tions that are video clips with a smaller length every portion follows a part of a video or a regular result the trailer of a movie is an example of video skimming finally dierent viewers will have dierent ences on what they nd interesting in a video with traditional hand editing viewers only see the segments deemed interesting by the editor and the time cost for a human editor to create multiple edits of a single video is signicant the ability to generate multiple possible summaries rather than just a single summary would be a very useful feature for a video summarization system to have as would be the ability to learn a specic users preferences over time the proposed framework has taken an image query for user preference and uses both global local tures to learn user preference from that image query to generate video summary eciently object detection is used major user preference based on objects as local features while a salient region is used for focused area colours in an image as global features the objective of the proposed work is to resolve the issue of the wrong selection of keyframe with a ematical model to minimize redundancy based on ilarity score between query image and frames of the input video to obtain the summarized video hence the proposed method has used the following features to measure the degree of similarity between an input query image and video i local features based on ject detection and its details global features based on salient regions the primary contributions of the presented work are a mathematical method to calculate summary ance to reduce redundancy between frames image conditioned keyframe based video summarization using object detection table related work to video summarization table dataset used for experiments evaluation measures for system generated summaries to user labeled summaries authors contribution i collect dense tags for the dataset a memory work using a sequential determinantal point process query focused video summarization the author uses the memory network to take user queries for a video within each shot onto dierent frames in query conditioned summarization is introduced using a three player generative adversarial network where generator is used to learn the joint representation of query and video the discriminator takes three dierent summaries as input and discriminate the real summary from the other two summaries which are randomly erated generic video summarization has been studied for global keyframes and ecient analysis of video for shot level summarization song et al nding important shots using learning of sual concepts shared between videos and images in distinguish highlight segments from non highlight ones by using a pair wise deep ranking model in frame level video summarization khosla et al use web based images before video summarization in model is used for learning of quential structures and generating further summaries object level video summarization extracts jects to perform summarization also there are two gan based networks that include adversarial training however user preferences is not considered so summaries may not generalize and robust for ferent users therefore the video summarization based on query conditioned came into focus and provide more personalized summary to user query conditioned video summarization takes queries given by user into consideration which are in the form of texts to learn and generate user oriented summaries to tackle this challenge trained a quential and hierarchical dpp sh dpp in to pick relevant and representative frames adopt a aware relevance model specically oosterhuis et al generate visual trailers based on graph based method for selecting most relevant frames to a given user query ji et al mulate incorporating web images task obtained from user query searches recently sharghi et al instead of using generic task datasets for query conditioned task they propose a new dataset its evaluation metric and a technique based on this new dataset they propose an adversarial network that summarize videos based on user queries and does not rely on external source web images datasets and settings in the proposed methodology experiments are plished on the prevailing ut egocentric ute dataset the dataset contains four videos each hours long completely dierent uncontrolled daily life situations a lexicon is provided for user queries consists of ous set of forty eight ideas based on daily life as for the queries four completely dierent situations videos are enclosed to formalize comprehensive queries also follow three scenarios which are i queries wherever all ideas seem along within the same video shot ii queries where all ideas seem however not within the same shot and iii queries wherever just one of the concepts seem also introduced conjointly created a tiny based query image oqi dataset of pictures for the testing image as a question and object detection and feature extraction during this dataset various pictures of totally dierent eighty categories every category has quite ve pictures images will have quite one totally dierent objects the proposed dataset got impressed by the coco dataset and conjointly contained tures are taken by us neeraj baghel et al fig mathematical model where represents a feature of that frame and z represents the selection value of that frame sv where sv is summary variance sq is distance score tr is trace function and are parameters to control the importance of each term so variance sv for the summary of a video is dened as n n thus by using trace of can be written as t zixt i xi zt i xixt zi by placing all n frames together using stacked variable it can be written as t z t p where z p t x and q xx t distance matrix d is dened as d dn v q where d is the cumulative distance function dened as q where q is a feature of the query image f v is a feature of video frames and is distance function m is feature no therefore distance score sq for the summary of a video can be dened as q m m v sq ddt z thus its trace can be written as t rz where r d t by putting the trace values of summary variance t and distance score t in the loss function z t z z t for solving used method same given the loss function represented by where and g are convex functions by utilized a well known cccp concave convex procedure algorithm to solve it where is z t z and is z t in this the loss function can be decomposed into the dierence of two convex functions in tth tion it will converge the values of z which lies between and as denoted as zm in zm values lies between and where values close to are for important frames while close to represent unrelated frame then threshold is required for selecting important frames calculated as the dard deviation as the values are gaussian distributed convert zm to z on the basis of adaptive threshold proposed method video summarization using object detection is a great challenge in the eld of articial intelligence for the chines as well as the programmers to train machines in such a way that it recognizes the keyframes cally the proposed method facilitates query conditioned video summarization by extracting an object s detail local feature and salient region global feature ture extraction it takes the image as a query into sideration with its features recognizes that same feature into the video frame the framework of the proposed approach is shown in fig the rst part extracts jects and its visual feature in order to provide hensive summary for the given image additionally introduced a small object based query image dataset of images for the testing image query in this dataset there are images of dierent classes each class has more than images image can have more than one dierent objects this dataset has spired by common objects in context coco dataset and also contained some other images which are taken by us the proposed method has also tested these on dierent videos and images that show this proposed model not restricted query image and videos to these datasets mathematical model consider x be a feature matrix for a video with n segment frame features the feature representation of the summary for video dened as vs x t z where z is selection matrix z and zn is the selection variable the model is based on view point in multiple videos loss function is dened by diverse content same group videos and dierent group videos whereas proposed method used dierent distance functions based on similarity between image query and input video objective to nd the values of zn in order to imize the loss function t t image conditioned keyframe based video summarization using object detection fig video segmentation frames are extracted from lected video and various pre processing techniques are plied feature extraction feature extraction methods should be video summary oriented the features are rst extracted from the query image and each frame of input video then aggregated within each segment xn to obtain a feature vector x the proposed framework has taken both global and cal features to generate video summary eectively for local features the proposed work utilized object tection the objective of object detection is to nize all occurrences of objects from a recognized sication similar to people vehicles or faces in an age an item extraction module is intended to provide the thing division strategy that furthermore gives a pendable contribution to the pursuit instrument the proposed method has utilized you only look once yolo method with dierent classes for nition and trained on coco dataset the classes considered in the proposed method are person umbrella tie backpack handbag suitcase cycle motorcycle bus truck car airplane train boat trac light stop sign bench re hydrant parking meter bird dog sheep elephant zebra cat horse cow bear girae frisbee snowboard kite baseball glove board skis sports ball baseball bat skateboard nis racket bottle cup knife bowl wine glass fork spoon banana sandwich broccoli hot dog donut apple ange carrot pizza cake chair potted plant dining table couch bed tv toilet mouse keyboard laptop remote cell phone toaster microwave refrigerator oven sink book vase teddy bear toothbrush clock scissors hair drier the yolo is one of the faster object detection methods the proposed method has used the trained model on the coco dataset the output for every frame using yolo consists of three lists each sentation are dimensional vectors makes predictions at three scales by down sampling the size of the input image into blocks of size and blocks however the proposed method has used only and blocks because very small objects are not require if the prediction condence score is generated if more than then the object is extracted from that frame then the output of these two feature vector and object category is matched with other frames of a given video with condence threshold and non maximal fig framework for video summarization contains four major steps video segmentation feature extraction frame scoring and summary generation value values having greater then threshold values will be considered as while rest are therefore selection matrix z is dened as z zm where is a standard deviation from zm at last generated summarized video by accumulated keyframe dened by selection matrix which can be ned as vs x t where x is feature matrix and z is selection matrix further the owchart of the proposed model the owchart of the proposed model of video rization is shown in fig the proposition is broadly classied into these phases a video segmentation feature extraction c frame scoring and summary generation video segmentation video segmentation is the signicant step in almost all video processing systems where for a target input video v a segmentation vn is generated this phase includes a pre processing step to eliminate undesirable frames in the pre processing step i frames are extracted from the input video ii histogram equalization is applied iii resizing of images to a default size iv ing an unnecessary part from all images extract frames per second and vi total no of frames from the input video each video of ute dataset v is partitioned into n frames seconds long for fair comparison with related work where n is the total number of frames casings second long is utilized outline extraction technique to compare with existing work neeraj baghel et al fig global feature extraction a processed query image b salient region on query image c processed video frame d salient region on video frame is calculating cumulative distance between the query image and input video a cumulative distance is a lection of various distance calculated between features of the query image and frames of the input video the proposed work have four distance function for i ferent objects ii location of the objects iii size of the objects iv salient region v v q v v denoted as the cumulative distance of ith frame distance function as a feature of query image as q and feature of the input video frame as f v also proposed the cumulative distance function as q q q v where q is an absolute value of the dierence tween the number of dierent objects in the query age and video frame q v is a summation of the dierence between location of a similar object in the query image and video frame q is a tion of the dierence between size of a similar object in the query image and video frame and then divide by total number of pixel and q is a summation of dierence between a salient region of the query image and video frame and then divide by total no of pixel v f v the second step is to generate a distance matrix with the collection of cumulative distance function with respect to every frame of the input video distance trix d is described as d dn where d is the cumulative distance between the query image and input video the third step is to calculate the values of p q and r as described p t x q xx t and r ddt where diag is a diagonal matrix x is a feature matrix d is a distance matrix the fourth step is to nd values of selection score which lies between and with the help of gence of concave convex procedure by minimizing the optimized equation of loss function in tth iteration fig local feature extraction a processed query image b object detection on query image c object extracted from query image d processed video frames e object detection on video frames object extracted from video frame suppression threshold finally the proposed method uses these various object properties as local features for global features the proposed method is using the salient region the objective of the salient region is to nd important regions in the video frames color bination and depth of that region recognize the portant region of image from the hsv color model author has used hsv color model to nd salient region in stereo images by using subtraction of left and right images with saturation and value components of hsv color space to obtain a salient region in mono age proposed methord used the hsv color model using functions sr s where v is value plane s is the saturation plane is parameter threshold to select the region exp is nential function and sr is the salient region the value of is always lie between to and the value is as much close to function select more salient pixel from the age however it may skip some pixel which may salient but due to some noise its value is less the value of empirically by testing this on various images of gqi dataset to be frame scoring in frame scoring various steps are used to generate the similarity score of the input video frame first the step image conditioned keyframe based video summarization using object detection fig salient region dierence a salient region on query image b salient region on video frame c dierence of salient region between query image and video frames fig keyframe selection a video frames selection trix c selected keyframe key frame selection keyframe selection is the last step in video tion where the important frame is selected from input video and generates the output video of those frames a keyframe is selected on the basis of similarity score in the proposed method rst selection matrix is ated which takes the decision of selection of keyframe the selection matrix is generated on the basis of the selection score where a threshold is applied to the lection score if the value is greater than the threshold then considered as keyframe otherwise that frame is discarded the threshold is calculated as the standard deviation as the values are gaussian distributed fore selection matrix z is dened as z zm where is a standard deviation from zm at last summarized video is generated by lated keyframe dened by selection matrix which can be dened as vs x t z where x is a feature matrix and z is a selection matrix implementation details the proposed video summarization system is mented in python at gtx ti gb card with gb on a single server work station in the rst part object detection is used the output for a frame consists of three lists each representation are dimensional vectors yolo makes predictions at scales that square measure exactly given by sampling the size of the input image by thirty two teen and eight severally in the second module if the prediction condence score is generated if more than then the object is extracted from that frame then output of these three feature vectors and object category is matched with other frames of a given video with condence old and non maximal suppression threshold in the third module accumulate the frame sequence and generate summarized video during the testing phase the proposed method obtain the feats consist of frameid indices class classid condence of predicted shot and score for each video shot experimental results and discussion in this chapter the proposed method has been dated the approach using ut egocentric ute dataset the proposed method successfully resolved the issues of i user preference ii recognize important objects in frames according to user preference and selecting keyframe in daily life videos with dierent illumination conditions experiments is performed on ut egocentric dataset shows the eciency of the proposed method evaluation metrics in qc dpp the mapping between the predicted summary and the ground truths is proposed with partite graph bipartite using weight matching also gives a similarity function between two video shots by using intersection over union iou on corresponding ideas to calculate the performance the iou is dened using edge weights therefore the predicted summary neeraj baghel et al fig result analysis score of the proposed work with seq dpp sh dpp qc dpp and ground truth belongs to dierent sides of tite graph precision pre recall rec and score area unit computed as follows total no of frames in summary total no of frames in ground truth dist distance summary ground truth pre rec pre rec quantitative results the proposed approach is compared with all other works which have been applied to ute dataset sion recall and score parameters are taken for parison for all four videos are shown in table tively the proposed method achieved average score for ute dataset it can be observed that the proposed approach outperforms the existing approach by such substantial improvement in the mance indicates the superiority of the proposed method by using an object detection method with other visual information and the image query the rest four works are based on architecture which can take a long time to learn temporal relations among video shots and queries however the proposed work facilitates key short tion using relation between video and image query to obtain a result comparison fast with ground truth values the proposed method has generated an vector list along with every summarized video so that it can compare this vector list with ground truth values the results and analysis of query conditioned video summarization proposed method using parameters pi precision ri recall and score the process time is also calculated for all four videos and compared with actual timing of videos proposed method is times less than actual time of video hence by using this method we can summarize a video in very less time as compared to manually by watching fig the average processing time on ute dataset actual time is the time length of videos process time is the time taken by proposed method for generating video summary that compact and representative summaries can be nd by the proposed method conclusion qualitative results the visual results obtained by the proposed method are shown in fig the proposed method uses an image query that contains two dierent objects person and car using or operation the axis represents the shot to video ground truth is denoted by blue lines for the given user query while the green lines in the bottom represents predicted key shots of the proposed method note that predicted summaries can be related to one or more details given a user query it can observe in the proposed work a mathematical model is sented to minimize redundancy based on the similarity score between the query image and input video to tain the summarized video the mathematical model contains a method to calculate the summary variance to reduce redundancy between frames a mathematical formula to calculate distance score between a query age and video frames is also presented the presented work dened a loss function for obtaining a selection score for frames in a video the proposed method sumes that the distribution of the keyframes is based image conditioned keyframe based video summarization using object detection table comparison of proposed work with previous work and a memory network based approach in proceedings of the ieee conference on computer vision and pattern recognition pages ke zhang wei lun chao fei sha and kristen man summary transfer exemplar based subset tion for video summarization in proceedings of the ieee conference on computer vision and pattern recognition pages yen liang lin vlad i morariu and winston hsu marizing while recording context based highlight tion for egocentric videos in proceedings of the ieee ternational conference on computer vision workshops pages yujia zhang michael kampmeyer xiaodan liang min tan and eric p xing query conditioned three player arxiv adversarial network for video summarization preprint aditya khosla raay hamid chih jen lin and neel sundaresan large scale video summarization using image priors in proceedings of the ieee conference on computer vision and pattern recognition pages gunhee kim leonid sigal and eric p xing joint marization of large scale collections of web images and videos for storyline reconstruction in proceedings of the ieee conference on computer vision and pattern recognition pages ke zhang wei lun chao fei sha and kristen man video summarization with long short term ory in european conference on computer vision pages springer zheng lu and kristen grauman story driven rization for egocentric video in proceedings of the ieee conference on computer vision and pattern tion pages yale song jordi vallmitjana amanda stent and jandro jaimes tvsum summarizing web videos using titles in proceedings of the ieee conference on puter vision and pattern recognition pages ting yao tao mei and yong rui highlight detection with pairwise deep ranking for rst person video marization in proceedings of the ieee conference on computer vision and pattern recognition pages boqing gong wei lun chao kristen grauman and fei sha diverse sequential subset selection for supervised video summarization in advances in neural information processing systems pages yujia zhang michael kampmeyer xiaodan liang min tan and eric p xing query conditioned three player adversarial network for video summarization arxiv preprint fig visualization results for the proposed method with shot number on the axis in video ground truths and predicted key shots are shown in blue and green lines respectively these results are for the query person or car on gaussian distribution thus an adaptive method to compute threshold is dened for the selection of keyframes using standard deviation the object based query image oqi dataset is prepared for a selection of query images the proposed model successfully solved the issues of i user preference ii recognize important frames and selecting that keyframe in daily life videos with dierent illumination conditions the proposed method achieved a average score for the ute dataset a video pre processing process which makes use of very low level features to eciently locate undesirable frames then uses these to compute optimal segments the processing time is times less than the actual time of video future work will be a cus on increasing local and global features to improve user subjectivity conicts of interest the authors declare no ict of interest references ravi kansagara darshak thakore and mahasweta joshi a study on video summarization techniques ternational journal of innovative research in computer and communication engi neering ba tu truong and svetha venkatesh video tion a systematic review and classication acm actions on multimedia computing communications and applications tomm aidean sharghi jacob s laurel and boqing gong query focused video summarization dataset evaluation neeraj baghel et al jingjing meng hongxing wang junsong yuan and peng tan from keyframes to key objects video marization by representative object proposal selection in proceedings of the ieee conference on computer vision and pattern recognition pages yujia zhang xiaodan liang dingwen zhang min tan and eric p xing unsupervised object level video marization with online motion auto encoder pattern recognition letters behrooz mahasseni michael lam and sinisa todorovic unsupervised video summarization with adversarial lstm networks in proceedings of the ieee conference on computer vision and pattern recognition pages zhong ji yaru ma yanwei pang and xuelong li query aware sparse coding for multi video tion arxiv preprint harrie oosterhuis sujith ravi and michael arxiv preprint semantic video trailers dersky aidean sharghi boqing gong and mubarak shah query focused extractive video summarization in ropean conference on computer vision pages springer arun balajee vasudevan michael gygli anna volokitin and luc van gool query adaptive video summarization via quality aware relevance estimation in proceedings of the acm international conference on multimedia pages acm tsung yi lin michael maire serge belongie james hays pietro perona deva ramanan piotr dollar and c lawrence zitnick microsoft coco common objects in context in european conference on computer vision pages springer a kanehira l van gool y ushiku and t harada viewpoint aware video summarization in proceedings of the ieee conference on computer vision and tern recognition salt lake city ut usa pages alan l yuille and anand rangarajan the in advances in neural convex procedure cccp mation processing systems pages alan l yuille and anand rangarajan the convex procedure neural computation joseph redmon and ali farhadi an incremental improvement arxiv preprint zhong liu weihai chen yuhua zou and cun hu gions of interest extraction based on hsv color space in ieee international conference on industrial matics pages ieee
