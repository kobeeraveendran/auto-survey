abstractive text classication sequence convolution neural networks taehoon kim jihoon yang data mining research laboratory department computer science engineering sogang university taehoonkim abstract propose new deep neural network model training scheme text classication model sequence convolution neural consists blocks sequential block summarizes input texts tion block receives summary input classies label trained end end classify length texts preprocessing puts xed length present gradual weight method stabilize training gws applied model loss function compared model word based textcnn trained different data preprocessing methods obtained signicant improvement classication accuracy word based textcnn ensemble data augmentation code able com tgisaturday introduction humans began record information form text necessary classify manage information certain category store retrieve information efciently need encouraged researchers develop good text classication technique assign predened categories kinds text document emails news articles reviews patents commercial world text classication techniques nave bayes support vector elds including spam ltering news categorization sentiment analysis recent development deep neural achieving excellent results extracting information text classifying certain classes convolutional neural achieved remarkable results computer researchers applied cnns text showed excellent results training cnns pretrained word character level hyperparameter tuning similar outperforming results compared text classication models textcnns performance text classication remarkable applied data input xed size number parameters textcnn determined length input text researchers crop pad input texts certain length train textcnn result information loss classifying longer texts cause performance degradation section performance textcnns improved training model summaries input text ways generate summary text extractive preprint work progress summarization mere selection existing sentences extracted source abstractive summarization compressed paraphrasing main contents source potentially vocabulary unseen source methods change texts lengths texts xed length maintaining important features source texts graph based ranking model extractive text summarization textrank gives ranking sentences text allowing extract short summaries training corpora textrank widely summarizing structured text like news articles researchers worked sequence sequence recurrent neural networks model abstractive text summarization attention lows neural networks focus different parts input rnns showing signicant results task abstractive paper introduce sequence convolution neural model consists blocks sequence block convolution block sequence block based attentional encoder decoder recurrent neural summarizes input texts feeds lution block convolution block based classies input texts certain classes summaries provided sequential block blocks share non static word embedding layer encouraging collaborate performance improvement simply connecting blocks train single end end procedure guarantee optimal results sequential block generate proper summaries early stages training solve problem propose new training scheme gradually shifts tuning summarization task tuning classication task training progresses model implemented code available com tgisaturday related work similar approach text classication summaries latent semantic extractive summarization proposed hybrid model unlabeled document classication svm classier classication rules generated summaries training documents directly compare performance domain difference training data discuss performance textcnn trained extractive summaries generated mnih came novel rnn models visual attention capable extracting information image video adaptively selecting sequence regions idea attention mechanism successfully applied machine translation bahdanau bahdanau improve performance sequential block approach use attentional encoder decoder rnns abstractive summarization closely related nallapati rst use rnns attention model abstractive text summarization implementation sequential block similar pan annotated english dataset train model achieved state art results june require training set input text corresponding example summary datasets evaluation example summary generate extractive summaries input texts feed sequential block example summary chose generate practical summaries short texts require training corpora architecture convolution block based textcnn model proposed rst use cnn text classication model baseline applied batch normalization convolution layer changed hyperparameters optimization training scheme closely related proposed faster cnn scheme alternates tuning different tasks tried train model training scheme alternates tuning summarization task classication task effective summaries generated early stages training lled series unk unknown word tokens instead training scheme gradually changes focus training summary generation text classication summaries generated early stages training lower classication accuracy convolution block model figure depicts overall structure model figure overview model consists sequential convolution blocks interact word embedding layer vectorized representation words summarization classication tasks sequential block baseline model sequential block corresponds attentional encoder decoder rnn model nallapati encodes source sentence xed length vector decoder generates abstractive summaries encoder consists bidirectional decoder consists uni directional long short term memory hidden units encoder decoder bahdanau attention mechanism inserted modules lstm layers regularize forward lstm encoder reads input sequence ordered backward lstm reads sequence reverse order way xed length vector encoder contains summaries preceding words following words help attention mechanism decoder decides parts source sentence pay attention focus vectors essential summarization convolution block structure convolution block based textcnn model proposed gets vectorized representation text number words inside text dimension size word embedding lter windows varying extracts feature performing convolution operation input apply max time pooling operation model uses multiple lters multiple features features passed fully connected softmax layer output probability distribution labels convolution block gets vectorized representation summaries generated sequential block rectied linear non linear activation function lter windows lters applied batch convolution layer batch accelerates training reducing internal covariate change distribution network activations change network parameters training applied batch normalization vectorized representation summaries stabilize entire training procedure reducing internal covariate shift sequential block convolution block regularization inserted module max pooling layer fully connected layer word embedding block word embedding block consists word embedding layer stores vectorized representations word vocabulary lookup table maps word corresponding vector representation vocabulary dictionary extracted words training data minimum word excluding words appeared times word embedding layer non static tuned propagation implementation set word embedding dimension loss function main objective classify texts lengths losing important features original context feature extraction use abstractive summarization method sequential block model mainly focused classication quality summary guaranteed convolution block successfully perform classication task taking consideration trained model minimize objective function weighted sum losses classication summarization loss function text dened lvocabj pwj index text mini batch size pyi predicted probability text classied ground truth label classication loss lcls cross entropy loss classication classes summarization loss lsum sequence loss summary output sequential block length summary example pwj predicted probability jth word generated summaryto match jth word summary example dened sequence loss average vocabulary losses lvocabs vocabulary loss jth word lvocabj cross entropy loss vocabulary dictionary stored word embedding block total loss weighted sum lcls lsum balancing weight normalized implementation multi class softmax layer pyi pwj normalized loss mini batch size applied gradual weight shift explained gradual weight shift section gradual weight shift implementation train sequential block convolution block end end propagation gradient optimizer loss function dened training scheme tunes model reduces training time compared training model independently constant value balancing weight caused sudden drops validation accuracy later stages training epochs found main cause phenomenon sequential block earlier stages training epochs sequential block generate practical summaries omits unk tokens instead huge difference quality summary training hinders optimization convolution block larger solves problem little bit giving weight lsum making sequential block converge faster larger gives weight lsum lcls leading model focus optimization sequential block end model designed classication gradually shifted weight lsum lcls exponentially decaying time exponential decay function dened initial value lambda decay rate current time step stabilize training model achieve higher test accuracy applying gradual weight shift loss function dened section explain result section sharing word embedding summarization classication designed model share word embedding summarization classication propagation happens sequential block tries update word embedding layer direction minimizing sequence loss hand convolution block tries update word embedding layer direction minimizing classication loss tunes word embedding layer optimization implementation trained model end end propagation stochastic gradient adam learning rate decayed epoch exponential rate dropout rate modules gradient gradient norm limited set loss balancing weight training news datasets initialized convolution layers initializer fully connected layer initializer weights initialized random values uniform distribution minimum maximum implementation trained model single nvidia titan gpu mini batch size took news answers training ensemble data augmentation techniques detailed information datasets given section experiments evaluated performance model comparing basic dened basic textcnn experiments vanilla cnn windows lters vanilla cnn convolution block trained vanilla cnn different data preprocessing methods text summarize dened length text number words text text default data preprocessing method removed unnecessary characters stopwords input texts padded xed length pad token xed length maximum length text dataset preprocessing method applied summarize xed length inside bracket table cropped text xed length example text longer words words starting texts shorter xed length padded pad token summarize instead cropping sentence generated extractive summary input text processed summary method generate summary example training sequential block datasets evaluated model different datasets news dbpedia yahoo offer fair evaluation performance sequential block removed input texts shorter xed length value table changing number training samples dbpedia yahoo answers number training samples news table statisics datasets vocabulary size number words train model min freq minimum word frequency decide vocabulary size dataset datasets classes training set test set vocabulary size min freq news dbpedia yahoo answers original apply changes test samples fair comparison best published results limited size vocabulary minimum word table words test samples included vocabulary dictionary word embedding block datasets contains summary examples train model generated summary examples feed summary samples evaluating test samples detailed statistics dataset given table text classication table classication results models numbers test accuracy percentage vanilla cnn basic model model stands text crop stands sum stands summarize labeled best result vanilla cnn blue worst result red best result labeled green model news dbpedia yahoo answers vanilla cnn vanilla cnn vanilla cnn vanilla cnn vanilla cnn extractive text classication table shows classication results vanilia cnn models rst evaluated data preprocessing methods vanilla cnn different xed length sizes labeled best result blue worst result red single data cessing method derived best performance datasets summarization tends work cases abstractive text classication model outperformed models cases bringing average growth compared vanilla cnn trained data cnn best published result news pretrained embedding data augmentation thesaurus model achieved competitive result news dataset pretrained word embedding data augmentation technique directly compare results changes explained section text summarization algorithm generate proper summary original text short result classication vanilla cnn cnn performed worst yahoo answers dataset robust short length texts shown table short length texts sequential block successfully generate summaries removing unimportant words original algorithm failed generate summary examples implementation tensorow greedy embedding helper instead training helper designed implementation textrank algorithm return rst sentence original text inference layer short table examples output produced sequential block short length texts textrank failed generate summary returning rst sentence input instead type label sentence original sports textrank sports sequential block sports original sci tech textrank sci tech sequential block sci tech great britain amir khan looked impressive winning pound championship junior international invitational boxing championships summer chance olympic gold medal lightweight division today great britain amir khan looked impressive winning pound championship junior international invitational boxing championships summer great britain amir khan looked impressive winning pound championship junior international invitational boxing championships summer chance olympic gold medal lightweight division today robot generate power eating ies developed british scientists idea produce electricity catching ies digesting special fuel cells break robot generate power eating ies developed british scientists robot generate power eating ies developed british entists idea produce electricity catching ies digesting special fuel cells break total loss sequence loss gws gws gws gws steps steps figure total loss sequence loss news dataset gradual weight gradual weight shift optimization sequence loss curve figure sequence loss model trained gws converges smoothly early stage training starts uctuate steps effects total loss curve unstabilizing training model contrast loss curves model gws converges smoothly end classication performance evaluated performance gradual weight shift news dataset table model trained gws achieved better results compared model trained gws model gws performed better table classication results news dataset gradual weight best result vanilla cnn included comparison model accuracy vanilla cnn vanilla cnn models outperform previous best published result zhang conclusion proposed sequence convolution neural networks efcient accurate text classication trained texts lengths text cessing method cropping summarizing presented new training theme model gradual weight applied models multi task loss function changing number balancing weights true strength comes exibility blocks replaced models designed tasks example sequential block replaced layer text variational convolution block replaced text classication models recurrent char adopted general sequence loss function uses predicted probability jth word generated summary matches jth word summary example calculate vocabulary losses think sequence loss evaluated accurately comparing words position bahdanau suggested specialized surrogate losses encoder decoder models sequence prediction tasks brought signicant performance improvements use pretrained word embeddings initialize word embedding block previous results word based textcnn suggests initializing embedding layers trained word vectors helps improves performances models future planning improve reassembling model possible methods references martn abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat ian goodfellow andrew harp geoffrey irving michael isard yangqing jia rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg dandelion man rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda vigas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng tensorflow scale machine learning heterogeneous systems software available tensorow org dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate corr dzmitry bahdanau dmitriy serdyuk philemon brakel nan rosemary jan chorowski aaron courville yoshua bengio task loss estimation sequence prediction corr lon bottou large scale machine learning stochastic gradient descent yves lier gilbert saporta editors proceedings pages heidelberg physica verlag jingnian chen houkuan huang shengfeng tian youli feature selection text classication nave bayes expert systems applications jan chorowski dzmitry bahdanau dmitriy serdyuk kyunghyun cho yoshua bengio attention based models speech recognition cortes lawrence lee sugiyama garnett editors advances neural information processing systems pages curran associates inc alexis conneau holger schwenk loc barrault yann lecun deep convolutional networks natural language processing corr xavier glorot yoshua bengio understanding difculty training deep feedforward neural networks yee whye teh mike titterington editors proceedings thirteenth international conference articial intelligence statistics volume proceedings machine learning research pages chia laguna resort sardinia italy pmlr marlene grace rajasekhar vijayapal reddy vinaya babu text classication summaries generated latent semantic analysis ijrsae graves mohamed hinton speech recognition deep recurrent neural networks ieee international conference acoustics speech signal processing pages kaiming xiangyu zhang shaoqing ren jian sun deep residual learning image recognition corr kaiming xiangyu zhang shaoqing ren jian sun delving deep rectiers surpassing human level performance imagenet classication corr sepp hochreiter jrgen schmidhuber long short term memory neural comput november sergey ioffe christian szegedy batch normalization accelerating deep network training reducing internal covariate shift proceedings international conference international conference machine learning volume pages jmlr org thorsten joachims text categorization support vector machines learning relevant features claire ndellec cline rouveirol editors machine learning pages berlin heidelberg springer berlin heidelberg armand joulin edouard grave piotr bojanowski tomas mikolov bag tricks efcient text classication corr yoon kim convolutional neural networks sentence classication corr diederik kingma jimmy adam method stochastic optimization corr siwei lai liheng kang liu jun zhao recurrent convolutional neural networks peter liu xin pan text summarization tensorow software available rada mihalcea paul tarau textrank bringing order texts dekang lin dekai editors proceedings emnlp pages barcelona spain july association computational linguistics tomas mikolov ilya sutskever kai chen greg corrado jeffrey dean distributed representations words phrases compositionality corr volodymyr mnih nicolas heess alex graves koray kavukcuoglu recurrent models visual attention corr text classication tensorow org ramesh nallapati bing xiang bowen zhou sequence sequence rnns text rization corr courtney napoles matthew gormley benjamin van durme annotated gigaword proceedings joint workshop automatic knowledge base construction web scale knowledge extraction akbc wekex pages stroudsburg usa association computational linguistics razvan pascanu tomas mikolov yoshua bengio understanding exploding gradient problem corr jeffrey pennington richard socher christopher manning glove global vectors word representation empirical methods natural language processing emnlp pages shaoqing ren kaiming ross girshick jian sun faster cnn real time object detection region proposal networks corr alexander rush sumit chopra jason weston neural attention model abstractive sentence summarization corr schuster paliwal bidirectional recurrent neural networks ieee transactions signal processing nov stanislau semeniuta aliaksei severyn erhardt barth hybrid convolutional variational autoencoder text generation corr karen simonyan andrew zisserman deep convolutional networks large scale image recognition corr nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov dropout simple way prevent neural networks overtting journal machine learning research ilya sutskever oriol vinyals quoc sequence sequence learning neural networks ghahramani welling cortes lawrence weinberger editors advances neural information processing systems pages curran associates inc christian szegedy sergey ioffe vincent vanhoucke inception inception resnet impact residual connections learning corr bing naiyan wang tianqi chen empirical evaluation rectied activations convolutional network corr zhang yun tao gong ling wang yong cheng improved idf approach text classication journal zhejiang university science aug xiang zhang junbo jake zhao yann lecun character level convolutional networks text classication corr chunting zhou chonglin sun zhiyuan liu francis lau lstm neural network text classication corr
