abstractive meeting summarization using dependency graph fusion siddhartha banerjee the pennsylvania state university university park pa usa prasenjit mitra qatar computing research institute tornado tower oor doha qatar kazunari sugiyama national university of singapore computing drive singapore p e s l c s c v v i x r a abstract automatic summarization techniques on meeting conversations veloped so far have been primarily extractive resulting in poor summaries to improve this we propose an approach to generate abstractive summaries by fusing important content from several terances any meeting is generally comprised of several discussion topic segments for each topic segment within a meeting tion we aim to generate a one sentence summary from the most portant utterances using an integer linear programming based tence fusion approach experimental results show that our method can generate more informative summaries than the baselines categories and subject descriptors articial intelligence natural language processing guage generation keywords abstractive meeting summarization integer linear programming introduction meeting summarization helps both participants and non participants by providing a short and concise snapshot of the most important content discussed in the meetings a recent study revealed that people generally prefer abstractive summaries table shows the human written abstractive summaries along with the generated extractive summaries from a meeting transcript as can be seen the utterances are highly noisy and contain unnecessary information even if an extractive summarizer can accurately sify these utterances as important and present them to a reader it is hard to read and synthesize information from such utterances in contrast human written summaries are compact and readable we propose an automatic way of generating short and concise abstractive summaries of meetings any meeting conversation cludes dialogues on several topics for example in table the participants converse on two topics design features and selling prices given the most important sentences within a topic ment our goal is to generate a one sentence summary from each segment and appending them to form a comprehensive summary of the meeting moreover we also aim to generate summaries that resemble human written summaries in terms of writing style permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page copyrights for party components of this work must be honored for all other uses contact the owner copyright is held by the author www companion may florence italy acm table two sets of extractive summaries and gold standard human generated abstractive summaries from a meeting set follows set set human generated extractive summary d um as well as uh characters d um different uh keypad styles and s symbols d well right away i m wondering if there s um uh like with players if there are zones a cause you have more complicated characters like european guages then you need more buttons d i m thinking the price might appeal to a certain market in one region whereas in another it ll be different so d kay trendy probably means something other than just basic abstractive summary the team then discussed various features to consider in making the remote set human generated extractive summary b like how much does you know a remote control cost b well twenty ve euro i mean that s um that s about like eighteen pounds or something d this is this gon na to be like the premium product kinda thing or b so i do nt know how how good a remote control that would get you um abstractive summary the project manager talked about the project nances and selling prices to aggregate the information from multiple utterances we adapt an existing integer linear programming ilp based fusion nique the fusion technique is based on the idea of merging dependency parse trees of the utterances the trees are merged on the common nodes that are represented by the word and parts speech pos combination each edge of the merged structure is represented as a variable in the ilp objective function and the tion will decide whether the edge has to be preserved or discarded we modify the technique by introducing an anaphora resolution step and also an ambiguity resolver that takes the context of words into account further to solve the ilp we introduce several straints such as desired length of the output to the best of our knowledge our work is the rst to address the problems of readability grammaticality and content selection jointly for meeting summary generation without employing a based approach we conduct experiments on the ami that consists of meeting transcripts and show that our best method performs extractive model signicantly on scores vs proposed approach dependency fusion on meeting data requires an algorithm that is robust for noisy data as utterances often have disuencies our work applies fusion to all the important utterances within the topic segment to generate the best sub tree that satises the constraints and maximizes the objective function of the optimization problem table probabilities of relations from produced vbn nsubjpass aux agent advmod auxpass table content selection evaluation rouge scores and and log likelihood score ll from the stanford dependency parser method our abstractive model our abstractive model without anaphora resolution extractive model baseline r ll these values using reuters corpora to obtain dominant relations from non conversational style of text for example table shows the probabilities of outgoing edges from a node produced vbn this term assigns the importance of grammatical relations to a node and only the relations that are more dominant from a node will be preferred the term denotes the informativeness of a node calculated using hori and furui s formula the last term in equation is based on the idea of lexical cohesion towards the end of any segment generally more important discussions might happen that will conclude a particular topic and then start another in order to take this fact into account we introduce the term px n where n and denote the total number of extracted utterances in a segment and the position of the utterance the edge belongs to in the set of n utterances respectively in order to solve the above ilp problem we impose a number of constraints some of the constraints have been directly adapted from the original ilp formulation for example we use the same constraints for restricting one incoming edge per node as well as we impose the connectivity constraint to ensure a connected graph structure further we restrict the subtree to have just one start edge and one end edge this helps in preserving one root node as well as it limits to one end node for the generated tree we also limit the generated subtree to have a maximum of nodes that controls the length of the summary sentence we also add few linguistic constraints that ensure the coherence of the put such as every node can have maximum of one determinant we also impose constraints to prevent cycles in the graph structure otherwise nding the best path from start and end nodes might be difcult the nal graph is linearized to obtain a coherent sentence in the linearization process we order the nodes based on their inal ordering in the utterance experimental results the ami meeting corpus contains meeting transcripts in the test set along with their corresponding abstractive human written summaries as well as the annotations of topic segments rouge is used to compare content selection of several approaches we compared the content selection of our approach to an extractive summarizer which works as a baseline we also compared our model without using anaphora resolution to see the impact of solving pronouns all the summaries were compared against the human written summaries as reference the results in table show that our method outperforms the other techniques on both and rouge r recall scores over we computed a coarse estimate of grammaticality using the log likelihood score ll from the parser our technique cantly outperforms the extractive method in future work we plan to design an end to end framework for summary generation from meetings figure a merged dependency graph structure edges in blue bold arrows to be retained to generate the summary for each topic segment anaphora resolution step replaces pronouns with the original nouns in the previous utterance that they refer to in order to increase the chances of merging consider the following utterances so we re designing a new remote control and um um as you can see it s supposed to be original without pronoun resolution these two utterances can not be merged once we apply anaphora resolution it in the second utterance is modied to a new remote control and then both the utterances are fused into a common structure the utterances are parsed using the stanford dependency parser every individual utterance has an plicit root node we add two dummy nodes in the graph the start node and the end node to ensure dened start and end points of the merged structure the words from the utterances are tively added onto the graph the words that have the same word form and pos tag are assigned to the same nodes ambiguity resolver suppose that a new word wi that has k biguous nodes where it can be mapped to the k ambiguous nodes are referred to as mappable nodes for every ambiguous mapping candidate we rst nd the words to the left and right of the pable node of the sentences and then compute the number of words in both the directions that are common to the words in either tion of the word wi finally wi is mapped to the node that has the highest directed context ilp formulation figure shows the sub graph marked using blue bold arrows that we wish to retain from the merged graph structure to generate a one sentence summary from several merged utterances all the sentences generated from each meeting script are concatenated to produce the nal abstractive summary we need to maximize the information content of the generated tence keeping it grammatical we model the problem as an teger linear programming ilp formulation similar to the dency graph fusion as proposed by fillipova and strube the directed edges in the graph binary variables are represented as xg d l where g d and l denote the governor node dependent node and the label of an edge respectively we maximize the following objective function xg d l g px n as shown in equation we introduce three different terms g and px n each relation in a dependency graph consists of the governing node the dependent node and the relation type the term g denotes the probabilities of the labels given a governor node for every node word and pos in the entire corpus the probabilities are represented as the ratio of the sum of the frequency of a particular label and the sum of the frequencies of all the labels emerging from a node in this work we calculate acknowledgments this material is based upon work supported by the national science foundation under grant no references filippova and strube sentence fusion via dependency graph compression in proc of emnlp pages hori and furui a new approach to automatic speech summarization ieee transactions on multimedia murray and carenini summarizing spoken and written conversations in proc of emnlp pages murray carenini and ng generating and validating abstracts of meeting conversations a user study in proc of inlg pages rose stevenson and whitehead the reuters corpus volume from yesterday s news to tomorrow s language resources in proc of lrec pages
