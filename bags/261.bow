interpretable multi headed attention abstractive summarization controllable lengths ritesh sarkhel moniba keymanesh arnab nandi srinivasan parthasarathy department computer science engineering ohio state university sarkhel edu v o n l c s c v v x r abstract abstractive summarization controllable lengths challenging task natural language cessing challenging domains limited training data available scenarios length summary known time comes trusting machine generated summaries explaining summary constructed human understandable terms critical propose multi level summarizer mls supervised method construct abstractive summaries text document controllable lengths key enabler method interpretable multi headed attention mechanism putes attention distribution input document array timestep independent mantic kernels kernel optimizes human interpretable syntactic semantic property exhaustive experiments low resource datasets english language mls performs strong baselines meteor score human evaluation maries suggests capture key concepts document length budgets introduction great progress recent years abstractive summarization text documents existing works sequence sequence networks attention gehring et al liu et al clear runners able constrain length summary ing desirable properties real world applications application content tion variable screen sizes online content creators news portals blogs advertisement agencies audiences multiple platforms customize content based display area best perience work summarization controllable lengths recently high variance screen sizes require extensive human supervision perform modications sequence sequence networks rush et al nallapati et al enforce length summary scenarios mentioned need employ ensemble works cover possible lengths major challenges following approach world applications training sequence sequence networks resource intensive task strubell et al train network generating summaries budgeted length b need parallel corpus text documents gold standard summaries length b constructing large corpus summaries budgeted b possible cost efcient number mains main reasons existing works abstractive summarization evaluate model large scale news corpus datasets nallapati et al hermann et al leaving number important low resource domains magooda litman parida motlicek number available training documents limited second range possible budgets known scenarios known late run time formalize summarization task addressed paper follows problem denition given document s length n tokens maximum token budget b aim construct abstractive summary sb satises following conditions information rst authors contributed equally work work licensed creative commons attribution international license license details org licenses input text police hunting man aged suspected robbing bank broad daylight running cash robbery took place lloyds bank branch fairwater cardiff police said detectives issued cctv images suspect ft ft wearing black clothing white male suspect greying black hair wore glasses captured camera inside bank detectives said injured robbery condent public able identify suspect detective sergeant andy miles fairwater cid said inquiries continuing identify culprit cctv clear condent members public know identity truncated summary compression budget police hunting man aged suspected robbing bank broad daylight running cash robbery took place lloyds bank branch fairwater cardiff police said white male suspect greying black hair wore glasses captured camera inside bank detectives issued cctv images suspect ft ft wearing black clothing detective sergeant andy miles fairwater cid said inquiries continuing identify culprit robbery took place lloyds bank branch fairwater cardiff detectives issued cctv images suspect detective sergeant andy miles fairwater cid said inquiries continuing identify culprit prototype summary figure mls expands highlighted sentences prototype summary boldfaced tokens input text construct summary budgeted half length input text redundancy minimized sb coverage major topics s maximized sb length sb maximal specied budget adversely affecting conditions e b sc ensure properties high quality summary preserved sb ensures sb largest possible summary constructed budget compromising quality note seemingly contradictory length summary increases goal nd optimal tradeoff early works incremental summarization buyukkokten et al yang wang leveraged structural tags supported document markup languages generate summaries lengths imposing constraint document formats e xml html come purview methods incremental sampling sentences based salience score otterbacher et al campana tombros partially solve problem constructing extractive summaries input document section sampling based methods fail preserve desirable properties high quality summary recent works kikuchi et al rst propose supervised method controlling length abstractive summarization work later extended fan et al introduced length summary input network instead exact input approximate length predened value range failing adhere allocated budget number cases liu et al address issue proposing convolutional encoder decoder network introducing desired summary length input initial state decoder compare report performance datasets experimental setup section unfortunately comes models e summaries came answer remains illusive explaining machine generated summary constructed come necessity newly introduced general data privacy regulation act itgp cially applications enterprise sarkhel nandi keymanesh et al biomedical domain moradi ghadiri sarkhel et al recent efforts proposed terpretable heatmaps baan et al generated attention distribution input sequence interpreting model behaviour limited jain wallace sistently explaining aspects neural summarizer leaves gap ongoing efforts song et al song et al generate abstractive summaries guided human interpretable semantic syntactic qualities briey main goal attention mechanism encoder decoder work assign softmax score encoder hidden state based relevance token ability explain present understandable terms human doshi velez kim decoded amplify assigned high scores weighted average target attention nallapati et al relies sequence computing scores self attention vaswani et al paulus et al operates elements current input sequence multi headed attention mechanism allows neural model speed training enabling parallelization timesteps number operations computation self attention scales quadratically input length making computationally expensive operation long input sequences training network summarization task require large parallel corpus input documents corresponding gold standard summaries budgeted b role attention heads abstractive summarization transparent baan et al dress replace self attention lightweight interpretable alternative instead projecting input sequence multiple timestep encode input sequence timestep independent kernel learned unsupervised distantly supervised way document kernel human interpretable syntactic semantic role attention head multi headed mechanism computes attention distribution input sequence unique kernel recycling timestep compared self attention proposed attention mechanism scales linearly input sequence length leverages signicantly number trainable eters section allows train network limited training samples low resource datasets propose mls supervised method erate abstractive summaries arbitrary lengths paper computes sb summary constrained length b budgeted soft switching copy expand operation prototype summary sp constructed ument key enabler process terpretable multi headed attention mechanism develop length aware network encoder decoder called pointer magnier leverages network attention mechanism construct summaries specied length train network limited training samples cross domain datasets msr narrative ouyang et al thinking machines dataset brockman exhaustive evaluation range success metrics shows mls performs competitively better strong baseline methods subsequent human evaluation summaries generated mls suggests accurately capture main concepts input document summarize major contributions work follows figure overview mls architecture pg network left constructs prototype summary sp input document pointer magnier network right constructs length constrained summary sp interpretable sentence level attention propose mls supervised approach generate abstractive summaries text document controllable lengths develop length aware encoder decoder network leverages interpretable multi headed attention mechanism construct length constrained summaries experimental results cross domain datasets trained limited training samples mls able generate summaries coherent captured key concepts document time compute query key value matrix vaswani et al input sequence proposed methodology mls constructs length constrained summary document steps derives prototype summary sp document covering major concepts expands shortens depending length budget create nal summary employ pair encoder decoder networks steps rst step extend pg network et al develop length aware decoder network second step describe steps greater detail following sections generating prototype summary extend pg network et al construct prototype summary sp document tokenize document feed encoder network sequentially encoder hidden states updated decoder network constructs prototype summary token time soft selecting tokens input document external vocabulary decoding process guided attention computed input document external vocabulary overview network shown fig point readers work et al background network example prototype summary shown figure contrary existing prototype text guided summarization methods liu et al saito et al specify length prototype summary input network infer outputting tokens eos token produced discuss training parameter settings network experiments section worth mentioning main reasons select pg network architecture choice step capability construct summary looking learned language model networks similar capabilities step transitive effect phase approach constructing length constrained summary construct summary length budget b develop pointer magnier network aware interpretable encoder decoder network overview network shown fig consists multiplex layer encoder yellow rectangles layer decoder green rectangles layer encoder layer takes prototype summary constructed previous step input decoder layer outputs nal summary describe layer detail multiplex layer interpretable kernels effort build transparent network embody qualitative properties associated high quality summary network high quality summary maximizes coverage major topics keywords appearing input document minimizing redundant information encode property semantic kernel learned unsupervised distantly supervised way input document kernel plays unique human interpretable syntactic semantic role constructing nal summary key components process multiplex layer m physically nested matrix dimensions shared encoder decoder layer row m contains following information distance metric disti scalar value wi c semantic kernel wi wi inference kernels measures contribution sentence prototype summary ing properties mentioned wi represents relative weight assigned property constructing nal summary compute kernels preprocessing step dening kernels encode property dene matrix dimensions row represents dominant topic vectors input document dimensional vector use unsupervised lda based model blei et al derive topic vectors symmetric kl divergence distance metric similarly encode property single dimensional vector length vector component represents relative frequency frequent keywords input document use rake rose et al publicly available library identify keywords document closely followed ofcial implementation com abisee pointer generator symmetric kl divergence distance metric finally encode matrix dimensions p ith row represents embedding ith sentence input document compute embedding vector sentence pretrained model le mikolov english wikipedia cosine similarity distance metric choice unsupervised distantly supervised kernels reects motivation section leverage limited number training samples experimental dataset construct nal summary discuss role played semantic kernel distance metric disti weight wi constructing nal summary sp following section b encoder layer encoder layer consists allelly stacked encoder blocks encoder block fig contains embedding layer local attention layer timestep t sentence sp fed ding layer encoder blocks computes xed length embedding sentence propagates local attention layer encoder block network mapped unique triplet disti wi multiplex layer compute local attention ci attributed sentence sp ith encoder block embed semantic space compute distance encoding space eq figure encoder layer consists parallelly stacked encoder blocks r disti r ni ni ci t eq represents kernel dimensions r ni represents embedding vector length ni embedding layer represents sentence sp encoding space nel associated block compute local attention ci taking column wise average distance matrix eq kernel reused sentences fed ith encoder block distribution obtained way normalized derive local attention tion sp nal attention distribution sp timestep t computed normalizing weighted average eq local attention distributions computed attention head norm wi m m worth noting attributing encoder block distinct attention head ensures dedicated pathway compute local attentions encoder block allows parallelize network speed decoding process constructing nal summary c decoder layer similar encoder decoder layer consists parallelly stacked decoder blocks decoder block contains embedding layer local attention layer eters th encoder block decoder block shared construct length constrained summary sb input document processing sentence sp sequentially depending remaining length budget timestep nal summary constructed soft switching copy expand operation process guided sentence level attention distribution eq computed sp copy operation selected sentence sp copied nal summary expand operation replaces sentence similar content input document sb original ordering sentences preserved copy operation probability copying sentence s prototype summary included nal summary sb till timestep t sb dened follows initialized t represents sentence level attention distribution sp timestep t eq update attention distribution timestep copy expand operation s represents sentence copied sb timestep t update attention distribution zeroing probability s renormalizing resulting distribution expand operation length prototype summary sp length budget b mls choose expand set sentences sp sentence s sp dene set sentence n gram similar s input document determine expansion set sentence beam search n grams input document included nal summary search objective maximizing e rst term denotes average pairwise cosine similarity s sentences second term denotes fraction tokens s appear minimize sentence repetitions summary candidates identied search process ranked chen bansal based number repeated word bigrams trigrams expansion set included nal summary obtained best performance initializing n changing later iterations decoding process denotes embedding vector k th sentence computed embedding layer decoder block dene probability expanding sentence s prototype summary nal summary follows e k r k t r e k e ni e m e wi ni m eq denotes semantic kernel shared th encoder block decoder block compute probability including kth sentence nal summary computing k optimizing qualitative property encoded rst eq contribution ce repeating process sentences followed normalization provides distribution e represents probability distribution obtain expansion probability sentence repeat process attention heads average eq probability expanding sentence s prototype summary obtained averaging expansion probability sentences sentence s expanded nal summary update attention distribution zeroing probability s renormalizing resulting distribution e ce ce soft selection copy expansion dene probability selecting copy expand operation sentence s prototype summary follows b b eq denotes partially constructed summary till timestep t length budget b smaller length prototype summary sp probability including sentence sp nal summary depends attention distribution t sentences sp included nal summary till timestep t scenarios acts soft switch copying expanding sentence sp sentence expanded exceed length budget probability sentence expansion set computed decoder attends position highest probability copies expands nal summary reaches b observed probability expanding sentence generation stops prototype summary instead copying increases allocated length budget training networks trained pg network pointer magnier network separately nvidia titan xp gpu batch size pretrained pg network cnn dailymail dataset nallapati et al ne tuned training samples experimental datasets evaluation script provided nallapati et al obtained training set pairs validation set pairs dataset encoder decoder weights allowed updated ne tuning stage following transfer pan yang weights pretrained network external vocabulary pretraining ne tuning stage consisted k frequent tokens training samples cnn dailymail dataset experimental dataset learning rate initial accumulator values set respectively adagrad duchi et al train network encoder fed maximum tokens decoder generated tokens pretraining values increased respectively ne tuning prevent overtting stopped training iterations ne tuning stage respect pointer magnier network learn optimal values wi associated attention head grid searching interval learning objective maximizing score validation set optimal weights assigned attention head corresponding topic coverage keyword coverage positive information redundancy assigned negative weight datasets experiments index dataset size max median mean seek answer key questions experiments given length constrained mary sb similar sb gold standard summary coherent representative input document abstractive sb answer rst questions evaluating summaries generated mls range success metrics datasets belonging low resource domains conduct user study measure representative summaries respect input documents representative summary covers main topics document answer question computing percentage n grams sb appear input document generated external vocabulary table minimum maximum median erage number sentences datasets msr narrative thinking machines datasets evaluate mls publicly available datasets low resource domains msr narrative ouyang et al dataset thinking machines brockman dataset msr narrative dataset contain personal stories shared users social networking website thinking machines dataset hand contains position papers popular scientic topic published educational website document datasets paired gold standard summary randomly selected document pairs construct training set document pairs construct validation set datasets rest comprised test corpus present overview important properties datasets table b metrics compare summaries constructed mls gold standard summaries meteor banerjee lavie rouge lin average score rouge l metrics obtained datasets shown table measure representativeness summary compute average kl divergence score py rouge benjamin heinzerling nltk library compute rouge meteor score respectively dataset metric budget budget mls mls mls budget mls budget mls budget rouge l meteor rouge l meteor table rouge meteor scores budgeted summaries constructed mls highlighted column baseline methods msr narrative thinking machines dataset topic vectors summary input document following srinivasan et al measure coherence summary computing average cosine similarity consecutive sentences report absolute difference coherence score computed summary input document table report kl divergence score sentiment vectors summary input document check potential biases polarity distribution publicly available library hutto gilbert derive sentiment vectors note lower values coherence kl divergence score desirable high quality summary c baselines compare mls baseline methods follow sampling based approach nal baseline method employs convolutional network construct length budgeted summaries rst baseline follows systematic sampling based approach construct length controlled summaries initialized randomly selected sentence rst sentences input document constructs nal summary including k th sentence sampled position set k experiments datasets sampling terminates budget limit exceeded end document reached second baseline method follows weighted graph based sampling strategy construct budgeted summaries represents sentence input document node undirected complete weighted graph weight assigned edge graph equal pairwise cosine similarity connecting nodes construct budgeted summary sample k nodes graph weighted pagerank algorithm halcea tarau sampling stops budget reached nal baseline method convolutional approach proposed liu et al sequence sequence network gated linear units dauphin et al takes desired length summary additional input initial state decoder network similar training protocol pretrain network cnn dailymail dataset rst ne tune training samples experimental datasets allowed weights updated ne tuning phase results discussion report performance competing methods ve length budgets specify length budget construct summary product number tokens input document budget c results experiments presented tables best performance achieved metric boldfaced highlight key ndings qualitative evaluation ve compression budgets general abstractive methods mls outperform sampling based approaches table datasets mls performs consistently budgets performance relatively better smaller budgets obtain absolute improvement score meteor score convolutional baseline datasets compression budget higher budgets performance comparable terms coherence mls performs comparably better table smaller coherence score suggests mls generated coherent summaries baseline methods small kl divergence topic distribution budgeted summary input document shows mls generated summaries representative document datasets fact topic coverage summaries generated mls better convolutional baseline liu et al performance comparable larger budgets dataset metric budget budget mls mls mls budget mls budget mls budget topic sentiment coherence topic sentiment coherence table coherence completeness budgeted summaries constructed mls highlighted column baseline methods msr narrative thinking machines dataset sentences prototype summary expanded nal summary mls outperfoms terms staying true sentiment distribution input document seen small kl divergence scores obtained sentiment distribution achieved mls table mls generated summaries abstractive higher budgets fig sion budget tokens summaries constructed dataset tokens dataset contributed external vocabulary ablative analysis figure abstractiveness mls generated summaries investigate effects pretraining end end results compare score maries constructed mls ablative baseline mls identical mls pg network pretrained second experiment compare mls ablative baseline constructs prototype summary following greedy heuristics otterbacher et al instead pg network mls outperforms baselines fig datasets ing pg network framework pretraining cnn dailymail dataset improved quality nal summaries finally investigate effects semantic kernels introduced pointer magnier network iteratively replaced semantic kernels section randomized kernel shufing rows columns observed absolute decrease score meteor score bigger impacts performance higher length budgets placing randomized kernel hand decreased average coherence score dataset summaries constructed compression budget approximately figure score mls ablative baselines mls datasets e half length input document human evaluation length controlled summaries conducted study evaluate completeness summaries constructed mls ically considered scenario user needs complete fact checking task chose documents datasets randomly asked participant verify presence key facts document summaries constructed mls baseline method participant instructed complete task solely based content summary depending previous knowledge example question story tell narrator red paired following summary tried return lost wallet customer accused stealing grabbed hair got physical ght red job participants chose yes information required participant selected option longer summary shown question task terminated wise addition mls stronger extractive baseline experimental setup add extreme settings content setting original document shown content setting textual content question shown participant content setting ensured question answered article content setting ensured questions contained hint answer fc nc mls index dataset accuracy duration s accuracy duration s table mean accuracy completion time mls content nc content fc settings task started showing ipant summary generated compression budget opted information shown provided summary generated method doubling compression budget time user responded yes reached budget key intuition users given complete representative summary able answer questions accurately good summarization model pick key concepts document shorter length budgets requiring expanded mind recorded task completion time user response treatment budgeted summaries constructed invited graduate students participate study participant shown summaries generated different methods random order information method revealed participant stage prevent information retention participant shown summary generated document balanced incomplete block design aschbacher settings methods datasets assigned subjects average accuracy task completion time recorded treatment shown table accuracy content setting datasets indicating questions contain hint correct answer content setting shows overall questions answered original documents summaries generated mls participants responded accurately content setting dataset times faster outperforming dataset participants accurate summaries constructed mls mls performed better document comparable worse document average accuracy conclusion proposed mls supervised approach construct abstractive summaries controllable lengths following extract compress paradigm develop pointer magnier network length aware encoder decoder network constructs length constrained summaries shortening expanding prototype summary inferred document key enabler network array semantic kernels clearly dened human interpretable syntactic semantic roles constructing summary given budget length train network limited training samples cross domain datasets experiments summaries constructed mls coherent reectively capture main concepts document human evaluation study suggest future like extend work construct task driven summaries interactive question answering tasks personalizing summary based user s past interaction model exciting direction future work acknowledgement like thank professors micha elsner joel bloch marie catherine de marneffe michael white valuable discussions comments like thank reviewers folks participated study sharing critical feedback helped improve work references michael aschbacher collineation groups symmetric block designs journal combinatorial theory series joris baan maartje ter hoeve marlies van der wees anne schuth maarten de rijke transformer attention heads provide transparency abstractive summarization arxiv preprint satanjeev banerjee alon lavie meteor automatic metric mt evaluation improved correlation man judgments proceedings acl workshop intrinsic extrinsic evaluation measures machine translation summarization anders johannsen benjamin heinzerling python wrapper rouge summarization evaluation package https org project david m blei andrew y ng michael jordan latent dirichlet allocation jmlr john brockman thinking machines think machines think edge org annual orkut buyukkokten hector garcia molina andreas paepcke seeing parts text summarization web browsing handheld devices web conference marco campana anastasios tombros incremental personalised summarisation novelty detection fqas yen chun chen mohit bansal fast abstractive summarization reinforce selected sentence rewriting arxiv works icml tion jmlr yann n dauphin angela fan michael auli david grangier language modeling gated convolutional finale doshi velez kim rigorous science interpretable machine learning arxiv preprint john duchi elad hazan yoram singer adaptive subgradient methods online learning stochastic angela fan david grangier michael auli controllable abstractive summarization arxiv jonas gehring michael auli david grangier denis yarats yann n dauphin convolutional sequence sequence learning icml jmlr org karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend advances neural information processing systems pages clayton j hutto eric gilbert vader parsimonious rule based model sentiment analysis social media text icwsm itgp eu general data protection regulation gdpr governance limited sarthak jain byron c wallace attention explanation proceedings conference north american chapter association computational linguistics human language technologies volume long short papers pages moniba keymanesh micha elsner srinivasan parthasarathy domain guided controllable summarization privacy policies natural legal language processing workshop kdd yuta kikuchi graham neubig ryohei sasano hiroya takamura manabu okumura controlling output length neural encoder decoders arxiv quoc le tomas mikolov distributed representations sentences documents icml chin yew lin rouge package automatic evaluation summaries text summarization branches fei liu jeffrey flanigan sam thomson norman sadeh noah smith abstractive summarization semantic representations arxiv yizhu liu zhiyi luo kenny zhu controlling length abstractive summarization convolutional neural chunyi liu peng wang jiang xu zang li jieping ye automatic dialogue summary generation customer network emnlp service sigkdd ahmed magooda diane litman abstractive summarization low resource data domain transfer data synthesis arxiv preprint rada mihalcea paul tarau textrank bringing order text emnlp milad moradi nasser ghadiri different approaches identifying important concepts probabilistic biomedical text summarization articial intelligence medicine ramesh nallapati bowen zhou caglar gulcehre bing xiang al abstractive text summarization sequence sequence rnns arxiv preprint jahna otterbacher dragomir radev omer kareem news hierarchical text summarization mobile devices sigir corpora eacl jessica ouyang serina chang kathy mckeown crowd sourced iterative annotation narrative summarization sinno jialin pan qiang yang survey transfer learning tkde shantipriya parida petr motlicek abstract text summarization low resource challenge emnlp association computational linguistics november romain paulus caiming xiong richard socher deep reinforced model abstractive summarization international conference learning representations stuart rose dave engel nick cramer wendy cowley automatic keyword extraction individual documents text mining applications theory alexander m rush sumit chopra jason weston neural attention model abstractive sentence summarization arxiv yuji matsumoto itsumi saito kyosuke nishida kosuke nishida atsushi otsuka hisako asano junji tomita hiroyuki shindo arxiv length controllable abstractive summarization guiding summary prototype ritesh sarkhel arnab nandi visual segmentation information extraction heterogeneous visually rich documents proceedings international conference management data pages ritesh sarkhel jacob j socha austin mount campbell susan moffatt bruce simon fernandez kashvi patel arnab nandi emily s patterson nurses identify hospitalized patients personal notes findings analyzing brains headers multiple raters proceedings international symposium human factors ergonomics health care volume pages sage publications sage los angeles abigail peter j liu christopher d manning point summarization pointer generator networks arxiv kaiqiang song logan lebanoff qipeng guo xipeng qiu xiangyang xue chen li dong yu fei liu joint parsing generation abstractive summarization proceedings aaai conference articial intelligence kaiqiang song bingqing wang zhe feng liu ren fei liu controlling verbatim copying abstractive summarization proceedings aaai conference articial intelligence balaji vasan srinivasan pranav maneriker kundan krishna natwar modani corpus based content construction coling arxiv preprint emma strubell ananya ganesh andrew mccallum energy policy considerations deep learning nlp ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia sukhin attention need nips christopher c yang fu lee wang fractal summarization mobile devices access large documents web web conference
