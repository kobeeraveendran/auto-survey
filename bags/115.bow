attention based models text dependent speaker verification rezaur rahman chowdhury quan wang ignacio lopez moreno wan washington state university wsu edu google inc usa quanw elnota liwan com abstract attention based models recently shown great performance range tasks speech recognition machine translation image captioning ability summarize relevant formation expands entire length input quence paper analyze usage attention mechanisms problem sequence summarization end end dependent speaker recognition system explore different gies variants attention layer compare different pooling methods attention weights ultimately attention based models improves equal error rate eer speaker verication system relatively compared non attention lstm baseline model index terms attention based model sequence tion speaker recognition pooling lstm challenge architecture introduced silence background noise captured speaker verication runs short window segmented keyword detector phonemes ally surrounded frames silence background noise ideally speaker embedding built frames sponding phonemes propose use attention layer soft mechanism emphasize relevant ements input sequence paper organized follows sec rst briey review lstm based vector baseline approach trained end end architecture sec introduce add attention mechanism baseline architecture covering ent scoring functions layer variants weights pooling methods sec setup experiments compare attention based els baseline model present eer results testing set conclusions sec introduction baseline architecture speaker verication process verifying based set reference enrollment utterances verication utterance belongs known speaker subtask global password text dependent speaker verication refers set problems transcripts reference enrollment verication utterances constrained specic phrase study focus google hey google global words relate voice match feature google home vector based systems combination verication ends probabilistic linear discriminant analysis plda dominating paradigm ous years recently rising deep learning machine learning applications efforts cusing neural networks speaker verication currently promising approaches end end integrated tures simulate enrollment verication stage process training example authors propose architectures semble components vector plda system tecture allowed bootstrap network parameters pretrained vector plda models better performance initialization stage constrained type network architectures deep neural networks dnn initialized classical vector plda models shown long short term memory lstm networks achieve better performance dnns integrated end end architectures scenarios author work intern google end end training architecture described fig training step tuple evaluation utterance rollment utterances xkn fed lstm network xkn represents features log energies xed length segment represent speakers utterances equal tuple includes single utterance speaker ferent utterance speaker tuple positive enrollment utterances speaker negative generate positive negative tuples alternatively utterance let output lstm layer frame xed dimensional vector frame output vector fig build new tuple centroid tuple represents voiceprint built utterances dened follows similarity dened cosine similarity function learnable tuple based end end loss nally dened fig baseline end end training architecture introduced standard sigmoid function equals equals end end loss function encourages larger value smaller value consider update positive negative tuples loss function similar triplet loss facenet attention based model basic attention layer baseline end end training directly frame output vector alternatively learn scalar score lstm output frame compute normalized weights scores nally shown fig form vector weighted average lstm outputs frames tht scoring functions different scoring functions different attention layers bias attention scalar note attention depend lstm output linear attention dimensional vector scalar fig lstm based vector baseline basic attention layer shared parameter linear attention dimensional vector scalar frames non linear attention matrix vectors dimension tuned development dataset shared parameter non linear attention frames scoring functions parameters able end end architecture attention layer variants apart basic attention layer described sec introduce variants cross layer attention divided layer tention cross layer attention fig scores weights computed outputs lstm layer evaluation utteranceenrollmentutterance accept rejectspeaker modelspeakerrepresentationscore function enrollmentutterance logistic regressioncosine similarityaveragelstm input featureslstmlstm outputsd vector input features lstmlstm outputsnormalized weightsd vector fig variants attention layer cross layer attention divided layer attention second layer outputs intermediate lstm layer vector weighted average layer output divided layer attention fig double dimension layer lstm output equally divide dimension parts use build vector learn scores tha weights pooling variation basic attention layer instead rectly normalized weights average lstm outputs optionally perform maxpooling attention weights additional pooling mechanism potentially work robust temporal variations input signals experimented maxpooling methods fig sliding window maxpooling run sliding window weights window largest value set values fig different pooling methods attention weights tth pixel corresponds weight brighter intensity means larger value weight global maxpooling largest values weights set values experiments datasets basic setup fairly compare different attention techniques use training testing datasets experiments training dataset collection anonymized user voice queries mixture google hey google utterances speakers ing dataset manual collection consisting speakers divided enrollment sets verication sets google hey google enrollment evaluation dataset contains respectively average evaluation utterances speaker report speaker verication equal error rate eer combinations enrollment set verication set baseline model layer lstm layer dimension projection layer dimension lstm linear layer dimension acoustic parametrization consists dimensional log cients computed window overlap acoustic features keyword detection speaker verication keyword spotting system isolates segments length frames contain global password segments form tuples mentioned keywords mixed multireader technique introduced basic attention layer compare baseline model basic attention layer sec different scoring function sec results shown table bias linear attention bring little improvement eer non linear improves performance signicantly especially shared parameters intermediate dimension non linear scoring functions use square matrices layer outputsd layer outputs vectorlast layer outputspart apart poolingsliding window maxpoolingglobal maxpooling table evaluation non attention baseline model basic attention layer different scoring functions test data enroll verify google google google hey google hey google google hey google hey google average non attention baseline basic attention fsl fnl fbo fsnl table evaluation basic attention layer variants fsnl scoring function cross layer divided layer test data hey hey hey hey average basic fsnl table evaluation different pooling methods tion weights fsnl divided layer test data hey hey hey hey average pooling sliding window variants compare basic attention layer variants sec use scoring function performs best vious experiment shared parameter non linear scoring function fsnl results table divided layer tention performs slightly better basic attention cross layer cost dimension lstm layer bled weights pooling compare different pooling methods attention weights troduced sec use divided layer attention parameter non linear scoring function sliding window pooling experimented different window sizes steps found window size frames step frames perform best evaluations global pooling found performance best results shown table sliding window maxpooling improves eer visualize attention weights training batch different pooling methods fig interesting observation pooling clear strand strand pattern batch pattern corresponds kay gle phoneme hey goo gle phoneme structure words experiments cross layer attention scores learned second layer fig visualized attention weights different pooling methods image axis time axis different utterances training batch pooling sliding window maxpooling window size step global ing apply sliding window maxpooling global maxpooling attention weights larger near end utterance easy understand lstm lated information near end beginning condent produce vector conclusions paper experimented different attention mechanisms keyword based text dependent speaker verication system experimental results best practice use shared parameter non linear scoring function use layer attention connection layer output lstm apply sliding window maxpooling attention weights combining best practices improved eer baseline lstm model ative improvement attention mechanisms especially ones shared parameter scoring functions potentially improve text independent speaker verication models speaker diarization systems sliding window global maxpooling hasim sak andrew senior francoise beaufays long short term memory recurrent neural network architectures large scale acoustic modeling fifteenth annual conference international speech communication association wan quan wang alan papir ignacio lopez moreno generalized end end loss speaker verication arxiv preprint quan wang carlton downey wan philip manseld ignacio lopez moreno speaker diarization lstm arxiv preprint references pinsky yury home blog google products assistant tomato google home supports multiple users tomato tomahto supports multiple google users mihai matei voice match google low androidheadlines voice allow google home recognize voice html recognize home voice najim dehak patrick kenny reda dehak pierre mouchel pierre ouellet end factor analysis speaker verication ieee transactions audio speech language processing vol daniel garcia romero carol espy wilson analysis vector length normalization speaker recognition systems interspeech yann lecun yoshua bengio geoffrey hinton deep learning nature vol johan rohdin anna silnova mireia diez oldrich plchot end end dnn based arxiv pavel matejka lukas burget speaker recognition inspired vector plda preprint georg heigold ignacio moreno samy bengio noam shazeer end end text dependent speaker verication acoustics speech signal processing icassp ieee international conference ieee sepp hochreiter jurgen schmidhuber long short term memory neural computation vol guoguo chen carolina parada georg heigold footprint keyword spotting deep neural networks acoustics speech signal processing icassp ieee international conference ieee rohit prabhavalkar raziel alvarez carolina parada preetum nakkiran tara sainath automatic gain control multi style training robust small footprint keyword spotting deep neural networks acoustics speech signal processing icassp ieee international conference ieee jan chorowski dzmitry bahdanau dmitriy serdyuk kyunghyun cho yoshua bengio attention based els speech recognition advances neural information processing systems minh thang luong hieu pham christopher manning effective approaches attention based neural machine lation arxiv preprint kelvin jimmy ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel yoshua gio attend tell neural image caption generation visual attention international conference machine learning florian schroff dmitry kalenichenko james philbin facenet unied embedding face recognition tering proceedings ieee conference computer vision pattern recognition
