attention based models text dependent speaker verification f rezaur rahman chowdhury quan wang ignacio lopez moreno li wan washington state university wsu edu google inc usa quanw elnota liwan com n j s s s e e v v x r abstract attention based models recently shown great performance range tasks speech recognition machine translation image captioning ability summarize relevant formation expands entire length input quence paper analyze usage attention mechanisms problem sequence summarization end end dependent speaker recognition system explore different gies variants attention layer compare different pooling methods attention weights ultimately attention based models improves equal error rate eer speaker verication system relatively compared non attention lstm baseline model index terms attention based model sequence tion speaker recognition pooling lstm challenge architecture introduced silence background noise captured speaker verication runs short window segmented keyword detector phonemes ally surrounded frames silence background noise ideally speaker embedding built frames sponding phonemes propose use attention layer soft mechanism emphasize relevant ements input sequence paper organized follows sec rst briey review lstm based d vector baseline approach trained end end architecture sec introduce add attention mechanism baseline architecture covering ent scoring functions layer variants weights pooling methods sec setup experiments compare attention based els baseline model present eer results testing set conclusions sec introduction baseline architecture speaker verication sv process verifying based set reference enrollment utterances verication utterance belongs known speaker subtask sv global password text dependent speaker verication td sv refers set problems transcripts reference enrollment verication utterances constrained specic phrase study focus ok google hey google global words relate voice match feature google home vector based systems combination verication ends probabilistic linear discriminant analysis plda dominating paradigm sv ous years recently rising deep learning machine learning applications efforts cusing neural networks speaker verication currently promising approaches end end integrated tures simulate enrollment verication stage process training example authors propose architectures semble components vector plda system tecture allowed bootstrap network parameters pretrained vector plda models better performance initialization stage constrained type network architectures deep neural networks dnn initialized classical vector plda models shown long short term memory lstm networks achieve better performance dnns integrated end end architectures td sv scenarios author work intern google end end training architecture described fig training step tuple evaluation utterance xj n rollment utterances xkn n n fed lstm network xj xkn represents features log energies xed length segment j k represent speakers utterances j equal tuple includes single utterance speaker j n ferent utterance speaker k tuple positive xj n enrollment utterances speaker e j k negative generate positive negative tuples alternatively utterance let output lstm s layer frame t xed dimensional vector ht t t frame output d vector ht fig build new tuple j kn centroid tuple kn represents voiceprint built n utterances dened follows ck n n kn similarity dened cosine similarity function s w ck learnable w tuple based end end loss nally dened ck k fig baseline end end training architecture introduced ex standard sigmoid function k equals j k equals end end loss function encourages larger value s j smaller value s j consider update positive negative tuples loss function similar triplet loss facenet attention based model basic attention layer baseline end end training directly frame output d vector ht alternatively learn scalar score et r lstm output ht frame t et ht t t compute normalized weights t scores t nally shown fig form vector weighted average lstm outputs frames t tht t scoring functions different scoring functions eq different attention layers bias attention bt scalar note attention depend lstm output et bt linear attention wt m dimensional vector bt scalar et t ht bt fig lstm based d vector baseline basic attention layer shared parameter linear attention m dimensional vector w scalar frames et wt ht non linear attention wt m matrix bt vt vectors dimension tuned development dataset et vt t bt shared parameter non linear attention w b v frames et vt scoring functions parameters able end end architecture attention layer variants apart basic attention layer described sec introduce variants cross layer attention divided layer tention cross layer attention fig scores et weights t computed outputs lstm layer evaluation utteranceenrollmentutterance accept rejectspeaker modelspeakerrepresentationscore function enrollmentutterance n logistic regressioncosine similarityaveragelstm input featureslstmlstm outputsd vector input features lstmlstm outputsnormalized weightsd vector fig variants attention layer cross layer attention b divided layer attention e second layer outputs intermediate lstm layer et t d vector weighted average layer output ht divided layer attention fig double dimension layer lstm output ht equally divide dimension parts ha t use build d vector b learn scores t b hb et hb t tha t t weights pooling variation basic attention layer instead rectly normalized weights t average lstm outputs optionally perform maxpooling attention weights additional pooling mechanism potentially work robust temporal variations input signals experimented maxpooling methods fig sliding window maxpooling run sliding window weights window largest value set values fig different pooling methods attention weights tth pixel corresponds weight t brighter intensity means larger value weight global k maxpooling largest k values weights set values experiments datasets basic setup fairly compare different attention techniques use training testing datasets experiments training dataset collection anonymized user voice queries mixture ok google hey google m utterances k speakers ing dataset manual collection consisting speakers s divided enrollment sets verication sets ok google hey google enrollment evaluation dataset contains respectively average evaluation utterances speaker report speaker verication equal error rate eer combinations enrollment set verication set baseline model layer lstm layer dimension projection layer dimension lstm linear layer dimension acoustic parametrization consists dimensional log cients computed window overlap acoustic features keyword detection speaker verication keyword spotting system isolates segments length t frames contain global password segments form tuples mentioned keywords mixed multireader technique introduced basic attention layer compare baseline model basic attention layer sec different scoring function sec results shown table bias linear attention bring little improvement eer non linear improves performance signicantly especially shared parameters intermediate dimension non linear scoring functions use wt w square matrices layer outputsd layer outputs vectorlast layer outputspart apart poolingsliding window maxpoolingglobal k maxpooling table evaluation non attention baseline model vs basic attention layer different scoring functions test data enroll verify ok google ok google ok google hey google hey google ok google hey google hey google average non attention baseline basic attention fsl fnl fl fbo fsnl table evaluation basic attention layer vs variants fsnl scoring function cross layer divided layer test data ok ok ok hey hey ok hey hey average basic fsnl table evaluation different pooling methods tion weights fsnl divided layer test data ok ok ok hey hey ok hey hey average pooling sliding window k variants compare basic attention layer variants sec use scoring function performs best vious experiment shared parameter non linear scoring function fsnl results table divided layer tention performs slightly better basic attention cross layer cost dimension lstm layer bled weights pooling compare different pooling methods attention weights troduced sec use divided layer attention parameter non linear scoring function sliding window pooling experimented different window sizes steps found window size frames step frames perform best evaluations global k pooling found performance best k results shown table sliding window maxpooling improves eer visualize attention weights training batch different pooling methods fig interesting observation s pooling clear strand strand pattern batch pattern corresponds o kay gle phoneme hey goo gle phoneme structure words experiments cross layer attention scores learned second layer fig visualized attention weights different pooling methods image axis time y axis different utterances training batch pooling sliding window maxpooling window size step global k ing k apply sliding window maxpooling global k maxpooling attention weights larger near end utterance easy understand lstm lated information near end beginning condent produce d vector conclusions paper experimented different attention mechanisms keyword based text dependent speaker verication system experimental results best practice use shared parameter non linear scoring function use layer attention connection layer output lstm apply sliding window maxpooling attention weights combining best practices improved eer baseline lstm model ative improvement attention mechanisms especially ones shared parameter scoring functions potentially improve text independent speaker verication models speaker diarization systems sliding window global k maxpooling hasim sak andrew senior francoise beaufays long short term memory recurrent neural network architectures large scale acoustic modeling fifteenth annual conference international speech communication association li wan quan wang alan papir ignacio lopez moreno generalized end end loss speaker verication arxiv preprint quan wang carlton downey li wan philip manseld ignacio lopez moreno speaker diarization lstm arxiv preprint references pinsky yury home blog google products assistant tomato google home supports multiple users tomato tomahto supports multiple google users mihai matei voice match google low androidheadlines voice allow google home recognize voice html recognize home voice najim dehak patrick j kenny reda dehak pierre mouchel pierre ouellet end factor analysis speaker verication ieee transactions audio speech language processing vol pp daniel garcia romero carol y espy wilson analysis vector length normalization speaker recognition systems interspeech pp yann lecun yoshua bengio geoffrey hinton deep learning nature vol pp johan rohdin anna silnova mireia diez oldrich plchot end end dnn based arxiv pavel matejka lukas burget speaker recognition inspired vector plda preprint georg heigold ignacio moreno samy bengio noam shazeer end end text dependent speaker verication acoustics speech signal processing icassp ieee international conference ieee pp sepp hochreiter jurgen schmidhuber long short term memory neural computation vol pp guoguo chen carolina parada georg heigold footprint keyword spotting deep neural networks acoustics speech signal processing icassp ieee international conference ieee pp rohit prabhavalkar raziel alvarez carolina parada preetum nakkiran tara n sainath automatic gain control multi style training robust small footprint keyword spotting deep neural networks acoustics speech signal processing icassp ieee international conference ieee pp jan k chorowski dzmitry bahdanau dmitriy serdyuk kyunghyun cho yoshua bengio attention based els speech recognition advances neural information processing systems pp minh thang luong hieu pham christopher d manning effective approaches attention based neural machine lation arxiv preprint kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel yoshua gio attend tell neural image caption generation visual attention international conference machine learning pp florian schroff dmitry kalenichenko james philbin facenet unied embedding face recognition tering proceedings ieee conference computer vision pattern recognition pp
