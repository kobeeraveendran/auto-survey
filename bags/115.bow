attention based models for text dependent speaker verification f a rezaur rahman chowdhury quan wang ignacio lopez moreno li wan washington state university wsu edu google inc usa quanw elnota liwan com n a j s a s s e e v v i x r a abstract attention based models have recently shown great performance on a range of tasks such as speech recognition machine translation and image captioning due to their ability to summarize relevant formation that expands through the entire length of an input quence in this paper we analyze the usage of attention mechanisms to the problem of sequence summarization in our end to end dependent speaker recognition system we explore different gies and their variants of the attention layer and compare different pooling methods on the attention weights ultimately we show that attention based models can improves the equal error rate eer of our speaker verication system by relatively compared to our non attention lstm baseline model index terms attention based model sequence tion speaker recognition pooling lstm however one challenge in our architecture introduced in is that silence and background noise are not being well captured though our speaker verication runs on a short window that is segmented by the keyword detector the phonemes are ally surrounded by frames of silence and background noise ideally the speaker embedding should be built only using the frames sponding to phonemes thus we propose to use an attention layer as a soft mechanism to emphasize the most relevant ements of the input sequence this paper is organized as follows in sec we rst briey review our lstm based d vector baseline approach trained with the end to end architecture in sec we introduce how we add the attention mechanism to our baseline architecture covering ent scoring functions layer variants and weights pooling methods in sec we setup experiments to compare attention based els against our baseline model and present the eer results on our testing set conclusions are made in sec introduction baseline architecture speaker verication sv is the process of verifying based on a set of reference enrollment utterances whether an verication utterance belongs to a known speaker one subtask of sv is global password text dependent speaker verication td sv which refers to the set of problems for which the transcripts of reference enrollment and verication utterances are constrained to a specic phrase in this study we focus on ok google and hey google global words as they relate to the voice match feature of google home i vector based systems in combination with verication back ends such as probabilistic linear discriminant analysis plda have been the dominating paradigm of sv in ous years more recently with the rising of deep learning in various machine learning applications more efforts have been cusing on using neural networks for speaker verication currently the most promising approaches are end to end integrated tures that simulate the enrollment verication two stage process during training for example in the authors propose architectures that semble the components of an i vector plda system such tecture allowed to bootstrap the network parameters from pretrained i vector and plda models for a better performance however such initialization stage also constrained the type of network architectures that could be used only deep neural networks dnn can be initialized from classical i vector and plda models in we have shown that long short term memory lstm networks can achieve better performance than dnns for integrated end to end architectures in td sv scenarios the author did this work during his intern at google our end to end training architecture is described in fig for each training step a tuple of one evaluation utterance xj and n rollment utterances xkn for n n is fed into our lstm network xj xkn where represents the features log energies from a xed length segment j and k represent the speakers of the utterances and j may or may not equal the tuple includes a single utterance from speaker j and n ferent utterance from speaker k we call a tuple positive if xj and the n enrollment utterances are from the same speaker i e j k and negative otherwise we generate positive and negative tuples alternatively for each utterance let the output of the lstm s last layer at frame t be a xed dimensional vector ht where t t we take the last frame output as the d vector ht fig and build a new tuple j kn the centroid of tuple kn represents the voiceprint built from n utterances and is dened as follows ck n n kn the similarity is dened using the cosine similarity function s w ck with learnable w and the tuple based end to end loss is nally dened as ck k fig our baseline end to end training architecture as introduced in here ex is the standard sigmoid function and k equals if j k otherwise equals to the end to end loss function encourages a larger value of s when j and a smaller value of s when j consider the update for both positive and negative tuples this loss function is very similar to the triplet loss in facenet attention based model basic attention layer in our baseline end to end training we directly take the last frame output as d vector ht alternatively we could learn a scalar score et r for the lstm output ht at each frame t et ht t t then we can compute the normalized weights t using these scores such that t and nally as shown in fig we form the vector as the weighted average of the lstm outputs at all frames t tht t scoring functions by using different scoring functions in eq we get different attention layers bias only attention where bt is a scalar note this attention does not depend on the lstm output et bt linear attention where wt is an m dimensional vector and bt is a scalar et t ht bt fig lstm based d vector baseline basic attention layer shared parameter linear attention where the m dimensional vector w and scalar are the same for all frames et wt ht non linear attention where wt is an m matrix bt and vt are vectors the dimension can be tuned on a development dataset et vt t bt shared parameter non linear attention where the same w b and v are used for all frames et vt in all the above scoring functions all the parameters are able within the end to end architecture attention layer variants apart from the basic attention layer described in sec here we introduce two variants cross layer attention and divided layer tention for cross layer attention fig the scores et and weights t are not computed using the outputs of the last lstm layer evaluation utteranceenrollmentutterance accept rejectspeaker modelspeakerrepresentationscore function enrollmentutterance n logistic regressioncosine similarityaveragelstm input featureslstmlstm outputsd vector input features lstmlstm outputsnormalized weightsd vector a fig two variants of the attention layer a cross layer attention b divided layer attention but e the second to last layer the outputs of an intermediate lstm layer et t however the d vector is still the weighted average of the last layer output ht for divided layer attention fig we double the dimension of the last layer lstm output ht and equally divide its dimension into two parts part a ha t we use part a to build the d vector while using part b to learn the scores t and part b hb et hb t tha t t weights pooling another variation of the basic attention layer is that instead of rectly using the normalized weights t to average lstm outputs we can optionally perform maxpooling on the attention weights this additional pooling mechanism can potentially make our work more robust to temporal variations of the input signals we have experimented with two maxpooling methods fig sliding window maxpooling we run a sliding window on the weights and for each window only keep the largest value and set other values to fig different pooling methods on attention weights the tth pixel corresponds to the weight t and a brighter intensity means a larger value of the weight global top k maxpooling only keep the largest k values in the weights and set all other values to experiments datasets and basic setup to fairly compare different attention techniques we use the same training and testing datasets for all our experiments our training dataset is a collection of anonymized user voice queries which is a mixture of ok google and hey google it has around m utterances from around k speakers our ing dataset is a manual collection consisting of speakers it s divided into two enrollment sets and two verication sets for each of ok google and hey google each enrollment and evaluation dataset contains respectively an average of and evaluation utterances per speaker we report the speaker verication equal error rate eer on the four combinations of enrollment set and verication set our baseline model is a layer lstm where each layer has dimension with a projection layer of dimension on top of the lstm is a linear layer of dimension the acoustic parametrization consists of dimensional log cients computed over a window of with of overlap the same acoustic features are used for both keyword detection and speaker verication the keyword spotting system isolates segments of length t frames that only contain the global password and these segments form the tuples mentioned above the two keywords are mixed together using the multireader technique introduced in basic attention layer first we compare the baseline model with basic attention layer sec using different scoring function sec the results are shown in table as we can see while bias only and linear attention bring little improvement to the eer non linear improves the performance signicantly especially with shared parameters the intermediate dimension of non linear scoring functions we use such that wt and w are square matrices last layer outputsd to last layer outputs vectorlast layer outputspart apart poolingsliding window maxpoolingglobal top k maxpooling table evaluation non attention baseline model vs basic attention layer using different scoring functions test data enroll verify ok google ok google ok google hey google hey google ok google hey google hey google average non attention baseline basic attention fsl fnl fl fbo fsnl table evaluation basic attention layer vs variants all using fsnl as scoring function cross layer divided layer test data ok ok ok hey hey ok hey hey average basic fsnl table evaluation different pooling methods for tion weights all using fsnl and divided layer test data ok ok ok hey hey ok hey hey average no pooling sliding window top k variants to compare the basic attention layer with the two variants sec we use the same scoring function that performs the best in the vious experiment the shared parameter non linear scoring function fsnl from the results in table we can see that divided layer tention performs slightly better than basic attention and cross layer at the cost that the dimension of last lstm layer is bled weights pooling to compare different pooling methods on the attention weights as troduced in sec we use the divided layer attention with parameter non linear scoring function for sliding window pooling we experimented with different window sizes and steps and found that a window size of frames and a step of frames perform the best in our evaluations also for global top k pooling we found that the performance is the best when k the results are shown in table we can see that sliding window maxpooling further improves the eer we also visualize the attention weights of a training batch for different pooling methods in fig an interesting observation is that when there s no pooling we can see a clear strand or strand pattern in the batch this pattern corresponds to the o kay gle phoneme or hey goo gle phoneme structure of the words our experiments for cross layer attention scores are learned from the second to last layer fig visualized attention weights for different pooling methods in each image axis is time and y axis is for different utterances in a training batch no pooling sliding window maxpooling where window size is and step is global top k ing where k when we apply sliding window maxpooling or global top k maxpooling the attention weights are much larger at the near end of the utterance which is easy to understand the lstm has lated more information at the near end than at the beginning thus is more condent to produce the d vector conclusions in this paper we experimented with different attention mechanisms for our keyword based text dependent speaker verication system from our experimental results the best practice is to use a shared parameter non linear scoring function use a layer attention connection to the last layer output of the lstm and apply a sliding window maxpooling on the attention weights after combining all these best practices we improved the eer of our baseline lstm model from to which is a ative improvement the same attention mechanisms especially the ones using shared parameter scoring functions could potentially be used to improve text independent speaker verication models and speaker diarization systems no sliding window global top k maxpooling hasim sak andrew senior and francoise beaufays long short term memory recurrent neural network architectures for large scale acoustic modeling in fifteenth annual conference of the international speech communication association li wan quan wang alan papir and ignacio lopez moreno generalized end to end loss for speaker verication arxiv preprint quan wang carlton downey li wan philip manseld and ignacio lopez moreno speaker diarization with lstm arxiv preprint references pinsky now yury home blog google products assistant tomato google home now supports multiple users tomato tomahto supports multiple google users mihai matei voice match will google low to androidheadlines voice will allow google home to recognize your voice html recognize home your voice najim dehak patrick j kenny reda dehak pierre mouchel and pierre ouellet front end factor analysis for speaker verication ieee transactions on audio speech and language processing vol no pp daniel garcia romero and carol y espy wilson analysis of i vector length normalization in speaker recognition systems in interspeech pp yann lecun yoshua bengio and geoffrey hinton deep learning nature vol no pp johan rohdin anna silnova mireia diez oldrich plchot end to end dnn based arxiv pavel matejka and lukas burget speaker recognition inspired by i vector and plda preprint georg heigold ignacio moreno samy bengio and noam shazeer end to end text dependent speaker verication in acoustics speech and signal processing icassp ieee international conference on ieee pp sepp hochreiter and jurgen schmidhuber long short term memory neural computation vol no pp guoguo chen carolina parada and georg heigold footprint keyword spotting using deep neural networks in acoustics speech and signal processing icassp ieee international conference on ieee pp rohit prabhavalkar raziel alvarez carolina parada preetum nakkiran and tara n sainath automatic gain control and multi style training for robust small footprint keyword spotting with deep neural networks in acoustics speech and signal processing icassp ieee international conference on ieee pp jan k chorowski dzmitry bahdanau dmitriy serdyuk kyunghyun cho and yoshua bengio attention based els for speech recognition in advances in neural information processing systems pp minh thang luong hieu pham and christopher d manning effective approaches to attention based neural machine lation arxiv preprint kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel and yoshua gio show attend and tell neural image caption generation with visual attention in international conference on machine learning pp florian schroff dmitry kalenichenko and james philbin facenet a unied embedding for face recognition and tering in proceedings of the ieee conference on computer vision and pattern recognition pp
