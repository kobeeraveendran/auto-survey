framerank a text processing approach to video summarization zhuo chao qian guoping doctoral innovation center of computer science the university of nottingham ningbo china of information engineering and guangdong key lab for intellignet information processing shenzhen university china of computer science the university of nottingham uk zhuo lei chao zhang qian edu cn guoping ac uk r a l c s c v v i x r a abstract video summarization has been extensively studied in the past decades however user generated video summarization is much less explored since there lack large scale video datasets within which human generated video summaries are biguously dened and annotated toward this end we pose a user generated video summarization dataset that consists of videos minutes in structing the dataset because of the subjectivity of generated video summarization we manually annotate summaries for each video which are in total summaries to the best of our knowledge it is currently the largest dataset for user generated video summarization based on this dataset we present framerank an pervised video summarization method that employs a to frame level afnity graph to identify coherent and mative frames to summarize a video we use the based graph to rank temporal ments according to the amount of semantic information tained in their frames we illustrate the effectiveness of our method by applying it to three datasets summe tvsum and and show it achieves state of the art results index terms video summarization unsupervised learning framerank kl divergence graph introduction user generated video is growing exponentially hence the demand for efcient ways of searching and retrieving desired content will cost huge amounts of resources like time man resources and machine congurations however users always think little of time spending cutting content and view selection thus user generated videos consist of long including illumination shakiness dynamic background and so on and unedited contents in this text video summarization plays an important role in assisting users to quickly browse through important events contained in it recently video summarization techniques have drawn a lot of attention especially for user generated videos the fig overview of the proposed framework essential of user generated video summarization is to identify important parts of original videos and dene their importance however the insufciency of publicly available datasets has limited this important line of research due to the tivity human generated summaries are the most needed to meet the purpose of training and evaluation for different generated video summarization methods to help alleviate this we introduce a new dataset which contains videos covering various user generated contents and human generated video summaries for each one more another challenge is no standard criteria for ing importance even humans can not agree on a universal sis for generating video summary in this paper we consider frame importance as the ones can most substitute others based on dataset we propose a novel pervised framework for user generated video summarization which identies coherent and informative video frames to summarize a video as shown in figure we rst divide an original video into disjoint segments with a dense based clustering method we then develop a graph based ranking method framerank to score and rank these ments according to the amount of semantic information table comparison between existing datasets summe ours tvsum dataset ute static video dynamic video egocentric video total frame avg frame total video length s avg video length s avg mary per video f score avg cronb n a n a n a nally we sample video segments with high scores to ate video summaries through systematic experiments and evaluation we show the proposed novel video summarization method is effective and outperforms state of the art methods on the new dataset and the other two existing datasets summe and tvsum our main tions are as the followings we introduce a new dataset for user generated video summarization to the best of our knowledge it is the largest user generated video tion dataset able to meet the purpose of training and ation for different methods we develop a new method framerank to assess the importance of video frames we proposed a novel approach for video temporal segmentation in which segments are semantically consistent and ate to produce good video summaries related work datasets to facilitate the comparison between these datasets and our dataset we show in table more dataset ics to be noted the annotation information on ute is not available since it does not apply human generated video summaries to conduct the evaluation while asking question to participants generally speaking previous datasets have greatly boosted the researches in user generated video marization but still have several drawbacks first video types are still insufcient second multiple human generated video summaries for each video are necessary due to their tivity third these datasets do not cover abundant categories user generated video summarization most works attempt to assess importance or interestingness of video frames with supervised methods trained a model according to signicant interaction between people and jects to learn the saliency in egocentric videos built links between objects to create story driven summaries grounded on s egocentric feature combined multiple tures to train a regressor to predict interestingness devised a two steam deep convolutional neural network cnn chitecture by fusing spatial and temporal information tively on each steam for video highlight detection learned non parametrically to transfer summary structures from ing videos to the test ones and improved long term memory lstm to model the variable range ral dependency among video frames in general supervised methods require a large amount of training data which is what is lacking in this eld existing datasets are not able to cover variety of user generated videos because what users are interested in can not be exactly dened while human generated summaries are labor intensive and time consuming and it requires multiple summaries for each single video due to the subjectivity therefore the learned models may be not portable to work on user generated videos some unsupervised methods are put forward in generated video summarization in detail these methods use various types of intuitive criteria or pre trained models from other elds to facilitate the assessment of importance or estingness of video frames with video s title or keywords as query obtained canonical viewpoints collected from website to predict important frames to achieve tion employed an auto encoder to train with internet videos of the same topic and then assessed importance cording to how well it can reconstruct input video s feature utilized a linear svm classier to obtain the condence of event type as importance scores however although tant frames can be effectively predicted these methods can only work on domain specic videos or require metadata of videos moreover retrieving these images or videos is sive even collected metadata may not be relevant or correct benchmark dataset user generated video summarization is a relatively plored domain and there are few public datasets with ple human generated summaries available we therefore lected a new dataset that contains videos and each has human annotated summaries video collection we collected videos captured either by ourselves or from youtube which are recorded in multiple ways including static dynamic and egocentric views the duration ranges from to minutes they are all raw or minimally edited to group video frames into disjoint segments of semantically consistent frames second we rank segments ing to the amount of semantic information contained in their frames using the graph based framerank finally we apply a greedy selection strategy to generate nal summaries video temporal segmentation comparing to we divide videos with a temporal tation method based on the clustering of deep semantic ity graph in general connection between frames can be sidered as a graph where vertexes refers to the video frames and edges are the pairwise similarities the initial idea was to transfer a video to text and form text summarization however image tagging and image caption are still open areas even with correctly classied bels it is still difcult to precisely represent semantic content of video frames besides frame contents may be different even though they have the same labels to convey tic information effectively we decide to use the probability in detail distribution of a set of labels to denote a frame we feed video frames to a deep cnn pre trained on ages of object categories from imagenet dataset to compute the probabilities of frames containing objects this representation enjoys the advantage of capturing information of the presence of a variety of object categories we choose the kl divergence to measure how well a frame can represent another which is a measurement of the difference between two probability distributions it is the amount of information loss when a distribution is used to proximate another distribution which can be interpreted how much a frame contains semantic information of another fig comparison between segments with and without using temporal constraint factor vertical axis is segment indexes where the negative represent generated segment indexes with temporal order factor and the positive represent the ones out temporal order factor horizontal axis is temporal order we construct a graph w where v fi are texes and w are edges between vertex fi and fj edge wij is the kl divergence computed as follows fig we show these videos represented by their thumbnails we collected videos encompassing various user generated contents like holidays events and sports compared with other datasets ours has more videos categories and generated summaries figure shows the thumbnails video annotation due to the subjectivity of user generated video tion it is almost impossible to obtain absolute ground truth bels thus evaluation is often carried out with multiple human judgment we asked participants to collect human generated summaries videos were shown to participants in a random order playing at the speed of second per frame participants were asked to watch entire video in a single take and provide time slots to generate video summary we muted audio to ensure scores are based solely on visual stimuli we have different annotations for each video taking more than hours following we calculated average wise score among collected human ground truth the score is which is approximately close to that of summe and tvsum dataset and meanwhile we puted cronbach which is a standard measure to assess the reliability of a psychometric test the dataset has a mean of the minimum value is and the mum value is the cronbach of summe and tvsum datasets are and respectively ideally is around while is considered acceptable in exploratory searches thus with dataset video summarization experiments can be carried out with condence we provide a full list of the videos including name camera type frame number video length average summary length average score and cronbach in supplementary material proposed framework the proposed framework consists of three main steps as shown in figure which inputs a user generated video and output a video summary we construct a graph where vertex corresponding to a frame and edge between two vertexes is the kl divergence of two frames semantic probability butions first we use a bundling center clustering algorithm log pfj k where i and j are frame indexes and means element wise multiplication represents the probability of label of frame fi we negate values to transfer the difference into ilarity and normalize matrix g in addition w is a constrained graph with a gaussian function to maintain poral order and smooth frame difference where w w ij are edges between frames each vertex can be represented as w ij e where is a control parameter to modify temporal tion and smoothness level hence a temporally constrained graph g tc can be represented as g tc g g furthermore cluster center can be multiple similar frames rather than a single one which is denoted as bundling center with a dense neighbor based clustering method we can identify local clusters based on the edge connectivity on g tc to be noted elements of a local cluster are locally similar to all of the other elements inside neighborhood instead of being close to a single element more details can be referred to we show examples of the comparison between results with or without temporal constraint factor in figure segment selection with framerank the difculty in video summarization is how to dene tant frames or segments to compose summary there are no standard criteria for measuring the importance of video ments even human subjects can not agree on a universal sis a good summary should be concise and retain the most informative and signicant contents in other words selected frames or segments that compose the summary should be able to represent the unselected ones as much as possible in this paper we dene important frames as the ones can substitute others with least information loss as described in section with w we develop the framerank method which works similarly to the trank text ranking method in natural language ing we build a graph with its vertices corresponding to video frames and edges measure the similarity between the frames we then implement a graph ranking technique to measure ative importance of each video frame as well as segment we calculate importance score of the vertex fi as d fj wji wjl where is a damping factor and which plays the role of integrating the model into the probability of ing from a given vertex to another random vertex in the graph in detail damping factor d can be interpreted as a dom suffer changing of visual contents which may be caused by a sudden camera moving in the case of user generated videos following we set d the running of the algorithm starts with arbitrary ues assigned to each vertex in the graph and iterates until in our implementation we stop the iteration convergence when importance scores between two consecutive iterations is below a given threshold let i be the score of tex fi at iteration k the iteration stops at kth iteration if i where is a pre set threshold after the algorithm converges each vertex has a score resenting the importance of video frames associated with the vertex the nal importance scores of the vertices of erank are not dependent on initial values only the number of iterations to converge may be different other related work tried to estimate the score of ment by summing up all frame importance scores however it may result in longer segments getting larger importance scores thus we compute relative importance score of the segment sn with the average importance tstart tstart tend where tstart and tend are start and end frames of the segment video summary generation we generate a video summary by selecting video segments that can substitute the others with the least information loss given the set of importance scores we want to nd a subset of segments with their total length below a pre dened imum l while the total importance scores is maximized in other words we want to solve the optimization problem max s t l where and xn indicates the segment is selected under the assumption of independence between scores this maximization is a standard knapsack problem with a greedy selection strategy furthermore generated videos rarely contain redundant interesting events hence we do not account for redundancy and diversity evaluation and discussion to demonstrate the effectiveness of our proposed video marization approach we evaluated and compared it with state of the art methods we carried on experiments on three video datasets summe interestingness modular dpp video mmr tvsum web image prior livelight dpplstm and random form following we compute f socre against man summaries for evaluation according to temporal overlap comparing to computed ones table we compare our approach with state of the art methods on summe tvsum and ugsum datasets dataset method f score fig quantitative results with different features and larity measurement interestingness submodular dpp vslstm dpplstm video mmr framerank ours livelight web image prior tvsum vslstm dpplstm framerank ours random uniform framerank framerank ours summe tvsum ugsum results table summarizes the performance of our methods and trasts to those attained by prior work the highlighted bers indicate framerank obtains the best performance in the corresponding setting we achieve the highest overall f score of on summe dataset and on tvsum dataset the previous state of the art published was and spectively furthermore we also carry on experiments on our dataset ours is superior to the other two methods where f score is it shows not only the fectiveness of the proposed framerank method but also the reliability of the proposed temporal segmentation method by comparing framerank and uniform framerank methods it proves our method is able to nd important segments to produce an informative summary the results demonstrate the proposed method can create video summaries closer to human level performance than other methods meanwhile it is interesting to see our result is better than all the supervised methods we analyze there are no standard rules to dene what important content is to summarize video thus human generated summaries may be quite different from each other due to different human perception and personal experience we believe training data for user generated video tion is not sufcient for supervised methods and the ated model is not able to characterize the property to marize videos moreover since tvsum is slightly different from summe and where it only contains egories of videos thus in theory the characteristic of sum should be suitable for supervised methods to learn video structure while our framerank still performs better fore we have reasons to believe a good unsupervised method is more appropriate for user generated video summarization analysis and discussion temporal video segmentation we analyze the performance gained by different temporal segmentation methods figure we compare our kl divergence based temporal tation approach with the following methods joint temporal segmentation jts uniform segmentation based temporal segmentation kts superframe motion based temporal segmentation keyframe marization highest scored frames disregarding the temporal segment process we employ the same segment selection method framerank and summary generation method figure shows our temporal segmentation method yields a better performance it demonstrates the signicance of structural analysis in video summarization first based summarization is better than keyframes we lieve summarization annotation is segment based rather than frame based because it is quite expensive and difcult for participants to give scores to individual frames in fact ticipants were not required to select a segment during tation moreover we nd our approach has a greater vantage on summe and datasets and close sults for tvsum may be caused by similar videos thus it demonstrates segment based summary is in better agreement with human perception and produces more reasonable maries because segments contain motion information paring to keyframes furthermore it also demonstrates our approach to cluster semantically similar frames matches ter with human perception it shows using such a grouping is indeed more semantically logical therefore the tal results prove the superiority of our segmentation approach which is capable of generating meaningful summaries feature and similarity metrics we investigate the portance and reliability of different features and similarity metrics in the framerank approach in figure we show the performance gained by using different features deep visual feature deep semantic feature label embedding and label and different similarity metrics euclidean cosine kl vergence and label overlap label overlap is the textrank as could be expected deep semantic features with the kl y j lee j ghosh and k grauman discovering portant people and objects for egocentric video rization in cvpr june pp z lu and k grauman story driven summarization for egocentric video in cvpr m gygli h grabner and l v gool video rization by learning submodular mixtures of objectives in cvpr t yao t mei and y rui highlight detection with pairwise deep ranking for rst person video tion in cvpr june k zhang w l chao f sha and k grauman mary transfer exemplar based subset selection for video summarization cvpr k zhang w l chao f sha and k grauman video summarization with long short term memory in eccv pp a khosla r hamid c j lin and n sundaresan large scale video summarization using web image ors in cvpr june pp g kim l sigal and e p xing joint summarization of large scale collections of web images and videos for storyline reconstruction in cvpr d potapov m douze z harchaoui and c schmid in eccv category specic video summarization pp z lei k sun q zhang and g qiu user video marization based on joint visual and semantic afnity graph in acm multimedia workshop on vision and language integration meets multimedia fusion k simonyan and a zisserman very deep tional networks for large scale image recognition in iclr o russakovsky j deng h su j krause s satheesh s ma z huang a karpathy a khosla m stein a c berg and f li imagenet large scale visual recognition challenge s kullback and r a leibler on information and sufciency ann math statist pp q zhang and g qiu bundling centre for landmark image discovery in acm icmr r mihalcea and p tarau textrank bringing order into text in conference on empirical methods in ural language processing pp y li and b merialdo multi video summarization based on video mmr in wiamis april fig quantitative results of different temporal segmentation methods kts and jts are short for kernel and joint based temporal segmentation methods respectively divergence metric performs the best first in most cases tures with semantic information are better than deep visual feature alone in summarizing video due to similar contents in tvsum deep visual feature achieves relatively good sults furthermore it is interesting to observe label ding feature has a comparable result to others except furthermore the kl divergence has a better bility in measuring information loss when using one frame to represent others hence it also proves our denition of portance of frames is reasonable and effective conclusion we introduce a new benchmark for generated video summarization we have proposed a new unsupervised method for video summarization with a novel dense neighbor based clustering method our approach rst partitions video into segments based on the deep semantic similarity of frames we then develop a graph based ing method framerank to rank these segments finally we sample segments with high information scores to generate video summary we show our framerank method achieved results which are superiority to state of the art methods acknowledgement the author acknowledges the nancial support from the national doctoral innovation centre ningbo education reau ningbo science and technology bureau and the versity of nottingham this work was also supported by the uk engineering and physical sciences research cil grant number ep references m gygli h grabner h riemenschneider and l v in creating summaries from user videos gool eccv pp y song j vallmitjana a stent and a jaimes sum summarizing web videos using titles in cvpr pp
