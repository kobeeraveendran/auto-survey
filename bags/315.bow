align summarize automatic alignment methods summarization corpus creation paul david janiszek yannick estve vincent nguyen lium le mans universit ubiqus labs universit paris descartes lia avignon universit com david fr yannick avignon fr com l u j l c s c v v x r abstract summarizing texts straightforward task considering text summarization determine kind summary expected information compressed relevant reformulate summary stick original phrasing state art automatic text summarization revolves news articles suggest considering wider variety tasks lead improvement eld terms generalization robustness explore meeting summarization generating reports automatic transcriptions work consists segmenting aligning transcriptions respect reports suitable dataset neural summarization bootstrapping approach provide pre alignments corrected human annotators making validation set evaluate automatic models consistently reduces annotators efforts providing iteratively better pre alignment maximizes corpus size annotations automatic alignment models evaluation conducted novel corpus aligned public meetings report automatic alignment summarization performances corpus automatic alignment relevant data annotation leads large improvement rouge scores summarization task keywords alignment summarization corpus annotation introduction automatic text summarization task producing short text captures salient points longer large variety tasks t tion factors critical summarization process rephrase source abstractiveness use source extractiveness length tio target source compression factor source target lengths variances information distribution e important information distributed text summarization benchmarks et al paulus et al gehrmann et al rely news articles cnn dailymail hermann et al nallapati et al exhibit particular istics extractive e picking tions text source opposite abstractive liu et al high compression factor summary times shorter source liu et al low variance source target length iv concentrating information ginning article example papers working cnn dailymail hermann et al nallapati et al truncate article rst words article et al gehrmann et al ziegler et al ignoring half contrast explore meeting data transcription source meeting report target contrary news articles high variance length speaker interventions data need rephrased written form abstractive process nature informative work focus called exhaustive reports meant capture information track speaker terventions information summarized speech compressed oral form written compression factor lower news tasks variance remains high depending verbose intervention data hand consist exhaustive reports produced ubiqus house editors ii audio recording meeting automated transcript produced ter automatic speech recognition system close described hernandez et al meignier merlin trained french language internal data data suitable summarization learning propose segment intervention level e said speaker starts particularly convenient nature dataset ensures interventions remain apart short ones chronological order preserved scription report reports explicitly mention ers making segmentation trivial error free transcriptions features present alignment process maps interventions reports related transcription sentences based similarity bootstrap corpus creation iterating matic pre alignment generations corrections man annotators aim jointly minimizing human effort ne tuning automatic alignment models eventually use alignment models automatic data annotation paper present methodology building marization based segmentation alignment reports transcriptions meetings strapping approach present novel public meeting dataset evaluate automatic alignment summarization tion models rst trained gold set human annotator automatic annotations automatic alignment models outperform baseline large margin considered rouge metrics source code data reproduction instructions found com pltrdy autoalign related work work aims jointly segment related les scription report meeting segment report actually corresponds j ment transcription report segmentation simple thanks ture focus transcription bearing mind task similar linear segmentation problem e nding borders segments hearst posed texttiling linear segmentation algorithm compares adjacent blocks text order nd subtopic shifts borders segments moving window text identifying borders thresholding proposed choi uses similarity ranking matrices instead clustering locate topic boundaries texttiling extended audio signals jee rudnicky said lack robustness atypical participant behavior common text work word embeddings order capture similarity query answer dialogue context song et al alemi ginsparg plore word embedding use segmentation ing existing algorithms showing improvements badjatiya et al address segmentation task end end attention based neural approach approach investigated future consider work lack reference data glavas et al use semantic relatedness graph resentation text derive semantically coherent ments maximal cliques graph issue approach searching large segments big texts requires decreasing threshold exponentially increases computational cost eventually making task intractable alignment alignment studied creation particular barzilay elhadad nelken shieber extract related segments pedia britannica britannica elementary simpler sion different work looking total alignment e documents fully aligned partially extracted furthermore alignment oral speech written form studied braunschweiler et al context audio books lecouteux et al subtitles transcripts e news report order prove automatic speech recognition engines approaches sound similar look act matches approximate alignment metrical data based textual similarity summarization datasets hermann et al nallapati et al proposed rst multi sentence summarization dataset training pairs sources words long truncated rst words et al gehrmann et al ziegler et al target words average similar dataset based ny times articles presented paulus et al times training pairs sources words targets words average liu et al work generating wikipedia introductions known leads reference articles web crawled data inputs outputs orders tude longer sources words targets range context dealing limited resources particular respect ready train data vated paper dataset comprises gold dard training pairs pairs taking account automatically aligned data currently ter training pairs order contain fewer words sentences future work explore wider range segment lengths methods task consists nding best alignment meeting transcription ti related human written report rj documents segmented mutually exclusive sets sentences t t m rn alignment maps transcription segment t m actly report segment rn based sentence level similarities si j r j ti r j alignment process pipeline different modules rst reads data second independently segments respectively report transcription computes similarity scores order nd alignment maximizes overall score section presents modules segmentation segmentation consists nding borders texts segment processed independently mentation granularity ne segments long learning cult result fewer training pairs coarse remain relevant short segments ingfully summarized consider speaker interventions e uninterrupted speech speaker appropriate segmentation level particular assumption task writing report roughly divided sub tasks consisting reports vention close approximation exhaustive ports report speaker s intervention plicitly mentioned special tags document particular style applied names identied based identication e looking mr ms transcription segments groups sentences dened automatic speech recognition system text representation similarity function alignment process consists nding scription segments t m related report segment rn words function n m m n n consider sentence level similarity matrix s transcription report si j scor r j r score function experimented rouge lin cosine similarity t d sentations cosine similarity based word embedding vectors pool ng function typically sum applied word embeddings produce sentence embeddings shown gure default sets use sliding windows overlap sentences document d k sliding window w d o s set s sentences having rst respectively o sentences common previous window resp w d o s k si sliding windows aggregate sentence representations single vector ag g function gure calculate scores pairs sliding windows sides s sl d ng l score g w t o s ag g w r o s l similarities assigned sentence level o s l s t o s w r si j r ed sl d ng l s reduction function r ed sum product calculates sentence scores sliding windows contain alignment having sentence level sentence windows similarities pairs transcription report alignment task maximize similarity document level use dynamic programming aims nd optimal path similarity matrix ensuring design transcription report aligned chronologically introduce alignment matrix nate j corresponds similarity eventually scaled power p plus maximal value left j neighbor coordinates ai j si j p j ai j p determined punctuation predicted speech recognition system transcription figure text representations windows words position j track previous tion e j hi j arg max j ac ultimately giving optimal path correspond sentence level alignment p hi j j pk pi j j figure shows simple example alignment process derive segment level alignment transcription segment t choose r j maximize similarity path arg max ai j j p n m s rn evaluation linear segmentation performance measured windowdiff pevzner hearst pares boundaries predicted algorithm reference moving window size k windowdiff based pk beeferman et al meant fairer respect false negatives number boundaries segment size near miss errors report windowdiff scores experiments consider simple metrics segment accuracy word accuracy experience scores micro averaged reference les experiments bootstrapping corpus creation build corpus scratch iterate phases generating pre alignments data tomatic alignment model correct pre alignment thanks human annotators gold reference set evaluate models respect new reference set iterations increase gold references allowing accurate evaluation automatic alignment models tually making annotators task easier wordembeddingswordssentenceembeddingssliding figure example dynamic programming algorithm nding optimal path coordinates j alignment right adds corresponding similarity s left highest neighbor value left j shown arrows equivalent h red arrows represent optimal path p similarity values arbitrary simplicity gold alignments developed ad hoc platform collect gold alignments thanks human annotators serve reference sets use automatic alignment models provide pre alignment corrected notator grid search order evaluate wide variety rameters reasonable computational cost use validation sets varying reference les evaluation process iteratively selects best parameters reducing number evaluates sub sets bigger reference set helps efciently explore parameter space spending effort ously sub performing parameter sets eventually tify critical parameters iteration diagonal alignment rst iteration started reference le way quantitatively evaluating auto alignment cess order provide pre alignment human annotator naive approach aligns segments diagonally compute similarity si j j alignment matrix stay diagonal e replace position history matrix h eq hi j j ri j r t r ri j iteration exploring scoring functions second iteration mainly explored different sentence resentations scoring functions plain text measure rouge scores lin precisely f f rl f use vector representations text based t d latent semantic analysis ii pre trained french word embeddings nier score sentences based cosine similarity word embeddings trained mikolov et al experimented cbow gram variants signicant performance differences measuring similarities sliding windows instead sentences directly meant reduce impact lated sentences low similarities fact data nt perfectly match sentences low similarity inside segments actually discuss point parameters related sliding windows window size overlap experimented combinations s related scores consider aggregation tion function parameters experiment ag g sum mean max r ed od uc iteration ne tuning embedding based models alignment phase found dynamic programming algorithm direction long time example report sentence high similarities lot transcription sentences resulting monotonical alignment limit behavior troduce horizontal vertical decay factors respectively hd typically lower scores direction consider decayed alignment matrix j ai j di j di j di j hd ai j ai j di j vd decay reset di j change direction iteration finally select set public meetings order available productions benchmarks smaller corpus test set ne tuning data alignment summarization tasks models consider linear segmentation baselines texttiling hearst linear segmentation baselines penalized comparison methods use report document content particular methods wrong segment number xed report tation fairer comparison consider parameters sets produce excepted number segments segment number explicitly set grid search texttiling parameters graphseg glavas et al considered producing long segments comparable work requires low relatedness threshold nentially increases computational cost summarization trained neural summarization models data incorporating rst gold set cally aligned data pre processing include ltering ments based number words sentences e consider segments wor d s sent ences opennmt et al train models vaswani et al similar line presented ziegler et al difference use copy mechanism evaluation conducted test set uses rouge f metric lin results automatic alignment evaluation table compares performances automatic alignment models diagonal baseline shows interesting performances ticular outperforms large margin linear tion algorithms tf idf rouge based models embeddings based approaches totally different level performances twice better diagonal baseline times better considered algorithm validation set introducing decays alignment stage meant avoid alignment monotonic started ing small decays horizontal vertical axes results clear decays key parameters particular found vertical decay vd greater impact horizontal decay hd turned maximal performances similarly scaling scores power p alignment improves model fact helps model distinguish good scores average ones sliding windows performs better sentence s o case tf idf models tion e reach scores observed different congurations sizes overlaps aggregations reduction functions reach high scores human evaluation human annotators align transcription segments spect report segments based pre alignment produced automatic alignment models ne tuning models provided better pre alignments eventually making annotator s task easier alignment process annotators consists checking pre alignment correcting mismatches segment time port human evaluated segment level accuracy ratio segments modied annotator total number segments figure table iteration accuracy distribution observe accuracy consistently creasing iterations com opennmt opennmt py documents annotator score mean med iteration iteration iteration table human evaluation automatic alignments figure annotator evaluation respect matic pre alignment iterations summarization summarization models rst trained human annotated alignments larger dataset contains training pairs emanating automatic alignment nd automatic ment data annotation makes substantial difference summarization performance rouge points table result encouraging motivates continue automatic annotation dataset pairs t r n t est rouge score f rl gold dataset gold automatic table scores set matic summarization models trained human references vs extend dataset annotations automatic alignment discussion future work alignment process assumption transcription segment aligned practice asked human annotators lter irrelevant segments segments validation set model window s o window scoring ag g r ed alignment hd vd p dev acc seg wor d dev wd test acc test wd seg wor d texttiling diagonal tf idf rouge embeddings sum sum mean mul max prod cat sum cat sum sum prod sum prod sum prod table automatic alignment models evaluation test set meetings metrics segment accuracy word accuracy windowdiff validation set reference meetings agged order assigned port segments evaluation penalize models false alignment assigned irrelevant segments results comparable future models capable noring transcription segments idea important phenomenon adapt word accuracy ignore irrelevant segments nd absolute ence wor wal g ned w wal g ned w wi r r ev ant word embedding vectors work trained fauconnier publicly results fully available ducible training embedding vectors data interesting area future research improve quality automatic alignment lastly like study alignment scores provided models predict ment quality predictions lter tomatic annotations use potentially relevant automatically aligned segments conclusion paper explored development automatic alignment models map speaker interventions ing reports corresponding sentences tion meetings hours making able sources training segmentation key pre processing step neural approaches automatic summarization models align transcription sentences provided speech recognition system spect report segments delimited tags document information vectors found fauconnier github header explicitly specifying change speaker introduce novel meeting marization corpus evaluate matic alignment summarization shown automatic alignment models allow greatly increase corpus size leading better marization performance rouge metrics rl bibliographical references alemi ginsparg p text tation based semantic word embeddings corr mar badjatiya p kurisinkel l j gupta m varma v attention based neural text segmentation lecture notes computer science including subseries lecture notes articial intelligence lecture notes bioinformatics banerjee s rudnicky texttiling based approach topic boundary detection ings proceedings annual conference ternational speech communication association speech volume pages barzilay r elhadad n sentence alignment monolingual comparable corpora proceedings conference empirical methods natural language processing volume pages ristown nj usa association computational guistics beeferman d berger lafferty j cardie c mooney r statistical models text tation machine learning braunschweiler n gales m j f buchholz s lightly supervised recognition automatic alignment large coherent speech recordings ceedings annual conference national speech communication association speech pages nelken r shieber s m robust context sensitive sentence alignment monolingual corpora eacl conference pean chapter association computational guistics proceedings conference pages paulus r xiong c socher r deep reinforced model abstractive summarization international conference learning representations iclr conference track proceedings pevzner l hearst m critique provement evaluation metric text segmentation computational linguistics liu p j manning c d point summarization pointer generator networks proceedings annual meeting tion computational linguistics volume long pers apr song y mou l yan r yi l zhu z hu x zhang m dialogue session tion embedding enhanced texttiling proceedings annual conference international speech interspeech communication association oct vaswani shazeer n parmar n uszkoreit j jones l gomez n kaiser polosukhin kaiser l polosukhin attention need guyon al editors advances neural information processing systems pages curran ciates inc jun ziegler z m melas kyriazi l gehrmann s rush m encoder agnostic adaptation tional language generation corr choi f y y advances domain dent linear text segmentation meeting north chapter association tional linguistics fauconnier github io j french word embeddings gehrmann s deng y rush m abstractive summarization proceedings emnlp glavas g nanni f ponzetto s p supervised text segmentation semantic relatedness graphs sem joint conference ical computational semantics proceedings pages berlin germany association tional linguistics hearst m texttiling segmenting text multi paragraph subtopic passages computational guistics hermann k m kocisk t grefenstette e espeholt l kay w suleyman m blunsom p teaching machines read comprehend vances neural information processing systems ume janua pages jun hernandez f nguyen v ghannay s tomashenko n estve y ted lium twice data corpus repartition experiments speaker adaptation lecture notes computer science cluding subseries lecture notes articial intelligence lecture notes bioinformatics volume lnai pages klein g kim y deng y senellart j rush m open source toolkit neural chine translation acl annual meeting association computational linguistics ings system demonstrations pages lecouteux b linars g oger s ing imperfect transcripts speech recognition systems building high quality corpora computer speech language lin c y rouge package automatic tion summaries proceedings workshop text summarization branches pages liu p j saleh m pot e goodrich b sepassi r kaiser shazeer n kaiser l shazeer n j liu p saleh m pot e goodrich b sepassi r kaiser l shazeer n generating wikipedia marizing long sequences international conference learning representations iclr conference track proceedings jan meignier s merlin t lium tion open source toolkit tion cmu spud workshop page mikolov t sutskever chen k corrado g dean j distributed representations ofwords phrases compositionality advances neural information processing systems nallapati r zhou b dos santos c n gulcehre c ang b dos santos c xiang b tive text summarization sequence sequence rnns proceedings conll
