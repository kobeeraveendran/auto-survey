improving social media text summarization by learning sentence weight distribution jingjing xu moe key laboratory of computational linguistics peking university school of electronics engineering and computer science peking university t c o l c s c v v i x r a abstract recently encoder decoder models are widely used in social media text rization however these models times select noise words in irrelevant tences as part of a summary by error thus declining the performance in der to inhibit irrelevant sentences and cus on key information we propose an effective approach by learning sentence in our model we weight distribution build a multi layer perceptron to predict sentence weights during training we use the rouge score as an alternative to the estimated sentence weight and try to imize the gap between estimated weights and predicted weights in this way we courage our model to focus on the key tences which have high relevance with the summary experimental results show that our approach outperforms baselines on a large scale social media corpus introduction text summarization is an important task in ral language processing it aims to understand the key idea of a document and generate a headline or a summary previous social media text marization systems rush al hu et al are mainly based on abstractive text marization most of them belong to a family of encoder decoders which have shown effective in many tasks like machine translation cho et al dzmitry bahdanau and bengio however these models sometimes select noise words in irrelevant sentences as part of a mary by error figure gives an example of noise words generated by a state of the art decoder model rnn context unlike lation which requires encoding all information to ensure the accuracy summarization tries to tract the most important information more only a small part of sentences convey the key information while the rest of sentences usually are useless thus these unrelated sentences make it hard for encoder decoder models to extract key information in order to address this issue we propose a novel method by learning sentence weight tribution to encourage models focus on key tences and ignore unimportant sentences in our approach we rst design a multi layer tron to predict sentence weights then ering that rouge is a popular evaluation rion for summarization we estimate the gold tence weights of training data by rouge scores between sentences and summaries during ing we design an end to end optimization method which minimizes the gap between predicted tence weights and estimated sentence weights we conduct experiments on a large scale social media dataset experimental results show that our method outperforms competitive baselines sides we do not limit our method to any cic neural network it can be extended to any sequence to sequence model related work summarization approaches can be divided into two typical categories extractive summarization radev et al aliguliyev woodsend and lapata ferreira et al cheng and lapata and abstractive tion knight and marcu bing et al rush et al hu et al gu et al for extractive summarizations most works ally select several sentences from a document as a summary or a headline for abstractive figure illustration of the negative inuence of noise words rnn context is the basic decoder model with the attention mechanism the gold summary shown in blue is that the rate of china economy growth slows down and the investment in real estate is certainly better than however the key words are unseen in the output of rnn context while words shown in pink in irrelevant sentences are selected as part of a summary rization most works usually encode a document into an abstractive representation and then ate words in a summary one by one most cial media summarization systems belong to stractive text summarizaition generally ing extractive summarization achieves better formance than abstractive summarization for long and normal documents however extractive marization is not suitable for social media text which are full of noises and very short neural abstractive text summarization is a newly proposed method and has become a hot research topic in recent years unlike the ditional summarization systems which consist of many small sub components that are tuned rately knight and marcu erkan and radev moawad and aref neural abstractive text summarization attempts to build and train a single large neural network that reads a document and outputs a correct summary rush al rst introduced the encoder decoder framework with the attention mechanism to abstractive text summarization bing et al proposed an abstraction based multi document summarization framework which can construct new sentences by exploring more ne grained syntactic units than sentences gu et al proposed a copy mechanism to address the problem of unknown words nallapati al proposed several novel models to address critical problems in marization proposed model our method is based on the basic encoder decoder framework proposed by cho et al and sutskever et al section introduces how to estimate sentence weight distribution in detail section describes how to generate the representation of sentence weight distribution section shows how to incorporate estimated sentence weights and dicted sentence weights in training estimating sentence weight distribution assume we are provided with a summary y and a document d dn where n is the number of sentences the rst step of our method is to compute the distribution of sentence weights for training data as w wi wn where wi is computed as wi and ei is computed as ei rou y where rouge is the evaluation metric to judge the quality of predicted summaries train devlopment test representation of sentence weight distribution table details of lcsts dataset the size is given in number of pairs short text summary in our model we rst produce sentence weight distribution over all sentences the tion is based on the sentence embeddings s and the position embeddings of sentences as w j lp sj lp where sj denotes vector tion of sj and mlp refers to a layer perceptron sj is produced as sj where returns all indexes of words which belong to the jth sentence then the new output of an encoder part is h h h i h n models rnn hu et al swd rnn context hu et al swd r l table rouge scores r l rouge l of the trained els computed on the test and development sets rnn and rnn context are two baselines we refer our method as swd experiments in this section we evaluate our proposed approach on a social media dataset and report the mance of the models furthermore we use a case to illustrate the improvement achieved by our proach where n is the number of hidden states and computed as i is dataset h i w where returns the weight of sentence which belongs to and hi is the output of rnn or bi lstm used in an encoder then the new output of an encoder h is delivered to a decoder which produces a summary training given the model parameter and an input text a corresponding summary y and sentence weight distribution s described in section the loss function is n n m w i where n is the batch size is the ditional probability of the output word yi given source texts xi i j is the predicted sentence weight and is the conditional ability of the sentence weight wi j descirbed in section given source texts xi we use the large scale chinese short tion dataset lcsts which is provided by hu al this dataset is constructed from a famous chinese social media called sina based on the statistic data on the training set we set the maximum number of sentences as and the maximum length of a sentence as in this paper experimental settings following previous works and experimental sults on the development set we set parameters as follows the character embedding dimension is and the size of hidden state is the parameter is all word dings are initialized randomly we use the layer encoder and the layer decoder in this paper we use the minibatch stochastic gradient scent sgd algorithm to train our model each gradient is computed using a minibatch of pairs document summary best validation curacy is reached after batches which quires around days of training for evaluation place where a lot of popular chinese media and ganizations post news and information figure comparisons of the predicted summaries between rnn context and our method the dicted summary shown in blue of rnn context comes from an unimportant sentence in contrast the predicted summary of our method covers some key words shown in purple we use the rouge metric proposed by lin and hovy unlike bleu which includes ious n gram matches there are several versions of rouge for different match lengths and rouge experiments are performed on a commodity bit dell precision workstation with one ghz core cpu ram and one titan x gpu models we do not limit our method on specic neural network it can be extended to any sequence sequence model in this paper we evaluate our method on two types of baselines rnn we denote rnn as the basic sequence sequence model with a bi lstm encoder and a bi lstm decoder it is a widely used framework rnn context rnn context is a sequence sequence framework with the attention nism results and discussions we compare our approach with baselines ing rnn and rnn context the main results are shown in table it can be seen that our proach achieves rouge improvement over both baselines in particular swd outperforms context by almost points finally we give an example summary as shown in figure this example is illustrated in section aimed to show the negative inuence of portant words on extracting key information on rnn context rnn context chooses some portant words as summary like some fundings from the central government shown in blue in contrast the outputs of our method contain some key words shown in pink like fan gang the rate of china economy growth slows down this example shows the effectiveness of our model on handling noise documents which are full of a ber of irrelevant words conclusion in this paper we propose a novel method by ing sentence weight distribution to improve the performance of abstractive summarization the target is to make models focus on important tences and ignore irrelevant sentences the sults on a large scale chinese social media dataset show that our approach outperforms competitive baselines we also give the example which shows that the summary produced by our method is more relevant to the gold summary besides our method can be extended to any sequence sequence model word based systems are potentially helpful to this task cause the words can incorporate more meaningful information in the future we will try several word segmentation methods sun et al xu and sun xu et al to improve the system references ramiz m aliguliyev a new sentence similarity measure and sentence based extractive technique for automatic text summarization expert systems with applications lidong bing piji li yi liao wai lam weiwei guo and rebecca passonneau abstractive document summarization via phrase selection and merging in proceedings of the annual ing of the association for computational linguistics and the international joint conference on ral language processing volume long papers association for computational linguistics beijing china pages jianpeng cheng and mirella lapata neural summarization by extracting sentences and words in proceedings of the annual meeting of the association for computational linguistics ume long papers association for tional linguistics berlin germany pages kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger learning schwenk and yoshua bengio phrase representations using rnn encoder decoder for statistical machine translation in proceedings of the conference on empirical methods in ural language processing emnlp association for computational linguistics doha qatar pages kyunghyun cho dzmitry bahdanau and yoshua gio neural machine translation by jointly learning to align and translate in iclr gunes erkan and dragomir r radev lexrank graph based lexical centrality as salience in text journal of articial intelligence summarization research rafael luciano ferreira souza cabral rafael dueire lins gabriel pereira e silva fred freitas george dc cavalcanti rinaldo lima steven j simske and luciano favaro assessing sentence scoring techniques for extractive text summarization expert systems with applications jiatao gu zhengdong lu hang li and victor incorporating copying mechanism in li in proceedings of sequence to sequence learning the annual meeting of the association for computational linguistics acl august berlin germany volume long papers baotian hu qingcai chen and fangze zhu sts a large scale chinese short text summarization dataset in proceedings of the conference on empirical methods in natural language ing emnlp lisbon portugal september pages kevin knight and daniel marcu tion beyond sentence extraction a probabilistic proach to sentence compression articial gence chin yew lin and eduard hovy matic evaluation of summaries using n gram occurrence statistics in human language ogy conference of the north american chapter of the association for computational linguistics naacl edmonton canada may june shuming ma and xu sun a semantic vance based neural network for text summarization and text simplication corr shuming ma xu sun jingjing xu houfeng wang wenjie li and qi su improving semantic relevance for sequence to sequence learning of nese social media text summarization in ibrahim f moawad and mostafa aref mantic graph reduction approach for abstractive text in computer engineering summarization tems icces seventh international ence on ieee pages ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang abstractive text summarization using sequence in proceedings of the sequence rnns and beyond signll conference on computational natural language learning conll berlin germany august pages dragomir r radev timothy allison sasha goldensohn john blitzer arda celebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu al mead a platform for in tidocument multilingual text summarization lrec alexander rush sumit chopra and jason weston a neural attention model for abstractive tence summarization in emnlp the association for computational linguistics pages xu sun wenjie li houfeng wang and qin lu feature frequency adaptive on line training for fast and accurate natural language processing tational linguistics xu sun houfeng wang and wenjie li fast line training with frequency adaptive learning rates for chinese word segmentation and new word tion in pages xu sun bingzhen wei xuancheng ren and shuming ma label embedding network learning bel representation for soft training of deep networks corr xu sun yaozhong zhang takuya matsuzaki masa tsuruoka and junichi tsujii a criminative latent variable chinese segmenter with hybrid word character information in human guage technologies conference of the north ican chapter of the association of computational linguistics proceedings may june boulder colorado usa pages xu sun yaozhong zhang takuya matsuzaki masa tsuruoka and junichi tsujii abilistic chinese word segmentation with non local inf process information and stochastic training manage ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural works in advances in neural information ing systems pages kristian woodsend and mirella lapata matic generation of story highlights in proceedings of the annual meeting of the association for computational linguistics association for tational linguistics pages jingjing xu shuming ma yi zhang bingzhen wei xiaoyan cai and xu sun transfer learning for low resource chinese word segmentation with a novel neural network in the conference on natural language processing and chinese computing jingjing xu and xu sun dependency based gated recursive neural network for chinese word mentation in pages
