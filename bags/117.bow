improving social media text summarization learning sentence weight distribution jingjing xu moe key laboratory computational linguistics peking university school electronics engineering computer science peking university edu cn t c o l c s c v v x r abstract recently encoder decoder models widely social media text rization models times select noise words irrelevant tences summary error declining performance der inhibit irrelevant sentences cus key information propose effective approach learning sentence model weight distribution build multi layer perceptron predict sentence weights training use rouge score alternative estimated sentence weight try imize gap estimated weights predicted weights way courage model focus key tences high relevance summary experimental results approach outperforms baselines large scale social media corpus introduction text summarization important task ral language processing aims understand key idea document generate headline summary previous social media text marization systems rush et al hu et al mainly based abstractive text marization belong family encoder decoders shown effective tasks like machine translation cho et al dzmitry bahdanau bengio models select noise words irrelevant sentences mary error figure gives example noise words generated state art decoder model rnn context unlike lation requires encoding information ensure accuracy summarization tries tract important information small sentences convey key information rest sentences usually useless unrelated sentences hard encoder decoder models extract key information order address issue propose novel method learning sentence weight tribution encourage models focus key tences ignore unimportant sentences approach rst design multi layer tron predict sentence weights ering rouge popular evaluation rion summarization estimate gold tence weights training data rouge scores sentences summaries ing design end end optimization method minimizes gap predicted tence weights estimated sentence weights conduct experiments large scale social media dataset experimental results method outperforms competitive baselines sides limit method cic neural network extended sequence sequence model related work summarization approaches divided typical categories extractive summarization radev et al aliguliyev woodsend lapata ferreira et al cheng lapata abstractive tion knight marcu bing et al rush et al hu et al gu et al extractive summarizations works ally select sentences document summary headline abstractive figure illustration negative inuence noise words rnn context basic decoder model attention mechanism gold summary shown blue rate china economy growth slows investment real estate certainly better key words unseen output rnn context words shown pink irrelevant sentences selected summary rization works usually encode document abstractive representation ate words summary cial media summarization systems belong stractive text summarizaition generally ing extractive summarization achieves better formance abstractive summarization long normal documents extractive marization suitable social media text noises short neural abstractive text summarization newly proposed method hot research topic recent years unlike ditional summarization systems consist small sub components tuned rately knight marcu erkan radev moawad aref neural abstractive text summarization attempts build train single large neural network reads document outputs correct summary rush et al rst introduced encoder decoder framework attention mechanism abstractive text summarization bing et al proposed abstraction based multi document summarization framework construct new sentences exploring ne grained syntactic units sentences gu et al proposed copy mechanism address problem unknown words nallapati et al proposed novel models address critical problems marization proposed model method based basic encoder decoder framework proposed cho et al sutskever et al section introduces estimate sentence weight distribution detail section describes generate representation sentence weight distribution section shows incorporate estimated sentence weights dicted sentence weights training estimating sentence weight distribution assume provided summary y document d dn n number sentences rst step method compute distribution sentence weights training data w wi wi computed wi ei computed ei rou y rouge evaluation metric judge quality predicted summaries train devlopment test representation sentence weight distribution table details lcsts dataset size given number pairs short text summary model rst produce sentence weight distribution sentences tion based sentence embeddings s position embeddings sentences w j lp sj lp sj denotes vector tion sj mlp refers layer perceptron sj produced sj returns indexes words belong jth sentence new output encoder h h h h n models rnn hu et al swd rnn context hu et al swd r l table rouge scores r l rouge l trained els computed test development sets rnn rnn context baselines refer method swd experiments section evaluate proposed approach social media dataset report mance models furthermore use case illustrate improvement achieved proach n number hidden states computed dataset h w returns weight sentence belongs hi output rnn bi lstm encoder new output encoder h delivered decoder produces summary training given model parameter input text corresponding summary y sentence weight distribution s described section loss function n n m w n batch size ditional probability output word yi given source texts xi j predicted sentence weight conditional ability sentence weight wi j descirbed section given source texts xi use large scale chinese short tion dataset lcsts provided hu et al dataset constructed famous chinese social media called sina based statistic data training set set maximum number sentences maximum length sentence paper experimental settings following previous works experimental sults development set set parameters follows character embedding dimension size hidden state parameter word dings initialized randomly use layer encoder layer decoder paper use minibatch stochastic gradient scent sgd algorithm train model gradient computed minibatch pairs document summary best validation curacy reached batches quires days training evaluation place lot popular chinese media ganizations post news information figure comparisons predicted summaries rnn context method dicted summary shown blue rnn context comes unimportant sentence contrast predicted summary method covers key words shown purple use rouge metric proposed lin hovy unlike bleu includes ious n gram matches versions rouge different match lengths rouge l experiments performed commodity bit dell precision workstation ghz core cpu ram titan x gpu models limit method specic neural network extended sequence sequence model paper evaluate method types baselines rnn denote rnn basic sequence sequence model bi lstm encoder bi lstm decoder widely framework rnn context rnn context sequence sequence framework attention nism results discussions compare approach baselines ing rnn rnn context main results shown table seen proach achieves rouge improvement baselines particular swd outperforms context points finally example summary shown figure example illustrated section aimed negative inuence portant words extracting key information rnn context rnn context chooses portant words summary like fundings central government shown blue contrast outputs method contain key words shown pink like fan gang rate china economy growth slows example shows effectiveness model handling noise documents ber irrelevant words conclusion paper propose novel method ing sentence weight distribution improve performance abstractive summarization target models focus important tences ignore irrelevant sentences sults large scale chinese social media dataset approach outperforms competitive baselines example shows summary produced method relevant gold summary method extended sequence sequence model word based systems potentially helpful task cause words incorporate meaningful information future try word segmentation methods sun et al xu sun xu et al improve system references ramiz m aliguliyev new sentence similarity measure sentence based extractive technique automatic text summarization expert systems applications lidong bing piji li yi liao wai lam weiwei guo rebecca passonneau abstractive document summarization phrase selection merging proceedings annual ing association computational linguistics international joint conference ral language processing volume long papers association computational linguistics beijing china pages jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings annual meeting association computational linguistics ume long papers association tional linguistics berlin germany pages aclweb org anthology kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger learning schwenk yoshua bengio phrase representations rnn encoder decoder statistical machine translation proceedings conference empirical methods ural language processing emnlp association computational linguistics doha qatar pages kyunghyun cho dzmitry bahdanau yoshua gio neural machine translation jointly learning align translate iclr gunes erkan dragomir r radev lexrank graph based lexical centrality salience text journal articial intelligence summarization research rafael luciano ferreira souza cabral rafael dueire lins gabriel pereira e silva fred freitas george dc cavalcanti rinaldo lima steven j simske luciano favaro assessing sentence scoring techniques extractive text summarization expert systems applications jiatao gu zhengdong lu hang li victor o k incorporating copying mechanism li proceedings sequence sequence learning annual meeting association computational linguistics acl august berlin germany volume long papers baotian hu qingcai chen fangze zhu sts large scale chinese short text summarization dataset proceedings conference empirical methods natural language ing emnlp lisbon portugal september pages kevin knight daniel marcu tion sentence extraction probabilistic proach sentence compression articial gence chin yew lin eduard h hovy matic evaluation summaries n gram occurrence statistics human language ogy conference north american chapter association computational linguistics naacl edmonton canada june shuming ma xu sun semantic vance based neural network text summarization text simplication corr shuming ma xu sun jingjing xu houfeng wang wenjie li qi su improving semantic relevance sequence sequence learning nese social media text summarization ibrahim f moawad mostafa aref mantic graph reduction approach abstractive text computer engineering summarization tems icces seventh international ence ieee pages ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre bing xiang abstractive text summarization sequence proceedings sequence rnns signll conference computational natural language learning conll berlin germany august pages dragomir r radev timothy allison sasha goldensohn john blitzer arda celebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu al mead platform tidocument multilingual text summarization lrec alexander m rush sumit chopra jason weston neural attention model abstractive tence summarization emnlp association computational linguistics pages xu sun wenjie li houfeng wang qin lu feature frequency adaptive line training fast accurate natural language processing tational linguistics xu sun houfeng wang wenjie li fast line training frequency adaptive learning rates chinese word segmentation new word tion pages xu sun bingzhen wei xuancheng ren shuming ma label embedding network learning bel representation soft training deep networks corr xu sun yaozhong zhang takuya matsuzaki masa tsuruoka junichi tsujii criminative latent variable chinese segmenter hybrid word character information human guage technologies conference north ican chapter association computational linguistics proceedings june boulder colorado usa pages xu sun yaozhong zhang takuya matsuzaki masa tsuruoka junichi tsujii abilistic chinese word segmentation non local inf process information stochastic training manage ilya sutskever oriol vinyals quoc v le sequence sequence learning neural works advances neural information ing systems pages kristian woodsend mirella lapata matic generation story highlights proceedings annual meeting association computational linguistics association tational linguistics pages jingjing xu shuming ma yi zhang bingzhen wei xiaoyan cai xu sun transfer learning low resource chinese word segmentation novel neural network conference natural language processing chinese computing jingjing xu xu sun dependency based gated recursive neural network chinese word mentation pages
