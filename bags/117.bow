improving social media text summarization learning sentence weight distribution jingjing moe key laboratory computational linguistics peking university school electronics engineering computer science peking university edu abstract recently encoder decoder models widely social media text rization models times select noise words irrelevant tences summary error declining performance der inhibit irrelevant sentences cus key information propose effective approach learning sentence model weight distribution build multi layer perceptron predict sentence weights training use rouge score alternative estimated sentence weight try imize gap estimated weights predicted weights way courage model focus key tences high relevance summary experimental results approach outperforms baselines large scale social media corpus introduction text summarization important task ral language processing aims understand key idea document generate headline summary previous social media text marization systems rush mainly based abstractive text marization belong family encoder decoders shown effective tasks like machine translation cho dzmitry bahdanau bengio models select noise words irrelevant sentences mary error figure gives example noise words generated state art decoder model rnn context unlike lation requires encoding information ensure accuracy summarization tries tract important information small sentences convey key information rest sentences usually useless unrelated sentences hard encoder decoder models extract key information order address issue propose novel method learning sentence weight tribution encourage models focus key tences ignore unimportant sentences approach rst design multi layer tron predict sentence weights ering rouge popular evaluation rion summarization estimate gold tence weights training data rouge scores sentences summaries ing design end end optimization method minimizes gap predicted tence weights estimated sentence weights conduct experiments large scale social media dataset experimental results method outperforms competitive baselines sides limit method cic neural network extended sequence sequence model related work summarization approaches divided typical categories extractive summarization radev aliguliyev woodsend lapata ferreira cheng lapata abstractive tion knight marcu bing rush extractive summarizations works ally select sentences document summary headline abstractive figure illustration negative inuence noise words rnn context basic decoder model attention mechanism gold summary shown blue rate china economy growth slows investment real estate certainly better key words unseen output rnn context words shown pink irrelevant sentences selected summary rization works usually encode document abstractive representation ate words summary cial media summarization systems belong stractive text summarizaition generally ing extractive summarization achieves better formance abstractive summarization long normal documents extractive marization suitable social media text noises short neural abstractive text summarization newly proposed method hot research topic recent years unlike ditional summarization systems consist small sub components tuned rately knight marcu erkan radev moawad aref neural abstractive text summarization attempts build train single large neural network reads document outputs correct summary rush rst introduced encoder decoder framework attention mechanism abstractive text summarization bing proposed abstraction based multi document summarization framework construct new sentences exploring grained syntactic units sentences proposed copy mechanism address problem unknown words nallapati proposed novel models address critical problems marization proposed model method based basic encoder decoder framework proposed cho sutskever section introduces estimate sentence weight distribution detail section describes generate representation sentence weight distribution section shows incorporate estimated sentence weights dicted sentence weights training estimating sentence weight distribution assume provided summary document number sentences rst step method compute distribution sentence weights training data computed computed rou rouge evaluation metric judge quality predicted summaries train devlopment test representation sentence weight distribution table details lcsts dataset size given number pairs short text summary model rst produce sentence weight distribution sentences tion based sentence embeddings position embeddings sentences denotes vector tion mlp refers layer perceptron produced returns indexes words belong jth sentence new output encoder models rnn swd rnn context swd table rouge scores rouge trained els computed test development sets rnn rnn context baselines refer method swd experiments section evaluate proposed approach social media dataset report mance models furthermore use case illustrate improvement achieved proach number hidden states computed dataset returns weight sentence belongs output rnn lstm encoder new output encoder delivered decoder produces summary training given model parameter input text corresponding summary sentence weight distribution described section loss function batch size ditional probability output word given source texts predicted sentence weight conditional ability sentence weight descirbed section given source texts use large scale chinese short tion dataset lcsts provided dataset constructed famous chinese social media called sina based statistic data training set set maximum number sentences maximum length sentence paper experimental settings following previous works experimental sults development set set parameters follows character embedding dimension size hidden state parameter word dings initialized randomly use layer encoder layer decoder paper use minibatch stochastic gradient scent sgd algorithm train model gradient computed minibatch pairs document summary best validation curacy reached batches quires days training evaluation place lot popular chinese media ganizations post news information figure comparisons predicted summaries rnn context method dicted summary shown blue rnn context comes unimportant sentence contrast predicted summary method covers key words shown purple use rouge metric proposed lin hovy unlike bleu includes ious gram matches versions rouge different match lengths rouge experiments performed commodity bit dell precision workstation ghz core cpu ram titan gpu models limit method specic neural network extended sequence sequence model paper evaluate method types baselines rnn denote rnn basic sequence sequence model lstm encoder lstm decoder widely framework rnn context rnn context sequence sequence framework attention nism results discussions compare approach baselines ing rnn rnn context main results shown table seen proach achieves rouge improvement baselines particular swd outperforms context points finally example summary shown figure example illustrated section aimed negative inuence portant words extracting key information rnn context rnn context chooses portant words summary like fundings central government shown blue contrast outputs method contain key words shown pink like fan gang rate china economy growth slows example shows effectiveness model handling noise documents ber irrelevant words conclusion paper propose novel method ing sentence weight distribution improve performance abstractive summarization target models focus important tences ignore irrelevant sentences sults large scale chinese social media dataset approach outperforms competitive baselines example shows summary produced method relevant gold summary method extended sequence sequence model word based systems potentially helpful task cause words incorporate meaningful information future try word segmentation methods sun sun improve system references ramiz aliguliyev new sentence similarity measure sentence based extractive technique automatic text summarization expert systems applications lidong bing piji liao wai lam weiwei guo rebecca passonneau abstractive document summarization phrase selection merging proceedings annual ing association computational linguistics international joint conference ral language processing volume long papers association computational linguistics beijing china pages jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings annual meeting association computational linguistics ume long papers association tional linguistics berlin germany pages aclweb org anthology kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger learning schwenk yoshua bengio phrase representations rnn encoder decoder statistical machine translation proceedings conference empirical methods ural language processing emnlp association computational linguistics doha qatar pages kyunghyun cho dzmitry bahdanau yoshua gio neural machine translation jointly learning align translate iclr gunes erkan dragomir radev lexrank graph based lexical centrality salience text journal articial intelligence summarization research rafael luciano ferreira souza cabral rafael dueire lins gabriel pereira silva fred freitas george cavalcanti rinaldo lima steven simske luciano favaro assessing sentence scoring techniques extractive text summarization expert systems applications jiatao zhengdong hang victor incorporating copying mechanism proceedings sequence sequence learning annual meeting association computational linguistics acl august berlin germany volume long papers baotian qingcai chen fangze zhu sts large scale chinese short text summarization dataset proceedings conference empirical methods natural language ing emnlp lisbon portugal september pages kevin knight daniel marcu tion sentence extraction probabilistic proach sentence compression articial gence chin yew lin eduard hovy matic evaluation summaries gram occurrence statistics human language ogy conference north american chapter association computational linguistics naacl edmonton canada june shuming sun semantic vance based neural network text summarization text simplication corr shuming sun jingjing houfeng wang wenjie improving semantic relevance sequence sequence learning nese social media text summarization ibrahim moawad mostafa aref mantic graph reduction approach abstractive text computer engineering summarization tems icces seventh international ence ieee pages ramesh nallapati bowen zhou ccero nogueira dos santos aglar gulcehre bing xiang abstractive text summarization sequence proceedings sequence rnns signll conference computational natural language learning conll berlin germany august pages dragomir radev timothy allison sasha goldensohn john blitzer arda celebi stanko dimitrov elliott drabek ali hakim wai lam danyu liu mead platform tidocument multilingual text summarization lrec alexander rush sumit chopra jason weston neural attention model abstractive tence summarization emnlp association computational linguistics pages sun wenjie houfeng wang qin feature frequency adaptive line training fast accurate natural language processing tational linguistics sun houfeng wang wenjie fast line training frequency adaptive learning rates chinese word segmentation new word tion pages sun bingzhen wei xuancheng ren shuming label embedding network learning bel representation soft training deep networks corr sun yaozhong zhang takuya matsuzaki masa tsuruoka junichi tsujii criminative latent variable chinese segmenter hybrid word character information human guage technologies conference north ican chapter association computational linguistics proceedings june boulder colorado usa pages sun yaozhong zhang takuya matsuzaki masa tsuruoka junichi tsujii abilistic chinese word segmentation non local inf process information stochastic training manage ilya sutskever oriol vinyals quoc sequence sequence learning neural works advances neural information ing systems pages kristian woodsend mirella lapata matic generation story highlights proceedings annual meeting association computational linguistics association tational linguistics pages jingjing shuming zhang bingzhen wei xiaoyan cai sun transfer learning low resource chinese word segmentation novel neural network conference natural language processing chinese computing jingjing sun dependency based gated recursive neural network chinese word mentation pages
