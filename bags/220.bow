unsupervised text summarization mixed model translation yacine jernite facebook ai research new york ny com g u l c s c v v x r abstract translation based approaches cently lead signicant progress pervised sequence sequence tasks machine translation style transfer work extend paradigm problem learning sentence summarization system unaligned data present initial models rely asymmetrical nature task perform rst translation step demonstrate value combining data created diverse initialization methods system outperforms rent state art unsupervised sentence summarization fully unaligned data rouge matches performance recent semi supervised approaches introduction machine summarization systems nicant progress recent years especially domain news text possible things popularization neural sequence sequence paradigm kalchbrenner blunsom sutskever et al cho et al development methods combine strengths extractive abstractive approaches summarization et al gehrmann et al ability large training datasets task gigaword cnn daily mail comprise m shorter k longer articles aligned summaries respectively fortunately lack datasets similar scale text genres remains limiting factor tempting advantage modeling advances supervised training algorithms work investigate application translation training summarization tem unsupervised fashion unaligned text summaries corpora translation successfully applied unsupervised training sequence sequence tasks machine translation lample et al style transfer subramanian et al outline main differences tings text summarization devise initialization strategies advantage rical nature task demonstrate vantage combining varied initializers proach outperforms previous state art unsupervised text summarization training data matches rouge scores recent semi supervised methods related work rush et al s work applying neural systems task text tion followed number works proving initial model architecture included changing base encoder ture chopra et al adding pointer nism directly use input words summary nallapati et al et al itly pre selecting parts text focus gehrmann et al comparatively attempts train els supervision auto encoding based proaches met success miao som wang lee miao blunsom s work endeavors use summaries discrete latent variable text auto encoder train system combination classical log likelihood loss supervised setting reconstruction jective requires text recoverable produced summary method able advantage belled data relies good initialization encoder system needs learned signicant number aligned pairs wang lee expand approach replacing need supervised data adversarial objectives encourage summaries structured like natural guage allowing train system fully unsupervised setting unaligned corpora text summary sequences finally song et al uses general purpose pre trained text encoder learn summarization system fewer examples proposed mass scheme shown efcient bert devlin et al denoising auto encoders dae vincent et al fu et al work proposes different approach supervised training based translation idea initial weak system create iteratively rene articial training data vised algorithm successfully applied semi supervised sennrich et al pervised machine translation lample et al style transfer subramanian et al investigate general paradigm applied task summarizing text mixed model translation let consider task transforming quence domain corresponding quence domain b e sentences guages machine translation let da db corpora sequences b mapping respective elements translation approach starts initial models ab f ba hand crafted learned aligned pairs uses create articial aligned training data ab ba dbo dao let s denote supervised learning algorithm takes set aligned sequence pairs turns mapping function articial data train iteration models turn create new cial training sets b switched ab ab ba dao model trained iteration articial inputs real outputs create new training inputs initial system nt far hope training pairs closer true data distribution step allowing turn train better models case summarization consider domains text sequences df maries ds attempt learn summarization ff s expansion fsf functions contrary translation case df ds interchangeable considering mary typically information sponding text choose dene initial f s models follow proposed procedure alternating directions step initialization models summarization initiate process case machine translation lample et al use different initialization models neural nmt phrase based pbsmt systems lies denoising auto encoders languages shared latent space uses pbsmt system koehn et al phrase table obtained unsupervised cabulary alignment grave et al methods work chine translation rely input output having similar lengths information content particular statistical machine translation rithm tries align input tokens word case text summarization inherent asymmetry text summaries press subset pose initialization systems implicitly model information loss implementation details provided appendix procrustes thresholded alignment pr thr rst initialization similar smt relies unsupervised vocabulary alignment specically train skipgram word embedding models fasttext janowski et al df ds align common space wasserstein procrustes method grave et al map word text sequence nearest neighbor aligned space tance smaller threshold skip erwise limit output length keeping rst n tokens refer function f pr f original france took important step power market liberalization monday braving union anger announce partial privatization state owned behemoth electricite france pr thr france launched partial unk state controlled utility privatization agency said dbae france s state owned gaz de france sa said tuesday considering partial partial privatization france s state owned nuclear power plants france launches initial public announcement wednesday european union announced soon undertake partial privatization title france launches partial edf privatization table text sequences generated f pr rst translation loop denoising bag word auto encoder dbae similarly lample et al wang lee devise starting model based dae major difference use simple bag words bow encoder xed pre trained word embeddings layer gru decoder nd bow encoder trained summaries reaches construction rouge l f score nearly test set indicating word presence tion sufcient model summaries noise model token remove probability add word drawn uniformly summary lary probability bow encoder advantages lacks models bias word order text summary secondly dbae predict summaries text weight input word dings corpus level probability ing summary forcing model pay attention words appear df denoising bag words auto encoder weighting referred f f order word moments matching propose extractive initialization model given bow representation dbae function s v predicts probability word v text sequence s present summary learn parameters f marginalizing output probability word text sequences matching rst order moments marginal probability word s presence summary let v s denote vocabulary ds v v s v psdf f s v psds minimize binary cross entropy bce tween output summary moments arg min x vv s psdf s v s v dene initial extractive summarization model applying f words sentence keeping ones output probability greater threshold fer model f f articial training data apply translation procedure outlined parallel initialization els example f yields following quence models articial aligned datasets f s f sf f s sf f s finally order advantage strengths initialization models concatenate articial training dataset odd iteration train summarizer e f f s f s f s experiments data model choices validate proach gigaword corpus comprises training set m article headlines sidered text titles summaries k validation pairs report test performance k set rush et al want learn systems fully unaligned data giving model opportunity learn implicit mapping pbsmt pre pr r l table test rouge trivial baseline tion systems lee split training set m examples use titles m headlines models initialization step mented convolutional architectures ing fairseq ott et al articial data eration uses sampling minimum length text maximum length summaries rouge scores obtained output vocabulary size k beam search size match wang lee initializers table compares test rouge different initialization models ial baseline simply copies rst words article nd simply olding distance word alignment step pr thr slightly better pbsmt system lample et al bow denoising auto encoder word weighting performs signicantly better dae initialization wang lee pre dae moments based initial model scores higher scores close pervised system wang lee order investigate effect different strategies rouge statistics generations corresponding rst iteration expanders given summary table unsupervised vocabulary alignment pr thr handles vocabulary shift especially changes verb tenses summaries tend present tense maintains word der adds little information conversely expansion function learned purely extractive summaries uses words summary change adds new information finally encoder based dbae signicantly increases sequence length variety strays pr advers sup r l k k k k m table comparison systems best scores unsupervised training bolded results lee et al blunsom et al original meaning examples pendix decoders learn facts world training article text edf gdf france s public power company models finally table compares marizers learned translation ations unsupervised semi supervised approaches overall system outperforms unsupervised adversarial reinforce wang lee translation loop semi supervised systems ond including song et al s mass pre trained sentence encoder miao som forced attention sentence sion fsc use k k aligned pairs respectively far translation proaches concerned note model performances correlated initializers scores reported table iterations low pattern addition nd combining data initializers training summarizer system iteration described section performs best suggesting greater variety articial text help model learn conclusion work use translation paradigm unsupervised training summarization system nd model benets combining initializers matching performance semi supervised approaches references piotr bojanowski edouard grave armand joulin tomas mikolov enriching word vectors subword information tacl kyunghyun cho bart van merrienboer dzmitry danau yoshua bengio properties neural machine translation encoder decoder proaches proceedings eighth workshop syntax semantics ture statistical translation doha qatar tober pages sumit chopra michael auli alexander m rush abstractive sentence summarization tentive recurrent neural networks naacl hlt conference north american chapter association computational guistics human language technologies san diego california usa june pages jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language standing corr zhenxin fu xiaoye tan nanyun peng dongyan zhao rui yan style transfer text proceedings exploration evaluation thirty second aaai conference articial gence innovative applications articial intelligence aaai symposium educational advances articial intelligence new orleans louisiana usa february pages sebastian gehrmann yuntian deng alexander m rush abstractive summarization proceedings conference empirical methods natural language processing brussels belgium october november pages edouard grave armand joulin quentin berthet unsupervised alignment embeddings wasserstein procrustes corr american chapter association tational linguistics hlt naacl edmonton canada june guillaume lample myle ott alexis conneau dovic denoyer marcaurelio ranzato phrase based neural unsupervised machine proceedings conference lation empirical methods natural language ing brussels belgium october november pages yishu miao phil blunsom language latent variable discrete generative models sentence compression proceedings conference empirical methods natural guage processing emnlp austin texas usa november pages ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre bing xiang abstractive text summarization sequence proceedings sequence rnns signll conference computational natural language learning conll berlin germany august pages myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier fairseq fast michael auli tensible toolkit sequence modeling corr alexander m rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing emnlp lisbon portugal september pages abigail peter j liu christopher d manning point summarization generator networks proceedings nual meeting association computational linguistics acl vancouver canada july august volume long papers pages sergey ioffe christian szegedy batch malization accelerating deep network training proceedings reducing internal covariate shift international conference machine learning icml lille france july pages rico sennrich barry haddow alexandra birch improving neural machine translation proceedings els monolingual data annual meeting association tational linguistics acl august berlin germany volume long papers nal kalchbrenner phil blunsom recurrent proceedings continuous translation models conference empirical methods ral language processing emnlp tober grand hyatt seattle seattle ton usa meeting sigdat special interest group acl pages philipp koehn franz josef och daniel marcu statistical phrase based translation man language technology conference north kaitao song xu tan tao qin jianfeng lu yan liu mass masked sequence quence pre training language generation ceedings international conference machine learning long beach california sandeep subramanian lample denoyer eric michael marcaurelio ranzato y lan boureau multiple attribute text style transfer corr guillaume ludovic smith ilya sutskever oriol vinyals quoc v le sequence sequence learning neural works advances neural information ing systems annual conference neural formation processing systems december montreal quebec canada pages pascal vincent hugo larochelle yoshua bengio pierre antoine manzagol extracting composing robust features denoising machine learning proceedings toencoders fifth international conference icml helsinki finland june pages yau shian wang hung yi lee learning encode text human readable summaries generative adversarial networks proceedings conference empirical methods ral language processing brussels belgium ber november pages implementation choices initialization models describe modeling choices tion models pr thr dbae hyper parameters systems set based models rouge l score idation set stated models use skipgram word embeddings shared input output layers dimension embeddings trained catenation text summary sequences df ds v vocabulary v f v s vocabularies df ds tively trained models use adam mizer learning rate convolutional models use fconv tecture previded pre trained input output word embeddings vocabulary size k text k maries expander generations collapse contiguous unk tokens cut sentence rst stop model generate eos token yielding outputs shorter words procrustes thresholded alignment pr thr model train sets word beddings df ds separately compute aligned vectors fasttext implementation grave et al map word input sequence closest word v s aligned space est neighbor eos token distance nearest neighbor aligned space greater threshold output sequence sists rst n mapped words order input sequence found dings dimension threshold maximum output length n yields best validation rouge l compare pr thr pbsmt baseline table use unsupervisedmt lample et al pre trained embedding perform hyper parameter search maximum length sets n readthedocs io test models html com fasttext tree master alignment com unsupervisedmt tree master pbsmt denoising bag word auto encoder dbae dbae trained sentences ds encoder dbae averages input word beddings applies linear transformation lowed batch normalization layer ioffe szegedy decoder layer gru recurrent neural network hidden dimension encoder output concatenated initial hidden state layers projected hidden dimension use model summarization form changes auto encoding setting perform weighted instead standard average words likely appear ds df weighted words v f v s dropped ically given word v v s weight wv summarization weighted bow encoder given v psdf f s v psds wv max s v f v secondly implement like pointer mechanism adding score input words output gru softmax test time creating articial data decode beam search beam size size maximum output length n input word bias order word moments matching moments matching model uses coder dbae followed linear ping summary vocabulary followed sigmoid layer log score words appear input set nately computing output probabilities sentences corpus computing nary cross entropy impractical plement batched version algorithm let corpus level moments f v dened equation let bf batch text quences dene v s v psbf f s v s v f v f v batch algorithm takes gradient step loss x vv s psbf s v s v prediction similar pr thr tem threshold f s nearest neighbor distance old maximum output length n b examples model predictions present examples expander summarizer models outputs tables table shows expander generations initial models translation epoch follow patterns outlined tion dbae showing variety ing faithful input table tions expander models different translation iteration interesting models slowly overcome dbae expander s initial limitations version faithful input rst moments based approach starts ing rephrases modeling vocabulary shift procrustes method benet successive iterations starts produce longer outputs finally table provides maries produced nal model model produce likely summaries note aside occasional synonym use bal tense change use explicit pointer mechanism standard attention model s outputs extractive n nnn ancient graves found greek metro dig pr thr n nnn ancient graves found greek metro unk dbae remains n nnn graves ancient greek island found ancient graves past days senior police ofcer said friday n nnn ancient graves found greek city alexandria northern greek city salonika connection greek metro dig deep underground ukraine crimea dreams union russia pr thr ukraine crimea unk union russia dbae ukraine signed agreements ukraine forming european union ukraine membership ukraine s crimea peninsula dreams unk soviet republic s unk country russia itar tass news agency reported malaysian opposition seeks international help release detainees pr thr malaysian opposition thursday sought international help release detainees malaysian opposition news reports said dbae malaysian prime minister abdullah ahmad badawi said tuesday government s decision release nnn detainees report said wednesday malaysian opposition parties said tuesday seeks help release detainees russia unify energy transport networks georgia rebels pr thr russia unify energy transport networks georgia rebels dbae russian government leaders met representatives international energy giant said monday networks trying unify areas energy supplies russia unify energy telecommunication networks cope georgia s separatist rebels government eu losing hope swift solution treaty crisis pr thr eu losing hope unk solution maastricht treaty crisis dbae european union losing hope swift solution crisis eu eu hoping s urgent referendum eu governments come hope swift solution european union treaty ended current nancial crisis table examples articial data rst translation iteration original malaysia drafted rst legislation aimed punishing computer hackers ofcial said wednesday pr malaysia enacted draft rst law unk computer hacking pr malaysia issued draft law computer hacking pr malaysia drafted rst law computer hacking internet hacking malaysia s parliament friday signed bill allow computer users monitor unk law country submitted parliament nnnn passed bill wednesday rst reading computer system ofcials said monday malaysia s national defense ministry drafted regulation computer hacking country prime minister said friday malaysia drafts rst law computer hacking malaysia started drafts rst law computer hacking malaysia today presented nation s rst law computer hacking country news reports said wednesday title malaysia drafts rst law computer hacking table evolution generated text sequences iterations article chinese permanent representative united nations wang guangya wednesday urged un international community continue supporting timor leste pred chinese permanent representative urges un continue supporting timor leste title china stresses continued international support timor leste article macedonian president branko crvenkovski spend orthodox christmas weekend country s troops serving iraq cabinet said thursday pred macedonian president spend orthodox christmas troops iraq title macedonian president visit troops iraq article televangelist pat robertson nt thinks god s purpose natural disasters pred evangelist pat robertson thinks god s purpose disasters title editorial blaming god disasters article sudanese opposition said thursday killed nnn government soldiers ambush east country pred sudanese opposition kills n government soldiers ambush title sudanese opposition says nnn government troops killed ambush table example model predicitons f
