unsupervised text summarization mixed model translation yacine jernite facebook research new york com abstract translation based approaches cently lead signicant progress pervised sequence sequence tasks machine translation style transfer work extend paradigm problem learning sentence summarization system unaligned data present initial models rely asymmetrical nature task perform rst translation step demonstrate value combining data created diverse initialization methods system outperforms rent state art unsupervised sentence summarization fully unaligned data rouge matches performance recent semi supervised approaches introduction machine summarization systems nicant progress recent years especially domain news text possible things popularization neural sequence sequence paradigm kalchbrenner blunsom sutskever cho development methods combine strengths extractive abstractive approaches summarization gehrmann ability large training datasets task gigaword cnn daily mail comprise shorter longer articles aligned summaries respectively fortunately lack datasets similar scale text genres remains limiting factor tempting advantage modeling advances supervised training algorithms work investigate application translation training summarization tem unsupervised fashion unaligned text summaries corpora translation successfully applied unsupervised training sequence sequence tasks machine translation lample style transfer subramanian outline main differences tings text summarization devise initialization strategies advantage rical nature task demonstrate vantage combining varied initializers proach outperforms previous state art unsupervised text summarization training data matches rouge scores recent semi supervised methods related work rush work applying neural systems task text tion followed number works proving initial model architecture included changing base encoder ture chopra adding pointer nism directly use input words summary nallapati itly pre selecting parts text focus gehrmann comparatively attempts train els supervision auto encoding based proaches met success miao som wang lee miao blunsom work endeavors use summaries discrete latent variable text auto encoder train system combination classical log likelihood loss supervised setting reconstruction jective requires text recoverable produced summary method able advantage belled data relies good initialization encoder system needs learned signicant number aligned pairs wang lee expand approach replacing need supervised data adversarial objectives encourage summaries structured like natural guage allowing train system fully unsupervised setting unaligned corpora text summary sequences finally song uses general purpose pre trained text encoder learn summarization system fewer examples proposed mass scheme shown efcient bert devlin denoising auto encoders dae vincent work proposes different approach supervised training based translation idea initial weak system create iteratively rene articial training data vised algorithm successfully applied semi supervised sennrich pervised machine translation lample style transfer subramanian investigate general paradigm applied task summarizing text mixed model translation let consider task transforming quence domain corresponding quence domain sentences guages machine translation let corpora sequences mapping respective elements translation approach starts initial models hand crafted learned aligned pairs uses create articial aligned training data dbo dao let denote supervised learning algorithm takes set aligned sequence pairs turns mapping function articial data train iteration models turn create new cial training sets switched dao model trained iteration articial inputs real outputs create new training inputs initial system far hope training pairs closer true data distribution step allowing turn train better models case summarization consider domains text sequences maries attempt learn summarization expansion fsf functions contrary translation case interchangeable considering mary typically information sponding text choose dene initial models follow proposed procedure alternating directions step initialization models summarization initiate process case machine translation lample use different initialization models neural nmt phrase based pbsmt systems lies denoising auto encoders languages shared latent space uses pbsmt system koehn phrase table obtained unsupervised cabulary alignment grave methods work chine translation rely input output having similar lengths information content particular statistical machine translation rithm tries align input tokens word case text summarization inherent asymmetry text summaries press subset pose initialization systems implicitly model information loss implementation details provided appendix procrustes thresholded alignment thr rst initialization similar smt relies unsupervised vocabulary alignment specically train skipgram word embedding models fasttext janowski align common space wasserstein procrustes method grave map word text sequence nearest neighbor aligned space tance smaller threshold skip erwise limit output length keeping rst tokens refer function original france took important step power market liberalization monday braving union anger announce partial privatization state owned behemoth electricite france thr france launched partial unk state controlled utility privatization agency said dbae france state owned gaz france said tuesday considering partial partial privatization france state owned nuclear power plants france launches initial public announcement wednesday european union announced soon undertake partial privatization title france launches partial edf privatization table text sequences generated rst translation loop denoising bag word auto encoder dbae similarly lample wang lee devise starting model based dae major difference use simple bag words bow encoder xed pre trained word embeddings layer gru decoder bow encoder trained summaries reaches construction rouge score nearly test set indicating word presence tion sufcient model summaries noise model token remove probability add word drawn uniformly summary lary probability bow encoder advantages lacks models bias word order text summary secondly dbae predict summaries text weight input word dings corpus level probability ing summary forcing model pay attention words appear denoising bag words auto encoder weighting referred order word moments matching propose extractive initialization model given bow representation dbae function predicts probability word text sequence present summary learn parameters marginalizing output probability word text sequences matching rst order moments marginal probability word presence summary let denote vocabulary psdf psds minimize binary cross entropy bce tween output summary moments arg min psdf dene initial extractive summarization model applying words sentence keeping ones output probability greater threshold fer model articial training data apply translation procedure outlined parallel initialization els example yields following quence models articial aligned datasets finally order advantage strengths initialization models concatenate articial training dataset odd iteration train summarizer experiments data model choices validate proach gigaword corpus comprises training set article headlines sidered text titles summaries validation pairs report test performance set rush want learn systems fully unaligned data giving model opportunity learn implicit mapping pbsmt pre table test rouge trivial baseline tion systems lee split training set examples use titles headlines models initialization step mented convolutional architectures ing fairseq ott articial data eration uses sampling minimum length text maximum length summaries rouge scores obtained output vocabulary size beam search size match wang lee initializers table compares test rouge different initialization models ial baseline simply copies rst words article simply olding distance word alignment step thr slightly better pbsmt system lample bow denoising auto encoder word weighting performs signicantly better dae initialization wang lee pre dae moments based initial model scores higher scores close pervised system wang lee order investigate effect different strategies rouge statistics generations corresponding rst iteration expanders given summary table unsupervised vocabulary alignment thr handles vocabulary shift especially changes verb tenses summaries tend present tense maintains word der adds little information conversely expansion function learned purely extractive summaries uses words summary change adds new information finally encoder based dbae signicantly increases sequence length variety strays advers sup table comparison systems best scores unsupervised training bolded results lee blunsom original meaning examples pendix decoders learn facts world training article text edf gdf france public power company models finally table compares marizers learned translation ations unsupervised semi supervised approaches overall system outperforms unsupervised adversarial reinforce wang lee translation loop semi supervised systems ond including song mass pre trained sentence encoder miao som forced attention sentence sion fsc use aligned pairs respectively far translation proaches concerned note model performances correlated initializers scores reported table iterations low pattern addition combining data initializers training summarizer system iteration described section performs best suggesting greater variety articial text help model learn conclusion work use translation paradigm unsupervised training summarization system model benets combining initializers matching performance semi supervised approaches references piotr bojanowski edouard grave armand joulin tomas mikolov enriching word vectors subword information tacl kyunghyun cho bart van merrienboer dzmitry danau yoshua bengio properties neural machine translation encoder decoder proaches proceedings eighth workshop syntax semantics ture statistical translation doha qatar tober pages sumit chopra michael auli alexander rush abstractive sentence summarization tentive recurrent neural networks naacl hlt conference north american chapter association computational guistics human language technologies san diego california usa june pages jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language standing corr zhenxin xiaoye tan nanyun peng dongyan zhao rui yan style transfer text proceedings exploration evaluation thirty second aaai conference articial gence innovative applications articial intelligence aaai symposium educational advances articial intelligence new orleans louisiana usa february pages sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference empirical methods natural language processing brussels belgium october november pages edouard grave armand joulin quentin berthet unsupervised alignment embeddings wasserstein procrustes corr american chapter association tational linguistics hlt naacl edmonton canada june guillaume lample myle ott alexis conneau dovic denoyer marcaurelio ranzato phrase based neural unsupervised machine proceedings conference lation empirical methods natural language ing brussels belgium october november pages yishu miao phil blunsom language latent variable discrete generative models sentence compression proceedings conference empirical methods natural guage processing emnlp austin texas usa november pages ramesh nallapati bowen zhou ccero nogueira dos santos aglar gulcehre bing xiang abstractive text summarization sequence proceedings sequence rnns signll conference computational natural language learning conll berlin germany august pages myle ott sergey edunov alexei baevski angela fan sam gross nathan david grangier fairseq fast michael auli tensible toolkit sequence modeling corr alexander rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing emnlp lisbon portugal september pages abigail peter liu christopher manning point summarization generator networks proceedings nual meeting association computational linguistics acl vancouver canada july august volume long papers pages sergey ioffe christian szegedy batch malization accelerating deep network training proceedings reducing internal covariate shift international conference machine learning icml lille france july pages rico sennrich barry haddow alexandra birch improving neural machine translation proceedings els monolingual data annual meeting association tational linguistics acl august berlin germany volume long papers nal kalchbrenner phil blunsom recurrent proceedings continuous translation models conference empirical methods ral language processing emnlp tober grand hyatt seattle seattle ton usa meeting sigdat special interest group acl pages philipp koehn franz josef och daniel marcu statistical phrase based translation man language technology conference north kaitao song tan tao qin jianfeng yan liu mass masked sequence quence pre training language generation ceedings international conference machine learning long beach california sandeep subramanian lample denoyer eric michael marcaurelio ranzato lan boureau multiple attribute text style transfer corr guillaume ludovic smith ilya sutskever oriol vinyals quoc sequence sequence learning neural works advances neural information ing systems annual conference neural formation processing systems december montreal quebec canada pages pascal vincent hugo larochelle yoshua bengio pierre antoine manzagol extracting composing robust features denoising machine learning proceedings toencoders fifth international conference icml helsinki finland june pages yau shian wang hung lee learning encode text human readable summaries generative adversarial networks proceedings conference empirical methods ral language processing brussels belgium ber november pages implementation choices initialization models describe modeling choices tion models thr dbae hyper parameters systems set based models rouge score idation set stated models use skipgram word embeddings shared input output layers dimension embeddings trained catenation text summary sequences vocabulary vocabularies tively trained models use adam mizer learning rate convolutional models use fconv tecture previded pre trained input output word embeddings vocabulary size text maries expander generations collapse contiguous unk tokens cut sentence rst stop model generate eos token yielding outputs shorter words procrustes thresholded alignment thr model train sets word beddings separately compute aligned vectors fasttext implementation grave map word input sequence closest word aligned space est neighbor eos token distance nearest neighbor aligned space greater threshold output sequence sists rst mapped words order input sequence found dings dimension threshold maximum output length yields best validation rouge compare thr pbsmt baseline table use unsupervisedmt lample pre trained embedding perform hyper parameter search maximum length sets readthedocs test models html com fasttext tree master alignment com unsupervisedmt tree master pbsmt denoising bag word auto encoder dbae dbae trained sentences encoder dbae averages input word beddings applies linear transformation lowed batch normalization layer ioffe szegedy decoder layer gru recurrent neural network hidden dimension encoder output concatenated initial hidden state layers projected hidden dimension use model summarization form changes auto encoding setting perform weighted instead standard average words likely appear weighted words dropped ically given word weight summarization weighted bow encoder given psdf psds max secondly implement like pointer mechanism adding score input words output gru softmax test time creating articial data decode beam search beam size size maximum output length input word bias order word moments matching moments matching model uses coder dbae followed linear ping summary vocabulary followed sigmoid layer log score words appear input set nately computing output probabilities sentences corpus computing nary cross entropy impractical plement batched version algorithm let corpus level moments dened equation let batch text quences dene psbf batch algorithm takes gradient step loss psbf prediction similar thr tem threshold nearest neighbor distance old maximum output length examples model predictions present examples expander summarizer models outputs tables table shows expander generations initial models translation epoch follow patterns outlined tion dbae showing variety ing faithful input table tions expander models different translation iteration interesting models slowly overcome dbae expander initial limitations version faithful input rst moments based approach starts ing rephrases modeling vocabulary shift procrustes method benet successive iterations starts produce longer outputs finally table provides maries produced nal model model produce likely summaries note aside occasional synonym use bal tense change use explicit pointer mechanism standard attention model outputs extractive nnn ancient graves found greek metro dig thr nnn ancient graves found greek metro unk dbae remains nnn graves ancient greek island found ancient graves past days senior police ofcer said friday nnn ancient graves found greek city alexandria northern greek city salonika connection greek metro dig deep underground ukraine crimea dreams union russia thr ukraine crimea unk union russia dbae ukraine signed agreements ukraine forming european union ukraine membership ukraine crimea peninsula dreams unk soviet republic unk country russia itar tass news agency reported malaysian opposition seeks international help release detainees thr malaysian opposition thursday sought international help release detainees malaysian opposition news reports said dbae malaysian prime minister abdullah ahmad badawi said tuesday government decision release nnn detainees report said wednesday malaysian opposition parties said tuesday seeks help release detainees russia unify energy transport networks georgia rebels thr russia unify energy transport networks georgia rebels dbae russian government leaders met representatives international energy giant said monday networks trying unify areas energy supplies russia unify energy telecommunication networks cope georgia separatist rebels government losing hope swift solution treaty crisis thr losing hope unk solution maastricht treaty crisis dbae european union losing hope swift solution crisis hoping urgent referendum governments come hope swift solution european union treaty ended current nancial crisis table examples articial data rst translation iteration original malaysia drafted rst legislation aimed punishing computer hackers ofcial said wednesday malaysia enacted draft rst law unk computer hacking malaysia issued draft law computer hacking malaysia drafted rst law computer hacking internet hacking malaysia parliament friday signed bill allow computer users monitor unk law country submitted parliament nnnn passed bill wednesday rst reading computer system ofcials said monday malaysia national defense ministry drafted regulation computer hacking country prime minister said friday malaysia drafts rst law computer hacking malaysia started drafts rst law computer hacking malaysia today presented nation rst law computer hacking country news reports said wednesday title malaysia drafts rst law computer hacking table evolution generated text sequences iterations article chinese permanent representative united nations wang guangya wednesday urged international community continue supporting timor leste pred chinese permanent representative urges continue supporting timor leste title china stresses continued international support timor leste article macedonian president branko crvenkovski spend orthodox christmas weekend country troops serving iraq cabinet said thursday pred macedonian president spend orthodox christmas troops iraq title macedonian president visit troops iraq article televangelist pat robertson thinks god purpose natural disasters pred evangelist pat robertson thinks god purpose disasters title editorial blaming god disasters article sudanese opposition said thursday killed nnn government soldiers ambush east country pred sudanese opposition kills government soldiers ambush title sudanese opposition says nnn government troops killed ambush table example model predicitons
