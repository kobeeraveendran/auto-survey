unsupervised text summarization via mixed model back translation yacine jernite facebook ai research new york ny com g u a l c s c v v i x r a abstract back translation based approaches have cently lead to signicant progress in pervised sequence to sequence tasks such as machine translation or style transfer in this work we extend the paradigm to the problem of learning a sentence summarization system from unaligned data we present several initial models which rely on the asymmetrical nature of the task to perform the rst back translation step and demonstrate the value of combining the data created by these diverse initialization methods our system outperforms the rent state of the art for unsupervised sentence summarization from fully unaligned data by over rouge and matches the performance of recent semi supervised approaches introduction machine summarization systems have made nicant progress in recent years especially in the domain of news text this has been made possible among other things by the popularization of the neural sequence to sequence paradigm kalchbrenner and blunsom sutskever et al cho et al the development of methods which combine the strengths of extractive and abstractive approaches to summarization see et al gehrmann et al and the ability of large training datasets for the task such as gigaword or the cnn daily mail which comprise of over m shorter and k longer articles and aligned summaries respectively fortunately the lack of datasets of similar scale for other text genres remains a limiting factor when tempting to take full advantage of these modeling advances using supervised training algorithms in this work we investigate the application of back translation to training a summarization tem in an unsupervised fashion from unaligned full text and summaries corpora back translation has been successfully applied to unsupervised training for other sequence to sequence tasks such as machine translation lample et al or style transfer subramanian et al we outline the main differences between these tings and text summarization devise initialization strategies which take advantage of the rical nature of the task and demonstrate the vantage of combining varied initializers our proach outperforms the previous state of the art on unsupervised text summarization while using less training data and even matches the rouge scores of recent semi supervised methods related work rush et al s work on applying neural systems to the task of text tion has been followed by a number of works proving upon the initial model architecture these have included changing the base encoder ture chopra et al adding a pointer nism to directly re use input words in the summary nallapati et al see et al or itly pre selecting parts of the full text to focus on gehrmann et al while there have been comparatively few attempts to train these els with less supervision auto encoding based proaches have met some success miao and som wang and lee miao and blunsom s work endeavors to use summaries as a discrete latent variable for a text auto encoder they train a system on a combination of the classical log likelihood loss of the supervised setting and a reconstruction jective which requires the full text to be mostly recoverable from the produced summary while their method is able to take advantage of belled data it relies on a good initialization of the encoder part of the system which still needs to be learned on a signicant number of aligned pairs wang and lee expand upon this approach by replacing the need for supervised data with adversarial objectives which encourage the summaries to be structured like natural guage allowing them to train a system in a fully unsupervised setting from unaligned corpora of full text and summary sequences finally song et al uses a general purpose pre trained text encoder to learn a summarization system from fewer examples their proposed mass scheme is shown to be more efcient than bert devlin et al or denoising auto encoders dae vincent et al fu et al this work proposes a different approach to supervised training based on back translation the idea of using an initial weak system to create and iteratively rene articial training data for a vised algorithm has been successfully applied to semi supervised sennrich et al and pervised machine translation lample et al as well as style transfer subramanian et al we investigate how the same general paradigm may be applied to the task of summarizing text mixed model back translation let us consider the task of transforming a quence in domain a into a corresponding quence in domain b e sentences in two guages for machine translation let da and db be corpora of sequences in a and b without any mapping between their respective elements the back translation approach starts with initial models ab and f ba which can be hand crafted or learned without aligned pairs and uses them to create articial aligned training data ab ba a dbo dao let s denote a supervised learning algorithm which takes a set of aligned sequence pairs and turns a mapping function this articial data can then be used to train the next iteration of models which in turn are used to create new cial training sets a and b can be switched here ab ab ba a dao the model is trained at each iteration on articial inputs and real outputs then used to create new training inputs thus if the initial system is nt too far off we can hope that training pairs get closer to the true data distribution with each step allowing in turn to train better models in the case of summarization we consider the domains of full text sequences df and of maries ds and attempt to learn summarization ff s and expansion fsf functions ever contrary to the translation case df and ds are not interchangeable considering that a mary typically has less information than the sponding full text we choose to only dene initial f s models we can still follow the proposed procedure by alternating directions at each step initialization models for summarization to initiate their process for the case of machine translation lample et al use two different initialization models for their neural nmt and phrase based pbsmt systems the former lies on denoising auto encoders in both languages with a shared latent space while the latter uses the pbsmt system of koehn et al with a phrase table obtained through unsupervised cabulary alignment as in grave et al while both of these methods work well for chine translation they rely on the input and output having similar lengths and information content in particular the statistical machine translation rithm tries to align most input tokens to an put word in the case of text summarization ever there is an inherent asymmetry between the full text and the summaries since the latter press only a subset of the former next we pose three initialization systems which implicitly model this information loss full implementation details are provided in the appendix procrustes thresholded alignment pr thr the rst initialization is similar to the one for smt in that it relies on unsupervised vocabulary alignment specically we train two skipgram word embedding models using fasttext janowski et al on df and ds then align them in a common space using the wasserstein procrustes method of grave et al then we map each word of a full text sequence to its nearest neighbor in the aligned space if their tance is smaller than some threshold or skip it erwise we also limit the output length keeping only the rst n tokens we refer to this function as f pr f original france took an important step toward power market liberalization monday braving union anger to announce the partial privatization of state owned behemoth electricite france pr thr france launched a partial unk of state controlled utility the privatization agency said dbae france s state owned gaz de france sa said tuesday it was considering partial partial privatization of france s state owned nuclear power plants france launches an initial public announcement wednesday as the european union announced it would soon undertake a partial privatization title france launches partial edf privatization table full text sequences generated by f pr and during the rst back translation loop denoising bag of word auto encoder dbae similarly to both lample et al and wang and lee we also devise a starting model based on a dae one major difference is that we use a simple bag of words bow encoder with xed pre trained word embeddings and a layer gru decoder indeed we nd that a bow encoder trained on the summaries reaches a construction rouge l f score of nearly on the test set indicating that word presence tion is mostly sufcient to model the summaries as for the noise model for each token in the put we remove it with probability and add a word drawn uniformly from the summary lary with probability the bow encoder has two advantages first it lacks the other models bias to keep the word order of the full text in the summary secondly when using the dbae to predict summaries from the full text we can weight the input word dings by their corpus level probability of ing in a summary forcing the model to pay less attention to words that only appear in df the denoising bag of words auto encoder with put re weighting is referred to as f f first order word moments matching we also propose an extractive initialization model given the same bow representation as for the dbae function s v predicts the probability that each word v in a full text sequence s is present in the summary we learn the parameters of f by marginalizing the output probability of each word over all full text sequences and matching these rst order moments to the marginal probability of each word s presence in a summary that is let v s denote the vocabulary of ds then v v s v psdf f and s v psds we minimize the binary cross entropy bce tween the output and summary moments arg min x vv s psdf s v s v we then dene an initial extractive summarization model by applying f to all words of an put sentence and keeping the ones whose output probability is greater than some threshold we fer to this model as f f articial training data we apply the back translation procedure outlined above in parallel for all three initialization els for example f yields the following quence of models and articial aligned datasets f s f sf f s sf f s finally in order to take advantage of the various strengths of each of the initialization models we also concatenate the articial training dataset at each odd iteration to train a summarizer e f f s f s f s experiments data and model choices we validate our proach on the gigaword corpus which comprises of a training set of m article headlines sidered to be the full text and titles summaries along with k validation pairs and we report test performance on the same k set used in rush et al since we want to learn systems from fully unaligned data without giving the model an opportunity to learn an implicit mapping we also pbsmt pre pr r l table test rouge for trivial baseline and tion systems and lee further split the training set into m examples for which we only use titles and m for headlines all models after the initialization step are mented as convolutional architectures ing fairseq ott et al articial data eration uses sampling with a minimum length of for full text and a maximum length of for summaries rouge scores are obtained with an output vocabulary of size k and a beam search of size to match wang and lee initializers table compares test rouge for different initialization models as well as the ial baseline which simply copies the rst words of the article we nd that simply olding on distance during the word alignment step of pr thr does slightly better then the full pbsmt system used by lample et al our bow denoising auto encoder with word weighting also performs signicantly better than the full dae initialization used by wang and lee pre dae the moments based initial model scores higher than either of these with scores already close to the full pervised system of wang and lee in order to investigate the effect of these three different strategies beyond their rouge statistics we show generations of the three corresponding rst iteration expanders for a given summary in table the unsupervised vocabulary alignment in pr thr handles vocabulary shift especially changes in verb tenses summaries tend to be in the present tense but maintains the word der and adds very little information conversely the expansion function which is learned from purely extractive summaries re uses most words in the summary without any change and adds some new information finally the encoder based dbae signicantly increases the sequence length and variety but also strays from pr advers sup r l k k k k m table comparison of full systems the best scores for unsupervised training are bolded results from and lee et al and blunsom and et al the original meaning more examples in the pendix the decoders also seem to learn facts about the world during their training on article text edf gdf is france s public power company full models finally table compares the marizers learned at various back translation ations to other unsupervised and semi supervised approaches overall our system outperforms the unsupervised adversarial reinforce of wang and lee after one back translation loop and most semi supervised systems after the ond one including song et al s mass pre trained sentence encoder and miao and som forced attention sentence sion fsc which use k and k aligned pairs respectively as far as back translation proaches are concerned we note that the model performances are correlated with the initializers scores reported in table iterations and low the same pattern in addition we nd that combining data from all three initializers before training a summarizer system at each iteration as described in section performs best suggesting that the greater variety of articial full text does help the model learn conclusion in this work we use the translation paradigm for unsupervised training of a summarization system we nd that the model benets from combining initializers matching the performance of semi supervised approaches references piotr bojanowski edouard grave armand joulin and tomas mikolov enriching word vectors with subword information tacl kyunghyun cho bart van merrienboer dzmitry danau and yoshua bengio on the properties of neural machine translation encoder decoder proaches in proceedings of eighth workshop on syntax semantics and ture in statistical translation doha qatar tober pages sumit chopra michael auli and alexander m rush abstractive sentence summarization with tentive recurrent neural networks in naacl hlt the conference of the north american chapter of the association for computational guistics human language technologies san diego california usa june pages jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language standing corr zhenxin fu xiaoye tan nanyun peng dongyan zhao and rui yan style transfer in text in proceedings of the exploration and evaluation thirty second aaai conference on articial gence the innovative applications of articial intelligence and the aaai symposium on educational advances in articial intelligence new orleans louisiana usa february pages sebastian gehrmann yuntian deng and alexander m rush bottom up abstractive summarization in proceedings of the conference on empirical methods in natural language processing brussels belgium october november pages edouard grave armand joulin and quentin berthet unsupervised alignment of embeddings with wasserstein procrustes corr american chapter of the association for tational linguistics hlt naacl edmonton canada may june guillaume lample myle ott alexis conneau dovic denoyer and marcaurelio ranzato phrase based neural unsupervised machine in proceedings of the conference on lation empirical methods in natural language ing brussels belgium october november pages yishu miao and phil blunsom language as a latent variable discrete generative models for sentence compression in proceedings of the conference on empirical methods in natural guage processing emnlp austin texas usa november pages ramesh nallapati bowen zhou ccero nogueira dos santos c aglar gulcehre and bing xiang abstractive text summarization using sequence in proceedings of the sequence rnns and beyond signll conference on computational natural language learning conll berlin germany august pages myle ott sergey edunov alexei baevski angela fan sam gross nathan ng david grangier fairseq a fast and michael auli tensible toolkit for sequence modeling corr alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of the tence summarization conference on empirical methods in natural guage processing emnlp lisbon portugal september pages abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proceedings of the nual meeting of the association for computational linguistics acl vancouver canada july august volume long papers pages sergey ioffe and christian szegedy batch malization accelerating deep network training by in proceedings reducing internal covariate shift of the international conference on machine learning icml lille france july pages rico sennrich barry haddow and alexandra birch improving neural machine translation in proceedings of the els with monolingual data annual meeting of the association for tational linguistics acl august berlin germany volume long papers nal kalchbrenner and phil blunsom recurrent in proceedings of continuous translation models the conference on empirical methods in ral language processing emnlp tober grand hyatt seattle seattle ton usa a meeting of sigdat a special interest group of the acl pages philipp koehn franz josef och and daniel marcu in statistical phrase based translation man language technology conference of the north kaitao song xu tan tao qin jianfeng lu and yan liu mass masked sequence to quence pre training for language generation in ceedings of the international conference on machine learning long beach california sandeep subramanian lample denoyer eric michael marcaurelio ranzato and y lan boureau multiple attribute text style transfer corr guillaume ludovic smith ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural works in advances in neural information ing systems annual conference on neural formation processing systems december montreal quebec canada pages pascal vincent hugo larochelle yoshua bengio and pierre antoine manzagol extracting and composing robust features with denoising in machine learning proceedings of toencoders the twenty fifth international conference icml helsinki finland june pages yau shian wang and hung yi lee learning to encode text as human readable summaries using generative adversarial networks in proceedings of the conference on empirical methods in ral language processing brussels belgium ber november pages a implementation choices for initialization and models we describe the modeling choices for tion models pr thr dbae and all hyper parameters for each of these systems are set based on the models rouge l score on the idation set unless otherwise stated all models use skipgram word embeddings which are shared across the input and output layers the dimension embeddings are trained on the catenation of the full text and summary sequences df ds v is the full vocabulary and v f and v s are the vocabularies of df and ds tively all trained models use the adam mizer with learning rate the convolutional models use the fconv tecture previded in with pre trained input and output word embeddings a vocabulary size of k for the full text and of k for the maries for the expander generations we collapse contiguous unk tokens and cut the sentence at the rst full stop even when the model did not generate an eos token yielding outputs that are sometimes shorter than words procrustes thresholded alignment pr thr for this model we train two sets of word beddings on df and ds separately and compute aligned vectors using the fasttext implementation of the grave et al we then map each word in an input sequence to its closest word in v s in the aligned space unless the est neighbor is the eos token or the distance to the nearest neighbor in the aligned space is greater than a threshold the output sequence then sists in the rst n mapped words in the order of the input sequence we found that using dings of dimension threshold and maximum output length n yields the best validation rouge l we compare pr thr to a pbsmt baseline in table we use the unsupervisedmt of lample et al with the same pre trained embedding and also perform a hyper parameter search over maximum length which sets n readthedocs io test models html com fasttext tree master alignment com unsupervisedmt tree master pbsmt denoising bag of word auto encoder dbae the dbae is trained on all sentences in ds the encoder of the dbae averages the input word beddings and applies a linear transformation lowed by a batch normalization layer ioffe and szegedy the decoder is a layer gru recurrent neural network with hidden dimension the encoder output is concatenated to the initial hidden state of both layers then projected back down to the hidden dimension to use the model for summarization we form two changes from the auto encoding setting first we perform a weighted instead of a standard average where words that are less likely to appear in ds than in df are down weighted and words that are in v f but not in v s are dropped ically given a word v v s its weight wv in the summarization weighted bow encoder is given as v psdf f and s v psds wv max s v f v secondly we implement something like a pointer mechanism by adding to the score of each of the input words in the output of the gru before the softmax at test time and when creating articial data we decode with beam search and a beam size of size maximum output length n and input word bias first order word moments matching the moments matching model uses the same coder as the dbae followed by a linear ping to the summary vocabulary followed by a sigmoid layer the log score of all words that do not appear in the input is set to nately computing the output probabilities for all sentences in the corpus before computing the nary cross entropy is impractical and so we plement a batched version of the algorithm let corpus level moments f v be dened as in equation let bf be a batch of full text quences we dene v and s v psbf f and s v s v f v f v for each batch the algorithm then takes a gradient step for the loss x vv s psbf s v s v the prediction is similar as for the pr thr tem except that we threshold on f s rather than on the nearest neighbor distance with old the maximum output length is also n b more examples of model predictions we present more examples of the expander and summarizer models outputs in tables and table shows more expander generations for all three initial models after one back translation epoch they follow the patterns outlined in tion with dbae showing more variety but ing less faithful to the input table show tions from the expander models at different translation iteration it is interesting to see that each of the three models slowly overcome their the dbae expander s third initial limitations version is much more faithful to the input than its rst while the moments based approach starts ing rephrases and modeling vocabulary shift the procrustes method seems to benet less from the successive iterations but still starts to produce longer outputs finally table provides maries produced by the nal model while the model does produce likely summaries we note that aside from the occasional synonym use or bal tense change and even though we do not use an explicit pointer mechanism beyond the standard attention the model s outputs are mostly extractive over n nnn ancient graves found in greek metro dig pr thr over n nnn ancient graves were found in a greek metro unk dbae the remains of n nnn graves on ancient greek island have been found in three ancient graves in the past few days a senior police ofcer said on friday over n nnn ancient graves have been found in the greek city of alexandria in the northern greek city of salonika in connection with the greek metro and dig deep underground ukraine crimea dreams of union with russia pr thr ukraine crimea unk of the union with russia dbae ukraine has signed two agreements with ukraine on forming its european union and ukraine as its membership ukraine s crimea peninsula dreams of unk one of the soviet republic s most unk country with russia the itar tass news agency reported malaysian opposition seeks international help to release detainees pr thr the malaysian opposition thursday sought international help to release detainees the malaysian opposition news reports said dbae malaysian prime minister abdullah ahmad badawi said tuesday that the government s decision to release nnn detainees a report said wednesday malaysian opposition parties said tuesday it seeks to help the release of detainees russia to unify energy transport networks with georgia rebels pr thr russia is to unify energy transport networks with georgia rebels dbae russian government leaders met with representatives of the international energy giant said monday that their networks have been trying to unify their areas with energy supplies russia is to unify its energy and telecommunication networks to cope with georgia s separatist rebels and the government eu losing hope of swift solution to treaty crisis pr thr the eu has been losing hope of a unk solution to the maastricht treaty crisis dbae the european union is losing hope it will be a swift solution to the crisis of the eu eu hoping that it s in an urgent referendum eu governments have already come under hope of a swift solution to a european union treaty that ended the current nancial crisis table more examples of articial data after the rst back translation iteration original malaysia has drafted its rst legislation aimed at punishing computer hackers an ofcial said wednesday pr malaysia has enacted a draft the rst law on a unk computer hacking pr malaysia has issued a draft of the law on computer hacking pr malaysia has drafted a rst law on the computer hacking and internet hacking malaysia s parliament friday signed a bill to allow computer users to monitor unk law the country has been submitted to parliament in nnnn passed a bill wednesday in the rst reading of the computer system ofcials said monday malaysia s national defense ministry has drafted a regulation of computer hacking in the country the prime minister said friday malaysia will have drafts the rst law on computer hacking malaysia has started drafts to be the rst law on computer hacking malaysia today presented the nation s rst law on computer hacking in the country news reports said wednesday title malaysia drafts rst law on computer hacking table evolution of generated full text sequences across iterations article chinese permanent representative to the united nations wang guangya on wednesday urged the un and the international community to continue supporting timor leste pred chinese permanent representative urges un to continue supporting timor leste title china stresses continued international support for timor leste article macedonian president branko crvenkovski will spend orthodox christmas this weekend with the country s troops serving in iraq his cabinet said thursday pred macedonian president to spend orthodox christmas with troops in iraq title macedonian president to visit troops in iraq article televangelist pat robertson it seems is nt the only one who thinks he can see god s purpose in natural disasters pred evangelist pat robertson thinks he can see god s purpose in disasters title editorial blaming god for disasters article the sudanese opposition said here thursday it had killed more than nnn government soldiers in an ambush in the east of the country pred sudanese opposition kills n government soldiers in ambush title sudanese opposition says nnn government troops killed in ambush table example of model predicitons for f
