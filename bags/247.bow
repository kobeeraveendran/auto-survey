controlling verbatim copying abstractive summarization kaiqiang bingqing zhe liu fei liu computer science department university central florida orlando fl usa robert bosch llc sunnyvale usa ucf edu bingqing wang zhe liu bosch com ucf edu v o n l c s c v v x r abstract abstract change meaning original text single effective way achieve increase copying allowing text abstraction human editors usually exercise control copying sulting summaries extractive abstractive vice versa remains poorly understood modern neural abstractive summarizers provide exibility e learning single reference summaries generate multiple summary hypotheses varying degrees copying paper present neural summarization model learning single human abstracts duce broad spectrum summaries ranging purely extractive highly generative ones frame task summarization language modeling exploit alternative mechanisms generate summary hypotheses method allows control copying training coding stages neural summarization model tensive experiments illustrate signicance posed method controlling verbatim ing achieve competitive results strong baselines analysis reveals interesting unobvious facts introduction ideal summarizer provide exibility ate summaries varying proportions reused text summaries required cater diverse usage scenarios e system abstracts contain excessive copied tent proper consecutive words longer considered eu standards author s lectual creation protected copyright law castilho et al proper control copying commercial summarizers held liable copyright fringements system abstracts ate copied content desirable highly abstractive ones likely suffer tent hallucination reiter better preserving meaning original text date remains poorly understood modern stractive summmarization provide needed exibility control copying generate diverse abstracts stractive summarizers encoder decoder architectures copy words source text generate new words unseen source liu manning chen bansal gehrmann deng rush recent work attempted increase use unseen words summaries weber et al kryscinski et al cases summarizers trained single reference abstracts produce single outputs xed corpus level copy rate multiple reference abstracts created input text varying grees copying teach system generate abstracts similar amounts copying time consuming costly create human abstracts unlikely humans learn exercise control copying understanding copy nism neural abstractive models producing abstracts varying degrees copying prove daunting best mission impossible worst paper goal generate abstractive summaries varying amounts reused text developing general framework learns single reference summaries dene copy rate percentage summary n grams pearing source text high copy rate suggests summary generated largely copying verbatim source text conversely low copy rate indicates text shortening word reordering paraphrasing straction involved generation process argue abstractive summarizers necessarily trained word reference summaries ought separate prediction summary words seen source text unseen underlying principle simple intuitively appealing summarizer trained predict seen words learns copy source text producing extractive summaries unseen words training summarizer gradually transforms copying copying generating new words present source text employing mix match strategy enable abstractive summarizer generate summaries copying copyright association advancement articial intelligence www aaai org rights reserved frame abstractive summarization language ing task present decoder framework uses question probable word hint word seen source text month old toddler reportedly abducted pennsylvania found dead district attorney said missing missing pennsylvania missing pennsylvania toddler missing pennsylvania toddler found reference summary missing pennsylvania toddler found dead question probable word hint word unseen source text rescuers suspended search coast santa cruz island passengers trapped aboard conception diving boat caught sank search search search suspended search suspended search suspended dive boat reference summary search suspended dive boat california coast table formulating summarization language modeling task rst model predicts summary words seen source text second model predicts seen words method provides exibility control copying mix matching types behaviors transformer architecture vaswani et al encode source text decode summary network parameters warm started pretrained deep representations contrast typical encoder decoder chitecture parameters encoder decoder warm started attention copy nism khandelwal et al method allows control copying training decoding stages neural model experiment varying portions seen unseen summary words training teach summarizer favor favor copying decoding time compare different search strategies rst search vs beam search reranking methods courage system abstracts use wording similar inal despite single reference summaries able benchmark evaluations able evaluate mary quality multiple dimensions automatic metrics based lexical similarity rouge lin semantic similarity bertscore zhang et al human assessment grammaticality ness system abstracts remain true original method demonstrates strong performance forming performing par best published results research contributions summarized follows introduce new summarization method provides needed exibility produce spectrum summaries input varying copied content summaries highly desirable cater diverse real world method emphasizes depth analysis copy behavior summarization frames abstractive rization language modeling task exploits multiple strategies training decoding stages generate verse summary hypotheses competitive results demonstrate effectiveness proposed method exercising control copying related work signicance controlling copying behavior summarization underestimated human tors reuse text original article produce summary jing mckeown adjust degree copying produce wide spectrum summaries e human written summaries newswire yen hermann et al meetings carletta et al liu liu scientic articles qazvinian et al online forums ouyang chang mckeown contain varying amounts reused text degree copying direct impact scores tomatic evaluation metrics rouge reported favor summaries use wording original ng abrecht reference summaries ing system summaries copying abstraction compression paraphrasing vantaged compared system summaries substantial copying urgent need paper makes rst attempt present tion framework capable producing summaries varying amounts reused text date extractive abstractive summarization techniques investigated nenkova mckeown rarely technique utilized produce extractive abstractive summaries given text extractive summarization selects important non redundant sentences original sentences optionally compressed remove tial phrases leading compressive summaries martins smith li et al wang et al filippova et al durrett berg kirkpatrick klein stractive summarization distills source text tial meanings performs language generation representation produce abstract barzilay liu et al liao lebanoff liu hardy vlachos systems rarely provide exibility end user indicate desired reused text summary eliminate need develop multiple systems extractive abstractive tion attempt introduce control copying ior neural abstractive summarization system neural abstractive summarization demonstrated siderable recent success utilizes encoder decoder architecture rush chopra weston liu manning chen bansal lebanoff song implementation models publicly available com ucfnlp control copying liu celikyilmaz et al recently ies attempted use deep contextualized tions bert devlin et al elmo peters et al boost encoder network converts source text x length vector conditioned decoder network unrolls summary word time tempting use pretrained deep sentations warm start encoder decoder khandelwal et al nd results satisfying attention weights pretrained paper adopts decoder framework dong et al transformer architecture encoding source text decoding summary copying help produce unseen words originally introduced framework neural machine translation gulcehre et al later abstractive summarization liu manning particularly knowles koehn examine inuence text sub words copying behavior nmt tem suppress copying kryciski et al introduce novelty metric optimized policy ing weber et al modify scoring function summary sequence decoding time fan grangier auli attempt control summary length entities source style portions address copying paper focus better understanding copying behavior summarization system present effective mechanisms control reused text cuss takes summarizer copy word explicit copying mechanism control behavior produce summaries copying following describe model great detail approach frame abstractive summarization language modeling task present decoder framework uses transformer architecture vaswani et al encode source text decode summary let v sequence source tokens y v summary tokens goal model conditional probability distribution p j transformer inspired architecture use byte pair encoding bpe sennrich et al tokenization vocabulary size kens bpe shown improve robustness curacy neural model training use parameter tying lowing token embeddings layer nal softmax layer transformer model method includes special tokens start end mask respectively denote start end quence masked token illustration tem architecture provided figure training construct source sequence prepending start appending end input text e start abeth taken hospital end illustrated figure similarly target sequence y constructed appending figure illustration copytrans architecture self attention mechanism allows source word attend lower level representations source words ing build higher level representation ii summary word attend source words summary words prior token current position mask build higher level representation end summary e y elizabeth hospitalized end system learns predict target sequence word time end token reached conditional probability shown eq p p j p training time argue system necessarily trained predict word target sequences selected collection sufce selected target tokens provides important potential steer system extractive abstractive vice versa divide tokens sequence categories summary tokens seen source text summary tokens unseen source c source tokens tation training system predict seen summary tokens reinforce copying behavior unseen tokens allow generation source words enable system learn better token representations mix matching tokens categories enable summarizer generate summaries copying randomly sample set tokens category bernoulli distribution probability value p varies category analysis provided experiments section let mi denote token z selected probability dened p mi p selected token replaced mask time meaning token masked quence z time replaced random startelizabethwastakentothehospitalelizabethwashospitalizedendendoriginalinputstartelizabethwasmasktothehospitalmaskwasmaskendendmaskedinputtransformerpredictedoutputtakenelizabethhospitalizeda missing worda seen wordan unseen wordsource textsummary token vocabulary v remains unchanged nal following use z represent masked sequence selected tokens predicted model training loss term dened follows algorithm best search procedure best m k input sequence model beam size log p important note apply binary mask self attention mechanism transformer architecture allow source token attend source tokens ing summary token attend source tokens summary tokens prior current token mask order learn deep contextualized sentations formulation similar dong et al binary mask dened eq square matrix th row represents mask token z source token mask allows attend source tokens m att j j summary token attend tokens prior current token m att j j m att j j input transformer consists embedding matrices wp ws respectively denote token position segment embeddings devlin et al z p s hot matrices retrieve embeddings tokens sequence z token position segment embeddings token added element wisely zwe pwp sws transformer model takes input embeddings binary mask m att produce sequence deep contextualized representations ticularly hi predict th missing token sequence use parameter tying allowing token embeddings input layer eq nal softmax layer model eq h m att p e hi decoding given trained model input text decoding stage searches summary sequence maximizes p present search algorithms stage search uses priority heap partial maries scored according heuristic function iteration search algorithm takes scoring partial summary extends word pushes new summary sequences priority heap erate k new summary sequences selecting k words highest probability log p j eq iteratively appending words partial summary highest scoring summary heap concludes end sentence symbol moved pool pleted summaries later reranking heap keeps init h init reset h priority queue answer collector current h pop current ends end continue candidates reset w v extended current s mask candidates extended topk k argmin candidates h return collection partial summaries varying lengths visited according scores provide illustration search algorithm algorithm contrast beam search essentially search maintains beam size time step containing partial summaries length partial mary algorithm extends word producing k new sequences appending k words highest probability log p j partial mary process generates total k k new summary sequences extending k partial summaries algorithm selects k best candidates beam iteration candidate summary cludes end sentence symbol moved pool completed summaries search beam search employ scoring function scores candidate summary sum log likelihoods eq differ search strategies beam search visits candidate summaries according summary length search favors candidates attaining higher scores y arg max log p j yy end s t compute p j trained copytrans model importantly mask token prompt model predict word e start beth taken hospital end elizabeth mask concatenation source text partial summary mask token fed copytrans model contextualized representation mask input size priority heap capped heap reached capacity new summary sequence needs pushed lowest scoring removed heap softmax layer predict token v imental results demonstrate dynamic ized representation mask performs reliably ing token represents important distinction shifting target sequence position tion common encoder decoder models reranking reranking step necessary candidate summaries decoded beam search rst search meet length requirement e overly short summary containing words rarely informative summary despite high likelihood score compare reranking gies offset limitation length normalization adopted et al frequently systems divides inal log likelihood score denoted y log p total number tokens summary effectively prevent long summary penalized bp norm introduces brevity penalty summaries meet length expectation illustrated eq bp norm performs length normalization adds penalty term log bp scoring function modify original penalty term yang huang ma favor summaries copying eq dene r copy rate e percentage summary tokens seen source text scaled factor c copy rate r set penalty dropped yang huang ma provides nice proof showing penalty term directly translate coefcient multiplied log likelihood score eq log bp bp r exp y bp exp log p j bp p j soft bounded word reward sbwr newly introduced method assigns word reward summary decoded summary longer expected lpred added words receive diminishing reward summary shorter lpred word receive reward method promotes summaries similar length predicted lpred sigmoid function smooth reward values r coefcient scale total reward tuned validation data y y r obtain predicted length lpred greedy search empirically offset predicted length words according validation set cases force decoder source text premier chang chun hsiung said thursday enraged saddened snail paced progress reconstruction areas hardest hit disastrous earthquake rattled taiwan sept summary premier expresses condolences taiwan quake victims premier angry reconstruction quake hit areas premier enraged saddened earthquake reconstruction premier enraged slow progress post quake reconstruction source text blue ribbon panel experts said wednesday german economic growth grind halt year raising doubts berlin s plans shield europe s biggest economy global turmoil summary german experts raise doubts economic recovery experts german growth grind halt year german experts grind halt year german economy grind halt experts table example system summaries produced generator networks method best abstract method pure extract human abstract output trigram ing common practice avoid repetitions paulus xiong socher experiments data evaluation metrics evaluate proposed method sentence rization task goal condense lengthy source tence title like summary comparing single document summarization sentence summarization deals tent selection ground truth summaries contain paraphrasing abstraction conduct experiments gigaword parker newsroom grusky man artzi datasets gigaword articles lected newsroom spans range pair rst sentence article title form instance train valid test splits contain instances gigaword instances newsroom experiment datasets understand copying behavior domain adaptation effects models despite gle reference summaries available benchmark tions able evaluate summary quality tiple dimensions automatic metrics based cal similarity rouge lin semantic similarity bertscore zhang et al human sessment grammaticality informativeness system abstracts remain true original experimental settings initialize model parameters pretrained base uncased model model ne tuned ing split gigaword newsroom dataset stractive summarization model uses layer training loss b d e g h gigaword gram gram gram gram average newsroom gram gram gram gram average table copy rate summarization models dene copy rate percentage summary n grams appearing source text average experiment selecting varying amounts seen summary tokens unseen summary tokens source tokens training circle corresponds million tokens gigaword tokens newsroom compute loss term system multi task entailment seass drgd pg networks biset search beam search r l bert s table summarization results gigaword test set lower table contains results system architecture hidden state size attention heads use adam optimizer learning rate set halved validation loss change training steps set weight decay regular layers weight decay dropout normalization sampling rate p set source words summary words seen unseen model ne tuned epochs epoch takes hours tesla gpu batch size set summarization results control copying bias summarizer duce summaries extractive abstractive vice versa summarizer trained solely mary words seen source text learn copy words testing generate new words seek answer questions section particularly vide tokens selected training categories summary tokens seen source text summary tokens unseen source c source tokens tation training system predict seen summary tokens reinforce copying behavior unseen tokens allow generation source words enable system learn richer representations mix matching tokens enable summarizer copy analyze copy rate summarization els table copy rate dened percentage mary n grams appearing source text set average high copy rate suggests summary generated largely copying verbatim source text experiment selecting varying amounts seen summary tokens unseen summary tokens source tokens training number circles proportional number tokens puting loss term summaries table decoded beam search reranking ndings suggest factor makes impact copying behavior summarizer portion seen unseen summary words ing model summarizer trained purely seen words case table reuses source words ing testing despite prevent tem generating new words gram copy rate case datasets minor gap tokenization discrepancies unseen words training summarizer gradually transforms copying copying generating new words present source text observe ratio seen vs unseen words ground truth summaries datasets newsroom slightly tractive gigaword analysis reveals portant maintain similar ratio training order achieve high rouge scores pure extracts attain high rouge scores ground truth summaries abstracts analysis suggests training source words little impact copying behavior system improves representation learning lead consistently improved f scores system comparison table shows results mark summarization data containing testing instances gigaword contrast system system m pg networks o o r s pure ext w e best abs n pure ext g best g r l bert s system human pg networks biset pure ext best inform gramm truthful bst wst table summarization results newsroom test set systems trained newsroom training data systems trained gigaword table human assessment informativeness cality truthfulness best worst scaling tion baselines developed recent years include nallapati et al multi task entailment sunuru bansal seass zhou et al drgd li et al guo sunuru bansal pg networks liu ning song zhao liu cao et al biset wang quan wang output summaries tems graciously provided authors ate summary quality automatic metrics including lin measures n gram overlap system reference summaries bertscore zhang et al quanties semantic similarity bert based contextualized representations results system achieves competitive mance surpassing strong systems having reported results dataset judged metrics results strate effectiveness transformer based architecture abstractive summarization observe beam search reranking yields highest sults case g table training bp norm sbwr appear outstanding reranking methods ter length normalization observation suggests search beam search produce similar outcome despite differ search gies beam search visiting candidates according mary length search favoring candidates having high log likelihood scores suggest future work plore search methods search domain adaptation investigate effect domain adaptation training model gigaword testing newsroom test set results reported table surprisingly performance degradation ing model cross domain setting observe model copying pure extract case e degrade gracefully counterpart best abstract case smaller performance gap cross domain tings models perform competitively comparing baseline methods human evaluation thoroughly analyze quality summaries ask man annotators assess system outputs sions including informativeness summary covered options important content source text grammaticality summary sentence grammatically correct ness summary successfully preserved meaning original text system human summaries scored according criteria likert scale worst best compare variants method erating pure extracts case e best abstracts case g baselines c pg networks d e biset human abstracts following liu lapata perform best worst scaling human selects best worst summary candidates nal ing system computed percentage times selected best minus worst ple instances gigaword test set evaluation instance assessed ve human evaluators amazon mechnical turk low quality annotations manually removed results presented table observe human summaries article titles imperfect contain details nonexistent source table provide means researchers train neural models annotating reference maries contrast systems perform slightly consistently better baselines conclusion paper present transformer based decoder framework generate summaries ing proposed method generate tractive abstractive summaries method emphasizes depth analysis copy behavior summarization exploits multiple strategies training decoding stages generate diverse summary hypotheses itive results demonstrate effectiveness posed method exercising control copying acknowledgments grateful reviewers helpful comments work performed kaiqiang song intern bosch research research supported national science foundation grant references barzilay r mckeown k r sentence fusion tidocument news summarization computational linguistics cao z li w li s wei f retrieve rerank rewrite soft template based neural summarization acl carletta j al ami meeting corpus mlmi celikyilmaz bosselut x choi y deep communicating agents abstractive summarization naacl chen y bansal m fast abstractive summarization reinforce selected sentence rewriting acl de castilho r e dore g margoni t labropoulou p gurevych legal perspective training models natural language processing proc lrec devlin j chang m lee k toutanova k bert pre training deep bidirectional transformers language standing proc naacl dong l yang n wang w wei f liu x wang y gao j zhou m hon h unied language model pre training natural language understanding generation org durrett g berg kirkpatrick t klein d based single document summarization compression anaphoricity constraints proc acl fan grangier d auli m controllable abstractive summarization workshop nmt generation filippova k alfonseca e colmenares c kaiser l vinyals o sentence compression deletion lstms proc emnlp gehrmann s deng y rush m stractive summarization proc emnlp grusky m naaman m artzi y newsroom dataset million summaries diverse extractive strategies proc naacl gulcehre c ahn s nallapati r zhou b bengio y pointing unknown words proc acl guo h pasunuru r bansal m soft layer specic multi task summarization entailment question generation proc acl hardy h vlachos guided neural language ation abstractive summarization abstract meaning sentation proc emnlp hermann k m kocisky t grefenstette e espeholt l kay w suleyman m blunsom p teaching machines read comprehend proc nips jing h mckeown k decomposition written summary sentences proc sigir khandelwal u clark k jurafsky d kaiser l ple efcient text summarization single pre trained org knowles r koehn p context copying neural machine translation proc emnlp kryscinski w paulus r xiong c socher r proving abstraction text summarization emnlp lebanoff l song k liu f adapting neural encoder decoder framework single multi document marization emnlp li c liu f weng f liu y document tion guided sentence compression emnlp li p lam w bing l wang z deep recurrent generative decoder abstractive text summarization emnlp liao k lebanoff l liu f abstract meaning sentation multi document summarization coling lin c rouge package automatic evaluation summaries wksp text summarization branches liu y lapata m hierarchical transformers document summarization acl liu f liu y abstractive speech tion exploring unsupervised supervised approaches ken utterance compression ieee trans aslp liu f flanigan j thomson s sadeh n smith n abstractive summarization semantic tations proc naacl martins f t smith n summarization joint model sentence extraction compression workshop integer linear programming natural language processing nallapati r zhou b dos santos c gulcehre c xiang b abstractive text summarization sequence sequence rnns proc signll nenkova mckeown k automatic summarization foundations trends information retrieval ng j abrecht v better summarization evaluation word embeddings rouge emnlp ouyang j chang s mckeown k crowd sourced iterative annotation narrative summarization corpora eacl p yen j introduction nist parker r english gigaword fth edition philadelphia linguistic data consortium pasunuru r bansal m multi reward reinforced marization saliency entailment naacl paulus r xiong c socher r deep reinforced model abstractive summarization proc emnlp peters m e neumann m iyyer m gardner m clark c lee k zettlemoyer l deep contextualized word representations proc naacl qazvinian v radev d r mohammad s m dorr b zajic d whidby m moon t generating extractive maries scientic paradigms jair reiter e structured review validity bleu computational linguistics rush m chopra s weston j neural attention model sentence summarization emnlp liu p j manning c d point summarization pointer generator networks proc acl song k zhao l liu f structure infused copy anisms abstractive summarization proc coling vaswani shazeer n parmar n uszkoreit j jones l gomez n kaiser l polosukhin attention need proc nips wang l raghavan h castelli v florian r cardie c sentence compression based framework query focused multi document summarization acl wang k quan x wang r biset bi directional selective encoding template abstractive summarization proc acl weber n shekhar l balasubramanian n cho k controlling decoding abstractive summaries based networks org yang y huang l ma m breaking beam search curse study methods stopping criteria neural machine translation emnlp zhang t kishore v wu f weinberger k q artzi y bertscore evaluating text generation bert org
