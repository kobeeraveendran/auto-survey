controlling verbatim copying abstractive summarization kaiqiang bingqing zhe liu fei liu computer science department university central florida orlando usa robert bosch llc sunnyvale usa ucf edu bingqing wang zhe liu bosch com ucf edu abstract abstract change meaning original text single effective way achieve increase copying allowing text abstraction human editors usually exercise control copying sulting summaries extractive abstractive vice versa remains poorly understood modern neural abstractive summarizers provide exibility learning single reference summaries generate multiple summary hypotheses varying degrees copying paper present neural summarization model learning single human abstracts duce broad spectrum summaries ranging purely extractive highly generative ones frame task summarization language modeling exploit alternative mechanisms generate summary hypotheses method allows control copying training coding stages neural summarization model tensive experiments illustrate signicance posed method controlling verbatim ing achieve competitive results strong baselines analysis reveals interesting unobvious facts introduction ideal summarizer provide exibility ate summaries varying proportions reused text summaries required cater diverse usage scenarios system abstracts contain excessive copied tent proper consecutive words longer considered standards author lectual creation protected copyright law castilho proper control copying commercial summarizers held liable copyright fringements system abstracts ate copied content desirable highly abstractive ones likely suffer tent hallucination reiter better preserving meaning original text date remains poorly understood modern stractive summmarization provide needed exibility control copying generate diverse abstracts stractive summarizers encoder decoder architectures copy words source text generate new words unseen source liu manning chen bansal gehrmann deng rush recent work attempted increase use unseen words summaries weber kryscinski cases summarizers trained single reference abstracts produce single outputs xed corpus level copy rate multiple reference abstracts created input text varying grees copying teach system generate abstracts similar amounts copying time consuming costly create human abstracts unlikely humans learn exercise control copying understanding copy nism neural abstractive models producing abstracts varying degrees copying prove daunting best mission impossible worst paper goal generate abstractive summaries varying amounts reused text developing general framework learns single reference summaries dene copy rate percentage summary grams pearing source text high copy rate suggests summary generated largely copying verbatim source text conversely low copy rate indicates text shortening word reordering paraphrasing straction involved generation process argue abstractive summarizers necessarily trained word reference summaries ought separate prediction summary words seen source text unseen underlying principle simple intuitively appealing summarizer trained predict seen words learns copy source text producing extractive summaries unseen words training summarizer gradually transforms copying copying generating new words present source text employing mix match strategy enable abstractive summarizer generate summaries copying copyright association advancement articial intelligence www aaai org rights reserved frame abstractive summarization language ing task present decoder framework uses question probable word hint word seen source text month old toddler reportedly abducted pennsylvania found dead district attorney said missing missing pennsylvania missing pennsylvania toddler missing pennsylvania toddler found reference summary missing pennsylvania toddler found dead question probable word hint word unseen source text rescuers suspended search coast santa cruz island passengers trapped aboard conception diving boat caught sank search search search suspended search suspended search suspended dive boat reference summary search suspended dive boat california coast table formulating summarization language modeling task rst model predicts summary words seen source text second model predicts seen words method provides exibility control copying mix matching types behaviors transformer architecture vaswani encode source text decode summary network parameters warm started pretrained deep representations contrast typical encoder decoder chitecture parameters encoder decoder warm started attention copy nism khandelwal method allows control copying training decoding stages neural model experiment varying portions seen unseen summary words training teach summarizer favor favor copying decoding time compare different search strategies rst search beam search reranking methods courage system abstracts use wording similar inal despite single reference summaries able benchmark evaluations able evaluate mary quality multiple dimensions automatic metrics based lexical similarity rouge lin semantic similarity bertscore zhang human assessment grammaticality ness system abstracts remain true original method demonstrates strong performance forming performing par best published results research contributions summarized follows introduce new summarization method provides needed exibility produce spectrum summaries input varying copied content summaries highly desirable cater diverse real world method emphasizes depth analysis copy behavior summarization frames abstractive rization language modeling task exploits multiple strategies training decoding stages generate verse summary hypotheses competitive results demonstrate effectiveness proposed method exercising control copying related work signicance controlling copying behavior summarization underestimated human tors reuse text original article produce summary jing mckeown adjust degree copying produce wide spectrum summaries human written summaries newswire yen hermann meetings carletta liu liu scientic articles qazvinian online forums ouyang chang mckeown contain varying amounts reused text degree copying direct impact scores tomatic evaluation metrics rouge reported favor summaries use wording original abrecht reference summaries ing system summaries copying abstraction compression paraphrasing vantaged compared system summaries substantial copying urgent need paper makes rst attempt present tion framework capable producing summaries varying amounts reused text date extractive abstractive summarization techniques investigated nenkova mckeown rarely technique utilized produce extractive abstractive summaries given text extractive summarization selects important non redundant sentences original sentences optionally compressed remove tial phrases leading compressive summaries martins smith wang filippova durrett berg kirkpatrick klein stractive summarization distills source text tial meanings performs language generation representation produce abstract barzilay liu liao lebanoff liu hardy vlachos systems rarely provide exibility end user indicate desired reused text summary eliminate need develop multiple systems extractive abstractive tion attempt introduce control copying ior neural abstractive summarization system neural abstractive summarization demonstrated siderable recent success utilizes encoder decoder architecture rush chopra weston liu manning chen bansal lebanoff song implementation models publicly available com ucfnlp control copying liu celikyilmaz recently ies attempted use deep contextualized tions bert devlin elmo peters boost encoder network converts source text length vector conditioned decoder network unrolls summary word time tempting use pretrained deep sentations warm start encoder decoder khandelwal results satisfying attention weights pretrained paper adopts decoder framework dong transformer architecture encoding source text decoding summary copying help produce unseen words originally introduced framework neural machine translation gulcehre later abstractive summarization liu manning particularly knowles koehn examine inuence text sub words copying behavior nmt tem suppress copying kryciski introduce novelty metric optimized policy ing weber modify scoring function summary sequence decoding time fan grangier auli attempt control summary length entities source style portions address copying paper focus better understanding copying behavior summarization system present effective mechanisms control reused text cuss takes summarizer copy word explicit copying mechanism control behavior produce summaries copying following describe model great detail approach frame abstractive summarization language modeling task present decoder framework uses transformer architecture vaswani encode source text decode summary let sequence source tokens summary tokens goal model conditional probability distribution transformer inspired architecture use byte pair encoding bpe sennrich tokenization vocabulary size kens bpe shown improve robustness curacy neural model training use parameter tying lowing token embeddings layer nal softmax layer transformer model method includes special tokens start end mask respectively denote start end quence masked token illustration tem architecture provided figure training construct source sequence prepending start appending end input text start abeth taken hospital end illustrated figure similarly target sequence constructed appending figure illustration copytrans architecture self attention mechanism allows source word attend lower level representations source words ing build higher level representation summary word attend source words summary words prior token current position mask build higher level representation end summary elizabeth hospitalized end system learns predict target sequence word time end token reached conditional probability shown training time argue system necessarily trained predict word target sequences selected collection sufce selected target tokens provides important potential steer system extractive abstractive vice versa divide tokens sequence categories summary tokens seen source text summary tokens unseen source source tokens tation training system predict seen summary tokens reinforce copying behavior unseen tokens allow generation source words enable system learn better token representations mix matching tokens categories enable summarizer generate summaries copying randomly sample set tokens category bernoulli distribution probability value varies category analysis provided experiments section let denote token selected probability dened selected token replaced mask time meaning token masked quence time replaced random startelizabethwastakentothehospitalelizabethwashospitalizedendendoriginalinputstartelizabethwasmasktothehospitalmaskwasmaskendendmaskedinputtransformerpredictedoutputtakenelizabethhospitalizeda missing worda seen wordan unseen wordsource textsummary token vocabulary remains unchanged nal following use represent masked sequence selected tokens predicted model training loss term dened follows algorithm best search procedure best input sequence model beam size log important note apply binary mask self attention mechanism transformer architecture allow source token attend source tokens ing summary token attend source tokens summary tokens prior current token mask order learn deep contextualized sentations formulation similar dong binary mask dened square matrix row represents mask token source token mask allows attend source tokens att summary token attend tokens prior current token att att input transformer consists embedding matrices respectively denote token position segment embeddings devlin hot matrices retrieve embeddings tokens sequence token position segment embeddings token added element wisely zwe pwp sws transformer model takes input embeddings binary mask att produce sequence deep contextualized representations ticularly predict missing token sequence use parameter tying allowing token embeddings input layer nal softmax layer model att decoding given trained model input text decoding stage searches summary sequence maximizes present search algorithms stage search uses priority heap partial maries scored according heuristic function iteration search algorithm takes scoring partial summary extends word pushes new summary sequences priority heap erate new summary sequences selecting words highest probability log iteratively appending words partial summary highest scoring summary heap concludes end sentence symbol moved pool pleted summaries later reranking heap keeps init init reset priority queue answer collector current pop current ends end continue candidates reset extended current mask candidates extended topk argmin candidates return collection partial summaries varying lengths visited according scores provide illustration search algorithm algorithm contrast beam search essentially search maintains beam size time step containing partial summaries length partial mary algorithm extends word producing new sequences appending words highest probability log partial mary process generates total new summary sequences extending partial summaries algorithm selects best candidates beam iteration candidate summary cludes end sentence symbol moved pool completed summaries search beam search employ scoring function scores candidate summary sum log likelihoods differ search strategies beam search visits candidate summaries according summary length search favors candidates attaining higher scores arg max log end compute trained copytrans model importantly mask token prompt model predict word start beth taken hospital end elizabeth mask concatenation source text partial summary mask token fed copytrans model contextualized representation mask input size priority heap capped heap reached capacity new summary sequence needs pushed lowest scoring removed heap softmax layer predict token imental results demonstrate dynamic ized representation mask performs reliably ing token represents important distinction shifting target sequence position tion common encoder decoder models reranking reranking step necessary candidate summaries decoded beam search rst search meet length requirement overly short summary containing words rarely informative summary despite high likelihood score compare reranking gies offset limitation length normalization adopted frequently systems divides inal log likelihood score denoted log total number tokens summary effectively prevent long summary penalized norm introduces brevity penalty summaries meet length expectation illustrated norm performs length normalization adds penalty term log scoring function modify original penalty term yang huang favor summaries copying dene copy rate percentage summary tokens seen source text scaled factor copy rate set penalty dropped yang huang provides nice proof showing penalty term directly translate coefcient multiplied log likelihood score log exp exp log soft bounded word reward sbwr newly introduced method assigns word reward summary decoded summary longer expected lpred added words receive diminishing reward summary shorter lpred word receive reward method promotes summaries similar length predicted lpred sigmoid function smooth reward values coefcient scale total reward tuned validation data obtain predicted length lpred greedy search empirically offset predicted length words according validation set cases force decoder source text premier chang chun hsiung said thursday enraged saddened snail paced progress reconstruction areas hardest hit disastrous earthquake rattled taiwan sept summary premier expresses condolences taiwan quake victims premier angry reconstruction quake hit areas premier enraged saddened earthquake reconstruction premier enraged slow progress post quake reconstruction source text blue ribbon panel experts said wednesday german economic growth grind halt year raising doubts berlin plans shield europe biggest economy global turmoil summary german experts raise doubts economic recovery experts german growth grind halt year german experts grind halt year german economy grind halt experts table example system summaries produced generator networks method best abstract method pure extract human abstract output trigram ing common practice avoid repetitions paulus xiong socher experiments data evaluation metrics evaluate proposed method sentence rization task goal condense lengthy source tence title like summary comparing single document summarization sentence summarization deals tent selection ground truth summaries contain paraphrasing abstraction conduct experiments gigaword parker newsroom grusky man artzi datasets gigaword articles lected newsroom spans range pair rst sentence article title form instance train valid test splits contain instances gigaword instances newsroom experiment datasets understand copying behavior domain adaptation effects models despite gle reference summaries available benchmark tions able evaluate summary quality tiple dimensions automatic metrics based cal similarity rouge lin semantic similarity bertscore zhang human sessment grammaticality informativeness system abstracts remain true original experimental settings initialize model parameters pretrained base uncased model model tuned ing split gigaword newsroom dataset stractive summarization model uses layer training loss gigaword gram gram gram gram average newsroom gram gram gram gram average table copy rate summarization models dene copy rate percentage summary grams appearing source text average experiment selecting varying amounts seen summary tokens unseen summary tokens source tokens training circle corresponds million tokens gigaword tokens newsroom compute loss term system multi task entailment seass drgd networks biset search beam search bert table summarization results gigaword test set lower table contains results system architecture hidden state size attention heads use adam optimizer learning rate set halved validation loss change training steps set weight decay regular layers weight decay dropout normalization sampling rate set source words summary words seen unseen model tuned epochs epoch takes hours tesla gpu batch size set summarization results control copying bias summarizer duce summaries extractive abstractive vice versa summarizer trained solely mary words seen source text learn copy words testing generate new words seek answer questions section particularly vide tokens selected training categories summary tokens seen source text summary tokens unseen source source tokens tation training system predict seen summary tokens reinforce copying behavior unseen tokens allow generation source words enable system learn richer representations mix matching tokens enable summarizer copy analyze copy rate summarization els table copy rate dened percentage mary grams appearing source text set average high copy rate suggests summary generated largely copying verbatim source text experiment selecting varying amounts seen summary tokens unseen summary tokens source tokens training number circles proportional number tokens puting loss term summaries table decoded beam search reranking ndings suggest factor makes impact copying behavior summarizer portion seen unseen summary words ing model summarizer trained purely seen words case table reuses source words ing testing despite prevent tem generating new words gram copy rate case datasets minor gap tokenization discrepancies unseen words training summarizer gradually transforms copying copying generating new words present source text observe ratio seen unseen words ground truth summaries datasets newsroom slightly tractive gigaword analysis reveals portant maintain similar ratio training order achieve high rouge scores pure extracts attain high rouge scores ground truth summaries abstracts analysis suggests training source words little impact copying behavior system improves representation learning lead consistently improved scores system comparison table shows results mark summarization data containing testing instances gigaword contrast system system networks pure ext best abs pure ext best bert system human networks biset pure ext best inform gramm truthful bst wst table summarization results newsroom test set systems trained newsroom training data systems trained gigaword table human assessment informativeness cality truthfulness best worst scaling tion baselines developed recent years include nallapati multi task entailment sunuru bansal seass zhou drgd guo sunuru bansal networks liu ning song zhao liu cao biset wang quan wang output summaries tems graciously provided authors ate summary quality automatic metrics including lin measures gram overlap system reference summaries bertscore zhang quanties semantic similarity bert based contextualized representations results system achieves competitive mance surpassing strong systems having reported results dataset judged metrics results strate effectiveness transformer based architecture abstractive summarization observe beam search reranking yields highest sults case table training norm sbwr appear outstanding reranking methods ter length normalization observation suggests search beam search produce similar outcome despite differ search gies beam search visiting candidates according mary length search favoring candidates having high log likelihood scores suggest future work plore search methods search domain adaptation investigate effect domain adaptation training model gigaword testing newsroom test set results reported table surprisingly performance degradation ing model cross domain setting observe model copying pure extract case degrade gracefully counterpart best abstract case smaller performance gap cross domain tings models perform competitively comparing baseline methods human evaluation thoroughly analyze quality summaries ask man annotators assess system outputs sions including informativeness summary covered options important content source text grammaticality summary sentence grammatically correct ness summary successfully preserved meaning original text system human summaries scored according criteria likert scale worst best compare variants method erating pure extracts case best abstracts case baselines networks biset human abstracts following liu lapata perform best worst scaling human selects best worst summary candidates nal ing system computed percentage times selected best minus worst ple instances gigaword test set evaluation instance assessed human evaluators amazon mechnical turk low quality annotations manually removed results presented table observe human summaries article titles imperfect contain details nonexistent source table provide means researchers train neural models annotating reference maries contrast systems perform slightly consistently better baselines conclusion paper present transformer based decoder framework generate summaries ing proposed method generate tractive abstractive summaries method emphasizes depth analysis copy behavior summarization exploits multiple strategies training decoding stages generate diverse summary hypotheses itive results demonstrate effectiveness posed method exercising control copying acknowledgments grateful reviewers helpful comments work performed kaiqiang song intern bosch research research supported national science foundation grant references barzilay mckeown sentence fusion tidocument news summarization computational linguistics cao wei retrieve rerank rewrite soft template based neural summarization acl carletta ami meeting corpus mlmi celikyilmaz bosselut choi deep communicating agents abstractive summarization naacl chen bansal fast abstractive summarization reinforce selected sentence rewriting acl castilho dore margoni labropoulou gurevych legal perspective training models natural language processing proc lrec devlin chang lee toutanova bert pre training deep bidirectional transformers language standing proc naacl dong yang wang wei liu wang gao zhou hon unied language model pre training natural language understanding generation org durrett berg kirkpatrick klein based single document summarization compression anaphoricity constraints proc acl fan grangier auli controllable abstractive summarization workshop nmt generation filippova alfonseca colmenares kaiser vinyals sentence compression deletion lstms proc emnlp gehrmann deng rush stractive summarization proc emnlp grusky naaman artzi newsroom dataset million summaries diverse extractive strategies proc naacl gulcehre ahn nallapati zhou bengio pointing unknown words proc acl guo pasunuru bansal soft layer specic multi task summarization entailment question generation proc acl hardy vlachos guided neural language ation abstractive summarization abstract meaning sentation proc emnlp hermann kocisky grefenstette espeholt kay suleyman blunsom teaching machines read comprehend proc nips jing mckeown decomposition written summary sentences proc sigir khandelwal clark jurafsky kaiser ple efcient text summarization single pre trained org knowles koehn context copying neural machine translation proc emnlp kryscinski paulus xiong socher proving abstraction text summarization emnlp lebanoff song liu adapting neural encoder decoder framework single multi document marization emnlp liu weng liu document tion guided sentence compression emnlp lam bing wang deep recurrent generative decoder abstractive text summarization emnlp liao lebanoff liu abstract meaning sentation multi document summarization coling lin rouge package automatic evaluation summaries wksp text summarization branches liu lapata hierarchical transformers document summarization acl liu liu abstractive speech tion exploring unsupervised supervised approaches ken utterance compression ieee trans aslp liu flanigan thomson sadeh smith abstractive summarization semantic tations proc naacl martins smith summarization joint model sentence extraction compression workshop integer linear programming natural language processing nallapati zhou dos santos gulcehre xiang abstractive text summarization sequence sequence rnns proc signll nenkova mckeown automatic summarization foundations trends information retrieval abrecht better summarization evaluation word embeddings rouge emnlp ouyang chang mckeown crowd sourced iterative annotation narrative summarization corpora eacl yen introduction nist parker english gigaword fth edition philadelphia linguistic data consortium pasunuru bansal multi reward reinforced marization saliency entailment naacl paulus xiong socher deep reinforced model abstractive summarization proc emnlp peters neumann iyyer gardner clark lee zettlemoyer deep contextualized word representations proc naacl qazvinian radev mohammad dorr zajic whidby moon generating extractive maries scientic paradigms jair reiter structured review validity bleu computational linguistics rush chopra weston neural attention model sentence summarization emnlp liu manning point summarization pointer generator networks proc acl song zhao liu structure infused copy anisms abstractive summarization proc coling vaswani shazeer parmar uszkoreit jones gomez kaiser polosukhin attention need proc nips wang raghavan castelli florian cardie sentence compression based framework query focused multi document summarization acl wang quan wang biset directional selective encoding template abstractive summarization proc acl weber shekhar balasubramanian cho controlling decoding abstractive summaries based networks org yang huang breaking beam search curse study methods stopping criteria neural machine translation emnlp zhang kishore weinberger artzi bertscore evaluating text generation bert org
