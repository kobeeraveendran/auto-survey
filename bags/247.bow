controlling the amount of verbatim copying in abstractive summarization kaiqiang bingqing zhe liu fei liu computer science department university of central florida orlando fl usa robert bosch llc sunnyvale ca usa ucf edu bingqing wang zhe liu bosch com ucf edu v o n l c s c v v i x r a abstract an abstract must not change the meaning of the original text a single most effective way to achieve that is to increase the amount of copying while still allowing for text abstraction human editors can usually exercise control over copying sulting in summaries that are more extractive than abstractive or vice versa however it remains poorly understood whether modern neural abstractive summarizers can provide the same exibility i e learning from single reference summaries to generate multiple summary hypotheses with varying degrees of copying in this paper we present a neural summarization model that by learning from single human abstracts can duce a broad spectrum of summaries ranging from purely extractive to highly generative ones we frame the task of summarization as language modeling and exploit alternative mechanisms to generate summary hypotheses our method allows for control over copying during both training and coding stages of a neural summarization model through tensive experiments we illustrate the signicance of our posed method on controlling the amount of verbatim ing and achieve competitive results over strong baselines our analysis further reveals interesting and unobvious facts introduction an ideal summarizer should provide the exibility to ate summaries with varying proportions of reused text such summaries are required to cater to diverse usage scenarios e system abstracts may not contain excessive copied tent without proper consecutive words or longer are considered by eu standards as the author s lectual creation and it is thus protected by copyright law castilho et al without proper control over copying commercial summarizers can be held liable for copyright fringements moreover system abstracts with an ate amount of copied content are more desirable than highly abstractive ones as they are less likely to suffer from tent hallucination reiter and better at preserving the meaning of the original text to date it remains poorly understood whether modern stractive summmarization can provide the needed exibility to control over copying and generate diverse abstracts stractive summarizers using encoder decoder architectures can either copy words from the source text or generate new words unseen in the source see liu and manning chen and bansal gehrmann deng and rush recent work further attempted to increase the use of unseen words in summaries weber et al kryscinski et al however in all cases the summarizers are trained on single reference abstracts to produce single outputs with a xed corpus level copy rate it can take multiple reference abstracts created for the same input text with varying grees of copying to teach the system to generate abstracts with similar amounts of copying however not only can it be time consuming and costly to create human abstracts but this is unlikely to be how humans learn to exercise control over copying without an understanding of the copy nism of neural abstractive models producing abstracts with varying degrees of copying can prove daunting at best and a mission impossible at worst in this paper our goal is to generate abstractive summaries with varying amounts of reused text by developing a general framework that learns from single reference summaries we dene copy rate as the percentage of summary n grams pearing in the source text a high copy rate suggests that the summary is generated largely by copying verbatim from the source text conversely a low copy rate indicates there are more text shortening word reordering paraphrasing and straction involved in the generation process we argue that abstractive summarizers are not necessarily trained on every word of reference summaries but they ought to separate the prediction of summary words that are seen in the source text from those unseen the underlying principle is simple and intuitively appealing if a summarizer is trained to predict only seen words it learns to copy them from the source text producing extractive summaries as more unseen words are used for training the summarizer gradually transforms from copying only to both copying and generating new words not present in the source text by employing a mix and match strategy we enable an abstractive summarizer to generate summaries with more or less copying copyright association for the advancement of articial intelligence www aaai org all rights reserved we frame abstractive summarization as a language ing task and present a decoder only framework for it it uses question what is the most probable next word hint the word is seen in the source text a month old toddler who was reportedly abducted in pennsylvania has been found dead a district attorney said missing missing pennsylvania missing pennsylvania toddler missing pennsylvania toddler found reference summary missing pennsylvania toddler found dead question what is the most probable next word hint the word is unseen in the source text rescuers have suspended their search off the coast of santa cruz island for passengers who were trapped aboard the conception when the diving boat caught re and sank search search has search has been suspended search has been suspended in the search has been suspended in the dive boat re off reference summary search has been suspended in the dive boat re off california coast table formulating summarization as a language modeling task the rst model predicts only summary words that are seen in the source text the second model predicts only seen words our method provides exibility to control over copying by mix and matching the two types of behaviors the same transformer architecture vaswani et al to both encode the source text and decode the summary all network parameters are warm started using pretrained deep representations in contrast in a typical encoder decoder chitecture only parameters of the encoder and decoder can be warm started but not those of the attention copy nism khandelwal et al further our method allows for control over copying during both training and decoding stages of the neural model we experiment with varying portions of seen and unseen summary words in training to teach the summarizer to favor or not to favor copying at decoding time we compare different search strategies rst search vs beam search and reranking methods to courage system abstracts to use wording similar to the inal despite that only single reference summaries are able in benchmark evaluations we are able to evaluate mary quality along multiple dimensions using automatic metrics based on lexical similarity rouge lin and semantic similarity bertscore zhang et al and through human assessment of grammaticality ness and whether system abstracts remain true to original our method demonstrates strong performance either forming or performing on par with the best published results the research contributions are summarized as follows we introduce a new summarization method that provides the needed exibility to produce a spectrum of summaries for the same input and with a varying amount of copied content such summaries are highly desirable to cater to diverse real world our method emphasizes on in depth analysis of the copy behavior in summarization it frames abstractive rization as a language modeling task and exploits multiple strategies at training and decoding stages to generate verse summary hypotheses we show competitive results and demonstrate the effectiveness of the proposed method on exercising control over copying related work the signicance of controlling over the copying behavior in summarization should not be underestimated human tors often reuse the text in the original article to produce a summary jing and mckeown but they can adjust the degree of copying to produce a wide spectrum of summaries e human written summaries for newswire over and yen hermann et al meetings carletta and et al liu and liu scientic articles qazvinian et al and online forums ouyang chang and mckeown contain varying amounts of reused text moreover the degree of copying can have a direct impact on scores of tomatic evaluation metrics rouge was reported to favor summaries that use the same wording as the original ng and abrecht if reference summaries are made by ing system summaries with less copying and perhaps more abstraction compression and paraphrasing will be vantaged when compared against other system summaries with substantial copying there is thus an urgent need and this paper makes a rst attempt to present a tion framework that is capable of producing summaries with varying amounts of reused text to date various extractive and abstractive summarization techniques have been investigated nenkova and mckeown however rarely has one technique been utilized to produce both extractive and abstractive summaries for any given text extractive summarization selects important and non redundant sentences from the original the sentences can be optionally compressed to remove tial phrases leading to compressive summaries martins and smith li et al wang et al filippova et al durrett berg kirkpatrick and klein stractive summarization distills the source text into its tial meanings then performs language generation from the representation to produce an abstract barzilay and own liu et al liao lebanoff and liu hardy and vlachos these systems rarely provide the exibility for an end user to indicate the desired amount of reused text in the summary to eliminate the need to develop multiple systems for extractive and abstractive tion we attempt to introduce control into the copying ior of a neural abstractive summarization system neural abstractive summarization has demonstrated siderable recent success it often utilizes an encoder decoder architecture rush chopra and weston see liu and manning chen and bansal lebanoff song and make our implementation and models publicly available at com ucfnlp control over copying liu celikyilmaz et al and more recently ies have attempted to use deep contextualized tions such as bert devlin et al and elmo peters et al to give a further boost to it an encoder network converts the source text to a x length vector conditioned on which a decoder network unrolls the summary one word at a time while it is tempting to use pretrained deep sentations to warm start the encoder decoder khandelwal et al nd that results can be less satisfying as the attention weights are still not pretrained in this paper we adopts a decoder only framework dong et al where the same transformer architecture is used for both encoding the source text and decoding the summary copying can help produce unseen words it was originally introduced to the framework for neural machine translation gulcehre et al and later for abstractive summarization see liu and manning particularly knowles and koehn examine the inuence of text and sub words on the copying behavior of an nmt tem to suppress copying kryciski et al introduce a novelty metric which is to be optimized during policy ing and weber et al modify the scoring function of the summary sequence at decoding time fan grangier and auli attempt to control over summary length entities source style and portions but they do not address copying in this paper we focus on better understanding the copying behavior of a summarization system and present effective mechanisms to control the amount of reused text we cuss what it takes for a summarizer to copy a word without an explicit copying mechanism and how we may control the behavior to produce summaries with more or less copying in the following we describe our model in great detail our approach we frame abstractive summarization as a language modeling task and present a decoder only framework for it it uses the same transformer architecture vaswani et al to both encode the source text and decode the summary let v be a sequence of source tokens and y v be summary tokens our goal is to model the conditional probability distribution p j using a transformer inspired architecture we use byte pair encoding bpe sennrich et al for tokenization with a vocabulary size of kens bpe has been shown to improve the robustness and curacy of neural model training we use parameter tying lowing the same token embeddings to be used in both the put layer and nal softmax layer of the transformer model our method also includes three special tokens start end and mask which respectively denote the start end of a quence and a masked out token an illustration of our tem architecture is provided in figure training we construct the source sequence by prepending start and appending end to the input text e start abeth was taken to the hospital end illustrated in figure similarly the target sequence y is constructed by appending figure an illustration of our copytrans architecture the self attention mechanism allows i a source word to attend to lower level representations of all source words ing itself to build a higher level representation for it and ii a summary word to attend to all source words summary words prior to it as well as the token at the current position mask to build a higher level representation end to the summary e y elizabeth was hospitalized end our system learns to predict the target sequence one word at a time until the end token has been reached the conditional probability is shown in eq p p j p i however at training time we argue that the system is not necessarily trained to predict every word of target sequences but a selected collection might sufce using selected target tokens provides important potential to steer the system to be more extractive than abstractive or vice versa we divide all tokens in the sequence into three categories summary tokens seen in the source text summary tokens unseen in the source and c source tokens with the tation that training the system to predict only seen summary tokens may reinforce the copying behavior unseen tokens allow for generation and source words enable the system to learn better token representations by mix and matching get tokens from three categories we enable a summarizer to generate summaries with more or less copying we randomly sample a set of tokens from each category using a bernoulli distribution with probability the value of p varies by category and more analysis is provided in the experiments section let mi denote whether the i token of z is selected its probability is dened as p mi p a selected token is replaced by mask of the time meaning that the token has been masked out from the quence z for of the time it is replaced by a random startelizabethwastakentothehospitalelizabethwashospitalizedendendoriginalinputstartelizabethwasmasktothehospitalmaskwasmaskendendmaskedinputtransformerpredictedoutputtakenelizabethhospitalizeda missing worda seen wordan unseen wordsource textsummary token from the vocabulary v it remains unchanged for the nal in the following we use z to represent the masked sequence whose selected tokens are to be predicted during model training our loss term is dened as follows algorithm best first search procedure best m k input sequence model and beam size log p i it is important to note that we apply a binary mask to the self attention mechanism of the transformer architecture to allow a source token to attend to all source tokens ing itself and a summary token to attend to all source tokens summary tokens prior to it as well as the current token mask in order to learn deep contextualized sentations the formulation is similar to dong et al our binary mask is dened by eq it is a square matrix whose i th row represents the mask of the i token of z if it is a source token i the mask allows it to attend to all source tokens m att i j for j if it is a summary token i it can attend to all tokens prior to it as well as the current token m att i j for j m att i j if j otherwise the input of transformer consists of embedding matrices we wp and ws respectively denote the token position and segment embeddings devlin et al z p and s are one hot matrices used to retrieve embeddings for tokens in sequence z the token position and segment embeddings for the i token are then added up element wisely zwe pwp sws our transformer model takes as input embeddings and the binary mask m att to produce a sequence of deep contextualized representations ticularly hi is used to predict the i th missing token in the sequence we use parameter tying allowing the same token embeddings we to be used in both the input layer eq and nal softmax layer of the model eq h m att p e hi decoding given a trained model and an input text the decoding stage searches for a summary sequence that maximizes p we present two search algorithms for this stage search uses a priority heap to keep partial maries which are scored according to a heuristic function at each iteration the search algorithm takes the scoring partial summary extends it by one word then pushes new summary sequences back to the priority heap we erate k new summary sequences by selecting k words that give the highest probability of log p j eq then iteratively appending the words to the partial summary if the highest scoring summary in the heap concludes with an end of sentence symbol it is moved to a pool of pleted summaries for later reranking the heap thus keeps a init h init a reset while h is not is not full do the priority queue the answer collector current h pop if current ends with end then a continue candidates reset for each w v do extended current s mask candidates extended topk k argmin candidates h return a collection of partial summaries of varying lengths which are visited according to their scores we provide an illustration of our search algorithm in algorithm in contrast beam search is essentially search it maintains a beam of size at any time step containing partial summaries of the same length for each partial mary the algorithm extends it by one word producing k new sequences by appending each of the k words that give the highest probability of log p j to the partial mary this process generates a total of k k new summary sequences by extending on each of the k partial summaries the algorithm then selects k best candidates which are put in the beam for next iteration if a candidate summary cludes with the end of sentence symbol it is moved to the pool of completed summaries both search and beam search employ the same scoring function that scores a candidate summary by the sum of log likelihoods eq however the two differ in their search strategies beam search visits candidate summaries according to the summary length whereas search favors candidates attaining higher scores y arg max log p j yy end s t we compute p j using our trained copytrans model importantly the mask token is used as a prompt for the model to predict the next word e start beth was taken to the hospital end elizabeth was mask is a concatenation of the source text partial summary and mask token it is fed to the copytrans model where the contextualized representation of mask is used as input to size of the priority heap is capped at if the heap has reached capacity and a new summary sequence needs to be pushed in the lowest scoring one will be removed from the heap a softmax layer to predict the next token v in imental results we demonstrate that a dynamic ized representation of mask performs reliably at ing the next token this represents an important distinction from shifting the target sequence by one position for tion which is common in encoder decoder models reranking a reranking step is necessary in part because candidate summaries decoded using beam search or rst search do not always meet the length requirement e an overly short summary containing only two words is rarely an informative summary despite that it may give a high likelihood score below we compare three reranking gies to offset this limitation length normalization is adopted by see et al and it is frequently used in many other systems it divides the inal log likelihood score denoted as y log p by the total number of tokens in the summary to effectively prevent a long summary from being penalized bp norm introduces a brevity penalty to summaries that do not to meet length expectation as illustrated in eq bp norm performs length normalization then adds a penalty term log bp to the scoring function we modify the original penalty term of yang huang and ma to make it favor summaries using more copying in eq we dene r to be the copy rate i e the percentage of summary tokens seen in the source text scaled by a factor c when the copy rate r is set to the penalty is dropped to yang huang and ma provides a nice proof showing that this penalty term can directly translate to a coefcient multiplied to the log likelihood score eq log bp bp r exp y bp exp log p j bp p j soft bounded word reward sbwr is a newly introduced method by us that assigns a per word reward to the summary if the decoded summary is longer than expected i lpred the added words receive a diminishing reward of if the summary is shorter i lpred every word of it will receive a reward the method thus promotes summaries of similar length to the predicted lpred a sigmoid function is used to smooth the reward values r is a coefcient to scale the total reward and it is tuned on the validation data y y r i we obtain the predicted length lpred using greedy search then empirically offset the predicted length by three words according to validation set in all cases we force the decoder source text premier chang chun hsiung said thursday he is enraged and saddened by the snail paced progress of the reconstruction of areas hardest hit by a disastrous earthquake that rattled taiwan on sept summary premier expresses condolences for taiwan quake victims premier angry over reconstruction of quake hit areas premier enraged and saddened by earthquake reconstruction premier enraged by slow progress of post quake reconstruction source text a blue ribbon panel of experts said on wednesday that german economic growth will grind to a halt next year raising doubts about berlin s plans to shield europe s biggest economy from the global turmoil summary german experts raise doubts about economic recovery experts say german growth will grind to a halt next year german experts to grind to halt next year german economy will grind to halt in say experts table example system summaries produced by generator networks our method best abstract our method pure extract and human abstract to never output the same trigram more than once during ing which is a common practice to avoid repetitions paulus xiong and socher experiments data and evaluation metrics we evaluate our proposed method on the sentence rization task the goal is to condense a lengthy source tence to a title like summary comparing to single document summarization sentence summarization deals less with tent selection its ground truth summaries also contain more paraphrasing and abstraction we conduct experiments on the gigaword parker and newsroom grusky man and artzi datasets gigaword articles were lected during and newsroom spans the range of we pair the rst sentence of each article with its title to form an instance the train valid test splits contain instances for gigaword and instances for newsroom we experiment with both datasets to understand not only the copying behavior but also domain adaptation effects for various models despite that only gle reference summaries are available in benchmark tions we are able to evaluate summary quality along tiple dimensions using automatic metrics based on cal similarity rouge lin and semantic similarity bertscore zhang et al and through human sessment of grammaticality informativeness and whether system abstracts remain true to original experimental settings we initialize the model parameters using pretrained base uncased model the model is ne tuned on the ing split of the gigaword or newsroom dataset for stractive summarization our model uses a layer training loss a b d e g h gigaword gram gram gram gram average newsroom gram gram gram gram average table the copy rate of various summarization models we dene copy rate as the percentage of summary n grams appearing in the source text where as well as an average of them we experiment with selecting varying amounts of seen summary tokens unseen summary tokens and source tokens for training a circle corresponds to about million tokens for gigaword and tokens for newsroom which are used to compute the loss term system multi task entailment seass drgd pg networks biset search beam search r l bert s table summarization results on the gigaword test set the lower part of the table contains results from our system former architecture its hidden state size is and has attention heads we use the adam optimizer with the learning rate is set to and it is halved whenever the validation loss does not change after training steps we set the weight decay to be for regular layers and no weight decay for dropout and normalization the sampling rate p is set to for source words and for summary words both seen and unseen each model is ne tuned for epochs an epoch takes about hours on a tesla gpu our batch size is set to be summarization results control over copying could we bias a summarizer to duce summaries that are more extractive than abstractive or vice versa if the summarizer is trained solely on mary words seen in the source text will it only learn to copy words during testing but not generate new words we seek to answer these questions in this section particularly we vide all tokens selected for training into three categories summary tokens seen in the source text summary tokens unseen in the source and c source tokens with the tation that training the system to predict only seen summary tokens may reinforce the copying behavior unseen tokens allow for generation and source words enable the system to learn richer representations by mix and matching tokens we enable a summarizer to copy more or less we analyze the copy rate of various summarization els in table copy rate is dened as the percentage of mary n grams appearing in the source text we set and the average of them a high copy rate suggests that the summary is generated largely by copying verbatim from the source text we experiment with selecting varying amounts of seen summary tokens unseen summary tokens and source tokens for training where the number of circles is proportional to the number of tokens used in puting the loss term all summaries in table are decoded using beam search without reranking our ndings suggest that the factor that makes the most impact on the copying behavior of a summarizer is the portion of seen and unseen summary words used for ing the model if the summarizer is trained on purely seen words case a in table it only reuses source words ing testing despite that there is nothing to prevent the tem from generating new words the gram copy rate for case a is about for both datasets with the minor gap due to tokenization discrepancies as more unseen words are used for training the summarizer gradually transforms from copying only to both copying and generating new words not present in the source text we observe that the ratio of seen vs unseen words in ground truth summaries is about in both datasets and newsroom is slightly more tractive than gigaword our analysis reveals that it is portant to maintain a similar ratio during training in order to achieve high rouge scores pure extracts do not attain high rouge scores as ground truth summaries themselves are abstracts our analysis further suggests that training on source words has little impact on the copying behavior of the system but it improves representation learning and has lead to consistently improved f scores system comparison table shows results on mark summarization data containing testing instances from gigaword we contrast our system with system m pg networks o o r s ours pure ext w e ours best abs n a ours pure ext g i ours best g r l bert s system human pg networks biset ours pure ext ours best inform gramm truthful bst wst table summarization results on the newsroom test set the top four systems are trained on newsroom training data whereas the bottom two systems are trained on gigaword table human assessment of informativeness cality truthfulness and best worst scaling tion baselines developed in recent years they include nallapati et al multi task entailment sunuru and bansal seass zhou et al drgd li et al guo sunuru and bansal pg networks see liu and ning song zhao and liu cao et al and biset wang quan and wang output summaries from the last four tems are graciously provided to us by the authors we ate summary quality using two automatic metrics including lin that measures n gram overlap between system and reference summaries and bertscore zhang et al that quanties their semantic similarity using bert based contextualized representations results show that our system achieves competitive mance surpassing strong systems having reported results on this dataset as judged by both metrics these results strate the effectiveness of our transformer based only architecture for abstractive summarization we observe that using beam search with reranking yields the highest sults using case g in table for training both bp norm and sbwr appear to be outstanding reranking methods ter than length normalization our observation also suggests that search and beam search can produce similar outcome despite that the two differ in their search gies with beam search visiting candidates according to mary length and search favoring candidates having high log likelihood scores we suggest future work to plore other search methods such as a search domain adaptation we investigate the effect of domain adaptation by training the model on gigaword then testing it on newsroom test set results are reported in table not surprisingly there is a performance degradation when ing the model in a cross domain setting we observe that the model with more copying pure extract case e seem to degrade more gracefully than its counterpart best abstract case with a smaller performance gap in cross domain tings both of our models perform competitively comparing to other baseline methods human evaluation to thoroughly analyze the quality of summaries we ask man annotators to assess system outputs along three sions including informativeness has the summary covered options important content of the source text grammaticality is the summary sentence grammatically correct and ness has the summary successfully preserved the meaning of the original text both system and human summaries are scored according to these criteria using a likert scale from worst to best we compare variants of our method erating a pure extracts case e and best abstracts case g baselines of c pg networks d e biset and human abstracts following liu and lapata we perform best worst scaling where a human selects the best and worst summary among six candidates the nal ing of the system is computed as the percentage of times it was selected as the best minus that of the worst we ple instances from the gigaword test set for evaluation each instance was assessed by ve human evaluators from amazon mechnical turk where low quality annotations are manually removed the results are presented in table we observe that human summaries article titles are imperfect they can contain details that are nonexistent in the source see table although they provide a means for researchers to train neural models without re annotating reference maries in contrast both of our systems perform slightly but consistently better than other baselines conclusion in this paper we present a transformer based decoder only framework to generate summaries with more or less ing the proposed method can be used to generate both tractive and abstractive summaries our method emphasizes on in depth analysis of the copy behavior in summarization it exploits multiple strategies at training and decoding stages to generate diverse summary hypotheses we show itive results and demonstrate the effectiveness of the posed method on exercising control over copying acknowledgments we are grateful to the reviewers for their helpful comments the work was performed in part while kaiqiang song was an intern at bosch research this research was supported in part by the national science foundation grant references barzilay r and mckeown k r sentence fusion for tidocument news summarization computational linguistics cao z li w li s and wei f retrieve rerank and rewrite soft template based neural summarization in acl carletta j and al the ami meeting corpus in mlmi celikyilmaz a bosselut a he x and choi y deep communicating agents for abstractive summarization in naacl chen y and bansal m fast abstractive summarization with reinforce selected sentence rewriting in acl de castilho r e dore g margoni t labropoulou p and gurevych i a legal perspective on training models for natural language processing in proc of lrec devlin j chang m lee k and toutanova k bert pre training of deep bidirectional transformers for language standing in proc of naacl dong l yang n wang w wei f liu x wang y gao j zhou m and hon h unied language model pre training for natural language understanding and generation org durrett g berg kirkpatrick t and klein d based single document summarization with compression and anaphoricity constraints in proc of acl fan a grangier d and auli m controllable abstractive summarization in the workshop on nmt and generation filippova k alfonseca e colmenares c kaiser l and vinyals o sentence compression by deletion with lstms in proc of emnlp gehrmann s deng y and rush a m bottom up stractive summarization in proc of emnlp grusky m naaman m and artzi y newsroom a dataset of million summaries with diverse extractive strategies in proc of naacl gulcehre c ahn s nallapati r zhou b and bengio y pointing the unknown words in proc of acl guo h pasunuru r and bansal m soft layer specic multi task summarization with entailment and question generation in proc of acl hardy h and vlachos a guided neural language ation for abstractive summarization using abstract meaning sentation in proc of emnlp hermann k m kocisky t grefenstette e espeholt l kay w suleyman m and blunsom p teaching machines to read and comprehend in proc of nips jing h and mckeown k the decomposition of written summary sentences in proc of sigir khandelwal u clark k jurafsky d and kaiser l ple efcient text summarization using a single pre trained former org knowles r and koehn p context and copying in neural machine translation in proc of emnlp kryscinski w paulus r xiong c and socher r proving abstraction in text summarization in emnlp lebanoff l song k and liu f adapting the neural encoder decoder framework from single to multi document marization in emnlp li c liu f weng f and liu y document tion via guided sentence compression in emnlp li p lam w bing l and wang z deep recurrent generative decoder for abstractive text summarization in emnlp liao k lebanoff l and liu f abstract meaning sentation for multi document summarization in coling lin c rouge a package for automatic evaluation of summaries in wksp on text summarization branches out liu y and lapata m hierarchical transformers for document summarization in acl liu f and liu y towards abstractive speech tion exploring unsupervised and supervised approaches for ken utterance compression ieee trans aslp liu f flanigan j thomson s sadeh n and smith n a toward abstractive summarization using semantic tations in proc of naacl martins a f t and smith n a summarization with a joint model for sentence extraction and compression in workshop on integer linear programming for natural language processing nallapati r zhou b dos santos c gulcehre c and xiang b abstractive text summarization using sequence to sequence rnns and beyond in proc of signll nenkova a and mckeown k automatic summarization foundations and trends in information retrieval ng j and abrecht v better summarization evaluation with word embeddings for rouge in emnlp ouyang j chang s and mckeown k crowd sourced iterative annotation for narrative summarization corpora in eacl over p and yen j an introduction to nist parker r english gigaword fth edition philadelphia linguistic data consortium pasunuru r and bansal m multi reward reinforced marization with saliency and entailment in naacl paulus r xiong c and socher r a deep reinforced model for abstractive summarization in proc of emnlp peters m e neumann m iyyer m gardner m clark c lee k and zettlemoyer l deep contextualized word representations in proc of naacl qazvinian v radev d r mohammad s m dorr b zajic d whidby m and moon t generating extractive maries of scientic paradigms jair reiter e a structured review of the validity of bleu computational linguistics rush a m chopra s and weston j a neural attention model for sentence summarization in emnlp see a liu p j and manning c d get to the point summarization with pointer generator networks in proc of acl song k zhao l and liu f structure infused copy anisms for abstractive summarization in proc of coling vaswani a shazeer n parmar n uszkoreit j jones l gomez a n kaiser l and polosukhin i attention is all you need in proc of nips wang l raghavan h castelli v florian r and cardie c a sentence compression based framework to query focused multi document summarization in acl wang k quan x and wang r biset bi directional selective encoding with template for abstractive summarization in proc of acl weber n shekhar l balasubramanian n and cho k controlling decoding for more abstractive summaries with based networks org yang y huang l and ma m breaking the beam search curse a study of methods and stopping criteria for neural machine translation in emnlp zhang t kishore v wu f weinberger k q and artzi y in bertscore evaluating text generation with bert org
