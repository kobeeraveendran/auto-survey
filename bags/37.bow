the links have it infobox generation by summarization over linked entities kezun zhang yanghua xiao hanghang tong haixun wang wei wang shawyh edu cn ccny cuny edu com school of computer science shanghai key laboratory of data science fudan university shanghai china city college cuny ny usa google research usa about an entity many wikipedia articles contain structured formation such as table image text citation all of which are the targets of information extraction more importantly many entities are associated with an infobox which consists of a set of property value pairs about the entities as an example figure shows the wikipedia article about steve jobs with an infobox on the right side wherein the rst property is born and its value is steven paul jobs february san francisco california us such structured information is the core ing block behind many applications including search engines for answering user questions about these entities n u j r i s c v v i x r a abstract online encyclopedia such as wikipedia has become one of the best sources of knowledge much eort has been devoted to panding and enriching the structured data by automatic tion extraction from unstructured text in wikipedia although remarkable progresses have been made their eectiveness and ciency is still limited as they try to tackle an extremely dicult natural language understanding problems and heavily relies on supervised learning approaches which require large amount eort to label the training data in this paper instead of performing information extraction over unstructured natural language text directly we focus on a rich set of semi structured data in wikipedia articles linked entities the idea of this paper is the following if we can summarize the relationship between the entity and its linked entities we ately harvest some of the most important information about the entity to this end we propose a novel rank aggregation approach to remove noise an eective clustering and labeling algorithm to extract knowledge we conduct extensive experiments to strate the eectiveness and eciency of the proposed solutions ultimately we enrich wikipedia with million new facts by our approach keywords labeling knowledge extraction rank aggregation clustering cluster introduction online encyclopedia has become one of the best sources of knowledge a typical example is which contains million articles for english language and covers a wide range of human knowledge another fast growing online encyclopedia is which contains million entities and is the largest knowledge base in chinese wikipedia and baidubaike are nized in similar ways and have become the caliber of other online encyclopedias in this paper we focus on these two encyclopedias for information extraction among others one critical reason that makes online pedias extremely valuable is that part of their data is structured and hence machine processible usually a wikipedia article is wikipedia org baike baidu permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specic permission a fee copyright acm acm xxxxx xx x xx xx figure fragment of steve jobs in wikipedia despite much eort to enrich structured data the current fobox in wikipedia is often incomplete and inconsistent this is mainly due to the fact that most infobox is generated by human editing which is not just labor intensive but also error prone to be specic about wikipedia articles do not have infobox these are not only those less popular articles but also new cles for articles that have infobox the information in the infobox is often incomplete some important ties may be missing and values of certain properties may be incomplete information in infoboxs is often inconsistent across dierent articles and entities for example the property place of birth in some infoboxes is also expressed as birthplace in other infoboxes the property value america and united states us america refer to the same country in order to address the drawbacks of human editing recently extensive eort has focused on expanding and enriching the tured data by automatic information extraction from tured text in wikipedia although remarkable gresses have been made their eectiveness and scalability are still somewhat limited mainly for the two reasons first these ods rely on several natural language understanding tasks e named entity recognition dependency parsing and relationship extraction which themselves are extremely challenging and ror prone second many of the existing approaches are costly since they are essentially supervised learning methods and hence require large amount of labeled training examples in this paper we propose an alternative approach for enriching instead of performing information extraction structured data over unstructured natural language text directly we focus on a rich set of semi structured data in wikipedia articles linked tities a wikipedia article typically consists of many links to other wikipedia articles intuitively the author of the article in describing a wikipedia entity refers the reader to many other tities that are important or related to the entity the key idea of this paper is the following if we can summarize the relationship between the entity and its linked entities then we immediately harvest some of the most important information about the entity table entities toy story cars film brave film intel dell apple inc blood pressure the public theater apple i apple lisa maria shriver lev grossman table knowledge property pixar animated films electronic companies american writers apple inc hardware value toy story cars film brave film dell intel apple inc maria shriver lev grossman apple i apple lisa let us use the example in figure to illustrate the intuition of our approach table lists some linked entities in the wikipedia article of steve jobs which cover a variety of dierent aspects of the article entity steve jobs if we further convert these linked entities into something shown in table where we assign a erty label to a linked entity or a set of linked entities the result provides a comprehensive structured summary of the entity steve jobs in order to fulll this basic idea there are the following lenges we need to address as follows how to accurately summarize linked entities in order to convert the unstructured linked entities list in table to structured property value pairs in table we need to group the similar linked entities i e values together as well as assign a label i e property for each group here our key tion is that it is relatively easier to summarize a group of ties than an individual because the group members disambiguate each other we thus propose a cluster then label approach we divide linked entities into dierent semantic groups and then give each group a semantic label a property more specically we propose a g means based clustering algorithm to cluster the linked entities into dierent semantic groups in the steve jobs example we obtain four clusters we further propose a label generating algorithm to generate a label for each group each labeled group is eventually a candidate property value pair for the infobox how to remove unrelated linked entities although most linked entities are semantically related to the article entity some might have weak or no semantic relevance to the article entity take the steve jobs example again we can see that some linked entities e blood pressure and the public theater are not related to steve jobs to remove these irrelevant linked entities we propose a novel ranking aggregation approach that integrates dierent ranking mechanisms to detect noisy linked entities contributions in summary this paper proposes an tive radically dierent approach for infobox generalization for online encyclopedia by focusing on linked entities we bypass including all the diculties posed by the existing approaches the challenging nlp tasks manual labeling and human editing more specially the main contributions of the paper are three fold first to extract knowledge from the linked entities we propose an eective clustering and labeling algorithm second we pose a novel rank aggregation approach to detect and remove noisy linked entities for wiki articles third we conduct sive experimental evaluations to show that our method generates comprehensive infobox with better quality the rest of the paper is organized as follows in section we give a detailed description of handling noisy linked entities in section we introduce the cluster and label algorithm datasets and experiments are described in section in section we duce some related works in section we conclude our paper remove noisy linked entities in this section we show how we remove noisy linked entities we rst show that the noisy entities are nontrivial problem in online encyclopedias by empirical studies then we propose a novel ranking aggregation approach to identify the noisy linked entities empirical studies in typical online encyclopedias some linked entities have weak relevance to the article entity these entities become noises for the understanding of the semantic of the article entity for ample steve jobs in wikipedia also has links to blood pressure the public theater each of which obviously has a weak tionship to steve jobs they are linked just because they have a corresponding entry in the knowledge base we need to identify and remove them next we design an experiment to show that noisy entities are not trivial phenomenon that is most articles have noisy linked entities for each article in wikipedia or baidubaike we calculate a semantic distance between the article and each of its linked entity in our study we use google distance inspired distance which is dened as t where b represents linked entities of article b and w represents entire articles in wikipedia we regard the linked entity as noise if the distance is larger than threshold we summarize the cumulative distribution of the percentage of noisy links and the results on wikipedia and baidubaike are shown in figure we found that in wikipedia nearly of articles have noisy linked entities only articles have no noisy entities and articles have more than noisy linked tities in baidubaike nearly articles have noisy links articles have no noisy entities and articles have more than noisy linked entities the existence of the noisy linked tities makes it dicult to accurately understand the semantic understanding of entities noisy links distribution in wikipedia noisy links distribution in baidubaike y t i t n e o g a t n e c r e p y t i t n e o g a t n e c r e p percentage of noise wikipedia percentage of noise baidubaike figure noisy linked entities distribution in wikipedia and baidubaike position aware ranking aggregation the basic idea to remove noisy linked entities is to rank all linked entities by their semantic relatedness to the article tity and then remove the semantically unrelated entities thus ranking the semantic relatedness becomes a key issue there are many individual ranking schemes of semantic relatedness ever in general each individual ranking can only characterize the a specic aspect of the semantic relatedness thus an aggregated ranking is necessary for the accurate identication of non related entities many existing ranking aggregation approaches have been proposed however most of them assume the uniform quality tribution of the ranking that is the ranking results have the same quality for any two elements in the ordering however we found that the individual ranking we used in this paper has a non uniform quality distribution which motivates us to propose a position aware ranking aggregation approach preliminaries we rst formalize the preliminary concepts a ranking ri can be considered as a linear ordering on the linked entities this means that given the linked entities set u with n elements ri is an one to one mapping from u to n we always assume that elements of higher or topper rankings have smaller value the quality function qr of the ranking r is dened as a function q n measures our belief on the fact that the i th element under the ranking r owns the ranking position hence qr is a function of the position of the ranking suppose we are given two rankings such that their quality function have opposite monotonicity that is i increases and i decreases with i thus or n quantify the quality of e ranking or we refer to them as the credit of e in the ranking the smaller or n is the more credit that e owns in or metrics our aggregated ranking is developed upon two wildly used sures co occurrence based metric and overlap coecient this subsection elaborates these two measures co occurrence entities may co occur in a common page as linked entities if two entities always co occur in a page they are more likely semantically related for example milk and bread always co occur in pages describing food and hence they are evant in semantic we use pmi pointwise mutual information to measure the degree of the co occurrence for a pair of entities p m i of entity and entity y is dened as non uniform quality distribution of rankings we have two ndings about these two rankings first the quality of an element under each ranking varies with its position in the the ranking that is to say both qw j c i and qp m i i depends on i second qw j c i and qp m i i have opposite monotonicity in our case we found that p m i is good at identifying the noisy entities but w jc is good at discovering the strongly related entities these ndings imply that the we should develop position aware aggregation approaches we give an example about entity apple inc to justify the above two ndings more support will be found in the ment sections we compare the ranked list of w jc and p m i as well as the aggregated measure that will be propped in the following text in table we can see that w jc can recognize the strongly related entities and p m i can correctly nd the noisy linked entities but w jc regard related entities as noises e iphone and p m i regard the unrelated entity e software in contrast our ranking aggregation update as related entity method take advantage of both two individual measures has less false positive and false negative results wjc macintosh table dierent ranking strategies for apple inc aggregation apple worldwide velopers conference os x mountain lion steve jobs apple tv magic mouse fortune magazine apple inc advertising pmi apple battery charger steve jobs os x apple worldwide velopers conference os x mountain lion apple time capsule business model greenpeace tional iphone software update ireland cork city chancellor of the chequer india macbook pro cork city video calling broadway books greenpeace tional p m y log y position aware ranking aggregation our new ranking aggregation is based on the linear tion given two rankings on u the generic linear tion dene the combined ranking as where y is the probability that and y co occur in the same entity page or is the probability that entity or y occurs in all co occurrence pairs p m i is zero when and y are independent and maximizes when and y are perfectly lated i e when y equals to or compared to the direct co occurrence number p m i evaluate their relatedness by statistical independence which penalizes the independent pairs with high co occurrence number overlap coefcient entities may share some common linked entities a pair of entities has a larger overlap of linked entities is intuitively more relevant in semantic for example the related entity pair milk and bread share a large number of common linked entities like food and drinks we use the weighted jaccard coecient wjc to quantify the overlap ratio for an entity pair for two entities and y w jc is dened as where nx is the linked entities of here is used as the weight of e dened as w y penxny penxny idf e log n for any e u where is used to control the preference to dierent rankings in the naive linear combination is a static constant that is we use the same for any e u however previous observation implies that the preference to dierent rankings is dependent on the position of the entity under dierent rankings hence in our new ranking aggregation we regard as a function of and so that it can express the best preference to rankings for dierent entities specically we dene as where is a parameter used to control the speed that the curve approaches to the climax based on we dene our new scoring function of e when we have the new scoring function as n where n is total number of articles and represents the number of articles containing a link to entity e compared to the naive jaccard w jc use the idf e as the weight to suppress the general entities like jaccard coecient the higher the wjc is the more related the entity pair is if all entities have the same weight w jc will degrade into the naive jaccard coecient it is easy to check that n given the new score values of linked entities we rst normalize them we use some articles as training data and label their linked entities as related or unrelated we build a binary classication model and draw its roc curve nding that is the best threshold to distinguish unrelated linked entities from others we use this threshold for all the other articles s e i t i n e o g a t n e c r e e a h p a l ratio ratio of entities as a function of ratio figure ratio and alpha rationality next we show how we derive the new ranking given two rankings with oppositely monotonic quality functions the aggregated ranking should bias towards to the one with higher quality specically for any entity e we evaluate according to there are three specic cases case in this case e owns lar credit in and hence and should be assigned a similar weight close to case in this case e owns more credits in than in hence should bias toward that means the weight of should be larger than in the linear combination case it is the reverse case of case in this case should bias toward clearly a sigmoid function can express the desired relationship between and the ratio specically we use the most widely used logistic function which is dened as where is a parameter used to control the speed that the curve approaches to the climax furthermore if we replace the ratio by the its log ratio all the requirement in the three cases can be satised the log ratio is dened as ln n substituting with the the log ratio we have the as dened in eq selection of we use linked entities of shanghai ple inc steve jobs china new york city barack obama as samples for each of these linked entity we calculate its ratio function value we use pmi and wjc as and respectively we plot their distribution of ratio in figure from the tribution we can see that most ratio values lie in the range of which hosts of all sampled linked entities we also give the simulation of as a function of see eq with set as dierent values in figure the simulation shows that the larger is the sharper increase happens around the simulation also reveals that when the range of ratio in which a signicant can be derived almost overlaps with the real range observed from the samples hence typically we set clustering and labeling after removing the noisy linked entities we keep only the mantically related linked entities next we use a g means based clustering approach to divide them into dierent semantic groups then we label each group with an appropriate property name in this way we discover a new property and its value for an entity from its linked entities distance metric is key for a clustering algorithm hence we rst elaborate the distance metric feature selection and distance metric to dene the distance metric we rst need to identify the fective features to characterize the objects to be clustered here we use category information of entities for the clustering in wikipedia or baidubaike an entity is usually assigned one or more categories by editors a category is widely used to sent the concept of an entity hence if a pair of entities has the similar categories they probably belong to same concept or main topic category or concepts information has been shown to be eective for the document clustering and topic cation which motivates us to use categories to construct the feature vector for the entity problem statement the naive solution is using direct gories of entities as the features let fe be the feature set for entity e in the naive solution fe contains all the direct egories of e let n be the number of all categories in wiki we dene a n dimensional feature vector for each entity i e where measures the signicance that concept ci characterizes e in general when ci is not in fe otherwise is dened by a certain measurement such as tf idf functions we will elaborate it in later texts given two feature vectors of two entities a their distance is dened by the cosine distance fb kfak kfbk however using the direct categories for the distance metrics has the following two weaknesses first many categories are not hypernyms of the entity some categories express the semantics other than isa lationship for example steve jobs has category can buddhists which is an isa relationship but it also has births a property apple inc works for ship and many other semantics other than isa in general it is hard to use these non isa categories to characterize the concept of an entity second many direct categories are quite specic we late the frequency of all categories in wikipedia we found that among the most frequent categories is in the form of year of birth or year of death ous these are specic categories that characterize a specic property of the entity in general the more specic the egory is the less possible two semantically close concepts can be matched in terms of the category for example in category graph shows in figure apple inc can only match with mosys an ip rich fabless semiconductor pany in term of a more abstract category technology panies instead of the specic one electronics companies weight of c wc idf c as in eq to eq mark c as a feature of e corresponding weight is wc algorithm feature selection and weighting algorithm require entity e set of concept category pair c ensure feature and corresponding weight of e isa taxonomy graph g isa ae reachable categories from e in g for c in ae do end for return function isa end for return g end function g isa taxonomy graph a directed graph a threshold parameter for each concept category in c do weight of category for concept is calculated by eq add an edge concept category to g if weight isa taxonomy construction to overcome the above nesses we need to extend the feature set from the direct egories to high level categories described in algorithm we may recursively use the categories of the categories for the pansion however the extension is not trivial because we need to ensure the expanded category can characterize the entity curately that is to say we expect to improve the recall without sacricing the precision for this purpose we generally need a certain constraints on the extension to ensure the accuracy a general constraint is to only select the categories that are nyms of the entity because a hypernym is a concept of the entity which is a natural interpretation of the entity thus the problem is reduced to identication of a category that is a hypernym of an entity we dene a scoring function to characterize the condence on category c being a hypernym of entity e the denition of depends on the hierarchal structure of the hypernyms of e for each entity we can construct a quality hierarchical taxonomy just according to the wiki gories the taxonomy for entity e denoted by ee we is a direct acyclic graph with each edge assigned a weight w which reects our condence on the fact that is a hypernym of algorithm to construct the taxonomy given a threshold parameter we construct the isa taxonomy ee we for an entity or category e by a level wise solution let c e suppose we have nished the i th level i starts from the i th level is as follows for each category of any element say in ci such that we add the direct edge from to into ee and use as the edge weight and add into c and ve if cj these newly added categories constitute we add the direct edge from to into ee and use as the edge weight the procedure is repeated until no more valid category can be found it is easy to prove that ge is a direct acrylic graph figure category graph in wikipedia scoring functions next we dene an observation is that many real hypernyms contains many frequent occurring words among the categories of the entities this inspiration plies that we can use the word frequency to dene cally for an entity or category e and its categories in wiki we rst score the words in hypernym c for e let s be the number of categories in that contains word s we have s let kc be the number of unique words in c the condence that the category c is an appropriate hypernym of e is dened as kc x wc let pec be the set of all the paths from e to c and pec be one of such path now we are ready to dene the condence score for any category c in ge as a hypernym for e max pecpec y ci cj pec the score is dened as the maximal accumulative product of the edge weight over all paths connecting e to c the larger the maximal produce the more possible the concept is a hypernym of the entity we give example to illustrate our scoring functions example scoring function consider apple inc its direct categories in wikipedia are electronics companies home computer hardware companies electronics companies of the united states computer companies of the united states steve jobs apple inc establishments in california the most frequent words in the categories are companies electronics computer united states thus the categories containing these words are likely hypernyms of apple inc such as electronics companies of the united states electronics companies but steve jobs will be dropped in our approach since it contains less frequent words in the construction of the isa taxonomy for the apple entity some high level categories such as technology companies will be covered consequently many indirect category will be used to characterize an entity improved distance metric finally we are ready to dene our improved distance metric which share the same expression as eq but with two improvements first fe is extend into ve e that is all categories in ge except e itself will be used as features second is dened according to we use the tf idf framework to dene we rst dene the idf of a concept c i e idf c as idf c log n where n is the total number of entities in the wiki and is the number of entities whose isa taxonomy contains c thus the nal weight of each feature is idf c to see the eectiveness of the above measurement we rank the categories of entity apple inc by in table we can see that most categories of higher rank can characterize the entity accurately and expressively table category ranking for apple inc computer companies of the united states electronics companies technology companies of the united states networking hardware companies retail companies of the united states home computer hardware companies steve jobs apple inc warrants issued in hong kong stock exchange clustering algorithm we may directly use k means approach as the basic framework for clustering given the distance metric but in our case the naive k means leads to bad results due to the following reasons first in naive k means the parameter k is specied by users which is impossible when millions of entity clustering tasks need to be executed second the naive k means randomly selection initial ters the selection of initial center is inuential on the nal results a smart selection strategy is expected to obtain a better clustering result to solve these problems we propose a new clustering approach the basic idea is using statistical test proposed in g means to guide the selection of best k and using a dynamically center selection strategy proposed in k to determine the best initial central points our clustering algorithm is described in algorithm the algorithm accepts the set of data points x as the input and return k clusters the algorithm recursively bi partition the data until the stop criteria is reached the bi partition procedure consists of three major steps in the rst step we select two data points g as the initial centers by k k is smarter than the random generation of two cluster centers it lows the principle that the probability of a datapoint to be center should be proportional to the distance from the ready selected centers following the idea we rst choose a datapoint uniformly at random from the group x then we select another datapoint from the group with probability pxx where represents distance between and in the second step we run k means on data points in g with k and the initial center as after the means reaches to the convergence state or gets maximal iteration we get two clusters and their new centers in the third step project datapoint in g onto vector i and let z be which the cumulative distribution of d i finally we test whether the anderson darling statistic value lies in the range of non critical values at signicance level if true keep the original group and abandon the splitting otherwise replace the group with two subclusters and continue bi partition them until no new clusters emerging algorithm g means clustering algorithm require datapoints x signicance level ensure k clusters k g x clusters bi return clusters function bi select two datapints from group g by k run k means with k and the initial center as let be the two clusters and be the if gaussiant then if datapoints in g sponding two cluster centers follow gaussian distribution return g else end function end if k k return bip for example we remove the noisy entities for apple inc in wikipedia and cluster them in above algorithm clusters show in table labeling the cluster next we assign a semantic label for each group in this way we explain why group of linked entities are linked to the article entity the semantic label as well as the group of entities thus becomes a property of the target entity and its corresponding value this information is a good supplement of the current infobox for example a cluster which contains google maps ios ibooks xsan itunes if we assign the semantic label ios software for the cluster we successfully enrich the infobox of apple inc with a software that is the problem of cluster labeling some researches have already conducted on cluster labeling a popular method for labeling cluster is applying the statistic technologies to select quency features that is identifying the most common terms from the text that best represent the cluster topic but the frequent terms may not convey meaningful message of the cluster because some popular terms are also frequently occur in other clusters as a result an appropriate cluster label should characterize the common topic of entities in each cluster and simultaneously informative a good cluster label should satisfy two requirements completeness it should cover most entities in the ter e g for rst cluster in table label tunisian jewish descent only cover one entity in the cluster so we want a wildly covered label which can represents the group rectly informativeness we hope the label is the most specic label while covering all entities in the cluster e in rst cluster people by status covers all entities in the cluster but it is not informative the completeness and the informativeness are contradicted to each other in general the more abstract a label the more entities that it can cover some improvements have been done to generate a meaningful label inverse frequent term takes both frequency and weight of a term into consideration a meaningful label for a cluster is a term with maximal inverse frequency baseline labeling strategies we rst give two naive methods to label clusters however the naive solution in general has one or more weakness which motivates us to a least common ancestor lca based solution in the previous subsection we have built the isa taxonomy graph ge for each entity e all categories in ge will be used for the labeling given a cluster x ek let c be the union of each vei we have two baseline labeling strategies most frequent category mf for short the direct solution is labeling the cluster using the most popular egory let tf c be the number of ge such that ve for all entities in the cluster thus mf selection strategy is arg max cc tf c most frequent yet informative category mfi for short apparently mf tend to select popular concept and most popular concept are abstract concept thus the formativeness is sacriced to avoid this we take the idf like factor into account formally mfi selection strategy is arg max cc tf c idf c idf c is dened by eq however the above labeling methods have the following ness mf tends to select general with good completeness but less informative label mfi can recognize specic labels but in many cases maybe over specic because some specic concepts own a large idf weight next we propose a least common ancestor model to handle the tricky tradeo between the informativeness and complexness lca based solution the lca model is dened on the isa taxonomy graph for the cluster x to be labeled given a cluster x ek we rst construct the isa taxonomy graph for x gx we dene gx as the union of all isa taxonomy graph ge such that e x here we ignore the weight of gx thus the union of two isa taxonomy graphs and is the graph e with v and e obviously gx is a dag we can also dene g as the union of all isa taxonomy ge for each entity e definition isa taxonomy graph for cluster x the isa taxonomy graph for cluster x gx is the union of all isa taxonomy graph ge for each e x proposition for a set of entities x gx is a directed acrylic graph problem model given gx nding a best cluster label for x thus is reduced to the problem of nding a least common ancestor of x from gx given two nodes u in g if u has a path to v then v is ancestor of u for a set of entities x a lca in gx is an ancestor of all entities in x which has no descendant that is an ancestor of entities in x the direct lca model clearly can ensure we nd a general enough concept to cover all entities however the model may rice the informativeness hence we need a more exible model allowing us to control the tradeo between informativeness and completeness we introduce a coverage restraint into lca to tune the tradeo between coverage and informativeness note that there may exist more than one lca we use idf function dened in eq to help select the best lca we propose maximal lca to reect all these requisites problem definition maximall lca given an isa taxonomy graph gx for the entity cluster x nd an node a from gx such that a is the lca of at least entities in x and idf a is maximized solution to nd the best solution we rst give the tonicity property of the idf function dened in eq the lemma states that if a category is ancestor of in g then it is obviously true because according to idf idf eq the number of descendants of is no less than that of the lemma suggests that bottom up level wise search solution for the maximal lca of x because the lower level close to the entities will certainly have a larger idf value than the upper level for example an isa taxonomy graph g shows in figure compose of entities and categories is parent category of so is ancestor of thus occurs in feature of and occurs in feature of then idf and idf similarly is ancestor of both entities then idf clearly idf of a category is always no larger than its descendant category specically we use li i to denote the categories to be tested in the i th level is dened as the parents of x in gx in the ith level we rst let li be the parents of categories of then we calculate the coverage of each category in li if any category cover at least entities we return the one with maximal idf value from li as the result otherwise the procedure proceeds into the i level note that in each level we use the idf function to select the most specic one among all lca discovered in the same level we also highlight that li many overlap with the above level wise search can certainly nd the optimal solution due to lemma lemma monotonicity given two categories if is an ancestor of in g we have idf idf example we give the example to show how maximal lca can be found suppose there is cluster x compose of in figure and we set first for categories in their coverage is respectively both coverage is lower than so we continue search upper level here both coverage of and is hence and satisfy the requirement of lca we select the most specic one as the maximal lca since idf weight of is larger than implementation optimizations in real implementations we have two issues to address first we set a maximal layer limit to boost the search procedure second we need to handle cases where no appropriate a lca is found next we elaborate our solutions to each issue we set a upper limit for the search level due to two reasons on one hand x may have no valid lca on the other hand even if we nd a lca in a higher layer the category we found may be too general thus is meaningless note that our algorithm may return no result due to two sons first the constraint posed by is too stricter second the upper limit may although boosted the search but may miss some valid solution occurring in upper level to solve this problem we run the maximal lca search iteratively with varying from to obviously the iterative search can certainly nd a solution if at least category occur in gx with increment as experiment in this section we present our experimental results we run the experiments on wikipedia released in january the basic statistics of wikipedia before and after revoking the noisy entities are shown in table we refer to the linked entity with at least one category as valid linked entity because we need to use the category information for the clustering we run all experiments on a bit windows server system with intel xeon cores cpu and g memory we implement all the programs in java we totally nd m clusters for m articles for each cle we nd cluster on average each cluster contains entities on average if we treat the article entity property an entity in a cluster as a single fact we extracted overall m facts table statistics of wikipedia before after ing the noisy linked entities item article categories article has linked entity linked entity per article article has valid entity before m m m m after m m m m effectiveness in this subsection we justify the eectiveness of our system with the comparison to two state of the art systems to extract knowledge from wikipedia both of the two competitors extract the relationship of entity pairs by handling natural language tences the rst system nds the sentences in an article mentioning two entities the sentences will be parsed to drive a dependency tree and the shortest dependency path from one tity to the other entity gives the syntactic structure expressing the relationship between the entity pair however an entity may be expressed in dierent formats known as the coreference lution problem which results into the low recall of to solve the coreference resolution problem in the second system we borrow the idea from to extract many syntactic patterns of an entity then use to extract facts from wikipedia we evaluate the precision and user satisfactory for all the tems we randomly select wikipedia articles and recruited volunteers to manually evaluate the quality of the extracted facts of these articles we present the existing infobox as reference to them and ask them to evaluate the systems each volunteer was asked to rate the knowledge by one of the options in fectly sensible well sensible somewhat sensible not sensible at all we assign each option with a score from sensible at all to sensible the comparison results are shown in table where time cost per fact is the average time cost on generating one fact the processing time including nding the sentences is not considered in and precision is measured as the percentage of sensible knowledge all three options except not sensible at all recall is the percentage of linked entities that can be found a relationship between it and the article entity user satisfactory is the average score for all samples note that we also give the user satisfactory for the existing infobox we can see from tbale that our system is signicantly more ecient than the two competitor systems besides this our system outperforms the competitors signicantly in precision recall and user satisfactory we highlight that the precision of our system is almost the recall of our system is the reason is that some linked entities are regarded as noises or do not have category information and consequently can not be clustered if table comparison to baseline systems matric time cost per precision recall user satisfactory infobox we did nt count them in the recall computation we will get an even better recall the user satisfactory of our system is close to that on the existing infobox suggesting that our extraction system has close quality to existing infobox comparing to has a higher recall but a lower precision because it can discover more sentences containing the article entity and linked entity remove noisy linked entities in this subsection we evaluate the eectiveness of our rank aggregation approach the statics of wikipedia after removing all unrelated linked entities are shown in table to quantify the goodness of a ranking scoring we rst manually label each linked entity as related or unrelated this manually labeled data set is used as the ground truth then for each ranking measure we generate an ordering by the measures and evaluate the ordering with the comparison to the ground truth by m m t where m is the set of linked entities labeled with related and k is the set of top k entities in the ordering by varying k from to the number of elements to be ordered we can draw the curve of m we can further quantify the closeness of a ranking measure r with respect to a range s t k s m m p t s where m and m are the m curve of the measure r and the ground truth respectively means the range from top s to top t the s t actually characterizes the average closeness in the range of when s and t n n is the number of all elements we have measures the entire closeness to the ground truth of the ranking measure r comparison to individual rankings we use steve jobs apple inc to evaluate the eectiveness of dierent ranking sures results on other articles are similar to them in our periment we order linked entities of the two samples by dierent rankings the m curves are shown in figure in which we compare our aggregated measure to the two individual ranking measures p m i and w jc we also give the m curve for the ground truth the closer to the ground truth curve the ter the measure is we can see that p m i is better than w jc in noise detection since p m i in general is closer than w jc to the ground truth curve in general the curve of our aggregated measure is closer to the ground truth curve than the two ual measures hence our rank aggregation is better than either p m i or w jc and outperforms them in both detecting strongly related entities and recognizing noisy entities comparison to other aggregated measures we next compare our aggregated ranking to the naive linear tion method with static we vary from to with ment of so that we can compare to the dierent linearly bined measures for the two samples we calculate the closeness between the ground truth and dierent ordering measure r the results are shown in figure where the zontal line is our aggregated measure we can see that that our aggregated is superior to the naive linearly combined measure consistently over dierent only in the case of apple inc with raining from to the linearly combined measure can reach the same goodness as our measure but in general users have no aggregation pmi wjc ground truth aggregation pmi wjc ground truth k num k num steve jobs apple inc figure for dierent ranking strategies num is number of entities in the ordering k m s s e n e s o c l k m s s e n e s o c l alpha steve jobs alpha apple inc figure comparison to other aggregated measures prior knowledge to set an appropriate value for instead our method automatically computes the appropriate and achieves the best performance rationality of the motivation next we justify the tion of renaming aggregation method recall that our tion is based on the fact that p m i is good at identifying the semantically unrelated entities and w jc is good at identifying the semantically related entities to verify this we need to alyze the entities in the head and tail part of the orderings we select articles randomly and manually label their linked tities as related and unrelated for each measure we calculate the closeness for the top head and last tail entities spectively by eq for comparison we also give the result of a random ordering the results are shown in figure we can see that in the head part w jc is better than p m i and both outperforms the random ordering but in the tail part p m i is better than w jc and random order in both head and tail part the aggregated measure perfumes the best which justify again the eectiveness of our ranking aggregation approach clustering and labeling we rst give the metrics used for the evaluation then present the experiment results some clustering and labeling results are shown in table pmi wjc random aggregation s s e n e s o c l head tail figure closeness for head and tail part in the order mj coverage for dierent labeling strategies metrics for the evaluation of clustering to evaluate the eectiveness of a cluster we use both the subjective and objective metric the objective metrics include the inter cluster distance average distance between cluster centers and intra cluster tance average distance between entities and corresponding ter center the two individual metrics can be furthered combined as a synthesis score known as valid index formally let k be the number of clusters mi be the center of cluster ci we have inter k k x x j k k x x ecj valid inter intra intra e a good clustering result has a large inter distance and a small intra distance which induces a large valid index when the cluster is labeled we may alternatively use subjective metric to evaluate the quality of the clustering we adopt cision to evaluate the quality of the extracted knowledge for a certain entity suppose its linked entities are clustered into c ck and each cluster ci has label li the precision of c under label set l li is dened as p c l x cic li where li is the percentage of entities in cluster ci that can be appropriately labeled by the li li is evaluated by humans metric for the labeling evaluation given a cluster c ci and their label set l li we use the following metrics to evaluate the accuracy of l with respect to c coverage coverage of li with respect to ci is the age of entities in ci which is the descendant of li in the isa taxonomy graph gc thus the coverage of l with respect to c is the average coverage of each label li with respect to corresponding ci correctness we use p c l to measure correctness of l with respective to c clustering results to evaluate the performance of ing we cluster the linked entities for china shanghai apple inc steve jobs barack obama new york city and using our clustering approach with and iteration we give the results in table to calculate p c l we use the labels generated by maximal lca we can see that average valid of clusters is around and average precision is approach to which suggests that the generated clusters are of high quality table evaluation of clustering results entity shanghai steve jobs apple inc barack obama china new york city average linked entity cluster time ms valid l correctness for dierent labeling strategies figure evaluation of cluster labeling strategies labeling results we compare our labeling approaches to the baseline approaches including mf mfi and a state of the art approach score sp uses wikipedia as nal source from which candidate cluster labels can be extracted given a cluster sp rst generate some concepts and categories as candidate labels from wikipedia by measuring the relevance to terms in the cluster for a cluster sp rst calculate the quency score of keywords in all candidate labels then propagate the score from keywords to label finally the label with highest score is selected as the cluster label we run maximal lca with for clusters generated from linked entities of above sample entities we use coverage and correctness to evaluate dierent labeling strategies the sults are shown in figure we can see from the figure that coverage of m f is larger than m f i and sp that is reasonable because the category voted by m f is the feature of most entities in the cluster and maximal lca has the largest coverage which approach to because the selected category is at least the ancestor of entities in the cluster for correctness m f is a little better than m f i and obviously outperform sp and also maximal lca performs better than other approaches table labeled clusters generated from apple inc line in column label represents mf mfi sp and maximal lca separately and each label is given with its coverage no cluster alan kay gil amelio andy hertzfeld ronald wayne guy kawasaki g bbc online electronic product mental assessment tool enhanced data for gsm evolution google maps ios ibooks xsan itunes dell foxconn ibm intel label people by status tunisian jewish descent apple inc employees tele conferencing tele conferencing open standards ios software ios software ios software ios software computer hardware companies computer hardware companies computer hardware companies computer hardware companies we also give the clustering results for apple inc under ent labeling approaches in table we can see that mi in general can nd the frequent but general category such as the rst ter mfi tends to nd the specic label which in general has a further propose a novel position aware rank aggregation method to detect the semantic related entities we also propose an tive cluster reuse strategy to run clustering for millions of entities in wilkipeida with these eective and ecient approach we extracted million new facts from wikipedia low coverage such as the second cluster the performance of sp is not stable which may generate either the general or specic label see the rst and second clusters of sp compared to these methods maximal lca method can generate specic label of high coverage in most clusters maximal lca enables us to nd knowledge such as apple inc ios software google maps ios ibooks xsan itunes related works data mining on encyclopedia many works have been done in online encyclopedia to achieve some applications especially in wikipedia one of the most valuable online data source esa and wikirelate use wikipedia to compute semantic ness for an entity pair and and use wikipedia as external knowledge for clustering or labeling cluster which enrich the resentation of document with additional features from wikipedia structural knowledge extraction in the work of structural knowledge extraction knowitall and textrunner tract open information from free text and some challenging task such as ner dependency parsing and relationship extraction are commonly use in text analysis some structural knowledge have also been extracted from wikipedia like yago and dbpedia dbpedia represents in rdf is a large scale tured knowledge base who extracts structured information from wikipedia and also links to other datasets on the web to wikipedia but dbpedia is built on existing infobox in wikipedia and tural knowledge in other datasets to make wikipedia more structural semantic wikipedia proposes a formalism to ture wikipedia s content as a collection of statements the ment can explain the relationship between article and linked tities and try to extract relationship of linked entity use syntactic and semantic information and refer to relationships in infobox these article related relationships can be good ment for infobox specically to supply attribute value for complete infobox kylin ipupulator and ibminer learn models from structured information to guide the text processing for example kylin rst predicts what attributes a sentence may contain and further use crf to extract attribute values from the candidate sentences document summarization instead of mining relationship of single linked entity we focus on all the linked entities for an article since each linked entity direct to a specic article in wikipedia multi document summarization is a good solution to handle it we can summarize the linked entities to groups and generate a theme for each group in document summarization selects important sentences or paragraphs in the set of ments and build a summary with these passages and forms the summary of documents to dierent event theme by using lda to capture the events being covered by the documents ing is another widely used method to do summarization such as xdox and select a representative passage from the cluster after clustering in this paper we use clustering method to summarize linked entities and dierent from above structural knowledge tion methods we use the structured entities and categories only in wikipedia to extract in this way we can avoid the text processing problem such as ner and dependency parsing conclusion discovering and enriching structural information in online cyclopedia is valuable and challenging work dierent from vious free text focused methods in this paper we propose an novel semi structured information based approach we extract knowledge from wikipedia using rich set of linked entities we propose an cluster then label approach which clusters the linked entities into dierent semantic groups and then give each group a semantic label a property in this way we can get groups of facts in the form of cluster and semantic label we
