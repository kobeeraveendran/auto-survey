n u j l c s c v v x r combination abstractive extractive approaches summarization long scientic texts tretyak vladislav ru stepanov denis denis com itmo university kronverksky pr st peterburg abstract research work present method generate maries long scientic documents uses advantages tractive abstractive approaches producing summary abstractive manner perform extractive step conditioning abstractor module pre trained based language models extractor abstractor ments showed extractive abstractive models jointly nicantly improves summarization results rouge scores introduction language modeling task general case process learning joint probability function sequences words natural language statistical language modeling language modeling development probabilistic models able predict word sequence given words precede known context language models operate ferent sequence levels small language models work sequences chars words big language model works sentences common language model operates sequences words language model standalone predict words according context usually language model solve challenging tasks example helps solve large range natural language tasks machine translation natural language understanding speech recognition mation retrieval text summarization words language models real world natural language processing applications quality applications depends language models performance s language modeling task plays major role natural language processing articial intelligence research rst neural language model feed forward architecture main features neural language models getting representation vector words sequences word vectors usually called embedding vector v tretyak et al embeddings similar words located closer dimension space having similar representations successful usage feed forward networks recurrent neural networks achieved better results language eling tasks ability account position words sentences producing contextual word embeddings long short term memory networks allows language model learn relevant context longer sequences feed forward rnn s sophisticated memory mechanism attention mechanism improvement language modeling tasks combination sequence sequence framework attention mechanism improves memory mechanism recurrent ral networks giving ability decoder network look context big step language modeling task developing architecture novel self attention mechanism helps model use context sentence eciently models account left right context sequence implemented bert model left context like gpt model transformer based lm disadvantages limited receptive eld means transformer proceed sequences limited length rent neural networks work unlimited sequences issue partially solved transformer xl model work continuous quences texts like recurrent neural networks despite disadvantage models like gpt large receptive eld trained large data capable capturing long range dependencies work propose method uses extractive abstractive approaches summarization task work improves previous approach pre trained lm instead training scratch research work arxiv dataset great example long scientic documents order compare work previous approaches split training process steps train extractive model classication task simply selects sentences selected summary second use extracted summary dierent article sections conditioning generating abstractive summary adding extractive summary conditioning crucial generating target summary experiments dierent variants conditioning found best combination ing experiments extracted summary introduction conclusion paper performs best contributions work combination extractive abstractive approaches improves quality produced summary pre trained transformer based guage models result applying proposed model improved rouge metric arxiv dataset abstractive extractive summarization related work automatic summarization process shortening set data tionally create summary represents important relevant information original content summarization main tasks natural language understanding main approaches creating summary extractive summarization aims identifying information extracted grouped form extractive summary means train machine learning model solves classication task information included summary abstractive summary generation rewrites entire document building internal semantic tation summary created natural language processing example ml model receives source document generates summary word word copying text source document rst summarization systems extractive techniques easier abstractive hsu et al combined extractive tive approaches creating summaries authors proposed unied model combines sentence level word level attention advantage proaches authors created novel inconsistency loss function order sure proposed model mutually benecial extractive abstractive summarization attempt extractive abstractive models proposed paper authors proposed model uses bert perform extractive summarization lstm pointer network selects sentences included summary extracted summary fed attention based ing mechanism handling vocabulary oov words model responsible paraphrasing extracted summary producing abstractive summary reinforcement learning technique mize rouge metric directly instead maximizing log likelihood word given previous ground truth words extractive tive models trained work authors took pre trained bert model rst trained extractive task cnn daily mail datasets continue training model abstractive rization subramanian et al arxiv dataset summarizing long scientic texts uses similar approach model consists parts rst hierarchical representation document points sentences document order construct extractive summary second transformer language model conditions extractive summary paper text proposed model model consist components extractive model classier choose sentences source text included summary tive model uses condition text produce abstractive summary v tretyak et al research work propose model combines advantages extractive abstractive approaches creating summaries proposed model consists parts rst extractive summarization model second stractive model similar approach paper dierence extractive abstractive models replaced pre trained based lms motivation pre trained language model cases shows signicant increase performance previous approach transformer model abstractive model instead sive pre trained lm research work list experiments conditional generation performed extractive summary ate abstractive summaries training performed steps extractive model trained second training abstractive model uses extractive summary produce better results dataset description mentioned dataset arxiv org training dataset contains long sequences texts scientic domain ground truth summaries structure dataset contains information article d abstract text article text labels section sections eld article text holds text paper section list papers sections sections contains text paper divided sections means identify text introduction section abstract section conclusion advantage experiments dataset statistics described table table arxiv dataset statistics num docs avg length words doc length avg mary words validation test parts took percent documents preprocessing step long short papers removed papers abstracts text paper removed replaced latex markup special tokens math graph table equation order help model recognize special tokens latex source code cleared removed irrelevant chars exclude latin letters preprocessing pipeline applied extractive abstractive tasks common approach creating dataset training extractive model create list sentence pairs abstract sentence sentence paper sentence abstract matched sentence abstractive extractive summarization paper text pair compute rouge metric particular compute rouge l average value score got scoring value pair sentences higher better choose pairs highest scores positive examples randomly sample sentences paper text mark pairs negative examples completing steps got dataset contains list sentence pairs labels save dataset extractive summarization separate le abstractive summarization task took best model solves extractive summarization tasks infer dataset words generate summaries paper dataset best extractive model necessary preprocessing save model generated maries corresponding papers abstracts steps dataset extractive model experiments extractive models architectures extractive summarization models bert roberta electra electra berts architecture pre training phase dierent original paper authors proposed new scheme models generator language model trained jointly generator process tences corrupts tokens corruption denotes process replacing tokens tokens t context sentence turn language model predicts tokens replaced generator scheme pre training improves performance guage model reduces number training iterations computing resources extractive models pre trained large corpus texts took base variants check large variants long training hardware constraints highly probable large variants better results extractive models construct special input tokens equation inputext seqgt tokencls seqcandidate xext denotes input extractive model concat denotes tion function tokencls denotes special classication token originaly bert model seqgt denotes sentence ground truth summary seqcandidate corresponds candidate sentence word piece tokenizer bert electra models original papers vocabulary size roberta uses bpe tokenizer original implementation cls token vocabulary s manually add training key benet word tokenizers small dataset size decreasing vocabulary cases v tretyak et al special tokens added math graph table equation extracted regular expressions pre processing extractive models trained cross entropy loss optimization function learning rate model input batch size applied gradient clipping technique training process stable applied distributed training gpus training process took approximately days architecture proposed models scribed figure scheme remains input model changes pre trained lm replaced bert electra fig architecture extractive model classication head gure denotes block contains stack linear layers output size sigmoid activation rouge metrics evaluation quality summaries evaluate summarization model rstly inference test set list pairs sentence abstract sentence paper score pair model use candidates highest score process got extractive summary use ground truth abstract calculate rouge scores scores averaged papers rouge l main evaluation metric summaries proposed models set training hyperparameters settings table conclude bert model achieves highest result models equal set abstractive extractive summarization hyperparameters bert model achieves better results table oracle denotes rouge score ground truth abstract extracted summary includes relevant sentences text paper oracle scores table indicate limit extractive models best summary according rouge metric order coherent text perform paraphrasing extracted summaries paraphrased sentences apply translation technique pre trained transformer lm trained translate sentences english german backward translate extractive summary german language pre trained transformer lm english language paraphrased summaries later experiments condition model results extractive models presented table table extractrive model results arxiv dataset ext denotes extractive model bert roberta electra type rouge l ext ext ext abstractive model nishing experiments extractive summarization ments abstractive approaches abstractive summarization trained autoregressive language models bart experiments conditional generation models conditional generation model context according generates text condition summary extracted best extractive model table conditioning paraphrased summaries input size gpt base variant input dened equation inputabs masksegment concatenate tc conditioning text target summary ts apply segment mask masksegment identies input condition target text experiments bart model performed similar way dierences model input bart consists parts rst encoder uses bert like architecture second decoder consists stacked transformer decoders similar architecture gpt parts connected encoder output fed decoder architecture decoder uses hidden states produced encoder encoder decoder trained end end means backward pass update weights decoder encoder scenario v tretyak et al fed conditioning barts encoder target text decoder training process propose dierent conditioning scenarios conditioning extractive summary introduction paper introduction concatenated conclusion introduction concatenated extractive summary conclusion assumption introduction conclusion concentrate valuable information generating summaries usually introdiction author describes problem details proposed method novelty conclusion authors usually recap conclusion results apply conditioning long texts restrictions input size condition plus target text cases t input size s conditioning summaries extracted models experiments results bart presented table table proposed model results arxiv dataset model bert roberta electra bart conditioned extractive summary bart conditioned introduction bart conditioned introduction clusion bart conditioned introduction tive summary clusion condition extractive summary tion extractive paraphrased summary ing subramanian s al cohan al et type rouge l approaches ext ext ext mix mix mix mix mix mix abs previous approaches mix mix gold extractive gold extractive oracle abstractive extractive summarization table conclude extractive summary plays cial role generating abstractive summaries removing extractive summary condition leads decreasing rouge score tion introduction conclusion holds relevant information conrmed obvious extractive summary bigger impact duction conclusion extractive summary holds lot vant according rouge score sentences investigate rouge l scores outperforms best model outperforms oracle cause abstractive summarization model produce words presented source document best model uses bert extractor bart abstractor presented fig bert performs extractive summarization article extracted summary concatenates introduction conclusion paper setup shows best mance according rouge metric outperforms previous approach applied arxiv dataset fig proposed method generating summaries long texts conclusion novel improvements proposed uses extractive tive approaches extracted model bert model abstractive model bart model research work comparison analysis dierent architectures extractive abstractive summarization approaches extractive modelabstractive modelextracted summaryrest texttext forsummarizationabstractive summary v tretyak et al experiments conditioning dierent parts source document produce abstractive summary conditioning tractive summary introduction conclusion paper shows best rouge score assumption conditioning paraphrased summary failed paraphrasing source sentence changed evant ground truth summary evaluation results obtained research outperforms previously applied algorithms arxiv dataset rouge l metrics ingly experiments showed advantages extractive stractive approaches improves quality produced summary extractive summarization plays crucial generating abstractive summaries future improvement proposed architecture end end learning applied extractor abstractor feature potentially improve quality abstractive summaries proposed architecture tested summarization datasets references bae s kim t kim j lee s summary level training sentence rewriting abstractive summarization arxiv preprint clark k luong m t le q v manning c d electra pre training text coders discriminators generators arxiv preprint dai z yang z yang y carbonell j le q v salakhutdinov r transformer xl attentive language models xed length context arxiv preprint devlin j chang m w lee k toutanova k bert pre training deep tional transformers language understanding arxiv preprint gal y ghahramani z theoretically grounded application dropout current neural networks advances neural information processing systems pp hochreiter s schmidhuber j long short term memory neural computation hsu w t lin c k lee m y min k tang j sun m unied model extractive abstractive summarization inconsistency loss arxiv preprint lewis m liu y goyal n ghazvininejad m mohamed levy o stoyanov v zettlemoyer l bart denoising sequence sequence pre training natural language generation translation comprehension arxiv preprint liu y lapata m text summarization pretrained encoders arxiv preprint liu y ott m goyal n du j joshi m chen d levy o lewis m zettlemoyer l stoyanov v roberta robustly optimized bert pretraining approach arxiv preprint abstractive extractive summarization mikolov t sutskever chen k corrado g s dean j distributed sentations words phrases compositionality advances neural information processing systems pp pascanu r mikolov t bengio y diculty training recurrent neural networks international conference machine learning pp peters m e neumann m iyyer m gardner m clark c lee k zettlemoyer l deep contextualized word representations arxiv preprint radford wu j child r luan d amodei d sutskever language models unsupervised multitask learners openai blog schuster m nakajima k japanese korean voice search ieee international conference acoustics speech signal processing icassp pp ieee sennrich r haddow b birch neural machine translation rare words subword units arxiv preprint subramanian s li r pilault j pal c extractive abstractive ral document summarization transformer language models arxiv preprint sutskever vinyals o le q v sequence sequence learning neural networks advances neural information processing systems pp vaswani shazeer n parmar n uszkoreit j jones l gomez n kaiser polosukhin attention need advances neural information processing systems pp
