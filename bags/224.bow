exploring domain shift extractive text summarization danqing wang pengfei liu ming zhong jie fu xipeng qiu xuanjing huang school computer science fudan university mila polytechnique montreal edu cn jie g u l c s c v v x r abstract domain shift explored nlp applications received little attention domain extractive text summarization result model utilizing nature training data ignoring difference distribution training sets shows poor generalization unseen domain tation mind paper rst extend conventional denition domain categories data sources text marization task purpose domain summarization dataset verify gap different domains inuences performance neural summarization els furthermore investigate learning strategies examine abilities deal domain shift problem experimental results different settings different characteristics new testbed source code including bert based learning methods multi domain rization learning purposed dataset multi sum available project com introduction text summarization important research topic widespread applications ing research works summarization mainly volve exploration neural architectures cheng lapata nallapati et al design training constraints paulus et al wu hu apart eral works try integrate document characteristics e domain enhance model performance haghighi vanderwende cheung penn cao et al isonuma et al authors contributed equally corresponding author wang et al narayan et al interpretable analysis existing neural marization models zhong et al despite success literature ung penn hua wang probes exact inuence domain bring investigates problem domain shift explored nlp tasks absence poses challenges current neural summarization models domain shift exactly affect performance existing neural architectures better advantage domain information improve performance current models new model built perform test set employed unseen mains sure learns useful summarization instead overtting source domains important reason lack proaches deal domain shift lay unawareness different domain denitions text summarization literature limits cept domain document categories latent topics uses extra loss cao et al isonuma et al feature embeddings wang et al narayan et al denition presumes category information affect summaries formulated information obtained easily accurately popular ve summarization datasets information training semantic categories clear ve datasets duc et al cnn daily et al new york times annotated corpus et al duc nyt annotated document categories duc designed petition test denition prevent previous work use domains existing datasets building new multi domain dataset multi domain learning easy explore domain connection datasets paper focus extractive rization demonstrate news publications cause data distribution differences means dened domains based purpose multi domain tion dataset multi sum explore issue domain shift methodologically employ types els characteristics different tings rst model inspired joint ing strategy second builds tion large scale pre trained models multi domain learning model directly constructs domain aware model introducing domain type information explicitly lastly tionally explore effectiveness meta learning methods better generalization ing performance domain domain cross dataset provide liminary guideline section future research multi domain learning summarization tasks contributions summarized follows analyze limitation current main denition summarization tasks extend article publications purpose dataset multi sum provide sufcient multi domain testbed domain domain best knowledge rst work introduces domain shift text marization demonstrate domain shift affects current system designing verication experiment instead pursuing unied model aim analyze different choices model signs inuence generalization ability dealing domain shift problem ding light practical challenges vide set guidelines future researchers domains text summarization section rst describe similar concepts domain summarization tasks example dining wine nyt refers food drink duc extend denition article sources verify rationality indicators illustrate data distribution purposed multi domain summarization dataset common domain denition domain dened content category text li zong blitzer et al image saenko et al initial motivation domain metadata attribute order divide data parts different distributions joshi et al text summarization differences data distribution attributed ment categories sports business latent topics articles caught classical topic models like latent dirichlet location lda blei et al vious works shown taking consideration distribution differences improve marization models performance isonuma et al wang et al related concept domain investigated rization tasks perspective multi domain learning publications domain paper extend concept article sources easily obtained clearly measures assume tions news affect data distribution inuence summarization styles order verify hypothesis use indicators coverage density sion dened grusky et al measure overlap compression ment summary pair coverage density word longest common subsequence lcs overlaps respectively compression length ratio document mary baselines calculate strong marization baselines publication hua wang studied domain adaptation tween news stories opinion articles nyt model trained single domain adapted different domain training evaluation settings existing benchmark datasets mixture ple publications idea collecting larger data cnn dailymail gigward newsroom statistics measures ext oralce train valid test cov den comp r l r l fn cnn ma nyt wtp avg nydn wsj usat tg time avg lead table statistics multi sum dataset measures refer coverage density pression respectively lead ext oracle common baselines summarization measures baselines calculated test set corresponding publication ve publication source domains training ones viewed domain lead baseline concatenates rst tences summary calculates rouge score baseline shows lead bias dataset essential factor news ticles ext oracle baseline evaluates performance ground truth labels viewed upper bound extractive marization models nallapati et al narayan et al multi sum recently proposed dataset newsroom grusky et al scraped major news publications select publications nytimes post foxnews theguardian nydailynews wsj usatoday cnn time mashable process way et al obtain ground truth labels extractive summarization task follow greedy approach introduced nallapati et al finally randomly divide domains groups ing test purposed subset newsroom multi sum indicate specially designed multi domain learning summarization tasks table nd data news publications vary indicators closely relevant summarization means ment summary pairs different publications unique summarization formation models need learn different semantic tures different publications furthermore follow simple experiment torralba et al train classier ve domains simple classication model glove ing words achieve accuracy chance ensures built bias publication reasonable view publication domain use multi publication multi sum multi domain dataset analytical experiment domain shift domain shift refers phenomenon model trained domain performs poorly different et al gopalan et al clearly verify existence main shift text summarization design simple experiment multi sum dataset concretely turns choosing domain use training data train basic model use testing data remaining domains evaluate model automatic metric rouge lin hovy basic model like recent approaches dene extractive summarization sequence beling task formally given document s sisting n sentences sn summaries extracted predicting sequence label y yi document yi represents th sentence document included summaries rouge l similar trends results attached appendix fn cnn ma nyt wtp nydn wsj usat tg time fn cnn ma nyt wtp nydn wsj usat tg time table results matrix v verication experiment based multi sum dataset scores model trained tested domain rii shown diagonal line regarded benchmark scores cells vij rij rjj j represents test domain j improvements obtained switch training domain j positive values higher benchmark negative values benchmark paper implement simple erful model based encoder decoder tecture choose cnn sentence encoder following prior works chen bansal employ popular modular transformer vaswani et al document encoder detailed settings described section results table nd values negative diagonal indicates els trained tested domain great advantage trained mains signicant performance drops strate domain shift problem rious extractive summarization tasks pose challenges current performed els trained evaluated particularly strong hypothesis training test data instances drawn identical data tion motivated vulnerability investigate domain shift problem multi domain training evaluation settings multi domain summarization observations mind ing approach alleviate domain shift problem effectively text summarization specically model perform source domains trained advantage unseen target domains involves tasks multi domain learning domain adaptation begin eral simple approaches multi domain rization based multi domain learning learning strategies y k facilitate following description rst set mathematical notations assuming k related domains refer dk dataset nk samples domain dk represent sequence sentences corresponding bel sequence document domain k spectively goal estimate conditional probability p y utilizing ities different domains y k nk modeli base simple effective model multi domain learning domains aggregated training set shared parameters notably domains model explicitly informed differences loss function domain written basic denotes cnn transformer coder framework described section means domains share parameters analysis model benets joint training strategy allow lithic model learn shared features different domains sufcient alleviate domain shift problem potential limitations remain joint model aware differences domains lead poor performance task evaluation task specic features shared tasks negative transferring happened new domains study different approaches address problems modelii bert recently unsupervised training achieved massive success nlp munity devlin et al peters et al usually provides tremendous external edge works building connection large scale pre trained els multi domain learning model explore external knowledge unsupervised pre trained models bring contribute domain learning new domain adaption achieve pre training basic model m odeli base bert devlin et al successful learning frameworks investigate bert provide domain information bring model good domain adaptability avoid introducing new structures use feature based bert parameters xed analysis model instructs processing multi domain learning utilizing external trained knowledge perspective dress problem algorithmically t ag domain type duced directly feature vector ment learned representations domain aware ability specically domain tag bedded low dimensional real valued vector concatenated sentence embedding loss function formulated iii worth noting unseen domains formation real domain tags available design domain tag x unknown domains randomly relabeled examples training real tag data tagged x source domain embedding force model learn shared features makes adaptive unseen domains experiment improves performance source domains target domains analysis domain aware model makes possible learn domain specic features suffers negative transfer problem private shared features entangled shared space bousmalis et al liu et al specically domain permission work radford et al apply pre trained language model wide range nlp tasks zero shot setting discuss differences related work section figure gradient update mechanism meta learning strategy modeliv m eta modify shared parameters makes easier update parameters different directions m eta modeliv order overcome itations try bridge communication gap different domains updating shared parameters meta learning finn et al li et al liu huang introduced communicating protocol claims domain tell updating details gradients different updating behaviors different domains consistent formally given main domain iliary domain b model rst compute gradients la regard model rameters model updated gradients calculate gradients b objective produce maximal mance sample y b lab min y b loss function domain nally written iv lkj weight coefcient l instantiated li eqn lii liii eqn analysis address multi domain ing task adaptation new domains bert modeliii t ag modeliv modelii m eta different angles specically modelii bert utilizes scale pre trained model modeliii t ag proposes introduce domain type information explicitly lastly modeliv m eta designed update ters consistently adjusting gradient direction main domain auxiliary domain b training mechanism puries shared feature space ltering domain specic features benet experiment domains basic modelii bert modeliii t ag modeliv m eta investigate effectiveness strategies evaluation settings domain domain cross dataset settings possible explicitly uate models quality domain aware text representation adaptation ability derive reasonable representations unfamiliar domains experiment setup perform experiments mainly domain multi sum dataset source domains dened rst ve domains domain table domains domain totally invisible training evaluation domain setting tests model ability learn different domain distribution multi domain set later domain investigates models perform unseen mains use cnn dailymail cross dataset evaluation environment provide larger distribution gap use modeli basic baseline model build modelii bert feature based bert modeliii t ag domain embedding develop modeliii t ag instantiation modeliv m eta detailed dataset statistics model settings hyper parameters reader refer appendix quantitative results compare models scores table note select sentences multi sum domains sentences cnn daily mail different average lengths reference summaries basic vs modeli t ag table serve domain aware model outperforms monolithic model domain domain settings signicant improvement domain demonstrates domain information effective summarization models trained multiple domains superior mance domain illustrates awareness domain difference benets zero shot setting suggest domain aware model capture specic features domain tags learned domain invariant features time transferred unseen domains domain setting fn cnn ma nyt wtp average ntdn wsj usat tg time average domain setting r cross dataset setting cnn dm table performance learning strategies multi sum dataset domain smaller r indicates corresponding model better generalization ability bold numbers best results red ones indicate minimum formance gap source target domains grey rows models average performance der evaluation settings basic vs modeliv modeli m eta despite little drop domain setting narrowed mance gap shown r table indicates modeliv m eta better generalization ability compensation performance decline mainly lies consistent way update eters puries shared feature space expense ltering domain specic tures excellent results cross dataset settings suggest meta learning strategy successfully improve model transferability domains multi sum different datasets modelii supported smaller r bert pared modeli base draw sion bert shows domain generalization ability multi sum ity inferior modeliii m eta leads worse performance dataset attribute success multi sum ability address domain learning domain adaptation instead t ag modeliv specic experiment analyze bert bert achieve domain generalization modelii appendix figure relative position selected sentence original document ve source domains overlap ground truth labels model results order highlight differences rows correspond model model iii section model r l et al narayan et al zhang et al chen bansal dong et al zhou et al basic model basic model tag basic model meta basic model bert basic model bert tag table comparison strategies extractive summarization models non anonymized cnn daily mail provided et al red arrows indicate performance improvement base model green arrows denote degradation suppose vast external knowledge bert provides superior ability feature extraction causes modelii bert overt multi sum perform excellently domains fails different dataset cnn daily mail observation suggests supervised pre trained models powerful radford et al place role supervised learning methods e modeliii t ag modeliv m eta designed specically addressing multi domain learning new domain adaptation best generalization ability cost tively lower domain performance modeliv m eta good choice domain performance matters end users modelii bert achieve best performance domain settings expense training time shows worse generalization ability modeliv m eta training time issue modelii bert good supplement methods results cnn dailymail inspired observations ploy learning strategies mainstream summarization dataset cnn dailymail et al includes different data sources cnn dailymail use tion domain train models training set table shows basic model comparable performance extractive marization models publication tags improve rouge scores signicantly points meta learning egy advantages dealing domain examples expected bert tags achieves best performance performance increment publication tags bring basic model suppose bert contained degree domain information analysis different model choices marize modeliii t ag simple efcient method achieve good performance domain setting shows certain generalization ability unseen domain modeliv m eta shows qualitative analysis furthermore design experiments probe potential factors tribute superior performance aware models monolithic basic model truthmodel truthmodel positionpercentage import auxiliary domain hurts model ability learn domain specic features results domain dataset settings indicate loss b informed s gradient information helps model learn general features ing generalization ability related work briey outline connections differences following related lines research domains summarization works summarization exploring cepts domains cheung penn plored domain specic knowledge associated template information hua wang investigated domain adaptation abstractive marization found content selection ferable new domain gehrmann et al trained selection mask abstractive rization proved excellent adaptability previous works investigated els trained single domain explore multi domain learning summarization multi domain learning mdl domain adaptation da focus testbed requires training evaluating performance set domains care questions learn model ing set contains multiple domains involving mdl adapt multi domain model new mains involving da investigation effective approaches like existing works rst veried domain shift inuences summarization tasks semi supervised pre training zero shot transfer long history ne tuning downstream tasks supervised unsupervised pre trained models le mikolov vlin et al peters et al rising interest applying large scale trained models zero shot transfer learning ford et al different works focus addressing domain shift ization problem explored methods semi supervised pre training combines pervised unsupervised approaches achieve zero shot transfer domain domain c cross dataset figure loss weight coefcients model iv y axis mean score rouge l different bins correspond different values label position sentence position known powerful feature especially tive summarization kedzie et al pare relative position sentences selected models ground truth labels source domains investigate models t distribution distinguish domains select tive models t ag illustrated figure base modeliii percentage rst sentence foxnews signicantly higher unaware different domains modeli base learns similar tribution domains seriously affected density extreme distribution togram probability rst sentence selected higher ground truth domains compared modeli base domain aware models robust learning different relative distributions different domains modeliii t ag constrains extreme trend especially obviously cnn mashable weight modeliv m eta investigate eral probe performance modeliv m eta eqn weight coefcient main domain model ignores focuses auxiliary domain b trained loss main main instantiation modeliii t ag figure shows increase rouge scores rise domain decline domain cross dataset formances domain settings prove plot density histogram relative locations ground truth labels source target domains attach appendix compared table nd relative position ground truth labels closely related rouge performance basic model picture appendix illustrates models performance conclusion paper explore publication text domain investigate domain shift problem summarization veried tence propose build multi domain testbed summarization requires training measuring performance set domains der new settings propose learning schemes preliminary explore istics different learning strategies dealing multi domain summarization tasks acknowledgment thank jackie chi kit cheung useful ments discussions research work ported national natural science foundation china hai municipal science technology sion hai municipal science technology major zjlab references david m blei andrew y ng michael jordan latent dirichlet allocation journal chine learning research john blitzer mark dredze fernando pereira biographies bollywood boom boxes blenders domain adaptation sentiment classication proceedings annual meeting ciation computational linguistics pages konstantinos bousmalis george trigeorgis nathan silberman dilip krishnan dumitru erhan domain separation networks advances neural information processing systems pages ziqiang cao wenjie li sujian li furu wei improving multi document summarization text classication proceedings conference articial intelligence aaai yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proceedings annual ing association computational linguistics volume long papers volume pages jianpeng cheng mirella lapata neural marization extracting sentences words proceedings annual meeting sociation computational linguistics volume long papers volume pages jackie chi kit cheung gerald penn abilistic domain modelling contextualized tributional semantic vectors proceedings annual meeting association tational linguistics volume long papers ume pages jackie chi kit cheung gerald penn wards robust abstractive multi document rization caseframe analysis centrality main proceedings annual meeting association computational linguistics ume long papers volume pages jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language ing arxiv preprint yue dong yikang shen eric crawford herke van hoof jackie chi kit cheung banditsum extractive summarization contextual bandit empirical methods natural language ing emnlp chelsea finn pieter abbeel sergey levine model agnostic meta learning fast adaptation deep networks international conference chine learning pages sebastian gehrmann yuntian deng alexander m rush abstractive summarization empirical methods natural language ing emnlp raghuraman gopalan ruonan li rama lappa domain adaptation object nition unsupervised approach national conference computer vision pages ieee max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies proceedings conference north american chapter association computational linguistics man language technologies volume long pers volume pages aria haghighi lucy vanderwende ing content models multi document tion proceedings human language gies annual conference north american chapter association tional linguistics pages association computational linguistics karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read advances neural comprehend tion processing systems pages xinyu hua lu wang pilot study main adaptation effect neural abstractive rization arxiv preprint masaru isonuma toru fujino junichiro mori yutaka matsuo ichiro sakata extractive marization multi task learning document proceedings classication ence empirical methods natural language processing pages mahesh joshi william w cohen mark dredze carolyn p rose multi domain learning proceedings domains matter joint conference empirical methods ural language processing computational ral language learning pages tion computational linguistics chris kedzie kathleen mckeown hal daum content selection deep learning models summarization empirical methods natural language processing emnlp quoc v le tomas mikolov distributed representations sentences documents ceedings icml da li yongxin yang yi zhe song timothy m hospedales learning generalize learning domain generalization arxiv preprint shoushan li chengqing zong domain sentiment classication proceedings annual meeting association putational linguistics human language nologies short papers pages association computational linguistics chin yew lin eduard hovy matic evaluation summaries n gram occurrence statistics proceedings man language technology conference north american chapter association tional linguistics pengfei liu xuanjing huang learning multi task communication arxiv preprint pengfei liu xipeng qiu xuanjing huang adversarial multi task learning text tion proceedings annual meeting association computational linguistics ume long papers volume pages ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments thirty aaai conference articial intelligence courtney napoles matthew gormley benjamin van durme annotated gigaword ceedings joint workshop automatic edge base construction web scale knowledge extraction pages association tional linguistics shashi narayan shay b cohen mirella ata nt details summary topic aware convolutional neural works extreme summarization arxiv preprint shashi narayan shay b cohen mirella lapata ranking sentences extractive rization reinforcement learning romain paulus caiming xiong richard socher deep reinforced model abstractive marization arxiv preprint matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word sentations proceedings conference north american chapter association computational linguistics human language technologies volume long papers volume pages alec radford jeffrey wu rewon child david luan dario amodei ilya sutskever language models unsupervised multitask learners kate saenko brian kulis mario fritz trevor rell adapting visual category models new european conference computer domains sion pages springer evan sandhaus new york times annotated corpus linguistic data consortium philadelphia abigail peter j liu christopher d manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers volume pages antonio torralba alexei efros al ased look dataset bias cvpr volume page citeseer ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention need advances neural information cessing systems pages li wang junlin yao yunzhe tao li zhong wei liu qiang du reinforced aware convolutional sequence sequence model abstractive text summarization arxiv preprint yuxiang wu baotian hu learning extract coherent summary deep reinforcement learning thirty second aaai conference articial telligence xingxing zhang mirella lapata furu wei ming zhou neural latent extractive document summarization ming zhong pengfei liu danqing wang xipeng qiu xuan jing huang searching tive neural extractive summarization works s proceedings ence association computational tics pages qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural ment summarization jointly learning score select sentences proceedings annual meeting association computational guistics volume long papers volume pages
