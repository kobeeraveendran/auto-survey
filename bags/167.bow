learning to encode text as human readable summaries using generative adversarial networks yau shian wang national taiwan university hung yi lee national taiwan university t c o l c s c v v i x r a abstract auto encoders compress input data into a latent space representation and reconstruct the original data from the representation this tent representation is not easily interpreted by humans in this paper we propose training an auto encoder that encodes input text into human readable sentences and unpaired stractive summarization is thereby achieved the auto encoder is composed of a generator and a reconstructor the generator encodes the input text into a shorter word sequence and the reconstructor recovers the generator input from the generator output to make the erator output human readable a discriminator restricts the output of the generator to ble human written sentences by taking the generator output as the summary of the put text abstractive summarization is achieved without document summary pairs as training data promising results are shown on both glish and chinese corpora introduction when it comes to learning data representations a popular approach involves the auto encoder chitecture which compresses the data into a tent representation without supervision in this paper we focus on learning text representations because text is a sequence of words to encode a sequence a sequence to sequence encoder li et al kiros et al is ally used in which a rnn is used to encode the input sequence into a xed length representation after which another rnn is used to decode the original input sequence given this representation although the latent representation learned by the auto encoder can be used in stream applications is usually not readable a human readable representation should comply the rule of human grammar and can be comprehended by human therefore in this work it we use comprehensible natural language as a tent representation of the input source text in an auto encoder architecture this human readable latent representation is shorter than the source text in order to reconstruct the source text it must ect the core idea of the source text intuitively the latent representation can be considered a mary of the text so unpaired abstractive rization is thereby achieved the idea that using human comprehensible guage as a latent representation has been plored on text summarization but only in a supervised scenario previous work miao and blunsom uses a prior distribution from a pre trained language model to constrain the erated sequence to natural language however to teach the compressor network to generate text summaries the model is trained using labeled data in contrast in this work we need no labeled data to learn the representation as shown in fig the proposed model is posed of three components a generator a inator and a reconstructor together the generator and reconstructor form a text auto encoder the generator acts as an encoder in generating the tent representation from the input text instead of using a vector as latent representation however the generator generates a word sequence much shorter than the input text from the shorter text the reconstructor reconstructs the original input of the generator by minimizing the tion loss the generator learns to generate short text segments that contain the main information in the original input we use the model in modeling the generator and reconstructor because both have input and output sequences with ent lengths however it is very possible that the ator s output word sequence can only be cessed and recognized by the reconstructor but is not readable by humans here instead of ularizing the generator output with a pre trained language model miao and blunsom we borrow from adversarial auto encoders makhzani al and cycle gan zhu et al and introduce a third component the tor to regularize the generator s output word quence the discriminator and the generator form a generative adversarial network gan fellow et al the discriminator inates between the generator output and written sentences and the generator produces put as similar as possible to human written tences to confuse the discriminator with the gan framework the discriminator teaches the tor how to create human like summary sentences as a latent representation however due to the non differential property of discrete distributions generating discrete distributions by gan is lenging to tackle this problem in this work we proposed a new kind of method on language eration by gan by achieving unpaired abstractive text rization machine is able to unsupervisedly extract the core idea of the documents this approach has many potential applications for example the output of the generator can be used for the stream tasks like document classication and timent classication in this study we evaluate the results on an abstractive text summarization task the output word sequence of the generator is garded as the summaries of the input text the model is learned from a set of documents out summaries as most documents are not paired with summaries for example the movie reviews or lecture recordings this technique makes it ble to learn summarizer to generate summaries for these documents the results show that the ator generates summaries with reasonable quality on both english and chinese corpora related work abstractive text summarization recent model architectures for abstractive text summarization basically use the sequence sequence sutskever et al framework in combination with various novel mechanisms one popular mechanism is attention bahdanau et al which has been shown helpful for rization nallapati al rush et al chopra et al it is also possible to directly optimize evaluation metrics such as rouge lin figure proposed model given long text the generator produces a shorter text as a summary the generator is learned by minimizing the struction loss together with the reconstructor and making discriminator regard its output as written text with reinforcement learning ranzato et al paulus et al bahdanau et al the hybrid pointer generator network see et al selects words from the original text with a pointer vinyals et al or from the whole vocabulary with a trained weight in order to inate repetition a coverage vector tu et al can be used to keep track of attended words and coverage loss see et al can be used to encourage model focus on diverse words while most papers focus on supervised learning with novel mechanisms in this paper we explore supervised training models gan for language generation in this paper we borrow the idea of gan to make the generator output human readable the major challenge in applying gan to sentence tion is the discrete nature of natural language to generate a word sequence the generator usually has non differential parts such as argmax or other sample functions which cause the original gan to fail in gulrajani al instead of feeding a discrete word sequence the authors directly feed the generator output layer to the discriminator this method works because they use the earth mover s distance on gan as proposed in jovsky et al which is able to evaluate the distance between a discrete and a continuous tribution seqgan yu et al tackles the sequence generation problem with reinforcement learning here we refer to this approach as versarial reinforce however the tor only measures the quality of whole sequence and thus the rewards are extremely sparse and the rewards assigned to all the generation steps are all the same mc search yu et al is proposed to evaluate the approximate reward at each time step but this method suffers from high time plexity following this idea li et al poses partial evaluation approach to evaluate the expected reward at each time step in this per we propose the self critical adversarial inforce algorithm as another way to evaluate the expected reward at each time step the formance between original wgan and proposed adversarial reinforce is compared in ment proposed method the overview of the proposed model is shown in fig the model is composed of three ponents generator g discriminator d and constructor both g and r are brid pointer generator networks see et al which can decide to copy words from encoder put text via pointing or generate from vocabulary they both take a word sequence as input and put a sequence of word distributions tor d on the other hand takes a sequence as input and outputs a scalar the model is learned from a set of documents and human written sentences yreal to train the model a training document xt xt where xt is fed to g which outputs resents a word a sequence of word distributions yn where yn is a distribution over all words in the lexicon then we randomly sample a word ys n from each distribution and a word sequence ys ys ys n is obtained according to we feed the sampled word quence ys to reconstructor r which outputs other sequence of word distributions x the constructor r reconstructs the original text from ys that is we seek an output of reconstructor x that is as close to the original text as possible hence the loss for training the reconstructor rloss is dened as ys rloss x k where the reconstruction loss x is the entropy loss computed between the reconstructor output sequence x and the source text or the negative conditional log likelihood of source text given word sequence ys sampled from the reconstructor output sequence x is forced by source text the subscript s in x indicates that x is reconstructed from ys k is the number of training documents and is the mation of the cross entropy loss over all the ing documents in the proposed model the generator g and constructor r form an auto encoder however the reconstructor r does not directly take the tor output distribution as input instead the reconstructor takes a sampled discrete sequence ys as input due to the non differentiable property of discrete sequences we apply the reinforce gorithm which is described in section in addition to reconstruction we need the criminator d to discriminate between the real quence yreal and the generated sequence ys to ularize the generated sequence satisfying the mary distribution d learns to give yreal higher scores while giving ys lower scores the loss for training the discriminator d is denoted as dloss this is further described in section g learns to minimize the reconstruction loss rloss while maximizing the loss of the nator d by generating a summary sequence ys that can not be differentiated by d from the real thing the loss for the generator gloss is gloss rloss loss where loss is highly related to dloss but not necessary the and is a hyper parameter after obtaining the optimal generator by ing we use it to generate summaries generator g and discriminator d together form a gan we use two different adversarial training methods to train d and g as shown in fig these two methods have their own discriminators and discriminator takes the generator put layer as input whereas discriminator takes the sampled discrete word sequence ys as input the two methods are described respectively in sections and minimizing reconstruction loss because discrete sequences are non differentiable we use the reinforce algorithm the ator is seen as an agent whose reward given the source text is x maximizing the ward is equivalent to minimizing the tion loss rloss in however the reconstruction found that if the reconstructor r directly takes as input the generator g learns to put the information about the input text in the distribution of making it difcult to sample meaningful sentences from loss has different formulations in different approaches this will be clear in sections and figure architecture of proposed model the generator network and reconstructor network are a hybrid pointer generator network but for simplicity we omit the pointer and the attention parts from the discriminator for use as a reward signal loss varies widely from sample to sample and thus to the generator the rewards to the generator are not stable either hence we add a baseline to reduce their difference we apply self critical sequence training rennie et al the modied reward x from reconstructor r with the baseline for the tor is method wasserstein gan in the lower left of fig the discriminator model of this method is shown as is a deep cnn with residual blocks which takes a sequence of word distributions as input and outputs a score the discriminator loss dloss is x x x b ya x where x is the baseline is also the same cross entropy reconstruction loss as x except that x is obtained from ya is a word sequence ya instead of ys n where ya n ya ya ya n is selected using the argmax function from the output distribution of generator as in the early training stage the sequence ys barely yields higher reward than sequence ya to encourage exploration we duce the second baseline score b which ally decreases to zero then the generator is dated using the reinforce algorithm with ward x to minimize rloss gan training with adversarial training the generator learns to produce sentences as similar to the human written sentences as possible here we conduct ments on two kinds of methods of language eration with gan in section we directly feed the generator output probability distributions to the discriminator and use a wasserstein gan wgan with a gradient penalty in section we explore adversarial reinforce which feeds sampled discrete word sequences to the inator and evaluates the quality of the sequence dloss k k k k k k where k denotes the number of training ples in a batch and k denotes the k ple the last term is the gradient penalty al we interpolate the tor output layer and the real sample yreal and apply the gradient penalty to the interpolated sequence yi determines the gradient penalty scale in equation for wgan the generator maximizes loss loss k k method self critic adversarial reinforce in this section we describe in detail the posed adversarial reinforce method the core idea is we use the lstm discriminator to ate the current quality of the generated sequence ys ys ys i at each time step the generator knows that compared to the last time step as the generated sentence either improves or worsens it can easily nd the problematic generation step in a long sequence and thus x the problem easily discriminator as shown in fig the is a unidirectional lstm network which takes a crete word sequence as input at time step i given input word ys i it predicts the current score based on the sequence yi the score is viewed as the quality of the current sequence an example of discriminator regularized by weight et al is shown in fig figure when the second arrested appears as the sentence becomes ungrammatical the tor determines that this example comes from the generator hence after this time step it outputs low scores in order to compute the discriminator loss dloss we sum the scores sn of the whole sequence ys to yield n n sn k k k k where n denotes the generated sequence length then the loss of discriminator is dloss k k similar to previous section the last term is dient penalty term with the loss mentioned above the discriminator attempts to quickly mine whether the current sequence is real or fake the earlier the timestep discriminator determines whether the current sequence is real or fake the lower its loss self critical generator since we feed a discrete sequence ys to the the gradient from the discriminator criminator can not directly back propagate to the generator here we use the policy gradient method at timestep i we use the i timestep score from the discriminator as its self critical baseline the reward rd i evaluates whether the quality of quence in timestep i is better or worse than that in timestep i the generator reward rd from i is rd i si if i otherwise however some sentences may be judged as bad sentences at the previous timestep but at later timesteps judged as good sentences and vice versa hence we use the discounted expected ward d with discount factor to calculate the counted reward di at time step i as di jird j n j i to maximize the expected discounted reward the loss of generator is loss eys i we use the likelihood ratio trick to approximate the gradient to minimize i experiment our model was evaluated on the english chinese gigaword datasets and cnn daily mail dataset in section and the experiments were conducted on english gigaword while the ments were conducted on cnn daily mail dataset and chinese gigaword dataset respectively in tions and we used as our evaluation during testing when ing the generator to generate summaries we used beam search with beam and we eliminated repetition we provide the details of the mentation and re processing respectively in appendix a and before jointly training the whole model we pre trained the three major components ator discriminator and reconstructor separately first we pre trained the generator in an vised manner so that the generator would be able to somewhat grasp the semantic meaning of the source text the details of the pre training are in appendix we pre trained the discriminator and reconstructor respectively with the pre trained generator s output to ensure that these two critic networks provide good feedback to the generator used pyrouge package with option to compute rouge score for all experiments task labeled m b trivial baseline c unpaired d semi supervised e transfer learning methods training on generator rush al chopra et al zhou et al pre trained generator wgan adversarial reinforce wgan adversarial reinforce and blunsom wgan adversarial reinforce and blunsom wgan adversarial reinforce pre trained generator wgan adversarial reinforce r l k k m table average rouge scores on english gigaword and r l refers to rouge rouge and rouge l respectively results marked with are obtained from corresponding papers in part a the model was trained supervisedly in row we select the article s rst eight words as its summary part c are the results obtained without paired data in part d we trained our model with few labeled data in part e we pre trained generator on cnn diary and used the summaries from cnn diary as real data for the discriminator english gigaword the english gigaword is a sentence tion dataset which contains the rst sentence of each article and its corresponding headlines the preprocessed corpus contains m training pairs and k validation pairs we trained our model on part of or fully unparalleled data on m ing set to have fair comparison with previous works the following experiments were evaluated on the k testing set same as rush al miao and blunsom we used the sentences in article headlines as real data for as shown in the following experiments the lines can even come from another set of ments not related to the training documents the results on english gigaword are shown in table wgan and adversarial reinforce refer to the adversarial training methods tioned in sections and respectively sults trained by full labeled data are in part a in row we trained our generator by of using general sentences as real data for criminator we chose sentences from headlines because they have their own unique distribution pervised training compared with the previous work zhou et al we used simpler model and smaller vocabulary size we did not try to achieve the state of the art results because the cus of this work is unsupervised learning and the proposed approach is independent to the rization models used in row we simply took the rst eight words in a document as its mary the results for the pre trained generator with method mentioned in appendix c is shown in row in part c we directly took the sentences in the summaries of gigaword as the training data of discriminator compared with the pre trained generator and the trivial baseline the proposed approach rows and showed good provement in fig we provide a real example more examples can be found in the appendix semi supervised learning in semi supervised training generator was trained with few available labeled data during training we conducted teacher forcing with beled data on generator after several updates out labeled data with k k and m beled data the teacher forcing was conducted ery and updates without paired data spectively in teacher forcing given source text as input the generator was teacher forced to dict the human written summary of source text teacher forcing can be regarded as regularization of unpaired training that prevents generator from producing unreasonable summaries of source text we found that if we teacher forced generator too frequently generator would overt on training data since we only used very few labeled data on semi supervised training the performance of semi supervised model in english gigaword regarding available labeled data is shown in table part d we compared our results with miao and blunsom which was the previous state of the art method on supervised summarization task under the same amount of labeled data with both k and m labeled data our method performed better thermore with only m labeled data using versarial reinforce even outperformed vised training in table with the whole m labeled data figure real examples with methods referred in table the proposed methods generated maries that grasped the core idea of the articles cnn daily mail dataset the cnn daily mail dataset is a long text marization dataset which is composed of news ticles paired with summaries we evaluated our model on this dataset because it s a popular mark dataset and we want to know whether the proposed model works on long input and long output sequences the details of processing can be found in appendix b in paired training to prevent the model from directly matching the input articles to its corresponding summaries we split the training pairs into two equal sets one set only supplied articles and the other set only supplied summaries the results are shown in table for vised approaches in part a although our model was similar to see et al due to the smaller vocabulary size we did nt tackle of vocabulary words simpler model architecture shorter output length of generated summaries there was a performance gap between our model and the scores reported in see et al pared to the baseline in part b which took the rst three sentences of articles as summaries the models fell behind that was cause news writers often put the most important information in the rst few sentences and thus even the best abstractive summarization model only slightly beat the baseline on rouge scores however during pre training or training we did nt make assumption that the most tant sentences are in rst few sentences we observed that our unpaired model yielded it yielded lower decent score but and rouge l score that was bly because the length of our generated sequence was shorter than ground truth and our lary size was small another reason was that the generator was good at selecting the most tant words from the articles but sometimes failed to combine them into reasonable sentences cause it s still difcult for gan to generate long sequence in addition since the reconstructor only evaluated the reconstruction loss of whole quence as the generated sequence became long the reconstruction reward for generator became extremely sparse however compared to trained generator rows our model still enhanced the rouge score an real example of generated summary can be found at appendix d transfer learning the experiments conducted up to this point quired headlines unpaired to the documents but in in the same domain to train discriminator this subsection we generated the summaries from english gigaword target domain but the maries for discriminator were from cnn daily mail dataset source domain the results of transfer learning are shown in ble part e table is the result of methods training on our generator see et al baseline see et al c unpaired pre trained generator wgan adversarial reinforce r l table rouge scores on cnn diary mail dataset in row b the rst three sentences were taken as summaries part c are the results obtained without paired data the results with symbol are directly obtained from corresponding papers a training with paired data supervised baseline methods pre trained generator wgan adversarial reinforce r l c unpaired table rouge scores on chinese gigaword in row b we selected the article s rst fteen words as its summary part c are the results obtained without paired data trained generator and the poor pre training result indicates that the data distributions of two datasets are quite different we nd that using sentences from another dataset yields lower rouge scores on the target testing set parts e c due to the mismatch word distributions between the maries of the source and target domains ever the discriminator still regularizes the ated word sequence after unpaired training the model enhanced the rouge scores of the trained model rows and it also surpassed the trivial baselines in part b gan training in this section we discuss the performance of two gan training methods as shown in the table in english gigaword our proposed versarial reinforce method performed better than wgan however in table our proposed method slightly outperformed by wgan in tion we nd that when training with wgan vergence is faster because wgan directly uates the distance between the continuous bution from generator and the discrete distribution from real data the distribution was sharpened at an early stage in training this caused generator to converge to a relatively poor place on the other hand when training with reinforce tor keeps seeking the network parameters that can better fool discriminator we believe that training gan on language generation with this method is worth exploring chinese gigaword the chinese gigaword is a long text tion dataset composed of paired headlines and news unlike the input news in english gigaword the news in chinese gigaword consists of eral sentences the results are shown in table row a lists the results using m summary pairs to directly train the generator out the reconstructor and discriminator this is the upper bound of the proposed approach in row b we simply took the rst fteen words in a ment as its summary the number of words was chosen to optimize the evaluation metrics part c are the results obtained in the scenario out paired data the discriminator took the maries in the training set as real data we show the results of the pre trained generator in row rows and are the results for the two gan training methods respectively we nd that despite the performance gap between the paired and supervised methods rows a the proposed method yielded much better performance than the trivial baselines rows b conclusion and future work using gan we propose a model that encodes text as a human readable summary learned out document summary pairs in future work we hope to use extra discriminators to control the style and sentiment of the generated summaries steven rennie etienne marcheret youssef mroueh jarret ross and vaibhava goel self critical sequence training for image captioning cvpr alexander rush sumit chopra and jason weston a neural attention model for abstractive tence summarization emnlp abigail see peter liu and christopher manning get to the point summarization with generator networks acl ilya sutskever oriol vinyals and quoc le sequence to sequence learning with neural works nips zhaopeng tu zhengdong lu yang liu xiaohua liu and hang li modeling coverage for neural machine translation acl oriol vinyals meire fortunato and navdeep jaitly pointer networks nips lantao yu weinan zhang jun wang and yong yu seqgan sequence generative adversarial nets with policy gradient aaai qingyu zhou nan yang furu wei and ming zhou selective encoding for abstractive sentence summarization acl jun yan zhu taesung park phillip isola and alexei efros unpaired image to image translation using cycle consistent adversarial works arxiv preprint references martin arjovsky soumith chintala and lon arxiv preprint tou wasserstein gan dzmitry bahdanau philemon brakel kelvin xu anirudh goyal ryan lowe joelle pineau aaron courville and yoshua bengio an actor critic algorithm for sequence prediction arxiv preprint dzmitry bahdanau kyunghyun cho and yoshua gio neural machine translation by jointly learning to align and translate iclr sumit chopra michael auli and alexander rush abstractive sentence summarization with tentive recurrent neural networks hlt naac ian goodfellow jean pouget abadie mehdi mirza bing xu david warde farley sherjil ozair aaron courville and yoshua bengio generative versarial networks arxiv preprint ishaan gulrajani faruk ahmed martin arjovsky cent dumoulin and aaron courville proved training of wasserstein gans arxiv preprint ryan kiros yukun zhu ruslan salakhutdinov richard zemel antonio torralba raquel sun and sanja fidler skip thought vectors nips jiwei li minh thang luong and dan jurafsky a hierarchical neural autoencoder for paragraphs and documents acl jiwei li will monroe tianlin shi sbastien jean alan ritter and dan jurafsky adversarial ing for neural dialogue generation arxiv preprint chin yew lin rouge a package for matic evaluation of summaries in text tion branches out acl workshop alireza makhzani jonathon shlens navdeep jaitly ian goodfellow and brendan frey ial autoencoders arxiv preprint yishu miao and phil blunsom language as a latent variable discrete generative models for tence compression emnlp ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre and bing xiang abstractive text summarization using sequence sequence rnns and beyond emnlp romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization arxiv preprint marcaurelio ranzato sumit chopra michael auli and wojciech zaremba sequence level ing with recurrent neural networks iclr a implementation network architecture the model architecture of generator and reconstructor is almost the same except the length of input and output sequence we adapt model architecture for our generator and reconstructor from see et al who used hybrid pointer network for text summarization the hybrid pointer networks of generator and constructor are all composed of two one layer directional lstms as its encoder and decoder spectively with a hidden layer size of since we use two kinds of methods on adversarial ing there are two discriminators with different model architecture in the section the criminator is composed of four residual blocks with hidden dimensions while in section we use only one layer unidirectional lstm with a hidden size of as our discriminator details of training in all experiments except in section we set the weight in ling rloss to in section to prevent tor from overtting to sentences from cnn daily mail summary we set the weight to which was larger than other experiments we nd that the if the value of is too large generator will start to generate output unlike human written tences on the other hand if the value of is too small the sentences generated by generator will sometimes become unrelated to input text of erator for all the experiments the baseline b in gradually decreases from to zero within updates on generator we set the weight of the gradient penalty in section to and used mizer with a learning rate of and on the generator and discriminator respectively in section the weight of gradient penalty terms was and used rmspropoptimizer with a learning rate of and on the ator and discriminator respectively it s also ble to apply weight clipping in discriminator ing but the performance of gradient penalty trick was better b corpus pre processing english gigaword we used the script of rush al to construct our training and testing datasets the vocabulary size was set to k in all experiments cnn diary mail we obtained ing pairs validation pairs and testing pairs identical to see et al by using the scripts provided by see et al to make our model easier to train during training and testing time we truncated input articles to tokens original articles has tokens on average and restricted the length of generator output summaries nal summaries has tokens on average to tokens the vocabulary size was set to chinese gigaword the chinese gigaword is a long text summarization dataset which is composed of m paired data of headlines and news we preprocessed the raw data as following first we selected the k most quent chinese characters to form our lary we ltered out headline news pairs with excessively long or short news segments or that contained too many out of vocabulary chinese characters yielding m news pairs from which we randomly selected k headline news pairs as our testing set k headline news pairs as our validation set and the remaining pairs as our training set ing training and testing the generator took the rst chinese characters of the source text as input c model pre training as we found that the different pre training ods for the generator inuenced nal performance dramatically in all of the experiments we felt it was important to nd a proper unsupervised training method to help the machine grasp mantic meaning the summarization tasks on two datasets is different one is sentence rization while the other is long text tion therefore we used the different pre training strategies on two datasets described below cnn diary mail the cnn diary mail is a long text summarization dataset in which the source text consists of several tences given the previous i sentences from the source text the generator predicted the next four sentences senti in the source text as its pre training target if more than of the words in target sentences senti did not appear in in each cnn diary mail article as tor input and generator randomly predicted one of the sentences of the article s summary in addition we used the one sentence from cnn diary mail summaries as real data to discriminator instead full summaries the given text we ltered out this pre training sample pair this pre training method lowed the generator to capture the tant semantic meanings of the source text although the rst few sentences of articles in cnn diary mail contains the main mation of articles we hope we can provide a more general pre training method which do nt have any assumption of dataset and can be easily applied to other datasets the chinese gigaword pre training method of chinese gigaword was similar to cnn diary mail except that generator predicted the next sentence instead of next consecutive four sentences english chinese gigaword as the source text of english gigaword is made up of only one sentence it is not feasible to split the last sentence from the source text hence the vious pre training method on chinese word is not appropriate for this dataset to properly initialize the set we randomly lected to consecutive words in the source text after which we randomly swapped of the words in the source text given text with incorrect word arrangements the ator predicted the selected words in the rect arrangement we pre trained in this way because we expect the generator to initialize in chinese with a rough language model gigaword we also conducted experiments on pre training in this manner but the results were not as good as those shown in the part c of table in addition we also used the retrieved paired data in row in table to pre train the generator in english word however pre training generator with this method does nt yield results better than those in table transfer learning before unsupervised training the generator was pre trained with paralleled data on cnn daily mail dataset however the characteristics for two datasets in english gigaword the are different ticles were short and the summaries consist of only one sentence while in cnn daily mail dataset the articles were extremely long and summaries consist of several sentences to overcome these differences during training time we took the rst words d examples figure figure figure figure figure figure figure an example of generated summary of cnn diary mail
