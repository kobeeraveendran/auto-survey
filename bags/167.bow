learning encode text human readable summaries generative adversarial networks yau shian wang national taiwan university com hung yi lee national taiwan university com t c o l c s c v v x r abstract auto encoders compress input data latent space representation reconstruct original data representation tent representation easily interpreted humans paper propose training auto encoder encodes input text human readable sentences unpaired stractive summarization achieved auto encoder composed generator reconstructor generator encodes input text shorter word sequence reconstructor recovers generator input generator output erator output human readable discriminator restricts output generator ble human written sentences taking generator output summary text abstractive summarization achieved document summary pairs training data promising results shown glish chinese corpora introduction comes learning data representations popular approach involves auto encoder chitecture compresses data tent representation supervision paper focus learning text representations text sequence words encode sequence sequence sequence encoder li et al kiros et al ally rnn encode input sequence xed length representation rnn decode original input sequence given representation latent representation learned auto encoder stream applications usually readable human readable representation comply rule human grammar comprehended human work use comprehensible natural language tent representation input source text auto encoder architecture human readable latent representation shorter source text order reconstruct source text ect core idea source text intuitively latent representation considered mary text unpaired abstractive rization achieved idea human comprehensible guage latent representation plored text summarization supervised scenario previous work miao blunsom uses prior distribution pre trained language model constrain erated sequence natural language teach compressor network generate text summaries model trained labeled data contrast work need labeled data learn representation shown fig proposed model posed components generator inator reconstructor generator reconstructor form text auto encoder generator acts encoder generating tent representation input text instead vector latent representation generator generates word sequence shorter input text shorter text reconstructor reconstructs original input generator minimizing tion loss generator learns generate short text segments contain main information original input use model modeling generator reconstructor input output sequences ent lengths possible ator s output word sequence cessed recognized reconstructor readable humans instead ularizing generator output pre trained language model miao blunsom borrow adversarial auto encoders makhzani et al cycle gan zhu et al introduce component tor regularize generator s output word quence discriminator generator form generative adversarial network gan fellow et al discriminator inates generator output written sentences generator produces similar possible human written tences confuse discriminator gan framework discriminator teaches tor create human like summary sentences latent representation non differential property discrete distributions generating discrete distributions gan lenging tackle problem work proposed new kind method language eration gan achieving unpaired abstractive text rization machine able unsupervisedly extract core idea documents approach potential applications example output generator stream tasks like document classication timent classication study evaluate results abstractive text summarization task output word sequence generator garded summaries input text model learned set documents summaries documents paired summaries example movie reviews lecture recordings technique makes ble learn summarizer generate summaries documents results ator generates summaries reasonable quality english chinese corpora related work abstractive text summarization recent model architectures abstractive text summarization basically use sequence sequence sutskever et al framework combination novel mechanisms popular mechanism attention bahdanau et al shown helpful rization nallapati et al rush et al chopra et al possible directly optimize evaluation metrics rouge lin figure proposed model given long text generator produces shorter text summary generator learned minimizing struction loss reconstructor making discriminator regard output written text reinforcement learning ranzato et al paulus et al bahdanau et al hybrid pointer generator network et al selects words original text pointer vinyals et al vocabulary trained weight order inate repetition coverage vector tu et al track attended words coverage loss et al encourage model focus diverse words papers focus supervised learning novel mechanisms paper explore supervised training models gan language generation paper borrow idea gan generator output human readable major challenge applying gan sentence tion discrete nature natural language generate word sequence generator usually non differential parts argmax sample functions cause original gan fail gulrajani et al instead feeding discrete word sequence authors directly feed generator output layer discriminator method works use earth mover s distance gan proposed jovsky et al able evaluate distance discrete continuous tribution seqgan yu et al tackles sequence generation problem reinforcement learning refer approach versarial reinforce tor measures quality sequence rewards extremely sparse rewards assigned generation steps mc search yu et al proposed evaluate approximate reward time step method suffers high time plexity following idea li et al poses partial evaluation approach evaluate expected reward time step propose self critical adversarial inforce algorithm way evaluate expected reward time step formance original wgan proposed adversarial reinforce compared ment proposed method overview proposed model shown fig model composed ponents generator g discriminator d constructor r g r brid pointer generator networks et al decide copy words encoder text pointing generate vocabulary word sequence input sequence word distributions tor d hand takes sequence input outputs scalar model learned set documents human written sentences yreal train model training document xt xt xt fed g outputs resents word sequence word distributions yn yn distribution words lexicon randomly sample word ys n distribution word sequence ys ys ys n obtained according feed sampled word quence ys reconstructor r outputs sequence word distributions x constructor r reconstructs original text ys seek output reconstructor x close original text possible loss training reconstructor rloss dened ys rloss x k reconstruction loss x entropy loss computed reconstructor output sequence x source text negative conditional log likelihood source text given word sequence ys sampled reconstructor output sequence x forced source text subscript s x indicates x reconstructed ys k number training documents mation cross entropy loss ing documents proposed model generator g constructor r form auto encoder reconstructor r directly tor output distribution input instead reconstructor takes sampled discrete sequence ys input non differentiable property discrete sequences apply reinforce gorithm described section addition reconstruction need criminator d discriminate real quence yreal generated sequence ys ularize generated sequence satisfying mary distribution d learns yreal higher scores giving ys lower scores loss training discriminator d denoted dloss described section g learns minimize reconstruction loss rloss maximizing loss nator d generating summary sequence ys differentiated d real thing loss generator gloss gloss rloss loss loss highly related dloss necessary hyper parameter obtaining optimal generator ing use generate summaries generator g discriminator d form gan use different adversarial training methods train d g shown fig methods discriminators discriminator takes generator layer input discriminator takes sampled discrete word sequence ys input methods described respectively sections minimizing reconstruction loss discrete sequences non differentiable use reinforce algorithm ator seen agent reward given source text x maximizing ward equivalent minimizing tion loss rloss reconstruction found reconstructor r directly takes input generator g learns information input text distribution making difcult sample meaningful sentences loss different formulations different approaches clear sections figure architecture proposed model generator network reconstructor network hybrid pointer generator network simplicity omit pointer attention parts discriminator use reward signal loss varies widely sample sample generator rewards generator stable add baseline reduce difference apply self critical sequence training rennie et al modied reward x reconstructor r baseline tor method wasserstein gan lower left fig discriminator model method shown deep cnn residual blocks takes sequence word distributions input outputs score discriminator loss dloss x x x b ya x x baseline cross entropy reconstruction loss x x obtained ya word sequence ya instead ys n ya n ya ya ya n selected argmax function output distribution generator early training stage sequence ys barely yields higher reward sequence ya encourage exploration duce second baseline score b ally decreases zero generator dated reinforce algorithm ward x minimize rloss gan training adversarial training generator learns produce sentences similar human written sentences possible conduct ments kinds methods language eration gan section directly feed generator output probability distributions discriminator use wasserstein gan wgan gradient penalty section explore adversarial reinforce feeds sampled discrete word sequences inator evaluates quality sequence dloss k k k k k k k denotes number training ples batch k denotes k ple term gradient penalty et al interpolate tor output layer real sample yreal apply gradient penalty interpolated sequence yi determines gradient penalty scale equation wgan generator maximizes loss loss k k method self critic adversarial reinforce section describe detail posed adversarial reinforce method core idea use lstm discriminator ate current quality generated sequence ys ys ys time step generator knows compared time step generated sentence improves worsens easily nd problematic generation step long sequence x problem easily discriminator shown fig unidirectional lstm network takes crete word sequence input time step given input word ys predicts current score based sequence yi score viewed quality current sequence example discriminator regularized weight et al shown fig figure second arrested appears sentence ungrammatical tor determines example comes generator time step outputs low scores order compute discriminator loss dloss sum scores sn sequence ys yield n n sn k k k k n denotes generated sequence length loss discriminator dloss k k similar previous section term dient penalty term loss mentioned discriminator attempts quickly current sequence real fake earlier timestep discriminator determines current sequence real fake lower loss self critical generator feed discrete sequence ys gradient discriminator criminator directly propagate generator use policy gradient method timestep use timestep score discriminator self critical baseline reward rd evaluates quality quence timestep better worse timestep generator reward rd rd si sentences judged bad sentences previous timestep later timesteps judged good sentences vice use discounted expected ward d discount factor calculate counted reward di time step di jird j n j maximize expected discounted reward loss generator loss eys use likelihood ratio trick approximate gradient minimize ys experiment model evaluated english chinese gigaword datasets cnn daily mail dataset section experiments conducted english gigaword ments conducted cnn daily mail dataset chinese gigaword dataset respectively tions evaluation metric testing ing generator generate summaries beam search beam eliminated repetition provide details mentation processing respectively appendix b jointly training model pre trained major components ator discriminator reconstructor separately pre trained generator vised manner generator able somewhat grasp semantic meaning source text details pre training appendix c pre trained discriminator reconstructor respectively pre trained generator s output ensure critic networks provide good feedback generator pyrouge package option compute rouge score experiments task labeled m b trivial baseline c unpaired d semi supervised e transfer learning methods training generator rush et al chopra et al zhou et al pre trained generator wgan adversarial reinforce wgan adversarial reinforce blunsom wgan adversarial reinforce blunsom wgan adversarial reinforce pre trained generator wgan adversarial reinforce r l k k m table average rouge scores english gigaword r l refers rouge rouge rouge l respectively results marked obtained corresponding papers model trained supervisedly row select article s rst words summary c results obtained paired data d trained model labeled data e pre trained generator cnn diary summaries cnn diary real data discriminator english gigaword english gigaword sentence tion dataset contains rst sentence article corresponding headlines preprocessed corpus contains m training pairs k validation pairs trained model fully unparalleled data m ing set fair comparison previous works following experiments evaluated k testing set rush et al miao blunsom sentences article headlines real data shown following experiments lines come set ments related training documents results english gigaword shown table wgan adversarial reinforce refer adversarial training methods tioned sections respectively sults trained labeled data row trained generator general sentences real data criminator chose sentences headlines unique distribution pervised training compared previous work zhou et al simpler model smaller vocabulary size try achieve state art results cus work unsupervised learning proposed approach independent rization models row simply took rst words document mary results pre trained generator method mentioned appendix c shown row c directly took sentences summaries gigaword training data discriminator compared pre trained generator trivial baseline proposed approach rows showed good provement fig provide real example examples found appendix d semi supervised learning semi supervised training generator trained available labeled data training conducted teacher forcing beled data generator updates labeled data k k m beled data teacher forcing conducted ery updates paired data spectively teacher forcing given source text input generator teacher forced dict human written summary source text teacher forcing regarded regularization unpaired training prevents generator producing unreasonable summaries source text found teacher forced generator frequently generator overt training data labeled data semi supervised training performance semi supervised model english gigaword available labeled data shown table d compared results miao blunsom previous state art method supervised summarization task labeled data k m labeled data method performed better thermore m labeled data versarial reinforce outperformed vised training table m labeled data figure real examples methods referred table proposed methods generated maries grasped core idea articles cnn daily mail dataset cnn daily mail dataset long text marization dataset composed news ticles paired summaries evaluated model dataset s popular mark dataset want know proposed model works long input long output sequences details processing found appendix b paired training prevent model directly matching input articles corresponding summaries split training pairs equal sets set supplied articles set supplied summaries results shown table vised approaches model similar et al smaller vocabulary size nt tackle vocabulary words simpler model architecture shorter output length generated summaries performance gap model scores reported et al pared baseline b took rst sentences articles summaries models fell cause news writers important information rst sentences best abstractive summarization model slightly beat baseline rouge scores pre training training nt assumption tant sentences rst sentences observed unpaired model yielded yielded lower decent score rouge l score bly length generated sequence shorter ground truth lary size small reason generator good selecting tant words articles failed combine reasonable sentences cause s difcult gan generate long sequence addition reconstructor evaluated reconstruction loss quence generated sequence long reconstruction reward generator extremely sparse compared trained generator rows v s model enhanced rouge score real example generated summary found appendix d fig transfer learning experiments conducted point quired headlines unpaired documents domain train discriminator subsection generated summaries english gigaword target domain maries discriminator cnn daily mail dataset source domain results transfer learning shown ble e table result methods training generator et al baseline et al c unpaired pre trained generator wgan adversarial reinforce r l table rouge scores cnn diary mail dataset row b rst sentences taken summaries c results obtained paired data results symbol directly obtained corresponding papers training paired data supervised baseline methods pre trained generator wgan adversarial reinforce r l c unpaired table rouge scores chinese gigaword row b selected article s rst fteen words summary c results obtained paired data trained generator poor pre training result indicates data distributions datasets different nd sentences dataset yields lower rouge scores target testing set parts e v s c mismatch word distributions maries source target domains discriminator regularizes ated word sequence unpaired training model enhanced rouge scores trained model rows v s surpassed trivial baselines b gan training section discuss performance gan training methods shown table english gigaword proposed versarial reinforce method performed better wgan table proposed method slightly outperformed wgan tion nd training wgan vergence faster wgan directly uates distance continuous bution generator discrete distribution real data distribution sharpened early stage training caused generator converge relatively poor place hand training reinforce tor keeps seeking network parameters better fool discriminator believe training gan language generation method worth exploring chinese gigaword chinese gigaword long text tion dataset composed paired headlines news unlike input news english gigaword news chinese gigaword consists eral sentences results shown table row lists results m summary pairs directly train generator reconstructor discriminator upper bound proposed approach row b simply took rst fteen words ment summary number words chosen optimize evaluation metrics c results obtained scenario paired data discriminator took maries training set real data results pre trained generator row rows results gan training methods respectively nd despite performance gap paired supervised methods rows v s proposed method yielded better performance trivial baselines rows v s b conclusion future work gan propose model encodes text human readable summary learned document summary pairs future work hope use extra discriminators control style sentiment generated summaries steven j rennie etienne marcheret youssef mroueh jarret ross vaibhava goel self critical sequence training image captioning cvpr alexander m rush sumit chopra jason weston neural attention model abstractive tence summarization emnlp abigail peter j liu christopher d manning point summarization generator networks acl ilya sutskever oriol vinyals quoc v le sequence sequence learning neural works nips zhaopeng tu zhengdong lu yang liu xiaohua liu hang li modeling coverage neural machine translation acl oriol vinyals meire fortunato navdeep jaitly pointer networks nips lantao yu weinan zhang jun wang yong yu seqgan sequence generative adversarial nets policy gradient aaai qingyu zhou nan yang furu wei ming zhou selective encoding abstractive sentence summarization acl jun yan zhu taesung park phillip isola alexei efros unpaired image image translation cycle consistent adversarial works arxiv preprint references martin arjovsky soumith chintala lon arxiv preprint tou wasserstein gan dzmitry bahdanau philemon brakel kelvin xu anirudh goyal ryan lowe joelle pineau aaron courville yoshua bengio actor critic algorithm sequence prediction arxiv preprint dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly learning align translate iclr sumit chopra michael auli alexander m rush abstractive sentence summarization tentive recurrent neural networks hlt naac ian j goodfellow jean pouget abadie mehdi mirza bing xu david warde farley sherjil ozair aaron courville yoshua bengio generative versarial networks arxiv preprint ishaan gulrajani faruk ahmed martin arjovsky cent dumoulin aaron courville proved training wasserstein gans arxiv preprint ryan kiros yukun zhu ruslan salakhutdinov richard s zemel antonio torralba raquel sun sanja fidler skip thought vectors nips jiwei li minh thang luong dan jurafsky hierarchical neural autoencoder paragraphs documents acl jiwei li monroe tianlin shi sbastien jean alan ritter dan jurafsky adversarial ing neural dialogue generation arxiv preprint chin yew lin rouge package matic evaluation summaries text tion branches acl workshop alireza makhzani jonathon shlens navdeep jaitly ian goodfellow brendan frey ial autoencoders arxiv preprint yishu miao phil blunsom language latent variable discrete generative models tence compression emnlp ramesh nallapati bowen zhou cicero nogueira dos santos caglar gulcehre bing xiang abstractive text summarization sequence sequence rnns emnlp romain paulus caiming xiong richard socher deep reinforced model abstractive marization arxiv preprint marcaurelio ranzato sumit chopra michael auli wojciech zaremba sequence level ing recurrent neural networks iclr implementation network architecture model architecture generator reconstructor length input output sequence adapt model architecture generator reconstructor et al hybrid pointer network text summarization hybrid pointer networks generator constructor composed layer directional lstms encoder decoder spectively hidden layer size use kinds methods adversarial ing discriminators different model architecture section criminator composed residual blocks hidden dimensions section use layer unidirectional lstm hidden size discriminator details training experiments section set weight ling rloss section prevent tor overtting sentences cnn daily mail summary set weight larger experiments nd value large generator start generate output unlike human written tences hand value small sentences generated generator unrelated input text erator experiments baseline b gradually decreases zero updates generator set weight gradient penalty section mizer learning rate generator discriminator respectively section weight gradient penalty terms rmspropoptimizer learning rate ator discriminator respectively s ble apply weight clipping discriminator ing performance gradient penalty trick better b corpus pre processing english gigaword script rush et al construct training testing datasets vocabulary size set k experiments cnn diary mail obtained ing pairs validation pairs testing pairs identical et al scripts provided et al model easier train training testing time truncated input articles tokens original articles tokens average restricted length generator output summaries nal summaries tokens average tokens vocabulary size set chinese gigaword chinese gigaword long text summarization dataset composed m paired data headlines news preprocessed raw data following selected k quent chinese characters form lary ltered headline news pairs excessively long short news segments contained vocabulary chinese characters yielding m news pairs randomly selected k headline news pairs testing set k headline news pairs validation set remaining pairs training set ing training testing generator took rst chinese characters source text input c model pre training found different pre training ods generator inuenced nal performance dramatically experiments felt important nd proper unsupervised training method help machine grasp mantic meaning summarization tasks datasets different sentence rization long text tion different pre training strategies datasets described cnn diary mail cnn diary mail long text summarization dataset source text consists tences given previous sentences source text generator predicted sentences senti source text pre training target words target sentences senti appear cnn diary mail article tor input generator randomly predicted sentences article s summary addition sentence cnn diary mail summaries real data discriminator instead summaries given text ltered pre training sample pair pre training method lowed generator capture tant semantic meanings source text rst sentences articles cnn diary mail contains main mation articles hope provide general pre training method nt assumption dataset easily applied datasets chinese gigaword pre training method chinese gigaword similar cnn diary mail generator predicted sentence instead consecutive sentences english chinese gigaword source text english gigaword sentence feasible split sentence source text vious pre training method chinese word appropriate dataset properly initialize set randomly lected consecutive words source text randomly swapped words source text given text incorrect word arrangements ator predicted selected words rect arrangement pre trained way expect generator initialize chinese rough language model gigaword conducted experiments pre training manner results good shown c table addition retrieved paired data row table pre train generator english word pre training generator method nt yield results better table transfer learning unsupervised training generator pre trained paralleled data cnn daily mail dataset characteristics datasets english gigaword different ticles short summaries consist sentence cnn daily mail dataset articles extremely long summaries consist sentences overcome differences training time took rst words d examples figure figure figure figure figure figure figure example generated summary cnn diary mail
