point abstractive summarization pointer generator networks freek boutkan university amsterdam david rau university amsterdam jorn ranzijn university amsterdam eelco van der wel university amsterdam r l c s c v v x r abstract pointer generator architecture shown big ment abstractive summarization models summaries produced model largely extractive generated sentences copied source text work proposes multihead attention mechanism pointer dropout new loss functions promote abstractive summaries maintaining similar rouge scores multihead tention dropout improve n gram novelty dropout acts regularizer improves rouge score new loss function achieves significantly higher novel n grams sentences cost slightly lower rouge score introduction data available web day instance form news articles scientific publications tracting relevant information increasingly difficult written summary able provide gist text help reduce effort obtaining relevant information automated text summarization fore received lot interest advent deep learning techniques process summarization divided extractive summarization abstractive summarization tractive summarization summary obtained copying relevant parts text abstractive summarization aims distil relevant information source text summary paraphrasing limited use exact phrases source text et al propose new approach abstractive rization combining sequence sequence model pointer network additionally et al incorporate erage mechanism aims tackle problem generation penalizing attention words source document received attention past timesteps issue pointer generator networks test time model focuses mainly source text summary generation introduce novel words resulting summaries tend extractive abstractive et al state focus source text likely caused word word supervision training time possible test time issue pointer generator architecture generator undertrained network learns use pointer mechanism early training arrives local minimum hypothesise factors main contributors reliance pointer mechanism test time limitation dataset evaluation metric rouge discussed detail section abstractive summarization candidate solutions provided dataset rarely contains perfectly viable summaries penalized closely related problem rouge metric produce low score viable summaries problems specific pointer generator model addressing obvious goal research increase number novel grams obtaining similar rouge scores improving abstraction end end trainable text summarization model contributions comprise multihead attention source text dropout mechanism pointer naive pointer regularization pointer regularization based word priors related work abstractive extractive summarization extractive tion fragments source text concatenated generate summary advantage task relatively easy obtain summary good fluency factual correctness contrast abstractive methods allow use synonyms alization rephrasing source text theory lead results closer human generated summaries jing difficult task extractive summarization common problems include factual grammatical mistakes generation words recent work hybrid models proposed combine strengths methods models create abstractive summaries extractive elements promote factual correctness vocabulary oov word generation pointer networks incorporation copying mechanism sequence sequence proved powerful addition summarization tasks copynet pointer generator propose adding mechanism bypass generator network order generate words directly input ument useful cases papers observe balancing strength pointer mechanism generator difficult task pointer generator easier train result generated summary generated directly copying source weber et al confirmed reliance pointer mechanism introduced penalty beam decoding order increase probability generating word generator distribution changes training process clear downside approach reliance solved training time song et al add structural elements copy mechanism possible problem copy mechanism looks semantic information structural information grammatical structure important generating good summaries attention main difference copynet architecture pointer generator copynet uses separate tion distribution pointing generating generator uses et al pose similar information needed pointing generating decoupling distributions lead loss performance popular recent architecture proposed machine translation takes opposite approach vaswani et al propose multi head attention mechanism able learn multiple attention tribution input sequence attention mechanisms merged projected linear layer theoretically code varied representation input sequence compared regular attention mechanism fan et al use multi head attention parable model architecture abstractive summarization multi head mechanisms useful summarization tasks different useful features learnt different attention heads particularly useful generator distribution pointer bution generator identical original architecture model evaluation generated summaries compared provided target summaries rouge score lin indicates recall overlapping n grams generated target summary rouge evaluation metric problematic noted dohare et al rouge scores correlate human judgement tally rouge evaluate restructured sentences proper way rouge matches overlap complete words reconstructed sentences different word forms lead low rouge scores et al shows example valid summary rouge score krantz kalita propose new metric vert pares similarity scores sentences method match exact word forms extend robust matical changes word reordering sentence reconstructions versatile evaluation reduced texts vert similarity dissimilarity sub score sentence vector created reference created summaries cosine larity vectors measure semantic similarity summaries dissimilarity sub score calculated word mover distance algorithm indicates created summary change order match reference mary new metric correlates stronger human judgement compared commonly rouge metric rouge vert measure accuracy ated sentences respect target summary provide insight abstractiveness measure abstractiveness use proportion new n grams generated summary low proportion higher order n grams indicates model copying long phrases input sequence ing extractive way improving rouge novel n grams like contradiction improving rouge decrease number new n grams generated summary rephrased way reference summary directly improving novel n grams policy learning kryciski et al optimize rouge score directly rouge ric differentiable reinforcement techniques policy improvement loss function bines maximum likelihood rouge objective addition abstractive reward added loss reward defined proportion novel n grams generated summary metric bias short summaries needs normalised length ratio generated ground truth summaries achieve similar rouge scores number new n grams increases significantly extractive methods section describe dataset baseline pointer generator network introduce extensions baseline network comprises multi head attention pointing penalty losses pointer dropout mechanism dataset use cnn dailymail dataset hermann et al use preprocessing training splits et al turn uses method nallapati et al training set consists approximately training pairs validation set thousand pairs test set thousand examples average article length tokens summary length average tokens sentences pointer generator network baseline model pointer generator network described et al model allows copying words source document pointing mechanism generation novel words selecting words fixed vocabulary main advantage approach previous methods allows model produce vocabulary words summary generation basic architecture sequence sequence attention model words source document fed sequentially single bidirectional lstm resulting sequence encoder hidden states decoder single layer lstm initialised final hidden states encoder specifically linear layer maps final bidirectional hidden states fixed size output represent initial values decoder time step decoding time step t attention distribution calculated source words et vt wcct bat t vt wh ws wc bat t learnable parameters st refers output decoder time step t hi representation word position produced encoder ct coverage vector et al include coverage mechanism model reduce repetition taking account attention given words source text previous time steps manage significantly decrease repetition produced summaries coverage vector time step t sum attention previous time steps ct t attention distribution indicates words source text relevant produce word summary information stored fixed size representation called context vector h t weighted combination encoder hidden states h t t hi based context vector decoder hidden state ity distribution calculated fixed size vocabulary context vector decoder hidden state concatenated sequently fed linear layers softmax obtain valid probability distribution vocabulary words called pvocab pvocab h t v v b learnable parameters copy tribution source words required order select words source text summary generation et al decided recycle corresponding attention distribution serve pointing distribution probabilities words occurred multiple times source text summed trade copying word help attention distribution generating word pvocab distribution generation probability pen introduced acts soft switch pen mean words pvocab distribution pointing distribution pen opposite effect t en p wt hh t wt s wt bpt r s wt h wt wt bpt r learnable parameters xt refers input decoder time step t sigmoid function document extended vocabulary union words document words fixed vocabulary probability word extended vocabulary defined ppoint pen pen pvocab w ppoint t wi w loss function time step t defined t losst t c t term negative log likelihood target word w second term coverage loss coverage loss introduced penalize repeated attention words reweighted hyperparameter final loss function defined average loss time steps loss t t t dropout mechanism propose dropout mechanism pointer network model dependent pointer mechanism generally dropout simple method prevent overfitting neural works dropping parts network training predefined probability weights set zero training ensures model rely hidden co dependencies generalises better evaluation pointer generator model tends rely pointer mechanism contribution generator network final output probability erage pointer dropout method implemented randomly setting pen probability ing value makes output distribution model output generator expect model rely pointing mechanism use copy mechanism necessary hopefully result model generates abstractive summaries multihead attention original paper pointer generator use exact attention distribution opinion problematic pointer generator carry different functions require different underlying features example generator use syntactical features create correct sentence structure point multiple words create abstract summary contrast pointer attends words wants copy summary order differentiate pointer generator attention distributions supply information pointer mechanism generator use modification head attention mechanism figure shows schematic new pointer generator multi head attention mechanism attention head shared pointer generator generator receives attention heads way naively implemented model minimize cross tropy ppoint high cause model attend uncommon words pointing loss supposed achieve prevent gradient loss term propagated ppoint training experiments section describes experimental setup shared models different model variations tested experimental setup experiments follow setup described et al fixed size vocabulary words source target words models use dimensional word embeddings learned training time hidden states pointer generator kept fixed size input summaries truncated tokens training test time reason important words summary appear beginning articles keeping longer source document decreases performance mary lengths limited tokens training time test time order speed training summaries generated beam search use beam size test time model parameters optimised adagrad learning rate accumulator value proved work best et al gradients clipped maximum norm value regularisation methods model variations baseline model pointer generator network described et al baseline trained coverage coverage mechanism trained separately epochs iterations including beginning turns decrease performance multi head attention mechanism tested heads head produces context vector th size context vector baseline context vector concatenated single vector resulting vector size independent number heads similar approach taken vaswani et al probability dropping pointer mechanism set decision drop pointer holds words summary generation means summaries model rely pointing mechanism pointing losses added end training iterations coverage loss losses experiments conducted different scalars best performing scalars nloss wploss found prior probabilities words calculated rence entire training set prior probabilities words occur generation vocab calculated words probability set zero adaptation baseline model proposed work tested separate addition baseline gives clear estimation influence adaptation leaves influence figure schematic pointer generator multi head attention attention heads head shared attention introducing regularizations pointer mechanism affect shared attention head dedicating rest attention heads generator specific features pointing losses et al model exploits pointing mechanism evaluation hypothesise pointing easier generating sentences model takes shortcut copying lot phrases sentences cases essary discourage network count pointer network add term loss called naive pointing loss add sum pointing probabilities weigh ter way model use pointer network directly contribute higher loss readability define pointer mechanism weight ppoint pen lnaive t t p t point disadvantage relatively simple penalty term word gets penalty words ing desired propose second pointer penalty term word prior pointing loss penalizes pointer word common vocabulary w p p t point p t w w x t p attention distribution pointer mechanism pw pre calculated word prior previous loss p hyper parameter weigh influence loss training t intuitively cross entropy prior pw p expresses surprisal pointing word given word prior prior high word high weight attention distribution loss term high case loss term small desired behaviour loss term ppoint gets mized attending words high prior possible interactions occur time intensive training use hyperparameters models order test results statistical significance form wilcoxon signed rank tests wilcoxon models coverage compared baseline models age tested baseline coverage model statistical tests rouge rouge rouge l conducted p value determine statistical significance vert score correlates strongly rouge scores separate tests needed differences novel n grams generally bigger lower variance rouge scores statistical tests needed large test set examples results table mean rouge vert scores tested els examples testset models trained epoch coverage steps nloss sponds naive pointing loss wploss word prior pointing loss scores star significantly ent baseline best results marked bold attention heads model extensions rouge rouge rouge l vert dropout dropout baseline baseline et al nloss nloss wploss wploss table average rouge scores reported models table shows novel n grams sentences models trained coverage results coverage included appendix obtained baseline rouge scores average points lower reported et al paper use pytorch implementation perform eter tuning optimize score reference compare models use baseline trained com lipiji neural summ cnndm pytorch table percentage novel n grams sentences produced tested models best results marked bold attention heads model extensions grams grams grams grams sentences dropout dropout baseline target summaries nloss nloss wploss wploss average multi head obtains slightly worse rouge scores simple case undertraining sub optimal choice hyperparameters notice case word prior model multi head architecture performs better possible explanation decrease weights pointer head hurt rouge score model relies pointer generator gets important task multi head beneficial hypothesis needs extensive training tuning prove scope research dropout single head model achieves slightly higher rouge scores increase n gram novelty multi head dropout model significantly different baseline proposed losses greatly improve number novel grams sentences especially noticeable case word prior loss number novel n grams double cases increase novel n grams decreases rouge score example clearly observe model favours generating pointing predicting simple words like articles prepositions common words like names uncommon nouns pointer model pointing table shows average pen train test time model uses pointer mechanism average baseline pen average line findings et al multihead change behaviour dropout model uses generator cantly training test time falls value baseline loss functions greatly increase generator expected actively penalizing pointer mechanism pointer mechanism test time table average value pen end training test time model baseline heads dropout nloss wploss train pen test pen examples appendix b pen value ated word baseline new losses model trained word prior loss shows achieves higher average pen common words articles verbs example fragments low pen ellie meredith syndrome let fragments cases want model point fragment clear inspection source article fragment starts direct quote article let s party like s table frequent novel words best model heads coverage word prior loss says scored beat diagnosed found unk premier year said taken boss novel words investigate novel words quent new words abstractive model multihead erage wploss calculated tab noted majority words verbs sentence rephrased happen root verb suffix similarly tense verb changes introduce new words suggests newly introduced words valid rephrases random words note tokens result incorrect parsing unk s tokens average reference summary length tokens length generated summaries approximately models single attention head word prior loss models including baseline models generated summary average longer target maries maximum allowed length generating summaries new n grams result simply generating text average length change instead novel n grams replace non novel n grams observation type newly generated words seen table gests novel n grams valid rephrasings random words model artefacts discussion multi head evaluation multi head attention mechanism improves results new loss function measurements rouge l score creases nloss wploss novel n grams drop slightly losses shows penalizing pointer mechanism attention pointer generator shared reduce overall quality summaries indicates multi head mechanism working intended splitting pointer generator attentions penalizing pointer affects generator figure shows kl divergence head head attention cell j corresponds dk column kl divergence attention tributions multi head attention distribution model attention head shown read plot head pointer head average different heads multi head means average attend different words source text perform different function generating words hand pointer head similar tion distribution single head model additionally heads multi head similar single head eachother result indicates single head model tempts incorporate information needed pointing generating desirable split information multiple heads figure average kl divergence pair heads multi head model head multi head single head model column dropout models dropout mechanism increased rouge score having lowered amounts novel n grams exact opposite expected possible explanation dropout produce gradients indicate ing mechanism wrong slows training force generator reliable leads better scores general line idea et al generator optimally trained reliance pointer dropout similar baseline model expect dropout generator opportunity better trained coverage ablation studies understand relationship coverage loss proposed losses trained models coverage nism tables appendix rouge l score decreases points models coverage loss seen number novel n grams creases significantly matches expectations n grams reflect order words observe decrease novel grams suggests coverage nism favours extractive summarization stands contrast goal abstractive model claim supported table shows introducing coverage mechanism reduces repetition wploss nloss result duplicate n grams losses interfere coverage coverage reduces repetition expense tractive summaries penalizing pointing mechanism introduces new n grams increases problem overgeneration table duplicated n grams summaries head models cov stands models trained coverage model extensions grams grams grams grams sentences baseline nloss wploss baseline nloss wploss cov cov cov pointer generator distributions examining examples produced model notice high pen model copying sentences effect seen figure appendix b sentence generator copy line source article issue pointing distribution lower cardinality compared generator distribution case orders magnitude smaller pointer generator means generally bias words source text high values pen conclude pointer mechanism definition decreases novel gram score makes model abstractive instead learning soft switch pen introduces bias hard pointing mechanism learnt reinforcement learning gumbel softmax approximation diminish bias rouge metric dataset aforementioned problems rouge limitation evaluation abstractive summarization models idea abstractive summarization valid summaries created ways rouge measures exact overlap set references summaries summaries created models means set reference summaries representative valid ways abstractive summaries produced need new metric rely exact overlap semantic similarity conclusion work investigated additions generator framework order improve abstraction taining similar summary quality multi head mechanism learns different features pointing generating improve rouge score dropout pointer mechanism multi head attention promote novel n grams produced summary models results similar dropout mechanism opposite expected novel n grams decreased rouge scores increased likely dropout partly removes reliance pointer gives improved performance compared baseline introduced loss functions prove generation novel n grams significantly word prior loss observe improvement novel tences compared baseline model cases manage maintain rouge scores problem loss functions training process vert metric looked like promising metric measure semantic similarity generated target summaries resulting vert scores completely correlated rouge metric produce new insights goal train model abstractive number novel n grams respect reference summary measure novel n grams score easily increased adding random words summary lead abstractive higher quality summaries metric evaluate abstractive summarization tasks effectively possible claim summaries abstractive based increase new n grams shown summary length increase suggests new words replace words instead adding words shown novel words plausible words rephrasing summarization tasks future work appears newly introduced losses interfere age mechanism increase generation problem case introducing coverage new loss point training produces gradients different training losses interfere vergence network future work interaction new loss function coverage investigated difference pointing behaviour training ference reduced scheduled teacher forcing gradually decreases frequency model receives ground truth input generator reduces difference training inference result higher values pen references mehdi allahyari seyedamin pouriyeh mehdi assefi saeid safaei elizabeth d trippe juan b gutierrez krys kochut text summarization techniques brief survey arxiv preprint tal baumel matan eyal michael elhadad query focused tive summarization incorporating query relevance multi document age summary length constraints models arxiv preprint samy bengio oriol vinyals navdeep jaitly noam shazeer scheduled sampling sequence prediction recurrent neural networks advances neural information processing systems shibhansh dohare harish karnick vivek gupta text summarization abstract meaning representation arxiv preprint john duchi elad hazan yoram singer adaptive subgradient methods online learning stochastic optimization j mach learn res july acm org citation lisa fan dong yu lu wang robust neural abstractive tion systems evaluation adversarial information arxiv preprint jiatao gu zhengdong lu hang li victor ok li incorporating copying mechanism sequence sequence learning arxiv preprint karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend advances neural information processing systems eric jang shixiang gu ben poole categorical reparameterization gumbel softmax arxiv preprint hongyan jing hidden markov modeling decompose written summaries computational linguistics diederik p kingma jimmy ba adam method stochastic tion corr org jacob krantz jugal kalita abstractive summarization attentive neural techniques arxiv preprint wojciech kryciski romain paulus caiming xiong richard socher improving abstraction text summarization arxiv preprint alex m lamb anirudh goyal alias parth goyal ying zhang saizheng zhang aaron c courville yoshua bengio professor forcing new algorithm training recurrent networks advances neural information processing systems piji li wai lam lidong bing zihao wang deep recurrent generative decoder abstractive text summarization arxiv preprint chin yew lin summaries rouge package automatic evaluation rouge package automatic evaluation microsoft com en research ramesh nallapati bowen zhou caglar gulcehre bing xiang al stractive text summarization sequence sequence rnns arxiv preprint abigail peter j liu christopher d manning point summarization pointer generator networks arxiv preprint kaiqiang song lin zhao fei liu structure infused copy mechanisms abstractive summarization arxiv preprint nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov dropout simple way prevent neural networks overfitting journal machine learning research ilya sutskever oriol vinyals quoc v le sequence sequence learning neural networks advances neural information processing systems ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention need advances neural information processing systems oriol vinyals meire fortunato navdeep jaitly pointer networks advances neural information processing systems noah weber leena shekhar niranjan balasubramanian kyunghyun cho controlling decoding abstractive summaries copy based networks arxiv preprint frank wilcoxon individual comparisons ranking methods biometrics bulletin appendices scores coverage table mean rouge vert scores tested models examples testset models trained coverage nloss corresponds naive pointing loss wploss word prior pointing loss attention heads model extensions rouge rouge rouge l vert baseline et al dropout dropout nloss nloss wploss wploss table percentage novel n grams sentences produced tested models attention heads model extensions grams grams grams grams sentences dropout dropout target summaries nloss nloss wploss wploss b examples following pages contain randomly chosen examples multihead model coverage combined new losses words highlighted red reflect overall attention model paid word constructing summary italic words denote vocabulary words green shading intensity represents value generation probability pen figure example average pen higher word prior loss model words low prior ellie meredith syndrome figure example shows clearly new losses introduce generation problems coverage loss aimed solve figure example average pen word significantly higher wploss model generated summaries figure sentence wploss example average uses generator exact copy line source article
