point less more abstractive summarization with pointer generator networks freek boutkan university of amsterdam david rau university of amsterdam jorn ranzijn university of amsterdam eelco van der wel university of amsterdam r a l c s c v v i x r a abstract the pointer generator architecture has shown to be a big ment for abstractive summarization models however the summaries produced by this model are largely extractive as over of the generated sentences are copied from the source text this work proposes a multihead attention mechanism pointer dropout and two new loss functions to promote more abstractive summaries while maintaining similar rouge scores both the multihead tention and dropout do not improve n gram novelty however the dropout acts as a regularizer which improves the rouge score the new loss function achieves significantly higher novel n grams and sentences at the cost of a slightly lower rouge score introduction more data is becoming available on the web every day for instance in the form of news articles and scientific publications and tracting the most relevant information is becoming increasingly difficult a well written summary should be able to provide the gist of a text and can help to reduce the effort of obtaining all the relevant information automated text summarization has fore received a lot of interest since the advent of deep learning techniques the process of summarization is often divided into extractive summarization and abstractive summarization in tractive summarization a summary is obtained by copying the relevant parts of the text abstractive summarization aims to distil the relevant information from the source text into a summary by paraphrasing and is therefore not limited to the use of the exact phrases of the source text see et al propose a new approach for abstractive rization by combining a sequence to sequence model with a pointer network additionally see et al incorporate a erage mechanism that aims to tackle the problem of over generation by penalizing attention to words in the source document that have already received attention in past timesteps one issue with pointer generator networks is that during test time the model focuses mainly on the source text during summary generation and does not introduce many novel words the resulting summaries therefore tend to be more extractive than abstractive see et al state that this focus on the source text is likely caused by the word by word supervision during training time which is not possible during test time another issue with the pointer generator architecture is that the generator is undertrained as the network learns to use the pointer mechanism early in training and arrives at a local minimum we hypothesise that these two factors are the main contributors to the over reliance on the pointer mechanism during test time another limitation is the used dataset and evaluation metric rouge this is discussed in detail in section in abstractive summarization there are many candidate solutions however the provided dataset rarely contains all of these and perfectly viable summaries are sometimes penalized this is closely related to the problem with the rouge metric as this can also produce low score for viable summaries these problems are not specific to the pointer generator model and addressing them is less obvious the goal of this research to increase the number of novel grams while obtaining similar rouge scores therefore improving abstraction in an end to end trainable text summarization model our contributions comprise of multihead attention over the source text dropout mechanism over the pointer naive pointer regularization pointer regularization based on word priors related work abstractive and extractive summarization in extractive tion fragments of the source text are concatenated to generate a summary an advantage of this task is that it is relatively easy to obtain a summary with good fluency and factual correctness in contrast abstractive methods allow for the use of synonyms alization and rephrasing of the source text while in theory this can lead to results that are closer to human generated summaries jing it is a much more difficult task than extractive summarization common problems include factual and grammatical mistakes but also over under generation of words in more recent work hybrid models are proposed to combine the strengths of both methods these models can create abstractive summaries with extractive elements to promote factual correctness and out of vocabulary oov word generation pointer networks the incorporation of a copying mechanism to the sequence to sequence has proved to be a powerful addition for summarization tasks both the copynet and pointer generator propose adding such a mechanism to bypass the generator network in order to generate words directly from the input ument while this is useful in many cases both papers observe balancing the strength of the pointer mechanism and the generator is a difficult task the pointer generator seems easier to train and as a result most of the generated summary is generated by directly copying from the source weber et al confirmed the over reliance on the pointer mechanism and introduced a penalty during beam decoding in order to increase the probability of generating a word from the generator distribution however no changes are made to the training process and the clear downside of this approach is that the over reliance is not solved during training time but only afterwards song et al add structural elements to the copy mechanism they say a possible problem of the copy mechanism is that it only looks at semantic information while structural information such as grammatical structure might be more important for generating good summaries attention a main difference between the copynet architecture and pointer generator is that copynet uses a separate tion distribution for pointing and generating while the generator only uses one see et al pose that similar information is needed for both pointing and generating and that decoupling the two distributions might lead to a loss in performance however a popular recent architecture proposed for machine translation takes an opposite approach vaswani et al propose a multi head attention mechanism which is able to learn multiple attention tribution over an input sequence these attention mechanisms are merged and projected with a linear layer and can theoretically code a more varied representation of the input sequence compared to the regular attention mechanism fan et al are the first to use multi head attention with a parable model architecture for abstractive summarization they show that multi head mechanisms are useful for summarization tasks and that different useful features are learnt by the different attention heads this could be particularly useful for the generator since the distribution used by the pointer and the bution for the generator are identical in the original architecture model evaluation generated summaries will be compared against provided target summaries the rouge score lin indicates the recall of overlapping n grams between the generated and target summary using rouge as the evaluation metric is problematic as has been noted by dohare et al not only because rouge scores do not correlate with human judgement but more tally because rouge can not evaluate restructured sentences in a proper way rouge matches overlap in complete words and in reconstructed sentences different word forms can be used which might lead to low rouge scores see et al shows an example of a valid summary that has a rouge score of krantz and kalita propose a new metric vert that pares similarity scores of sentences since this method does not match exact word forms and it is to some extend robust to matical changes word reordering and sentence reconstructions versatile evaluation of reduced texts vert is made up out of a similarity and dissimilarity sub score a sentence vector is created out of the reference and created summaries and the cosine larity between these two vectors is measure of semantic similarity between the summaries the dissimilarity sub score is calculated using the word mover distance algorithm that indicates how much a created summary has to change in order to match a reference mary this new metric correlates stronger with human judgement compared to the commonly used rouge metric both rouge and vert only measure the accuracy of the ated sentences with respect to the target summary but they provide no insight in the abstractiveness to measure abstractiveness we use the proportion of new n grams in the generated summary a low proportion of higher order n grams indicates that the model is copying long phrases from the input sequence and is thus ing in a more extractive way improving both rouge and novel n grams seems like a contradiction since improving rouge will decrease the number of new n grams if the generated summary is not rephrased in the same way as the reference summary directly improving novel n grams using policy learning kryciski et al optimize the rouge score directly since the rouge ric is not differentiable this can only be done by using reinforcement techniques such as policy improvement the loss function bines the maximum likelihood and rouge objective in addition an abstractive reward is added to the loss this reward is defined as the proportion of novel n grams in the generated summary this metric has a bias towards very short summaries and needs to be normalised using the length ratio of the generated and ground truth summaries they achieve similar rouge scores as but show that the number of new n grams increases significantly and thus is less extractive methods in this section we describe our dataset and the baseline pointer generator network then we introduce our extensions over the baseline network which comprises our multi head attention pointing penalty losses and pointer dropout mechanism dataset we use the cnn dailymail dataset hermann et al we use the same preprocessing and training splits as see et al which in turn uses the method from nallapati et al the training set consists of approximately training pairs with a validation set of thousand pairs and test set of thousand examples the average article length is tokens and the summary length is on average tokens sentences pointer generator network the baseline model is the pointer generator network described by see et al this model allows for copying of words from the source document using a pointing mechanism and also generation of novel words by selecting words from a fixed vocabulary the main advantage of this approach over previous methods is that it allows the model to produce out of vocabulary words during summary generation the basic architecture is a sequence to sequence attention model words from the source document are fed sequentially into a single bidirectional lstm resulting in a sequence of encoder hidden states the decoder is a single layer lstm that is initialised with the final hidden states of the encoder more specifically a linear layer maps the final bidirectional hidden states to a fixed size output that represent the initial values of the decoder at the first time step during the decoding at time step t an attention distribution is calculated over the source words et i vt wcct i bat t at here vt wh ws wc bat t are learnable parameters st refers to the output of the decoder at time step t and hi is the representation of the word at position i produced by the encoder ct i is the coverage vector see et al include a coverage mechanism in their model to reduce the amount of repetition by taking into account the amount of attention that has been given to words from the source text in previous time steps they manage to significantly decrease repetition in the produced summaries the coverage vector at time step t is just the sum of attention of the previous time steps ct t the attention distribution indicates which words from the source text are relevant to produce the next word of the summary this information is stored in a fixed size representation called the context vector h t that is a weighted combination of the encoder hidden states a h t t i hi i based on the context vector and decoder hidden state a ity distribution is calculated over the fixed size vocabulary the context vector and decoder hidden state are concatenated and sequently fed through two linear layers and a softmax to obtain a valid probability distribution over the vocabulary words called pvocab pvocab h t here v v and b are the learnable parameters a copy tribution over the source words is also required in order to select words from the source text during summary generation see et al decided to recycle the corresponding attention distribution and also made it serve as the pointing distribution the probabilities of the words that occurred multiple times in the source text were summed a trade off has to be made between copying a word with the help of the attention distribution and generating a word by the pvocab distribution therefore a generation probability pen was introduced that acts as a soft switch a pen of would mean that only words from the pvocab distribution can be used and none from the pointing distribution while a pen of has the opposite effect t en p wt hh t wt s wt bpt r s wt h wt here wt bpt r are learnable parameters xt refers to the input of the decoder at time step t and is the sigmoid function for every document there is an extended vocabulary that is the union of the words in that document and all the words in fixed vocabulary now the probability of a word in this extended vocabulary is defined as where ppoint pen pen pvocab w ppoint a t i i wi w the loss function at time step t is defined as t i losst t c t i i where the first term is the negative log likelihood of target word w and the second term is the coverage loss this coverage loss is introduced to penalize repeated attention to the same words and is reweighted by a hyperparameter the final loss function is defined as the average loss over all time steps loss t t t dropout mechanism we propose a dropout mechanism on the pointer network to make the model less dependent on the pointer mechanism generally dropout is a simple method to prevent overfitting in neural works by dropping parts of the network during training with a predefined probability weights are set to zero during training this ensures that the model can not rely on hidden co dependencies and generalises better during evaluation the pointer generator model tends to rely to much on the pointer mechanism the contribution of the generator network to the final output probability is on erage only our pointer dropout method can be implemented by randomly setting pen with probability to during ing where a value of makes the output distribution of the model the same as the output of the generator we expect the model to rely less on the pointing mechanism and use the copy mechanism only when necessary hopefully this would result in a model that generates more abstractive summaries multihead attention in the original paper the pointer and the generator make use of the exact same attention distribution in our opinion this is problematic because pointer and generator carry out different functions that require different underlying features for example the generator might use syntactical features to create a correct sentence structure or point to multiple words to create a more abstract summary in contrast the pointer only attends to words that it wants to copy to the summary in order to both differentiate between pointer and generator attention distributions but still supply all information of the pointer mechanism to the generator we use a modification of the head attention mechanism figure shows a schematic of our new pointer generator multi head attention mechanism where the first attention head is shared between the pointer and the generator whereas the generator receives all attention heads this way by naively implemented the model could also minimize the cross tropy when ppoint is high this would cause the model to attend to uncommon words while pointing which is not what the loss is supposed to achieve to prevent this the gradient of this loss term is only back propagated to ppoint during training experiments this section describes the experimental setup that is shared between all of the models and also the different model variations that are tested experimental setup all the experiments follow the same setup as described by see et al a fixed size vocabulary of words is used for both the source and target words all models use dimensional word embeddings that are learned during training time and hidden states for the pointer and generator are kept at a fixed size of the input summaries are truncated to tokens during training and test time the reason for this is that the most important words for the summary appear in the beginning of the articles and keeping longer source document even decreases performance mary lengths are limited to tokens during training time and during test time in order to speed up training summaries are generated using beam search and use a beam size of at test time all the model parameters are optimised with adagrad using a learning rate of and accumulator value of as this proved to work best for see et al gradients are clipped with a maximum norm value of and no further regularisation methods are used model variations the baseline model is the pointer generator network described by see et al the baseline is trained with and without coverage where the coverage mechanism is trained separately after epochs for iterations as including this from the beginning turns out to decrease performance the multi head attention mechanism is tested with four heads every head produces a context vector that is th of the size of the context vector of the baseline next these context vector are concatenated into a single vector resulting in a vector of the same size that is independent on the number of heads this is similar to the approach taken by vaswani et al the probability for dropping out the pointer mechanism is set to the decision to drop out the pointer holds for all the words during summary generation this means that for some summaries the model can not rely on the pointing mechanism at all the pointing losses are added at the end of training and for the same amount of iterations as the coverage loss for both losses experiments were conducted with different scalars as best performing scalars for nloss and for wploss were found prior probabilities of the words are calculated on the rence in the entire training set also only the prior probabilities for words that occur in the generation vocab are calculated for the other words this probability is set to zero every adaptation to the baseline model that is proposed in this work is tested as a separate addition to the baseline this gives a clear estimation of the influence of each adaptation although it leaves out the influence figure a schematic of the pointer generator multi head attention with four attention heads where the first head is used as shared attention introducing regularizations to the pointer mechanism we only affect the shared attention head while dedicating the rest of the attention heads to generator specific features pointing losses see et al show that the model exploits the pointing mechanism during evaluation we hypothesise this is because pointing is easier than generating sentences the model takes a shortcut by copying a lot of phrases and sentences in cases where this might not be essary to discourage the network to count on the pointer network we add a term to the loss called the naive pointing loss we add the sum of all pointing probabilities and weigh it with a ter this way the model can still use the pointer network but it will directly contribute to a higher loss for readability we define the pointer mechanism weight ppoint pen lnaive t t p t point a disadvantage of this relatively simple penalty term could be that every word gets the same penalty even for words where ing is desired we propose a second pointer penalty term the word prior pointing loss that only penalizes the pointer when a word is common in the vocabulary w p p t point p t a w w x t a where p is the attention distribution used for the pointer mechanism and pw is a pre calculated word prior as with the previous loss p is a hyper parameter to weigh the influence of the loss during training t intuitively the cross entropy between the prior pw and p a expresses the surprisal of not pointing to a word given the word prior if the prior is high and the word has a high weight in the attention distribution this loss term will be high in any other case the loss term will be small the desired behaviour of this loss term is that ppoint gets mized when attending to words with a high prior however when of possible interactions that might occur due to the time intensive training we use the same hyperparameters for all models in order to test our results for statistical significance we form wilcoxon signed rank tests wilcoxon all models without coverage are compared to the baseline whereas models with age are tested against the baseline coverage model statistical tests for rouge rouge and rouge l are conducted a p value of is used to determine statistical significance since the vert score correlates strongly with the rouge scores separate tests are not needed the differences in novel n grams are generally much bigger and have lower variance than rouge scores so statistical tests are not needed for a large test set examples results table mean rouge and vert scores of the tested els examples in testset all models were trained from epoch on with coverage for steps here nloss sponds to naive pointing loss and wploss to the word prior pointing loss scores with a star are not significantly ent from the baseline best results are marked in bold attention heads model extensions rouge rouge rouge l vert dropout dropout baseline baseline see et al nloss nloss wploss wploss in table the average rouge scores are reported on all models and table shows the amount of novel n grams and sentences these models are all trained with coverage results without coverage are included in appendix a the obtained baseline rouge scores are on average points lower than reported in see et al paper however we use a pytorch re implementation and did not perform any eter tuning to optimize this score as a reference to compare our models to we therefore use the baseline that we trained ourselves com lipiji neural summ cnndm pytorch table percentage of novel n grams and sentences that are produced for each of the tested models best results are marked in bold attention heads model extensions grams grams grams grams sentences dropout dropout baseline target summaries nloss nloss wploss wploss on average multi head obtains slightly worse rouge scores this can be a simple case of undertraining and sub optimal choice of hyperparameters however we notice that in case of the word prior model the multi head architecture performs better a possible explanation is that the decrease of weights in the pointer head might hurt the rouge score if the model mostly relies on the pointer and when the generator gets a more important task the multi head might be beneficial this hypothesis needs more extensive training and tuning to prove and is not in the scope of this research dropout on the single head model achieves slightly higher rouge scores and does increase n gram novelty the multi head dropout model is not significantly different from the baseline both proposed losses greatly improve the number of novel grams and sentences this is especially noticeable in case of the word prior loss the number of novel n grams is more than double however in both cases this increase in novel n grams decreases the rouge score in example we can clearly observe that the model favours generating over pointing when predicting simple words like articles or prepositions on less common words like names and uncommon nouns the pointer is still used is the model pointing less table shows the average pen during train and test time which show how much the model uses the pointer mechanism on average the baseline has a pen of on average which is in line with the findings of see et al the multihead does not change this behaviour while the dropout model uses the generator cantly more during training during test time it falls back to the same value as the baseline both loss functions greatly increase the amount the generator is used this is to be expected when actively penalizing the pointer mechanism the pointer mechanism is used less at test time table average value of pen during the end of training and test time model baseline heads dropout nloss wploss train pen test pen the examples in appendix b show the pen value on each ated word for the baseline and two new losses the model trained word prior loss shows that it achieves a much higher average pen on common words such as articles and verbs in the first example only three fragments have a low pen ellie meredith down syndrome and let the first two fragments are cases where we want the model to point whereas the third fragment is less clear on inspection of the source article this fragment starts a direct quote from the article let s party like it s table most frequent novel words for the best model heads coverage and word prior loss says scored been he beat has diagnosed say found unk premier year said taken boss since by novel words to investigate the novel words that are used further the most quent new words for the most abstractive model multihead erage and wploss is calculated tab it can be noted that the majority of the words are verbs when a sentence is rephrased it can happen that the same root of a verb is used but with another suffix similarly when the tense of a verb changes this can introduce new words this suggests that the newly introduced words are valid rephrases and not random words note that the tokens and are the result of incorrect parsing of unk and s tokens the average reference summary length is tokens the length of the generated summaries is approximately for models with a single attention head and word prior loss and around for all other models including the baseline for none of the models the generated summary was on average longer than the target maries the maximum allowed length for generating summaries is the new n grams are thus not a result of simply generating more text because the average length does not change instead the novel n grams replace non novel n grams this observation and the type of newly generated words as can be seen in table gests that the novel n grams are valid rephrasings and not random words or model artefacts discussion multi head evaluation the multi head attention mechanism improves the results of the new loss function in our measurements the rouge l score creases by nloss and wploss but the novel n grams drop slightly for both losses this shows that penalizing the pointer mechanism when the attention between pointer and generator is shared can reduce the overall quality of summaries which indicates the multi head mechanism is working as intended by splitting the pointer and generator attentions penalizing the pointer affects the generator less figure shows the kl divergence between each head of the head attention where cell i j corresponds to dk in the last column the kl divergence between the attention tributions of the multi head and the attention distribution of the same model with just one attention head is shown we can read from the plot that the head used for the pointer head is on average very different from the other heads in the multi head this means that on average they attend to different words in the source text and perform a different function when generating words on the other hand the pointer head is most similar to the tion distribution in the single head model additionally the other heads in the multi head are more similar to the single head than to eachother this result indicates that the single head model tempts to incorporate information needed for both pointing and generating but that it can be desirable to split this information into multiple heads figure the average kl divergence between each pair of heads of the multi head model and between each head of the multi head and the single head model last column dropout models that used the dropout mechanism show an increased rouge score while having lowered amounts of novel n grams this is the exact opposite of what was expected a possible explanation is that dropout does not produce gradients that indicate that the ing mechanism is wrong but that it only slows its training still it does force the generator to be more reliable and this leads to better scores in general this is in line with the idea of see et al that the generator might not be optimally trained because of over reliance on the pointer using dropout is therefore very similar to the baseline model expect that dropout does give the generator the opportunity to be better trained coverage ablation studies to understand the relationship between the coverage loss and our proposed losses we trained all models without the coverage nism tables and in the appendix show that the rouge l score decreases about points for all models without using the coverage loss further it can be seen that the number of novel n grams creases significantly this matches our expectations for n grams which reflect the order of words however we can also observe a decrease in novel grams which suggests that the coverage nism favours extractive summarization which stands in contrast to our goal towards a more abstractive model this claim is supported by table which shows that introducing the coverage mechanism reduces the amount of repetition whereas both wploss and nloss result in more duplicate n grams the losses thus interfere with coverage coverage reduces repetition at the expense of more tractive summaries penalizing the pointing mechanism introduces new n grams but also increases the problem of overgeneration table duplicated n grams within summaries for head models where cov stands for models that have been trained with coverage model extensions grams grams grams grams sentences baseline nloss wploss baseline nloss wploss cov cov cov pointer and generator distributions when examining the examples produced by the model we notice that even with a high pen the model is copying full sentences this effect can be seen in figure appendix b the last sentence is mostly made with the generator but is a copy of line in the source article an issue of the pointing distribution is that it has a much lower cardinality compared to the generator distribution in our case it is two orders of magnitude smaller for the pointer for the generator this means that there is generally a bias towards words from the source text even with high values of pen we can conclude that the pointer mechanism by definition decreases the novel gram score and makes the model less abstractive instead of learning the soft switch pen which introduces this bias a hard pointing mechanism could be learnt by reinforcement learning or with a gumbel softmax approximation to diminish this bias rouge metric and dataset the aforementioned problems of rouge are a serious limitation to the evaluation of abstractive summarization models the idea of abstractive summarization is that valid summaries can be created in many ways however rouge measures the exact overlap between a set of references summaries and summaries created by our models this means that either the set of reference summaries should be representative of all valid ways in which abstractive summaries can be produced or that there is a need for a new metric that does not rely on exact overlap but rather on semantic similarity conclusion in this work we investigated several additions to the generator framework in order to improve abstraction while taining similar summary quality while the multi head mechanism learns different features for pointing and generating it does not improve the rouge score when dropout is used on the pointer mechanism the multi head attention does promote novel n grams in the produced summary but in other models the results are very similar the dropout mechanism does the opposite of what was expected as the amount of novel n grams decreased while the rouge scores increased it seems likely that dropout partly removes the reliance on the pointer and therefore gives improved performance compared to the baseline the two introduced loss functions prove the generation of novel n grams significantly for the word prior loss we observe an improvement of more novel tences compared to the baseline model however in both cases we did not manage to maintain the same rouge scores this might be a problem with the loss functions but also with the training process the vert metric looked like a promising metric to measure the semantic similarity between generated and target summaries however the resulting vert scores are completely correlated with the rouge metric and do not produce any new insights as our goal was to train a model that is more abstractive we used the number of novel n grams with respect to the reference summary as a measure the novel n grams score can easily be increased by adding random words to the summary this does not lead to more abstractive or higher quality summaries since there is no metric to evaluate abstractive summarization tasks effectively it is not possible to claim that the summaries are more abstractive based on just the increase of new n grams however we have shown that the summary length does not increase which suggests that new words replace other words instead of adding more words we have also shown that the novel words are plausible words for rephrasing summarization tasks future work it appears that the newly introduced losses interfere with the age mechanism and increase the over generation problem it could be the case that by introducing the coverage and new loss at the same point in training produces gradients that are too different from training without these losses which would interfere with the vergence of the network in future work the interaction between these new loss function and the coverage can be investigated the difference in pointing behaviour between training and ference could be reduced by using scheduled teacher forcing which gradually decreases the frequency the model receives the ground truth as input to the generator this reduces the difference between training and inference which could result in higher values of pen references mehdi allahyari seyedamin pouriyeh mehdi assefi saeid safaei elizabeth d trippe juan b gutierrez and krys kochut text summarization techniques a brief survey arxiv preprint tal baumel matan eyal and michael elhadad query focused tive summarization incorporating query relevance multi document age and summary length constraints into models arxiv preprint samy bengio oriol vinyals navdeep jaitly and noam shazeer scheduled sampling for sequence prediction with recurrent neural networks in advances in neural information processing systems shibhansh dohare harish karnick and vivek gupta text summarization using abstract meaning representation arxiv preprint john duchi elad hazan and yoram singer adaptive subgradient methods for online learning and stochastic optimization j mach learn res july acm org citation lisa fan dong yu and lu wang robust neural abstractive tion systems and evaluation against adversarial information arxiv preprint jiatao gu zhengdong lu hang li and victor ok li incorporating copying mechanism in sequence to sequence learning arxiv preprint karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend in advances in neural information processing systems eric jang shixiang gu and ben poole categorical reparameterization with gumbel softmax arxiv preprint hongyan jing using hidden markov modeling to decompose written summaries computational linguistics diederik p kingma and jimmy ba adam a method for stochastic tion corr org jacob krantz and jugal kalita abstractive summarization using attentive neural techniques arxiv preprint wojciech kryciski romain paulus caiming xiong and richard socher improving abstraction in text summarization arxiv preprint alex m lamb anirudh goyal alias parth goyal ying zhang saizheng zhang aaron c courville and yoshua bengio professor forcing a new algorithm for training recurrent networks in advances in neural information processing systems piji li wai lam lidong bing and zihao wang deep recurrent generative decoder for abstractive text summarization arxiv preprint chin yew lin of summaries rouge a package for automatic evaluation of rouge a package for automatic evaluation microsoft com en us research ramesh nallapati bowen zhou caglar gulcehre bing xiang al stractive text summarization using sequence to sequence rnns and beyond arxiv preprint abigail see peter j liu and christopher d manning get to the point summarization with pointer generator networks arxiv preprint kaiqiang song lin zhao and fei liu structure infused copy mechanisms for abstractive summarization arxiv preprint nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever and ruslan salakhutdinov dropout a simple way to prevent neural networks from overfitting the journal of machine learning research ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural networks in advances in neural information processing systems ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in advances in neural information processing systems oriol vinyals meire fortunato and navdeep jaitly pointer networks in advances in neural information processing systems noah weber leena shekhar niranjan balasubramanian and kyunghyun cho controlling decoding for more abstractive summaries with copy based networks arxiv preprint frank wilcoxon individual comparisons by ranking methods biometrics bulletin appendices a scores without coverage table mean rouge and vert scores of the tested models examples in testset all models were trained without coverage here nloss corresponds to naive pointing loss and wploss to the word prior pointing loss attention heads model extensions rouge rouge rouge l vert baseline see et al dropout dropout nloss nloss wploss wploss table percentage of novel n grams and sentences that are produced for each of the tested models attention heads model extensions grams grams grams grams sentences dropout dropout target summaries nloss nloss wploss wploss b examples the following pages contain randomly chosen examples from the multihead model with coverage combined with the new losses the words highlighted in red reflect the overall attention the model paid to a word during the constructing the summary italic words denote out of vocabulary words the green shading intensity represents the value of the generation probability pen figure in this example the average pen is much higher in the word prior loss model except on words with a low prior ellie meredith down syndrome figure this example shows clearly that the new losses re introduce the over generation problems that the coverage loss aimed to solve figure another example while the average pen of each word is significantly higher in the wploss model the generated summaries are the same figure the last sentence in the wploss example on average mostly uses the generator but is an exact copy of line in the source article
