m l c s c v v x r regularizing output distribution abstractive chinese social media text summarization improved semantic consistency bingzhen wei peking university china xuancheng ren peking university china xu sun peking university china yi zhang peking university china xiaoyan cai northwestern polytechnical university china qi su peking university china abstractive text summarization highly difficult problem sequence sequence model shown success improving performance task generated summaries inconsistent source content semantics cases generating summaries model selects semantically unrelated words respect source content probable output problem attributed heuristically constructed training data summaries unrelated source content containing semantically unrelated words spurious word correspondence paper propose regularization approach sequence sequence model use model learned regularize learning objective alleviate effect problem addition propose practical human evaluation method address problem existing automatic evaluation method evaluate semantic consistency source content properly experimental results demonstrate effectiveness proposed approach outperforms existing models especially proposed approach improves semantic consistency terms human evaluation ccs concepts computing methodologies natural language generation neural networks ization additional key words phrases abstractive text summarization semantic consistency chinese social media text natural language processing introduction abstractive test summarization important text generation task applying sequence sequence model publication large scale datasets quality automatic generated summarization greatly improved chen et al chopra et al gu et al hu et al li et al mcauley leskovec nallapati et al rush et al et al semantic consistency automatically generated summaries far satisfactory authors contributed equally paper corresponding author authors addresses bingzhen wei moe key laboratory computational linguistics school electronics engineering computer science peking university yiheyuan road beijing china edu cn xuancheng ren moe key laboratory computational linguistics school electronics engineering computer science peking university yiheyuan road beijing china edu cn xu sun moe key laboratory computational linguistics school electronics engineering computer science peking university yiheyuan road beijing china edu cn yi zhang moe key laboratory computational linguistics school electronics engineering computer science peking university yiheyuan road beijing china edu cn xiaoyan cai school automation northwestern polytechnical university xian shannxi china edu cn qi su school foreign languages peking university yiheyuan road beijing china edu cn commonly large scale datasets deep learning models constructed based naturally annotated data heuristic rules chopra et al hu et al nallapati et al summaries written source content specifically suggests provided summary semantically consistent source content example dataset chinese social media text summarization lcsts contains text summary pairs related according statistics manually checked data et al table example semantic inconsistency lcsts dataset example reference summary concluded source content semantics summary contained source text short semantics benefits concluded source content source content sec end hong kong stock exchange rejected partnership equity structure alibaba group s different shareholding rights alibaba group forced goodbye partners turned invest arms securities exchange commission sec picture shows alibaba empire reference summary alibaba listed benefits table shows example semantic inconsistency typically reference summary contains extra information understood source content hard conclude summary human inconsistency system extract information source text hard model learn generate summary accordingly model encode spurious correspondence summary source content memorization kind correspondence superficial actually needed generating reasonable summaries information harmful generating semantically consistent summaries unrelated information modeled example word benefits summary related source content remembered model source content correspondence spurious word related word source content following refer problem spurious correspondence caused semantically inconsistent data work aim alleviate impact semantic inconsistency current dataset based sequence sequence model propose regularization method heuristically learning spurious correspondence unrelated information dataset represented model incorporate new soft training target achieve goal output time training addition gold reference word current output targets softened output word distribution regularizes current output word distribution way robust correspondence source content output words learned potentially output summary semantically consistent obtain softened output word distribution propose methods based sequence model uses output layer decoder generate distribution higher temperature softmax normalization keeps relative order possible output words guides model smaller discriminative margin spurious correspondence different examples output distribution likely different effective discriminative margin established true correspondence different examples output distribution likely margin gradually established second introduces additional output layer generate distribution analogous multi task learning additional output layer provides alternative view data regularize output distribution effectively additional output layer differs original stable information e spurious correspondence learned model represented differently relative order regularized method detailed explanation introduced section problem abstractive text summarization system summary easily evaluated automatically rouge lin widely summarization evaluation rouge designed extractive text summarization deal summary paraphrasing abstractive text summarization rouge based reference requires high quality reference summary reasonable evaluation lacking existing dataset chinese social media text summarization argue proper evaluation text generation task human evaluation avoided propose simple practical human evaluation evaluating text summarization summary evaluated source content instead reference handles problems paraphrasing lack high quality reference contributions work summarized follows propose approach regularize output word distribution semantic inconsistency e words related source content exhibited training data derrepresented model add cross entropy based regularization term overall loss propose methods obtain soft target distribution regularization results demonstrate effectiveness proposed approach outperforms existing systems particular semantic consistency improved terms human evaluation conduct analysis examine effect proposed method output summaries output label distributions showing improved consistency results regularized output distribution propose simple human evaluation method assess semantic consistency generated summary source content kind evaluation absent existing work text summarization proposed human evaluation summary evaluated source content reference summary better measure consistency generated summary source content high quality reference available proposed method base fact spurious correspondence stable realization model prone change propose alleviate issue heuristically regularization use cross entropy annealed output distribution regularization term loss little fluctuation distribution depressed robust stable correspondence learned correspondence mean relation current output source content partially generated output furthermore propose use additional output layer generate annealed output distribution fact output layers differ words superficially co occur output distribution better regularized regularizing neural network annealed distribution typically training sequence sequence model hot hard target cross entropy based loss function example training set loss output fig illustration proposed methods left self train right dual train vector j yi log zi m z output vector y hot hard target vector m number labels y hot vector elements zero representing correct label loss j z log zl l index correct label loss summed output sentences minibatch source error signal backpropagation hard target cause problems training soft training methods try use soft target distribution provide generalized error signal training summarization task straight forward way use current output vector soft target contains knowledge learned current model e correspondence source content current output word losses combined new loss function j zi log zi m j z log zl zi log zi m l index true label strength soft training loss refer approach self train left figure output model seen refined supervisory signal learning model added loss promotes learning stable correspondence output learns hot distribution distribution generated model training output neural network close hot distribution solve soft target soften output distribution apply softmax temperature computed convenience description omit related trainable parameters model uses simplified notation zi zj unnormalized output e output softmax operation m trainencoderdecoderencoderdecoderdual train transformation keeps relative order labels higher temperature output distributed evenly key motivation model confident generate current output word supervision reference summary means correspondence spurious reference output unlikely concluded source content makes sense force model learn correspondence regularization follows motivation case error signal significant compared hot target case model extremely confident generate current output annealed distribution resemble hot target regularization effective use model identify spurious correspondence regularize output distribution accordingly dual output layers aforementioned method tries regularize output word distribution based learned relative order output words kept self dependency desirable regularization better correspondence spurious identified paper propose obtain soft target different view model different knowledge dataset mitigate overfitting problem additional output layer introduced generate soft target output layers share hidden representation independent parameters learn different knowledge data refer approach dual train clarity original output layer denoted new output layer outputs denoted respectively output layer acts original output layer apply soft training output output layer increase ability generalization suppose correct label l target output includes hot distribution distribution generated j log l log m new output layer trained normally originally hard target output layer prediction purpose generate soft target facilitate soft training suppose correct label l target output includes hot distribution j log l random initialization parameters output layers learn different things diversified knowledge helpful dealing spurious dence data seen online kind ensemble methods different instances model softly aggregated classification right figure shows architecture proposed dual train method experiments evaluate proposed approach chinese social media text summarization task based sequence sequence model analyze output text output label distribution models showing power proposed approach finally cases correspondences learned proposed approach problematic explained based approach adopt dataset large scale chinese short text summarization dataset lcsts constructed et al dataset consists million text summary pairs total constructed famous chinese social media microblogging service dataset split parts pairs training pairs ii validation pairs iii testing authors dataset manually annotated relevance scores ranging text summary pairs ii iii suggested pairs scores evaluation leaves pairs ii pairs iii statistics ii iii pairs dropped maintain semantic quality indicates training set manually annotated checked contains huge quantity unrelated text summary pairs experimental settings use sequence sequence model sutskever et al attention bahdanau et al jean et al luong et al mi et al baseline encoder decoder based single layer lstm hochreiter schmidhuber word embedding size hidden state size lstm unit conduct experiments word level convert character sequences word sequences use segment words existing work gu et al hu et al self train dual train implemented based baseline model parameters temperature soft training strength use simple setting tasks set pre train model applying soft training objective epochs total epochs use adam optimizer kingma ba tasks default settings testing use beam search generate summaries beam size set report test results epoch achieves best score development set evaluation protocol text summarization common automatic evaluation method rouge lin ated summary evaluated reference summary based unigram recall bigram recall recall longest common subsequence rouge l facilitate comparison existing systems adopt rouge automatic evaluation method rouge calculated character level following previous work et al abstractive text summarization rouge sub optimal assess semantic consistency summary source content especially reference piece text reason content expressed different ways different focuses simple word match recognize paraphrasing case existing large scale datasets aforementioned rouge calculated character level chinese text summarization making metrics favor models character level practice chinese word smallest semantic element uttered isolation character extreme case generated text completely intelligible characters match theory calculating rouge metrics word level alleviate problem word segmentation non trivial task chinese kinds segmentation rules produce different rouge scores argue com python org pypi acceptable introduce additional systematic bias automatic evaluations automatic evaluation semantically related tasks serve reference avoid deficiencies propose simple human evaluation method assess semantic consistency summary candidate evaluated text reference candidate irrelevant incorrect text candidate understandable candidate labeled bad candidate labeled good accuracy good summaries proposed evaluation simple straight forward focuses relevance summary text semantic consistency major consideration putting text summarization methods practice current automatic methods judge properly detailed guidelines human evaluation refer appendix human evaluation text summary pairs dispatched human annotators native speakers chinese setting summary evaluated reference number pairs needs manually evaluated times number pairs test set need compare systems total decrease workload hint annotation quality time adopt following procedure randomly select pairs validation set human annotators evaluate pair annotated twice inter annotator agreement checked find protocol inter annotator agreement high evaluation test set pair annotated accelerate evaluation maintain consistency summaries source content distributed different annotators experimental results table results human evaluation showing summaries semantically consistent source content generated summary evaluated directly source content good total accuracy methods reference baseline self train dual train results human evaluation focuses semantic consistency summary source content evaluate systems implemented reference conduct human evaluations existing systems work output summaries needed available baseline system implemented competitive terms rouge achieves better performance existing systems results listed table surprising accuracy reference summaries reach means test set contains text summary pairs poor quality removing pairs relevance scores lower suggested authors dataset dual train improves accuracy rigorous definition good results mean summaries semantically consistent source content self train performance drop compared baseline investigating generated summaries find major reason generated summaries grammatically complete stop early generated related source content definition good improved relevance loss intelligibility table comparisons existing models terms rouge metrics methods rnn context et al srb ma et al copynet gu et al rnn distract chen et al drgd li et al baseline self train dual train rouge l compare automatic evaluation results table applying soft training adaptation self train hurts performance additional output layer dual train performance greatly improved baseline proposed method simple baseline model second best compared state art models surpasses promising applying proposed method state art model improve performance automatic evaluation original test set facilitate comparison existing work reasonable setting exclude test instances found bad human evaluation quality automatic evaluation depends reference summary existing methods provide test output non trivial task reproduce results reported performance nonetheless change fact rouge handle issues abstractive text summarization properly experimental analysis examine effect proposed method reveal proposed method improves consistency compare output baseline dual train based output text output label distribution conduct error analysis discover room improvements analysis output text gain better understanding results analyze summaries generated baseline model proposed model summaries listed table shown table summaries generated proposed method better baseline believe precise informative references baseline system generates grammatical unrelated summary proposed method generates informative summary second baseline system generates related ungrammatical summary proposed method generates summary related source content different reference believe generated summary actually better reference focus visit event purpose baseline system generates related grammatical summary facts stated completely incorrect summary generated proposed method comprehensive reference reference includes facts sentence source content short generated summary proposed method consistent source content exhibits necessity proposed human evaluation generated summary evaluated reference redundant wrong actually true source content arguable generated summary better reference table examples summaries generated baseline dual train test set summaries generated proposed better ones generated baseline informative precise references short text china railway news starting january bikes carried platform train riders voice complaints travelers wish soon train bike ride begin check bikes getting board consult customer service check nearest service station advance shipping costs kilogram depend shipping mileage reference bikes carried train baseline bikes bikes stop think proposal china railway bikes carried platform short text afternoon liu yunshan member standing committee political bureau cpc central committee secretary secretariat cpc central committee paid visit yu min zhang cunhao recipients state preeminent science technology award liu yunshan pointed scientists technologists study pragmatic research spirit older generation indifferent fame fortune devote scientific research strive create rate scientific research achievements reference liu yunshan paid visit prominent science technology experts baseline liu yunshan science technology research research proposal liu yunshan scientists technologists study pragmatic research spirit older generation short text september geneva based world economic forum released global competitiveness report switzerland ranks years row competitive country world singapore united states taking second place place china ranks highest bric countries reference global competitiveness rankings china ranks highest bric countries baseline global competitiveness report china ranks eighth world proposal global competitiveness report switzerland takes place china ranks doubt generated summary proposed method better baseline improvement properly shown existing evaluation methods furthermore examples suggest proposed method learn better correspondence highlighted words example table share previous words baseline considers stop related words sign noisy word relations learned training examples proposed method generates platform related human thinks second example human selects expert dual train selects worker baseline selects research fails generate grammatical sentence later reference baseline use word dual train chooses word meaning concluded dual train learns better word relations generalize test set good word relations guide decoder generate semantically consistent summaries analysis output label distribution generated text proposed method related source content analyze label distribution e word distribution generated output layer output word selected illustrate relationship calculate representation word based label distributions representation associated specific label word denoted l dimension shows likely label indexed generated instead label representation run model training set output vectors table examples labels related labels highlighted words indicate problematic word relations baseline system encodes semantically unrelated words word relations proposed method learns relatedness precisely robustly related labels label word proposal china nationwide nation domestic baseline china nationwide domestic state council country proposal motor vehicle automobile beijing driver baseline restricted license beijing vehicle subsidy old car proposal ren ge baseline china ge ming proposal figure photo image picture baseline photo figure de image proposal automobile electrombile electric vehicle tesla baseline automobile usa internet tesla proposal futures reform bond enterprise futures industry baseline futures china innovation sleet long sandstorm railway police baseline rescue somebody railway police police internship proposal snow sleet snowfall strong wind baseline sleet exist snowfall snowfall proposal long de time baseline ma de know proposal windstorm sand dust xinjiang usa baseline sand dust snowstorm rainstorm short time proposal railway police police train railway decoder averaged respect corresponding labels form representation obtain related words word simply selecting highest values representation table lists labels labels likely replace labels hint correspondence learned model results observed dual train learns better semantic relevance word compared baseline spurious word correspondence alleviated regularization example possible substitutes word long considered dual train include long time relatedness learned poorly baseline know number particles possible substitutes considered baseline representative example word image baseline includes particles related words phenomenon shows baseline suffers spurious correspondence data learns noisy harmful relations rely co occurrence contrast proposed method capture stable semantic relatedness words text summarization grouping words topic help model generate sentences coherent improve quality summarization relevance source content proposed method resolves large number noisy word relations cases related words eliminated example similar words futures industry proposed method include reform related baseline harmful text summarization problem arise fact words rarely occur training data relatedness reflected data issue particles e de related words possible explanation particles contexts word hard models distinguish real semantically related words proposed approach based regularization common correspondence reasonable kind relation eliminated case categorized data sparsity usually needs aid knowledge bases solve second case characteristics natural language words closed class words case resolved manually restricting relatedness words related work related work includes efforts designing models chinese social media text summarization task efforts obtaining soft training target supervised learning systems chinese social media text summarization large scale chinese short text summarization dataset proposed et al datasets et al proposed systems solve task rnn rnn context sequence sequence based models gru encoder decoder difference rnn context attention mechanism rnn conducted experiments character level word level rnn distract chen et al distraction based neural model attention mechanism focused different parts source content copynet gu et al incorporated copy mechanism allow generated summary copied source content copy mechanism explained results word level model better results character level model srb ma et al sequence sequence based neural model improve semantic relevance input text output summary drgd li et al deep recurrent generative decoder model combining decoder variational autoencoder methods obtaining soft training target soft target aims refine supervisory signal supervised learning related work includes soft target traditional learning algorithms model distillation deep learning algorithms soft label methods typically binary classification nguyen et al human annotators assign label example information confident annotation main difference method soft label methods require additional annotation information e confidence information annotated labels training data costly text summarization task prior studies model distillation deep learning distills big models smaller model distillation hinton et al combined different instances model single output distributions previously trained models soft target distribution train new model similar work model distillation soft target regularization method aghajanyan image classification instead outputs instances exponential average past label distributions current instance soft target distribution proposed method different compared existing model distillation methods proposed method require additional models additional space record past soft label distributions existing methods suitable text summarization tasks training additional model costly additional space huge massive number data proposed method uses current state soft target distribution eliminates need train additional models store history information conclusions propose regularization approach sequence sequence model chinese social media summarization task proposed approach use cross entropy based regularization term model neglect possible unrelated words propose methods obtaining soft output word distribution regularization dual train proves effective experimental results proposed method improve semantic consistency terms human evaluation shown analysis proposed method achieves improvements eliminating semantically related word correspondence proposed human evaluation method effective efficient judging semantic sistency absent previous work crucial accurate evaluation text summarization systems proposed metric simple conduct easy interpret provides insight practicable existing systems real world scenario standard human evaluation table examples case human evaluation rules examined order rule met following rules need checked procedure leaves specific cases total bad good bad fluency source beginning year number brokers finding husbands july million shares qilu securities listed beijing financial asset exchange century securities shenyin wanguo yunnan securities currently listed major property rights exchanges stockholders equity unveiled country summary qilu securities million shares find good fluency bad relatedness source august xiaomi s valuation exceeded billion mainly xiaomi company valued hardware company smart terminal equipment industry hardware company regarded company downstream industrial chain smile curve words acer s founder shi zhengrong summary xiaomi s defense marketing channels lei jun combats scalpers multiple measures good fluency relatedness bad faithfulness source ceo tian tongsheng years old times marathon cctv narrator yu jia selling car runs kilometers day work lenovo vice president wei jianglei running brain real estate ceo liu aiming running marathon situation stimulate matter fast run people obviously older running summary liu chuanzhi years old times marathon runs kilometers day work good fluency relatedness faithfulness source chengdu s software information technology service industry maintained rapid growth momentum recent years ing midwest cities silicon valley western china chengdu software information technology service industry development report released summary chengdu s efforts build west silicon valley human evaluation annotators asked evaluate summary source content based goodness summary summary understandable relevant correct according source content summary considered bad concretely annotators asked examine following aspects determine summary good fluency summary understood summary good judged grammatical correctness checking major semantic roles predicate agent experiencer missing allows summary certain particles e aspect markers e missing content understood common issue summary repeated words view repetition affect intelligibility summary treat summary good relatedness impossible decide summary correct wrong according source content summary bad definition nonsense ruled faithfulness summary correct according source content summary labeled bad treat summary relevant correct according source content general bad rule met summary labeled bad following rules need checked table examples cases rule summary fluent patient predicate seek missing second summary fluent content related source determine lei jun actually fighting scalpers based source content summary fluent related source content facts wrong summary facts different people met rules considered good acknowledgments work supported national natural science foundation china grant references armen aghajanyan softtarget regularization effective technique reduce fitting neural networks dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align corr translate corr qian chen xiaodan zhu zhenhua ling si wei hui jiang distraction based neural networks modeling documents proceedings international joint conference artificial intelligence new york new york usa sumit chopra michael auli alexander m rush abstractive sentence summarization attentive recurrent neural networks naacl hlt conference north american chapter association computational linguistics human language technologies jiatao gu zhengdong lu hang li victor o k li incorporating copying mechanism sequence sequence learning proceedings annual meeting association computational linguistics volume long papers berlin germany geoffrey e hinton oriol vinyals jeffrey dean distilling knowledge neural network nips deep learning workshop sepp hochreiter jrgen schmidhuber long short term memory neural computation baotian hu qingcai chen fangze zhu lcsts large scale chinese short text summarization dataset proceedings conference empirical methods natural language processing lisbon portugal sbastien jean kyunghyun cho roland memisevic yoshua bengio large target vocabulary neural machine translation proceedings annual meeting association computational linguistics acl diederik p kingma jimmy ba adam method stochastic optimization corr piji li wai lam lidong bing zihao wang deep recurrent generative decoder abstractive text summarization proceedings conference empirical methods natural language processing copenhagen denmark chin yew lin rouge package automatic evaluation summaries text summarization branches proceedings workshop association computational linguistics barcelona spain junyang lin shuming ma qi su xu sun decoding history based adaptive control attention neural machine translation corr org thang luong hieu pham christopher d manning effective approaches attention based neural machine translation proceedings conference empirical methods natural language processing emnlp shuming ma xu sun wei li sujian li wenjie li xuancheng ren query output generating words querying distributed word representations paraphrase generation corr org shuming ma xu sun junyang lin xuancheng ren hierarchical end end model jointly improving text summarization sentiment classification corr org shuming ma xu sun jingjing xu houfeng wang wenjie li qi su improving semantic relevance sequence learning chinese social media text summarization proceedings annual meeting association computational linguistics volume short papers vancouver canada julian john mcauley jure leskovec amateurs connoisseurs modeling evolution user expertise online reviews international world wide web conference www rio de janeiro brazil haitao mi zhiguo wang abe ittycheriah supervised attentions neural machine translation proceedings conference empirical methods natural language processing emnlp ramesh nallapati bowen zhou ccero nogueira dos santos aglar glehre bing xiang abstractive text marization sequence sequence rnns proceedings signll conference computational natural language learning conll berlin germany august quang nguyen hamed valizadegan milos hauskrecht learning classification models soft label information journal american medical informatics association alexander m rush sumit chopra jason weston neural attention model abstractive sentence tion proceedings conference empirical methods natural language processing emnlp lisbon portugal september abigail peter j liu christopher d manning point summarization pointer generator networks proceedings annual meeting association computational linguistics acl vancouver canada july august volume long papers ilya sutskever oriol vinyals quoc v le sequence sequence learning neural networks advances neural information processing systems annual conference neural information processing systems jingjing xu xu sun xuancheng ren junyang lin bingzhen wei wei li dp gan diversity promoting generative adversarial network generating informative diversified text corr org
