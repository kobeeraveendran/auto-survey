r l c s c v v x r textkd gan text generation knowledge distillation generative adversarial networks md akmal haidar mehdi rezagholizadeh akmal haidar mehdi com huawei noah s ark lab montreal research center montreal canada abstract text generation particular interest nlp plications machine translation language modeling text summarization generative adversarial networks gans achieved markable success high quality image generation computer vision recently gans gained lots interest nlp nity achieving similar success nlp challenging discrete nature text work duce method knowledge distillation eectively exploit gan setup text generation demonstrate autoencoders aes providing continuous representation sentences smooth representation assign non zero probabilities word distill representation train generator synthesize similar smooth representations perform number periments validate idea dierent datasets proposed approach yields better performance terms bleu score jensen shannon distance jsd measure compared traditional gan based text generation approaches pre training keywords text generation generative adversarial networks edge distillation introduction recurrent neural network rnn based techniques language models popular approaches text generation rnn based text generators rely maximum likelihood estimation mle solutions teacher forcing e model trained predict item given previous observations known literature mle simplistic objective complex nlp task mle based methods suer exposure bias means training time model exposed gold data test time observes predictions gans based adversarial loss function generator discriminator networks suers mentioned problems gans provide better image generation framework comparing traditional mle based methods achieved substantial success eld computer vision generating realistic sharp images great success motivated researchers apply framework nlp applications authors suppressed excessive length gans exploited recently nlp applications machine translation dialogue models question answering natural language generation applying gan nlp challenging discrete nature text consequently propagation feasible discrete outputs straightforward pass gradients discrete output words generator existing gan based solutions categorized according technique leveraged handling problem discrete nature text reinforcement learning rl based methods latent space based solutions approaches based continuous approximation discrete sampling versions rl based techniques introduced literature including gan maskgan leakgan need training computationally expensive compared methods categories latent space based solutions derive latent space representation text ae attempt learn data manifold space approach generating text gans nd continuous approximation discrete sampling gumbel softmax technique approximating non dierentiable argmax operator continuous function work introduce textkd gan new solution main tleneck gan text generation knowledge distillation technique transfer knowledge softened output teacher model student model solution based ae teacher derive smooth tation real text smooth representation fed textkd gan discriminator instead conventional hot representation generator student tries learn manifold softened smooth representation ae textkd gan outperforms conventional gan based text generators need pre training remainder paper organized follows sections preliminary background generative adversarial networks related work literature reviewed proposed method presented section section experimental details discussed finally section conclude paper background generative adversarial networks include separate deep networks generator discriminator generator takes random variable following distribution attempt map data distribution output distribution generator expected converge data distribution training hand discriminator expected discern real samples generated ones outputting zeros ones respectively training generator discriminator generate samples classify respectively adversarially aecting performance regard adversarial loss function employed training min g max d v d g title suppressed excessive length player minimax game nash equilibrium point derived finding solution game non trivial great extent literature dedicated regard fig simplistic text generator gan stated gans text generation challenging discrete nature text clarify issue figure depicts simplistic ture gan based text generation main bottleneck design argmax operator dierentiable blocks gradient ow discriminator generator min g knowledge distillation knowledge distillation studied model compression knowledge large cumbersome model transferred small model easy deployment studies studied knowledge transfer technique starts training big teacher model ensemble model train small student model tries mimic characteristics teacher model hidden representations s output probabilities directly generated sentences teacher model neural machine translation rst teacher student framework knowledge distillation proposed introducing softened teacher s output paper propose gan framework text generation generator student tries mimic reconstructed output representation auto encoder teacher instead mapping conventional hot representations improved wgan generating text pure gans inspired improved wasserstein gan iwgan work iwgan character level language model developed authors suppressed excessive length based adversarial training generator discriminator extra element policy gradient reinforcement learning generator produces softmax vector entire vocabulary discriminator responsible distinguishing hot representations real text softmax vector generated text iwgan method described figure disadvantage technique discriminator able tell apart hot input softmax input easily generator hard time fooling discriminator vanishing gradient problem highly probable fig improved wgan text generation related work new version wasserstein gan text generation gradient penalty discriminator proposed generator cnn network generating xed length texts discriminator cnn receiving tensors input sentences determines tensor coming generator sampled real data real sentences generated ones represented hot softmax representations respectively similar approach proposed rnn based generator curriculum learning strategy produce sequences gradually increasing lengths training progresses rnn trained generate text gan curriculum learning authors proposed procedure called teacher helping helps generator produce long sequences conditioning shorter ground truth sequences approaches use discriminator discriminate generated softmax output hot real data figure clear downside reason discriminator receives inputs dierent representations hot vector real data probabilistic vector output generator makes discrimination trivial aes exploited gans dierent architectures computer vision application aae ali hali similarly title suppressed excessive length aes gans generating text instance adversarially regularized ae arae proposed generator trained parallel ae learn continuous version code space produced ae encoder discriminator responsible distinguishing encoded hidden code continuous code generator basically approach continuous distribution generated corresponding encoded code text methodology aes useful denoising text transferring code space encoding reconstructing original text code aes combined gans order improve generated text section introduce technique aes replace conventional hot representation continuous softmax representation real data discrimination distilling output probabilities ae textkd gan generator stated conventional text based discrimination approach real generated input discriminator dierent types hot softmax simply tell apart way avoid issue derive continuous smooth representation words hot train discriminator dierentiate continuous representations work use conventional ae teacher replace hot representation softmax reconstructed output smooth representation yields smaller variance gradients proposed model depicted figure seen instead hot representation real words feed softened reconstructed output ae discriminator technique makes discrimination harder discriminator gan generator student softmax output tries mimic ae output distribution instead conventional hot representations literature fig textkd gan model text generation authors suppressed excessive length textkd gan work better iwgan suppose apply iwgan language vocabulary size words hot representation words points cartesian coordinates span generated softmax outputs line segment connecting depicted left panel figure evident graphically task discriminator discriminate points line connecting simple easy task let s consider textkd gan idea word language example depicted figure right panel output locus gan decoder red line segments instead points hot case line segments lie output locus generator generator successful fooling discriminator fig locus input vectors discriminator word language model left panel iwgan right panel textkd gan model training train ae textkd gan simultaneously order break objective function terms reconstruction term ae discriminator loss function gradient penalty adversarial cost generator mathematically min min min ww min ww min expx min ezpz title suppressed excessive length losses trained alternately optimize dierent parts model employ gradient penalty approach iwgan training discriminator gradient penalty term need calculate gradient norm random samples x px according proposal random samples obtained sampling uniformly line connecting pairs generated real data samples x px px xgen complete training algorithm described algorithm textkd gan text generation require adam hyperparameters batch size m initial ae parameters encoder decoder discriminator parameters initial generator parameters number training iterations px compute code vectors ci ae training sample reconstructed text backpropagate reconstruction loss update train discriminator k times px sample sample compute generated text backpropagate discriminator loss update w n end train generator sample compute generated text backpropagate generator loss update px sample end n experiments dataset experimental setup carried experiments dierent datasets google billion benchmark language modeling stanford natural language inference snli text generation performed character level sentence statmt org lm stanford edu projects authors suppressed excessive length length google dataset rst million sentences extract frequent characters build vocabulary snli dataset entire preprocessed training data contains sentences total built vocabulary characters train ae layer lstm cells encoder decoder train autoencoder adam optimizer learning rate decoding output previous time step input time step hidden code c additional input time step decoding greedy search approach applied best output cnn based generator discriminator residual blocks discriminator trained times gan generator iteration train generator discriminator adam optimizer learning rate use bleu n score evaluate techniques bleu n score calculated according following equation bleu n bp exp n pn probability n gram wn n calculate bleu n scores n grams brevity penalty train models iterations results best bleu n scores generated texts reported calculate bleu n scores generate batches sentences candidate texts e sentences character sentences use entire test set reference texts experimental results results experiments depicted table seen tables proposed textkd gan approach yields signicant improvements terms scores iwgan arae approaches softened smooth output decoder useful learn better discriminator traditional hot representation lower bleu improvement google dataset compared snli dataset reason sentences google dataset diverse complicated finally note text based hot discrimination iwgan proposed method better traditional code based arae technique examples generated text snli experiment listed table seen generated text proposed textkd gan approach meaningful contains correct words compared iwgan provide training curves jensen shannon distances jsd tween n grams generated sentences training real ones com aboev arae tf tree master title suppressed excessive length table results bleu n scores million sentences billion google dataset model iwgan arae textkd gan table results bleu n scores snli dataset model iwgan arae textkd gan table example generated sentences model trained snli dataset textkd gan iwgan people standing s people laying angold man walting beach woman standing bench man looking af tre walk aud people ride comp woman sleeping brick man standing beach man standing standing people eating food man looking af tre walk aud dog main near man party members walking hal people looking people running l boy playing sitting black man going figure distances derived snli experiments calculated calculating log probabilities n grams generated real sentences depicted gure textkd gan approach minimizes jsd compared literature methods sion approach learns powerful discriminator turn generates data distribution close real data distribution discussion results experiment shows superiority textkd gan method conventional gan based techniques compared technique gan based generators need pre training explains included rl based techniques results showed power continuous smooth representations known tricks work discontinuity text gans aes textkd gan adds important dimension technique latent space authors suppressed excessive length c d fig jensen shannon distance jsd generated training tences n grams derived snli experiments b c represent jsd grams respectively modeled exploited separate signal discriminating generated text real data worth mentioning observations experiments training text based generators easier training code based techniques arae observed gradient penalty term plays signicant terms reducing mode collapse generated text gan furthermore work focused based techniques textkd gan applicable word based settings bear mind pure gan based text generation techniques newborn stage powerful terms learning semantics complex datasets large sentences lack capacity capturing long term information cnn networks address problem rl employed empower pure gan based techniques textkd gan step conclusion future work work introduced textkd gan new solution knowledge distillation main bottleneck gan generating text title suppressed excessive length discontinuity text solution based ae teacher derive continuous smooth representation real text smooth representation distilled gan discriminator instead conventional hot resentation demonstrated rationale approach discrimination task discriminator real generated texts dicult consequently providing richer signal generator time training textkd gan generator student try learn manifold smooth representation later mapped real data distribution applying argmax operator evaluated textkd gan benchmark datasets bleu n scores jsd measures quality output generated text results showed proposed textkd gan approach outperforms traditional gan based text generation methods need pre training iwgan arae finally summarize plan future work following evaluated textkd gan character based level mance approach word based level needs investigated current textkd gan implemented cnn based generator able improve textkd gan rnn based generators textkd gan core technique text generation similar pure gan based techniques powerful generating long sentences rl tool accommodate weakness references belghazi m rajeswar s mastropietro o rostamzadeh n mitrovic j courville hierarchical adversarially learned inference arxiv preprint bengio y louradour j collobert r weston j curriculum learning proceedings annual international conference machine learning pp acm cer d manning c d jurafsky d best lexical metric phrase based statistical mt system optimization human language technologies annual conference north american chapter association putational linguistics pp association computational linguistics dumoulin v belghazi poole b mastropietro o lamb arjovsky m courville adversarially learned inference arxiv preprint fedus w goodfellow dai m maskgan better text generation lling arxiv preprint goodfellow pouget abadie j mirza m xu b warde farley d ozair s courville bengio y generative adversarial nets advances neural information processing systems pp gulrajani ahmed f arjovsky m dumoulin v courville improved training wasserstein gans arxiv preprint guo j lu s cai h zhang w yu y wang j long text generation adversarial training leaked information arxiv preprint authors suppressed excessive length hinton g vinyals o dean j distilling knowledge neural network arxiv preprint hochreiter s schmidhuber j long short term memory neural computation j w r david z learning algorithm continually running fully recurrent neural networks neural computation kim y rush m sequence level knowledge distillation emnlp pp kim y zhang k rush m lecun y al adversarially regularized autoencoders generating discrete structures arxiv preprint kusner m j hernndez lobato j m gans sequences discrete elements gumbel softmax distribution arxiv preprint li j monroe w shi t ritter jurafsky d adversarial learning neural dialogue generation arxiv preprint liu c w lowe r serban v noseworthy m charlin l pineau j evaluate dialogue system empirical study unsupervised evaluation metrics dialogue response generation arxiv preprint makhzani shlens j jaitly n goodfellow frey b adversarial coders arxiv preprint papineni k roukos s ward t zhu w bleu method automatic evaluation machine translation acl pp press o bar bogin b berant j wolf l language generation recurrent generative adversarial networks pre training arxiv preprint rajeswar s subramanian s dutil f pal c courville adversarial tion natural language arxiv preprint romero ballas n kahou s e chassang gatta c bengio y fitnets hints thin deep nets iclr salimans t goodfellow zaremba w cheung v radford chen x proved techniques training gans advances neural information processing systems pp sutton r s mcallester d singh s p mansour y policy gradient methods reinforcement learning function approximation nips pp wu l xia y zhao l tian f qin t lai j liu t y adversarial neural machine translation arxiv preprint yang z chen w wang f xu b improving neural machine translation conditional sequence generative adversarial nets arxiv preprint yang z hu j salakhutdinov r cohen w w semi supervised qa ative domain adaptive nets arxiv preprint yu l zhang w wang j yu y seqgan sequence generative adversarial nets policy gradient aaai pp zhang y gan z fan k chen z henao r shen d carin l adversarial feature matching text generation arxiv preprint zhu y lu s zheng l jiaxian g weinan z jun w yong y texygen benchmarking platform text generation models arxiv preprint
