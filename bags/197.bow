textkd gan text generation knowledge distillation generative adversarial networks akmal haidar mehdi rezagholizadeh akmal haidar mehdi com huawei noah ark lab montreal research center montreal canada abstract text generation particular interest nlp plications machine translation language modeling text summarization generative adversarial networks gans achieved markable success high quality image generation computer vision recently gans gained lots interest nlp nity achieving similar success nlp challenging discrete nature text work duce method knowledge distillation eectively exploit gan setup text generation demonstrate autoencoders aes providing continuous representation sentences smooth representation assign non zero probabilities word distill representation train generator synthesize similar smooth representations perform number periments validate idea dierent datasets proposed approach yields better performance terms bleu score jensen shannon distance jsd measure compared traditional gan based text generation approaches pre training keywords text generation generative adversarial networks edge distillation introduction recurrent neural network rnn based techniques language models popular approaches text generation rnn based text generators rely maximum likelihood estimation mle solutions teacher forcing model trained predict item given previous observations known literature mle simplistic objective complex nlp task mle based methods suer exposure bias means training time model exposed gold data test time observes predictions gans based adversarial loss function generator discriminator networks suers mentioned problems gans provide better image generation framework comparing traditional mle based methods achieved substantial success eld computer vision generating realistic sharp images great success motivated researchers apply framework nlp applications authors suppressed excessive length gans exploited recently nlp applications machine translation dialogue models question answering natural language generation applying gan nlp challenging discrete nature text consequently propagation feasible discrete outputs straightforward pass gradients discrete output words generator existing gan based solutions categorized according technique leveraged handling problem discrete nature text reinforcement learning based methods latent space based solutions approaches based continuous approximation discrete sampling versions based techniques introduced literature including gan maskgan leakgan need training computationally expensive compared methods categories latent space based solutions derive latent space representation text attempt learn data manifold space approach generating text gans continuous approximation discrete sampling gumbel softmax technique approximating non dierentiable argmax operator continuous function work introduce textkd gan new solution main tleneck gan text generation knowledge distillation technique transfer knowledge softened output teacher model student model solution based teacher derive smooth tation real text smooth representation fed textkd gan discriminator instead conventional hot representation generator student tries learn manifold softened smooth representation textkd gan outperforms conventional gan based text generators need pre training remainder paper organized follows sections preliminary background generative adversarial networks related work literature reviewed proposed method presented section section experimental details discussed finally section conclude paper background generative adversarial networks include separate deep networks generator discriminator generator takes random variable following distribution attempt map data distribution output distribution generator expected converge data distribution training hand discriminator expected discern real samples generated ones outputting zeros ones respectively training generator discriminator generate samples classify respectively adversarially aecting performance regard adversarial loss function employed training min max title suppressed excessive length player minimax game nash equilibrium point derived finding solution game non trivial great extent literature dedicated regard fig simplistic text generator gan stated gans text generation challenging discrete nature text clarify issue figure depicts simplistic ture gan based text generation main bottleneck design argmax operator dierentiable blocks gradient discriminator generator min knowledge distillation knowledge distillation studied model compression knowledge large cumbersome model transferred small model easy deployment studies studied knowledge transfer technique starts training big teacher model ensemble model train small student model tries mimic characteristics teacher model hidden representations output probabilities directly generated sentences teacher model neural machine translation rst teacher student framework knowledge distillation proposed introducing softened teacher output paper propose gan framework text generation generator student tries mimic reconstructed output representation auto encoder teacher instead mapping conventional hot representations improved wgan generating text pure gans inspired improved wasserstein gan iwgan work iwgan character level language model developed authors suppressed excessive length based adversarial training generator discriminator extra element policy gradient reinforcement learning generator produces softmax vector entire vocabulary discriminator responsible distinguishing hot representations real text softmax vector generated text iwgan method described figure disadvantage technique discriminator able tell apart hot input softmax input easily generator hard time fooling discriminator vanishing gradient problem highly probable fig improved wgan text generation related work new version wasserstein gan text generation gradient penalty discriminator proposed generator cnn network generating xed length texts discriminator cnn receiving tensors input sentences determines tensor coming generator sampled real data real sentences generated ones represented hot softmax representations respectively similar approach proposed rnn based generator curriculum learning strategy produce sequences gradually increasing lengths training progresses rnn trained generate text gan curriculum learning authors proposed procedure called teacher helping helps generator produce long sequences conditioning shorter ground truth sequences approaches use discriminator discriminate generated softmax output hot real data figure clear downside reason discriminator receives inputs dierent representations hot vector real data probabilistic vector output generator makes discrimination trivial aes exploited gans dierent architectures computer vision application aae ali hali similarly title suppressed excessive length aes gans generating text instance adversarially regularized arae proposed generator trained parallel learn continuous version code space produced encoder discriminator responsible distinguishing encoded hidden code continuous code generator basically approach continuous distribution generated corresponding encoded code text methodology aes useful denoising text transferring code space encoding reconstructing original text code aes combined gans order improve generated text section introduce technique aes replace conventional hot representation continuous softmax representation real data discrimination distilling output probabilities textkd gan generator stated conventional text based discrimination approach real generated input discriminator dierent types hot softmax simply tell apart way avoid issue derive continuous smooth representation words hot train discriminator dierentiate continuous representations work use conventional teacher replace hot representation softmax reconstructed output smooth representation yields smaller variance gradients proposed model depicted figure seen instead hot representation real words feed softened reconstructed output discriminator technique makes discrimination harder discriminator gan generator student softmax output tries mimic output distribution instead conventional hot representations literature fig textkd gan model text generation authors suppressed excessive length textkd gan work better iwgan suppose apply iwgan language vocabulary size words hot representation words points cartesian coordinates span generated softmax outputs line segment connecting depicted left panel figure evident graphically task discriminator discriminate points line connecting simple easy task let consider textkd gan idea word language example depicted figure right panel output locus gan decoder red line segments instead points hot case line segments lie output locus generator generator successful fooling discriminator fig locus input vectors discriminator word language model left panel iwgan right panel textkd gan model training train textkd gan simultaneously order break objective function terms reconstruction term discriminator loss function gradient penalty adversarial cost generator mathematically min min min min min expx min ezpz title suppressed excessive length losses trained alternately optimize dierent parts model employ gradient penalty approach iwgan training discriminator gradient penalty term need calculate gradient norm random samples according proposal random samples obtained sampling uniformly line connecting pairs generated real data samples xgen complete training algorithm described algorithm textkd gan text generation require adam hyperparameters batch size initial parameters encoder decoder discriminator parameters initial generator parameters number training iterations compute code vectors training sample reconstructed text backpropagate reconstruction loss update train discriminator times sample sample compute generated text backpropagate discriminator loss update end train generator sample compute generated text backpropagate generator loss update sample end experiments dataset experimental setup carried experiments dierent datasets google billion benchmark language modeling stanford natural language inference snli text generation performed character level sentence statmt org stanford edu projects authors suppressed excessive length length google dataset rst million sentences extract frequent characters build vocabulary snli dataset entire preprocessed training data contains sentences total built vocabulary characters train layer lstm cells encoder decoder train autoencoder adam optimizer learning rate decoding output previous time step input time step hidden code additional input time step decoding greedy search approach applied best output cnn based generator discriminator residual blocks discriminator trained times gan generator iteration train generator discriminator adam optimizer learning rate use bleu score evaluate techniques bleu score calculated according following equation bleu exp probability gram calculate bleu scores grams brevity penalty train models iterations results best bleu scores generated texts reported calculate bleu scores generate batches sentences candidate texts sentences character sentences use entire test set reference texts experimental results results experiments depicted table seen tables proposed textkd gan approach yields signicant improvements terms scores iwgan arae approaches softened smooth output decoder useful learn better discriminator traditional hot representation lower bleu improvement google dataset compared snli dataset reason sentences google dataset diverse complicated finally note text based hot discrimination iwgan proposed method better traditional code based arae technique examples generated text snli experiment listed table seen generated text proposed textkd gan approach meaningful contains correct words compared iwgan provide training curves jensen shannon distances jsd tween grams generated sentences training real ones com aboev arae tree master title suppressed excessive length table results bleu scores million sentences billion google dataset model iwgan arae textkd gan table results bleu scores snli dataset model iwgan arae textkd gan table example generated sentences model trained snli dataset textkd gan iwgan people standing people laying angold man walting beach woman standing bench man looking tre walk aud people ride comp woman sleeping brick man standing beach man standing standing people eating food man looking tre walk aud dog main near man party members walking hal people looking people running boy playing sitting black man going figure distances derived snli experiments calculated calculating log probabilities grams generated real sentences depicted gure textkd gan approach minimizes jsd compared literature methods sion approach learns powerful discriminator turn generates data distribution close real data distribution discussion results experiment shows superiority textkd gan method conventional gan based techniques compared technique gan based generators need pre training explains included based techniques results showed power continuous smooth representations known tricks work discontinuity text gans aes textkd gan adds important dimension technique latent space authors suppressed excessive length fig jensen shannon distance jsd generated training tences grams derived snli experiments represent jsd grams respectively modeled exploited separate signal discriminating generated text real data worth mentioning observations experiments training text based generators easier training code based techniques arae observed gradient penalty term plays signicant terms reducing mode collapse generated text gan furthermore work focused based techniques textkd gan applicable word based settings bear mind pure gan based text generation techniques newborn stage powerful terms learning semantics complex datasets large sentences lack capacity capturing long term information cnn networks address problem employed empower pure gan based techniques textkd gan step conclusion future work work introduced textkd gan new solution knowledge distillation main bottleneck gan generating text title suppressed excessive length discontinuity text solution based teacher derive continuous smooth representation real text smooth representation distilled gan discriminator instead conventional hot resentation demonstrated rationale approach discrimination task discriminator real generated texts dicult consequently providing richer signal generator time training textkd gan generator student try learn manifold smooth representation later mapped real data distribution applying argmax operator evaluated textkd gan benchmark datasets bleu scores jsd measures quality output generated text results showed proposed textkd gan approach outperforms traditional gan based text generation methods need pre training iwgan arae finally summarize plan future work following evaluated textkd gan character based level mance approach word based level needs investigated current textkd gan implemented cnn based generator able improve textkd gan rnn based generators textkd gan core technique text generation similar pure gan based techniques powerful generating long sentences tool accommodate weakness references belghazi rajeswar mastropietro rostamzadeh mitrovic courville hierarchical adversarially learned inference arxiv preprint bengio louradour collobert weston curriculum learning proceedings annual international conference machine learning acm cer manning jurafsky best lexical metric phrase based statistical system optimization human language technologies annual conference north american chapter association putational linguistics association computational linguistics dumoulin belghazi poole mastropietro lamb arjovsky courville adversarially learned inference arxiv preprint fedus goodfellow dai maskgan better text generation lling arxiv preprint goodfellow pouget abadie mirza warde farley ozair courville bengio generative adversarial nets advances neural information processing systems gulrajani ahmed arjovsky dumoulin courville improved training wasserstein gans arxiv preprint guo cai zhang wang long text generation adversarial training leaked information arxiv preprint authors suppressed excessive length hinton vinyals dean distilling knowledge neural network arxiv preprint hochreiter schmidhuber long short term memory neural computation david learning algorithm continually running fully recurrent neural networks neural computation kim rush sequence level knowledge distillation emnlp kim zhang rush lecun adversarially regularized autoencoders generating discrete structures arxiv preprint kusner hernndez lobato gans sequences discrete elements gumbel softmax distribution arxiv preprint monroe shi ritter jurafsky adversarial learning neural dialogue generation arxiv preprint liu lowe serban noseworthy charlin pineau evaluate dialogue system empirical study unsupervised evaluation metrics dialogue response generation arxiv preprint makhzani shlens jaitly goodfellow frey adversarial coders arxiv preprint papineni roukos ward zhu bleu method automatic evaluation machine translation acl press bar bogin berant wolf language generation recurrent generative adversarial networks pre training arxiv preprint rajeswar subramanian dutil pal courville adversarial tion natural language arxiv preprint romero ballas kahou chassang gatta bengio fitnets hints thin deep nets iclr salimans goodfellow zaremba cheung radford chen proved techniques training gans advances neural information processing systems sutton mcallester singh mansour policy gradient methods reinforcement learning function approximation nips xia zhao tian qin lai liu adversarial neural machine translation arxiv preprint yang chen wang improving neural machine translation conditional sequence generative adversarial nets arxiv preprint yang salakhutdinov cohen semi supervised ative domain adaptive nets arxiv preprint zhang wang seqgan sequence generative adversarial nets policy gradient aaai zhang gan fan chen henao shen carin adversarial feature matching text generation arxiv preprint zhu zheng jiaxian weinan jun yong texygen benchmarking platform text generation models arxiv preprint
