cascade approach neural abstractive summarization content selection fusion logan lebanoff franck dernoncourt doo soon kim walter chang fei liu computer science department university central florida orlando adobe research san jose ucf edu dernonco dkim com ucf edu abstract present empirical study favor cade architecture neural text summarization summarization practices vary widely news summarization provide sufcient training data meet requirement end end neural stractive systems perform content tion surface realization jointly generate abstracts systems pose challenge summarization evaluation force tent selection evaluated text generation evaluation remains unsolved problem paper present empirical results showing performance cascaded pipeline separately identies important content pieces stitches gether coherent text comparable outranks end end systems pipeline architecture allows exible tent selection nally discuss advantage cascaded pipeline ral text summarization shed light portant directions future research introduction variety successful summarization plications afford large number annotated examples sufcient meet requirement end end neural abstractive summarization examples range ing radiology reports jing zhang congressional bills kornilova man meeting conversations mehdad koay lack annotated resources suggests end end systems lution neural text summarization increasing need develop cascaded architectures allow customized content selectors bined general purpose neural text generators realize potential neural abstractive summarization advocate explicit content selection lows rigorous evaluation visualization intermediate results module associating text generation existing ral abstractive systems perform content tion implicitly end end models celikyilmaz raffel lewis explicitly nal module select important sentences words aid generation tan gehrmann chen bansal kryscinski hsu lebanoff liu lapata content selection concerns selection important ments document cohesiveness selected segments text selected order neural text generator duce summary paper aim investigate feasibility cascade approach neural text tion explore constrained summarization task abstract created sentence time cascaded pipeline pipeline ture chooses sentences source document highlights summary worthy segments uses basis composing summary sentence pair sentences selected important ensure fusible exists cohesive devices tie sentences coherent text avoid generating nonsensical outputs geva lebanoff highlighting sentence segments allows perform grained content selection guides neural text generator stitch selected segments coherent sentence contributions work summarized follows figure model architecture divide task main components rst component performs sentence selection grained content selection posed classication problem tagging problem respectively second component receives rst component outputs supplementary information generate summary cascade architecture provides necessary exibility separate content selection surface realization abstractive summarization present empirical study favor cascade architecture neural text marization cascaded pipeline chooses sentences document highlights important segments ments passed neural generator duce summary sentence quantitative results mance cascaded pipeline comparable outranks end end systems added benet exible content selection discuss advantage cade architecture shed light important directions future research cascade approach cascaded summarization approach focuses shallow abstraction makes use text mations sentence shortening paraphrasing fusion jing mckeown contrast deep abstraction tic analysis document required shallow approach helps produce abstracts vey important information crucially ing faithful original follows describe approach select single sentences sentence pairs document highlight summary worthy segments perform summary generation conditioned highlights selection singletons pairs approach iteratively selects sentences input document serve basis ing single summary sentence previous research suggests human written summary code publicly available com ucfnlp cascaded summ sentences created shortening single tence merging pair sentences lebanoff adopt setting present coarse strategy content selection strategy begins selecting sentence singletons pairs followed highlighting important ments sentences importantly strategy allows control segments bined summary sentencecompatible ments come single document sentence pair fusible sentences contrast important segments document provided neural generator gehrmann happen generator ily stitches text segments unrelated sentences yielding summary contains cinated content fails retain meaning original document falke lebanoff kryscinski expect sentence singleton pair lected document contains salient tent pair sentences contain content compatible given sentence pair sentences document model predicts valid instance compressed merged form summary sentence follow lebanoff use bert devlin perform cation bert natural choice takes sentences generates classication prediction treats input singleton pair sentences sequence tokens tokens fed series transformer block layers sisting multi head self attention modules rst transformer layer creates contextual sentation token successive layer renes representations additional sentpredhighlightnon grainedcontent selectionhighlightstudentnon highlight start dukestudentstudenthas encoderdecoderwordembeddinghighlightembeddinggeneration figure comparison highlighting strategies thresholding obtains best performance cls token added contain sentence resentation bert tuned task adding output layer nal layer resentation sequence seen vector weights sigmoid function model predicts psent sentence singleton pair appropriate based cls token representation scribe training data task fine grained content selection ing note previous architecture naturally extended perform grained content selection highlighting important words tences sentences selected erate fusion sentence desirable identify segments text sentences tentially compatible coarse method allows examine intermediate results compare ground truth cretely add classication layer nal layer representation token target word loss interpolated instance prediction sentences loss ing coefcient multi task learning jective shown improve performance number tasks guo vector weights sigmoid function model predicts phighlight ken token included output fusion calculated based given token representation information fusion given sentences taken document grained lights proceed describing fusion process generates summary sentence lected content model employs decoder architecture based pointer generator networks shown strong performance adaptations gehrmann feed sentence gleton pair encoder highlights derived grained content selector come form binary tags tags transformed highlight embedding token chosen content selector highlight embedding token chosen highlight embeddings added token embeddings element wise manner highlight token embeddings learned illustration shown figure highlights provide valuable intermediate resentation suitable shallow abstraction approach provides alternative methods use sophisticated representations syntactic semantic graphs filippova strube banarescu liu straightforward incorporate highlights encoder decoder fusion model obtaining highlights sequence tagging tially adapted new domains experimental results data annotation enable direct ison end end systems conduct ments widely cnn dataset report results cascade approach use procedure described lebanoff create training instances sentence selector grained content selector training data contains stances instance contains date sentences positive instance truth summary sentence formed pressing merging sentences instance tive positive instances highlight lemmatized unigrams appearing summary excluding punctuation add smoothing labels highlighting single words score score score probability thresholdingproportional input input valuethreshold valuethreshold valuepercentage words percentage words percentage words system sumbasic vanderwende lexrank erkan radev pointer generator chen bansal bert extr lebanoff bottomup gehrmann bert abs lebanoff cascade fusion cascade tag sent sys tag sent sys tag fusion sent tag sent tag fusion system sents duke student admitted hanging noose rope tree near student union university ofcials said thursday student identied investigation pus police ofce student affairs admitted placing noose tree early wednesday university said cascade fusion duke student identied tion campus police ofce student affairs admitted placing noose tree early wednesday sents news release said student longer pus face student conduct review duke university private college students durham north carolina sents fusion duke university student longer pus face student conduct review reference student longer duke university campus face disciplinary review table left summarization results cnn test set cascade approach performs comparable strong extractive abstractive baselines oracle models ground truth sentences segment highlights perform best right example source sentences fusions dark highlighting content taken rst sentence light highlighting comes second cascade fusion approach effectively performs entity replacement replacing student second sentence duke student rst sentence nect highlighted phrases ing isolated stopwords test time scored instances selected document important segments highlighted content lector passed fusion step produce summary sentence hyperparameter weighing target word loss set highlighting threshold value model hyperparameters tuned validation split summarization results experimental results standard test set evaluated rouge metrics lin table mance cascade approaches cascade fusion cascade tag comparable outranks number extractive abstractive baselines particularly cascade tag use fusion step output grained content selection cascade fusion provides direct parison bert abs lebanoff uses sentence selection fusion lacks grained content selector results suggest coarse content selection strategy remains necessary guide fusion model produce informative sentences observe addition fusion model moderate impact rouge scores fusion process reorder text segments create true grammatical sentences shown ble analyze performance number oracle models use ground truth sentence selection sent tagging tag given ground truth sentences input cascade models achieve points improvement rouge metrics models given ground truth highlights achieve additional points improvement preliminary amination observe highlights included summary fusion indicating space improvement results cascade architectures great potential generate shallow abstracts future emphasis placed accurate content selection highlight important quantify highlighting required generating summary sentence highlighting little unhelpful experiment methods determine appropriate words highlight probability olding chooses set threshold words probability higher threshold highlighted proportional input highest probability words iteratively lighted target rate reached highlighting proportional total ber words instance sentences document containing sentences selected document investigate effect varying highlighting figure ods probability thresholding performs best gives freedom content selection model scores words sentences highly correspondingly highlight words words score highly pick highlighting certain percentage words tend perform dataset old value produces best rouge scores interestingly thresholds end lighting words sentence compared generator trained median sentence lighted system rate highlighting higher model highlighting rate set similar ground truth yields lower rouge scores threshold value ure observation suggests highlighting related effectiveness content selector better highlight conclusion present cascade approach neural tive summarization separates content selection surface realization importantly approach makes use text highlights intermediate resentation derived tences coarse content selection egy passed neural text generator pose summary sentence successful cascade approach expected accurately select sentences highlight appropriate text customized domain specic tasks acknowledgments grateful anonymous reviewers comments suggestions research supported national science tion grant references laura banarescu claire bonial shu cai madalina georgescu kira griftt ulf hermjakob kevin knight philipp koehn martha palmer nathan schneider abstract meaning representation sembanking proceedings tic annotation workshop interoperability discourse pages soa bulgaria tion computational linguistics yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proceedings annual ing association computational tics volume long papers pages bourne australia association computational linguistics jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language derstanding proceedings conference north american chapter association computational linguistics human language technologies volume long short papers pages minneapolis minnesota ation computational linguistics gunes erkan dragomir radev lexrank graph based lexical centrality salience text journal articial intelligence summarization research tobias falke leonardo ribeiro prasetya ajie ido dagan iryna gurevych utama ranking generated summaries correctness teresting challenging application natural guage inference proceedings annual meeting association computational guistics pages florence italy tion computational linguistics katja filippova michael strube sentence fusion dependency graph compression ceedings conference empirical ods natural language processing pages honolulu hawaii association tional linguistics sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference pirical methods natural language processing pages brussels belgium association computational linguistics mor geva eric malmi idan szpektor jonathan berant discofuse large scale dataset discourse based sentence fusion ings conference north american chapter association computational guistics human language technologies volume long short papers pages neapolis minnesota association computational linguistics asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents proceedings abstractive summarization conference north american chapter association computational linguistics man language technologies volume long pers pages new orleans louisiana association computational linguistics han guo ramakanth pasunuru mohit bansal autosem automatic task selection proceedings ing multi task learning conference north american chapter association computational linguistics man language technologies volume long short papers pages minneapolis nesota association computational linguistics wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun unied model extractive abstractive summarization proceedings inconsistency loss annual meeting association putational linguistics volume long papers pages melbourne australia association computational linguistics baoyu jing zeya wang eric xing describe conclude exploiting structure information chest ray reports proceedings annual meeting association computational linguistics pages rence italy association computational tics hongyan jing kathleen mckeown cut paste based text summarization meeting north american chapter association computational linguistics jia jin koay alexander roustai xiaojin dai alec dillon fei liu domain ogy affects meeting summarization performance proceedings international conference computational linguistics coling anastassia kornilova vladimir eidelman billsum corpus automatic summarization legislation proceedings workshop new frontiers summarization pages hong kong china association computational linguistics wojciech kryscinski nitish shirish keskar bryan cann caiming xiong richard socher neural text summarization critical evaluation proceedings conference empirical methods natural language processing international joint conference natural guage processing emnlp ijcnlp pages hong kong china association tional linguistics wojciech kryscinski romain paulus caiming xiong richard socher improving abstraction text summarization proceedings conference empirical methods natural guage processing pages brussels gium association computational linguistics logan lebanoff john muchovej franck dernoncourt doo soon kim seokhwan kim walter chang fei liu analyzing sentence fusion proceedings stractive summarization workshop new frontiers summarization pages hong kong china association computational linguistics logan lebanoff john muchovej franck dernoncourt doo soon kim lidan wang walter chang fei liu understanding points dence sentences abstractive tion proceedings annual meeting association computational linguistics dent research workshop seattle united states sociation computational linguistics logan lebanoff kaiqiang song franck dernoncourt doo soon kim seokhwan kim walter chang fei liu scoring sentence singletons pairs abstractive summarization proceedings annual meeting association computational linguistics pages rence italy association computational tics logan lebanoff kaiqiang song fei liu adapting neural encoder decoder framework single multi document summarization proceedings conference empirical methods natural language processing pages brussels belgium association computational linguistics mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov luke zettlemoyer bart denoising sequence sequence training natural language generation translation comprehension proceedings nual meeting association computational linguistics pages online association computational linguistics manling lingyu zhang heng richard radke meeting summaries topic abstractive multi modal meeting summarization proceedings association computational linguistics pages florence italy association tational linguistics annual meeting chin yew lin rouge package matic evaluation summaries text tion branches pages barcelona spain association computational linguistics fei liu jeffrey flanigan sam thomson norman sadeh noah smith tive summarization semantic representations proceedings conference north american chapter association tional linguistics human language technologies pages denver colorado association computational linguistics yang liu mirella lapata hierarchical formers multi document summarization ceedings annual meeting ciation computational linguistics pages florence italy association tional linguistics yashar mehdad giuseppe carenini frank tompa raymond abstractive meeting marization entailment fusion ings european workshop natural guage generation pages soa bulgaria association computational linguistics colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei peter liu exploring limits transfer learning unied text text abigail peter liu christopher ning point summarization proceedings pointer generator networks annual meeting association computational linguistics volume long papers pages vancouver canada association computational linguistics jiwei tan xiaojun wan jianguo xiao abstractive document summarization proceedings based attentional neural model annual meeting association computational linguistics volume long papers pages vancouver canada association computational linguistics lucy vanderwende hisami suzuki chris brockett ani nenkova sumbasic focused summarization sentence tion lexical expansion information processing management yuhao zhang derek merck emily bao tsai pher manning curtis langlotz timizing factual correctness summary study summarizing radiology reports ings annual conference tion computational linguistics acl
