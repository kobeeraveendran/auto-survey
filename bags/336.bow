a cascade approach to neural abstractive summarization with content selection and fusion logan lebanoff franck dernoncourt doo soon kim walter chang fei liu computer science department university of central florida orlando fl adobe research san jose ca ucf edu dernonco dkim com ucf edu abstract we present an empirical study in favor of a cade architecture to neural text summarization summarization practices vary widely but few other than news summarization can provide a sufcient amount of training data enough to meet the requirement of end to end neural stractive systems which perform content tion and surface realization jointly to generate abstracts such systems also pose a challenge to summarization evaluation as they force tent selection to be evaluated along with text generation yet evaluation of the latter remains an unsolved problem in this paper we present empirical results showing that the performance of a cascaded pipeline that separately identies important content pieces and stitches them gether into a coherent text is comparable to or outranks that of end to end systems whereas a pipeline architecture allows for exible tent selection we nally discuss how we can take advantage of a cascaded pipeline in ral text summarization and shed light on portant directions for future research introduction there is a variety of successful summarization plications but few can afford to have a large number of annotated examples that are sufcient to meet the requirement of end to end neural abstractive summarization examples range from ing radiology reports jing et al zhang et al to congressional bills kornilova and man and meeting conversations mehdad et al li et al koay et al the lack of annotated resources suggests that end end systems may not be a one all lution to neural text summarization there is an increasing need to develop cascaded architectures to allow for customized content selectors to be bined with general purpose neural text generators to realize the full potential of neural abstractive summarization we advocate for explicit content selection as it lows for a rigorous evaluation and visualization of intermediate results of such a module rather than associating it with text generation existing ral abstractive systems can perform content tion implicitly using end to end models see et al celikyilmaz et al raffel et al lewis et al or more explicitly with an nal module to select important sentences or words to aid generation tan et al gehrmann et al chen and bansal kryscinski et al hsu et al lebanoff et al liu and lapata however content selection concerns not only the selection of important ments from a document but also the cohesiveness of selected segments and the amount of text to be selected in order for a neural text generator to duce a summary in this paper we aim to investigate the feasibility of a cascade approach to neural text tion we explore a constrained summarization task where an abstract is created one sentence at a time through a cascaded pipeline our pipeline ture chooses one or two sentences from the source document then highlights their summary worthy segments and uses those as a basis for composing a summary sentence when a pair of sentences are selected it is important to ensure that they are fusible there exists cohesive devices that tie the two sentences together into a coherent text to avoid generating nonsensical outputs geva et al lebanoff et al highlighting sentence segments allows us to perform ne grained content selection that guides the neural text generator to stitch selected segments into a coherent sentence the contributions of this work are summarized as follows t c o l c s c v v i x r a figure model architecture we divide the task between two main components the rst component performs sentence selection and ne grained content selection which are posed as a classication problem and a tagging problem respectively the second component receives the rst component s outputs as supplementary information to generate the summary a cascade architecture provides the necessary exibility to separate content selection from surface realization in abstractive summarization we present an empirical study in favor of a cascade architecture for neural text marization our cascaded pipeline chooses one or two sentences from the document and highlights their important segments these ments are passed to a neural generator to duce a summary sentence our quantitative results show that the mance of a cascaded pipeline is comparable to or outranks that of end to end systems with added benet of exible content selection we discuss how we can take advantage of a cade architecture and shed light on important directions for future research a cascade approach our cascaded summarization approach focuses on shallow abstraction it makes use of text mations such as sentence shortening paraphrasing and fusion jing and mckeown and is in contrast to deep abstraction where a full tic analysis of the document is often required a shallow approach helps produce abstracts that vey important information while crucially ing faithful to the original in what follows we describe our approach to select single sentences and sentence pairs from the document highlight summary worthy segments and perform summary generation conditioned on highlights selection of singletons and pairs our approach iteratively selects one or two sentences from the input document they serve as the basis for ing a single summary sentence previous research suggests that of human written summary code is publicly available at com ucfnlp cascaded summ sentences are created by shortening a single tence or merging a pair of sentences lebanoff et al we adopt this setting and present a coarse strategy for content selection our strategy begins with selecting sentence singletons and pairs followed by highlighting important ments of the sentences importantly the strategy allows us to control which segments will be bined into a summary sentencecompatible ments come from either a single document sentence or a pair of fusible sentences in contrast when all important segments of the document are provided to a neural generator all at once gehrmann et al it can happen that the generator ily stitches together text segments from unrelated sentences yielding a summary that contains cinated content and fails to retain the meaning of the original document falke et al lebanoff et al kryscinski et al we expect a sentence singleton or pair to be lected from the document if it contains salient tent moreover a pair of sentences should contain content that is compatible with each other given a sentence or pair of sentences from the document our model predicts whether it is a valid instance to be compressed or merged to form a summary sentence we follow lebanoff et al to use bert devlin et al to perform the cation bert is a natural choice since it takes one or two sentences and generates a classication prediction it treats an input singleton or pair of sentences as a sequence of tokens the tokens are fed to a series of transformer block layers sisting of multi head self attention modules the rst transformer layer creates a contextual sentation for each token and each successive layer further renes those representations an additional sentpredhighlightnon grainedcontent selectionhighlightstudentnon highlight start dukestudentstudenthas encoderdecoderwordembeddinghighlightembeddinggeneration figure comparison of various highlighting strategies thresholding obtains the best performance cls token is added to contain the sentence resentation bert is ne tuned for our task by adding an output layer on top of the nal layer resentation hl for sequence s as seen in eq where u is a vector of weights and is the sigmoid function the model predicts psent whether the sentence singleton or pair is an appropriate one based on the cls token representation we scribe the training data for this task in fine grained content selection it is ing to note that the previous architecture can be naturally extended to perform ne grained content selection by highlighting important words of tences when two sentences are selected to erate a fusion sentence it is desirable to identify segments of text from these sentences that are tentially compatible with each other the coarse ne method allows us to examine the intermediate results and compare them with ground truth cretely we add a classication layer to the nal layer representation hl i for each token wi eq the per target word loss is then interpolated with instance prediction one or two sentences loss ing a coefcient such a multi task learning jective has been shown to improve performance on a number of tasks guo et al i where v is a vector of weights and is the sigmoid function the model predicts phighlight for each ken whether the token should be included in the output fusion calculated based on the given token s representation information fusion given one or two sentences taken from a document and their ne grained lights we proceed by describing a fusion process that generates a summary sentence from the lected content our model employs an decoder architecture based on pointer generator networks that has shown strong performance on its own and with adaptations see et al gehrmann et al we feed the sentence gleton or pair to the encoder along with highlights derived by the ne grained content selector the latter come in the form of binary tags the tags are transformed to a highlight on embedding for each token if it is chosen by the content selector and a highlight off embedding for each token not chosen the highlight on off embeddings are added to token embeddings in an element wise manner both highlight and token embeddings are learned an illustration is shown in figure highlights provide a valuable intermediate resentation suitable for shallow abstraction our approach thus provides an alternative to methods that use more sophisticated representations such as syntactic semantic graphs filippova and strube banarescu et al liu et al it is more straightforward to incorporate highlights into an encoder decoder fusion model and obtaining highlights through sequence tagging can be tially adapted to new domains experimental results data and annotation to enable direct ison with end to end systems we conduct ments on the widely used cnn dm dataset see et al to report results of our cascade approach we use the procedure described in lebanoff et al to create training instances for the sentence selector and ne grained content selector our training data contains stances every instance contains one or two date sentences it is a positive instance if a truth summary sentence can be formed by pressing or merging sentences of the instance tive otherwise for positive instances we highlight all lemmatized unigrams appearing in the summary excluding punctuation we further add smoothing to the labels by highlighting single words that r l f score f score f score probability thresholdingproportional to input all to input valuethreshold valuethreshold valuepercentage of words percentage of words percentage of words system r l sumbasic vanderwende et al lexrank erkan and radev pointer generator see et al chen and bansal bert extr lebanoff et al bottomup gehrmann et al bert abs lebanoff et al cascade fusion ours cascade tag ours gt sent sys tag gt sent sys tag fusion gt sent gt tag gt sent gt tag fusion system sents a duke student has admitted to hanging a noose made of rope from a tree near a student union university ofcials said thursday the student was identied during an investigation by pus police and the ofce of student affairs and admitted to placing the noose on the tree early wednesday the university said cascade fusion a duke student was identied during an tion by campus police and the ofce of student affairs and admitted to placing the noose on the tree early wednesday gt sents in a news release it said the student was no longer on pus and will face student conduct review duke university is a private college with about students in durham north carolina gt sents fusion duke university student was no longer on pus and will face student conduct review reference student is no longer on duke university campus and will face disciplinary review table left summarization results on cnn dm test set our cascade approach performs comparable to strong extractive and abstractive baselines oracle models using ground truth sentences and segment highlights perform the best right example source sentences and their fusions dark highlighting is content taken from the rst sentence and light highlighting comes from the second our cascade fusion approach effectively performs entity replacement by replacing student in the second sentence with a duke student from the rst sentence nect two highlighted phrases and by ing isolated stopwords at test time four scored instances are selected per document their important segments are highlighted by content lector then passed to the fusion step to produce a summary sentence each the hyperparameter for weighing the per target word loss is set to and highlighting threshold value is the model hyperparameters are tuned on the validation split summarization results we show experimental results on the standard test set and evaluated by rouge metrics lin in table the mance of our cascade approaches cascade fusion and cascade tag is comparable to or outranks a number of extractive and abstractive baselines particularly cascade tag does not use a fusion step and is the output of ne grained content selection cascade fusion provides a direct parison against bert abs lebanoff et al that uses sentence selection and fusion but lacks a ne grained content selector our results suggest that a coarse content selection strategy remains necessary to guide the fusion model to produce informative sentences we observe that the addition of the fusion model has only a moderate impact on rouge scores but the fusion process can reorder text segments to create true and grammatical sentences as shown in ble we analyze the performance of a number of oracle models that use ground truth sentence selection gt sent and tagging gt tag when given ground truth sentences as input our cascade models achieve points of improvement in all rouge metrics when the models are also given ground truth highlights they achieve an additional points of improvement in a preliminary amination we observe that not all highlights are included in the summary during fusion indicating there is space for improvement these results show that cascade architectures have great potential to generate shallow abstracts and future emphasis may be placed on accurate content selection how much should we highlight it is important to quantify the amount of highlighting required for generating a summary sentence highlighting too much or too little can be unhelpful we experiment with three methods to determine the appropriate amount of words to highlight probability olding chooses a set threshold whereby all words that have a probability higher than the threshold are highlighted when proportional to input is used the highest probability words are iteratively lighted until a target rate is reached the amount of highlighting can be proportional to the total ber of words per instance one or two sentences or per document containing all sentences selected for the document we investigate the effect of varying the amount of highlighting in figure among the three ods probability thresholding performs the best as it gives more freedom to content selection if the model scores all of the words in sentences highly then we should correspondingly highlight all of the words if only very few words score highly then we should only pick those few highlighting a certain percentage of words tend to perform less well on our dataset a old value of produces the best rouge scores interestingly these thresholds end up lighting of the words of each sentence compared to what the generator was trained on which had a median of of each sentence lighted the system s rate of highlighting is higher if the model s highlighting rate is set to be similar to that of the ground truth it yields much lower rouge scores threshold value of in ure this observation suggests that the amount of highlighting can be related to the effectiveness of content selector and it may be better to highlight more than less conclusion we present a cascade approach to neural tive summarization that separates content selection from surface realization importantly our approach makes use of text highlights as intermediate resentation they are derived from one or two tences using a coarse content selection egy then passed to a neural text generator to pose a summary sentence a successful cascade approach is expected to accurately select sentences and highlight an appropriate amount of text both can be customized for domain specic tasks acknowledgments we are grateful to the anonymous reviewers for their comments and suggestions this research was supported in part by the national science tion grant references laura banarescu claire bonial shu cai madalina georgescu kira griftt ulf hermjakob kevin knight philipp koehn martha palmer and nathan schneider abstract meaning representation for sembanking in proceedings of the tic annotation workshop and interoperability with discourse pages soa bulgaria tion for computational linguistics yen chun chen and mohit bansal fast tive summarization with reinforce selected sentence rewriting in proceedings of the annual ing of the association for computational tics volume long papers pages bourne australia association for computational linguistics jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language derstanding in proceedings of the conference of the north american chapter of the association for computational linguistics human language technologies volume long and short papers pages minneapolis minnesota ation for computational linguistics gunes erkan and dragomir r radev lexrank graph based lexical centrality as salience in text journal of articial intelligence summarization research tobias falke leonardo f r ribeiro prasetya ajie ido dagan and iryna gurevych utama ranking generated summaries by correctness an teresting but challenging application for natural guage inference in proceedings of the annual meeting of the association for computational guistics pages florence italy tion for computational linguistics katja filippova and michael strube sentence fusion via dependency graph compression in ceedings of the conference on empirical ods in natural language processing pages honolulu hawaii association for tional linguistics sebastian gehrmann yuntian deng and alexander rush bottom up abstractive summarization in proceedings of the conference on pirical methods in natural language processing pages brussels belgium association for computational linguistics mor geva eric malmi idan szpektor and jonathan berant discofuse a large scale dataset in for discourse based sentence fusion ings of the conference of the north american chapter of the association for computational guistics human language technologies volume long and short papers pages neapolis minnesota association for computational linguistics asli celikyilmaz antoine bosselut xiaodong he and yejin choi deep communicating agents for in proceedings of the abstractive summarization conference of the north american chapter of the association for computational linguistics man language technologies volume long pers pages new orleans louisiana association for computational linguistics han guo ramakanth pasunuru and mohit bansal autosem automatic task selection and in proceedings of the ing in multi task learning conference of the north american chapter of the association for computational linguistics man language technologies volume long and short papers pages minneapolis nesota association for computational linguistics wan ting hsu chieh kai lin ming ying lee kerui min jing tang and min sun a unied model for extractive and abstractive summarization in proceedings of the using inconsistency loss annual meeting of the association for putational linguistics volume long papers pages melbourne australia association for computational linguistics baoyu jing zeya wang and eric xing show describe and conclude on exploiting the structure information of chest ray reports in proceedings of the annual meeting of the association for computational linguistics pages rence italy association for computational tics hongyan jing and kathleen r mckeown cut and paste based text summarization in meeting of the north american chapter of the association for computational linguistics jia jin koay alexander roustai xiaojin dai alec dillon and fei liu how domain ogy affects meeting summarization performance in proceedings of the international conference on computational linguistics coling anastassia kornilova and vladimir eidelman billsum a corpus for automatic summarization of us legislation in proceedings of the workshop on new frontiers in summarization pages hong kong china association for computational linguistics wojciech kryscinski nitish shirish keskar bryan cann caiming xiong and richard socher neural text summarization a critical evaluation in proceedings of the conference on empirical methods in natural language processing and the international joint conference on natural guage processing emnlp ijcnlp pages hong kong china association for tional linguistics wojciech kryscinski romain paulus caiming xiong and richard socher improving abstraction in text summarization in proceedings of the conference on empirical methods in natural guage processing pages brussels gium association for computational linguistics logan lebanoff john muchovej franck dernoncourt doo soon kim seokhwan kim walter chang and fei liu analyzing sentence fusion in in proceedings of the stractive summarization workshop on new frontiers in summarization pages hong kong china association for computational linguistics logan lebanoff john muchovej franck dernoncourt doo soon kim lidan wang walter chang and fei liu understanding points of dence between sentences for abstractive tion in proceedings of the annual meeting of the association for computational linguistics dent research workshop seattle united states sociation for computational linguistics logan lebanoff kaiqiang song franck dernoncourt doo soon kim seokhwan kim walter chang and fei liu scoring sentence singletons and pairs for abstractive summarization in proceedings of the annual meeting of the association for computational linguistics pages rence italy association for computational tics logan lebanoff kaiqiang song and fei liu adapting the neural encoder decoder framework in from single to multi document summarization proceedings of the conference on empirical methods in natural language processing pages brussels belgium association for computational linguistics mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy veselin stoyanov and luke zettlemoyer bart denoising sequence to sequence training for natural language generation translation and comprehension in proceedings of the nual meeting of the association for computational linguistics pages online association for computational linguistics manling li lingyu zhang heng ji and richard j radke keep meeting summaries on topic abstractive multi modal meeting summarization in the proceedings of association for computational linguistics pages florence italy association for tational linguistics the annual meeting of chin yew lin rouge a package for matic evaluation of summaries in text tion branches out pages barcelona spain association for computational linguistics fei liu jeffrey flanigan sam thomson norman sadeh and noah a smith toward tive summarization using semantic representations in proceedings of the conference of the north american chapter of the association for tional linguistics human language technologies pages denver colorado association for computational linguistics yang liu and mirella lapata hierarchical formers for multi document summarization in ceedings of the annual meeting of the ciation for computational linguistics pages florence italy association for tional linguistics yashar mehdad giuseppe carenini frank tompa and raymond t ng abstractive meeting marization with entailment and fusion in ings of the european workshop on natural guage generation pages soa bulgaria association for computational linguistics colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu exploring the limits of transfer learning with a unied text to text former abigail see peter j liu and christopher d ning get to the point summarization in proceedings with pointer generator networks of the annual meeting of the association for computational linguistics volume long papers pages vancouver canada association for computational linguistics jiwei tan xiaojun wan and jianguo xiao abstractive document summarization with a in proceedings based attentional neural model of the annual meeting of the association for computational linguistics volume long papers pages vancouver canada association for computational linguistics lucy vanderwende hisami suzuki chris brockett and ani nenkova beyond sumbasic focused summarization with sentence tion and lexical expansion information processing and management yuhao zhang derek merck emily bao tsai pher d manning and curtis p langlotz timizing the factual correctness of a summary a study of summarizing radiology reports in ings of the annual conference of the tion for computational linguistics acl
