international journal on digital libraries manuscript no will be inserted by the editor scientic document summarization via citation contextualization and scientic discourse arman cohan nazli goharian n u j l c s c v v i x r a received date accepted date abstract the rapid growth of scientic literature has made it dicult for the researchers to quickly learn about the developments in their respective elds tic document summarization addresses this challenge by providing summaries of the important contributions of scientic papers we present a framework for tic summarization which takes advantage of the tations and the scientic discourse structure citation texts often lack the evidence and context to support the content of the cited paper and are even sometimes accurate we rst address the problem of inaccuracy of the citation texts by nding the relevant context from the cited paper we propose three approaches for textualizing citations which are based on query mulation word embeddings and supervised learning we then train a model to identify the discourse facets for each citation we nally propose a method for marizing scientic papers by leveraging the faceted tations and their corresponding contexts we evaluate our proposed method on two scientic summarization datasets in the biomedical and computational tics domains extensive evaluation results show that our methods can improve over the state of the art by large margins this is a pre print of an article published on ijdl the nal publication is available at springer via arman cohan e mail cs georgetown edu nazli goharian e mail cs georgetown edu information retrieval lab department of computer ence georgetown university washington dc usa introduction the rapid growth of scientic literature in recent decades has created a challenge for researchers in ious elds to keep up with the newest developments according to a recent study by bibliometric analysts the global scientic output doubles approximately every nine years further signifying this challenge tence of surveys in dierent elds shows that nding an overview of key developments in scientic areas is sirable however procuring such surveys requires sive human eorts scientic summarization aims at dressing this problem by providing a concise tion of important ndings and contributions of scientic papers reducing the time required to overview the tire paper to understand important contributions ticle abstracts are a basic form of scientic summaries while abstracts provide an overview of the paper they do not necessarily convey all the important tions and impacts of the paper the authors might ascribe contributions to their papers that are not existent some important contributions might not be included in the abstract the contributions stated in the abstract do not convey the article s impact over time iv abstracts usually provide a very broad view of the papers and they may not be detailed enough for people seeking detailed contributions v the tent distribution in the abstracts are not evenly drawn from dierent sections of the papers these lems have inspired another type of scientic summaries which are obtained by utilizing a set of citations encing the original paper each citation is ten accompanied by a short description explaining the ideas methods results or ndings of the cited work this short description is called citation text or citance therefore a set of citation texts by dierent arman cohan nazli goharian reference article voorhoeve et al these mirnas could neutralize mediated cdk inhibition possibly through direct inhibition of the expression of the tumor suppressor citing articles kloosterman and plasterk in a genetic screen and were found to allow proliferation of primary human cells voorhoeve et al okada et al two oncogenic mirnas and directly inhibit the expression of thereby allowing tumorigenic growth in the presence of voorhoeve et al fig example of epistemic value drift the claims that voorhoeve et al state as possibilities becomes fact in later citations okada et al kloosterman and plasterk pers can provide an overview of the main ideas methods and contributions of the cited paper and thus can form a summary of the referenced paper these community based summaries capture the important contributions of the paper view the article from multiple aspects and reect the impact of the article to the community at the same time there are multiple problems sociated with citation texts they are written by ferent authors so they may be biased toward another work the citation texts lack the context in terms of the details of the methods the data assumptions and results more importantly the points and claims by the original paper might be misunderstood by the citing authors certain contributions might be ascribed to the cited work that are not on par with the original thor s intent another serious problem is the tion of the epistemic value of claims which states that many claims by the original author might be stated as facts in the future citations an example of this is shown in figure as illustrated while the original authors write on some possibilities later the citing thors state them as known facts these problems are even more serious in biomedical domain where slight misrepresentations of the specic ndings about ments diagnosis and medications could directly aect human lives one way to address such problems is to consider the citations in their context from the reference article therefore citation texts should be linked to the cic parts in the reference paper that correctly reect them we call this citation contextualization tion contextualization is a challenging task due to the terminology variations between the citing and cited thor s language usage scientic papers have the unique characteristic of following a specic discourse structure for example a typical scientic discourse structure follows this form problem and motivation methods experiments results and implications the rhetorical status of a citation vides additional useful information that can be used in applications such as information extraction retrieval and summarization each citation text could refer to specic discourse facets of the referenced paper for example one citation could be about the main method of the referenced paper while the other one could tion their results identifying these discourse facets has distinct values for scientic summarization it allows creating more coherent summaries and diversifying the points included in the generated scientic summaries scientic summarization is recently further vated by summarization track and the computation linguistics summarization shared task following these works and motivated by the challenges mentioned above we propose a framework for scientic summarization based on citations our approach sists of the following steps contextualizing citation texts we propose several approaches for contextualizing citations finding the exact reference context for the citations is lenging due to discourse variation and terminology dierences between the citing and the referenced thors therefore traditional information retrieval ir methods are inadequate for nding the vant contexts we propose to address this challenge by query reformulations utilizing word embeddings and domain specic knowledge our main proach is a retrieval model for nding the ate context of the citations and is designed to handle terminology variations between the citing and cited authors discourse structure after extracting the context of the citation texts we classify them into dierent course facets we use a linear classier with variety of features for classifying the citations summarization we propose two approaches for summarizing the papers both approaches are based on summarization through the scientic community where the main points of a paper are captured by a set of given citations our approach extends the vious works on citation based summarization by including the reference context to address the inaccuracy problem associated with the citation texts after extracting the citation contexts from the reference paper we group them into dierent text analysis conference nist scientic document summarization via citation contextualization and scientic discourse discourse facets then using the most central tences in each group we generate the nal summary in particular our contributions are summarized as follows an approach for extracting the context of the citation texts from the reference article fying the discourse facets of the citation contexts a scientic summarization framework utilizing citation contexts and the scientic discourse structure tensive evaluation on two scientic domains related work citation text analysis citations play an integral role in the scientic opment they help disseminate the new ndings and they allow new works to be grounded on previous forts while there is a large body of related work on analysis of citation networks instead of link ysis we focus on textual aspects of the citations to better utilize the citations researchers have explored ways to extract citation texts which are short textual parts describing some aspects of the cited work amples of the proposed approaches for extracting the citation texts include jointly modeling the link mation and the citation texts supervised markov random fields classiers and sequence labeling with segment classication these approaches focus on nding the sentences or textual spans in the citing article that explain some aspects of the cited work in this work we assume that citation texts are already tained either manually or by using one of these works given the citation texts we instead focus on alizing these citation texts using the reference we nd the text spans in the reference article that most closely reect the citation text there exists some related work on further ing the citations for nding their function or rhetorical status in these works the authors tried to identify the reasons behind citations which can be a statement of weakness contrast or comparison age or compatibility or a neutral category they posed a classication framework based on lexically and linguistically inspired features for classifying citation functions the distribution of citations within the ture of scientic papers have been also studied the authors of have investigated the problem of suring the intensity of the citations in scientic papers and in the authors proposed using the discourse facets for scientic article recommendation recently a framework for understanding citation function has been proposed which unies all the previous forts in terms of denition of citation functions while citation function can provide additional information for summarization in this work we do not utilize these formation instead we utilize the discourse facet of the citation contexts in a reference paper citation contextualization more recently there has been some eorts in alizing citations from the reference in particular tac summarization and the cl scisumm shared task on computational linguistic summarization have released datasets to promote research for tion contextualization the former is more domain cic focusing on biomedical scientic literature while the latter is in a more general domain consisting of lications in computational linguistics to our edge there is no overview paper on tac we briey discuss the successful approaches in cl scisumm the authors of used an svm rank approach with features such as tf cosine similarity position of the reference sentence section position and named entity features in another approach the authors used an svm classier with sentence similarity and lexicon based features the authors of proposed a hybrid model based on tf idf similarity and a single layer neural network that scores the relevant reference texts above the irrelevant ones finally in the work by the thors proposed the use of textsentencerank algorithm which is an enhanced version of the textrank rithm for ranking keywords in the documents here we specically focus on the problem of terminology tion between the citing and cited authors we propose approaches that address this problem our proposed approaches are based on query reformulations word embeddings and domain specic knowledge text summarization document summarization has been an active research area in nlp in recent decades there is a rich literature on text summarization approaches towards rization can be divided into the following categories i topic modeling based in these proaches the content or topical distribution of the nal summary is estimated using a probabilistic work solving an optimization problem these approaches cast the summarization problem as nist term frequency inverted document frequency arman cohan nazli goharian an optimization problem where an objective function needs to be optimized with respect to some constraints iii supervised models where selection of sentences in the summary are learned using a vised framework iv graph based these proaches seek to nd the most central sentences in a document s graph where sentences are nodes and edges are similarities v heuristic based these works approach the summarization problem by greedy selection of the content vi neural networks more cently there has been some eorts on utilizing neural networks and sequence to sequence models for erating summaries of short texts and sentences most of these works have focused on general domain summarization and news articles scientic articles are much dierent than news articles in elements such as length language complexity and structure one of the rst works in scientic article rization is done by where the authors trained a supervised naive bayes classier to select informative content for the summary later the impact of citations to generate scientic summaries was realized in the work by the authors proposed an approach for citation based summarization based on a clustering proach while in and the focused on producing coherent scientic summaries we argue that citation texts by themselves are not always accurate and they lack the context of the cited paper therefore if we only use the citation texts for scientic summarization the resulting summary would potentially suer from the same problems and it might not accurately reect the claims made in the original paper we address this problem by leveraging the citation contexts from the reference paper we also utilize the inherent discourse structure of the scientic documents to capture the portant content from all sections of the paper we present a comprehensive framework for scientic summarization which utilizes and builds upon our lier eorts we propose new approaches for citation contextualization we further extend our periments on an additional dataset cl scisum and evaluate our approaches on both tac and scisum datasets providing detailed analysis methodology our proposed method is a pipeline for summarizing entic papers it consists of the following steps citation contextualization extracting the relevant context from the reference paper summarization we rst explain our proposed methods for ization we then describe our approach for identifying discourse facets of the citation contexts and nally we outline our summarization approach citation contextualization citation contextualization refers to extracting the vant context from the reference article for a given tion text we propose the following three approaches for this problem query reformulation word beddings and domain knowledge and supervised classication query reformulation qr we cast the contextualization problem as an mation retrieval ir task we rst extract textual spans from the reference article and index them ing an ir model the textual spans are of ity of sentences in order to capture longer contexts those consisting of multiple consecutive sentences we also index sentence n grams that is we index each n consecutive sentences as a separate text span after constructing the index we consider the citation text as the query and we seek to nd the relevant context from the indexed spans since the citation texts are ten longer than usual queries in standard ir tasks we apply query reformulation methods on the citation to better retrieve the related context we utilize both eral and domain specic query reformulations for this purpose we rst remove the citation markers author names and year and numbered citations from the tations as they do not appear in the reference text and hence are not helpful we design several regular pressions to capture these names the proposed query reformulation qr methods are described below query reduction since the citation texts are usually more verbose than standard queries there might be many uninformative terms in them that do not tribute in nding the correct context hence we apply query reduction methods to only retain the important concepts in the citation after removing the stop words from the citation we further experiment with the lowing three query reduction methods noun phrases qr np citation texts are ally linguistically well formed as they are extracted identifying the discourse facet of the extracted we indexed up to consecutive sentences in our text ments scientic document summarization via citation contextualization and scientic discourse from scientic papers this allows us to apply a riety of linguistic tagging and chunking methods to the query to capture the informative phrases vious works have shown that noun phrases are good representation of informative concepts in the query we thus extract noun phrases from the citation text and omit all other terms key concepts qr kw key concepts or keywords are single or multi word expressions that are mative in nding the relevant context we use the inverted document frequency idf measure to nd the key concepts the terms that are lent throughout all the text spans do not provide much information in retrieval idf values help turing the terms and concepts that are more specic for key concept extraction we limit the idf values between some threshold that can be tuned ing to the dataset we consider phrases of up to three terms ontology qr domain domain specic ontologies are expert curated lexicons that contain specic concepts in this reformulation method we use an ontology to only keep important specic concepts in the query since the tac dataset is in the biomedical domain we use the umls thesaurus which is a comprehensive tology of biomedical concepts we specically use the snomed ct subset of umls as explained in section the indexing approach also contains consecutive sentences therefore our trieval approach can nd text spans that have laps with each other furthermore retrieving multiple spans from around the same location in the text nals the importance of that specic location we apply a reranking and merging method to the retrieved spans to remove shared spans and better rank the more evant context we merge the two overlapping spans if the retrieval score of the larger span is higher than the smaller span we also evaluated other query tion methods such as pseudo relevance feedback however they performed worse than the baseline and thus we do not discuss them further contextualization using word embeddings and domain knowledge to explicitly account for terminology variations and paraphrasing between the citing and the cited authors we propose another model for citation contextualization utilizing word embeddings and domain specic edge we empirically set this threshold to and for the tac and cl scisum datasets respectively embeddings word embeddings or distributed sentations of words are mapping of words to dense tors according to a distributional space with the goal that similar words will be located close to each other we extend the language modeling lm for tion retrieval model by utilizing word embeddings to account for terminology variations given a citation text query q and a reference span document d the lm scores based on the probability that has ated q using standard simplifying assumptions of term independence and uniform document prior we have n where qi i n are the terms in the query in lm with dirichlet smoothing is calculated using a smoothed maximum likelihood estimate d wv w d where f is the frequency function shows the background probability of term qi in collection c v is the entire vocabulary and is the dirichlet parameter our model extends the above formulation eq by using word embeddings in particular we estimate the probability according to the following equation djd dj dj d dj wv where dj are terms in the document d and s is a tion that captures the similarity between the terms and is dened as dj if otherwise where shows the unit vector corresponding to the embedding of word qi is a threshold and is a formation function below we explain the role of eter and the transformation function word embeddings can capture the similarity values of words according to some distance function most bedding methods represent the distance in the tional semantics space therefore similarities between two words qi and can be captured using the dot uct of their corresponding embeddings i e while high values of this product suggest syntactic and semantic relatedness between the two terms arman cohan nazli goharian word word similarity marker notebook capture blue produce mint sky promotion sky make table example of similarity values between terms ing to the dot product of their corresponding embeddings using the pre trained model on google news pus the top part of the table shows pairs of random words while the bottom part shows similarity values for pairs of related words many unrelated words have non zero dot products an example is shown in table therefore considering them in the retrieval model introduces noise and hurts the performance we address this issue by rst ering a threshold below which all similarity values are squashed to zero this ensures that only highly relevant terms contribute to the retrieval model to identify an appropriate value for we select a random set of words from the embedding model and calculate the average and standard deviation of point wise absolute values of similarities between the pairs of terms from these ples we then set to be two standard deviations larger than the average similarities to only consider very high similarity values we also observe that for high ity values between the terms the values are not criminative enough between more or less related words this is illustrated in figure where we can see that the most similar terms to the given term are not too discriminative in other words the similarity values cline slowly as moving away from top similar words we instead want only very top similar words to contribute to the retrieval score therefore we transform the ilarity values according to a logit function equation to dampen the eect of less similar words see figure log while any approach for training the word dings could be used we use the method which has proven eective in several word ity tasks we train on the recent dump of wikipedia since the tac dataset is in biomedical domain we also train embeddings on a domain specic collection we use the trec genomics collections and which together consist of billion kens wikimedia org fig normalized similarity values between a random word in the embedding model and the top similar words to it the axis is the word indexes and the y axis is the similarity values the orange line with markers shows the original similarity values while the green line with triangle markers shows the transformed values using the logit function the logit function dampens the similarity values of less similar words incorporating domain knowledge word embedding models learn the relationship between terms by being trained on a large corpus they are based on the butional hypothesis which states that similar words appear in similar contexts while these models have been very successful in capturing semantic relatedness recent related works have shown that domain ontologies and expert curated lexicons may contain information that are not captured by embeddings hence we account for the domain knowledge according to the following retrotting embeddings in this method we apply a post processing step called retrotting to the word embeddings used in the model retrotting optimizes an objective function that is based on lationships between words in a lexicon it intuitively pulls closer the words that are related to each other and pushes farther the words that are not related to each other according to a given ontology for the tology since tac data is in biomedical domain we use two domain specic ontologies and protein ontology pro for the cl scisum data since it is less domain specic we use the wordnet lexicon interpolating in the lm in this method instead of modifying the word vectors we incorporate the main knowledge directly in the retrieval model we do so by interpolation of two following probability estimates medical subject headings georgetown edu dot productnormalized logit scientic document summarization via citation contextualization and scientic discourse where is estimated using eq and is a ilar model that counts in the is synonym relations is syn in calculating similarities its formulation is exactly like eq except it replaces the function s with the following function if if qi is syn o this function is essentially partially counting the synonyms in calculation of the probability estimate by the amount of we empirically set the value of word embedding based methods are shown by we in short in the results supervised classication the two previous context retrieval models are pervised and as such do not take advantage of the ready labeled data cl scisum dataset includes rate training and testing sets which allow us to also vestigate supervised approaches we propose a rich classier to nd the correct context for each given citation our approach aims to capture the semantic latedness between a given citation text and a candidate context sentence we specically utilize the following features to capture this relatedness word match counts the number of identical words between the source citation text and the candidate reference context normalized by length fuzzy word match same as above with the ence that we use character n grams to capture tial matches between the words embedding based alignment measures the ity between the source and target sentences using word embedding alignment specically for the two sentences and the following function f scores the sentences based on their similarity v where s is a similarity function according to the equation intuitively captures the similarity tween the two sentences without only relying on ical overlaps it takes into account the similarity ues between the terms distance between average of embeddings measure the similarity between the two sentences by dot product of the average of their constituent word tors feature name citation text extracted reference context verb features ralative section position table features for identifying discourse facets similarity score between the citation text and the candidate reference tf idf and count vectorized similarities dot product between the sparse tf idf weighted or count weighted vectors associated with the source citation and get reference context character n gram tf idf and count vectorized larities same as above except that we used gram characters to allow partial word matches we train a standard linear classier e logistic regression using these features to identify the correct context for a given citation text identifying discourse facets the organization of scientic papers usually follows a standardized discourse pattern where the authors rst describe the problem or motivation then they talk about their methods then the results and nally cussion and implications our goal is to capture the important content from all sections of the paper fore after extracting the citation contexts we identify the associated discourse facet for each of the citation contexts retrieved from the previous step each citation context refers to some specic discourse facets of the reference document to identify the correct discourse facets we train a simple supervised model with tures listed in table essentially we use the citation text and the extracted reference context represented by character n grams the verbs in the context sentence and the relative position of the retrieved context in the paper as features for the classier while the textual features citation and it s context were the most ful we empirically observed slight improvements by corporating the verb and section position features we train the model using an svm classier for the textual features we transform them using character grams to allow fuzzy matching between the terms generating the summary after extracting reference contexts for the citations as described in section and identifying their discourse arman cohan nazli goharian facet section we generate a summary of the erence paper our goal is to create a summary that contains information from dierent discourse facets of the paper this helps not only in diversifying the tent in the summary but also in creating a more ent summary to generate a summary we rst identify the most representative sentences in each group itively we only need a few top representative sentences from each discourse facet to include in the summary in order to nd the most representative sentences we consider sentences in each facet as nodes and their ilarities as weighted edges in a graph we then apply the power method which is an algorithm similar to the pagerank random walk ranking model that nds the most central nodes in a graph it works by eratively updating the score of each sentence according to its centrality total weight of incoming edges and the centrality of its neighbors after ranking the tences in each group according to their centrality score we select sentences for the nal summary we use the following methods for creating the nal summary iterative this method simply iterates over the course facets and selects the top representative tence from each group until the summary length threshold is met greedy the iterative approach could result in lar sentences ending up in the summary this results in redundant information and potential exclusion of other important aspects of the paper from the mary to address this potential problem we use a heuristic that accounts for both the informativeness of candidate sentence and their novelty with respect to what is already included in the summary imal marginal relevance is one such heuristic that has these properties it is based on the linear interpolation of the informativeness and the novelty of the sentences experiments data we conducted our experiments on two scientic marization datasets the rst dataset is the tac scientic summarization dataset the tac benchmark is in biomedical domain and is publicly available upon request from nist the second dataset is the cl scisumm dataset which is available on a lic and contains scientic articles from the nist national institute of standards and technology com wing nus scisumm corpus characteristic tac cl scisum documents reference documents avg citing docs for each ref total citation texts avg gold summary length words stdev gold summary length words no separate train test sets yes table characteristics of the datasets number of avg average and stdev standard deviation computational linguistics domain to our knowledge these two are the only datasets on scientic rization the tac dataset only has one training set sisting of topics there is one reference article in each topic and another set of articles citing the erence for each topic annotators have identied the relevant contexts the correct discourse facet and they have written a summary the documents are vided as plain text les and there is no predened tence boundaries and sections on the other hand the cl scisumm data contain separate train development and test sets with topics in total similar to tac each topic consists of reference and a set of citing cles but in the computational linguistics domain the articles are in xml format with known sentence aries and sections another distinction is that topics in the cl scisumm data are annotated by one annotator at a time the full statistics of the datasets is illustrated in table the distribution of the discourse facets in the two datasets is also shown in figure since the two datasets are in dierent domains the dierence between the distribution of the facets is expected citation contextualization evaluation evaluation of the retrieved contexts is based on the overlap of the position of the retrieved contexts and the gold standard contexts per tac evaluation of the tac benchmark was performed using character oset overlaps weighted by human annotators more formally for a set of tem retrieved contexts s and gold standard context r by m annotators the weighted character based precision pchar and recall rchar are dened as follows pchar i m rchar i i nist biomedsumm guidelines html scientic document summarization via citation contextualization and scientic discourse a tac dataset cl scisum dataset fig distribution of discourse facets in each dataset method baselines vsm lmd lmd lda this work qr domain qr np qr kw wewiki webio it webio domain character oset overlap rouge pchar rchar fchar table results of citation contextualization on tac dataset the reported results are based on top retrieved contexts the top part shows the baselines and the bottom part shows our proposed model values are percentages domain query reformulation by domain ontology umls qr np query reformulation by noun phrases qr kw query reformulation by key words wewiki word embedding model with wikipedia embeddings webio word embedding model with biomedical embeddings it incorporating domain knowledge in biomedical embeddings by retrotting webio domain interpolated language model the ocial metric for the cl scisum challenge was sentence level overlaps of the retrieved contexts with the gold standard this was possible because unlike the articles in tac which were in plaintext format the tence boundaries in cl scisum were pre specied we also report character level metrics for the cl scisum corpus as we will see the character level and sentence level metrics are more or less comparable one problem with position based evaluation metrics character or sentence is that a system might retrieve a context that is in a dierent position than gold dard but similar to the content of the gold standard in such cases the system is not rewarded at all this is possible because authors might talk about a similar concept in dierent sections of the paper to consider textual similarities of the retrieved context with the gold standard we also compute rouge n scores comparison to our knowledge no review paper about the tac challenge was released hence for the tac dataset we compare our method against the following baselines vsm ranking by vector space model vsm with tf idf weighting of the citations and the target erence contexts scoring model which is a abilistic framework for ranking the relevant ments based on the query terms appearing in each document regardless of their relative proximity lmd language modeling with dirichlet smoothing lmd is a probabilistic framework that models the probability of documents generating the given query lmd lda an extension of the lmd retrieval model using latent dirichlet allocation lda which is recently proposed this model ers latent topics in ranking the relevant documents for the cl scisum data we also compare against the top best performing system for brief description about these approaches refer to section results the results on the tac dataset are presented in table we observe that our proposed methods arman cohan nazli goharian method psent rsent fsent pchar rchar fchar sentence overlap rouge character oset overlap other methods vsm lm tsr tf idf neural net svm rank jaccard fusion tf this work qr np qr kw wewiki it supervised table results of citation contextualization on cl scisum dataset the reported values are percentages the top part shows the baselines and state of the art models while the bottom part shows our methods p precision r recall f score sent subscript shows overlap by sentences and char subscript shows character oset overlaps qr np query reformulation by noun phrases qr kw query reformulation by key words wewiki word embedding model with wikipedia embeddings it incorporating domain knowledge in embeddings by retrotting prove over all the baselines query reformulation ods np and kw respectively obtain character oset scores of and which improve the best line by and they also obtain higher rouge scores this shows that noun phrases and key words can capture informative concepts in the citation that help better retrieving the related reference context our models based on word embeddings are also ing the baselines in virtually all metrics general main embeddings trained on wikipedia wewiki and domain specic embeddings trained on genomics data webio achieve scores of and with and improvement over the best baseline tively higher performance of the biomedical dings in comparison with general embeddings is pected because the words are captured in their correct context an example is shown in table where the top similar words to the word expression are shown the word expression in the biomedical context is dened as the process by which genetic instructions are used to synthesize gene products as we can see using general domain embeddings we might fail to capture this tion incorporating domain knowledge in the model sults in further improvement as shown in last two rows of table the model using retrotting it improves the best baseline by while the lated model achieves the highest provement by these results show the eectiveness of domain knowledge in the model general wiki interpretation sense emotion function show domain specic bio upregulation mrna protein induction cell table the words with highest similarity values to sion according to trained on wikipedia general domain and genomics collections biomedical domain table shows the results for the cl scisum dataset the rst rows are baselines that also are reported in tac evaluation in addition to those lines we also consider top performing state of the art systems of cl scisum lines as additional baselines to compare with for the cl scisum ipating systems we report the ocial sentence based evaluation metrics the rouge scores and character based metrics were not reported in the ocial ation of the task some of our methods are specic to the biomedical domain such as webio therefore we do not evaluate those on the cl scisum dataset which is in a completely dierent domain as shown in table our methods outperform the state of the art on this dataset as well the embedding based model with wikipedia trained scientic document summarization via citation contextualization and scientic discourse eect of the parameter on the interpolated alization model tuned on the tac dataset eect of the parameter on the interpolated alization model tuned on the tac dataset the eect cut o of point in returning the top results the wewiki model tuned on cl scisum dataset for fig parameters of the model for contextualization feature character n gram tf idf similarity tf idf similarity embedding based alignment distance average embeddings similarity score character n gram count similarity fuzzy word match count based similarity word match weight table the weights normalized corresponding to the top features in the supervised method for citation ization cl scisum dataset tf idf similarity based features and embedding based features are the most helpful while the count based similarity and word matching features are among the least helpful features beddings wewiki achieves the best results with score of sentence overlaps which is slightly higher than the score of achieved by the best ous work tf in the table interestingly we observe that retrotting it does not improve over the standard embedding based approach this is likely due to the choice of the wordnet lexicon for retrotting while wordnet contains general domain terms it does not necessarily capture relationships of words in the context of computational linguistics in contrast to tac where we had a domain specic icon suitable for the dataset for the cl scisum data we did not nd any lexicon capturing the term tionships in the computational linguistics domain we believe that retrotting with such lexicon could result in further improvements while query based approaches improve over most of the baselines their performance fall below the best baseline system on the other hand our supervised method also proves the best baseline achieving the highest overall prevision and and number of citations number of annotators with at least partial agreement no agreement table the table shows the number of citations grouped by the number of annotators that agree at least partially on the context scores it is encouraging that our based models method names starting with we in the table which are unsupervised models achieve the best results on this task and surpass the mance of the feature rich supervised models table shows the importance of each feature for our vised method explained in while the most important features are n gram and character n gram based tf idf similarity embedding based alignment and distance of average embeddings are also important in nding the correct context as evident from tables and the absolute tem performances are not high which further shows that this task is challenging since the tac data are annotated by people we investigate the diculty of this task for the human annotators to do so we culate the agreement of the annotators with respect to the relevant context for the citations table shows the number of citations grouped by the number of tors that agree at least partially on the correct context as illustrated there are citations out of that all annotators have partial agreement on the context span this shows that the contextualization task is not trivial even for the human expert annotators we do not report results of supervised model on tac dataset because the tac data do not have separate train and test sets arman cohan nazli goharian method other methods smo decision tree fusion method jaccard cascade jaccard focused method this work qr np qr kw wewiki supervised p r f table results for identifying the discourse facets for the retrieved contexts the metrics are precision p recall r and score f of the identied discourse facets contingent on the correct retrieved span parameters our interpolated model of embeddings and domain knowledge has two main rameters and figure shows the sensitivity of our model to dierent parameters we observe that the best performance is achieved when and our models retrieve a ranked list of contexts for the tions we choose a cut o point for returning the nal results figure shows the eect of the cut o point on one of our models we observe that the optimal cut o point for best sentence score is discourse facet p r f aim hypothesis implication method results average total table the classier s intrinsic performance for ing the discourse facets on the cl scisum dataset identifying discourse facets evaluation the ocial metric for evaluation of course facet identication is the precision recall and scores of the discourse facets conditioned on the correctness of the retrieved reference context therefore we report the results for the cl scisum data based on this metric for the tac dataset the cial metric is the classication accuracy weighted by the annotator agreements the accuracy for a tem returned discourse facet is the number of tors agreeing with that discourse facet divided by total number of annotators results table shows the results of our methods pared with the top performing ocial submitted runs to the cl scisum we do not report the results of low performing systems the classication algorithm for identifying the discourse facets is the method scribed in section across all our methods however since only the correct retrieved contexts are rewarded the cut o point has similar eect on all the models nist biomedsumm the performance of each model diers based on the curacy of retrieving the correct contexts we observe that most of our methods except for the qr np prove over all the baselines in terms of all metrics we obtain substantial improvements especially in terms of precision the best method for identifying the discourse facets is the supervised method indicated with vised in the table which obtains score proving the best baseline jaccard focused method by embedding methods also perform well by taining scores of for the wikipedia dings and for the retrotted embeddings these results further show the eectiveness of our ization methods along with the proposed classier for identifying the facets we also demonstrate the intrinsic performance of our classier for identifying the discourse facets in ble as illustrated the weighed average mance over all discourse facets is one challenge in identifying the discourse facets is the unbalanced dataset and the limited number of training examples for some specic facets as also reected in the table we observe that for categories with smaller number of scientic document summarization via citation contextualization and scientic discourse svm rf lr oracle tac cl scisum table eect of learning algorithms in identifying the discourse facets svm support vector machine with linear kernel rf random forest lr logistic regression cle highest achievable score numbers are weighted accuracy scores by annotators instances the performance is generally lower we fore believe that having more training samples in the rare categories could further increase the performance table shows the results of facet identication in the tac dataset as well as the eect of learning rithms since for the tac dataset there are tors and the ocial metric is weighted accuracy scores we also calculate the oracle score by always predicting what the majority of the annotators agree on the acle achieves percent suggesting that identifying discourse facets is not trivial for humans we can see that the svm classier achieves the highest results with relative accuracy to the oracle for the cl scisum dataset there is only one annotator per discourse facet and therefore the weighted accuracy metrics translates to simple accuracy scores summarization we evaluate our summarization approach against the gold standard summaries written by human annotators we set the summary length threshold to the average length of summary by words in each dataset see table table shows the results for the summarization task the rst lines show the baselines which are ing summarization approaches including the sic algorithm and the original citation based marization approach the next four lines are the top state of the art systems on the cl scisum dataset for the cl scisum systems the ocial reported sults only included and rouge scores as illustrated in the table virtually all our methods improve over the state of the art showing the ness of our proposed summarization approach our best method qr np greedy is based on the noun phrases query reformulation using the greedy strategy of tence selection it achieves score of which improves over the best baseline by in general we can see that the greedy sentence selection strategy works better than the iterative approach this is because the greedy strategy takes into account both the informativeness and the redundancy of the selected sentences table shows the results of summarization ing on the tac dataset the reported approaches all use the greedy sentence selection strategy as it sistently outperforms the iterative approach in eral while all our approaches outperform the baseline query reformulation based approaches achieve the est rouge scores query reformulation method using noun phrases qr np achieves and and scores respectively which is the est scores the interpolated word embedding based model achieves the highest score comparing tables and we notice that the scores for the tac dataset are lower than that of cl scisum this is due to the length of the generated summaries as shown in table the average human summary length in the tac data is almost words more than the cl scisum summaries an interesting observation in these two tables is regarding the relative poor performance of the citation based summarization baseline clexrank that only uses citation texts in comparison with our methods that also take advantage of the citation context and the discourse structure of the articles this observation further conrms our tial hypothesis that relying only on the citation texts could result in summaries that do not accurately ect the content of the original paper and that adding citation contexts can help produce better summaries to better analyze the eect of identifying discourse facets on the overall quality of the summary we pare the rouge scores of the summary generated by our approach with and without this step table shows the overall summarization results based on our qr np approach when we only use contextualized tations compared with when we use faceted tualized citations we observe that grouping citation contexts by their corresponding discourse facet has a positive eect on the quality of the summary on both datasets and improvements over tac and cl scisum datasets in terms of tively this is because identifying facets and grouping the contextualized citations by facets results in a mary that captures the content from all sections of the paper we observe similar trends for other variants of our approaches for brevity we only show the results for qr np as an illustrative analysis on the eect of tifying discourse facets on the quality of the generated summary finally an example of the generated summaries by our system qr np greedy that uses citation contexts and discourse facets is illustrated in figure we serve that compared with the human summary the summary generated by our system can capture the nicant points of the paper arman cohan nazli goharian rouge lexrank clexrank sumbasic summa lmkl lmeq cist qr kw iter qr kw greedy qr np iter qr np greedy wewiki iter wewiki greedy supervised iter supervised greedy lexrank clexrank sumbasic qr np qr domain qr kw wewiki webio it rouge table summarization results on the cl scisum dataset metrics are rouge f scores the top part shows the baselines and the state of the art systems bottom systems show our method variants based on dierent contextualization approaches and sentence selection strategy from the discourse facets iter iterative and greedy refer to the sentence selection approach for the nal summary table summarization results on the tac dataset metrics are rouge f scores the top part shows the baselines and the state of the art systems bottom systems show our method variants based on dierent contextualization approaches and the greedy sentence selection strategy r discussion tac qr np no facet tac qr np faceted cl scisum qr np no facet cl scisum qr np faceted table the eect of discourse facets on the tion results on the tac and cl scisum dataset based on qr np approach by greedy sentence selection strategy on the identied facets other approaches show similar positive trends metrics are rouge f scores citations are a signicant part of scientic papers and analysis of citation texts can provide valuable mation for various scholary applications our work provides new approaches for contextualizing citations which is a sub task for enriching citation texts and thus can benet various bibliometric enhanced nlp cations such as information extraction information trieval article recommendation and article tion our work provides a comprehensive new work for summarizing scientic papers that helps erating better scientic summaries we note that our evaluation was based on the rouge automatic summarization evaluation scientic document summarization via citation contextualization and scientic discourse example summary human summary our system clexrank the limited coverage of lexical semantic resources is a signicant problem for nlp systems which can be alleviated by automatically classifying the unknown words supersense tagging assigns unknown nouns one of broad semantic gories used by lexicographers to organise their manual insertion into wordnet lexical semantic resources have been applied successful to a wide range of natural language processing nlp problems ranging from collocation extraction and class based smoothing to text classication and question answering some specialist topics are better covered in wordnet than others a considerable amount of research addresses structurally and statistically manipulating the hierarchy of wordnet and the construction of new wordnet using the concept structure from english ciaramita and johnson implement a supersense tagger based on the multi class preceptor classier which uses the standard collocation spelling and syntactic features common in wsd and named entity recognition systems the authors demonstrate the use of a very ecient shallow nlp pipeline to process a massive corpus such a corpus is needed to acquire reliable contextual information for the often very rare nouns they are attempting to supersense tag the limited coverage of lexical semantic resources is a signicant problem for nlp systems which can be alleviated by automatically classifying the unknown words ciaramita and johnson present a tagger which uses synonym set glosses as annotated training examples our approach uses voting across the known supersenses of automatically extracted synonyms to select a sense for the unknown nouns the extracted synonyms are ltered before contributing to the vote with their our development experiments are performed on the wordnet test set with one nal run on the net test set in particular wordnet fellbaum has signicantly inuenced research in nlp these results also support ciaramita and johnsons view that abstract concepts like communication cognition and state are much harder lexicographers can not possibly keep pace with language evolution sense distinctions are continually made and merged words are coined or become obsolete and technical terms migrate into the vernacular another related task is supersense tagging ciaramita and johnson curran ciaramita and altun supersense tagging ciaramita and johnson curran evaluates a model s ability to cluster words by their semantics in contrast some research have been focused on using predened sets of sense groupings for learning based classiers for wsd although we could adapt our method for use with an automatically induced inventory our method which uses wordnet might also be combined with one that can automatically nd new senses from text and then relate these to wordnet synsets as ciaramita and johnson and curran do with unknown nouns an additional potential is to integrate automatically acquired relationships with the information found in wordnet which seems to suer from several serious limitations curran and typically overlaps to a rather limited extent with the output of automatic acquisition methods previous work on prediction at the supersense level ciaramita and johnson curran has focused on lexical acquisition nouns exclusively thus aiming at word type classication rather than tagging fig example summary generated by our system qr np greedy on one of the papers from the cl scisum dataset compared with a human written summary and the output generated by clexrank work automatic evaluation metrics have their own itations and can not fully characterize the eectiveness of the systems manual or semi manual evaluation of summarization e through pyramid framework are alternative evaluation approaches that can provide ditional insights into the performance of the systems yet due to expense and reproduction issues most of the standard evaluation benchmarks including tac and cl scisum have been evaluated through rouge as it is standard in the eld and to be able to compare our results with the related work we used the rouge framework for evaluation we also note that our focus has been on the content quality of the summaries and other criteria such as coherence and linguistic cohesion have not been the focus of our approach future work can investigate approaches for improving coherence and linguistic properties of the generated summaries word embeddings and domain knowledge in our ods to capture the terminology variations between the citing and cited authors we furthermore took tage of the scientic discourse structure of the articles we demonstrated the eectiveness of our approach on two scientic summarization benchmarks each in a ferent domain we improved over the state of the art by large margins in most of the tasks while the results are encouraging the absolute values of some metrics pecially in the contextualization task suggest that this problem is worth further exploration contextualizing citations is a new task and not only it helps improving scientic summarization but also it can benet other bibliometric enhanced end to end applications such as keyword extraction information retrieval and article recommendation references conclusions we presented a unied framework for scientic marization our framework consists of three main parts nding the context for the citations in the reference per identifying the discourse facet of each citation text and generating the summary from the faceted tion contexts we utilized query reformulation methods abu jbara a ezra j radev d r purpose and in larity of citation towards nlp based bibliometrics naacl hlt pp abu jbara a radev d coherent citation based marization of scientic papers in proceedings of the annual meeting of the association for putational linguistics human language volume pp association for computational linguistics arman cohan nazli goharian abu jbara a radev d reference scope identication in naacl hlt pp acl in citing sentences atanassova i bertin m v bawden d on the composition of scientic abstracts journal of mentation bendersky m croft w b discovering key concepts in verbose queries in proceedings of the annual ternational acm sigir conference on research and velopment in information retrieval pp acm bengio y courville a vincent p representation learning a review and new perspectives ieee tions on pattern analysis and machine intelligence bengio y ducharme r vincent p janvin c a ral probabilistic language model the journal of machine learning research berg kirkpatrick t gillick d klein d jointly ing to extract and compress in proceedings of the annual meeting of the association for computational linguistics human language technologies volume pp association for computational linguistics bertin m atanassova i gingras y v the invariant distribution of references in scientic ticles journal of the association for information science and technology doi asi url wiley asi bodenreider o the unied medical language system umls integrating biomedical terminology nucleic acids research bornmann l mutz r growth rates of modern science a bibliometric analysis based on the number of tions and cited references journal of the association for information science and technology cao g nie j y gao j robertson s selecting good expansion terms for pseudo relevance feedback in ceedings of the annual international acm sigir conference on research and development in information retrieval pp acm cao z li w wu d polyu at cl scisumm in birndl joint workshop on bibliometric enhanced information retrieval and nlp for digital libraries carbonell j goldstein j the use of mmr based reranking for reordering documents and producing summaries in sigir pp acm celikyilmaz a hakkani tur d a hybrid cal model for multi document summarization in acl pp association for computational linguistics chakraborty t krishna a singh m ganguly n goyal p mukherjee a ferosa a faceted dation system for scientic articles in pacic asia ference on knowledge discovery and data mining pp springer chakraborty t narayanam r all ngers are not in equal intensity of references in scientic articles proceedings of the conference on empirical ods in natural language processing pp sociation for computational linguistics austin texas url org anthology chali y hasan s a query focused multi document summarization automatic data annotations and vised learning approaches nat lang eng doi url chopra s auli m rush a m abstractive tence summarization with attentive recurrent neural in proceedings of the conference of the works north american chapter of the association for tational linguistics human language technologies pp association for computational linguistics san diego california url aclweb anthology clarke j lapata m global inference for sentence compression an integer linear programming approach j artif int res url http acm org citation cohan a goharian n scientic article tion using citation context and article s discourse ture in proceedings of the conference on empirical methods in natural language processing pp association for computational linguistics lisbon tugal url org anthology cohan a goharian n contextualizing citations for scientic summarization using word embeddings and main knowledge in proceedings of the tional acm sigir conference on research and opment in information retrieval sigir doi url acm cohan a soldaini l goharian n matching citation text and cited spans in biomedical literature a oriented approach in proceedings of the hlt pp association for computational guistics url org conroy j m davis s t vector space and language models for scientic document summarization in ceedings of naacl hlt pp conroy j m schlesinger j d kubina j rankel p a oleary d p classy at tac guided and lingual summaries and evaluation metrics in ings of the text analysis conference de waard a maat h p epistemic modality and knowledge attribution in scientic discourse a omy of types and overview of features in proceedings of the workshop on detecting structure in scholarly course pp association for computational guistics durrett g berg kirkpatrick t klein d based single document summarization with compression in proceedings of the and anaphoricity constraints annual meeting of the association for tional linguistics volume long papers association for computational linguistics berlin germany elkiss a shen s fader a erkan g states d radev d blind men and elephants what do citation summaries tell us about a research article journal of the american society for information science and nology erkan g radev d r lexrank graph based lexical centrality as salience in text summarization j artif intell res jair erkan g radev d r lexrank graph based lexical centrality as salience in text summarization journal of articial intelligence research faruqui m dodge j jauhar k s dyer c hovy e smith a n retrotting word vectors to in naacl hlt pp tic lexicons scientic document summarization via citation contextualization and scientic discourse ation for computational linguistics url http org anthology garzone m mercer r e towards an automated tion classier in conference of the canadian society for computational studies of intelligence pp springer gong y liu x generic text summarization using evance measure and latent semantic analysis in ceedings of the annual international acm sigir conference on research and development in information retrieval pp acm guo s sanner s probabilistic latent maximal marginal relevance in sigir pp acm harris z s distributional structure word hernandez alvarez m gomez j m survey about tion context analysis tasks techniques and resources natural language engineering hersh w voorhees e trec genomics special issue overview information retrieval doi hill f reichart r korhonen a uating semantic models with genuine similarity tion comput linguist doi url hulth a improved automatic keyword extraction given more linguistic knowledge in proceedings of the conference on empirical methods in natural language processing pp association for computational linguistics huston s croft w b evaluating verbose query cessing techniques in proceedings of the tional acm sigir conference on research and ment in information retrieval pp acm jaidka k chandrasekaran m k rustagi s kan m y overview of the computational linguistics entic document summarization shared task cl scisumm in proceedings of the joint workshop on bibliometric enhanced information retrieval and ral language processing for digital libraries birndl jha r coke r radev d surveyor a system for generating coherent survey articles for scientic topics ann arbor jian f huang j x zhao j he t hu p a simple enhancement for ad hoc information retrieval via topic modelling in sigir pp acm jones k s walker s robertson s e a tic model of information retrieval development and parative experiments part information processing management jurgens d kumar s hoover r mcfarland d rafsky d citation classication for behavioral analysis of a scientic eld corr kataria s mitra p bhatia s utilizing context in generative bayesian models for linked corpus in aaai vol p klamp s rexha a kern r identifying referenced text in scientic publications by summarisation and sication techniques in jcdl pp le q mikolov t distributed representations of tences and documents in icml pp li l mao l zhang y chi j huang t cong x peng h cist system for cl scisumm shared task in birndl joint workshop on enhanced information retrieval and nlp for digital braries lin c y rouge a package for automatic evaluation in text summarization branches out of summaries proceedings of the workshop pp lin j madnani n dorr b j putting the user in the loop interactive maximal marginal relevance for focused summarization in naacl hlt pp association for computational linguistics lipscomb c e medical subject headings mesh letin of the medical library association mihalcea r tarau p textrank bringing order into texts association for computational linguistics mikolov t sutskever i chen k corrado g s dean j distributed representations of words and phrases and their compositionality in nips pp miller g a wordnet a lexical database for english communications of the acm moraes l baki s verma r lee d university of houston at cl scisumm svms with tree kernels and sentence similarity in jcdl pp mrksic n seaghdha d o thomson b gasic m rojas barahona l su p h vandyke d wen t h young s word vectors to linguistic straints in naacl hlt nakov p i schwartz a s hearst m citances tion sentences for semantic analysis of bioscience text in proceedings of the workshop on search and discovery in bioinformatics pp nomoto t neal a neurally enhanced approach to ing citation and reference in birndl joint shop on bibliometric enhanced information retrieval and nlp for digital libraries osborne m using maximum entropy for sentence traction in proceedings of the workshop on automatic summarization volume pp tion for computational linguistics page l brin s motwani r winograd t the ank citation ranking bringing order to the web paul m zhai c girju r summarizing contrastive viewpoints in opinionated text in emnlp pp association for computational linguistics url org anthology pennington j socher r manning c d glove global vectors for word representation emnlp ponte j m croft w b a language modeling approach to information retrieval in proceedings of the nual international acm sigir conference on research and development in information retrieval pp acm qazvinian v radev d mohammad s generating extractive summaries of scientic paradigms j artif intell res qazvinian v radev d r scientic paper tion using citation summary networks in proceedings of the international conference on computational linguistics volume pp association for putational linguistics qazvinian v radev d r identifying non explicit ing sentences for citation based summarization in ceedings of the annual meeting of the association for computational linguistics pp association for computational linguistics arman cohan nazli goharian qazvinian v radev d r mohammad s m dorr b zajic d whidby m moon t generating tractive summaries of scientic paradigms j artif int res url acm citation robertson s zaragoza h the probabilistic relevance framework and beyond now publishers inc rush a m chopra s weston j a neural attention model for abstractive sentence summarization in ceedings of the conference on empirical methods in natural language processing pp association for computational linguistics lisbon portugal url org anthology saggion h aburaed a ronzano f trainable citation enhanced summarization of scientic articles in cabanac g chandrasekaran mk frommholz i jaidka k kan m mayr p wolfram d editors proceedings of the joint workshop on bibliometric enhanced tion retrieval and natural language processing for ital libraries birndl june newark united states ceur workshop p ceur workshop proceedings snomed c systematized nomenclature of international health terminology clinical terms dards development organisation sparck jones k a statistical interpretation of term specicity and its application in retrieval journal of umentation steinberger j jezek k using latent semantic analysis in text summarization and summary evaluation in proc pp sutskever i vinyals o le q v sequence to sequence in advances in neural learning with neural networks information processing systems pp teufel s moens m summarizing scientic cles experiments with relevance and rhetorical status comput linguist doi url teufel s siddharthan a tidhar d automatic sication of citation function emnlp p vanderwende l suzuki h brockett c nenkova a beyond sumbasic task focused summarization with tence simplication and lexical expansion information processing management wang s manning c d baselines and bigrams ple good sentiment and topic classication in ings of the annual meeting of the association for computational linguistics short papers volume pp association for computational linguistics zhai c laerty j a study of smoothing methods for language models applied to information retrieval acm transactions on information systems tois
