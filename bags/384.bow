contextualized rewriting text summarization guangsheng yue school engineering westlake university institute advanced technology westlake institute advanced study baoguangsheng edu abstract extractive summarization suffers irrelevance dancy incoherence existing work shows abstractive rewriting extractive summaries improve ness readability rewriting systems consider tracted summaries input relatively cused lose important background knowledge paper investigate contextualized rewriting ingests entire original document formalize contextualized rewriting problem group alignments troducing group tag solution model alignments identifying extracted summaries content based dressing results approach signicantly forms non contextualized rewriting systems ing reinforcement learning achieving strong improvements rouge scores multiple extractive summarizers introduction extractive text summarization systems nallapati zhai zhou narayan cohen lapata liu pata work identifying salient text segments ically sentences input document summary shown outperform abstractive systems rush chopra weston nallapati chopra auli rush terms content selection faithfulness input extractive rizers exhibit limitations sentences extracted input document tend contain irrelevant dundant phrases durrett berg kirkpatrick klein chen bansal gehrmann deng rush second extracted sentences weak ence regard discourse relations cross sentence anaphora dorr zajic schwartz cheng ata address issues line work investigates editing extractive summarizer outputs grammar tree trimming considered reducing irrelevant content sentences dorr zajic schwartz rule based methods investigated ing redundancy enhancing coherence durrett kirkpatrick klein rise neural corresponding author copyright association advancement articial intelligence www aaai org rights reserved source document thousands live earthworms falling sky norway biology teacher discovered worms surface snow skiing mountains near bergen weekend teacher erstad told norwegian news website local gold summary live worms snow extractive summary biology teacher discovered worms surface snow skiing mountains near bergen weekend rewritten summary biology teacher erstad ered worms snow teacher karstein erstad found thousands figure example showing contextual information benet summary rewriting works recent line work considers tive models rewriting extracted outputs sentence tence chen bansal bae wei huang gao xiao human evaluation shows rewriting systems effectively improve ness readability interestingly existing rewriters improve rouge scores compared extractive baselines existing abstractive rewriting systems extracted maries input hand information original document serve useful background knowledge inferring factual details figure example salient summary extracting sentence biology teacher weekend rewriter simplify sentence making better summary provide additional details sentence document context considered example teacher given extractive summary infer teacher erstad context sentences making summary informative propose contextualized rewriting input document context rewriting extractive mary sentences encoding extractive summary use neural representation model encode input document representing extractive summary document representation inform rewriter current sentence rewritten use source document resident coach technical expert chris meadows plenty experience sport worked biggest names golf chris worked golfers career growing nick faldo meadows learned success golf comes develping clear understanding committed objective dedicated coach early age soon realized gift development meadows simple holistic approach learning personally shared golfers career spanning decades instructional books best sellers career recently recognized professional golfers association advanced fellow pga chris living golf resident golf expert rewritten summary chris meadows worked golf big names personally coached golfers chris advanced fellow pga figure example step summarization process selecting grouping rewriting content based addressing graves wayne danihelka specically figure shows unique group tag index extracted sentence source ment matching increasing sentence index tive rewriter rewriter generates output group tags guide rst second rewritten summary sentences respectively choose bert devlin base model document encoder building extractive marizer abstractive rewriter following basic models liu lapata models ated cnn dataset hermann sults contextualized rewriter gives cantly improved rouge lin scores compared state art extractive baseline outperforming tional rewriter baseline large margin addition method gives better compression lower redundancy ter coherence contextualized rewriter achieves strong consistent improvements multiple extractive rizers knowledge rst report improved rouge rewriting extractive summaries release code com baoguangsheng ctx rewriter summ git related work extractive summarizers received constant research tention early approaches textrank mihalcea tarau select sentences based weighted similarities recently nallapati zhai zhou use neural sier choose sentences selector rank chen bansal use pointer network vinyals nato jaitly extract sentences liu lapata use linear classier bert method gives current state art result extractive tion choose baseline rewriting systems manipulate extractive summaries reducing irrelevance redundancy incoherence durrett berg kirkpatrick klein use compression rules reduce unimportant content sentence anaphoricity constraints improve cross sentence ence dorr zajic schwartz trim unnecessary phrases sentence hurting grammar correctness nding syntactic structures sentences contrast work consider neural abstractive rewriting solve issues systematically recently neural rewriting attracted research attention chen bansal use model copy mechanism liu manning rewrite extractive summaries sentence sentence reranking post process applied avoid repetition extractive model tuned reinforcement learning reward signals rewritten sentence bae use similar strategy bert document encoder reward signals summary wei huang gao use binary classier bert document encoder select sentences transformer decoder vaswani copy mechanism generate summary sentence xiao build hierarchical representation input document pointer network copy rewrite mechanism designed choose sentences copying rewriting followed vanilla model rewriter model decisions sentence selecting copying rewriting tuned reinforcement learning compared methods method computationally simpler thanks freedom reinforcement learning copy mechanism methods addition mentioned earlier contrast methods consider rewriting including document level context tentially improve details factual faithfulness hybrid extractive abstractive summarization models line work cheng lapata use hierarchical encoder extracting words straining conditioned language model generating ent summaries gehrmann deng rush consider method neural classier select portant words input document informing abstractive summarizer restricting copy source pointer generator network selected content similar work use extracted content guiding tive summary different work cuses word level investigate sentence level straints guiding abstractive rewriting method regarded group tags guide reading context abstractive summarization rush chopra weston nallapati liu manning group tags obtained extractive summary compared vanilla stractive summarization advantages fold extractive summaries guide abstractive summarizer salient information second training difculty abstractive model reduced important tents marked inputs summarization cedure interpretable associating crucial source sentence target sentence figure architecture contextualized rewriter group tag embeddings tied encoder left gure decoder right gure decoder address corresponding tokens document group alignments key contribution method model alized rewriting mapping problem group alignments input sequence output sequence group set describes set segment wise alignments mapping problem dened ing estimation argy max denotes number elements ber elements number groups group denotes pair text segments belongs group taking ure example rst extractive sentence document rst sentence summary form group problem simplied given fact group text segment known responding segment dynamically decided generation separate nents redene mapping problem argy max table randomly initialized jointly trained encoder decoder vector representations enrich vector representations respectively result tokens tagged vector component content based addressing tention mechanism garg group tag serves mechanism constrain attention corresponding decoding unlike proaches modify model rules hsu gehrmann deng rush group tag ables modication exible trainable contextualized rewriting system step process generating summary extractive summarization model select set sentences original document guiding source second guiding source text matched nal document set group tags assigned token abstractive rewriter applied tagged document group tags serve guidance summary generation formally use represent document contains tokens sent nal resulting summary contains tokens extractive summarizer group group tag assigned text segment group linked segment group example figure encoder decoder framework convert vector representations shared embedding following liu lapata use bert encode input document special cls token added beginning sentence interval segments ing applied distinguish successive sentence bert representation cls tokens extractor stacked select sentences extractor uses transformer vaswani encoder generate inter sentence resentations extracting summary layer sigmoid activation calculate probability sentence extracted encoder use bert encoder bertenc vert source document sequence token dings taking cls embeddings representation source sentences denoted extractor use transformer encoder transenc convert sentence embeddings nal inter sentence representations calculate extraction probability sentence according given sequence extraction denotes extk means sentence extracted model trainable parameters probabilities number sentences decision sentence cording hyper parameters minimum number sentences extract min sel maximum number sentences extract max sel probability threshold particular sort sentences descending order based sentences rank min sel selected default sentences rank min sel max sel decided paring probability value threshold sentences probability threshold selected decide hyper parameter values dev experiments note method slightly different tive model liu lapata extracts probable sentences summary purpose rewriting strong compression method allows extract sentences summary better recall source group tagging match extracted summary original document group tagging taking sentence extracted mary group rst summary sentence matched sentence forms group second group formally document extractive mary summary sentence matched token assigned group tag particular instantiated sequence group tags document contextualized rewriter contextualized rewriter extends abstractive rizer liu lapata standard sequence sequence model bert coder figure shows integrate group tag guidance group tag embeddings added encoder decoder formally extractive summary set group tags closed set use lookup table represent embeddings group tags shared encoder decoder encoder original document processed way extractive model cls token added sentence interval segments distinguish successive sentences bert encoding bertenc representation token added group tag embedding producing nal representation embwg embwg denotes retrieved embeddings lookup table group tag sequence decoder summary sentences synthesized single sequence special token bos beginning sep sentences eos end decoder lows standard transformer architecture treat sentence summary group sequently group tag sequence fully determined summary particular tokens mary sentence assigned group tag instantiated decoding group tag generated beam search step starting special token bos increasing special token sep embedding group tag retrieved lookup table added token embedding position embedding embwg represents tagged token embeddings encoder outputs decoder transdec dicts token probabilities position based tagged token embeddings position encoder puts consuming memory keys ues multi head attention sequence group tags decoder address group tags encoder rewritten sentence corresponds extracted sentence document training train extractive summarizer abstractive rewriter separately pre processed dataset labeled standard extractions generate gold standard extraction match sentence human summary ment sentence choosing sentence best ing score gold extraction summary sentence specically use average recall scoring function follows wei huang gao differing existing work liu lapata aims set sentences maximizes rouge matching human summary best match summary sentence result ber extracted sentences number tences human summary strategy adopted wei huang gao bae matching summary document obtain gold standard extraction training extractive model convert gold standard extraction label sentence set train model binary entropy loss function lext log denotes number sentences training abstractive rewriter convert standard extractions group tags following train model negative log likelihood loss lwrt log experimental setup evaluate model cnn daily mail dataset mann comprises online news articles human written highlights average article samples total use non anonymized version follow standard splitting hermann includes samples training dev testing testing preprocess dataset following liu manning splitting sentences stanford corenlp manning tokenize sentences subword tokens truncate documents tokens evaluate models automatically rouge lin reporting unigram overlap gram overlap metrics informativeness longest common subsequence rouge indicator uency scores calculated pyrouge extractive summarizer document encoder initialized pre trained cased bert base transformer layers output embedding size transformer extractor set layers embedding size domly initialized use adam optimizer kingma encoder extractor jointly trained total steps learning rate schedule vaswani step warmup model trained gpus hours inference select sentences according parameters min sel max sel threshold chosen grid search best erage score rouge dev dataset org project method rouge liu manning bertsumext liu lapata extractive abstractive bertsumabs liu lapata bertsumextabs liu lapata rnn chen bansal bert hybrid wei huang gao bert bae xiao models bert ext oracle bert abs bert table results best scores bold cantly better scores marked test ext abs denotes extractive abstractive models spectively means reinforcement learning contextualized rewriter initialize document encoder pre trained uncased bert base model initialize decoder randomly transformer decoder layers embedding size tied input output embeddings press wolf use adam optimizer default setting model trained total steps steps warming coder steps warming decoder lrenc step enc lrdec step dec use learning rate encoder decode applying dropout probability label smoothing szegedy factor word dropout bowman probability decoder train model gpus machine hours inference constrain decoding sequence minimum length maximum length length penalty beam size beam search block paths repeated trigram generated trigram blocking paulus xiong socher results analysis compare models existing summarization els analysing contextualized rewriter automatic evaluation results shown table section consists extractive models middle section contains abstractive models hybrid systems rewriter tion lists models comparison bertsumext extractive model bert ext gives lower result ferences extraction goal discussed earlier method rnn bertsumext bertsumextabs bert faith read info conc table human evaluation faithfulness faith informativeness info conciseness ability read conc compared extractive baseline bert ext model bert improves respectively shows effectiveness contextualized rewriting isolate effect rewriter extractive rizer experiment oracle contextualized tractive summary input rewriter shows gap tween bert result result shows room improvement extractive summarizer stronger row bert abs shows result bert based abstractive summarizer copies structure settings bert ext contextrewriter excluding ponents related group tags figure contrast tween bert model abs model shows usefulness extractive summary guiding abstractive rewriting compared rewriting system bert hybrid increases bert ext contextrewriter respectively demonstrates fectiveness contextualized rewriting compared contextualized rewriting help inforcement learning better result achieved non contextualized rewriting system results bert algorithm inevitably shows increased compared best rewriting system contextualized rewriter bert ext contextrewriter shows signicant provement rouge tively despite model purely generative copying tokens source document complexity compared strong extractive model sumext bert ext contextrewriter gives better score rouge metrics signicant margin respectively ering different length extractive summaries summaries normalize rouge scores following sun relative improvement model normalization larger improves sumext relatively ized score compared improvement relatively knowledge rst report improved rouge scores compared state art extractive baseline abstractive rewriting human evaluation given section include bart lewis table reports method oracle contextrewriter contextrewriter bertsumext tri bloc contextrewriter bert ext contextrewriter rouge words table results extractive summarizers applied contextualized rewriter tri bloc means trigram blocking spectively different pre training method data bart compared models table use bert base bart summarization uses large model second models table use rst kens document bart uses tokens human evaluation intuitively model paraphrase extractive summaries instead generating summaries scratch proving faithfulness furthermore abstractiveness contextualized rewriter enhance readability strong compression improve conciseness hypothesis conduct human evaluation randomly select samples test set scoring fulness readability informativeness conciseness independent annotators report nal result averaging annotators result shown table compared contextualized rewriter rnn ized rewriter shows obvious advantage pects compared extractive baseline bertsumext rewriter enhances readability informativeness conciseness signicant margin keeping faithfulness enhancement readability mainly tributed reduced redundancy improved coherence improvement conciseness conrms strong pression rewriter comparison abstractive baseline bertsumextabs rewriter improves fulness informativeness keeping readability conciseness close conciseness rewriter lower generates summaries word longer abstractive model average having text rewriter obtains improved mativeness universality rewriter contextualized abstractive rewriter serve eral summary rewriter evaluate rewriter different extractive summarizers including sumext bert ext oracle table shows contextualized rewriter improves summaries generated extractive summarizers particular basic extractive summarizer rouge scores improve large margin best tractive summarizer bertsumext rewriter hances summary quality especially rouge point improvement extractive summaries figure comparison ability generate redundant summaries extractive summary oratilwe hlongwane learning words toddler able select play music laptop phenomenon south africa year old oratilwe hlongwane johannesburg south africa ing words able play music laptop making worldwide phenomenon rewritten summary oratilwe hlongwane learning words able play music laptop making worldwide phenomenon figure example ability reduce redundancy improved points rouge dicates signicant improvement uency table rouge scores bertsumext trigram blocking worse bertsumext cause redundant information applied rewriter similar scores difference point proof rewriter robust input redundant extractive summaries analysis quantitatively evaluate contextualized rewriter ability reduce redundancy compress tences improve abstractiveness enhance coherence redundancy redundancy major problem automatic summarization study impact trigram blocking model performance comparing work liu lapata figure shows trigram blocking post process removed models lower rouge scores bertsumext ences signicant drop bertsumextabs smaller drop redundancy tive summarizer contextrewriter suffers drop halving bertsumextabs shows contextualized rewriter effectively reduces redundancy example shown figure demonstrates ability compression column avg words table shows extractive summarizers tualized rewriter signicantly compress summaries oracle extractive summaries compresses size half models compresses maries original summaries average looking summaries generated extractive mary sentences changed rewriter compressed shorter versions rewritten new sentences obtain numbers test dataset adopting edit sequence generation algorithm zhang method gold bertsumextabs bert grams grams grams table percentage novel grams source document university iowa student died nearly months fall andrew mogni glen ellyn illinois arrived chicago died sunday rewritten summary andrew mogni glen ellyn illinois arrived semester program italy incident happened uary chicago air ambulance march died sunday swap group tags andrew mogni chicago air bulance march died sunday arrived semester program italy incident happened january figure example ability maintain coherence litman generate sequence word editing actions mapping extracted summary sentence rewritten categorize sentence rewritten sequence contains action adding modifying pressed sequence contains action deleting unchanged according samples test dataset compressions phrases instead single words thermore removed phrases unimportant given fact removed words included erence summaries instance returned reaves girl named lying compressed returned reaves girl lying novel grams measure abstractiveness culate percentage novel grams table shows results textualized rewriter generates summaries novel grams compared bertsumextabs suggests better abstractiveness coherence text generation process alized rewriter controlled extractive input observe behavior rewriter figure uses output example demonstrate rewriter maintains coherence student mentioned rst summary sentence pronoun second sentence swap group tags section shows swap group tags source document content summary sentences swap positions student presented rst sentence pronoun second sentence case cross sentence anaphora maintained correctly conclusion investigate contextualized rewriting extractive maries neural abstractive rewriter formalizing task problem group alignments group tags represent alignments constraining tention rewriting sentence content based ing results standard benchmarks textual information original document highly benecial summary rewriting model outperforms existing abstractive rewriters signicant margin ing strong rouge improvements multiple extractive summarizers rst time method group alignments general potentially applied nlg tasks acknowledgments like thank anonymous reviewers valuable feedback thank wenyu inspiring discussion references bae kim kim lee summary level training sentence rewriting abstractive proceedings workshop new marization frontiers summarization hong kong china sociation computational linguistics bowman vilnis vinyals dai jozefowicz bengio generating sentences tinuous space proceedings signll ence computational natural language learning berlin germany association computational tics chen bansal fast abstractive marization reinforce selected sentence rewriting proceedings annual meeting tion computational linguistics volume long papers melbourne australia association tional linguistics cheng lapata neural summarization extracting sentences words proceedings annual meeting association computational linguistics volume long papers berlin association computational linguistics chopra auli rush abstractive sentence summarization attentive recurrent neural proceedings conference networks north american chapter association tional linguistics human language technologies san diego california association computational guistics devlin chang lee toutanova bert pre training deep bidirectional transformers language understanding proceedings ference north american chapter association computational linguistics human language gies volume long short papers neapolis minnesota association computational guistics dorr zajic schwartz hedge mer parse trim approach headline generation proceedings hlt naacl text summarization workshop durrett berg kirkpatrick klein learning based single document summarization compression anaphoricity constraints proceedings annual meeting association tational linguistics volume long papers berlin germany association computational tics garg peitz nallasamy paulik jointly learning align translate transformer models proceedings conference ical methods natural language processing international joint conference natural language cessing emnlp ijcnlp hong kong china association computational linguistics gehrmann deng rush proceedings abstractive summarization conference empirical methods natural language processing brussels belgium association computational linguistics graves wayne danihelka neural ing machines corr hermann kocisky grefenstette espeholt kay suleyman blunsom cortes ing machines read comprehend lawrence lee sugiyama garnett eds advances neural information processing systems annual conference neural information processing systems december montreal quebec canada hsu lin lee min tang sun unied model extractive tive summarization inconsistency loss ings annual meeting association putational linguistics volume long papers melbourne australia association computational guistics kingma adam method stochastic optimization bengio lecun eds international conference learning representations iclr san diego usa ence track proceedings lewis liu goyal ghazvininejad hamed levy stoyanov zettlemoyer bart denoising sequence sequence pre training natural language generation translation hension proceedings annual meeting association computational linguistics line association computational linguistics lin rouge package automatic ation summaries text summarization branches barcelona spain association computational linguistics liu lapata text summarization proceedings pretrained encoders ence empirical methods natural language ing international joint conference natural literature proceedings workshop methods optimizing evaluating neural language generation minneapolis minnesota association tional linguistics szegedy vanhoucke ioffe shlens wojna rethinking inception architecture puter vision ieee conference computer vision pattern recognition cvpr vaswani shazeer parmar uszkoreit jones gomez kaiser polosukhin guyon luxburg attention need bengio wallach fergus vishwanathan garnett eds advances neural information ing systems curran associates inc vinyals fortunato jaitly pointer networks cortes lawrence lee sugiyama garnett eds advances neural information processing systems annual conference neural information processing systems december montreal quebec canada wei huang gao sharing pre trained bert decoder hybrid summarization sun huang liu liu eds chinese tational linguistics china national conference ccl kunming china october proceedings volume lecture notes computer science springer schuster chen norouzi macherey krikun cao gao macherey klingner shah johnson liu kaiser gouws kato kudo kazawa stevens kurian patil wang young smith riesa rudnick vinyals corrado hughes dean google neural machine translation tem bridging gap human machine lation org xiao wang jin copy rewrite hybrid summarization hierarchical forcement learning proceedings aaai conference articial intelligence aaai zhang litman sentence level rewriting detection proceedings ninth workshop vative use nlp building educational applications baltimore maryland association tional linguistics language processing emnlp ijcnlp hong kong china association computational linguistics manning surdeanu bauer finkel bethard mcclosky stanford corenlp natural language processing toolkit proceedings nual meeting association computational tics system demonstrations baltimore maryland association computational linguistics mihalcea tarau textrank bringing order text proceedings conference pirical methods natural language processing barcelona spain association computational tics nallapati zhai zhou summarunner recurrent neural network based sequence model tractive summarization documents singh markovitch eds proceedings thirty aaai conference articial intelligence february san francisco california usa aaai press nallapati zhou dos santos xiang abstractive text summarization sequence sequence rnns proceedings signll conference computational ral language learning berlin germany ation computational linguistics narayan cohen lapata ing sentences extractive summarization ment learning proceedings conference north american chapter association putational linguistics human language technologies ume long papers new orleans louisiana association computational linguistics paulus xiong socher deep reinforced model abstractive summarization corr press wolf output embedding improve language models proceedings conference european chapter association computational linguistics volume short papers valencia spain association computational guistics rush chopra weston neural tention model abstractive sentence summarization proceedings conference empirical methods natural language processing lisbon gal association computational linguistics liu manning point summarization pointer generator networks proceedings annual meeting tion computational linguistics volume long papers vancouver canada association tional linguistics sun shapira dagan nenkova compare summarizers target length pitfalls solutions examination neural summarization
