relabeling summarizing posterior distributions signal decomposition problems number components unknown alireza roodaki julien bect gilles fleury n j o c t t s v v x r abstract paper addresses problems relabeling summarizing posterior distributions typically arise bayesian framework dealing signal tion problems unknown number components posterior distributions dened union subspaces differing dimensionality sampled modern monte carlo techniques instance increasingly popular rj mcmc method generic approach available summarize resulting variable dimensional samples extract component specic parameters propose novel approach named variable dimensional approximate posterior relabeling summarizing vapors problem consists approximating rior distribution interest simplebut dimensional parametric distribution distance distributions measured kullback leibler vergence stochastic em type algorithm driven rj mcmc sampler proposed estimate parameters signal decomposition problems considered capability vapors relabeling summarizing variable dimensional posterior distributions classical lem detecting estimating sinusoids white gaussian noise hand particle counting problem motivated pierre auger project astrophysics hand index terms bayesian inference signal decomposition trans dimensional mcmc label switching stochastic em introduction nowadays owing advent markov chain monte carlo mcmc sampling methods bayesian data ysis considered conventional approach machine ing signal image processing data mining problems applications tical challenges remain process extracting generated samples quantities interest summarize posterior distribution summarization consists loosely speaking providing simple interpretable parameters graphics end user statistical method instance case scalar parameter unimodal posterior distribution measures location dispersion e empirical mean standard deviation median tile range typically provided addition graphical alireza roodaki ltci cnrs tlcom paristech paris france email al com julien bect gilles fleury supelec systems sciences department signal processing electronic systems supelec gif yvette france email rstname fr results presented ph d thesis rst author summary distribution e histogram kernel density estimate case multimodal distributions summarization difcult carried instance approximation posterior gaussian mixture model gmm summarizing approximating posterior distributions designing proposal distributions metropolis hastings mh samplers adaptive mcmc framework e paper addresses problem summarizing posterior distributions case trans dimensional problems e problems number things nt know things nt know specically concentrate problem signal position number components unknown important case trans dimensional problem examples problems include detection estimation sinusoids white gaussian noise related problem estimating directions arrival array processing detection objects images detection physical particles neutrons muons noisy data types sensors instance spectroscopy astrophysics let y vector n observations superscript t stands vector transposition signal decomposition problems model space nite countable set models m mk k k k denotes number components k n set possible values assumed mk components vectors component specic parameters qqq qqq qqq k q feature problems considering common invariance likelihood y k qqq respect permutations relabeling components called label switching issue literature e discuss issue section k q rd q bayesian framework joint posterior density k qqq p k qqq y obtained bayes formula number components vector specic parameters assigning prior distributions k qqq p y k qqq k p qqq p k variable dimensional space q differing dimensionality e q indicates proportionality density dened union subspaces q q posterior density completely describes tion associated uncertainty provided data q q q psfrag replacements k candidate models vector unknown rameters known normalizing constant cases monte carlo simulation methods reversible jump mcmc rj mcmc sampler widely approximate label switching issue challenging issues attempting summarizing posterior distributions occurs dimensional situations label switching phenomenon e caused invariance likelihood prior distribution permutations components consequence component specic marginal posterior distributions equal useless purpose summarizing information tained posterior distribution individual nents simplest way dealing label switching issue introduce identiability constraint ic sorting components respect parameters discussion concerning use ics problem bayesian analysis gmm practical examples choosing appropriate ic manually feasible relabeling algorithms developed undo label switching effect automatically restricted case xed dimensional posterior distributions recent advances references variable dimensional posterior distributions extra uncertainty presence components dition location challenging problem hindered previous attempts undo label switching dimensional scenario according meaning individual components vacuous argument claried following illustrative example b illustrative example joint bayesian detection tion sinusoids white gaussian noise example assumed mk observed signal y composed k sinusoidal components observed white gaussian noise mk ac ji j ji k ac j cosine sine amplitudes w j radial frequency jth sinusoidal component n white gaussian noise variance s j ac w unknown parameters number k sinusoidal components vectors qqq j specic parameters j k noise variance s q p q use hierarchical model prior distributions rj mcmc sampler proposed problem interested reader referred q q k fact birth death moves acceptance ratio provided seminal paper erroneous chapter justication true expression acceptance ratio paper y w figure posterior distributions k left sorted radial cies www given k right output rj mcmc samples true number components vertical dashed lines right gure locate true radial frequencies figure represents posterior distributions number components sorted radial cies www w w given k obtained ples generated rj mcmc sampler note sorting mitigate effect label switching ization row dedicated value k k observe models negligible posterior ties k y experiment served signal length n consists sinusoids energies j c j s j phases fff k p p j j ac j true radial frequencies www snr t kd d n design matrix sines cosines associated www set moderate value db ac ns roughly speaking approaches co exist literature summarizing variable dimensional posterior distributions bayesian model selection bms bayesian model aging bma bms approach ranks models according posterior probabilities y selects model denoted k map map stands maximum riori summarizes posterior distribution component specic parameters xed dimensional selected model price losing valuable tion provided discarded models instance example figure information small harder detect middle component lost selecting posteriori probable model hand bma approach consists reporting results averaged possible models bma approach suitable signal reconstruction prediction purposes e references appropriate studying component specic parameters q number changes information concerning approaches found references best knowledge generic method currently available allow summarize information easily read figure simple example sinusoidal components observed noisy signal middle having smaller probability presence c outline paper paper propose novel approach named dimensional approximate posterior relabeling marizing vapors relabeling summarizing posterior distributions dened variable dimensional subspaces typically arise signal decomposition problems number components unknown consists mating true posterior distribution parametric model varying dimensionality minimization leibler kl divergence distributions stochastic expectation maximization algorithm driven output rj mcmc sampler estimate parameters approximate model vapors shares similarities relabeling rithms proposed solve label switching problem em type algorithm context adaptive mcmc algorithms dimensional setting main contribution paper introduction original variable dimensional parametric model allows tackle directly difcult problem approximating distribution dened union subspaces differing dimensionality provides rst solution trans dimensional label switching problem speak algorithm propose seen realization idea m stephens mind stated page raises question able obtain alternative view variable dimensional rior combining results different k grouping components similar similar predictive density estimates attempts failed produce easily interpretable results paper organized follows section ii introduces proposed model stochastic algorithm relabeling summarizing variable dimensional posterior distributions section iii illustrates performance vapors signal decomposition examples problem joint bayesian detection estimation sinusoids white sian noise problem joint bayesian detection estimation particles auger project astrophysics section iv conrms performances vapors monte carlo experiment finally section v concludes paper gives directions future work ii vapors assume target posterior distribution dened variable dimensional space q k admits probability density function pdf respect dimensional lebesgue measure q kk q k k k q s objective approximate true posterior density simple parametric model qhhh hhh vector parameters dening model pdf qhhh dened variable dimensional space q e xed dimensional approximation bms approach assume monte carlo sampling method e rj mcmc sampler available generate m ples f denote qqq m qqq variable dimensional parametric model instead trying describe proposed parametric family densities qh directly let adopt generative point view e valued random variable qqq k qqq corresponding probability distribution assume positive integer l given represents number components present posterior let describe sample q generate independently l ponents binary indicator variable drawn bernoulli distribution l l indicates corresponding component present absent qqq actual number components l parameter erated samples dened k l p l called probability presence lth component second given vector indicator variables xxx l q random vector generated component present e l l random vector generated according probability distribution associated component assumed q dimensional gaussian distribution mean mmm l covariance matrix s order achieve required invariance respect component relabeling generated vectors arranged vector qqq qqq qqq k l contemplating posterior distributions sorted radial frequencies depicted right panel figure particularly plots related models sinusoidal components observed diffuse parts rj mcmc output samples resulting heavy metric tails components clear model gaussian components capable describing diffuse samples parsimonious way abnormal observations respect bulk observed data simply outliers adversely inuence dimensional parametric family distributions point literature gaussian distribution chosen convenient mean describing compact unimodal dimensional distribution intensity plot provided section iii figure precisely permutation k components present example bma summary related component specic parameter drawn uniformly set permutations q q q q q s s l p l mmm l l l l qqq figure proposed variable dimensional parametric model generative viewpoint process tting approximate posterior true posterior distribution interest consequently lead meaningless parameter estimates overcome robustness issue propose include model noise like poisson point process ppp e account presence outliers observed samples assume ppp q intensity l number components generated ppp follows poisson distribution mean l consistent previous notations denote n number note l values extended vector xxx follows probability distribution p xxx ppp l el l l p l l p l given xxx random samples generated uniformly q inserted randomly samples drawn gaussian components denote qhhh pdf random variable qqq k qqq generated l mmm l l hhh hhh hhh l p l figure provides directed acyclic graph model hhh l s b distribution labeled samples random variable qqq k qqq qqq drawn density qhhh thought unlabeled sample l l l component label qqq j j k originates recovered qqq let introduce variable dimensional allocation vector k zk l k kk provides missing piece information l indicates qqq j originates lth gaussian component l l z l indicates qqq j originates point process component refer pair qqq z labeled sample following derive joint distribution qhhh qqq z qhhh qqq distribution allocation vector z qhhh xxx xxx qhhh xxx given note xxx deterministic function j l l l compute rst term remember points generated components parametric model randomly arranged qqq n l k qhhh xxx arrangements differ position points corresponding ppp rise allocation vector conditional distribution qhhh qqq z reads qhhh qqq z qhhh qqq j z j k qhhh qqq j z j mmm s qqq n z l z l equations qhhh qqq z l el jk z j l l p l l p n qqq j mmm s z z z set allocation vectors e set z kk l k l l l c estimating model parameters propose t parametric distribution qhhh posterior interest minimizing divergence measure qhhh use kl divergence divergence f measure paper divergence measures denoting kl divergence dkl qqq kqhhh qqq dene criterion minimized qhhh j hhh dkl qqq k qhhh qqq qqq log zq qqq qqq dqqq samples generated rj mcmc sampler criterion approximated j hhh m m log qhhh qqq c assumed sake simplicity elaborate non homogeneous models easily accommodated approach needed chapter divergence measure proposed problem robustness reasons s s s s s s s s s q q r iteration s step draw allocation vectors m imh step target q hhh r qqq e step construct pseudo completed log likelihood m qhhh qqq log m step estimate hhh hhh argminhhh figure proposed sem type algorithm c constant depend hhh note minimizing j hhh amounts choosing hhh argmaxhhh m log qhhh qqq estimate model parameters hhh n extensively algorithms maximum likelihood ml parameter estimation latent variable models em algorithm proposed turns em algorithm similar works appropriate solving problem computing expectation e step intricate explicitly problem computational burden summation e step set possible allocation vectors increases rapidly l k fact moderate values l k l k summation far expensive compute involves l terms paper propose use sem algorithm variation em algorithm e step substituted stochastic simulation latent variables conditional posterior distributions given vious estimates unknown parameters words iteration r sem algorithm denoting estimated parameters iteration r hhh r m allocation vectors drawn q hhh r qqq step called stochastic random samples construct called pseudo completed log likelihood exact sampling q hhh r qqq required s step sem type algorithm unfortunately feasible accept reject algorithm heavily binatorial expression normalizing constant q hhh r qqq instead q hhh r qqq q hhh r qqq possible assign prior remark tributions unknown parameters hhh study posterior distributions example mcmc sampler latent variable z added state chain spirit data augmentation algorithm leave label switching issue unsolved invariance qhhh permutations components remark convergence results sem algorithm general form provided particular example mixture analysis problems unfortunately assumptions hold problem dealing observed samples qqq correlated owing fact generated true posterior distribution mcmc methods e rj mcmc sampler mh sampler draw conditional posterior distribution empirical evidence good convergence properties type algorithm proposed provided sections d robustied algorithm preliminary experiments sem type algorithm scribed figure satisfactory sample mean estimates m step obtained minimizing kl divergence posterior tion parametric model qhhh suffer sensitivity outliers observed samples including poisson point process component workaround propose use robust estimates means gaussian distributions instead empirical means m step example case univariate gaussian distributions use median interquartile range robust estimators mean variance respectively section discussion robustness issue including alternative solution robust divergence remark similar robustness concerns widespread clustering literature e references iii illustrative examples section investigate capability vapors summarizing variable dimensional posterior distributions signal decomposition examples joint bayesian detection estimation sinusoids white gaussian noise joint bayesian detection estimation astrophysical particles auger project chapters results discussion phasize output trans dimensional monte carlo sampler e g rj mcmc sampler paper considered observed data vapors computed normalizing constant choose use independent metropolis hasting imh step q hhh r qqq stationary distribution rithm details proposed sem type algorithm summarized figure joint bayesian detection estimation sinusoids white gaussian noise let consider problem detection estimation sinusoidal components introduced section b psfrag replacements psfrag replacements m p l jm gaussian comp m s p gaussian comp m s p gaussian comp m s p point process comp l y t s n e t n sem iteration figure evolution model parameters jm dened iterations vapors l rion rj mcmc output samples shown figure unknown parameters number components component specic parameters ac j w j j k noise variance s amplitudes noise variance analytically integrated focus summarizing joint posterior distribution www y form illustrated figure assume proposed parametric model introduced section ii consists univariate gaussian components means m l l probabilities presence p l l l variances estimated space component specic parameters q p r launching vapors need rst initialize parametric model natural deduce number l gaussian components posterior distribution k set percentile y probable models play initialize gaussian nents parameters e m l l l l robust estimates means variances marginal posterior distributions sorted radial frequencies given k l finally set p l l l l ran robustied stochastic algorithm introduced section ii specic example shown figure iterations l gaussian components note posterior probability k approximately assess convergence vapors figure illustrates evolution model parameters hhh criterion j substantial facts showing convergence vapors deduced gure rst decreasing jm constant behavior criterion iteration convergence parameters parametric model particularly means m l bilities presence p l l l naive initialization procedure iteration signicant parameter estimates w w figure histogram labeled samples samples allocated gaussian poisson point process components pdf estimated gaussian components model black solid line vapors sinusoid detection example estimated parameters component presented corresponding panel trans dimensional setting figures shows histograms labeled samples e qqq m pdf estimated gaussian components black solid line summaries provided vapors component presented corresponding panel average sem iterations parameter estimates recommended sem literature example comparing distributions labeled samples ones posterior distributions sorted radial frequencies given shown figure highly multimodal reveals capability vapors solving label switching variable dimensional setting looking right panel figure role point process component capturing outliers observed samples described gaussian components clearer note point process component outliers allocated gaussian components consequently induce signicant deterioration parameter estimates table presents summaries provided vapors ones obtained bms approach trary bms approach vapors enabled benet information probable models summaries middle harder detect component turning results vapors seen estimated means compatible true radial frequencies furthermore estimated probabilities presence consistent certainty variable dimensional posterior shown figure discussed section main objectives algorithm proposed solve label switching issue observe better goodness estimated gaussian components panel figure depicts psfrag replacements k psfrag replacements y t s n e t n comp m s p m bms sbms psfrag replacements table summaries variable dimensional posterior distribution shown figure vs bms approach w figure posterior distribution sorted radial frequencies www given k normalized pdf tted gaussian components normalized posterior distributions sorted radial frequencies given gure validate coherency estimated summaries information variable dimensional posterior distribution seen gures shape pdf estimated gaussian components coherent location dispersion ones posterior sorted radial frequencies useful validating estimated summaries compare intensity estimated parametric model qhhh dened general p n mmm l s l l ignore point process component histogram intensity radial frequencies obtained bma approach chapter information figure shows gure specic example section solid black line indicates intensity estimated parametric model gures indicate goodness tted approximate posterior true finally validate estimated probabilities ence gaussian components mean parameter l poisson point process component figure illustrates posterior distribution number components approximated versions vapors seen gure vapors captured information provided true posterior number components obtain normalized densities rst normalized estimated pdf s maximum equal multiplied estimated probability presence gaussian component corresponding normalized estimated pdf maximum normalized density equal corresponding estimated probability presence w figure histogram intensity radial frequencies samples bma approach intensity tted parametric model obtained vapors y t l b b o r p k figure posterior distribution number number components black approximated version gray obtained tted model b joint bayesian detection estimation astrophysical particles auger project second illustrative example results signal decomposition problem encountered international astrophysics collaboration called auger auger project aimed studying ultra high energy cosmic rays energies order energetic particles found far universe long term objective project study nature ultra high energy cles determine origin universe observed directly fact collide earth s atmosphere host secondary particles generated muons nally reach ground detect pierre auger cosmic ray observatory built consists independent detectors array surface detectors sd number fluorescence detectors fd number muons arrival times indications chemical composition origin primary particles information concentrate signal decomposition problem goal count number muons estimate individual parameters signals observed sd detectors results use bayesian algorithm rj mcmc sampler developed trans dimensional problem joint detection estimation s s figure observed signal n intensity model aaam ttt dened k muons signal true arrival times e ttt m indicated vertical dashed lines psfrag replacements e p y t s n e t n psfrag replacements muons section rst briey describe problem use vapors relabel summarize dimensional output samples rj mcmc sampler oped muon crosses sd tank generates photoelectrons pe track captured detectors create discrete observed signal denote vector observed signal n nn nn element ni indicates number pe deposited muons time interval ti itd absolute starting time signal td ns signal resolution length bin muon component specic parameters arrival time tm signal amplitude absorption process photons generated muon modeled non homogeneous poisson point process intensity section tm pt td t tm pt td t time response distribution td time t exponential decay measured ns figure exponential shape intensities expected number pe bin obtained integrating intensity corresponding bin tm pt td t tm conditioning number muons vector parameters ttt m tm tm k aaam k assuming number pe bin independent likelihood written k ttt m aaam aaam ttt m aaam ttt m poisson distribution mean aaam ttt m assuming independence expected number pe ith bin muons e aaam ttt m given k ttt m aaam ti z n k nnn figure posterior distributions number muons left sorted arrival times ttt given k right constructed rj mcmc output samples discarding burn period true number components ve vertical dashed lines right gure locate arrival times aaam ttt m j tm j illustrate performance vapors simulated pe counting signal chapter results simulated experiments observed signal illustrative example considered consists ve muons located ttt m figure posterior distributions number muons sorted arrival times shown figure note example muons equal arrival times e fourth muons bms approach model muons selected n similar posterior probability observe marginal posterior arrival time component bimodal signicantly ran vapors l gaussian components rj mcmc output samples shown figure note n figure shows histogram labeled samples estimated parameters components gure seen bimodality effects caused label switching exhibited figure removed completely estimated gaussian components enjoy reasonable variances presented summary muons high probabilities presence corresponding ones shown row figure muons comparatively low probabilities presence fact samples allocated point process component shown row figure regarded residuals tted model observed samples l gaussian components qhhh able psfrag replacements p y t s n e t n psfrag replacements psfrag replacements y t s n e t n y t s n e t n l y t s n e t n e z l m r o n l l l gaussian comp gaussian comp m s p m s p l l l l gaussian comp gaussian comp m s p m s p l l l l gaussian comp gaussian comp m s p m s p figure histograms residuals tted model vapors different values l point process comp l figure histogram labeled samples pdf estimated gaussian components model black solid line vapors l variable dimensional postrior shown figure estimated parameters component presented corresponding panel figure normalized pdf s tted gaussian components vapors different values l describe residuals usual statistics tool goodness diagnostics model choice figure illustrates histograms residuals tted model different values l seen left panel figure distribution residuals corresponding case l contains signicant peaks peaks gradually removed adding gaussian components l component added tm captures samples distributed signicant peak left panel figure exist peaks particularly tm captured l distribution residuals case l l differ signicantly note decrease value l increasing l figure compares normalized intensities mated gaussian components l seen gure changing l reasonable range l inuence signicantly nal inference cases gaussian components estimated case l exist moving l l additional gaussian components added obtained summary low probabilities presence improve t change nal inference iv monte carlo experiment examples section iii illustrated capability vapors relabel summarize variable dimensional posterior distributions encountered signal sition problems order conrm ndings investigate systematically means monte carlo simulation experiment faithfully approximate posterior distribution preserves certain features true posterior distribution realizations sinusoid detection ment described section b figure simulated analyzed rj mcmc sampler number rj mcmc iterations set rst samples discarded burn period samples thinned fth initialize systematic fashion set l parametric model qhhh largest k posterior probability process sem type algorithms sufcient number samples allocated gaussian component equivalently probability presence fades zero remove parametric model decrease l approach generally results approximate posterior distributions richer provided bms sense l k map k map argmaxk initialize gaussian components parameters e means m l variances l previously robust estimates mean variances posterior distributions sorted radial frequencies given k l figure compares features tted mate posterior distribution hhh obtained iterations vapors corresponding features true variable dimensional posterior distribution features described rest section scatter plots shown panels c compare posterior distribution number components e approximated version denoted runs posterior probabilities k k comparison probabilities close zero digits situated right points panel indicate number occurrence corresponding event runs k map argmaxk seen panels information preserved approximated posterior distributions compare performance vapors direct bma reconstructing noiseless signal d end estimated reconstructed noiseless signal dened y zq kk k qqq y qqq y dqqq direct bma approach samples generated rj mcmc sampler integral approximated ybma m m design matrix ith vector sampled radial frequencies www posterior mean amplitudes given www hyperparameters reconstruct noiseless signal tted approximate posterior q hhh vapors generate r pairs r samples www explained section ii set yvapors r r panel d compares normalized reconstruction errors vapors ones direct bma approach db dened post processing step gaussian component endowed probability presence p l l l decide discard ones p l smaller certain threshold section discussion idea direct mean posterior means approximated rj mcmc samples directly vapors posterior norm set ybma yvapors bma approach vapors spectively seen gure normalized errors reconstructed noiseless signals compact summary obtained vapors comparable ones obtained bma approach finally scatter plots panels compare expected number components intervals p p p vapors ones obtained direct bma approach bma approach expected number components interval t p given y k y kk m m number radial frequencies observed t ith sample hand summary provided vapors expected number components interval t e hhh y p l n t hhh l l l n t hhh l denotes probability t sian distribution parameters hhh gures conrm expected number components chosen intervals computed approaches similar results shown section conrmed proximate posterior distribution hhh obtained vapors preserves faithfully important features true posterior distribution section results vein including numerical investigation comparison properties estimators derived vapors v conclusion paper proposed novel algorithm relabel summarize variable dimensional posterior butions encountered signal decomposition problems number component unknown purpose variable dimensional parametric model designed approximate posterior interest parameters approximate model estimated means sem type algorithm samples true posterior distribution generated trans dimensional monte carlo sampler e rj mcmc sampler modications initial sem type algorithm proposed order cope lack robustness maximum likelihood type estimates relevance proposed algorithm marizing relabeling variable dimensional posterior distributions illustrated signal decomposition examples problem detection estimation sinusoids gaussian white noise particle counting problem motivated astrophysics project auger notably vapors shown rst approach literature capable solving label switching issue trans dimensional problems shown proposed psfrag replacements y p p m k s r o p v psfrag replacements k map map estimator k posterior probability k psfrag replacements psfrag replacements direct bma posterior probability k reconstruction error db psfrag replacements psfrag replacements y p s r o p v s r o p v direct bma e p direct bma p figure comparison features true posterior distribution vapors approximation parametric model provides good approximation teriors encountered applications vapors provide user insight concerning component specic parameters uncertainties presence believe algorithm useful vast domain signal decomposition mixture model analysis enhance inference trans dimensional problems theoretical investigations required order extend available existing convergence results sem algorithm sem type algorithm paper correlated input data metropolis hastings updates future work focus vapors design efcient adaptive trans dimensional mcmc methods continuation ideas presented acknowledgment authors like express gratitude b kgl collaboration providing data auger example references roodaki signal decompositions trans dimensional bayesian methods thesis cole suprieure dlectricit suplec gif sur yvette france ph d n metropolis w rosenbluth m n rosenbluth h teller e teller equation state calculations fast computing machines journal chemical physics vol pp w k hastings monte carlo sampling methods markov chains m stephens dealing label switching mixture models journal royal statistical society series b statistical methodology pp jasra c c holmes d stephens markov chain monte carlo methods label switching problem bayesian mixture modeling statistical science vol pp g celeux m hurn c p robert computational inferential difculties mixture posterior distributions journal american statistical tion pp s frhwirth schnatter dealing label switching model uncertainty mixtures estimation applications k mengersen c p robert d terington eds pp wiley online library m sperrin t jaki e wit probabilistic relabelling strategies label switching problem bayesian mixture models statistics computing vol pp w yao model based labeling mixture models statistics computing pp c p robert discussion bayesian analysis mixtures unknown number components s richardson p j green journal royal statistical society series b statistical methodology vol pp roodaki j bect g fleury note computation hastings ratio birth death moves trans dimensional mcmc algorithms signal decomposition problems technical report cole suprieure dlectricit suplec gif sur yvette france m clyde e george model uncertainty statistical science vol pp g celeux j diebolt sem algorithm probabilistic teacher algorithm derived em algorithm mixture problem computational statistics quaterly vol pp g celeux j diebolt stochastic approximation type em algorithm mixture problem stochastics stochastics reports vol pp s f nielsen stochastic em algorithm estimation asymptotic results bernoulli vol pp m stephens bayesian methods mixture normal distributions ph d thesis d phill thesis university oxford oxford applications biometrika vol pp f karr point processes statistical inference second edition crc j s liu monte carlo strategies scientic computing springer verlag c p robert g casella monte carlo statistical methods second edition springer verlag basu r harris n l hjort m c jones robust efcient estimation minimising density power divergence biometrika vol pp p dempster n b laird d b rubin maximum likelihood incomplete data em algorithm journal royal statistical society series b statistical methodology m tanner w h wong calculation posterior distributions data augmentation journal american statistical association vol pp j diebolt g celeux asymptotic properties stochastic em algorithm estimating mixing proportions stochastic models vol pp p j huber e m ronchetti robust statistics edition wiley r n dav r krishnapuram robust clustering methods unied view ieee transactions fuzzy systems vol pp b kgl bayesian estimation metropolis hastings algorithm simple example technical report lal university paris sud cnrs france r bardenet b kgl d veberic single muon response signal model technical report lal university paris sud cnrs france m west approximating posterior distributions mixture journal royal statistical society series b statistical methodology vol pp h haario e saksman j tamminen adaptive metropolis algorithm bernoulli pp y bai r v craiu f di narzo divide conquer based approach regional adaptation mcmc journal computational graphical statistics vol pp r bardenet o capp g fort b kgl adaptive metropolis algorithm online relabeling proceeding international conference articial intelligence statistics aistats p j green reversible jump mcmc computation bayesian model nation biometrika vol pp p j green trans dimensional markov chain monte carlo highly structured stochastic systems p j green n l hjort s richardson eds pp o u p c andrieu doucet joint bayesian model selection estimation noisy sinusoids reversible jump mcmc ieee transactions signal processing vol pp j r larocque j p reilly reversible jump mcmc joint detection estimation sources coloured noise ieee transactions signal processing vol pp h rue m hurn bayesian object identication biometrika vol pp m ortner x descombes j zerubia building outline extraction digital elevation models marked point processes international journal computer vision vol pp c andrieu e barat doucet bayesian deconvolution noisy ltered point processes ieee transactions signal processing vol pp auger collaboration pierre auger project design report second edition auger org html auger collaboration properties performance prototype instrument pierre auger observatory nuclear instruments methods physics research vol pp s richardson p j green bayesian analysis mixtures unknown number components journal royal statistical society series b statistical methodology vol pp
