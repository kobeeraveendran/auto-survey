abstractive text summarization incorporating reader comments shen xiuying piji zhaochun lidong dongyan rui computer science technology peking university beijing china data science peking university beijing china lab shenzhen china com beijing china center singapore machine intelligence technology alibaba damo academy shengao chen zhaody edu com com inc com abstract neural abstractive summarization eld conventional sequence sequence based models suffer marizing wrong aspect document respect main aspect tackle problem propose task reader aware abstractive summary generation lizes reader comments help model produce better summary main aspect unlike traditional abstractive summarization task reader aware summarization confronts main challenges comments informal noisy jointly modeling news document reader ments challenging tackle challenges sign adversarial learning model named reader aware mary generator rasg consists components sequence sequence based summary generator reader attention module capturing reader focused aspects supervisor modeling semantic gap erated summary reader focused aspects goal tracker producing goal generation step supervisor goal tacker guide training work adversarial manner extensive experiments conducted large scale real world text summarization dataset results rasg achieves art performance terms automatic metrics human evaluations experimental results strate effectiveness module framework release large scale dataset introduction abstractive summarization regarded sequence mapping task source text mapped target summary drawn attention deep ral networks widely applied natural language ing eld recently sequence sequence work sutskever vinyals proved fective task abstractive summarization chopra auli rush liu manning text generation tasks tao gao paper use aspect denote topic described specic paragraph sentence news document use main aspect denote central corresponding author rui yan edu copyright association advancement articial intelligence www aaai org rights reserved table examples text summarization text red denotes focused aspect good summary text blue described bad summary text underline focused aspect reader comments document comments good summary bad summary august according person familiar matter toyota motor corporation invest million lars uber taxi service company valuation billion dollars investment focus driverless car technology development path smooth march year uber driverless car hit woman caused death year softbank invested uber valuation billion toyota investment uber wise choice million investment lot money toyota invests million uber valuation billion uber driverless car hits passerby death topic author tends convey readers document describe event ent aspects summary document focus main aspect shown table good summary describes main aspect bad summary describes trivial aspect main point document focus main aspect marization methods sun zhou bansal chen rst select sentences main aspect generate summary challenging discover main aspect news document nowadays great number news comments erated readers express opinions event comments mention main aspect ument times case table ample focused aspect reader investment toyota main aspect document specic dene reader focused aspect denote focused aspect reader comments itively reader comments help summary erator capture main aspect document ing quality generated summary paper investigate new problem setting task abstractive text summarization paradigm extension reader aware abstractive text summarization effect comments social contexts ment summarization explored vious works sun lim yang bing lam unlike approaches directly extract sentences nal document sun lim yang aim generate natural sounding mary scratch instead extracting words ument generally existing text summarization approaches challenges addressing reader aware rization task rst challenge reader comments noisy informative information provided comments useful modeling reader focused aspects crucial model ability capturing main aspect ltering noisy mation incorporating reader comments second challenge generate summaries jointly modeling main aspect document reader focused aspect revealed comments model sensitive diverse unimportant aspects introduced reader comments simply absorbing reader aspect information directly guide model erate summary feasible generator lose ability modeling main aspect paper propose summarization framework named reader aware summary generator rasg corporates reader comments improve summarization performance specically architecture tion mechanism employed basic summary ator rst calculate alignment reader ments words document words alignment formation regarded reader attention representing reader focused aspect treat decoder tion weights focused aspect generated summary decoder focused aspect decoding step supervisor designed measure distance reader focused aspect decoder focused aspect given distance goal tracker provides goal decoder induce reduce distance training framework rasg conducted adversarial way evaluate performance model collect large document summary pairs associated reader comments social media website extensive periments conducted dataset rasg icantly outperforms state art baselines terms rouge metrics human evaluations sum contributions summarized lows propose reader aware abstractive text tion task solve task propose end end ing framework conduct reader attention modeling reader aware summary generation design supervisor goal tracker guide generator focus main aspect document reduce noisy information introduced reader comments propose denoising module identify comments helpful summary generation matically release large scale abstractive text summarization dataset associated reader comments experimental sults dataset demonstrate effectiveness posed framework related work text summarization classied extractive stractive methods extractive methods jadhav rajan narayan cohen lapata read article representations sentences article lect sentences summaries generated extractive methods suffer redundancy problem recently emergence neural network models text eration vast majority literature summarization dedicated abstractive summarization bansal chen zhou text marization benchmark dataset cnn dailymail state art abstractive methods outperform best extractive method terms rouge score methods stractive text summarization based sequence sequence model sutskever vinyals encodes source texts semantic representation encoder generates summaries resentation decoder tackle vocabulary problem researchers employ copy mechanism copy words input document summary liu manning capture main aspect document chen propose lect salient sentences rewrite sentences concise summary approach achieves state art text summarization cnn dailymail benchmark dataset unlike document summarization needs code long text social media summarization usually reads short noisy text popular task days propose short text tion dataset social media researchers follow task lin propose based model uses cnn rene representation source context wang use convolutional model summarize text use policy gradient algorithm directly optimize rouge score marization models utilize reader comments generating summaries consider reader comments text tion reader aware summarization proposed mainly takes form extractive approaches graph based method comment oriented tion task sun lim identify relations topic quotation tion comments linked recently nguyen publish small extractive sentence comment dataset train neural models small size pose unsupervised compressive multi document rization model sparse coding method following vious work models bing lam variational auto encoder model latent semantic original article reader comments ferent abstractive summarization task related works based extractive compressive approaches problem formulation presenting approach reader aware marization rst introduce notations key cepts begin document assume comment set comment denotes word document denotes word comment sentence given ument summary generator reads comments generates summary finally use difference generated summary ground truth summary training signal optimize model parameters proposed rasg model overview section propose reader aware summary erator abbreviated rasg overview rasg shown figure split main parts summary generator based architecture attention copy mechanisms reader attention module learns semantic alignment word document comments tures reader focused aspect supervisor measures semantic gap decoder focused aspect reader focused aspect criminator uses convolutional neural network tract features distinguishes similar decoder focused aspect reader focused aspect goal tracker utilizes semantic gap learned visor features extracted learned tor set goal utilized specic guidance summary generator produce better summary summary generator beginning use embedding matrix map hot representation word document comments high dimensional vector space note embedding representation word embedding representations employ directional recurrent neural network rnn model temporal interactions words denotes hidden state step rnn document denote nal hidden state rnnd vector representation document following liu manning choose long short term memory lstm rnn cell apply linear transform layer input use output ument vector representation layer initial state decoder lstm shown tion order reduce burden compressing ment information initial state use attention mechanism bahdanau cho bengio rize input document context vector cally detail following sections concatenate context vector embedding previous step output feed decoder lstm shown equation use notion concatenation vectors wdhd lstm decoding step use decoder state attend document states resulting attention distribution shown equation use attention distribution weighted sum document states context vector tanh whhd exp exp ihd finally output projection layer applied nal generating distribution vocabulary shown equation concatenate goal vector gap content output decoder lstm input projection layer goal vector represents goal current generation step gap content denotes mantic gap generated summary reader focused document details variables following sections softmax order handle vocabulary oov problem equip pointer network vinyals nato jaitly liu manning decoder makes decoder capable copy words source text design pointer network model liu manning omit procedure paper limited space use negative log likelihood loss tion log denoising module fact reader comments kind mal text consist noisy information comments helpful generating better maries consequently employ denoising module distinguish comments helpful employ rnnc model comment word embeddings denotes hidden state word ment use average pooling operation figure overview rasg divide model parts summary generator generates summary describe main aspect document reader attention module models readers attention document supervisor models gap focused document aspect generated summary reader comments goal tracker sets goal summary generator according gap given supervisor hidden states produce vector representation comment shown equation finally apply ear transform sigmoid function predict comment useful sigmoid output seen salience score comment given document representation train denoising module use cross entropy loss supervise procedure ground truth salience score ments denotes comment helpful generating summary vice versa reader attention modeling model reader focused aspect rst calculate word alignment reader comments document use embeddings words document ments calculate semantic alignment score precisely alignment socre document word word comment shown equation equation use max operation alignment signify word document focused comment regard alignment score reader attention weight reader comment document word order reduce interference caused noisy comments employ comment salience score tained denoising module weighted combine reader attention shown equation means noisy comments contribute procedure reader attention modeling exp exp finally reader attention document word softmax function shown equation supervisor model semantic gap generated summary reader focused aspects design supervisor ule decoder need know aspect document focused summary generator past decoding steps sum latest attention butions result focus distribution generated summary ument words shown equation use weighted sum document hidden states result ihd represents focused aspect latest coding steps decoder focused aspect use reader attention weighted sum document hidden states reader supervisorcomment contentdiscriminatorgoal goal trackerreader focused aspectgenerated aspectgap contentdocumentattentiondocumentattentiondocumentattentioncontext summary generator represents reader focused aspect encouraging decoder focused aspect ilar reader focused aspect employ cnn based discriminator signify difference decoder focused aspect reader focused aspect use difference guide decoder focus reader focused aspect typically discriminator binary classier decomposed tional feature extractor shown equation moid classication layer shown equation denotes convolutional operation trainable rameter denotes convolutional kernel classication probabilities note token generated time inuence gradient received time gradient subsequent time steps intuitively decoding attention decoding step similar attention nal summary earlier steps propose dene cumulative loss discount factor loss functions note training objective inator interpreted maximizing log likelihood classication input equation comes reader focused aspect decoder focused aspect order model gap reader focused aspect decoder focused aspect subtract reader tion resulting attention difference shown equation use attention difference sum document hidden states ihd denotes semantic unfocused document pects summary generator gap content age summary generator focus unfocused ment aspects feed gap content generator shown equation goal tracker discriminator provides scalar guiding nal decoding step relatively formative sentence length goes larger inspired leakgan guo proposed rasg work allows discriminator provide additional information denoted goal vector view certain ship goal current decoding step previous steps need model temporal interactions goal step specically introduce goal tracker module lstm takes extracted feature vector gap content input step outputs goal vector order achieve higher consistency reader focused pect feed goal vector generator guide generation word shown equation model training model trained adversarial manner split parameters model parts tion module including parameters summary generator reader attention module goal tracker discriminator module including parameters cnn classier training generation module sum loss function denoising module cross entropy ground truth result discriminator shown tion use optimize parameters tion module train discriminator module maximize probability assigning correct label generated aspect reader focused aspect specically optimize parameters discriminator module according loss function calculated equation experimental setup research questions list research questions guide experiments rasg outperform baselines effect module rasg rasg capture useful information noisy comments goal tracker helpful guidance decoder dataset collect document summary comments pair data weibo largest social network website china users read document post ment document website sample data contains document summary reader comments comments readers opinion focused aspect document order train denoising module ground truth label comment common word summary comment regard comment helpful generating summary accordingly comment contains mon word total training dataset contains training samples erage length document words average length comment words average length summary words average comments number document evaluation metrics evaluation metrics adopt rouge score lin widely applied summarization evaluation sun chen rouge metrics compare generated summary reference summary puting overlapping lexical units including igram gram rouge longest mon subsequence table ablation models comparison table rouge scores different ablation models acronym gloss rouge rasg rasg denoising module rasg rasg rasg gtd rasg goal tracker discriminator rasg gap content rasg goal tracker table rouge scores comparison baselines rasg rasg rasg rasg gtd rasg rouge cgu rasg textrank comparison methods order prove effectiveness module rasg conduct ablation models introduced table evaluate performance proposed dataset model compare following baselines sequence sequence framework sutskever vinyals proposed language generation task simply add reader tention attention distribution decoding step cgu lin propose use volutional gated unit rene source representation achieves state art performance social media text summarization dataset commonly baseline nallapati zhai zhou liu manning selects rst tence document summary textrank cea propose build graph add tence vertex use link represent semantic ity sentences sorted based nal scores greedy algorithm employed select summary sentences implementation details implement experiments tensorflow abadi nvidia gpu word embedding mension set number hidden units set equation tion use adagrad optimizer duchi hazan singer optimizing algorithm employ beam search beam size generate uency mary sentence experimental results overall performance research question examine performance model terms rouge table lists mances comparisons terms rouge score rasg achieves increment state art method cgu terms rouge respectively worth noticing baseline model achieves better performance demonstrates effectiveness rating reader focused aspect summary generation compared rasg achieves lower formance terms rouge score simply adding reader focused aspect generation procedure good reader aware summarization method ablation study turn research question conduct lation tests usage denoising module supervisor goal tracker rouge score result shown table discriminator provides scalar training signal generator training feature tor goal tracker consequently crement rasg gtd rasg terms rouge demonstrates tiveness discriminator effectiveness goal tracker compared rasg rasg rasg gtd offers decrease terms respectively demonstrates goal tracker feature discriminator plays tant role producing better summary goal tracker feature extracted nator help improve performance mary generator shown performance rasg gtd finally rasg offers decrease compared rasg terms rouge strates effectiveness denoising module denoising ability turn research question fact denoising module learned supervised way ground truth label associated comment predict salience score classify helpful comment vice versa denoising module regarded binary classier classify ment calculate classication recall score comments measure performance module recall curve shown figure ing progresses recall score steady upward curve proves improved performance denoising ule conclude denoising module ful salience score subsequent process figure cosine distance decoding attention reader attention recall score denoising module table consistency uency comparison human evaluation fluency consistency mean variance mean variance cgu rasg analysis goal tracker section turn research question main purpose employing goal tracker help summary generator utilize reader focused aspect intuitively want know summary generator follows goal set goal tracker calculate sine distance decoder attention equation reader attention equation figure compare cosine distance lation model rasg gtd rasg rasg observes decrease cosine distance conversely rasg gtd observes increment cosine distance fact rasg narrow cosine distance proves goal tracker discriminator lead generator follow reader focused aspect human evaluation ask highly educated students rate erated summaries different models according tency uency annotators native speakers rating score ranges best average score summaries nal score model shown table seen rasg outperforms baseline models sentence uency consistency large margin calculate kappa statistics terms uency consistency score respectively prove signicance results paired student test model cgu model row shaded background value uency tency respectively case analysis figure shows document corresponding maries generated different methods observe figure examples generated summary rasg models generate uent summary generated aspect contradictory focused aspect reader ground truth summary rasg overcomes shortcoming goal vector gap content given goal tracker supervisor training stage produces summary uent consistent main aspect document conclusion paper propose new framework named aware summary generator rasg aims generate summaries document social media incorporating reader comments order capture reader focused aspect design reader attention component noising module capture alignment comments document employ supervisor measure mantic gap generated summary reader focused aspect goal tracker uses information semantic gap feature extracted discriminator produce goal vector guide summary generator iments demonstrated effectiveness rasg found signicant improvements state art baselines terms rouge human evaluations veried effectiveness module rasg improving summarization performance cosinedistancebetweencurrentcontentandreadercontent consistencyanduencycomparisonbyhumanevaluation consistencyanduency wenallytaketheaverageacrosssummariesandannotators inta toprovethesignicanceoftheaboveresults wealsodothepairedstudentt testbetweenourmodelandbaselinemeth ods thep sistencyrespectively eratedsummariesbydifferentmethods butarecontradictorytothefocusofreaderandgroundtruthsummary rasgovercomesthisshortcomingbyusinggoalvectorandgapcontentgivenbysupervisorandgoaltrackerattrainingstage andproducethesummarynotonlyuentbutalsocon sistentwiththefocusofreaders conclusioninthispaper wehaveproposedthetaskofreader awaresummarygeneration whichaimstogeneratesummariesfortextfromsocialmediaincorporatethecommentsofread ers toaddressthistask wehaveproposedreader sequencebasedsummarygeneratorisusedtoencodethedocumentandthengeneratethesummarysentencewithattentionandcopymechanism inordertocapturethefocusedaspectbythereaders weuseareaderattentionmoduletomodelthealign examplesofthegeneratedsummarybyrasgandothermodels xyi carryouteducationandtrainingforpractitioners andstrivetocreatenewthenationalteamandthemainforceofthemediaplatform seelongweibofordetails comments ownedcapitalmustbeinvolved tralnetworkofceshouldcuretheseblacksheep establishingstate thecentralnetworkofcewillprovidepolicysupporttoestablishstate ownedcapital workinghardtobuildanationalteamofnewmediaplat form mentofcommentsanddocument weemployasupervi sortomeasurethesemanticgapbetweenaspectofgener atedsummaryandreader focuseddocument finally agoaltrackerusestheinformationofsemanticgapandthefea tureextractedbythediscriminatortoproduceagoalvec tortoguidethesummarygenerator inourexperiments wehavedemonstratedtheeffectivenessofrasgandhavefoundsignicantimprovementsoverstate artbase linesintermsofrougeandhumanevaluations wehaveveriedtheeffectivenessofeachmoduleinrasgforimprovingreader awaresummarygeneration futureworkinvolvesextendingourmodeltoimprovedadversarialtrainingskillslikewassersteingan weplantopursueanoveltextmatchingmethodtodenoisingmoduleforimprovingtheaccuracy acknowledgments like thank anonymous reviewers constructive comments like thank jun zhang sicong jiang helps project work supported national key research development program china national science foundation china nsfc alibaba innovative research air fund rui yan sponsored ccf tencent open research fund microsoft research asia msra laborative research program references abadi barham chen chen davis dean devin ghemawat irving isard tensorow system large scale machine ing osdi volume bahdanau cho bengio neural chine translation jointly learning align translate iclr bansal chen fast abstractive rization reinforce selected sentence rewriting acl chen gao tao song zhao yan iterative document representation learning summarization polishing emnlp chopra auli rush abstractive sentence summarization attentive recurrent neural works hlt naacl duchi hazan singer adaptive subgradient methods online learning stochastic mization jmlr gao ren zhao zhao yin yan product aware answer generation commerce question answering wsdm ing copying mechanism sequence sequence learning corr guo cai zhang wang long text generation adversarial training leaked information corr chen zhu lcsts large scale chinese short text summarization dataset emnlp sun lim comments oriented blog summarization sentence extraction cikm sun lim comments oriented document summarization understanding documents readers feedback sigir jadhav rajan extractive summarization swap net sentences words alternating pointer networks acl bing lam liao aware multi document summarization sparse coding ijcai wang lam ren bing salience estimation variational auto encoders document summarization aaai bing lam reader aware document summarization enhanced model rst dataset proceedings workshop new frontiers summarization lin sun global encoding abstractive summarization acl lin rouge package automatic evaluation summaries text summarization branches sun lin ren hierarchical end end model jointly improving text summarization sentiment classication ijcai sun lin wang coder assistant supervisor improving text representation chinese social media text summarization acl mihalcea tarau textrank bringing order text emnlp nallapati zhai zhou summarunner recurrent neural network based sequence model tive summarization documents aaai narayan cohen lapata ranking sentences extractive summarization reinforcement learning naacl hlt nguyen tran tran nguyen solscsum linked sentence comment dataset social context summarization cikm liu manning point summarization pointer generator networks arxiv preprint sun hsu lin lee min tang unied model extractive abstractive summarization inconsistency loss acl sutskever vinyals sequence sequence learning neural networks nips tao gao shang zhao yan point utterance learning effective responses multi head attention mechanism ijcai vinyals fortunato jaitly pointer networks nips wang yao tao zhong liu reinforced topic aware convolutional sequence sequence model abstractive text summarization cai yang cai tang zhang social context summarization sigir zhou yang wei zhou selective encoding abstractive sentence summarization acl zhou yang wei zhou sequential copying networks aaai
