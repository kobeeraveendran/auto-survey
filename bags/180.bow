abstractive text summarization incorporating reader comments shen xiuying piji zhaochun lidong dongyan rui computer science technology peking university beijing china data science peking university beijing china ai lab shenzhen china com beijing china center singapore machine intelligence technology alibaba damo academy shengao xy chen zhaody edu cn com com l inc com c e d l c s c v v x r abstract neural abstractive summarization eld conventional sequence sequence based models suffer marizing wrong aspect document respect main aspect tackle problem propose task reader aware abstractive summary generation lizes reader comments help model produce better summary main aspect unlike traditional abstractive summarization task reader aware summarization confronts main challenges comments informal noisy jointly modeling news document reader ments challenging tackle challenges sign adversarial learning model named reader aware mary generator rasg consists components sequence sequence based summary generator reader attention module capturing reader focused aspects supervisor modeling semantic gap erated summary reader focused aspects goal tracker producing goal generation step supervisor goal tacker guide training work adversarial manner extensive experiments conducted large scale real world text summarization dataset results rasg achieves art performance terms automatic metrics human evaluations experimental results strate effectiveness module framework release large scale dataset introduction abstractive summarization regarded sequence mapping task source text mapped target summary drawn attention deep ral networks widely applied natural language ing eld recently sequence sequence work sutskever vinyals le proved fective task abstractive summarization chopra auli rush liu manning text generation tasks tao et al gao et al paper use aspect denote topic described specic paragraph sentence news document use main aspect denote central corresponding author rui yan edu cn copyright association advancement articial intelligence www aaai org rights reserved cn table examples text summarization text red denotes focused aspect good summary text blue described bad summary text underline focused aspect reader comments document comments good summary bad summary august according person familiar matter toyota motor corporation invest million u s lars uber taxi service company valuation billion u s dollars investment focus driverless car technology development path smooth march year uber driverless car hit woman caused death year softbank invested uber valuation billion toyota s investment uber wise choice million investment lot money toyota invests million uber valuation billion uber driverless car hits passerby death topic author tends convey readers document describe event ent aspects summary document focus main aspect shown table good summary describes main aspect bad summary describes trivial aspect main point document focus main aspect marization methods sun et al zhou et al bansal chen rst select sentences main aspect generate summary challenging discover main aspect news document nowadays great number news comments erated readers express opinions event comments mention main aspect ument times case table ample focused aspect reader investment toyota main aspect document specic dene reader focused aspect denote focused aspect reader comments itively reader comments help summary erator capture main aspect document ing quality generated summary paper investigate new problem setting task abstractive text summarization paradigm extension reader aware abstractive text summarization effect comments social contexts ment summarization explored vious works hu sun lim yang et al li et al li bing lam unlike approaches directly extract sentences nal document hu sun lim yang et al li et al aim generate natural sounding mary scratch instead extracting words ument generally existing text summarization approaches challenges addressing reader aware rization task rst challenge reader comments noisy informative information provided comments useful modeling reader focused aspects crucial model ability capturing main aspect ltering noisy mation incorporating reader comments second challenge generate summaries jointly modeling main aspect document reader focused aspect revealed comments model sensitive diverse unimportant aspects introduced reader comments simply absorbing reader aspect information directly guide model erate summary feasible generator lose ability modeling main aspect paper propose summarization framework named reader aware summary generator rasg corporates reader comments improve summarization performance specically architecture tion mechanism employed basic summary ator rst calculate alignment reader ments words document words alignment formation regarded reader attention representing reader focused aspect treat decoder tion weights focused aspect generated summary decoder focused aspect decoding step supervisor designed measure distance reader focused aspect decoder focused aspect given distance goal tracker provides goal decoder induce reduce distance training framework rasg conducted adversarial way evaluate performance model collect large document summary pairs associated reader comments social media website extensive periments conducted dataset rasg icantly outperforms state art baselines terms rouge metrics human evaluations sum contributions summarized lows propose reader aware abstractive text tion task solve task propose end end ing framework conduct reader attention modeling reader aware summary generation design supervisor goal tracker guide generator focus main aspect document reduce noisy information introduced reader comments propose denoising module identify comments helpful summary generation matically release large scale abstractive text summarization dataset associated reader comments experimental sults dataset demonstrate effectiveness posed framework related work text summarization classied extractive stractive methods extractive methods jadhav rajan narayan cohen lapata read article representations sentences article lect sentences summaries generated extractive methods suffer redundancy problem recently emergence neural network models text eration vast majority literature summarization dedicated abstractive summarization bansal chen ma et al zhou et al text marization benchmark dataset cnn dailymail state art abstractive methods outperform best extractive method terms rouge score methods stractive text summarization based sequence sequence model sutskever vinyals le encodes source texts semantic representation encoder generates summaries resentation decoder tackle vocabulary problem researchers employ copy mechanism copy words input document summary gu et al liu manning capture main aspect document chen et al propose lect salient sentences rewrite sentences concise summary approach achieves state art text summarization cnn dailymail benchmark dataset unlike document summarization needs code long text social media summarization usually reads short noisy text popular task days hu et al propose short text tion dataset social media researchers follow task lin et al propose based model uses cnn rene representation source context wang et al use convolutional model summarize text use policy gradient algorithm directly optimize rouge score marization models utilize reader s comments generating summaries consider reader s comments text tion reader aware summarization proposed mainly takes form extractive approaches graph based method comment oriented tion task hu sun lim identify relations topic quotation tion comments linked recently nguyen et al publish small extractive sentence comment dataset train neural models small size li et al pose unsupervised compressive multi document rization model sparse coding method following vious work models li bing lam li et al variational auto encoder model latent semantic original article reader comments ferent abstractive summarization task related works based extractive compressive approaches xd problem formulation presenting approach reader aware marization rst introduce notations key cepts xc xd begin document x d xd t d assume comment set x c ct c th comment xd xc ci xc denotes th word document x d xc j denotes j th word th comment sentence ci given ument x d summary generator reads comments x c generates summary y yt y finally use difference generated summary y ground truth summary y training signal optimize model parameters t c proposed rasg model overview section propose reader aware summary erator abbreviated rasg overview rasg shown figure split main parts summary generator based architecture attention copy mechanisms reader attention module learns semantic alignment word document comments tures reader focused aspect supervisor measures semantic gap decoder focused aspect reader focused aspect criminator uses convolutional neural network tract features distinguishes similar decoder focused aspect reader focused aspect goal tracker utilizes semantic gap learned visor features extracted learned tor set goal utilized specic guidance summary generator produce better summary summary generator beginning use embedding matrix e map hot representation word document x comments x c high dimensional vector space note embedding representation word embedding representations employ bi directional recurrent neural network bi rnn model temporal interactions words bi hd t hd hd t denotes hidden state t th step bi rnn document x d denote nal hidden state hd t d bi rnnd vector representation document x d following liu manning ma et al choose long short term memory lstm rnn cell apply linear transform layer input t d use output ument vector representation hd layer initial state decoder lstm shown tion order reduce burden compressing ment information initial state use attention mechanism bahdanau cho bengio rize input document context vector cally detail following sections concatenate context vector embedding previous step output feed decoder lstm shown equation use notion concatenation vectors wdhd st lstm t d bd t th decoding step use decoder state attend document states hd resulting attention distribution t rt d shown equation use attention distribution t weighted sum document states context vector tanh whhd d exp t t j t w t exp d t ihd finally output projection layer applied nal generating distribution pv vocabulary shown equation concatenate goal vector gt gap content dt output decoder lstm st input projection layer goal vector gt represents goal current generation step gap content dt denotes mantic gap generated summary reader focused document details variables following sections pv softmax gt bv order handle vocabulary oov problem equip pointer network gu et al vinyals nato jaitly liu manning decoder makes decoder capable copy words source text design pointer network model liu manning omit procedure paper limited space use negative log likelihood loss tion lg y log denoising module fact reader comments kind mal text consist noisy information comments helpful generating better maries consequently employ denoising module distinguish comments helpful employ bi rnnc model comment word embeddings t bi hc t t denotes hidden state t th word ment ci use average pooling operation figure overview rasg divide model parts summary generator generates summary describe main aspect document reader attention module models readers attention document supervisor models gap focused document aspect generated summary reader comments goal tracker sets goal summary generator according gap given supervisor hidden states produce vector representation ai th comment shown equation finally apply ear transform sigmoid function predict comment useful sigmoid output seen salience score th comment given document representation hd ai hd t d hc t bi t c train denoising module use cross entropy loss supervise procedure ld c ground truth salience score ments denotes th comment ci helpful generating summary vice versa reader attention modeling model reader focused aspect rst calculate word alignment reader comments document use embeddings words document ments calculate semantic alignment score precisely j k alignment socre th document word xd k th word j th comment xc j shown equation j k j j t c j k j equation use max operation alignment j signify th word document focused j th comment regard alignment score reader attention weight j th reader comment th document word order reduce interference caused noisy comments employ comment salience score j tained denoising module weighted combine j th reader attention shown equation means noisy comments contribute procedure reader attention modeling c j exp j d exp j finally reader attention r th document word softmax function shown equation supervisor model semantic gap generated summary reader focused aspects design supervisor ule decoder need know aspect document focused summary generator past decoding steps sum latest k attention butions t result t rt d focus distribution generated summary t d ument words shown equation use t weighted sum document hidden states hd result mt t k mt d t ihd ti mt represents focused aspect latest k coding steps decoder focused aspect use reader attention weighted sum document hidden states hd u d reader supervisorcomment contentdiscriminatorgoal goal trackerreader focused aspectgenerated aspectgap contentdocumentattentiondocumentattentiondocumentattentioncontext summary generator u represents reader focused aspect encouraging decoder focused aspect ilar reader focused aspect employ cnn based discriminator signify difference decoder focused aspect mt reader focused aspect u use difference guide decoder focus reader focused aspect typically discriminator binary classier decomposed tional feature extractor f shown equation moid classication layer shown equation x m t bf u bf denotes convolutional operation trainable rameter wc denotes convolutional kernel m t u classication probabilities note token generated time t inuence gradient received time gradient subsequent time steps intuitively decoding attention t decoding step similar attention nal summary earlier steps propose dene cumulative loss discount factor loss functions note training objective inator interpreted maximizing log likelihood classication input equation comes reader focused aspect decoder focused aspect t u m t m order model gap reader focused aspect decoder focused aspect subtract reader tion rt d resulting attention difference t rt d shown equation use attention difference t sum document hidden states hd d ld c d lg t rt d t dt d t ihd t t dt denotes semantic unfocused document pects summary generator gap content age summary generator focus unfocused ment aspects feed gap content dt generator shown equation goal tracker discriminator provides scalar guiding nal m t decoding step relatively formative sentence length t y goes larger inspired leakgan guo et al proposed rasg work allows discriminator provide additional information denoted goal vector gt view certain ship goal current decoding step previous steps need model temporal interactions goal step specically introduce goal tracker module lstm takes extracted feature vector gap content input step t outputs goal vector gt gt order achieve higher consistency reader focused pect feed goal vector gt generator guide generation word shown equation model training model trained adversarial manner split parameters model parts tion module including parameters summary generator reader attention module goal tracker discriminator module including parameters cnn classier training generation module sum loss function denoising module ld cross entropy ground truth lg result discriminator lg c shown tion use l optimize parameters tion module l lg ld lg c train discriminator module maximize probability assigning correct label generated aspect mt reader focused aspect u specically optimize parameters discriminator module according loss function ld calculated equation experimental setup research questions list research questions guide experiments rasg outperform baselines effect module rasg rasg capture useful information noisy comments goal tracker helpful guidance decoder dataset collect document summary comments pair data weibo largest social network website china users read document post ment document website sample data contains document summary reader comments comments readers opinion focused aspect document order train denoising module ground truth label th comment common word summary comment regard comment helpful generating summary accordingly comment contains mon word total training dataset contains training samples erage length document words average length comment words average length summary words average comments number document evaluation metrics evaluation metrics adopt rouge score lin widely applied summarization evaluation sun et al chen et al rouge metrics compare generated summary reference summary puting overlapping lexical units including igram bi gram rouge l longest mon subsequence table ablation models comparison table rouge scores different ablation models acronym gloss rouge l rasg dm rasg denoising module rasg g rasg gt rasg gtd rasg goal tracker discriminator rasg gap content rasg goal tracker table rouge scores comparison baselines rasg dm rasg g rasg gt rasg gtd rasg rouge l cgu rasg textrank comparison methods order prove effectiveness module rasg conduct ablation models introduced table evaluate performance proposed dataset model compare following baselines sequence sequence framework sutskever vinyals le proposed language generation task simply add reader tention attention distribution t decoding step cgu lin et al propose use volutional gated unit rene source representation achieves state art performance social media text summarization dataset commonly baseline nallapati zhai zhou liu manning selects rst tence document summary textrank cea et al propose build graph add tence vertex use link represent semantic ity sentences sorted based nal scores greedy algorithm employed select summary sentences implementation details implement experiments tensorflow abadi et al nvidia gpu word embedding mension set number hidden units set equation tion use adagrad optimizer duchi hazan singer optimizing algorithm employ beam search beam size generate uency mary sentence experimental results overall performance research question examine performance model terms rouge table lists mances comparisons terms rouge score rasg achieves increment state art method cgu terms rouge l respectively worth noticing baseline model achieves better performance demonstrates effectiveness rating reader focused aspect summary generation compared rasg achieves lower formance terms rouge score simply adding reader focused aspect generation procedure good reader aware summarization method ablation study turn research question conduct lation tests usage denoising module supervisor goal tracker rouge score result shown table discriminator provides scalar training signal lg c generator training feature tor goal tracker consequently crement rasg gtd rasg gt terms rouge l demonstrates tiveness discriminator effectiveness goal tracker compared rasg rasg gt rasg gtd offers decrease terms respectively demonstrates goal tracker feature discriminator plays tant role producing better summary goal tracker feature extracted nator help improve performance mary generator shown performance rasg gtd finally rasg dm offers decrease compared rasg terms rouge l strates effectiveness denoising module denoising ability turn research question fact denoising module learned supervised way ground truth label associated comment predict salience score classify helpful comment vice versa denoising module regarded binary classier classify ment calculate classication recall score comments measure performance module recall curve shown figure ing progresses recall score steady upward curve proves improved performance denoising ule conclude denoising module ful salience score subsequent process figure cosine distance decoding attention reader attention recall score denoising module table consistency uency comparison human evaluation fluency consistency mean variance mean variance cgu rasg analysis goal tracker section turn research question main purpose employing goal tracker help summary generator utilize reader focused aspect intuitively want know summary generator follows goal set goal tracker calculate sine distance decoder attention t y rt d equation reader attention rt d equation figure compare cosine distance lation model rasg gtd rasg rasg observes decrease cosine distance conversely rasg gtd observes increment cosine distance fact rasg narrow cosine distance proves goal tracker discriminator lead generator follow reader focused aspect human evaluation ask highly educated ph d students rate erated summaries different models according tency uency annotators native speakers rating score ranges best average score summaries nal score model shown table seen rasg outperforms baseline models sentence uency consistency large margin calculate kappa statistics terms uency consistency score respectively prove signicance results paired student t test model cgu model row shaded background value uency tency respectively case analysis figure shows document corresponding maries generated different methods observe figure examples generated summary rasg models generate uent summary generated aspect contradictory focused aspect reader ground truth summary rasg overcomes shortcoming goal vector gap content given goal tracker supervisor training stage produces summary uent consistent main aspect document conclusion paper propose new framework named aware summary generator rasg aims generate summaries document social media incorporating reader comments order capture reader focused aspect design reader attention component noising module capture alignment comments document employ supervisor measure mantic gap generated summary reader focused aspect goal tracker uses information semantic gap feature extracted discriminator produce goal vector guide summary generator iments demonstrated effectiveness rasg found signicant improvements state art baselines terms rouge human evaluations veried effectiveness module rasg improving summarization performance cosinedistancebetweencurrentcontentandreadercontent consistencyanduencycomparisonbyhumanevaluation consistencyanduency wenallytaketheaverageacrosssummariesandannotators inta toprovethesignicanceoftheaboveresults wealsodothepairedstudentt testbetweenourmodelandbaselinemeth ods thep sistencyrespectively eratedsummariesbydifferentmethods butarecontradictorytothefocusofreaderandgroundtruthsummary rasgovercomesthisshortcomingbyusinggoalvectorandgapcontentgivenbysupervisorandgoaltrackerattrainingstage andproducethesummarynotonlyuentbutalsocon sistentwiththefocusofreaders conclusioninthispaper wehaveproposedthetaskofreader awaresummarygeneration whichaimstogeneratesummariesfortextfromsocialmediaincorporatethecommentsofread ers toaddressthistask wehaveproposedreader sequencebasedsummarygeneratorisusedtoencodethedocumentandthengeneratethesummarysentencewithattentionandcopymechanism inordertocapturethefocusedaspectbythereaders weuseareaderattentionmoduletomodelthealign examplesofthegeneratedsummarybyrasgandothermodels q xyi s carryouteducationandtrainingforpractitioners andstrivetocreatenewthenationalteamandthemainforceofthemediaplatform seelongweibofordetails comments ownedcapitalmustbeinvolved tralnetworkofceshouldcuretheseblacksheep eh establishingstate s na q thecentralnetworkofcewillprovidepolicysupporttoestablishstate ownedcapital q s workinghardtobuildanationalteamofnewmediaplat form mentofcommentsanddocument weemployasupervi sortomeasurethesemanticgapbetweenaspectofgener atedsummaryandreader focuseddocument finally agoaltrackerusestheinformationofsemanticgapandthefea tureextractedbythediscriminatortoproduceagoalvec tortoguidethesummarygenerator inourexperiments wehavedemonstratedtheeffectivenessofrasgandhavefoundsignicantimprovementsoverstate artbase linesintermsofrougeandhumanevaluations wehaveveriedtheeffectivenessofeachmoduleinrasgforimprovingreader awaresummarygeneration futureworkinvolvesextendingourmodeltoimprovedadversarialtrainingskillslikewassersteingan weplantopursueanoveltextmatchingmethodtodenoisingmoduleforimprovingtheaccuracy acknowledgments like thank anonymous reviewers constructive comments like thank jun zhang sicong jiang helps project work supported national key research development program china national science foundation china nsfc alibaba innovative research air fund rui yan sponsored ccf tencent open research fund microsoft research asia msra laborative research program references abadi m barham p chen j chen z davis dean j devin m ghemawat s irving g isard m et al tensorow system large scale machine ing osdi volume bahdanau d cho k bengio y neural chine translation jointly learning align translate iclr bansal m chen y fast abstractive rization reinforce selected sentence rewriting acl chen x gao s tao c song y zhao d yan r iterative document representation learning summarization polishing emnlp chopra s auli m rush m abstractive sentence summarization attentive recurrent neural works hlt naacl duchi j c hazan e singer y adaptive subgradient methods online learning stochastic mization jmlr gao s ren z zhao y e zhao d yin d yan r product aware answer generation e commerce question answering wsdm gu j lu z li h li v o k ing copying mechanism sequence sequence learning corr guo j lu s cai h zhang w yu y wang j long text generation adversarial training leaked information corr hu b chen q zhu f lcsts large scale chinese short text summarization dataset emnlp hu m sun lim e comments oriented blog summarization sentence extraction cikm hu m sun lim e comments oriented document summarization understanding documents readers feedback sigir jadhav rajan v extractive summarization swap net sentences words alternating pointer networks acl li p bing l lam w li h liao y aware multi document summarization sparse coding ijcai li p wang z lam w ren z bing l salience estimation variational auto encoders document summarization aaai li p bing l lam w reader aware document summarization enhanced model rst dataset proceedings workshop new frontiers summarization lin j sun x ma s su q global encoding abstractive summarization acl lin c rouge package automatic evaluation summaries text summarization branches ma s sun x lin j ren x hierarchical end end model jointly improving text summarization sentiment classication ijcai ma s sun x lin j wang h coder assistant supervisor improving text representation chinese social media text summarization acl mihalcea r tarau p textrank bringing order text emnlp nallapati r zhai f zhou b summarunner recurrent neural network based sequence model tive summarization documents aaai narayan s cohen s b lapata m ranking sentences extractive summarization reinforcement learning naacl hlt nguyen m tran c tran d nguyen m l solscsum linked sentence comment dataset social context summarization cikm liu p j manning c d point summarization pointer generator networks arxiv preprint sun m hsu w t lin c lee m min k tang j unied model extractive abstractive summarization inconsistency loss acl sutskever vinyals o le q v sequence sequence learning neural networks nips tao c gao s shang m wu w c zhao d yan r point utterance learning effective responses multi head attention mechanism ijcai vinyals o fortunato m jaitly n pointer networks nips wang l yao j tao y zhong l liu w du q reinforced topic aware convolutional sequence sequence model abstractive text summarization cai yang z cai k tang j zhang l su z li j social context summarization sigir zhou q yang n wei f zhou m selective encoding abstractive sentence summarization acl zhou q yang n wei f zhou m sequential copying networks aaai
