stepwise extractive summarization planning structured transformers shashi narayan joshua maynez shashinarayan joshuahm com jakub adamek daniele pighin blaz bratanic biondo blazb com ryan mcdonald google research abstract propose encoder centric stepwise els extractive summarization tured transformers hibert zhang extended transformers ainslie enable stepwise rization injecting previously generated summary structured transformer auxiliary sub structure models efcient modeling structure long inputs rely task specic redundancy aware modeling making general purpose extractive tent planner different tasks ated cnn dailymail extractive tion stepwise models achieve state art performance terms rouge redundancy aware modeling sentence ing holds true rotowire text generation models surpass previously reported metrics content tion planning ordering highlighting strength stepwise modeling structured transformers test stepwise extended transformers provides best formance datasets sets new standard challenges introduction extractive document summarization task creating summary identifying sequently concatenating important tences document erkan radev nenkova mckeown recent years task matured signicantly thanks advances deep neural networks cheng lapata conceptualize extractive tion sequence labeling task rst archical long short term memory network lstm equal contribution code data available com google research google tree master etcsum hochreiter schmidhuber code document lstm predict sentence included summary architecture later adopted nallapati nallapati narayan zhang dong following success pre trained based architectures tasks vaswani devlin current state art approach extractive summarization uses formers learn sentence representations rank sentences saliency liu liu lapata zhang zhong scoring sentences assembled produce extract ument summaries built fashion cheng lapata narayan zhang dong prone contain dant information recent approaches explored mechanisms better handle redundancy heuristic based trigram blocking triblk liu lapata wang crafted feature driven models ren redundancy aware neural sequence models zhou common lem models focus limited content overlap respecting length gets small subset dimensions necessary produce informative coherent summaries ideally models utilize enriched document summary representations order implicitly learn better extractive plans producing summaries liu mendes method stepwise marization liu summary constructed incrementally choosing new content conditioned previously planned content paper propose encoder centric wise models extractive summarization structured transformers structured transformers transformer based architectures exibility model form structure input hierarchical document structure paper specically study architectures hibert zhang extended formers construction etc ainslie details given sections enable stepwise summarization injecting previously planned summary content tured transformer auxiliary sub structure model holistically learn level coherence properties saliency dancy ordering embodied gold maries differs methods task specic redundancy aware ing holistic manually curated features liu added vantage structured encoders break quadratic attention mechanism transformers vlin making efcient able process longer inputs instead truncating inputs tokens liu lapata critical long inputs outputs require non trivial planning evaluated cnn dailymail summarization dataset hermann achieve art performance terms rouge lin hovy redundancy zhou sentence selection nisms liu lapata model task agnostic approach allows implicitly learn leverage content plans directly data structured transformers form basis model exible terms content type text tables modeled demonstrate learning intricate extractive content plan rotowire table text generation task wiseman task requires generation long summaries large score tables detailing specics sports match necessitates dedicated content selection planning models ate high quality summary wiseman puduppully wise framework achieves higher content selection planning ordering scores relative prior work task specic planning mechanisms contributions paper follows rst study use etc ainslie summarization ability exibility better model long structured inputs pose augmentions structured transformers hibert etc order enable stepwise els extractive planning demonstrate pirically models general purpose adapted extractive document rizer content planner table text tion experiments highlight effectiveness stepwise modeling specically stepwise etc sets new standard tasks related work redundancy summarization models use dedicated sentence selection step sentence scoring address redundancy maximal marginal relevance carbonell goldstein based methods select content maximal score minimally redundant ously constructed partial summary treated sentence selection optimization problem der constraints summary length donald lin bilmes liu pata wang based trigram blocking triblk redundancy elimination ren trained ral networks handcrafted features rank sentences model redundancy sentence selection zhou proposed redundancy aware models modeling redundancy saliency jointly scoring process ing neural sequence models contrast proaches models redundancy aware stead implicitly model redundancy ing previously generated summary representations virtue models text specic applied tasks section partial summary representations ultilizing representations partially generated summaries relatively studied summarization mendes proposed dynamically model generated summary lstm iteratively increment summaries based previously tracted information liu forward neural network driven hand curated tures capturing prevalence domain subtopics source summary best knowledge models rst use mary representations structured transformers summarization models learn summary informed sentence predictions hand curated features long form summarization known better content selection benets abstractive summarizers generate summaries uent informative gehrmann hsu xiao particularly important generating long stractive summaries liu liu pata summarizing multiple documents yasunaga earlier multi document summarization methods addressed issue long form input graph based representations sentences passages erkan radev christensen recently yasunaga proposed neural version work graph convolutional networks kipf welling liu lapata cross document attention mechanism share formation opposed simply concatenating text spans hierarchical transformers similar motivation explore better encoding long inputs structured transformers table text content planning wiseman introduced rotowire dataset requires multi sentence summaries large tables works found key generate uent informative summaries task dedicated content planning realization steps puduppully miculicich miculicich gong transformer encoder gong multi dimensional hierarchical lstm encoders compute better table entry representations following lines work evaluate models generate long content plans task structured transformers problem stepwise content extraction dene general paradigm stepwise content extraction easily tailored tractive summarization table text generation given input tent units goal learn extractive content plan length unit denoting end plan formulate tive ranking problem liu step given input previously selected plan select probability model parameters selected content added construct best plan dened arg extractive document summarization let document sentences goal learn extractive plan summary case best summarizes table text generation represent table records aim erate plan text generator generate meaningful coherent summary exposition use extractive document summarization setup introduce stepwise models hibert zhang etc ainslie following sections specically use sentence content unit previously partially generated summary previously selected content plan stepwise hibert hierarchical encodings model structure lstms nallapati cheng lapata narayan zhang proposed hibert stacked transformer encoders vaswani extractive summarization middle diagram figure sentence encoder pendently builds representations sentence document document encoder erates sentence encodings build contextual representations sentences contextual sentence representations ingested sier predict salience score sentence document standard transformers encoders multiple layers layer posed multi head self attention layer followed feed forward sub layer residual tions layer normalizations stepwise hibert time step modify document encoder content plan previously selected sentences summary depicted ure left allows model implicitly select new sentences relative previously generated summary sentence document encoders let document token rst mapped ous space esi ptoken token sentence ptoken figure memory usage attentions standard transformers devlin hibert zhang etc ainslie positional embeddings token tively transformer based sentence encoder transforms esi list hidden tations hidden representation following standard tice devlin liu lapata rst hidden representation representation sentence zhang use standard transformer document encoder takes document sentation psent representation sentence encoder positional bedding sentence document tively builds contextual sentence tions psent let stepwise modeling step partial summary previously extracted sentences tion document encoder takes mary representation psum representation sentence encoder sentence psum positional embedding sentence layer document encoder employs levels nested multi headed attentions vaswani build summary informed contextual sentence representations ument self attention summary self attention document summary attention figure left rst operate parallel followed document summary attention document self attention learns textual hidden representation hdocdoc sentence document summary attention learns contextual hidden tion hsumsum sentence share parameters document summary attention layers document summary attention builds contextual hidden representation hdocsum ing linear projections hdocdoc hsumsum sentence document query key values vaswani addition introduction stepwise anism hibert positional embeddings ptoken shared better model individual sentences document different styles summary zhang shared token ptoken sitional embeddings use lute position encodings original bert model devlin sentence psent psum stepwise etcsum growing interest addressing limitation transformer architecture bert devlin memory age scales quadratically size guo dai child rae beltagy roy hibert alleviates problem modeling sentence dently memory usage hibert scales square number sentences square maximum length sentence main disadvantage approach token level attention sentences hibited long range attention happens rectly second stage encoder middle diagram figure recently extended construction etc ainslie provides alternative alleviates quadratic memory growth introducing sparsity tion mechanism novel global local attention mechanism rightmost diagram figure permits encoding long enables mechanism model structure directly nodes global attention layer recent architectures yang kitaev nodesinput tokensinput tokensinput tokenslayer tokensblockembeddingsblocksinput tokensspecialglobal token tokenslayer transformer hierarchical attention transformer star transformer compressive transformer long inputglobal inputfull attentionfull attention global local attention etc nodesinput tokensinput tokensinput tokenslayer tokensblockembeddingsblocksinput tokensspecialglobal token tokenslayer transformer hierarchical attention transformer star transformer compressive transformer long inputglobal inputfull attentionfull attention global local attention etc figure stepwise hibert left etcsum right models hibert builds summary informed representation jointly modeling partially generated summary document document encoding etcsum takes input document appended partially generated summary global local attention etc model tecture receives inputs long input cases corresponds text encoded auxiliary global input serves ductive bias features model builds attention map called long long long input sparse local attention xed length bypasses quadratic memory complexity allows scale input lengths thousands tokens limits attention span tokens nearest neighbors overcome limitation global local tention denes attention parts global global long long global unrestricted attention allows tokens trarily far apart attend hop global input tokens refer reader ainslie details right parts figures illustrate types attentions sparsity diagrams cell row column ent white input token attend input token relative position embeddings indicated color stepwise modeling given document partial summary step construct input concatenating document partial summary etc replaces absolute position encodings relative position encodings shaw easily adapt greater input lengths seen pretraining addition ing relative positions input sequence relative position encodings etc model arbitrary pairwise token relations useful tured inputs auxiliary global input represent sentence structure specically following ainslie placed auxiliary token global input sentence input linked global tokens input tokens relative position labels represent token belongs sentence global global attention left unrestricted allowing sentences attend result summary informed contextualized input token resentations attention global nodes rest paper refer rizer stepwise etcsum similar hibert rst token hidden representation representation sentence finally tence embeddings passed softmax layer salience scoring hibert etcsum trained cross entropy loss extractive document summarization experimental setup dataset evaluate models cnn dailymail news highlights datasets mann standard splits documents training validation testing anonymize tities lower case tokens narayan zhou zhang liu lapata documents cnn dailymail dataset long average lengths words sentences cnn words sentences dailymail human written abstracts words cnn dailymail respectively ated summarization quality rouge baselines compared stepwise hibert etcsum models lead oracle baselines lead selects rst sentences form mary oracle baselines creates summary selecting best possible set sentences document gives highest average rouge scores respect human written summary oracle cates input document tokens compared models aware models neusum zhou aredsum models uses trigram blocking triblk liu lapata redundancy elimination sentence tion second block table understand importance modeling long documents extractive summarization trained bertsum similar liu lapata receptive capacity tokens initialized bert checkpoint sum differs slightly liu lapata use segment embeddings report roberta liu initialized version bertsum trained non stepwise variants ert etcsum models block ble setting hibert etc partial summaries input instead simply input document generate salient scores sigmoid layer sentence document sentences bled generate summary implementation hibert differs zhang example pretrain hibert scratch document modeling zhang instead initialize hibert models publicly available roberta liu checkpoints following superior performance models lead oracle oracle latent zhang refresh narayan banditsum dong neusum zhou exconsum mendes jecs durrett zhong luo hibert zhang pnbert zhong bertsum liu lapata aredsum ctx hsg wang bertsum large bertsum robertasum hibert etcsum non stepwise models stepwise models stepwise robertasum stepwise stepwise hibert stepwise stepwise etcsum stepwise table rouge scores cnn dailymail test set boldfaced numbers best results comparable models bertsum large builds bertlarge layers architectures build bertbase layers architectures robertasum bertsum use different number layers document encoder ldoc sentence encoder lsent opposed equal number layers encoders zhang layers document sentence encoders initialized layers roberta respectively etcsum models ized uncased version etc pretrained checkpoints ainslie pretrained standard masked language model task contrastive predictive coding van den oord report effect triblk models experiment sized models layers den size lter size attention lowercased candidate reference summaries pyrouge parameters thank authors ainslie sharing etc checkpoints human etcsum stepwise etcsum stepwise summary length figure length distributions etcsum summaries cnn dailymail test set heads comparison report results bertsum large liu lapata uses layers finally employ beam coding predict summaries stepwise models use beam size maximum steps allow repeated sentences requirement refer reader supplementary material implementation reproducibility details generating extractive oracles following narayan train models predict sentences oracle non stepwise stepwise training learns training gradually step train model predict sentence oracle earlier predicted sentences document testing human written abstracts reference summaries evaluate models results long form summarization experiments etcsum appears far superior ert modeling long documents extractive summarization etcsum outperformed hibert cases including stepwise non stepwise dictions trigram blocking downside hibert token level tion sentences possible mal modeling documents etcsum performed better bertsum respectively results suggest importance modeling ument etcsum truncating tokens bertsum improvement attributed solely sum ability model long inputs better initialization etc checkpoints ainslie specially improvement minishes compared robertasum stepwise non stepwise models trigram ltering key ing redundancy generated summaries stepwise models helps models cluding hibert etcsum single case robertasum estingly observe pattern stepwise models observe wise models hibert etcsum triblk consistently improve stepwise counterparts stepwise plied triblk ments conjecture stepwise models inherently better avoiding dancy generated summaries edge previously generated summary diction step improvements triblk complementary strated figure density curves wise etcsum follows human distribution better etcsum stepwise signicant improvement stepwise sum report stepwise robertasum lines performance dropped compared sponding non stepwise models structure transformer simple summary concatenation good method stepwise robertasum distinguish document summary better ways vanilla concatenation stepwise etcsum hibert natural stepwise tasum loses access end input partial summary grows documents close tokens length finally stepwise etcsum model explicit redundancy sentence selection mechanisms achieved comparable performance state art cnn dailymail consider access modeling long puts etcsum truncated inputs bertsum robertasum initializing etcsum bert roberta checkpoints etc checkpoint fair etcsum bert roberta uses absolute position embeddings devlin etc uses relative position embeddings shaw models wiseman puduppully hierenc gong puduppully miculicich end end miculicich systran detok gong nle saleh hierarchical rebuffel stepwise hibert realized stepwise hibert planning stepwise etcsum realized stepwise etcsum planning bleu table standard metrics rotowire relation generation precision content selection precision recall content ordering complement normalized damerau levenshtein distance bleu score models marked directly comparable boldfaced numbers best results comparable models extractive summarization task smaller model bertsum large liu lapata parameters achieved scores eters achieved comparatively stepwise hibert equally ument summarization sequential nature input demonstrate section suited extractive content planner table text generation rouge scores table computed condence interval wise signicantly better variants hierbert etcsum stepwise models condence interval deciding factor performed way anova posthoc tukey hsd tests best model stepwise etcsum performs icantly better stepwise average rouge scores table text generation task explore model ability learn content plans rotowire data text generation task wiseman task generate summary nba game box score table statistics detailing mance teams player dataset consists pairs box scores summaries nba games played data split train tion test examples average rotowire dataset available download https com harvardnlp boxscore data records box score game average summary words sentences similar puduppully pose problem sub problems solve independently content planning sists selecting records table mentioned summary order organized sentences realization uses content plan create human readable summary refer reader supplementary material example main focus paper demonstrate els ability model long structured rotowire input tables generate long meaningful content plans realization simply use roberta liu initialized sequence sequence transformer model rothe trained emit realization sentence sentence train stepwise models score table partially generated content plan predict element content plan entries score table sentence break token marking end plan unlike extractive summarization optimal extractive content plan repeated entries input table team names better preserve generate discourse relations sentences target summary pully making challenging task iterative models prohibit redundancy details model implementation realization induction oracle content plans training refer reader supplementary material report typical rotowire metrics wiseman standard information tion system described puduppully extract box score table relations mentioned generated target summary metrics measure text quality bleu score relation generation quality precision relations extracted box score table content selection quality precision recall relations extracted extracted content dering quality complement normalized damerau levenshtein distance sequences relations extracted ducted human evaluation rotowire summaries results focus evaluating stepwise hibert etcsum models results presented table realized scores assess quality realized summaries parable systems rst block table found stepwise hibert stepwise sum content selection particularly high precision scores respectively combined good recall respectively outperform puduppully recent models score terms content ordering bleu score wise hibert bleu dld forms worse puduppully bleu dld stepwise etcsum performs signicantly better bleu dld possible higher bleu score achieved improving simple sentence sentence realization method report content selection scores output content planning modules ning models table drop city date entries content plans puting metrics order ble table roundtrip realization subsequent information extraction decreases quality slightly models absolute drop score stepwise hibert stepwise etcsum human evaluation participants shown summaries nba game asked pare respect informativeness summary present better selection reproduce bertsum robertasum lines reasons sequential models optimal tabular data bounded input length tokens average length linearized score tables tokens game report non stepwise models suitable generate ordered content plans required task informativeness readability models baseline stepwise hibert truncated stepwise etcsum truncated gold table human evaluation rotowire summaries vant facts game readability summary better narrative ier read randomly selected nba bles evaluated summaries baseline man stepwise hibert stepwise etc gold number sentences baseline stepwise hibert stepwise etc gold respectively included truncated summaries stepwise bert stepwise etc match number sentences corresponding gold summaries elicited judgements different tors pair report scaling scores louviere woodworth louviere results presented table overall stepwise etc summaries ranked informative performed worst readability shelf sentence level izer supplementary material favors statistics dense sentences baseline maries tends hallucinate dense plans future work aim address itation infromativeness stepwise etc maries signicantly better gold stepwise etc truncated stepwise hibert truncated summaries stepwise hibert summaries nicantly better truncated variants differences signicant readability baseline summaries signicantly better etc variants stepwise ert differences signicant conclusion stepwise structured transformer paradigm emplied hibert etcsum easily adapted extractive document tion content planning table text generation stepwise etcsum particular sets new dard tasks future work focus extending models generate extractive plans better abstractive summarization long tiple documents liu acknowledgments thank joshua ainslie santiago ontanon sharing etc code checkpoints giving feedback early draft paper thank annie louis london zurich generation teams reviewers action editor invaluable feedback thank enrique alfonseca hadar shemtov support longform summarization references joshua ainslie santiago ontanon chris alberti philip pham anirudh reddy ravula sumit sanghai etc encoding long structured data proceedings transformers ence empirical methods natural language processing online jimmy jamie ryan kiros geoffrey hinton layer normalization corr beltagy matthew peters arman cohan longformer long document transformer corr keping rahul jha bruce croft asli likyilmaz aredsum adaptive aware iterative sentence ranking extractive ment summarization corr jaime carbonell jade goldstein use mmr diversity based reranking reordering ments producing summaries proceedings annual international acm sigir ence research development information retrieval pages new york usa jianpeng cheng mirella lapata neural marization extracting sentences words proceedings annual meeting sociation computational linguistics pages berlin germany rewon child scott gray alec radford ilya sutskever generating long sequences sparse transformers corr janara christensen mausam stephen soderland coherent oren etzioni proceedings document summarization conference north american chapter association computational linguistics man language technologies pages lanta georgia zihang dai zhilin yang yiming yang jaime bonell quoc ruslan salakhutdinov transformer attentive language models proceedings xed length context annual meeting association tional linguistics pages florence italy jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language proceedings conference standing north american chapter association computational linguistics human language technologies pages minneapolis nesota yue dong yikang shen eric crawford herke van hoof jackie chi kit cheung sum extractive summarization contextual dit proceedings conference pirical methods natural language processing pages brussels belgium gunes erkan dragomir radev lexrank graph based lexical centrality salience text journal articial intelligence summarization search sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings conference cal methods natural language processing pages brussels belgium heng gong xiaocheng feng bing qin ting liu table text generation effective archical encoder dimensions row column time proceedings conference empirical methods natural language cessing international joint conference natural language processing pages hong kong china gong josep crego jean senellart tran wngt dgt task ings workshop neural generation translation pages hong kong qipeng guo xipeng qiu pengfei liu yunfan shao xiangyang xue zheng zhang proceedings transformer ence north american chapter ation computational linguistics human guage technologies pages lis minnesota kaiming xiangyu zhang shaoqing ren jian sun deep residual learning image nition corr karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read advances neural comprehend tion processing systems pages ran associates inc sepp hochreiter jurgen schmidhuber neural computation long short term memory wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun unied model extractive abstractive summarization inconsistency loss corr jordan louviere george woodworth best worst scaling model largest ence judgments university alberta working thomas kipf max welling supervised classication graph convolutional networks corr nikita kitaev lukasz kaiser anselm levskaya reformer efcient transformer corr chin yew lin eduard hovy automatic uation summaries gram occurrence statistics proceedings human guage technology conference north chapter association computational linguistics pages hui lin jeff bilmes class lar functions document summarization ceedings annual meeting ation computational linguistics human guage technologies pages portland gon usa jingyun liu jackie chi kit cheung annie louis comes extractive corr marization sentence prediction peter liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser noam shazeer generating wikipedia ing long sequences proceedings national conference learning representations vancouver canada yang liu fine tune bert extractive marization corr yang liu mirella lapata hierarchical transformers multi document summarization proceedings annual meeting ciation computational linguistics pages florence italy yang liu mirella lapata text tion pretrained encoders proceedings conference empirical methods natural language processing international joint conference natural language processing pages hong kong china yinhan liu myle ott naman goyal jingfei dar joshi danqi chen omer levy mike lewis luke zettlemoyer veselin stoyanov roberta robustly optimized bert pretraining approach corr jordan louviere terry flynn anthony fred john marley best worst scaling ory methods applications cambridge sity press ling luo xiang yan song feiyang pan min yang qing reading like man reading inspired extractive summarization proceedings conference empirical methods natural language processing international joint conference natural guage processing pages hong kong china ryan mcdonald study global inference gorithms multi document summarization ceedings european conference search page berlin heidelberg springer verlag afonso mendes shashi narayan sebastiao miranda zita marinho andre martins shay hen jointly extracting compressing uments summary state representations ceedings conference north ican chapter association computational linguistics human language technologies pages minneapolis minnesota lesly miculicich marc marone hany hassan selecting planning rewriting ular approach data document generation translation proceedings workshop neural generation translation pages hong kong ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization proceedings thirty aaai uments conference articial intelligence pages ramesh nallapati bowen zhou mingbo classify select neural architectures corr extractive document summarization ramesh nallapati bowen zhou cicero dos santos caglar gulcehre bing xiang tive text summarization sequence sequence proceedings rnns signll conference computational natural guage learning pages berlin germany shashi narayan ronald cardenas nikos topoulos shay cohen mirella lapata sheng chang document eling external attention sentence extraction proceedings annual meeting association computational linguistics pages melbourne australia shashi narayan shay cohen mirella lapata ranking sentences extractive tion reinforcement learning proceedings conference north american ter association computational linguistics human language technologies pages new orleans louisiana association tional linguistics ani nenkova kathleen mckeown matic summarization foundations trends information retrieval aaron van den oord yazhe oriol vinyals representation learning contrastive dictive coding corr ratish puduppully dong mirella lapata data text generation content tion planning proceedings aaai ference articial intelligence volume pages ratish puduppully dong mirella lapata data text generation entity proceedings annual meeting ing association computational linguistics pages florence italy ratish puduppully jonathan mallinson mirella lapata university edinburgh sion document level generation proceedings tion shared task shop neural generation translation pages hong kong jack rae anna potapenko siddhant jayakumar timothy lillicrap compressive formers long range sequence modelling corr laure clement rebuffel soulier geoffrey scoutheeten patrick gallinari hierarchical model data text generation advances information retrieval pages cham springer international publishing pengjie ren zhumin chen zhaochun ren furu wei jun maarten rijke leveraging contextual sentence relations extractive rization neural attention model ings international acm sigir ence research development information retrieval new york usa sascha rothe shashi narayan aliaksei leveraging pre trained checkpoints eryn transactions sequence generation tasks association computational linguistics aurko roy mohammad taghi saffar ashish vaswani david grangier efcient content based sparse attention routing transformers corr fahimeh saleh alexandre berard ioan calapodescu laurent besacier naver labs europe systems document level generation proceedings lation task wngt workshop neural generation tion pages hong kong peter shaw jakob uszkoreit ashish vaswani self attention relative position proceedings conference tations north american chapter association computational linguistics human language nologies pages new orleans louisiana ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez lukasz kaiser illia polosukhin attention need advances neural information cessing systems pages curran ciates inc danqing wang pengfei liu yining zheng xipeng qiu xuanjing huang heterogeneous graph neural networks extractive document marization proceedings annual ing association computational linguistics pages online sam wiseman stuart shieber alexander rush challenges data document generation proceedings conference cal methods natural language processing pages copenhagen denmark liqiang xiao wang hao yaohui jin copy rewrite hybrid summarization chical reinforcement learning proceedings thirty fourth aaai conference articial gence jiacheng greg durrett neural extractive text summarization syntactic compression proceedings conference empirical methods natural language processing international joint conference natural guage processing pages hong kong china zhilin yang zihang dai yiming yang jaime bonell ruslan salakhutdinov quoc xlnet generalized autoregressive pretraining language understanding corr michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan dragomir radev graph based neural multi document summarization proceedings ence computational natural language learning pages vancouver canada zihao qipeng guo quan gan xipeng qiu zheng zhang transformer modelling long range context binary partitioning corr xingxing zhang mirella lapata furu wei ming zhou neural latent extractive document marization proceedings conference empirical methods natural language ing pages brussels belgium xingxing zhang furu wei ming zhou bert document level pre training hierarchical bidirectional transformers document proceedings annual meeting tion association computational linguistics pages florence italy ming zhong pengfei liu danqing wang xipeng qiu xuanjing huang searching tive neural extractive summarization works proceedings annual meeting association computational guistics pages florence italy ming zhong danqing wang pengfei liu xipeng qiu xuanjing huang closer look data bias neural extractive summarization models proceedings workshop new frontiers summarization pages hong kong china qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural ment summarization jointly learning score select sentences proceedings annual meeting association computational guistics pages melbourne australia implementation reproducibility details hibert wide range hyperparameter search hibert experimented number layers document encoder ldoc number layers sentence encoder lsent ldoc lsent initialization sharing position embeddings ptoken psum initialization sharing document sentence encoder parameters bert roberta checkpoints representation sentence token embedding average token embeddings sentence encoder extractive summarization hibert transformer layer sentence encoder transformer layer document encoder model parameters word position embedding sentence encoder initialized ing roberta checkpoint document summary sentence position embeddings learned scratch document self attention mary self attentions shared initialized roberta checkpoint document summary attention initialized roberta checkpoint truncate document tences sentence words trained hibert models steps saving points steps batch size following liu lapata choose best model based mle loss validation set rotowire use hibert layer sentence encoder layer document encoder model trainable parameters use document sentence position embeddings towire input consists set entries table use summary sentence position embedding capture order content plan use roberta vocabulary discussed use roberta pretraining instead initializing random weights trained model batch size auc score predicting content plan entry validation dataset attened came steps dataset examples element target content plan rotowire example model saw entire dataset approximately times hibert models cloud tpu accelerators training adam optimizer learning rate etcsum etcsum model extractive rization table text generation uses layer transformer described ainslie model pretrained mlm cpc tives described ainslie total model trainable parameters comes long input tokens attention global tokens trained model batch size steps approximately equivalent epochs cloud tpu accelerators training inference taking hours predictions test set model selection models performance validation set models stepwise models subset dation set instead consisting rst examples given longer inference times wide range hyperparameter search experimented learning rate relative position coding vocabulary size representation sentences token embedding average token embeddings sentence coder additionally non stepwise models experimented positive label weight loss calculation finally adam optimizer learning rate realization model use robertashare model following rothe model trainable parameters trained model reached maximum bleu score tion data trained model batch size steps dataset amples element target content plan rotowire example model saw entire dataset approximately times cloud tpu accelerators training adam optimizer learning rate table text generation task table shows prototypical input table rotowire possible content plan realization shown example formed content plan repeat entries input table generating oracle content plans rotowire dataset contain ground truth content plans summaries instead infer following similar approach puduppully minor modications use single convolutional model stead ensemble convolutional models lstms plans maintain sentence order information include repetitions piece information repeated sentence target summary plans include tence breaks remove sentences table entries content plans include match date mentioned text saturday resolve pronoun emit corresponding player team tent plan respect table realization reference summary applying process obtain content plan shown middle table average plans inferred fashion table entries sentences presenting actual example legal reasons content planning technical details hibert conceptually input hibert sequence strings use special strings beg eos eot explicitly mark beginning content plan end tence end plan text respectively strings values table points order appear text practice attempt leverage roberta pre training place value strings natural language sentences generate value plates listed table numeric values number points team player larly puduppully compute rank value instances table entry type include templated sentence form nth best respect example table value points represented natural language sentence team points scored chicago bulls best observe signicant benet terms auc predicting content plan entry validation data eventually ized model random weights retained natural language representation value strings hibert sentence limit step discarding table entries likely mentioned summary player entries valued entries valued needed table entries naturally ordered feed positional embedding psent document encoder feed summary encoder given table entries partial summary bert computes distribution input tences eos corresponds emitting sentence break eot corresponds ending content plan beg sample content plans trained model greedy decoding modication entries allowed repeat content plan sentence breaks team names team cities highest probability sentence repeat instead emit second highest use words nth best high number logically detrimental team represents losses input table match date saturday october team chicago bulls lakers player michael jordan shaquille neal bulls lakers michael shaquille city chicago los angeles surname jordan oneal home home away team chicago bulls lakers wins points losses points rebounds assists rebounds content plan chicago bulls city chicago bulls lakers city lakers chicago bulls points lakers points match date eos lakers lakers wins lakers losses shaquille neal surname shaquille neal points eos chicago bulls city chicago bulls chicago bulls wins chicago bulls losses michael jordan michael jordan surname michael jordan points michael jordan rebounds eos chicago bulls won los angeles lakers saturday poor showing lakers spite oneal point contribution chicago bulls best player predictably michael jordan points rebounds realization table hypothetical example rotowire dataset nba game possible sentence content plan sponding realized sentences etcsum etc models ltered set table entries hibert input catenated entries input sequence similarly special strings eos eot beg correspond cepts hibert end sentence end text beginning text respectively special strings appended beginning input sequence partial summary input constructed catenating special string beg entries predicted far order tion eos indicating sentences breaks input sequence constructed concatenating cls delimiter input sequence special separator sep partial summary nally separator sep input sequence partial summary padded respectively adding total strings input including special delimiters model uses additional inputs construct global local attention global token assigned segment input special limiter gets assigned global token ery sentence input partial summary model maximum global token taken account examples number segments input sequence sentences special delimiters partial inputs larger examples assign global tokens tail input sequence consistent use decoding egy sample content plans greedily repeated entries allowed content plan sentence breaks team names team cities rotowire realization model generated content plans realized sequence sequence transformer model ized roberta liu following rothe trained emit realization sentence sentence input model concatenation following text previous sentence string rst sentence model use pronominalize team player names introduced literal string beg separator templated realizations cfr table entries sentence content plan space arated literal string context rator templated representation match date teams templated representations team team city pts team wins team losses team playing home away space separated player sentence content plan templated representations start position team player space separated input context tor provided noticed content plan provide necessary information realizing sentence ple target text refer player template match date match year yyyy month day day week team team city team quarter points team quarter points team quarter points team quarter points team free throw percentage team points scored team assists team losses team wins team rebounds team turnovers team point eld goal percentage team eld goal percentage table entry type match date team team city team pts team pts team pts team pts team pct team pts team ast team losses team wins team reb team tov team pct team pct team playing home away home away team match player rst player second player pts player fgm player fga player min player player player stl player ftm player fta player blk player ast player player player reb player start position player oreb player dreb player pct player pct player pct team player belongs player rst player second player points scored player eld goals player eld goals attempted player minutes played player point eld goals player point eld goals attempted player steals player free throws player free throws attempted player blocks player assists player turnovers player fouls player rebounds player starting position player offensive rebounds player defensive rebounds player eld goals percentage player point eld goals percentage player free throws percentage player table templates use create textual representations table entries templates yyyy mmm placeholders encodes day week monday sunday team player value team player given table entry dataset names table entry column correspond names properties rotowire dataset possible models stepwise hibert realized stepwise hibert planning stepwise etcsum realized stepwise etcsum planning bleu table standard metrics rotowire validation data starting position team mation provided realizer create training data rotowire maries inferred content plans ting sentences inferred content plans realize content plans gressively feeding sentence produced previous step input step validation data performance report performance best models rotowire validation data table
