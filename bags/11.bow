noisy channel model document compression hal daume iii daniel marcu information sciences institute university southern california admiralty way suite marina del rey hdaume edu abstract present document compression tem uses hierarchical noisy channel model text production sion system rst automatically derives syntactic structure sentence overall discourse structure text given input system uses tistical hierarchical model text tion order drop non important tactic discourse constituents coherent grammatical document compressions arbitrary length tem outperforms baseline sentence based compression system operates simplifying sequentially sentences text results support claim discourse knowledge plays important role document tion introduction single document summarization systems proposed date fall following classes extractive summarizers simply select present user important sentences text mani maybury marcu mani comprehensive overviews methods algorithms accomplish headline generators noisy channel tic systems trained large corpora hheadline texti pairs banko berger mittal systems duce short sequences words tive content text given input sentence simplication systems chandrasekar mahesh carroll grefenstette jing knight marcu capable compressing long sentences deleting unimportant words phrases extraction based summarizers produce puts contain non important sentence fragments example hypothetical extractive summary text shown table pacted deleting clause ready win headline based maries shown table usually indicative text content informative grammatical coherent repeatedly applying sentence simplication algorithm sentence time compress text outputs erated way likely incoherent contain unimportant information rizing text sentences dropped gether ideally like build systems strengths classes approaches document compression entry table shows grammatical coherent summary text generated hypothetical document compression system preserves tant information text deleting sentences phrases words subsidiary main message text obviously generating ent grammatical summaries produced hypothetical document compression system table trivial conicting type summarizer extractive summarizer headline generator sentence simplier document compressor hypothetical output output contains important info output coherent output grammatical john doe secured vote democrats constituency win support governer shaky ground mayor vote constituency governer mayor looking election john doe secured vote democrats constituency shaky ground john doe secured vote democrats shaky ground table hypothetical outputs generated types summarizers deletion certain sentences result incoherence information loss deletion certain words phrases lead maticality information loss mayor looking election john doe secured vote democrats constituency win support governer shaky grounds paper present document compression system uses hierarchical models discourse syntax order simultaneously manage conicting goals compression system rst automatically derives syntactic structure sentence overall discourse structure text given input system uses tistical hierarchical model text production der drop non important syntactic discourse units generate coherent grammatical ument compressions arbitrary length system outperforms baseline sentence based compression system operates simplifying quentially sentences text document compression document compression task conceptually simple given document wni goal produce new document dropping words order achieve goal number systems use outputs tive summarizers repair improve coherence duc duc unfortunately exible produce shot good summaries taneously coherent grammatical extent noisy channel model proposed knight marcu system compressed tences dropping syntactic constituents applied entire documents sentence basis discussed section adequate resulting summary contain compressed sentences vant order extend knight marcu approach sentence level need glue tences tree structure similar sentence level rhetorical structure theory rst mann thompson provides glue tree figure depicts rst structure text rst discourse structures binary trees leaves correspond elementary discourse units edus internal nodes correspond contiguous text spans internal node rst tree characterized ical relation example rst sentence text provides background information preting information sentences contrast relation figure lation holds adjacent non overlapping text spans called nucleus satellite exceptions rule relations list contrast multinuclear tinction nuclei satellites comes empirical observation nucleus expresses essential writer purpose satellite system able analyze discourse structure document syntactic structure sentences edus compresses document dropping syntactic course constituents documents containing incoherently juxtaposed sentences noisy channel model given document want summary text maximizes bayes rule end maximizing left modelling probability distributions probability document given summary probability summary assume given discourse structure document syntactic structures edus intuitive way thinking tion bayes rule reffered noisy channel model start summary add noise yielding longer document noise added model consists words phrases discourse units instance given document john doe secured vote democrats add words word ate john doe secured vote democrats choose add tire syntactic constituent instance prepositional phrase generate john doe secured vote democrats constituency examples sentence expansion ously knight marcu system ability pand core message adding discourse stituents instance decide add discourse constituent original summary john doe secured vote democrats contrasting information summary uncertainty support yielding text john doe secured vote democrats support governor shaky ground noisy channel application parts account build complete document compression system channel model source model decoder describe source model assigns string ity probability summary good english source model ideally disfavor ungrammatical sentences channel model assigns ment summary pair probability models extent good expansion instance mayor looking election mayor looking election secure vote democrats major looking election sharks sharp teeth expect higher expands elaboration shifts different topic yielding incoherent text decoder searches possible maries document summary maximizes posterior probability parts described source model job source model assign score compression independent original document source model measure good english summary independent good compression currently use bigram measure quality trigram scores tested failed difference combined non lexicalized context free tic probabilities context free discourse ities giving pdp better use ized context free grammar possible given decoder channel model channel model allowed add syntactic constituents stochastic operation called constituent expand discourse units stochastic operation called edu expand operations performed bined discourse syntax tree called tree tree text shown figure ence suppose start summary mayor looking election sat background npb mayor root nuc span nuc contrast nuc contrast vbz advp vpa vbg looking john doe secured vote democrats constituency nuc span sat evaluation sat condition nuc span win support governer shaky ground npb punc reelection figure discourse partial tree text expand operation insert syntactic stituent year tic tree constituent expand operation add single words instance word added looking yielding mayor looking election probability inserting word based syntactic structure node serted knight marcu describe detail noisy channel model explains short tences expanded longer ones inserting expanding syntactic constituents words constituent expand stochastic operation simply reimplements knight marcu model focus refer reader knight marcu details addition adding syntactic constituents system able add discourse units consider summary john doe secured vote democrats consituency sequence discourse expansions expand summary reach original text plete discourse expansion process occur starting initial summary generate original document shown figure gure follow sequence steps required generate original text ning summary eration project discourse crease depth tree adding intermediate nuc span node projection adds factor nuc span nuc nuc span ity sequence operations shown arrow able perform second operation expand expand core sage contained adding satellite uates information presented expansion adds probability performing expansion called discourse expansion probabilities pde example discourse expansion probability nuc span nuc span sat nuc span nuc span reects probability adding uation satellite nuclear span rest figure shows remaining steps produce original document step beled appropriate probability factors probability entire expansion uct listed probabilities combined appropriate probabilities syntax things order produce nal score document summary pair multiply expansion probabilities path ing estimating parameters discourse models rst corpus wall street journal articles penn treebank obtained ldc documents corpus range size words erage words document document paired discourse structure root nuc span john doe secured vote democrats constituency dproject nuc span span nuc span nuc span dexpand span nuc span sat evaluation nuc span nuc span root nuc span john doe secured vote democrats constituency root nuc span nuc span john doe secured vote democrats constituency sat evaluation win root nuc span dproject nuc contrast span nuc contrast nuc span nuc span john doe secured vote democrats constituency sat evaluation win root sat background nuc span root nuc span nuc span sat background mayor looking reelection root nuc span contrast nuc span nuc contrast contrast sat condiation nuc span nuc contrast nuc span dexpand dproject nuc contrast nuc contrast nuc contrast nuc contrast nuc contrast nuc span sat evaluation sat condition nuc span nuc span sat evaluation sat condition nuc span nuc span john doe secured vote democrats constituency win support governer shaky ground john doe secured vote democrats constituency win support governer shaky ground john doe secured vote democrats constituency sat evaluation win figure sequence discourse expansions text probability factors span nuc contrast nuc contrast nuc span nuc contrast root nuc span nuc contrast shaky ground ally built style rst carlson details concerning corpus notation process corpus able estimate parameters discourse pcfg standard maximum likelihood methods furthermore document corpus paired extractive summaries edu level human annotators asked edus important suppose example tree figure annotators marked second fth edus starred ones stars propagated discourse unit descendent considered important ered important annotations deduce compress nuc contrast children nuc span sat evaluation drop evaluation satellite similarly compress nuc contrast children sat condition nuc span dropping rst discourse constituent finally compress root deriving sat background nuc span dropping sat background constituent counts examples lected normalize discourse pansion probabilities decoder goal decoder combine vast number potential compressions large tree efciently pack shared forest structure described detail knight marcu entry shared forest structure associated probabilities source syntax pcfg source discourse pcfg expansion template probabilities described section generated forest representing possible compressions original document want extract best best trees taking account pansion probabilities channel model bigram syntax discourse pcfg ties source model thankfully generic extractor built langkilde purposes extractor selects trees best combination expansion scores performing exhaustive search ble summaries returns list trees possible length system system developed works pipelined ion shown figure rst step pipeline generate discourse structure use decision based discourse parser described marcu course structure send edu discourse parser achieves score edu identication identifying hierarchical spans nuclearity identication relation tagging input document discourse parser syntax parser forest generator decoder length chooser output summary figure pipeline system components tactic parser collins syntax trees edus merged discourse tree forest generator create tree similar shown figure tree ate forest subsumes possible compressions forest passed forest ranking system decoder langkilde decoder gives list possible compressions possible length example compressions text shown figure respective log probabilities order choose best compression possible length rely log probabilities lest system choose shortest possible compression order sate normalize length practice simply dividing log probability length compression insufcient longer documents experimentally found reasonable metric compression length divide log probability job length chooser figure enabled choose single compression document evaluation figure compression chosen length selector cized shortest results testing began sets data rst set drawn wall street journal wsj portion penn treebank consists uments containing words second set drawn collection tends case short documents compressions sufciently long length ization effect dent compositions consists documents containing words set mitre corpus hirschman liked run evaluations longer ments unfortunately forests generated relatively small documents huge exponential number summaries generated given decoder runs memory longer documents lected shorter subtexts original documents wsj mitre data uation wanted formance system varies text genre mitre data consists short sentences erage document length mitre sentences constrast typically long sentences wall street journal articles average document length wsj sentences purpose comparison mitre data compressed systems random drops random words word chance dropped baseline hand hand compressions human concat sentence compressed individually results concatenated knight marcu system parison edu system described paper sent syntactic parsers tend work parsing clauses system merges leaves discourse tree sentence proceeds scribed paper wall street journal data evaluated systems additions correct discourse trees known data thought wise test systems human built discourse trees instead tomatically derived ones additionall tems edu edu perfect discourse trees available rst corpus carlson theory text words possible compressions len log prob best compression mayor looking mayor looking win mayor looking support shaky ground mayor looking support governer shaky ground mayor looking election support governer shaky ground mayor looking win support governer shaky ground figure possible compressions text sent sent perfect discourse trees human evaluators rated systems according metrics rst presented evaluators grammaticality coherence presented separately summary ity grammaticality judgment good english compressions coherence included compression owed stance anaphors lacking antecedent lower coherence summary quality hand judgment compression tained meaning original document measure rated scale worst best draw conclusions uation results shown table age compression rate cmp length pressed document divided original length clear genre inuences results mitre data contained short tences syntax discourse parsers fewer errors allowed better compressions generated mitre corpus compressions tained starting discourse trees built sentence level better compressions tained starting discourse trees built edu level wsj corpus compression tained starting discourse trees built sentence level grammatical herent compressions obtained starting course trees built edu level choosing manner discourse syntactic sentations texts mixed inuenced genre texts interested compress run system mitre data perfect discourse trees hand built discourse trees corpus wsj mitre cmp grm coh qual cmp grm coh qual concat random edu sent edu sent hand table evaluation results compressions obtained starting fectly derived discourse trees indicate perfect discourse structures help greatly improving ence grammaticality generated summaries surprising summary quality affected negatively use perfect discourse structures statistically signicant believe happened text fragments summarized extracted longer documents likely discourse structures built specically short text snippets different nent designed handle cohesion pected compressions contain gling references overall systems outperformed random baseline concat systems empirically discourse important role document summarization performed tests results found wall street journal data differences score concat sent systems grammaticality coherence statistically signicant level difference score summary quality mitre data differences score concat sent systems cality summary quality statistically icant level difference score coherence score differences maticality coherence summary quality systems baselines statistically nicant level results table sessed inspecting compressions figure spite success far away human performance levels error system makes dropping ments dropped phrase election complement looking currently experimenting icalized models syntax prevent compression system dropping required verb guments consider methods scaling decoder handling documents realistic length acknoledgements work partially supported darpa ito grant nsf grant usc dean fellowship hal daume iii thanks kevin knight discussions related project references michele banko vibhu mittal michael witbrock headline generation based statistical lation proceedings annual meeting association computational linguistics acl pages hong kong october adam berger vibhu mittal query relevant summarization faqs proceedings annual meeting association tional linguistics pages hong kong october lynn carlson daniel marcu mary ellen okurowski building discourse tagged corpus framework rhetorical structure theory ceedings sigdial workshop discourse dialogue eurospeech aalborg denmark september john carroll guidon minnen yvonne canning siobhan devlin john tait practical simplication english newspaper text assist aphasic readers proceedings workshop integrating articial intelligence assistive technology chandrasekar christy doran srinivas bangalore motivations methods text proceedings sixteenth international tion conference computational linguistics coling copenhagen denmark michael collins generative lexicalized models statistical parsing proceedings annual meeting association tational linguistics pages madrid spain july proceedings document understanding ference new orleans september proceedings second document understanding conference philadelphia july gregory grefenstette producing intelligent graphic text reduction provide audio scanning working notes aaai service blind spring symposium intelligent text summarization pages stanford university march hirschman light breck burger deep read reading comprehension system ceedings annual meeting association computational linguistics jing sentence reduction automatic text summarization proceedings annual meeting north american chapter ciation computational linguistics pages seattle kevin knight daniel marcu statistics based summarization step sentence compression national conference articial gence pages austin july august irene langkilde forest based statistical sentence generation proceedings annual meeting north american chapter association computational linguistics seattle washington april kavi mahesh hypertext summary extraction fast document browsing proceedings aaai spring symposium natural language processing world wide web pages inderjeet mani mark maybury editors vances automatic text summarization mit press inderjeet mani automatic summarization william mann sandra thompson rhetorical structure theory functional ory text organization text daniel marcu theory practice course parsing summarization mit press cambridge massachusetts
