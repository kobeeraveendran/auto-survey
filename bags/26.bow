m l c s c v v x r new alignment methods discriminative book summarization work progress david bamman noah smith school computer science carnegie mellon university pittsburgh pa usa dbamman cmu edu abstract consider unsupervised alignment text book human written summary presents challenges seen text alignment problems including disparity length consequent violation expectation individual words phrases align large passages chapters distilled single summary phrase present new methods based hidden markov els specically targeted problem demonstrate gains extractive book marization task room improvement unsupervised ment holds intrinsic value offering insight features book deemed thy summarization introduction task extractive summarization select subset sentences source document present summary supervised approaches problem use training data form source documents paired existing summaries marcu osborne jing mckeown ceylan mihalcea methods learn features source sentence likely result sentence appearing summary news articles example strong predictive tures include position sentence ment earlier better sentence length shorter better number words sentence frequent document supervised discriminative summarization relies alignment source document summary short texts training pairs alignment source abstract sentences expected standard niques machine translation applied cluding word level alignment brown et al vogel et al och ney longer phrasal alignment daum marcu cially adapted monolingual setting quirk et al longer texts inference possible word alignments intractable effective approximations stricting space available target alignments match identity source word jing mckeown use alignment techniques book rization challenges tions rst disparity length source document summary ratio abstracts source documents benchmark ziff davis corpus newswire marcu approximately words vs words length text book greatly overshadows length simple summary figure illustrates dataset comprised books project gutenberg paired plot summaries extracted wikipedia set books scribed fully average ratio summary corresponding book disparity size leads potential violation second assumption expect words phrases source document align words phrases target disparity great expect entire graph page chapter book aligns single summary sentence method jing mckeown set literary novels methods present methods involve timating parameters hidden markov model hmm hmms differ denitions states observations parameterizations emission distributions present generic hmm instantiate models discussing respective inference learning gorithms turn let s set hidden states k observation sequence t v assigned probability n n zsn z sequence hidden states k distribution start states s s s s k s s emission transition distributions respectively note avoid stopping probabilities conditioning sequence length passage model passage model hmm state corresponds contiguous passage source document intuition approach following word phrasal alignment attempts ture ne grained correspondences source target document longer documents tilled comparatively short summaries stead long topically coherent passages summarized single sentence example following summary sentence wikipedia plot synopsis summarizes long episodic passages adventures tom sawyer playing hooky school friday dirtying clothes ght tom whitewash fence punishment day aim nd sequence passages source document aligns sequence mary sentences identify hmm figure size disparity summaries texts summaries average size ing book mean quantile help adapt existing methods supervised document summarization books present alignment techniques specically adapted problem book alignment aligns sages varying size source document tences summary guided unigram guage model probability sentence passage generalizes hmm ment model och ney case long sparsely aligned documents related work work builds long history unsupervised word phrase alignment originating chine translation literature task ing alignments parallel text brown et al vogel et al och ney ero et al monolingual quirk et al comparable corpora barzilay elhadad related task ment abstract alignment draw work ment summarization marcu osborne daum marcu past approaches tional summarization including short stories kazantseva szpakowicz books halcea ceylan tended discriminative methods notable exception ceylan applies viterbi alignment summary length bookdensity states s observations transitions emissions passage model source document passages summary sentences passage order difference unigram distribution token model source document tokens summary tokens distance bin lexical identity synonyms table summary passage model token model state s s source document positions js summary sentence sampled state s emission probability dened follows s punigram bis js bis js passage source document position position js avoid stop symbol implicitly assuming lengths xed exogenously unigram distribution punigram bis js estimated directly source ment passage bis js transition distribution state s s s operationalized following hmm word alignment formulation vogel et al transition events ordered pairs states binned difference passages ranks source document formula relative frequency estimation transition distributions boundary positions states advance estimate alongside traditional hmm parameters figure illustrates scenario sequence words source document sentences target summary b c case states correspond inference given source document b target summary t aim infer likely passage sentence depends parameters passages associated state estimate seeking imize likelihood approach em like rithm dempster et al initialization iterates steps e step calculate posterior butions t sentence tk forward backward algorithm s s m step estimate posteriors usual hmm m step c denotes count jumps particular length measured distance rank order passages document count jump passage passage passage note distance signed distance backwards jump passage passage jump hmm states spans constrained overlap need cover source document know ranks xed inference procedure low passages overlap leapfrog iterations s step sample new passages state sampling distribution considers state s moving subject overlapping constraint js moving js subject overlapping constraint denero et al details emission distribution s updated js change equation experiments described section source document initially divided k length passages k initial sion probabilities dened tialized uniform distribution boundary samples collected iteration e step m step total iterations figure illustration passage hmm hmm states correspond passages source document emission summary sentence sampling chunk boundaries summary s step sample boundaries hmm state s passage favoring stochastically boundaries observations likely expect early chunks radically reduced smaller spans match closely target sentences aligned high ability subsequent iterations longer spans favored adding words ary offsets cost adding non essential words old new boundary greedy step analogous m step use estimate parameters way s step span s boundaries positions maximize likelihood revised language model good local choices lead suboptimal global results turn instead sampling note model dened marginal distribution passage ary positions source document sampling step interpreted markov chain monte carlo em algorithm wei tanner distribution equates xed uniform distribution valid non overlapping passage boundaries implication probability ular state s s passage s end position portional probability observations erated given span following e step signment observations s fractional means likelihood function particular values js depends sentences js s n n punigram bis js example figure start position second span word word past end previous span word end span js values sampled probability proportional equation sampling distribution calculating l different boundaries requires recalculating emission probabilities s language model changes efciently linear time decomposing language model probability represent state ary positions source document use relative frequency estimate punigram log j log bi j j consider change remove rst word s s passage boundaries log bi j let bi denote source document s word position log log log j log bi bi j log j j recurrence easy solve possible left boundaries respecting overlap constraints track word frequencies span source document calculate punigram similar rence holds right boundary passage figure illustrates result sampling cedure start end positions single source passage heart darkness erations samples seen uctuate span approximately words modes relatively peaked likely start position likely end sition yielding span words figure density plot accumulated samples passage hmm state heart darkness left boundary shown black solid right ary red dashed token model jing mckeown introduced hmm states correspond tokens source ument observation sequence target summary tokens restricting types found source document emission ities xed source target words match zero stance v v target summary assumed aligned instance v source transition parameters xed manually late ranked set transition types e transitions sentence likely tions sentences parameter estimation viterbi algorithm nd probable alignment allowable transition space bounded f f frequency common token source document sulting model scalable large source documents ceylan mihalcea ceylan potential issue model lacks concept null source articulated original hmm alignment model vogel al added och ney null source word summary generated word source ument consequence decision viterbi alignment summary pick distant low probability word source document closer word available ally choice enforce lexical identity strains state space limits range ical variation captured second model extends jing s approach ways introduce parameter inference learn values start probabilities transitions maximize likelihood data em algorithm operationalize transition bilities following vogel et al strain state space measuring transititions xed bucket lengths absolute position source word tive frequency estimator transitions s s s c denotes count event function transforms difference token positions coarser set bins example transform distance source documentdensity bin distance different bin distance range bin difference fourth future work include dynamically learning timizal bin sizes boundaries learned passage hmm second introduce concept null source generate words target sentence sentence sentence translation setting source sentence m words long och ney add m corresponding null tokens source word position able adequately model transitions null tokens alignment source document words long clearly infeasible plexity single round forward backward inference n number words target summary t solve problem noting transition ability dened measured dividual words positions coarser grained chunks contain word coarsing transitions model jump xed set b bins b m effectively need add b null tokens making inference tractable nal restriction disallow tions source state positions j experiments described section expand emission probabilities allow translation source word xed set synonyms e derived roget s saurus expands coverage important lexical variants constraining allowable emission space reasonable size synonyms word available potential translations exact translation probability e purchase buy learned inference experiments evaluate alignment methods pare past work evaluate downstream task extractive book summarization data available data includes book plot maries extracted november dump english language english language books project gutenberg restrict book summary pairs text book contains words paired abstract contains words stopwords punctuation excluded results dataset book summary pairs average book length words average summary length words counting stopwords tuation ratio summaries books dataset approximately smaller previous work domain past work involving literary novels ceylan makes use collection books paired relatively long summaries sparknotes cliffsnotes gradesaver average summary length words focus instead concise case targeting summaries distill entire book approximately words discriminative summarization follow standard approach discriminative summarization experiments described use fold cross validation partition data disjoint sets train test remaining held partition evaluations conducted total ported accuracy average sets source books paired summaries training set aligned pervised methods described passage hmm token hmm jing sentences source book summary pairs featurized sentences aligned sentence summary assiged label appearing summary appearing summary featurized representation train binary gistic regression classier regularization training data learn features gutenberg org wikimedia org gutenberg org indicative source sentence appearing mary following previous work devise level features readily computed parison document tence found comparison tion documents yeh et al shen et al feature values binary sentence position document discretized membership deciles tures sentence contains salient tionalize salient capitalized words document highest tf idf score comparison rest data non sentence initial tokens calculate counts features contains lexical item frequent words captures tendency actions kills dies likely appear summary tures contains rst mention lexical item frequent words tures contains word words having highest tf idf scores book features trained model learned weights features featurize sentence test book according set features described predict appear summary sentences ranked ability sentences chosen create summary words create summary tences ordered according position source document evaluation document summarization standard fect evaluation rouge score lin hovy n gram recall measure stresses ability candidate summary recover words reference evaluate cally generated summary calculate rouge score generated summary reference summary wikipedia book consider measures overlap unigrams sures bigram overlap case single erence translation rouge n calculated following w ranges unigrams grams reference summary depending n c count n gram text wref wref figure lists results fold test available book summary pairs alignment models described moderate ment method jing et al comparison present baseline simply choosing rst words book summary model block hmm word hmm jing table rouge summarization scores method actually work tice task generating summaries manually inspecting generated summaries veals automatic summarization books great room improvement alignment methods involved appendix shows sentences extracted summary heart darkness independent quality generated maries held test data practical benet training binary log linear models resulting feature weights interpretable providing driven glimpse qualities sentence conducive appearing human created summary table lists strongest features predicting inclusion summary rank averaged training splits presence sentence highly predictive position beginning book decile end decile strongest lexical features trate importance character s persona ticularly relation father son natural importance major life events death importance features generated summary heart darkness clear nearly sentence contains important plot point captured life event mistah kurtz dead tf idf mr tf idf father love son brother years young mother family daughter wife man boy life death house chapter child sir table strongest features predicting inclusion mary conclusion present new methods optimized aligning text books comparatively shorter summaries assumptions possibility exact word phrase ment hold methods perform competitively downstream evaluation book summarization clearly remains challenging task improved book summary ments hold intrinsic value shedding light features work deemed summarizable human editors potentially exploited tasks summarization generated summary heart darkness said marlow suddenly dark places earth man followed sea worst said represent class took trouble grunt presently said slow thinking old times romans rst came nineteen years ago day light came river knights looked waiting patiently till end ood long silence said hesitating voice suppose fellows remember turn fresh water sailor bit knew fated ebb began run hear marlow s inconclusive experiences know wife high personage ministration man lots inuence determined end fuss appointed skipper river steamboat fancy shook hands fancy murmured vaguely satised french found offer good swede s kurtz felt weary irritable kurtz best agent exceptional man greatest importance company understand anxiety heard kurtz pronounced words advantage unfortunate accident men manager kurtz continued severely general manager won t opportunity blew candle suddenly went outside approach kurtz grubbing ivory wretched bush beset dangers enchanted princess sleeping lous castle moment came jump possessed hands shook continuously gabbled brother sailor honour sure delight introduce russian son arch priest government tambov s sailor smoke pipe soothed gradually run fallen wonder rendered kurtz justice away school gone sea russian ship ran away served time english ships reconciled arch priest informed lowering voice kurtz ordered attack steamer carried kurtz pilot house air suddenly manager s boy insolent black head doorway said tone scathing contempt mistah kurtz dead pilgrims rushed remained loyal kurtz long time heard voice echo icent eloquence thrown soul cently pure cliff crystal kurtz s knowledge unexplored regions necessarily extensive peculiar owing great abilities deplorable circumstances placed assured mr private letters withdrew threat legal proceedings saw fellow calling kurtz s cousin appeared days later anxious hear details dear relative s ments incidentally gave understand kurtz essentially great musician reason doubt statement day unable kurtz s profession greatest talents visitor informed kurtz s proper sphere ought politics popular furry straight eyebrows bristly hair cropped short eyeglass broad ribbon expansive confessed opinion kurtz couldn t write bit heavens man talk kurtz s passed hands soul body station plans ivory career jove impression powerful died yesterday nay minute given reason infer impatience comparative poverty drove friend heard speak saying references barzilay regina barzilay noemie elhadad sentence alignment monolingual comparable corpora proceedings ference empirical methods natural language cessing emnlp pages stroudsburg pa usa association computational linguistics brown et al peter f brown john cocke stephen della pietra vincent j della pietra fredrick linek john d lafferty robert l mercer paul s roossin statistical approach machine translation comput linguist june ceylan hakan ceylan rada halcea decomposition human written book summaries pages hakan ceylan investigating tractive summarization literary novels ph d sis university north texas daum hal daum iii daniel marcu induction word phrase ments automatic document summarization linguist december dempster et al p dempster m n laird d b rubin maximum likelihood plete data em algorithm journal royal statistical society series b statistical methodology denero et al john denero alexandre sampling alignment ct dan klein structure bayesian translation model proceedings conference empirical ods natural language processing emnlp pages stroudsburg pa usa association computational linguistics jing hongyan jing kathleen r mckeown decomposition proceedings written summary sentences annual international acm sigir conference research development information retrieval sigir pages new york ny usa acm kazantseva anna kazantseva stan szpakowicz summarizing short stories computational linguistics lin chin yew lin eduard hovy automatic evaluation summaries proceedings gram co occurrence statistics conference north american chapter association computational linguistics human language technology volume naacl pages stroudsburg pa usa association putational linguistics daniel marcu automatic struction large scale corpora summarization proceedings annual search tional acm sigir conference research velopment information retrieval sigir pages new york ny usa acm mihalcea rada mihalcea hakan ceylan explorations automatic book marization proceedings joint ence empirical methods natural language cessing computational natural language ing emnlp conll pages prague czech republic june association computational guistics och franz josef och hermann ney systematic comparison statistical alignment models comput linguist march miles osborne maximum entropy sentence extraction proceedings workshop automatic summarization ume pages stroudsburg pa usa sociation computational linguistics quirk et al chris quirk chris brockett william dolan monolingual machine lation paraphrase generation dekang lin dekai wu editors proceedings emnlp pages barcelona spain july association computational linguistics shen et al dou shen jian tao sun hua li qiang yang zheng chen document tion conditional random elds proceedings international joint conference artical intelligence pages san cisco usa morgan kaufmann publishers inc vogel et al stephan vogel hermann ney christoph tillmann hmm based word proceedings ment statistical translation conference computational linguistics ume coling pages stroudsburg pa usa association computational linguistics wei greg c g wei martin ner monte carlo implementation em algorithm poor man s data augmentation gorithms journal american statistical ation yeh et al jen yuan yeh hao ren ke wei pang yang heng meng text summarization trainable summarizer latent semantic ysis inf process manage january
