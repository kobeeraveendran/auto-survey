combining word embeddings n grams unsupervised document summarization zhuolin jiang manaj srivastava sanjay krishna gouda david akodes richard schwartz raytheon bbn technologies cambridge ma zhuolin jiang manaj srivastava sanjaykrishna gouda david akodes rich com r l c s c v v x r abstract graph based extractive document summarization relies quality sentence similarity graph bag words tf idf based sentence similarity uses exact word matching fails measure semantic similarity tween individual words consider semantic ture sentences order improve similarity measure sentences employ shelf deep embedding features tf idf features introduce new text ilarity metric improved sentence similarity graph built submodular objective function tractive summarization consists weighted erage term diversity term transformer based pression model developed sentence compression aid document summarization summarization approach extractive unsupervised experiments demonstrate approach outperform tf idf based approach achieve state art performance dataset comparable performance fully vised learning methods cnn dm nyt datasets introduction state art summarization performance achieved supervised learning methods mainly based neural network architectures require large corpus document summary pairs alternative approaches document tion employ unsupervised techniques include graph based extractive summarization ods require similarity graph sentences input summarization system similarity sentences usually computed ing bag words tf idf features porate similarity semantics modeling sentence mantic similarity challenging variability linguistic expression different words different orders express meanings set words different orders express totally different ings traditional sparse hand crafted tures bag words tf idf vectors fail tively capture similarity individual words semantic structure context sentences alternatively distributed semantic representations word embeddings word glove better job capturing word sentence level tics widely nlp tasks represent embedding sentence ing embedding vectors word sentence limited work uses deep word dings unsupervised setting extractive document summarization introduces summarization method estimates kl divergence document summary based embedding distributions paper explore popular deep embedding features bert submodular work document extractive summarization ument summarization framework unsupervised useful case limited reference summary pairs order use strengths types features combine prove similarity measure addition investigate effect abstractive sentence compression tive document summarization end train transformer model compress sentences document performing submodular sentence tion main contributions improve sentence similarity graph ing shelf neural word embedding models graph based submodular sentence selection similarity graph pair wise sentences required provide thorough experimental comparisons tween different sentence similarity measures combining shelf neural word beddings tf idf features improve mance document summarization transformer based sentence sion method improve performance ment summarization unsupervised document summarization similar extract subset sentences set sentences v document d mary maximizing submodular objective function similarity graph construction text semantic similarity given document d construct undirected ilarity graph g v e vertices v v sentences d edges e e model pairwise lation sentences weight wi associated edge ei j measures similarity tices sentences vi vj wi j computed wi j xj xi feature descriptor vi xj measures difference xi xj suggested set normalization tor ij select scaling parameter vi local statistic vi s neighborhood set xk xk corresponds k th nearest neighbor vi sentence selection submodularity selected subset representative cover unselected sentences set v associate nonnegative cost sentence s introduce weighted coverage term selecting sentences max jav x iv wi j s t b psa denotes total cost selecting b budget selecting sentences maximizing term encourages selected subset tive compact addition selected sentences diverse diversity term introduced piv wi j pk p partition v number elements v k qpjpk combine terms obtain nal objective tion maxa maxa s t v b objective function submodular monotonically increasing solve problem greedy algorithm given selected sentences step optimization step lect element highest marginal gain marginal gain argmaxsv takes element cost account element cost sentence s related position m document dened greedy algorithm guaranteed nd solution e optimal solution proved complexity tion steps accelerated lazy greedy approach construct max heap elements v evaluate max heap order approach time complexity log stead quadratic edge weight g serves similarity sentences compute similarity tences ri j pwsi sj pwsj sj maximal cosine similarity input word w words sentence sj tion words ltered computing ity similarity value ri j measures semantic lap sentences compute distance sentences similarity graph xj ri j combination different features order leverage strengths deep word beddings n grams features combine graph fusion weight assigned edge similarity graph computed similarity measure tween pairwise sentences combine graphs ferent features simple weighted average edge late fusion ranking lists weights ent features combined popular borda count rithm sentence compression order obtain compressed summarized form sentences fed unsupervised extractive algorithm trained standard transformer model encoder decoder composed stacked layers transformer neural ture shown promising results tasks applied problem sentence sion byte pair encoding subword tation order handle unseen words named entities time decoding experiments approach evaluated multi document marization dataset single document datasets cnn dm news multi document summarization dataset constructed document summarization task english news articles multiple reference summaries document clusters documents cluster evaluation f score recall mary length bytes summary baselines compare approach lines lead simply uses rst bytes options methods lead centroid submodular mckp lexrank rnn cnn tf idf wmd tss bert latefusion graphfusion tss compression bert compression graphfusion compression r table document summarization performance dataset recent document cluster winning system centroid uses embeddings summarization unsupervised marization methods compared submodular mckp lexrank methods learn sentence embeddings compared uses recursive neural networks rnn uses convolutional neural networks cnn learning sentence embeddings include results approach different bert similarity measures word embeddings sentence embedding computed mean word embeddings pretrained bert model pairwise similarity sentences cosine similar bert embeddings larity model note ne tune bert embeddings wmd sentence similarity measure word mover distance introduced tss text semantic similarity sure equation graphfusion latefusion tf idf bert wmd tss combined summarize results use different features compare results state art proaches table cnn rnn models achieve better results bert models trained datasets approach totally unsupervised uses shelf neural word bert embeddings ne tuning results graph fusion better results approaches including comparable methods r l table sentence compression performance gigaword dataset methods gig r l table sentence compression performance dataset sentence compression gigaword sentence compression dataset train transformer model gigaword dataset comprises nearly m training sentence pairs rst lines word news articles paired headlines byte pair encoding subword segmentation der determine efcacy trained model sentence pairs gigaword test set sentence pairs sentence sion dataset results gigaword beat current sentence compression baselines nearly points absolute f scores rouge l rics additional ments variants rouge metrics publicly released subset google sentence compression dataset addition gigaword dataset google pression dataset comprises nearly k sentence pairs k pairs train set k validation set metrics dataset point lute improvement rouge metrics current baselines approach summarization uses compressed sentences document sentence selection document level summarization sentence sion document summarization performance proach improved outperforms compared approaches shown table sentence compression model aid document level summarization gigaword dataset training additional improvements dataset additional google compression dataset single document summarization cnn dm dataset consists online news articles cnn daily mail websites corpus contains total article summary pairs methods oracle pointer refresh tf idf wmd tss bert graphfusion latefusion cnn dm r l nyt r l table document summarization performance cnn dm dataset pairs validation articles test pairs remaining training use sand validation pairs tuning meta parameters completely ignored training set dataset subset new york times corpus introduced use subset documents summaries words subset known nal test dataset includes test examples nal articles evaluate datasets terms rouge l l f datasets use budget sentences baselines compare approach state art supervised learning methods pointer fresh provide results extractive cle system maximizes rouge score reference summary baseline creates summary selecting rst sentences ment results datasets summarized table datasets results deep features marginally better tf idf features note proach unsupervised use training data results surprisingly comparable results supervised learning methods including conclusions explore popular deep word embeddings extractive document summarization task compared tf idf based features deep embedding features better capturing semantic similarity sentences achieve better document summarization performance options note nt report summarization results tence compression cnn dm dataset compressed sentence lose information nal performance proved constraint number selected sentences sentence similarity measure improved bining word embeddings n gram features transformer based sentence compression model duced evaluated summarization approach showing improvement summarization performance dataset summarization approach pervised achieves comparable results supervised learning methods cnn dm nyt datasets acknowledgement work supported intelligence advanced research projects activity iarpa department fense air force research laboratory contract number references z cao f wei l dong s li m zhou ranking recursive neural networks application document summarization ninth aaai conference articial intelligence z cao f wei s li w li m zhou w houfeng learning summary prior representation extractive marization proceedings annual meeting association computational linguistics national joint conference natural language processing volume short papers volume pages s chopra m auli m rush abstractive sentence summarization attentive recurrent neural networks proceedings conference north american chapter association computational linguistics human language technologies pages san diego california june association computational guistics j devlin m chang k lee k toutanova bert training deep bidirectional transformers language derstanding corr g durrett t berg kirkpatrick d klein based single document summarization compression s narayan s b cohen m lapata ranking sentences extractive summarization reinforcement learning arxiv preprint g l nemhauser l wolsey m l fisher analysis approximations maximizing submodular set functions math program dec j pennington r socher c manning glove global vectors word representation proceedings conference empirical methods natural language cessing pages g rossiello p basile g semeraro centroid based text summarization compositionality word beddings proceedings multiling workshop summarization summary evaluation source types genres pages m rush s chopra j weston neural tion model abstractive sentence summarization corr p j liu c d manning point marization pointer generator networks arxiv preprint r sennrich b haddow birch neural chine translation rare words subword units corr h takamura m okumura text summarization model based maximum coverage problem variant ceedings conference european chapter association computational linguistics pages vaswani n shazeer n parmar j uszkoreit l jones n gomez l u kaiser polosukhin attention need advances neural information processing systems pages l zelnik manor p perona self tuning spectral ing proceedings international conference neural information processing systems anaphoricity constraints arxiv preprint c dwork r kumar m naor d sivakumar rank aggregation methods web proceedings international conference world wide web www pages g erkan d r radev lexrank graph based lexical centrality salience text summarization journal cial intelligence research k filippova e alfonseca c colmenares l kaiser o vinyals sentence compression deletion lstms proceedings conference empirical methods natural language processing s gehrmann y deng m rush tive summarization arxiv preprint m kgebck o mogren n tahmasebi d hashi extractive summarization continuous vector proceedings workshop space models continuous vector space models ity cvsc pages h kobayashi m noguchi t yatsuka proceedings tion based embedding distributions conference empirical methods natural guage processing pages m j kusner y sun n kolkin k q weinberger word embeddings document distances ings international conference international conference machine learning volume pages j leskovec krause c guestrin c faloutsos c sos j vanbriesen n glance cost effective outbreak proceedings acm detection networks sigkdd international conference knowledge discovery data mining kdd pages h lin j bilmes multi document summarization budgeted maximization submodular functions human language technologies annual conference north american chapter association tional linguistics hlt pages h lin j bilmes class submodular functions document summarization proceedings annual meeting association computational linguistics human language technologies pages r mihalcea p tarau textrank bringing order text proceedings conference empirical methods natural language processing t mikolov sutskever k chen g corrado j dean distributed representations words phrases proceedings international compositionality conference neural information processing systems ume pages r nallapati b zhou c gulcehre b xiang al tive text summarization sequence sequence rnns arxiv preprint
