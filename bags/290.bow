combining word embeddings grams unsupervised document summarization zhuolin jiang manaj srivastava sanjay krishna gouda david akodes richard schwartz raytheon bbn technologies cambridge zhuolin jiang manaj srivastava sanjaykrishna gouda david akodes rich com abstract graph based extractive document summarization relies quality sentence similarity graph bag words idf based sentence similarity uses exact word matching fails measure semantic similarity tween individual words consider semantic ture sentences order improve similarity measure sentences employ shelf deep embedding features idf features introduce new text ilarity metric improved sentence similarity graph built submodular objective function tractive summarization consists weighted erage term diversity term transformer based pression model developed sentence compression aid document summarization summarization approach extractive unsupervised experiments demonstrate approach outperform idf based approach achieve state art performance dataset comparable performance fully vised learning methods cnn nyt datasets introduction state art summarization performance achieved supervised learning methods mainly based neural network architectures require large corpus document summary pairs alternative approaches document tion employ unsupervised techniques include graph based extractive summarization ods require similarity graph sentences input summarization system similarity sentences usually computed ing bag words idf features porate similarity semantics modeling sentence mantic similarity challenging variability linguistic expression different words different orders express meanings set words different orders express totally different ings traditional sparse hand crafted tures bag words idf vectors fail tively capture similarity individual words semantic structure context sentences alternatively distributed semantic representations word embeddings word glove better job capturing word sentence level tics widely nlp tasks represent embedding sentence ing embedding vectors word sentence limited work uses deep word dings unsupervised setting extractive document summarization introduces summarization method estimates divergence document summary based embedding distributions paper explore popular deep embedding features bert submodular work document extractive summarization ument summarization framework unsupervised useful case limited reference summary pairs order use strengths types features combine prove similarity measure addition investigate effect abstractive sentence compression tive document summarization end train transformer model compress sentences document performing submodular sentence tion main contributions improve sentence similarity graph ing shelf neural word embedding models graph based submodular sentence selection similarity graph pair wise sentences required provide thorough experimental comparisons tween different sentence similarity measures combining shelf neural word beddings idf features improve mance document summarization transformer based sentence sion method improve performance ment summarization unsupervised document summarization similar extract subset sentences set sentences document mary maximizing submodular objective function similarity graph construction text semantic similarity given document construct undirected ilarity graph vertices sentences edges model pairwise lation sentences weight associated edge measures similarity tices sentences computed feature descriptor measures difference suggested set normalization tor select scaling parameter local statistic neighborhood set corresponds nearest neighbor sentence selection submodularity selected subset representative cover unselected sentences set associate nonnegative cost sentence introduce weighted coverage term selecting sentences max jav psa denotes total cost selecting budget selecting sentences maximizing term encourages selected subset tive compact addition selected sentences diverse diversity term introduced piv partition number elements qpjpk combine terms obtain nal objective tion maxa maxa objective function submodular monotonically increasing solve problem greedy algorithm given selected sentences step optimization step lect element highest marginal gain marginal gain argmaxsv takes element cost account element cost sentence related position document dened greedy algorithm guaranteed solution optimal solution proved complexity tion steps accelerated lazy greedy approach construct max heap elements evaluate max heap order approach time complexity log stead quadratic edge weight serves similarity sentences compute similarity tences pwsi pwsj maximal cosine similarity input word words sentence tion words ltered computing ity similarity value measures semantic lap sentences compute distance sentences similarity graph combination different features order leverage strengths deep word beddings grams features combine graph fusion weight assigned edge similarity graph computed similarity measure tween pairwise sentences combine graphs ferent features simple weighted average edge late fusion ranking lists weights ent features combined popular borda count rithm sentence compression order obtain compressed summarized form sentences fed unsupervised extractive algorithm trained standard transformer model encoder decoder composed stacked layers transformer neural ture shown promising results tasks applied problem sentence sion byte pair encoding subword tation order handle unseen words named entities time decoding experiments approach evaluated multi document marization dataset single document datasets cnn news multi document summarization dataset constructed document summarization task english news articles multiple reference summaries document clusters documents cluster evaluation score recall mary length bytes summary baselines compare approach lines lead simply uses rst bytes options methods lead centroid submodular mckp lexrank rnn cnn idf wmd tss bert latefusion graphfusion tss compression bert compression graphfusion compression table document summarization performance dataset recent document cluster winning system centroid uses embeddings summarization unsupervised marization methods compared submodular mckp lexrank methods learn sentence embeddings compared uses recursive neural networks rnn uses convolutional neural networks cnn learning sentence embeddings include results approach different bert similarity measures word embeddings sentence embedding computed mean word embeddings pretrained bert model pairwise similarity sentences cosine similar bert embeddings larity model note tune bert embeddings wmd sentence similarity measure word mover distance introduced tss text semantic similarity sure equation graphfusion latefusion idf bert wmd tss combined summarize results use different features compare results state art proaches table cnn rnn models achieve better results bert models trained datasets approach totally unsupervised uses shelf neural word bert embeddings tuning results graph fusion better results approaches including comparable methods table sentence compression performance gigaword dataset methods gig table sentence compression performance dataset sentence compression gigaword sentence compression dataset train transformer model gigaword dataset comprises nearly training sentence pairs rst lines word news articles paired headlines byte pair encoding subword segmentation der determine efcacy trained model sentence pairs gigaword test set sentence pairs sentence sion dataset results gigaword beat current sentence compression baselines nearly points absolute scores rouge rics additional ments variants rouge metrics publicly released subset google sentence compression dataset addition gigaword dataset google pression dataset comprises nearly sentence pairs pairs train set validation set metrics dataset point lute improvement rouge metrics current baselines approach summarization uses compressed sentences document sentence selection document level summarization sentence sion document summarization performance proach improved outperforms compared approaches shown table sentence compression model aid document level summarization gigaword dataset training additional improvements dataset additional google compression dataset single document summarization cnn dataset consists online news articles cnn daily mail websites corpus contains total article summary pairs methods oracle pointer refresh idf wmd tss bert graphfusion latefusion cnn nyt table document summarization performance cnn dataset pairs validation articles test pairs remaining training use sand validation pairs tuning meta parameters completely ignored training set dataset subset new york times corpus introduced use subset documents summaries words subset known nal test dataset includes test examples nal articles evaluate datasets terms rouge datasets use budget sentences baselines compare approach state art supervised learning methods pointer fresh provide results extractive cle system maximizes rouge score reference summary baseline creates summary selecting rst sentences ment results datasets summarized table datasets results deep features marginally better idf features note proach unsupervised use training data results surprisingly comparable results supervised learning methods including conclusions explore popular deep word embeddings extractive document summarization task compared idf based features deep embedding features better capturing semantic similarity sentences achieve better document summarization performance options note report summarization results tence compression cnn dataset compressed sentence lose information nal performance proved constraint number selected sentences sentence similarity measure improved bining word embeddings gram features transformer based sentence compression model duced evaluated summarization approach showing improvement summarization performance dataset summarization approach pervised achieves comparable results supervised learning methods cnn nyt datasets acknowledgement work supported intelligence advanced research projects activity iarpa department fense air force research laboratory contract number references cao wei dong zhou ranking recursive neural networks application document summarization ninth aaai conference articial intelligence cao wei zhou houfeng learning summary prior representation extractive marization proceedings annual meeting association computational linguistics national joint conference natural language processing volume short papers volume pages chopra auli rush abstractive sentence summarization attentive recurrent neural networks proceedings conference north american chapter association computational linguistics human language technologies pages san diego california june association computational guistics devlin chang lee toutanova bert training deep bidirectional transformers language derstanding corr durrett berg kirkpatrick klein based single document summarization compression narayan cohen lapata ranking sentences extractive summarization reinforcement learning arxiv preprint nemhauser wolsey fisher analysis approximations maximizing submodular set functions math program dec pennington socher manning glove global vectors word representation proceedings conference empirical methods natural language cessing pages rossiello basile semeraro centroid based text summarization compositionality word beddings proceedings multiling workshop summarization summary evaluation source types genres pages rush chopra weston neural tion model abstractive sentence summarization corr liu manning point marization pointer generator networks arxiv preprint sennrich haddow birch neural chine translation rare words subword units corr takamura okumura text summarization model based maximum coverage problem variant ceedings conference european chapter association computational linguistics pages vaswani shazeer parmar uszkoreit jones gomez kaiser polosukhin attention need advances neural information processing systems pages zelnik manor perona self tuning spectral ing proceedings international conference neural information processing systems anaphoricity constraints arxiv preprint dwork kumar naor sivakumar rank aggregation methods web proceedings international conference world wide web www pages erkan radev lexrank graph based lexical centrality salience text summarization journal cial intelligence research filippova alfonseca colmenares kaiser vinyals sentence compression deletion lstms proceedings conference empirical methods natural language processing gehrmann deng rush tive summarization arxiv preprint kgebck mogren tahmasebi hashi extractive summarization continuous vector proceedings workshop space models continuous vector space models ity cvsc pages kobayashi noguchi yatsuka proceedings tion based embedding distributions conference empirical methods natural guage processing pages kusner sun kolkin weinberger word embeddings document distances ings international conference international conference machine learning volume pages leskovec krause guestrin faloutsos sos vanbriesen glance cost effective outbreak proceedings acm detection networks sigkdd international conference knowledge discovery data mining kdd pages lin bilmes multi document summarization budgeted maximization submodular functions human language technologies annual conference north american chapter association tional linguistics hlt pages lin bilmes class submodular functions document summarization proceedings annual meeting association computational linguistics human language technologies pages mihalcea tarau textrank bringing order text proceedings conference empirical methods natural language processing mikolov sutskever chen corrado dean distributed representations words phrases proceedings international compositionality conference neural information processing systems ume pages nallapati zhou gulcehre xiang tive text summarization sequence sequence rnns arxiv preprint
