n u j l c s c v v i x r a efcient adaptation of pretrained transformers for abstractive summarization andrew hoang antoine bosselut asli celikyilmaz yejin choi allen school of computer science engineering university of washington seattle wa allen institute for articial intelligence seattle wa microsoft research redmond wa antoineb washington edu org abstract large scale learning of transformer language models has yielded improvements on a variety of natural language understanding tasks whether they can be tively adapted for summarization however has been less explored as the learned representations are less seamlessly integrated into existing neural text production architectures in this work we propose two solutions for efciently adapting pretrained transformer language models as text summarizers source embeddings and domain adaptive training we test these solutions on three abstractive marization datasets achieving new state of the art performance on two of them finally we show that these improvements are achieved by producing more focused summaries with fewer superuous and that performance improvements are more pronounced on more abstractive datasets introduction recent work in large scale language models has allowed pretrained contextual sentations to be easily adapted for a variety of downstream tasks yielding improvements on many benchmarks evaluating natural language understanding less explored however has been the effect of these pretrained representations on text production tasks such as abstractive summarization where state of the art performance is still achieved with sequence to sequence models these sequence to sequence methods typically use an encoder and decoder model with separate parameters to represent the input article and produce the output summary and the most successful solutions use attention mechanisms that learn an alignment between encoder and decoder states pretrained language models however do not learn the parameters for such a task specic alignment making it challenging to integrate their learned representations into a summarization architecture at a higher level of abstraction than the word embedding in this work we adapt full transformer language models for abstractive summarization building off the work of liu et al who rst proposed concatenating input and output text to a joint sequence and using a common transformer to encode both we use a language model as a summarizer rather than an encoder decoder with this approach representations from a pretrained transformer language model in this case gpt can be used to fully initialize the parameters of the summarization model allowing it to leverage the representational power of a model trained at much larger scale to accomplish this effectively we outline two strategies for adapting pretrained representations for abstractive summarization in the rst we augment the input representation of the summarization model by instantiating source embeddings that encode the token type of the text being read this change allows the model to recognize whether a given token belongs to the input article or the output summary thereby learning how to distinguish both types of text when encoding in the second we preprint under review figure the embedding process for inputs to the transformer sm model introduce a domain adaptive training procedure that ne tunes the transformer toward understanding general newswire text before training on the summarization end task directly allowing the model to learn the general structure and language distribution of newswire text before being ne tuned to produce summaries a comprehensive empirical study across three datasets cnn dailymail xsum and room shows that transformer language models can be used to train abstractive summarizers producing summaries that are more concise and focused than state of the art baselines our tion also empirically validates several observations about the abstractive summarization task first echoing the results of sun et al the most common summarization evaluation metric rouge is highly sensitive to summary length providing an advantage to methods that produce longer summaries either through learning or with minimum summary length constraints second achieving higher rouge scores is not strongly consistent with human assessments of abstractive summary quality finally despite being conceived as abstractive summarizers most current state of the art models are highly extractive copying phrases and even sentences verbatim from the document model in this paper we focus on a variant of the transformer that has been pretrained on a large corpus of natural language stories the gpt model as our architecture is practically identical to the one proposed in radford et al we point readers to that work for background on the architecture of the model and focus below on the enhancements to the input representation made in our approach input representation xa each article is represented as a sequence of m tokens xa m and its corresponding summary is a sequence of n tokens xs n as outlined in figure the input structure of the training set is a pair of article and corresponding summary concatenated into two sequences similar to xa xs xs where t m n and d and e are special tokens identifying the delimitation and end of the sequence below we dene the process of encoding these sequences as inputs to the transformer xa d xs e x word embedding first each token xt in the concatenated sequence x indexes a word embedding rh from a joint vocabulary for the article and summary and special tokens position embedding second since the transformer a self attention model has no concept of ordering of tokens a position embedding pt rh is initialized for each absolute position in the sequence the embedding for each position in the sequence is added to the word embedding of the token occupying that position augmenting the nal representation of the input for example each token in the article would be represented as wa m m pm once the delimitation token d is dadads dsdsdssourceembeddings reached the position counter is reset for example the rst token of the article xa of the summary xs both receive as a positional embedding to augment their representations and the rst token source embedding finally because the transformer must recognize pragmatic differences between the text of the article it reads and the text of the summary it learns to produce an additional specic embedding is initialized rh the source embedding encodes whether a token is from the article portion da of the concatenated input or the summary portion ds for any article token eq or summary token eq then the nal encoding is wa m m pm ws n n pn ds in contrast to the other embeddings in the model the source embeddings are not pretrained ducing the potential that they could dominate pretrained representations for the word and position embeddings when summed eq to avoid this we normalize the random initialization of the source embeddings to have norm equal to half of the average norm of the word embeddings training the model is initialized with pretrained parameters from the gpt model that was trained on the bookscorpus following this initialization we pursue two additional training procedures domain adaptive training and end task training domain adapative training despite the benet of using pretrained representations from the gpt model to initialize a summarizer there is a language shift between the storybooks data on which the gpt model was trained and the type of language found in newswire summarization datasets additionally there are structural differences between how articles are written usually expressing salient points early on followed by details later and how stories unfold less front loading of key information to address this discrepancy we propose domain adaptive training dat to adapt the transformer summarization model to the language distribution of newswire text by maximizing the conditional loglikelihood of the article tokens and summary tokens given all previous tokens in their concatenated input representation see figure ldat log p xa log p xs m n where m is length of the article n is the length of the summary is the set of all tokens in the article that precede xa n and is the set of all article tokens in this framework the model is adapted to produce newswire like language before being trained on the summarization end task which only focuses on learning for summary production m is the set of all tokens in the summary that precede xs end task training during end task training ett the model is trained specically to be able to produce a summary given a document constraining the loss function toward maximizing the conditional loglikelihood of producing only the correct summary tokens given the set of article tokens lett log p xs where is the set of tokens in the summary that precede xs n n table comparison of summarization datasets with respect to dataset size proportion of unique n grams mean article length in words and mean summary length in words dataset newsroom xsum cnn dailymail split size train validation test novel n grams in gold summary mean words unigrams bigrams trigrams grams article summary experimental setup datasets the cnn daily mail dataset consists of articles from cnn and daily mail each article is associated with several descriptive bullet point highlights similar to previous work we concatenate the highlights to create a target summary for each article in the dataset and use the same dataset splits the extreme summarization xsum dataset consists of article summary pairs taken from the bbc each summary is a single sentence long and is professionally written usually by the author making the dataset exhibit more abstractive content than typical summarization datasets such as cnn dailymail the newsroom dataset consists of m article summary pairs scraped from the internet archive the articles come from a set of publishers and cover diverse topics we provide statistics about each dataset in table data preprocessing we used a bytepair encoding bpe for tokenization for each summarization dataset we use the bpe to tokenize each article and summary and then truncate the articles to a maximum length of tokens and each summary to a maximum length of tokens we then format each article summary pair into the format outlined in figure model specications we used a transformer decoder with n blocks and h masked self attention heads in each block we set the dimensionality of each self attention head to be dmodel unless stated otherwise we use the pretrained weights of radford et al to initialize the parameters of the model special tokens that are added to the vocabulary i e the end token start token and delimiter token are initialized by sampling from the standard normal distribution our full model with source embeddings is denoted as as transformer sm and we also train an ablation transformer lm that does not use source embeddings training details all models were trained with a learning rate of and a minibatch size of when domain adaptive training dat is used we train for epochs using dat and then for an additional epochs using end task training ett without dat we train on the end task for epochs unless specied otherwise the nal model trained for each dataset uses both domain adaptive training and end task training we did not tune hyperparameters all models were trained using the pytorch and the huggingface implementation of gpt we trained each model on tesla training for a total of epochs took approximately day of clock time for the xsum and cnn daily mail datasets and days for the newsroom dataset our source code is publicly available generation we perform generation by using beam search with a beam size of we use the trigram trick during beam search each summary token is generated by decoding from the distribution yielded by the model from processing an input tensor that is the concatenation of the article tokens the delimiter token and any previously generated summary tokens evaluation we evaluate our system with common summarization metrics a measure of unigram recall between the summary and document a similar measure of bigram recall and rouge l r l a measure of the longest common subsequence between the summary and document we also report the length of the summary in terms of tokens produced for each dataset for evaluation on the test set we selected models with the largest score on a subset of samples from the validation set com huggingface pytorch openai transformer lm com transformer abstractive summarization experiments cnn daily mail baselines we report the results from various models previously trained and evaluated on the cnn daily mail dataset the pgen and pgen coverage models consist of attentive rnn encoder decoders that integrate the ability to directly copy from the article when generating tokens pasunuru and bansal extend this work by adding policy gradient training with a mixture of rewards that promote saliency and entailment bottom up summarization and the copy transformer also extend see et al by using the copy mechanism to compress the article to only relevant content before summarizing it chen and bansal also look at performing content selection but extract full sentences from the document with a novel extractor model finally the dca model uses multiple separate communicating encoders over different parts of the document to produce representations that are more focused on salient details table rouge results on the test set of cnn daily mail best model results are bolded model pgen pgen coverage rougesal ent rl bottom up summ copytransformer rnn ext rl dca transformer lm transformer sm r l length l automatic metrics we report our results using automatic metrics in table on this dataset our main model transformer sm performs slightly worse than other state of the art models we note that our model tends to generate shorter summaries than the gold summaries shorter which could lower rouge recall performance in figure we investigate the correlation of rouge l scores with summary length and note that a minimum decoding length used by state of the art algorithms places baseline generated summaries in length bins of higher average rouge l performance when transformer sm produces summaries in these same length bins i e more than tokens its performance is only consistently beaten by the dca model which was ne tuned with rl figure average rouge l for summaries in different length bins scatter plots correspond to rouge l scores for each bin while solid lines correspond to the number of summaries in each bin table head to head comparison between test set outputs of left dca and sm right and transformer sm analyses done on summaries for cnn dailymail model non redundancy coherence focus dca same t sm same t sm overall human evaluation while rouge scores are negatively inuenced by the shorter average length of the summaries produced by our model it is not clear that shorter summaries are correlated with worse quality to evaluate this hypothesis we perform a human evaluation on article summary pairs randomly sampled from the test set the article and model generated summaries were presented to three workers from amazon mechanical turk amt each worker was presented two model generated summaries one produced by the sm model and one from the dca model or the model workers were asked to select the better summary for four different quality metrics from celikyilmaz et al redundancy fewer of the same ideas are repeated coherence ideas are expressed clearly focus the main ideas of the document are shared while avoiding superuous details and overall the results are presented in table interestingly the summaries from transformer sm are tently preferred by humans across all evaluations dimensions compared to those from the dca and models indicating that the transformer sm s lower rouge scores observed in table are not necessarily correlated with human judgments of quality table rouge l precision r l p recall l r and r l scores computed between generated summaries and input cnn dailymail articles after removing stop words table ablation study of training schedules on cnn dailymail pt model initialized with pretrained weights dat model uses adaptive training ett trained on end task r l p r l r model name pgen bottom up rnn ext rl dca transformer lm transformer sm l gold summary model t lm ett t lm t lm t lm t sm ett t sm t sm t sm r l efciency due to the large improvements over the baseline models in the human evaluation gories of non redundancy and focus and the generally shorter summaries produced by sm we investigate whether transformer sm is able to more efciently express key ideas of the document to evaluate the efciency of each model we remove non content words from the generated summaries and articles and compute the rouge score between them this measure serves as a proxy for the rate at which ideas expressed in the summary can be found in the document we report these results in table and observe that transformer sm reports comparable l recall scores to other baselines when evaluated with respect to the article despite producing summaries that on average shorter meanwhile rouge l precision is also very similar to the baseline models indicating that the summaries of all models indicate a similar degree of information relevance combined with the results from table we conjecture that transformer sm is able high precision scores across all models conrm that despite being conceived as abstractive generators these models display highly extractive behavior to more efciently express key ideas from the document while other models may be producing longer summaries that yield higher rouge performance table the additional tokens may reect redundant and unsalient information which human evaluators penalize analysis of domain adaptive training and source embeddings our approach involved two gies for efciently using transformer language models for abstractive summarization domain adaptive training and source embeddings to assess their individual impact we evaluate multiple training schedule permutations e various combinations of using pretrained representations from the gpt model and using domain adaptive training as well as the impact of source embeddings our results in table yield multiple interesting conclusions first in general domain adaptive training dat in table provides a clear improvement over training directly on the end task irrespective of whether pretrained representations are used similarly using source embeddings t sm in table provides a repeated improvement over the t lm ablation surprisingly when pretrained initializations dat and source embeddings are used in tandem performance drops slightly compared to not using dat or not using source embeddings we note however that this observation does not hold true for the xsum dataset and conjecture that the extractive nature of the cnn dailymail dataset may make these approaches have redundant effects in this setting xsum a study on the quality of abstractive summaries is best performed on the xsum dataset which is specically designed with gold summaries that are less extractive than the other datasets table baselines we report the performance of transformer sm on this dataset in comparison to baselines originally reported in narayan et al an attention based sequence to sequence model a pointer generator model capable of generating words and copying directly from the input pgen a second pointer generator model with a coverage mechanism to prevent repetition and the top performing variant of the topic aware convolutional sequence to sequence model t in which the encoder and decoder are provided with word topic and document topic distributions obtained using lda as additional inputs our nal baseline is the multi level memory network mmn which applies attention over multiple memory layers for varying levels of abstraction results we report our results in table our els signicantly outperform the comparison lines across all three variants of the rouge metric interestingly the transformer sm achieves able improvement over the transformer lm model suggesting that both source embeddings and domain adaptive training are helpful when target summaries are more abstractive examples of model generated summaries from the xsum dataset illustrate the provement over baselines qualitatively in table in support of results presented earlier the model duces abstractive summaries that provide focused information about the main points of the articles newsroom table comparison results on the xsum test set using the variants of rouge model pgen t mmn r l transformer lm transformer sm finally we report the performance of our model on the newsroom dataset the largest of the evaluation datasets due to the large cost of training only the transformer sm model was evaluated baselines as baselines we report the performance of models released by the authors of the newsroom dataset these models included an attentive encoder decoder attn and a generator network pgen we also compared against a complex encoder decoder that uses lstms encoder attention intra decoder attention and pointer generation to produce summaries we also compare against the multi level memory network mmn mentioned earlier the authors of this baseline only evaluated on the abstractive subset of the newsroom dataset table xsum samples for the baseline t model and transformer sm along with the gold summary articles are shortened for brevity capitalization was manually added for ease of reading source source text article snippet ofcials said the attack happened at the europa shopping centre in the capital minsk police later arrested the year old suspect he cut one woman with the chainsaw and hit her with a hammer she died he also attacked others the injured woman was taken to a local hospital the attacker had brought the chainsaw and the axe to the shopping centre t transformer sm a man has been arrested on suspicion of attempted murder by after a knife attack on a shopping centre in central london a teenage girl has been killed by a chainsaw attack at a shopping centre in central russia police say gold a young man has attacked people with a chainsaw and an axe at a shopping centre in belarus killing one woman and injuring another article snippet the year old sweden striker s contract with the french champions expires in the summer and he has been linked with manchester united la galaxy and ac milan psg said ibrahimovic leaves as the greatest striker and one of the very best players in the club s history t transformer sm paris st germain have completed the signing of zlatan ibrahimovic from paris st germain for an undisclosed fee zlatan ibrahimovic says he will leave paris st germain at the end of the season to return to the club gold zlatan ibrahimovic will leave paris st germain at the end of the season article snippet the animal was taken from lathom pets and aquatics in ormskirk on tuesday afternoon lancashire police said the shop s owner said cctv showed a man taking the tortoise which needs calcium supplements out of the tank t transformer sm a tortoise has been stolen from a pet shop a puppy s pet shop has been stolen from a shop in lancashire gold a baby tortoise has been stolen from a pet shop in lancashire table rouge results on validation subsets and full validation set for newsroom model name pgen mmn transformer sm extractive r l mixed abstractive newsroom d r l r l r l table comparison results on the room test set using rouge model attn pgen r l t sm results we report our results with rouge style tomatic metrics in table showing that sm outperforms the previous best model across all metrics interestingly our model achieves its highest performance increase over baseline models on rouge l the metric usually considered as being most strongly correlated with strong summaries thermore an analysis of different validation subsets of the newsroom dataset in table split on the level of extractiveness of the gold summaries shows that transformer sm performs better than baselines proaches on all varieties of summary types related work abstractive summarization there has been a large variety of work exploring different methods for neural abstractive document summarization attention mechanisms have been shown to improve a variety of models and is one of the motivating factors for this work pointer generator networks introduced in see et al have been shown to increase summary veracity and inspired the tangential usage of copy mechanisms in transformers for document summarization gehrmann et al other works have also explored the use of reinforcement learning to directly optimize summarization models on the rouge metric contextualized representations our approach is also relevant to recent work on contextualized language representations that are pretrained on large scale language corpora these representations can then be simply integrated or ne tuned for improved performance on many downstream tasks ssl cove and elmo all learned contextualized representations through training rnn language models and encoder decoders follow up work extended these ideas but replaced the rnn with a deep transformer that was trained to learn language patterns on a large story dataset bert more clearly extended the idea of using transformers for language modeling by making the encoded representations bidirectional and adding two new loss functions a masked token loss and next sentence prediction loss for more accurate discourse representations more recently expanded the scale of pretrained language models and showed promising results on zero shot tasks conclusion in this work we introduce two approaches for effectively adapting pretrained language model sentations to abstractive summarization domain adaptive training and source embeddings we uate the effect of both approaches across three abstractive summarization testbeds cnn dailymail xsum and newsroom and achieve state of the art rouge l results on two of them while showing superior human evaluation performance on the third in the process we show that the rouge l metric often used for abstractive summarization evaluation is quite sensitive to summary length allowing it to be exploitable by approaches that use heuristics to control summary length references asli celikyilmaz antoine bosselut xiaodong he and yejin choi deep communicating agents for abstractive summarization in naacl yen chun chen and mohit bansal fast abstractive summarization with reinforce selected sentence rewriting in acl jianpeng cheng and mirella lapata neural summarization by extracting sentences and words arxiv preprint andrew m dai and quoc v le semi supervised sequence learning in nips jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training of deep bidirectional transformers for language understanding in naacl sebastian gehrmann yuntian deng and alexander m rush bottom up abstractive summarization in emnlp max grusky mor naaman and yoav artzi newsroom a dataset of million summaries with diverse extractive strategies in naacl karl moritz hermann tomas kocisky edward grefenstette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend in advances in neural information processing systems pages byeongchang kim hyunwoo kim and gunhee kim abstractive summarization of reddit posts with multi level memory networks arxiv preprint chin yew lin looking for a few good metrics automatic summarization evaluation how many samples are enough in ntcir chin yew lin rouge a package for automatic evaluation of summaries text rization branches out peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer generating wikipedia by summarizing long sequences in iclr bryan mccann james bradbury caiming xiong and richard socher learned in translation contextualized word vectors in nips ramesh nallapati feifei zhai and bowen zhou summarunner a recurrent neural network based sequence model for extractive summarization of documents in thirty first aaai conference on articial intelligence ramesh nallapati bowen zhou ccero nogueira dos santos aglar glehre and bing xiang abstractive text summarization using sequence to sequence rnns and beyond in conll shashi narayan shay b cohen and mirella lapata do nt give me the details just the summary topic aware convolutional neural networks for extreme summarization in emnlp ramakanth pasunuru and mohit bansal multi reward reinforced summarization with saliency and entailment in acl romain paulus caiming xiong and richard socher a deep reinforced model for abstractive summarization in iclr matthew e peters mark neumann mohit iyyer matt gardner christopher clark kenton lee and luke zettlemoyer deep contextualized word representations in proc of naacl alec radford karthik narasimhan tim salimans and ilya sutskever improving language understanding by generative pre training url us amazonaws com openai assets research covers language unsupervised pdf alec radford jeff wu rewon child david luan dario amodei and ilya sutskever language models are unsupervised multitask learners abigail see peter j liu and christopher d manning get to the point summarization with pointer generator networks in acl tian shi yaser keneshloo naren ramakrishnan and chandan k reddy neural tive text summarization with sequence to sequence models arxiv preprint simeng sun ori shapira ido dagan and ani nenkova how to compare summarizers without target length pitfalls solutions and re examination of the neural summarization literature in proceedings of naacl workshop on optimizing and evaluating neural language generation neuralgen jiwei tan xiaojun wan and jianguo xiao abstractive document summarization with a graph based attentional neural model in proceedings of the annual meeting of the association for computational linguistics volume long papers volume pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser and illia polosukhin attention is all you need in advances in neural information processing systems pages alex wang amanpreet singh julian michael felix hill omer levy and samuel r bowman glue a multi task benchmark and analysis platform for natural language understanding arxiv preprint yukun zhu ryan kiros richard s zemel ruslan r salakhutdinov raquel urtasun antonio torralba and sanja fidler aligning books and movies towards story like visual explanations by watching movies and reading books ieee international conference on computer vision iccv pages a reproducibility we provide additional details relevant to the experimental environment here data sources the cnn daily mail dataset consists of articles from cnn and daily mail each article is associated with several descriptive bullet point highlights similar to previous work we concatenate the highlights to create a target summary for each article in the dataset the newsroom dataset consists of m article summary pairs scraped from the internet archive the articles come from a set of publishers and cover diverse topics finally the extreme summarization xsum dataset consists of article summary pairs taken from the bbc each summary is a single sentence long and is professionally written usually by the author for all datasets we use the splits dened in the original works that proposed them because the datasets are too large to provide as supplementary material we provide pointers in the source code readme for acquiring them hyperparameters details about important hyperparameters can be found in section of the paper additional training hyperparameters can be found as the default parameters in the training script of the source code most hyperparameter values selected were the same ones suggested by previous work on transformer language models the only hyperparameter we varied that is not measured as an ablation i e training schedules and whether to include source embeddings was the initialization of source embeddings if they were included for this hyperparameter we explored three different initializations initializing both source embeddings with zero vectors initializing both source embeddings with values sampled from the standard normal distribution and initializing both source embeddings with values sampled from a normal distribution with mean and standard deviation equal to half the norm of the average norm among pretrained embeddings from the gpt language model this last one is the one we report in all experiments experimental process each experiment was run as follows for any given model and dataset first we trained the model as described in the paper after every minibatches we compute rouge for a random but persistent example subset of the validation set when the score of the model stopped rising we used the previous checkpoint as a model to generate summaries for all articles in the test set we used beam search to decode summaries using a beam with of we ran exactly one evaluation run for each result we include in our paper cnn com dailymail co bbc com transformer abstractive summarization
