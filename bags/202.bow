n u j l c s c v v x r efcient adaptation pretrained transformers abstractive summarization andrew hoang antoine bosselut asli celikyilmaz yejin choi allen school computer science engineering university washington seattle wa allen institute articial intelligence seattle wa microsoft research redmond wa antoineb washington edu org abstract large scale learning transformer language models yielded improvements variety natural language understanding tasks tively adapted summarization explored learned representations seamlessly integrated existing neural text production architectures work propose solutions efciently adapting pretrained transformer language models text summarizers source embeddings domain adaptive training test solutions abstractive marization datasets achieving new state art performance finally improvements achieved producing focused summaries fewer superuous performance improvements pronounced abstractive datasets introduction recent work large scale language models allowed pretrained contextual sentations easily adapted variety downstream tasks yielding improvements benchmarks evaluating natural language understanding explored effect pretrained representations text production tasks abstractive summarization state art performance achieved sequence sequence models sequence sequence methods typically use encoder decoder model separate parameters represent input article produce output summary successful solutions use attention mechanisms learn alignment encoder decoder states pretrained language models learn parameters task specic alignment making challenging integrate learned representations summarization architecture higher level abstraction word embedding work adapt transformer language models abstractive summarization building work liu et al rst proposed concatenating input output text joint sequence common transformer encode use language model summarizer encoder decoder approach representations pretrained transformer language model case gpt fully initialize parameters summarization model allowing leverage representational power model trained larger scale accomplish effectively outline strategies adapting pretrained representations abstractive summarization rst augment input representation summarization model instantiating source embeddings encode token type text read change allows model recognize given token belongs input article output summary learning distinguish types text encoding second preprint review figure embedding process inputs transformer sm model introduce domain adaptive training procedure ne tunes transformer understanding general newswire text training summarization end task directly allowing model learn general structure language distribution newswire text ne tuned produce summaries comprehensive empirical study datasets cnn dailymail xsum room shows transformer language models train abstractive summarizers producing summaries concise focused state art baselines tion empirically validates observations abstractive summarization task echoing results sun et al common summarization evaluation metric rouge highly sensitive summary length providing advantage methods produce longer summaries learning minimum summary length constraints second achieving higher rouge scores strongly consistent human assessments abstractive summary quality finally despite conceived abstractive summarizers current state art models highly extractive copying phrases sentences verbatim document model paper focus variant transformer pretrained large corpus natural language stories gpt model architecture practically identical proposed radford et al point readers work background architecture model focus enhancements input representation approach input representation xa article represented sequence m tokens xa m corresponding summary sequence n tokens xs n outlined figure input structure training set pair article corresponding summary concatenated sequences similar xa xs xs t m n d e special tokens identifying delimitation end sequence dene process encoding sequences inputs transformer xa d xs e x word embedding token xt concatenated sequence x indexes word embedding rh joint vocabulary article summary special tokens position embedding second transformer self attention model concept ordering tokens position embedding pt rh initialized absolute position sequence embedding position sequence added word embedding token occupying position augmenting nal representation input example token article represented wa m m pm delimitation token d dadads dsdsdssourceembeddings reached position counter reset example rst token article xa summary xs receive positional embedding augment representations rst token source embedding finally transformer recognize pragmatic differences text article reads text summary learns produce additional specic embedding initialized rh source embedding encodes token article portion da concatenated input summary portion ds article token eq summary token eq nal encoding wa m m pm ws n n pn ds contrast embeddings model source embeddings pretrained ducing potential dominate pretrained representations word position embeddings summed eq avoid normalize random initialization source embeddings norm equal half average norm word embeddings training model initialized pretrained parameters gpt model trained bookscorpus following initialization pursue additional training procedures domain adaptive training end task training domain adapative training despite benet pretrained representations gpt model initialize summarizer language shift storybooks data gpt model trained type language found newswire summarization datasets additionally structural differences articles written usually expressing salient points early followed details later stories unfold loading key information address discrepancy propose domain adaptive training dat adapt transformer summarization model language distribution newswire text maximizing conditional loglikelihood article tokens summary tokens given previous tokens concatenated input representation figure ldat log p xa log p xs m n m length article n length summary set tokens article precede xa n set article tokens framework model adapted produce newswire like language trained summarization end task focuses learning summary production m set tokens summary precede xs end task training end task training ett model trained specically able produce summary given document constraining loss function maximizing conditional loglikelihood producing correct summary tokens given set article tokens lett log p xs set tokens summary precede xs n n table comparison summarization datasets respect dataset size proportion unique n grams mean article length words mean summary length words dataset newsroom xsum cnn dailymail split size train validation test novel n grams gold summary mean words unigrams bigrams trigrams grams article summary experimental setup datasets cnn daily mail dataset consists articles cnn daily mail article associated descriptive bullet point highlights similar previous work concatenate highlights create target summary article dataset use dataset splits extreme summarization xsum dataset consists article summary pairs taken bbc summary single sentence long professionally written usually author making dataset exhibit abstractive content typical summarization datasets cnn dailymail newsroom dataset consists m article summary pairs scraped internet archive articles come set publishers cover diverse topics provide statistics dataset table data preprocessing bytepair encoding bpe tokenization summarization dataset use bpe tokenize article summary truncate articles maximum length tokens summary maximum length tokens format article summary pair format outlined figure model specications transformer decoder n blocks h masked self attention heads block set dimensionality self attention head dmodel stated use pretrained weights radford et al initialize parameters model special tokens added vocabulary e end token start token delimiter token initialized sampling standard normal distribution model source embeddings denoted transformer sm train ablation transformer lm use source embeddings training details models trained learning rate minibatch size domain adaptive training dat train epochs dat additional epochs end task training ett dat train end task epochs specied nal model trained dataset uses domain adaptive training end task training tune hyperparameters models trained pytorch huggingface implementation gpt trained model tesla training total epochs took approximately day clock time xsum cnn daily mail datasets days newsroom dataset source code publicly available generation perform generation beam search beam size use trigram trick beam search summary token generated decoding distribution yielded model processing input tensor concatenation article tokens delimiter token previously generated summary tokens evaluation evaluate system common summarization metrics measure unigram recall summary document similar measure bigram recall rouge l r l measure longest common subsequence summary document report length summary terms tokens produced dataset evaluation test set selected models largest score subset samples validation set com huggingface pytorch openai transformer lm com transformer abstractive summarization experiments cnn daily mail baselines report results models previously trained evaluated cnn daily mail dataset pgen pgen coverage models consist attentive rnn encoder decoders integrate ability directly copy article generating tokens pasunuru bansal extend work adding policy gradient training mixture rewards promote saliency entailment summarization copy transformer extend et al copy mechanism compress article relevant content summarizing chen bansal look performing content selection extract sentences document novel extractor model finally dca model uses multiple separate communicating encoders different parts document produce representations focused salient details table rouge results test set cnn daily mail best model results bolded model pgen pgen coverage rougesal ent rl summ copytransformer rnn ext rl dca transformer lm transformer sm r l length l automatic metrics report results automatic metrics table dataset main model transformer sm performs slightly worse state art models note model tends generate shorter summaries gold summaries shorter lower rouge recall performance figure investigate correlation rouge l scores summary length note minimum decoding length state art algorithms places baseline generated summaries length bins higher average rouge l performance transformer sm produces summaries length bins e tokens performance consistently beaten dca model ne tuned rl figure average rouge l summaries different length bins scatter plots correspond rouge l scores bin solid lines correspond number summaries bin table head head comparison test set outputs left dca sm right transformer sm analyses summaries cnn dailymail model non redundancy coherence focus dca t sm t sm overall human evaluation rouge scores negatively inuenced shorter average length summaries produced model clear shorter summaries correlated worse quality evaluate hypothesis perform human evaluation article summary pairs randomly sampled test set article model generated summaries presented workers amazon mechanical turk amt worker presented model generated summaries produced sm model dca model model workers asked select better summary different quality metrics celikyilmaz et al redundancy fewer ideas repeated coherence ideas expressed clearly focus main ideas document shared avoiding superuous details overall results presented table interestingly summaries transformer sm tently preferred humans evaluations dimensions compared dca models indicating transformer sm s lower rouge scores observed table necessarily correlated human judgments quality table rouge l precision r l p recall l r r l scores computed generated summaries input cnn dailymail articles removing stop words table ablation study training schedules cnn dailymail pt model initialized pretrained weights dat model uses adaptive training ett trained end task r l p r l r model pgen rnn ext rl dca transformer lm transformer sm l gold summary model t lm ett t lm t lm t lm t sm ett t sm t sm t sm r l efciency large improvements baseline models human evaluation gories non redundancy focus generally shorter summaries produced sm investigate transformer sm able efciently express key ideas document evaluate efciency model remove non content words generated summaries articles compute rouge score measure serves proxy rate ideas expressed summary found document report results table observe transformer sm reports comparable l recall scores baselines evaluated respect article despite producing summaries average shorter rouge l precision similar baseline models indicating summaries models indicate similar degree information relevance combined results table conjecture transformer sm able high precision scores models conrm despite conceived abstractive generators models display highly extractive behavior efciently express key ideas document models producing longer summaries yield higher rouge performance table additional tokens reect redundant unsalient information human evaluators penalize analysis domain adaptive training source embeddings approach involved gies efciently transformer language models abstractive summarization domain adaptive training source embeddings assess individual impact evaluate multiple training schedule permutations e combinations pretrained representations gpt model domain adaptive training impact source embeddings results table yield multiple interesting conclusions general domain adaptive training dat table provides clear improvement training directly end task irrespective pretrained representations similarly source embeddings t sm table provides repeated improvement t lm ablation surprisingly pretrained initializations dat source embeddings tandem performance drops slightly compared dat source embeddings note observation hold true xsum dataset conjecture extractive nature cnn dailymail dataset approaches redundant effects setting xsum study quality abstractive summaries best performed xsum dataset specically designed gold summaries extractive datasets table baselines report performance transformer sm dataset comparison baselines originally reported narayan et al attention based sequence sequence model pointer generator model capable generating words copying directly input pgen second pointer generator model coverage mechanism prevent repetition performing variant topic aware convolutional sequence sequence model t encoder decoder provided word topic document topic distributions obtained lda additional inputs nal baseline multi level memory network mmn applies attention multiple memory layers varying levels abstraction results report results table els signicantly outperform comparison lines variants rouge metric interestingly transformer sm achieves able improvement transformer lm model suggesting source embeddings domain adaptive training helpful target summaries abstractive examples model generated summaries xsum dataset illustrate provement baselines qualitatively table support results presented earlier model duces abstractive summaries provide focused information main points articles newsroom table comparison results xsum test set variants rouge model pgen t mmn r l transformer lm transformer sm finally report performance model newsroom dataset largest evaluation datasets large cost training transformer sm model evaluated baselines baselines report performance models released authors newsroom dataset models included attentive encoder decoder attn generator network pgen compared complex encoder decoder uses lstms encoder attention intra decoder attention pointer generation produce summaries compare multi level memory network mmn mentioned earlier authors baseline evaluated abstractive subset newsroom dataset table xsum samples baseline t model transformer sm gold summary articles shortened brevity capitalization manually added ease reading source source text article snippet ofcials said attack happened europa shopping centre capital minsk police later arrested year old suspect cut woman chainsaw hit hammer died attacked injured woman taken local hospital attacker brought chainsaw axe shopping centre t transformer sm man arrested suspicion attempted murder knife attack shopping centre central london teenage girl killed chainsaw attack shopping centre central russia police gold young man attacked people chainsaw axe shopping centre belarus killing woman injuring article snippet year old sweden striker s contract french champions expires summer linked manchester united la galaxy ac milan psg said ibrahimovic leaves greatest striker best players club s history t transformer sm paris st germain completed signing zlatan ibrahimovic paris st germain undisclosed fee zlatan ibrahimovic says leave paris st germain end season return club gold zlatan ibrahimovic leave paris st germain end season article snippet animal taken lathom pets aquatics ormskirk tuesday afternoon lancashire police said shop s owner said cctv showed man taking tortoise needs calcium supplements tank t transformer sm tortoise stolen pet shop puppy s pet shop stolen shop lancashire gold baby tortoise stolen pet shop lancashire table rouge results validation subsets validation set newsroom model pgen mmn transformer sm extractive r l mixed abstractive newsroom d r l r l r l table comparison results room test set rouge model attn pgen r l t sm results report results rouge style tomatic metrics table showing sm outperforms previous best model metrics interestingly model achieves highest performance increase baseline models rouge l metric usually considered strongly correlated strong summaries thermore analysis different validation subsets newsroom dataset table split level extractiveness gold summaries shows transformer sm performs better baselines proaches varieties summary types related work abstractive summarization large variety work exploring different methods neural abstractive document summarization attention mechanisms shown improve variety models motivating factors work pointer generator networks introduced et al shown increase summary veracity inspired tangential usage copy mechanisms transformers document summarization gehrmann et al works explored use reinforcement learning directly optimize summarization models rouge metric contextualized representations approach relevant recent work contextualized language representations pretrained large scale language corpora representations simply integrated ne tuned improved performance downstream tasks ssl cove elmo learned contextualized representations training rnn language models encoder decoders follow work extended ideas replaced rnn deep transformer trained learn language patterns large story dataset bert clearly extended idea transformers language modeling making encoded representations bidirectional adding new loss functions masked token loss sentence prediction loss accurate discourse representations recently expanded scale pretrained language models showed promising results zero shot tasks conclusion work introduce approaches effectively adapting pretrained language model sentations abstractive summarization domain adaptive training source embeddings uate effect approaches abstractive summarization testbeds cnn dailymail xsum newsroom achieve state art rouge l results showing superior human evaluation performance process rouge l metric abstractive summarization evaluation sensitive summary length allowing exploitable approaches use heuristics control summary length references asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents abstractive summarization naacl yen chun chen mohit bansal fast abstractive summarization reinforce selected sentence rewriting acl jianpeng cheng mirella lapata neural summarization extracting sentences words arxiv preprint andrew m dai quoc v le semi supervised sequence learning nips jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language understanding naacl sebastian gehrmann yuntian deng alexander m rush abstractive summarization emnlp max grusky mor naaman yoav artzi newsroom dataset million summaries diverse extractive strategies naacl karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend advances neural information processing systems pages byeongchang kim hyunwoo kim gunhee kim abstractive summarization reddit posts multi level memory networks arxiv preprint chin yew lin looking good metrics automatic summarization evaluation samples ntcir chin yew lin rouge package automatic evaluation summaries text rization branches peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser noam shazeer generating wikipedia summarizing long sequences iclr bryan mccann james bradbury caiming xiong richard socher learned translation contextualized word vectors nips ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based sequence model extractive summarization documents thirty aaai conference articial intelligence ramesh nallapati bowen zhou ccero nogueira dos santos aglar glehre bing xiang abstractive text summarization sequence sequence rnns conll shashi narayan shay b cohen mirella lapata nt details summary topic aware convolutional neural networks extreme summarization emnlp ramakanth pasunuru mohit bansal multi reward reinforced summarization saliency entailment acl romain paulus caiming xiong richard socher deep reinforced model abstractive summarization iclr matthew e peters mark neumann mohit iyyer matt gardner christopher clark kenton lee luke zettlemoyer deep contextualized word representations proc naacl alec radford karthik narasimhan tim salimans ilya sutskever improving language understanding generative pre training url amazonaws com openai assets research covers language unsupervised pdf alec radford jeff wu rewon child david luan dario amodei ilya sutskever language models unsupervised multitask learners abigail peter j liu christopher d manning point summarization pointer generator networks acl tian shi yaser keneshloo naren ramakrishnan chandan k reddy neural tive text summarization sequence sequence models arxiv preprint simeng sun ori shapira ido dagan ani nenkova compare summarizers target length pitfalls solutions examination neural summarization literature proceedings naacl workshop optimizing evaluating neural language generation neuralgen jiwei tan xiaojun wan jianguo xiao abstractive document summarization graph based attentional neural model proceedings annual meeting association computational linguistics volume long papers volume pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan n gomez ukasz kaiser illia polosukhin attention need advances neural information processing systems pages alex wang amanpreet singh julian michael felix hill omer levy samuel r bowman glue multi task benchmark analysis platform natural language understanding arxiv preprint yukun zhu ryan kiros richard s zemel ruslan r salakhutdinov raquel urtasun antonio torralba sanja fidler aligning books movies story like visual explanations watching movies reading books ieee international conference computer vision iccv pages reproducibility provide additional details relevant experimental environment data sources cnn daily mail dataset consists articles cnn daily mail article associated descriptive bullet point highlights similar previous work concatenate highlights create target summary article dataset newsroom dataset consists m article summary pairs scraped internet archive articles come set publishers cover diverse topics finally extreme summarization xsum dataset consists article summary pairs taken bbc summary single sentence long professionally written usually author datasets use splits dened original works proposed datasets large provide supplementary material provide pointers source code readme acquiring hyperparameters details important hyperparameters found section paper additional training hyperparameters found default parameters training script source code hyperparameter values selected ones suggested previous work transformer language models hyperparameter varied measured ablation e training schedules include source embeddings initialization source embeddings included hyperparameter explored different initializations initializing source embeddings zero vectors initializing source embeddings values sampled standard normal distribution initializing source embeddings values sampled normal distribution mean standard deviation equal half norm average norm pretrained embeddings gpt language model report experiments experimental process experiment run follows given model dataset trained model described paper minibatches compute rouge random persistent example subset validation set score model stopped rising previous checkpoint model generate summaries articles test set beam search decode summaries beam ran exactly evaluation run result include paper cnn com dailymail co bbc com transformer abstractive summarization
