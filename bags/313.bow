mind facts knowledge boosted coherent abstractive text summarization beliz gunel department electrical engineering stanford university stanford edu chenguang zhu michael zeng xuedong huang cognitive services research group microsoft redmond chezhu nzeng com abstract neural models successful producing abstractive summaries human readable uent models critical shortcomings respect facts included source article known humans commonsense knowledge produce coherent summaries source article long work propose novel architecture extends transformer encoder decoder architecture order improve shortcomings incorporate entity level knowledge wikidata knowledge graph encoder decoder architecture injecting structural world knowledge wikidata helps abstractive summarization model fact aware second utilize ideas transformer language model proposed encoder decoder architecture helps model producing coherent summaries source article long test model cnn daily mail summarization dataset improvements rouge scores baseline transformer model include model predictions model accurately conveys facts baseline transformer model introduction summarization task generating shorter text contains key information source text task good measure natural language understanding generation broadly approaches summarization extractive abstractive extractive approaches simply select rearrange sentences source text form summary neural models proposed extractive summarization past years current state art model extractive approach tunes simple variant popular language model bert extractive summarization task hand abstractive approaches generate novel text able paraphrase sentences forming summary hard task humans hard evaluate subjectivity considered ground truth summary evaluation recently neural abstractive summarization models proposed use lstm based sequence sequence attentional models transformer backbone architectures models integrate techniques backbone architecture coverage copy mechanism content selector module order enhance performance recent work abstractive summarization based reinforcement learning techniques optimize objectives addition standard maximum likelihood loss current neural abstractive summarization models achieve high rouge scores popular benchmarks able produce uent summaries main shortcomings conference neural information processing systems neurips vancouver canada respect facts included source article known humans commonsense knowledge produce coherent summaries source article long work propose novel architecture extends transformer encoder decoder architecture improve challenges incorporate entity level knowledge wikidata knowledge graph encoder decoder architecture injecting structural world knowledge wikidata helps abstractive summarization model fact aware second utilize ideas transformer language model encoder decoder architecture helps model producing coherent summaries source article long proposed method transformer transformer recently transformer architectures immensely successful natural language processing applications including neural machine translation question answering neural marization pretrained language modeling transformers xed length context results worse performance encoding long source text addition xed length context segments respect sentence boundaries resulting context fragmentation problem short sequences recently transformer offered effective solution long range dependency problem context language modeling introduced notion recurrence self attention based model reusing hidden states previous segments introduced idea relative positional encoding recurrence scheme possible transformer state art perplexity performance learns dependency longer vanilla transformers times faster vanilla transformers inference time language modeling tasks inspired strong performance transformer language model modeling long range dependency extend transformer encoder decoder architecture based transformer architecture words calculate attention scores multi head attention layer architecture shown figure based transformer attention decomposition compare attention decompositions vanilla transformer transformer equations attention computation query key vector segment matrix shows absolute positional encoding matrix token embedding matrix represent query key matrices transformer attention formulation rij relative positional encoding matrix trainable parameters trainable parameters avanilla wkexj eexj axl wkuj wkuj rrij eexj rrij wkexj overall transformer architecture shown segment transformer layer denotes stop gradient denotes concatenation refer readers original transformer paper discussion new parameterization attention calculations details design decisions architecture rrij rrij important note vanilla transformer fully connected forward network layers multi head attention layers residual connections sublayers followed layer normalizations layers omitted figure simplicity empirically observe coherent articles transformer encoder decoder architecture compared transformer baseline figure shows comparison input source article sampled cnn daily mail dataset wikidata knowledge graph entity embeddings wikidata free open multi relational knowledge graph serves central storage structured data services including wikipedia sample wikidata million entities million relationship triples learn entity embeddings sampled entities popular multi relational data modeling method simple powerful method represents relationships fact triples translations operating low dimensional entity embedding space specically minimize margin based ranking criterion entity relationship set norm dissimilarity measure shown equation set relationship triplets entities set entities represents relationships set relationships construct corrupted relationship triplets forms negative set margin based objective replacing head tail relationship triple random entity low dimensional entity relationship embeddings optimized stochastic gradient descent constraint norms entity embeddings unit sphere important order obtain meaningful embeddings denotes positive margin hyperparameter model architecture overall model architecture shown figure extend encoder decoder architecture entity information effectively incorporated model encoder separate attention channel entities parallel attention channel tokens channels followed multi head token self attention multi head cross token entity attention decoder multi head masked token self attention multi head masked entity self attention multi head cross attention encoder decoder respectively finally layer multi head token attention followed feed forward layer softmax output tokens multi head attention conducted based transformer decomposition section entity linker modules use shelf entity extractor disambiguate extracted entities wikidata knowledge graph extracted entities initialized pretrained wikidata knowledge graph entity embeddings learned transe discussed section entity conversion learner modules use series feed forward layers relu activation modules learn entities subspace corresponding tokens text experiments dataset evaluate models benchmark dataset summarization cnn daily mail dataset contains online news articles words average paired multi sentence summaries words average use standard splits include training pairs validation pairs test pairs anonymize entities instead operate directly original text truncate articles tokens summaries tokens train time tokens test time preprocessing remove case higher quality entity extraction entity linking module quantitative results evaluate model baseline based rouge metric compares generated summary human written ground truth summary counts overlap grams figure model architecture stands positional encoding single encoder decoder layers shown parenthesis multi layer architectures layers curly brackets stacked grams longest common sequence rouge use pyrouge package obtain scores report scores rouge types baseline vanilla transformer encoder decoder architecture commonly backbone architecture abstractive summarization models baseline proposed model use transformer layers heads utilize beam search decoding use dimensions entity token embeddings bertadam optimizer minimum sentence generation length hyperparameter search set learning rate dropout rate beam width maximum sentence length inference start entity extraction decoder produces tokens results cnn daily mail dataset shown table model improves baseline points rouge points test set fact better improvements test model higher entity density slice test set demonstrated table specically model improves baseline points rouge points test set article summary pairs entities source article include results table initialized entity embeddings randomly test benet wikidata entity embeddings random entity embeddings decreased model performance wikidata entity embeddings increased model performance vanilla transformer transformer backbone architectures supports hypothesis injecting structural world knowledge external knowledge bases abstractive summarization models improves model performance table results cnn daily mail dataset abbreviation rouge model transformer baseline transformer entity random entity emb transformer entity wikidata emb transformer entity wikidata emb model table results cnn daily mail dataset high density entities abbreviation rouge ent denotes slice test data entities source article model ent ent ent transformer baseline model qualitative results conduct qualitative analysis model predicted summaries include samples figure compare transformer baseline output output model sampled input article cnn daily mail corpus baseline model makes factual errors based manual fact checking mcclaren paul clement years old time article published steve mcclaren paul clement worked manchester united boss hand model respects facts incorporating world knowledge wikidata knowledge graph based manual fact checking paul clement working real madrid appointed manager derby county england boss broad work chelsea figure compare transformer baseline output output transformer decoder model entity integration order test effect architecture summary coherency baseline model produces incoherent summary transformer encoder decoder model outputs coherent human readable summary ground truth summary steve mcclaren expected newcastle job derby rams currently battling championship promotion play offs paul clement leading candidate job derby formal contact real madrid mcclaren leaves transformer baseline output steve mcclaren leading candidate replace steve mcclaren year old established reputation european football leading coaches recent years working mainly carlo ancelotti manchester united boss keen secure promotion premier league season output model paul clement leading candidate replace steve mcclaren derby county england boss established reputation europe leading football coaches recent years clement currently real madrid coach figure comparison transformer baseline output output proposed model ground truth summary sampled cnn daily mail summarization corpus baseline model makes factual errors model respects facts incorporating entity level knowledge wikidata knowledge graph transformer baseline output wayne oliveira scored goals seven games oliveira oliveira recovered training ground ankle injury oliveira says happy injured gives chance gomis ruled weeks injured oliveira believes bafetimbi gomis form seven swansea appearances transformer output portugal striker ruled weeks nelson oliveira sidelined weeks injury scored goals seven matches recovered training ground injury year old swansea debut home defeat chelsea january figure comparison transformer baseline output output transformer decoder model output source article sampled cnn daily mail summarization corpus baseline model produces incoherent summary transformer encoder decoder model outputs coherent human readable summary discussion future work present end end novel encoder decoder architecture effectively integrates entity level knowledge wikidata knowledge graph attention calculations utilizes ideas encode longer term dependency performance improvements transformer baseline resources terms number layers number heads number dimensions hidden states popular cnn daily mail summarization dataset conduct preliminary fact checking include examples model respectful facts baseline transformer model similar previous works abstractive summarization rouge metric representative performance terms human readability coherence factual correctness rouge denition rewards extractive strategies evaluating based word overlap ground truth summary output model summary metric exible rephrasing limits model ability output abstractive summaries important note ground truth subjective abstractive summarization setting correct abstractive summary source article believe nding metrics representative desired performance important research direction finally believe entity linking end end training instead separate pipeline beginning possible lose valuable information entity extraction disambiguation chosen knowledge graph references asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents abstractive summarization proceedings naacl conference romain paulus caiming xiong richard socher deep reinforced model abstractive summarization proceedings iclr conference abigail peter liu christopher manning point summarization pointer generator networks proceedings acl conference ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez ukasz kaiser illia polosukhin attention need advances neural information processing systems pages caglar gulcehre sungjin ahn ramesh nallapati bowen zhou yoshua bengio pointing unknown words proceedings acl conference ramesh nallapati bowen zhou cicero dos santos caglar gulcehre bing xiang abstractive text summarization sequence sequence rnns computational natural language learning zhengyan zhang han zhiyuan liu xin jiang maosong sun qun liu ernie enhanced language representation informative entities proceedings acl conference zihang dai zhilin yang yiming yang jaime carbonell quoc ruslan salakhutdinov transformer attentive language models fixed length context proceedings acl conference sebastian gehrmann yuntian deng alexander rush abstractive summarization proceedings emnlp conference yang liu fine tune bert extractive summarization arxiv qingyu zhou nan yang furu wei shaohan huang ming zhou tiejun zhao neural document summarization jointly learning score select sentences proceedings acl conference jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language understanding proceedings naacl conference karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend neural information processing systems antoine bordes nicolas usunier alberto garca durn jason weston oksana yakhnenko translating embeddings modeling multi relational data neural information processing systems wikidata chin yew lin rouge package automatic evaluation summaries text summarization branches acl workshop pypi python org pypi jianpeng cheng mirella lapata neural summarization extracting sentences words proceedings acl conference
