conducting sparse feature selection arbitrarily long phrases text corpora focus interpretability luke miratrix robin ackerman july disclaimer analyses opinions conclusions expressed paper necessarily reect views united states department labor note paper accepted statistical analysis data mining proofed version abstract propose general framework topic specic summarization large text pora illustrate analysis dierent contexts osha database fatality catastrophe reports facilitate surveillance patterns circumstances leading injury death legal decisions workers compensation claims explore relevant case law summarization framework built sparse classication methods compromise simple word frequency based methods currently wide use heavyweight model intensive ods latent dirichlet allocation lda particular topic interest mental health disability carbon monoxide exposure regress labeling uments high dimensional counts words phrases documents resulting small set phrases found predictive harvested summary branch bound approach method extended allow phrases arbitrary length allows potentially rich summarization discuss focus purpose summaries inform choices tuning parameters model constraints evaluate tool comparing computational time summary statistics resulting word lists methods literature present new package textreg overall argue sparse methods oer text analysis branch research considered context keywords concise comparative summarization sparse classication regularized sion lasso text summarization text mining key phrase extraction text classication dimensional analysis normalization introduction regularized high dimensional regression extract meaningful information large text corpora producing key phrase summaries capture specic set documents interest dier baseline collection text summarization approach called concise comparative summarization ccs underscoring fundamental tures tool comparison class documents baseline complete set order remove generic terminology characteristics overall corpus resulting production short easy read summary comprised key phrases summaries useful understanding makes document collection distinct inform media analysis understand incident reports investigate trends legal decisions classic methods text summarization tend focus single words short phrases approaches latent dirichlet allocation extend naturally phrases hand regression based method allow longer phrases allow rescaling counts phrases text based overall frequency appearance phrases negatively impact quality resulting summaries paper merge ccs approaches allow rescaled arbitrary length key phrases include gaps briey discuss new ccs tool easily new package textreg allows rapid exploration text corpora gigabytes size given tools researcher desires conduct specic analysis faced choices particular implementation regularization regression ways impact choosing ways foci paper particular argue researcher specic goals interpretation mind goals inform choice tuning parameters example faced corpus documents interest rest baseline researcher choose allow positive weights phrases order simplify interpretation similarly choice tuning parameter governed researcher level interest pruning rare phrases oer method testing signicant relationship text labeling provides threshold regularization value compare tool related state art methods compare multinomial inverse regression mnir text regression method primarily signed distributed cores order able handle massive data compare classic linear lasso approach similar method run pre computed document term matrices exibility nally compare original ifrim method building blocks work comparisons investigate computation time prediction accuracy dierent features resulting word lists dierent approaches dierent types lists hope work gives guidance practitioner sort options case study use tool examine large collection occupational fatality catastrophe reports generated occupational safety health administration osha united states motivating example examine hazardous exposure methylene chloride neurotoxin bathtub renishing operations osha national institute occupational safety health niosh jointly issued hazard alert calling attention recurring pattern nature following deaths workers related circumstances sheer volume information describing occupational fatalities catastrophes initially obscured pattern years preceding detection osha maintains database narrative reports describing fatalities catastrophes fat cats similar patterns preventable exposure occupational hazards dicult identify eciently manual review given large number narratives database methylene chloride case study consider text mining techniques help identify important patterns circumstances hazardous exposures framework summary list key words phrases ideally represents reveals overall content collection narrative reports example summary narratives related methylene chloride contained words bathtub stripper qualitatively evaluate tool manually examine words phrases context original reports consider text summarization tool eectively characterizes circumstances bathroom renishing fatalities general explore construct text mining algorithms applied entire corpus uncover needles haystack patterns connection bathroom renishing overexposure methylene chloride stage focused rates relative risks particular patterns instead focused crude detection textual patterns represent meaningful information certain types injuries fatalities occur ndings involve recorded deaths injuries facilitate prevention future fatalities particularly context emerging hazards examine tool ability extract information collection legal cisions employees compensation appeals board ecab handles appeals determinations oce workers compensation programs owcp department labor dol investigate information extract ferent categories cases particular examine cases involving question work environment caused mental health condition emotional condition parlance ecab ccs tool extract meaningful information relating cases interest work needs obtain nuanced summaries overall ccs approach allow exploration text extract meaningful information extending earlier xed length phrase tools allow longer phrases phrases gaps increase expressive capability summaries methods picking tuning parameter possibly bit aggressive conservative provide alternative paradigm data analysis eye extracting human meaning text overview summarization paper extends concept concise comparative summarization ccs discussed incorporating prior approach proposed result overall improved ology concise comparative summarization involves comparing pre specied set uments baseline set think regularized regression labeling documents normally collection possible summary key phrases example analysis label documents relating incidents involving bon monoxide remaining documents potential key phrase feature considered covariate regression principle represented vector measures feature presence appearance count documents sparse regularization small subset possible summary key phrases selected phrases taken nal summary example resulting phrases related regression ideally indicate dierent related events compared workplace injuries fatalities root taking phrases useful classifying set documents given labeling summary positive set documents compared negative baseline set worth emphasizing focus predictive quality selected features object interest quality measured usefulness original human interpretation based question motivated exploration methods principle require human analog experiments validate ndings examples text classication tools classication byproduct possible choices implement regression including use logistic linear regression rescale frequency appearance phrases regression prior work shown rescaling phrase frequency important failing appropriately result summaries little informative content predictive accuracy maintained surprising term frequency text known concern data mining large text corpora rst illustrated information retrieval literature text classication extension key phrase extraction text classiers desire allow phrases unigram single word features approach calculate use phrases words long overall feature set long phrases quickly intractable blowup number possible phrases corpus contain solve problem ifrim allow arbitrary length phrases generating features optimization added benet approach easily allows gaps phrases wildcard words greatly enhances potential expressiveness resultant summaries ifrim algorithm based work earlier elastic penalized logistic regression features consisting entire space phrases examples regression text overview elastic nets regularized regression methods general ifrim initially propose algorithm solve penalized logistic regression arg min log feature vector document cij binary indicators presence feature document class label number features including phrases regularization tuning parameter regularization function later extend allow alternate loss functions hinge loss allow rescaling features modifying methods rescaling incorporated overall approach allow intercept term introduce diculty summarization process number positive features close extend algorithm allow non penalized intercept term discussed implement modications extending code wrap resulting algorithm new package textreg easier use rapid exploratory manner provide useful tools visualizations help researchers understand resulting summaries core idea algorithm greedy coordinate descent coupled bound algorithm step algorithm searches entire feature space feature highest gradient current obviously large search space pruned relationship bounds size gradient sub phrase known calculation parent phrase search track current optimal feature new feature considered bound children feature smaller current optimum prune children search related work ccs distinct classication classication focused sorting documents attributing authorship counting types news events text classication attempted wide array machine learning methods naive bayes linear discriminant analysis support vector machines easily allow large numbers features words phrases incorporated comparisons dierent methods text svms approach particular text book methods evaluations features primary interest classication instead attempt extract meaning documents contrasting sets similar key phrase extraction literature right example interpreting text dicult task variety ways example use text predict roll votes lda latent dirichlet allocation algorithm order understand language law correlated political support model political text explore dominates policy debates truly validating nding generally dicult requires great deal eort properly papers illustrate tools primarily intended exploration validation ccs scope additional human validation eorts alternative techniques approaches text analysis variations lda particular cently widely investigated consist bayesian hierarchical model describe documents corpus mixture number dierent distributions words reveal structure corpus shown capture human meaning generating novel models specic circumstances dicult mild changes model technically challenging consist entire research topic right computationally expensive solved mately variational methods spirit diversity research approaches dierent path sparse regression methods text new example use sparsity model topics representing small collections phrases stand background showcase methods sparse pca investigate large corpora aspect approach believe novel allowing complex features form overlapping phrases especially phrases wildcard words maintaining ability rescale features allows great exibility expressiveness possible summaries generated obvious naturally extend methods lda rely generative model words picked distribution rescaled gram regression initial methods regress vector counts cij phrase appears document binary indicators appearance elements cij indicating presence phrase document problematic common phrases obviously usually end having higher variance common ones easier pick random uctuations section discussion rescaling features correct pointed particular rescaling transforms vectors new covariate vectors xij cij similar standardizing columns lasso regression phrases high variability need smaller coecients similar impacts prediction makes cheaper regularization appear frequently random chance feature space standardized phrase having length regress rescaled intercept high dimensional problem want small number phrases use sparse regularization penalty use squared hinge loss obtain denoting maximum loss function penalized prediction standpoint wish fall short quadratic loss loss zero overshoot zero loss penalty predicting document label use squared hinge loss similar lasso shown eective monotonic needed optimization algorithm note penalty term include intercept generalize framework taking notation ifrim let loss vidual document use monotonic loss functions squared hinge loss similar ols type penalty logistic regardless choice loss term expressed original counts cij obtain arg min alternatively letting penalty term regress counts gives identical loss seen considering rescaled columns scaling column penalizing associated ties weighted lasso approaches adaptive lasso feature specic penalties gradients change aect optimization appendix solving equation greedy coordinate descent algorithm greedy coordinate descent repeatedly feature highest gradient mize corresponding line search loss function convex problem converge global maximum iteration decrease gradient coordinates maximum proof cache non zero features model need calculate store possible features main computational cost algorithm nding feature largest gradient dynamically generate features exploiting nested structure multiword phrase having smaller phrase prex inner algorithm shown algorithm rst examine unigrams bigrams forth eligible phrases rst calculate gradient unigrams enter queue phrases queue placeholders family superphrases pull phrase queue check prune children determine phrases children calculate gradients children nally enter queue algorithm work pruning able prune zero children feature examining achieve large speed ups pruning possible trick bounding child gradient based structure parent rescaling makes keeping bound tight dicult original ifrim presentation main idea bound gradients family features magnitude current best gradient prune discuss nding bounds algorithm greedy coordinate descent eatures converged ndhighestgradient eatures end algorithm ndhighestgradient eatures non zero features far bestf arg maxf eatures unigrams dictionary queue queue features check bestf end add end bestf bestf end add end end end bounding gradients feature corresponding appearance pattern documents feature feature prex know cki cji write know cki counts given phrase phrase phrase prex count vector bounded phrase count vector search consider phrases shorter longer phrase based phrase appearance pattern text calculate bound magnitude highest gradient best case hypothetical superphrase prex smaller current best achieved gradient prune consideration phrases phrase prex superphrase want small possible tight phrases easier prune derived appendix overall bound max max phrase bounds computed summing documents phrase rendering computationally tractable rescaling allows theoretically predictive relatively rare phrases bounds unfortunately loose making hard substantially trim search tree possible avenue improvement integrate preprocessing step suggested special cases gives bound maximum document phrase gives bound maximum sums negative positive documents containing phrase ifrim bound corresponding rescaling scaling maximum occurrence phrase single document easily extended elastic net note appendix choices rescaling additional constraints choices rescaling rescaling restrictions tion problem focus ccs tool dierent aspects summary seek generate summaries general specic words example enforce contrast target set larger background set eases interpretability discuss following subsections rescaling phrases vary greatly overall appearance text long tail words phrases appear nearly document bulk phrases appearing times phrase rate appearance connected underlying variance represent phrase count vector cause problems selecting meaningful phrases particular common phrases easily dominate greater variance typically handled stop word lists lists words priori deemed low information dropped analysis thorough discussion discusses stop word removal nicky general imperfect easily adapted diering contexts languages furthermore implement stop word removal phrases object interest unclear rescaling serve function stop word list superior job rescaling critical widely known information retrieval rescaling stop words easily selected virtually text mining procedures stop words dropped typically runner common phrases selected primarily random variation appearance pattern test nearly shelf text mining tool removing stop words rst methods fail results dominated low information words stop word lists hard threshold solution soft threshold tapering appropriate rescaling oers tapering approach question rescaling use alternatively tapering want use rescaling oers class choices integrates gradient descent algorithm rescaling dierent choices weight phrases relatively dierently allowing focus common uncommon phrases desire researcher overall lower means generally higher normalization factors change appropriate regularization main point concern relative sizes weights rare phrases compared common phrases general rescaling heavily penalizes common phrases rescaling hand rescaling penalizes rare phrases slightly lower choices illustrate figure consider sequence phrases appear documents dierent series weights rescaled phrase appears time times weight choices idf rescaling related strategy rescaling idf rescaling information trieval typically like variations exist xij log cij total number documents containing phrase diers corrects document length documents roughly length relevant idf puts extreme dierence weights rare common phrases scaled total number documents rare mid range phrases idf rescaling similar rescaling large figure impact dierent choices rescaling dierent rescaling factors phrases single appearance documents total documents common phrases penalized greatly relative rare phrases interpretation negative coecients ccs returns list phrases non zero coecients interpreting coecients dicult ols model based interpretation changing feature change prediction holding features xed given normalization means changing feature count change outcome given lack motivated model context interpreting magnitude coecients somewhat dubious nonetheless wish interpret sign coecients positive indicates feature associated positive class documents negative indicates negative class negative group baseline object direct interest especially case larger diverse positive class case regression ideally subtracting baseline characteristics leaving researcher makes positive class distinct noteworthy interpreting negative coecients dicult interpretation features conspicuous absence unfortunately mix positive negative features end unclear interpretations sign holding features constant aspect example negative weight feature osetting positive weight highly correlated alternate feature fact features positive correlation labeling case interpreting negative sign conspicuous absence erroneous accurate feature conspicuous absence given normal association second feature hard communicate solution extend optimization consider set positive forcing negative coecients exist easy extension algorithm simply rescaling results different drop lower bound gradient search truncate line search update negative features useless example allow negative features coecients positive suggest positive group clearer signal negative group phrases found positive group dierentiating groups suggest distinct language use larger vocabulary specic turns phrase positive group interest right picking regularization parameter regularized regression settings picking regularization parameter riously dicult problem general higher lead fewer features concise summaries low summaries verbose overly low allows context means obtaining features detected soley random uctuations appearance patterns phrases need ensure suciently large prune noise classically selection methods cross validation optimize prediction accuracy sample predictions prediction primary focus look methods select enhance quality interpretability maries generated lack appropriateness prediction accuracy somewhat motivated literature prediction accuracy example model selection illustrated choice aic bic selection methods regression present methods rooted goals ccs select rst conduct permutation test select gives statistically signicant summary summary non indicates presence systematic dierences text positively negatively marked documents second select minimum guarantee pruning rare phrases discuss select approach use discussion case studies permutation test summary wonder phrases returned ccs simply random chance dierent phrases reasonable believe randomly associated document labeling control permutation test exact test resulting value easy interpret test meaningful generate summary repeatedly randomize labeling documents regress corresponding zeros features given random permutation labels gives null distribution appropriate signal finally compare originally observed obs distribution fake calculate value obs small conclude needed regularization zero originally observed signal greater random signal relationship labeling text similarly pick percentile permutation distribution condent resulting summary non relationship labeling text random chance individual phrases specically tested signicant possible change example given mild perturbations data theless test provides useful minimum bound nal lower bound result non summary purely random chance similar manner check coherence nal summaries generating maries dierent permutations labeling potentially adjusting ation similar length summaries permutations creating list lists humans reliably pick actual actual summary randomly inserted summary fake ones indication structure summary random chance idea based assessing quality eda visualizations pruning rare phrases potential phrases rare showing handful times large corpora selecting phrases introduces severe multiple testing problem seek appropriately regularize regression solve particular rare phrases times selected happen fall positive set generally improper rescaling features term shows positive examples high count times negative examples low count selected contrary interpretive goal selecting predictors want phrases general summaries informing researcher aspects multiple documents problems partially remedied proper selection tuning parameter investigate minimal guarantee rare phrases dropped investigating called perfect predictors consider feature cij cij documents moment assume multiple counts document perfect predictor predicting cij positive documents perfect predictors identify subset positive examples incurring loss negative examples cost including predictor regularization term set high cost prohibitive select fact cut phrase suces appendix derivation prediction document hypothetical perfect predictor documents pruned comparison table shows case positive negative examples case roughly equal numbers positive negative documents needed cut cuto generally overly aggressive predictors predict documents gain including perfect predictor potentially table needed prune rare perfect predictors rescaling discussed end section rescaling bad appropriately handling common phrases rescaling bad appropriately handling rare ones connection innity norm row table rescaling features makes dicult prune perfect predictors singleton predictors special case singleton predictors appear entire corpus appear positive example normally rare phrase appears positive example pruned described regardless order remove singletons predict document predictors generalized somewhat consider count phrase single positive document count negative documents phrase normalizing constant cki single positive document xki xki negative documents approximately singleton phrase circumstance pruned proper selection tuning parameter better approach pruning cutting dropping phrases low counts ignoring computational issues prune near singleton phrases high counts circumstance arises study fat cats corpus word lion times report involving plague ridden mountain lion corpse positive example times negative examples kept predictor nal summary disease low regularization cross validation traditional form selecting tuning parameter cross validation metric predictive performance optimized example context calculate predicted labeling set aside documents select average squared distance predictions actual minimized tends longer lists interpretable important signal buried relevant terms generally predictive performance directly measure primary focus interpretability signicance selected phrases regularization early stopping elastic net original ifrim method includes penalty term loss function regularizes stopping convergence dierent choices aect resulting model early stopping easy way obtain list specied length quickly list non sensical obviously good ifrim initial paper fact penalty term loss function entire regularization early stopping relationship forms regularization unclear early stopping clearly great computational advantages need convergence checks simply stop found features interest unclear example approach successfully prune rare phrases computational comparisons compare ccs tool methods studies rst compares running time general characteristics nal word lists generally default values recommended approaches setting tuning parameters second compares prediction accuracy methods study examine ccs tool dierent choices data use fat cats corpus ecab corpus describe fully case studies section methods main comparison method multinomial inverse regression mnir text sion method primarily designed distributed cores order able handle massive data parallelizes regression conducting individual inverse regressions counts phrases feature space case binary labeling regularized giving sparse feature vector recommendation mnir tuning use aic bic penalization selected bic interested interpreting covariates quality prediction bic known superior model recovery compared aic generally textir package mnir resulted long lists thousand words truncated list taking words weights practical use advocate greater regularization cross validation restrict list compare classic linear lasso approach earlier work shows gains logistic linear minimal computational expense large use glmnet package selecting regularization parameter cross validation rmse test error standard package automatically standardizes columns method uses rescaling generally lasso lists short words truncated nally compare original gram method ifrim package implementing approach replicated method binary features option rescaling columns direct comparison select tuning parameter permutation approach initially advocated early stopping regularize later work package extended modied package introduced direct regularization rst studies normalization method select ways permutation approach discussed xed prune perfect predictors fewer documents allow gaps phrases upper bound phrase length permutation permuted times took maximum permutations vary ran trials primarily cleaned fat cat dataset stemmed porter ming package section details methods data stored cleaned stemmed text line responding single document rst methods generated document term matrix text dropping terms appeared fewer times matrix manageable resulted unigrams bigrams trigrams documents total ran lasso twice rst unigrams second unigrams trigrams mnir use unigrams number tures generated expanded trigrams computationally prohibitive single computer obtain labeling selected random sample keywords associated fat cat reports weighted frequency keywords appearance form labeling schemes evaluate keyword dropped phrases eration feature phrases carbon monoxide carbon monoxide keyword dropped passing words banned words algorithm dropping relevant columns document term matrix understand computational timing replicated rst comparison study ecab legal decisions discussed judges overlapping subsets decisions compared judge baseline labeling primary interest case study rst computational comparison ran methods data compared runtimes characteristics nal word lists second computation comparison set aside random data predicted labeling set aside data method lasso ifrim method produce nominal predictive scores overly low massive imbalance positive negative classes cut point classication logistic regression model predictive value came method training set classied determining predicted log odds mnir method rst projected results inverse regression described resulting score covariate logistic regression modeling step calculated auc scores raw predictive scores methods investigation impact dierent choices rescaling method generated word lists keywords calculated average length average frequency words resulting lists upper bound phrase length simulations run macbook pro ghz intel core processor memory solid state drive reproducible code simulations available request comparison results labeling method calculated dierent statistics word lists timed total time generate list table average measures labelings general trends evident list characteristics overall resulting lists dierent character culated average list length lists truncated terms calculated mean median frequency words gauge common rare selected phrases nally averaged means medians dierent labeling runs dierent methods dierent terms scores mnir gives long lists specic words probably inverse regression selects phrases relevant regard correlation structure lasso tends longer lists average ance selected words par gram feature rescaled regressions lasso access trigrams highly targeted specic median frequency plummeted pentagrams went expected ifrim method column rescaling selects general terms median frequency osha dataset permutation values tended close xed giving overall word lists similar ecab dataset longer documents permutation selected higher word lists shorter greater regularization resulting words typically general overall clear practitioner use dierent methods dierent types lists believe generally wants short lists phrases phrases overly general runtime runtimes generally comparable osha data widely dierent ecab data discuss osha data rst lasso method including cross validation step comparable textreg method respect time fast implementation robust wide feature matrices note average time trigrams unigrams ifrim method faster textreg methods likely improved pruning mnir fast event designed parallel systems ran single core generally computational times generate summaries comparable mnir trigrams workable mnir linear number features blow features time increase course multiple machines parallel structure avoid surprised time statistics insensitive number terms glmnet package investigate calculated document term matrix phrases words giving unique terms giving extra row table average runtimes lasso increased modestly mean seconds computational times spread ecab data half table substantially increased time moving unigrams trigrams lasso timing textreg methods exploded investigating time dierential found selected phrases words furthermore runs reached maximum number iterations convergence indicating surfaces weakness greedy coordinate descent discuss overall potentially use boilerplate language legal writing long informative phrases wish summaries pre generation candidate phrases prohibitively expensive mnir method selected unigrams unigram apparently dierence use judges selected bic penalization times lasso mnir include time generate document term matrix minutes grams minutes trigrams method ifrim lasso lasso trigrams lasso pentagrams mnir textreg textreg xed ifrim lasso lasso trigrams mnir textreg textreg xed time avg sec list length avg word freq avg median table results comparison study osha reports ecab legal decisions runtime include time generate document term matrix lasso mnir predictive accuracy results assess predictive accuracy macro averaged statistic calculated trials averaged table methods score test set documents positive giving undened scores indicated table figure shows scores component precision recall auc scores keywords methods dened predictive accuracy comparable methods text regularization suer regularization permutation method overall predictive accuracy low recall particular tends run methods lasso trigrams noticeably superior examined auc scores cost textreg regularization conservative classication reduces sensitivity greatly lowering roc auc scores auc scores deceptively high imbalance positive negative examples generally superior performance lasso trigrams indicates rescaling tures coupled richer feature set multiple words useful prediction tasks methods results indicate regularization detrimental prediction primarily diminished ability predict positive documents positive shown recall scores method mnir lasso lasso trigrams ifrim textreg missing table scores dierent methods averaged keywords examined standard deviations number keywords dened indicated selection results anticipated dierent values produce lists dierent quality results table higher corresponds lists relatively common words phrases low duces lists tend phrases words list half phrases words longer found length list increased longer lists common words low algorithm selected targeted phrases regularization parameter comparable recalculated permutation approach run value mean values shown table reference generally rule thumb worth remembering longer lists tend general terms happen adjusting rst simulation compare dierent example figure sample classication rates dierent methods lasso trigrams generally best substantial variability keywords methods failed return scores dropped charts substantively keywords included method textreg textreg textreg time avg sec list length phrase length word freq word freq median avg avg avg avg table word list characteristics dierent rescaling norms length length list mean size average number words selected phrases frequency refers phrase occurrence corpus mean average regularization value discussion overall method succeeds accommodating multiple word phrases allowing intercept rescaling features shown extensions critical generating manageable lists phrases overly common picking control list length commonality words lists predicted initial discussion terms computational time method perform particularly compared lasso glmnet package especially considering lasso overall time includes cross validation method similar lasso regression pre constructed phrase matrix diering uses hinge loss instead squared loss naturally ask alternative approach simply generate document term matrix use glmnet entirely convinced discuss llllllllllllllllifrimtextregmnirlassolasso generating document term matrices including phrases wildcards computationally tedious especially imagine extensions optionally included words expensive time memory grows increasingly number possible phrases minutes generate grams times stored matrix twice size initial raw text second hinge loss dierent classic loss core dierences overall behavior area future investigation speed glmnet package partially lars approach entire regularization path computed arguably package particularly developed optimized eciency similar ways substantially speed package method competitive example potential slow determined examining convergence paths runs initial selection small list phrases slow hill climb step dierent selected phrase selected adjustment comes coordinate descent optimal gradient path angle following requires small steps associated coordinate directions trace path unfortunately step algorithm conducts search highest gradient phrases expensive instead phrase selected maximum point given set phrases selected point new phrase introduced algorithm steps eectively lars approach important area enhancement furthermore greedy ascent iteration involves intercept ratcheting features ratchet suggests possible direction forcing intercept corpora rare positive labeling rates estimating intercept corresponds predicting documents negative default distinct intercept predicts uncertainty default allowing intercept tends predict overall base rate default specifying intercept value save intercept update step optimization problem nding collection phrases positive documents positive weight aecting negative documents heavily regardless truly massive data especially number documents grows large approach lasso work instead methods allow easy parallelization mnir key area future exploration determine regularize mnir shorter lists induce lists common phrases case studies illustrate ccs tool case studies rst case study compare resulting summaries ccs methods discussed overview code generate results textreg package available cran appendix scripts data available website studies address harvard edu lmiratrix software textreg package similar tools example presenting results discuss data representation data representation cleaning choices preparing corpus statistical analysis common example convert text lowercase drop punctuation approach convert digits preserve presence numbers case informative convert hyphens blank spaces sequence hyphenated words coincide non hyphenated similar phrase possible single word analyses text analysis packages convert raw text matrix counts dropping stop words greedy coordinate descent algorithm unknown phrase length related generation features store text raw strings string document controversy stem documents tails words cropped collapse number possible words example clean cleaning cleaner cropped disadvantage making resulting text output somewhat dicult read especially considering phrases stemming lose textual meaning dierent suxes fact important context advantage particularly phrases collapsing dierent versions phrases provide stemming option enhance readability output append end stemmed words roots indicate potentially cropped provide tools wildcard search stemmed phrases original text recover examples complete phrases given context involves words known nearly priori meaning provide option custom short stop word lists list banned words prohibited summary phrase generally lists built iterative process rst summary generated contain words immediately recognizable inconsequential researcher pre existing contextual knowledge correlated labeling researcher drop words rerun algorithm repeat necessary way avoid case studies illustrate fat cats main investigation relies osha publicly available summaries occupational talities catastrophes fat united states summaries describe workplace incidents resulted death inpatient talization workers event occurs employer report osha course conducting resulting investigation osha generates address gov views recently expanded list reportable events include loss eye amputation inpatient hospitalization occupational injury illness reporting requirements naics update narrative report publicly available annotated set keywords categorize narrative reports terms specic chemicals involved machinery involved body parts aected salient features publicly available records primarily consist title short paragraph summary incident date incident involved fatality covariates concatenated title paragraph description form documents documents tend words long quantiles minimum length words longest report words stemming unique unigrams word stems appeared times appeared times investigate corpus keyword generate labeling narrative reports setting reports tagged keyword remainder ccs summarize collection reports marked keyword comparing reports ideally words common reports employee general work place terms leaving phrases identied set stand interpret summary distillation distinct category fat cats compared fat cats general periodically summarizing reports keyword interest researchers gain information emerging hazards trends circumstances hopefully resulting summaries faster read individual narratives contain hints general themes narratives chemical exposure area particular interest enhanced surveillance standing generated background comparison set documents identifying keywords deemed loosely associated chemical exposure dened chemical family narratives narratives labeled keywords allowed compare categories narratives ited context chemical family larger context types narratives changing background set highlights dierent aspects sets apart marked collection reports overview overall number narratives dierent topics interest table shows appearance pattern categories examined discuss methylene chloride carbon monoxide defer chemical reaction supplementary document table shows narratives involved fatality methylene chloride motivating example rst examined methylene chloride initially selected value ensure prune singleton perfect predictors section reports marked methylene chloride keyword running ccs reports returns words methylene chloride words interest immediately added words ban list reran table displays resulting summary comparing narratives narratives reporting revisions fed reg september methylene chloride carbon monoxide chemical reaction general fatality total chemical table number narratives dierent keyword labelings second pair columns restrict database reports related chemical exposure summarizer picks coherent theme reports bathroom nishing example encouraging given prior knowledge dangers methylene chloride utility ccs detecting unknown patterns remains seen select based percentile permutations obtain needed result null summary comparison obs conclude statistically signicant relationship text keyword presence banned words summary informative sponding summary succinct containing bathtub stripper contained picking statistical signicance appears drop informative phrases phrase bathtub paint stripper stripper contained reglazing bathtub remover contained stripping agent tub head phrase reports tag tag phrase table methylene chloride rescaling reports phrase total occurrence phrase reports number reports containing phrase tag number methylene chloride reports containing phrase tag percent phrase appearances methylene chloride reports phrase percent methylene chloride reports containing phrase summaries necessarily capture information tagged documents case example methylene chloride reports phrases table represented manual review reports revealed involved strippers tile oors furniture involved explosion terse referred methylene chloride gas carbon monoxide examined reports relating carbon monoxide asphyxiant odorless gas ran ccs dierent values rescaling examine impact dierent levels rescaling compared cases cases involving set keywords predetermined related sort chemical exposure narratives marked members chemical family reduce computational time limited attention phrases appear times corpus results table obtain results summarized iterative process words bon monoxide gas poisoning exposed exposure overexposed hemoglobin ppm levels partspermillion overcome eventually dropped removed specialized hyperbaric having ical intervention poisoning cohb abbreviation carboxyhemoglobin molecular complex hemoglobin carbon monoxide form body words appeared initial summaries technical obvious aspects poisoning reveal trends characteristics interest obscure desired results words appeared conventional stop word list fact correlated category way automatically removing nal results reect known patterns poisoning example exhaust gasoline propane powered engines major culprits exposures particularly combination poorly ventilated enclosed spaces appearance phrase cold room appears reect incidents propane fueled forklifts oor cleaning devices source carbon monoxide exposure cold storage areas ventilation poor investigating hospitalization found poisoning cases contained hospitalized versus chemical related narratives chemical narratives department mentioned nearly narratives versus baseline lower rates fatality poisoning cases involving fatalities compared chemical family reports non chemical family reports interestingly dead appears narratives compared remainder chemical family dierent rescalings dierent styles summaries smaller specic phrases gasoline blood appear positively marked documents larger phrases overall phrases appear higher rates positive negative class example employees appearances appears overall infrequent specic phrases relatively easy interpret common phrases patterns appearance striking employees example appears narratives versus baseline mere prisingly enclosed appears time versus baseline table contrasts carbon monoxide incidents labeled chemical related keywords compared cases set cases database summarized collection reports dierent baseline point comparison results table appendix broadly similar analyzed data stemming table appendix results broadly similar possibly harder read stemming collapses phrases helpful hampers human readability phrase propane powered blood gasoline powered concrete saw hour department measured cold operating propane propane operated gasoline propane powered powered forklifts fresh generator hour overexposure exhaust generator blood gasoline average found treated department hour source ventilation enclosed taken employees employees department propane hospitalized dead warehouse reports tagged tagged phrase table dierent summaries carbon monoxide compared chemical family narratives finally compare ccs methods mnir lasso ifrim approach methods returned summaries dierent lengths mnir lasso ifrim textreg truncate mnir lasso display lists taking union words list displaying weights truncation maximal overlap table words sorted frequency appearance corpus lists dierent mild overlap mnir generally targets rare phrases displayed ifrim general ones mnir restricted unigrams instance computational concerns overlap textreg mid range phrases rare phrases perfect predictors phrases included document term matrix rarity lasso mnir lists phrases similar meanings example ifrim general versions specic lasso textreg legal decisions context legal decisions motivating question eciently learn characteristics certain types cases extracting associated phrases topics corpus cases exploratory case study chose examine publicly available decisions employees compensation appeals board ecab siders appeals determinations oce workers compensation programs owcp department labor dol owcp handles compensation claims federal mnir lasso ifrim textreg phrase blood vanguard decatur mek twa carbonyl newton transient stratton num phrase table dierent summaries carbon monoxide comparing dierent methods workers injured course employment ebac handles appeals year case law particular area interest ecab handles compensation claims called emotional conditions cases challenging number interesting reasons example establishing employee condition mately caused workplace conditions requires analysis causation unique ways appropriate context physical condition probe potential utility css extracting useful information large bodies technical text performed exploratory analysis collection ecab decisions relate mental health conditions causality sought automated summaries reveal meaningful patterns decisions publicly available examined years scraping website address gov ecab decisions main html ended legal decisions documents generally ranging length words quartiles median length words shortest words longest unique words appear times appear times counts include case identiers character strings words attempt remove directly automatically labeled decisions sets dummy variables emotional condition discussion causality work relatedness injury labeled documents contained set handpicked key phrases tuned collection key phrases took random sample positively negatively marked documents conducted manual review labeling clearly perfect illustrated table ideally ccs method able produce relevant summaries despite noise missed labels possible specic types positive decisions systematically missed labeling discovery meaningful summary phrases suggestive labeling emotional condition emotional condition total cases causality work relatedness work relatedness total cases total correct sample estimated positive negative sense spec table manual review labeling quality legal decisions column table shows rst pass summary cases involve emotional condition revolve issues causality fairly general terms boilerplate language necessary explore raw text discover contexts phrases easily package example positively marked decision injury illness related claimant employment compensable case lillian cutler board explained distinctions type employment situations giving rise compensable emotional condition feca docket issued november emphasis added establish emotional condition arose performance duty claimant submit following medical evidence establishing emotional psychiatric disorder factual evidence identifying employment factors incidents alleged caused contributed condition rationalized medical opinion evidence establishing emotional condition causally related identied compensable employment factors ecab emphasis added illness cutler ecab requirement imposed psychiatrist incidents alleged caused lillian cutler compensable disorder factor employment factor employment covered reaction anxiety cutler ecab xxx xxxx depression lillian cutler compensable factor employment results environment administrative personnel requirement imposed allegations main ecab depression table summary cases involving emotional condition discussion causality dierent columns correspond number dropped phrases column emotion condition rest adding phrases column adds column depression column illustration purposes column includes terms illustration stability ccs consider columns table column corresponds dropping terms consideration note transition fourth column drops case references lillian cutler explicitly drop words phrases ccs selected phrases picked context phrases removed indicating case number lillian cutler longer selected phrases including parts phrase instead obtain cluster phrases showcasing dierent aspects cases dropping phrases aect summary phrases summary nal columns additional phrases summary column care taken understand complex dependencies phrases context ecab decisions ccs tool provides phrases plate language case citations degree phrases appear reect precedent common statements law characterize given category cases results exploratory inexact particularly revealing suggest rened ccs tool day facilitate development automated case content analysis aid development rened legal taxonomies discussion studies illustrate tools understand text dierent far precise activity working correctly classify text common problems machine learning approaches selecting methods selecting tuning parameters exacerbated uncertain area researcher left decisions vague guidance method decisions prominent method use selection regularization parameter method use selection normalization parameter need determine remove domain specic stop words picking selecting options especially include picking regularization parameter optimizing predictive performance option select dicult especially easy metric nal quality focus prediction computational investigations shed light problem ideally use maximum permutation approach rare phrase pruning approach guarantee nding summary found discard rare phrases speak general trends positive documents free test permutation selected phrases fact signicantly associated text real boon view moves presentation results known entirely noise said future work stability documents perturbed selected phrases change example furthermore acknowledge examples suggest permutation selected severe severe cross validation similar means lose human meaning illustrated methylene chloride example richness summary greater slightly reduced relaxing regularization achieved oriented approaches achieve longer lists informative improved prediction undermine guarantees provided work testing individual phrases false discovery rates better balance regardless compare nally chosen table degree discarding perfect predictors degree leave remainder potentially picked provides human interpretation impact regularization picking selecting admittedly dicult design gives dierent views data general specic advocate exploring range values best practice case studies example range terms tables provided complex rich story pooling lists exploring pooled lists way forward underscore view tools exploratory researcher extract small snippits text oer clue thorough investigation similar spirit example xgobi stop words avoid stop word lists rst pass approach needed generate specialized stop word lists way removed words human intervention prediction standpoint removed words key indicators given labeling unfortunately selecting occludes terms enhance human understanding calculated thresholds add words ban list plummets removing words correlated labeling eventually reach point conditioned connection labeling text potential avenue exploring data overall advocate generating modest length stop word ban lists substance matter knowledge coupled rescaling generic stop lists allow milquetoast words obviously wrong conclusions present method comparing sets documents simple sparse fast argue qualities important text analysis especially surveillance exploratory tasks simple means summaries technical nature example presence absence features easier interpret regression weights need sparsity humans lazy number phrases summary faster summaries computed better exploratory analysis discovery bogged argue results tools fact text analysis tools know taken ultimate proof particular substantive theme meaning summaries suggestive researchers need investigate substantiate suggestions alternatively secondary analyses blind scoring key phrases sentiment lead traditional statistical conclusions cases ccs viewed dimension reduction tool providing targeted small number informative features complicated form data technical eectively provided implementation lasso style gression set features dynamically created loss squared hinge loss normal quadratic loss work shows implementing sparse sion greedy coordinate oers viable direction summarization phrases words furthermore approach dynamically building features shows promise customizations skipping dropping words automatically detect related phrases collapsing single features admittedly work needs optimize particular implementation exactly fast currently algorithm sub optimal fully currently selected features iteration said compared methods pre computed design matrix comparably fast exible grams considered allows trickiness having gaps key phrases card words enforcement non negative weights additionally textreg package natural use allowing users avoid calculating phrase matrix instead works raw text easily allows customization dierent rescaling schemes going text analysis methods hint ways incorporating interaction terms features high dimensional regression phrase features simply interactions nearby word features similar bounding methods exist area future exploration acknowledgements like thank anonymous reviewers detailed comments paper improved feedback enormous thanks matt taddy invaluable speedy support textir package comparison studies work builds conversations ideas discussed statnews research particular thanks group berkeley led bin laurent ghaoui garvesh raskutti ideas impact dierent rescaling choices authors grateful opportunities inspirations thanks kevin portion code package janet ackerman collaboration earlier incarnation project initial data collection appendix appendix consists supplemental tables showing alternate summaries case studies discussed table compares carbon monoxide narratives narratives table demonstrates series summaries stemmed corpus reports tagged tagged phrase phrase gasoline propane powered oor blood hemoglobin cold room found powered forklifts gasoline powered propane powered overexposure exhaust fumes exhaust calculated generator employees treated evacuated department ventilation treated propane powered blood hospitalized found employees stratton headaches source gasoline passed enclosed department cold hours room table dierent summaries carbon monoxide comparing cases phrase cold xxx cold cold reports tagged tagged phrase table dierent summaries carbon monoxide stemming compared chemical family appendix derivations derivations work rst obtain bound gradient second alternate formulation loss function gives dierent approach nding feature maximal gradient obtain minimal ensure perfect predictors pruned bound gradient gradient phrase cij cij cij consider phrases phrase prex currently model currently maximize possible gradients similar argument gives bound negative gradients vectors let component wise relationship set contains potential appearance patterns phrase phrase prex wish calculate actual phrases optimize set potential phrases results optimization problem max max max norm vector weights inner product nonpositive rst term rst line negative second line follows setting document increases gradient simultaneously drop negative terms shrink penalty term negative examining gradients step negative direction indicated rst term gradient immediately shrunk include intercept xed constants determined current location optimization path eectively maximizing inner product vector weights overall bound assessing maximum possible utility hypothetical super phrase boils maximizing weights positive examples normalization renders problem dicult bound optimization following relationship inequality gives max cos cos cos max max angle coupling similar argument minimization gives overall bound note elastic net elastic net penalize loss function ifrim notation regularization tends groups correlated features picking borrow stability ridge regression potentially useful small features weak signals setting corresponds regularization problem gradient search changed subtraction zero potential new features gradients calculated features model extra term derivative second term alternate gradient formulation redening change optimization problem rescaling term penalty gives dierent bounds gradients super phrases based phrase changes gradients change path optimization problem dierent features initially largest gradient assuming true convergence nal solution identical particular dene loss term reexpressed gradient phrase czj czj consider phrases phrase prex maximize gradient yielding optimization problem czj max max second line comes noticing setting document increase gradient dropping negative terms shrinking penalty term negative step negative direction indicated rst term gradient immediately shrink consider phrases occurrences corpus roughly bound max maximizing terms separately rst simply add maximum weight regard normalizing constant normalizing constant given total count occurrences maximum putting singletons documents giving total similarly bounding gives overall bound max max cij cij bound appears useful presented main paper rescaling tend common phrases selected rst rescaling rst term allowing grow large perfect predictors count vector perfect predictor regression count vector rescaled giving assume feature set aside optimized intercept current set predictions overall reintroduce feature loss function predicted mean considering feature dropped terms dependent convex derivative set equal minimum xijj negative examine positive case allowing drop sgn term set equal solve giving term outer parenthesis average prediction documents having perfect predictor document cik predictive features true documents predicted approximately documents predicted features included model sum necessary pruning reduced rearrange obtain approximate cut drop perfect predictors perfectly predict documents case positive examples xij giving prediction rst term takes prediction perfectly second term shrinks coecient away half predictions predictors predict documents shrunk fewer raw coecients larger appendix textreg package text regression package textreg cran wrapper extensively modied version code written georgiana ifrim designed integrate package commonly package dealing text package fully documented research code meaning gaps errors possible author appreciate notication order primary method package regression textreg method takes corpus labeling vector returns textreg result object contains nal regression result diagnostic information use somewhat edited function heading default values textreg labeling banned null maxiter verbosity positive false binary features false regularization false min support min pattern max pattern gap convergence main arguments method listed corpus vector strings corpus object built strings labeling vector values means drop consideration banned vector unigrams words allowed summary phrase tuning parameter regularization rescaling terms treated innity maxiter maximum number iterations allowed terminating convergence verbosity means silent larger numbers mean diagnostic printout positive allow positive features intercept useful positive documents negative baseline documents binary features feature vectors vectors indicating phrase given document compared vectors counts times phrases document feature vectors rescaled regardless regularization true features rescaled recovers ifrim algorithm min support phrases appear times considered viable features increasing number greatly decrease running time algorithm force dropping rare phrases regardless rescaling regularization choice min pattern max pattern minimum maximum lengths phrases gap number words appear gap phrase multiple gaps ered length resulting textreg result object printed plotted explored try console typing plot method reformat textreg gives nice table table summary statistics nal phrases summary table table passing list textreg result objects list table method calc loss gives nal loss result predict return individual document level predictions labeling method rule matrix gives design matrix nal selected phrases including intercept pick tuning parameter use find threshold labeling ban words method returns length list numbers rst number choice return null model labeling given subsequent numbers constitute found values return null model random permutation labeling holding zeros xed takes parameters textreg maxiter sure use remaining values calls find threshold culminates corresponding correct model family exploring text sample fragments phrase labeling corpus useful grab fragments prole specic phrases possibly phrases results use phrase count table cluster plot phrases relate cluster phrases matrices occurrence phrases phrase correlation chart bit demonstrated fully explained vignette bathtub demo comes package read discussion ideas references jinzhu jia luke miratrix bin brian gawalt laurent ghaoui luke moore sophie clavier concise comparative summaries ccs large text corpora human experiment annals applied statistics david blei andrew michael jordan latent dirichlet allocation journal machine learning research georgiana ifrim gokhan bakir gerhard weikum fast logistic regression text categorization variable length grams acm sigkdd international conference knowledge discovery data mining pages georgiana ifrim carsten wiuf bounded coordinate descent biological sequence classication high dimensional predictor space acm sigkdd international conference knowledge discovery data mining pages matt taddy multinomial inverse regression text analysis journal american statistical association robert tibshirani iain johnstone efron hastie angle regression annals statistics grimmer bayesian hierarchical topic model political texts measuring pressed agendas senate press releases political analysis january jonathan chang jordan boyd graber sean gerrish chong wang david blei neural information reading tea leaves humans interpret topic models processing systems nips jonathan bischof edoardo airoldi capturing semantic content word frequency exclusivity international conference machine learning inburgh scotland february gerard salton christopher buckley term weighting approaches automatic text retrieval information processing management salton developments automatic text retrieval science taku kudo yuji matsumoto boosting algorithm classication structured text conference empirical methods natural language processing pages barcelona spain association computational lingusitics robert schapire yoram singer boostexter boosting based system text categorization machine learning alexander genkin david lewis david madigan large scale bayesian logistic regression text categorization technometrics tong zhang frank oles text categorization based regularized linear ication methods information retrieval hastie robert tibshirani friedman elements statistical learning springer hui zou trevor hastie regularization variable selection elastic net journal royal statistical society series statistical methodology mosteller wallace applied bayesian classical inference case federalist papers springer verlag airoldi anderson fienberg wrote ronald reagan radio addresses bayesian analysis gary king lowe automated information extraction tool international conict data performance good human coders rare events evaluation design international organization september daniel hopkins gary king method automated nonparametric content analysis social science american journal political science cortes vapnik support vector networks machine learning george forman extensive empirical study feature selection metrics text classication journal machine learning research dumais platt heckerman inductive learning algorithms tions text categorization proceedings seventh international conference information knowledge management acm pages thorsten joachims learning classify text support vector machines springer international series engineering computer science springer stuart rose dave engel nick cramer wendy cowley automatic keyword traction individual documents michael berry jacob kogan editors text mining applications theory pages john wiley sons ltd unknown eibe frank gordon paynter ian witten carl gutwin craig manning domain specic keyphrase extraction sixteenth international joint conference articial intelligence pages california gan kaufmann jilin chen benyu zhang dou shen qiang yang zheng chen qiansheng cheng diverse topic phrase extraction text collection world wide web conference pages edinburgh citeseer sean gerrish david blei predicting legislative roll calls text international conference machine learning pages wei lee shyi ming chen new methods text categorization based new feature selection method new similarity measure documents advances applied articial intelligence pages advances applied articial intelligence yang pendersen comparative study feature selection text tion international conference machine learning pages nashville jacob eisenstein amr ahmed eric xing sparse additive generative models text international conference machine learning pages bellevue usa february laurent ghaoui guan cheng viet duong pham ashok srivastava kanishka bhaduri sparse machine learning methods understanding large text corpora application flight reports conference intelligent data derstanding pages june burt monroe michael colaresi kevin quinn fightin words lexical ture selection evaluation identifying content political conict political analysis hui zou adaptive lasso oracle properties journal american statistical association tong tong kenneth lange coordinate descent algorithms lasso penalized regression annals applied statistics luo tseng convergence coordinate descent method convex dierentiable minimization journal optimization theory applications laurent ghaoui vivian viallon tarek rabbani safe feature elimination sparse supervised learning berkeley hadley wickham dianne cook heike hofmann andreas buja graphical inference infovis ieee transactions visualization computer graphics speed bin model selection prediction normal regression annals institute statistical mathematics jerome friedman trevor hastie rob tibshirani regularization paths eralized linear models coordinate descent journal statistical software martin porter algorithm sux stripping program david meyer kurt hornik ingo feinerer text mining infrastructure journal statistical software yiming yang xin liu examination text categorization methods ceedings annual international acm sigir conference research development information retrieval pages acm andreas buja dianne cook deborah swayne interactive high dimensional data visualization journal computational graphical statistics
