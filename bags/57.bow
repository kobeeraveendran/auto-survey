l u j l c s c v v i x r a conducting sparse feature selection on arbitrarily long phrases in text corpora with a focus on interpretability luke miratrix robin ackerman july disclaimer the analyses opinions and conclusions expressed in this paper do not necessarily reect the views of the united states department of labor note this paper has been accepted to statistical analysis and data mining please see proofed version there abstract we propose a general framework for topic specic summarization of large text pora and illustrate how it can be used for analysis in two quite dierent contexts an osha database of fatality and catastrophe reports to facilitate surveillance for patterns in circumstances leading to injury or death and legal decisions on workers compensation claims to explore relevant case law our summarization framework built on sparse classication methods is a compromise between simple word frequency based methods currently in wide use and more heavyweight model intensive ods such as latent dirichlet allocation lda for a particular topic of interest e mental health disability or carbon monoxide exposure we regress a labeling of uments onto the high dimensional counts of all the other words and phrases in the documents the resulting small set of phrases found as predictive are then harvested as the summary using a branch and bound approach this method can be extended to allow for phrases of arbitrary length which allows for potentially rich summarization we discuss how focus on the purpose of the summaries can inform choices of tuning parameters and model constraints we evaluate this tool by comparing computational time and summary statistics of the resulting word lists to three other methods in the literature we also present a new r package textreg overall we argue that sparse methods have much to oer text analysis and is a branch of research that should be considered further in this context keywords concise comparative summarization sparse classication regularized sion lasso text summarization text mining key phrase extraction text classication dimensional analysis normalization introduction regularized high dimensional regression can extract meaningful information from large text corpora by producing key phrase summaries that capture how a specic set of documents of interest dier from some baseline collection this text summarization approach has been called concise comparative summarization ccs underscoring two fundamental tures of this tool the comparison of a class of documents to a baseline or complete set in order to remove generic terminology and characteristics of the overall corpus and the resulting production of a short easy to read summary comprised of key phrases such summaries can be useful for understanding what makes a document collection distinct and can be used to inform media analysis understand incident reports or investigate trends in legal decisions many classic methods of text summarization tend to focus on single words or short phrases only approaches such as latent dirichlet allocation also do not extend naturally to phrases on the other hand one regression based method that does allow for longer phrases does not allow for rescaling of the counts of phrases in the text based on the overall frequency of appearance of such phrases which can negatively impact the quality of resulting summaries in this paper we merge two ccs approaches to allow for rescaled arbitrary length key phrases that can include gaps we briey discuss how this is done below our new ccs tool be easily used via our new r package textreg which allows for rapid exploration of text corpora of up to a few gigabytes in size even given these tools when a researcher desires to conduct a specic analysis he or she is faced with many choices in particular the implementation and regularization of the regression itself can be done in several ways and the impact of choosing among these ways is one of the foci of this paper in particular we argue that if the researcher has specic goals for interpretation in mind these goals can inform choice of tuning parameters for example when faced with a corpus where only a few documents are of interest and the rest are to be used as a baseline a researcher may choose to allow only positive weights on phrases in order to simplify interpretation similarly choice of tuning parameter can be governed by a researcher s level of interest in pruning rare phrases we also oer a method for testing for a signicant relationship between the text and the labeling that also provides a threshold regularization value we compare this tool to other related state of the art methods first we compare to multinomial inverse regression mnir a text regression method that is primarily signed to be distributed across many cores in order to be able to handle massive data we also compare to a classic linear lasso approach see e which is similar to this method run on pre computed document term matrices without some of the exibility we nally compare to the original ifrim et al method that is one of the building blocks of this work in these comparisons we investigate computation time prediction accuracy and dierent features of the resulting word lists the dierent approaches give very dierent types of lists and we hope this work gives some guidance to the practitioner as to how to sort through the options as a case study we use this tool to examine a large collection of occupational fatality and catastrophe reports generated by the occupational safety and health administration osha in the united states as a motivating example we examine hazardous exposure to methylene chloride a neurotoxin during bathtub renishing operations in osha and the national institute for occupational safety and health niosh jointly issued a hazard alert calling attention to a recurring pattern of this nature following the deaths of least workers since in related circumstances however the sheer volume of information describing occupational fatalities and catastrophes may have initially obscured this pattern in the years preceding its detection although osha maintains a database of narrative reports describing fatalities and catastrophes fat cats similar patterns of preventable exposure to occupational hazards may be dicult to identify eciently through manual review alone given the large number of narratives in this database thus using methylene chloride as a case study we consider whether text mining techniques can help identify important patterns in circumstances of hazardous exposures in our framework a summary list of key words and phrases ideally represents and reveals the overall content of a collection of narrative reports for example one summary for all narratives related to methylene chloride contained the words bathtub and stripper to qualitatively evaluate our tool we manually examine these words and phrases in the context of the original reports and consider whether our text summarization tool eectively characterizes the circumstances of the bathroom renishing fatalities in general we explore whether we can construct text mining algorithms that when applied to an entire corpus can uncover needles in the haystack patterns such as the connection between bathroom renishing and overexposure to methylene chloride at this stage we are not focused on rates or relative risks of particular patterns we are instead focused on the crude detection of textual patterns that may represent meaningful information about how certain types of injuries and fatalities occur such ndings even if they involve only a few recorded deaths or injuries may facilitate the prevention of many future fatalities particularly in the context of emerging hazards we also examine our tool s ability to extract information from a collection of legal cisions from the employees compensation appeals board ecab which handles appeals of determinations of the oce of workers compensation programs owcp in the u s department of labor dol here we investigate what information we can extract about ferent categories of cases in particular we examine cases involving a question as to whether the work environment caused a mental health condition an emotional condition in the parlance of ecab we nd that while the ccs tool does extract meaningful information relating to the cases of interest further work needs to be done to obtain more nuanced summaries overall the ccs approach does allow for exploration of text and does extract meaningful information extending earlier xed length phrase tools to allow for longer phrases and phrases with gaps does increase the expressive capability of the summaries the methods for picking a tuning parameter while possibly a bit aggressive and conservative do provide an alternative paradigm for data analysis with an eye to extracting human meaning from text overview of summarization this paper extends the concept of concise comparative summarization ccs discussed in incorporating a prior approach proposed by to result in an overall improved ology concise comparative summarization involves comparing a pre specied set of uments to a baseline set one can think of it as a regularized regression of some labeling of the m documents normally and onto the collection of all possible summary key phrases for example in one analysis we label documents relating to incidents involving bon monoxide co as and the remaining documents as each potential key phrase or feature is considered to be a covariate in the regression and is in principle represented as an m vector of measures of the feature s presence e appearance count in each of the m documents by using sparse regularization only a small subset of these possible summary key phrases is selected these phrases are taken as the nal summary so for example the resulting phrases of our co related regression would ideally indicate what is dierent about co related events when compared to other workplace injuries and fatalities at root we are taking those phrases most useful for classifying a set of documents by the given labeling as the summary of the positive set of documents as compared to the negative baseline set it is worth emphasizing that the focus is not predictive quality the selected features themselves are the object of interest and the quality can only be measured by their usefulness in the original human interpretation based question that motivated the exploration thus these methods in principle require human analog experiments to validate their ndings this can be done see or for examples we are using text classication tools but the classication is a byproduct there are many possible choices for how to implement this regression including whether to use logistic or linear regression and whether to rescale the frequency appearance of the phrases before regression prior work has shown that rescaling phrase frequency is quite important failing to appropriately do so can result in summaries that have little informative content even while predictive accuracy is maintained this is not surprising term frequency in text is a known and serious concern when data mining large text corpora e as was rst illustrated in the information retrieval literature e in text classication and by extension key phrase extraction via text classiers there is some desire to allow for phrases as well as unigram single word features one approach is to calculate and use all phrases up to n words long as the overall feature set for long phrases this can quickly become intractable as there is a blowup in the number of possible phrases a corpus may contain to solve this problem ifrim et al allow for arbitrary length phrases by generating the features on as part of the optimization as an added benet this approach easily allows for gaps i e phrases with wildcard words which greatly enhances the potential expressiveness of the resultant summaries ifrim et al s algorithm based on work of and even earlier ts an elastic penalized logistic regression with the features consisting of the entire space of all phrases also see or for other examples of regression on text and or for an overview of elastic nets and other regularized regression methods in general ifrim et al initially propose an algorithm to solve a penalized logistic regression of arg min p m i log i with ci being the feature vector for document i with the cij as binary indicators of the presence of feature j in document i yi being the class label being the number of features including all phrases c being a regularization tuning parameter and being some regularization function they later extend this to allow for alternate loss functions such as a hinge loss however they do not allow for rescaling features by modifying their methods we show how rescaling can be incorporated into their overall approach they also do not allow for an intercept term which can introduce diculty with the summarization process if the number of positive features is not close to we extend their algorithm to allow for a non penalized intercept term as discussed in we implement these modications by extending their code and then wrap the resulting algorithm in a new r package textreg to make it easier to use in a rapid and exploratory manner we also provide some useful tools and visualizations to help researchers understand the resulting summaries the core idea behind the algorithm is a greedy coordinate descent coupled with a and bound algorithm with each step the algorithm searches the entire feature space for the feature that has the highest gradient at the current this is obviously a very large search space but it can be pruned using a relationship that bounds the size of a gradient of a sub phrase by a known calculation on any parent phrase in the search we track the current optimal feature and then for each new feature considered if the bound on all the children of that feature is smaller than the current optimum prune all those children from the search related work ccs is distinct from classication classication is focused on sorting documents such as for attributing authorship or counting types of news events text classication has been attempted using a wide array of machine learning methods such as naive bayes linear discriminant analysis or support vector machines which easily allow for large numbers of features the words and phrases to be incorporated for comparisons of these dierent methods on text see or for svms as an approach in particular for text see the book of for such methods and these evaluations however the features themselves are not of primary interest classication is we instead attempt to extract meaning from documents by contrasting sets to each other this is most similar to key phrase extraction a literature in its own right see for example or interpreting text is a dicult task and can be done in a variety of ways for example use text to predict roll call votes with an lda latent dirichlet allocation algorithm in order to understand how language of law is correlated with political support model political text to explore who dominates policy debates truly validating a nding is generally quite dicult and requires a great deal of eort to do properly as these papers illustrate quite well our tools are primarily intended for exploration validation is not within ccs s scope without additional human validation eorts or alternative techniques of the many approaches to text analysis variations of lda in particular have cently been widely investigated and used these consist of a bayesian hierarchical model that describe documents in a corpus as a mixture of some number of dierent distributions on the words they can reveal structure in a corpus and have been shown to capture human meaning however generating novel models for specic circumstances is dicult even mild changes to the model can be technically quite challenging and consist of an entire research topic in its own right they are either computationally expensive or only solved mately via e variational methods in the spirit of diversity in research approaches we take a dierent path this is not to say that using sparse regression methods on text is new see for example and use sparsity to model topics by representing them as small collections of phrases that stand out against a background showcase several methods such as sparse pca to investigate large corpora there are many others one aspect of our approach that we believe is novel is allowing for complex features in the form of overlapping phrases especially phrases with wildcard words while maintaining the ability to rescale features this allows great exibility in the expressiveness of the possible summaries generated and it is not obvious how to naturally extend methods such as lda which rely on a generative model where words are picked i i from some distribution to do this rescaled n gram regression initial methods regress the yi on the ci where ci is either the vector of counts with cij being how often phrase j appears in document i or of binary indicators of appearance with the elements cij indicating the presence of phrase j in document i this can be problematic in that common phrases e the or less obviously usually end up having much higher variance than less common ones and thus it is easier to pick them up due to random uctuations see section for further discussion rescaling the features can correct this as pointed out in e in particular rescaling for transforms the vectors ci into new covariate vectors xi as xij where zj cij n cq ij q this is similar to standardizing columns in a lasso regression if you do not then phrases with high variability need smaller coecients to have similar impacts on prediction this makes them cheaper under the regularization and therefore appear more frequently due to random chance once our feature space has been standardized with each phrase having an lq length of we regress our y onto these rescaled and an intercept this is a high dimensional problem with p m as we want a small number of phrases we use a sparse regularization penalty we also use a squared hinge loss to obtain i c n with a denoting the maximum of a and for this loss function an over is not penalized from a prediction standpoint we wish i yi if we fall short we have quadratic loss if it does the loss is zero and if we overshoot we still have zero loss there is no penalty for over predicting a document s label we use squared hinge loss as this is similar to the lasso shown to be eective in but also monotonic which is needed for the optimization algorithm also note the penalty term does not include the intercept to generalize this framework taking notation from ifrim et al let our loss for an vidual document be with mi i we can use any monotonic loss functions with everywhere the squared hinge loss from above is m this is very similar to an ols type penalty of logistic would be em regardless of the choice of the loss term can be expressed in the original counts as yi j c p cij n n c we then obtain as arg min alternatively by letting j j zj we can move the zj to the penalty term and regress on on the counts ci this gives the identical loss as seen by considering mi i the rescaled columns scaling a column by zj is the same as penalizing the associated j by zj this has ties to weighted lasso approaches such as the adaptive lasso in that we now have feature specic penalties the gradients change however which can aect the optimization see appendix b solving equation is done with greedy coordinate descent see algorithm for greedy coordinate descent we repeatedly nd the feature with the highest gradient and then mize its corresponding j with a line search over the loss function because this is a convex problem this will converge on the global maximum as each iteration will decrease and since the gradient along all the coordinates can only be if we are at a maximum for a proof see e or we keep a cache of all the non zero features in our model we do not need to ever calculate or store all possible features the main computational cost of the algorithm is in nding the feature with the largest gradient to do this we dynamically generate the features by exploiting the nested structure of any multiword phrase having a smaller phrase as a prex this inner algorithm is shown on algorithm here we rst examine all unigrams then bigrams and so forth until there are no more eligible phrases we rst calculate the gradient for all unigrams and enter them into the queue phrases in the queue are placeholders for their family of superphrases when we pull a phrase out of the queue we check to see if we can prune all of its children and if we can not we determine the phrases children calculate gradients for these children and nally enter them into the queue this algorithm would work without pruning but if we were able to prune all the at zero children of a feature before examining them we could achieve large speed ups and indeed some pruning is possible due to a trick of bounding a child s gradient based on the structure of its parent although the rescaling makes keeping this bound tight more dicult than in the original ifrim et al presentation the main idea is if a bound on the gradients of a family of features is less in magnitude than our current best gradient we can prune them all we discuss nding such bounds next algorithm greedy coordinate descent eatures while not converged do ndhighestgradient eatures end while algorithm ndhighestgradient f eatures all non zero features so far bestf arg maxf f eatures all unigrams in dictionary q queue a queue of all features to check for u do if then bestf u end if q add u end for while q is not empty do q next if not bestf then for c do bestf c end if q add c end for end if end while if then bounding gradients take any feature j with corresponding appearance pattern across the documents cj for any feature with feature j as a prex we know that cki cji for i n which we write as ck cj we also know that cki for i n because they are counts so ck i e given a phrase j any phrase with phrase j as a prex can only have a count vector bounded between and phrase j s count vector during our search we consider phrases from shorter to longer for each phrase j we based on that phrase s appearance pattern in the text calculate a bound bj on the magnitude of the highest gradient a best case hypothetical superphrase with that prex could have if this bj is smaller than the current best achieved gradient then we can prune from consideration all phrases with phrase j as a prex because if bj we have for any superphrase of j therefore we want bj to be as small as possible i e tight to make phrases easier to prune bj dk as derived in appendix b one such overall bound is for any q max dk max c r r where r q for any phrase j these bounds can be computed by summing over only those documents that have phrase j rendering them computationally tractable because the rescaling allows for theoretically very predictive yet relatively rare phrases these bounds are unfortunately quite loose making it hard to substantially trim the search tree one possible avenue for improvement would be to integrate the preprocessing step suggested by as special cases q gives r and a bound of the maximum for any document i that has phrase j and q gives r and a bound of the maximum of the two sums of the across the negative and positive documents containing phrase j which is ifrim s bound corresponding to no rescaling up to the scaling of the maximum occurrence of the phrase in any single document all of the above is easily extended to an elastic net see note in appendix b choices of rescaling and additional constraints choices of rescaling e the q in the lq rescaling and further restrictions on the tion problem can focus the ccs tool on dierent aspects of the summary we can seek to generate summaries with more general or more specic words for example or enforce a contrast of a target set to a larger background set which eases interpretability we discuss how in the following subsections rescaling phrases vary greatly in their overall appearance in text with a very long tail of words and phrases that appear in nearly every document and the bulk of phrases appearing only one or two times a phrase s rate of appearance is connected to its underlying variance if we represent the phrase with its count vector this can cause problems when selecting the most meaningful phrases in particular common phrases can easily dominate because they have greater variance typically this is handled with stop word lists which are lists of words that are a priori deemed low information and dropped before analysis for a thorough discussion of this see and as discusses stop word removal is nicky not general and imperfect they can not easily be adapted to diering contexts or languages furthermore how to implement stop word removal when phrases are the object of interest is unclear rescaling however can not only serve the function of a stop word list but do a superior job rescaling is critical as is widely known in information retrieval without rescaling stop words are easily selected by virtually all text mining procedures even with stop words being dropped typically the runner up most common phrases are then selected primarily due to random variation in their appearance pattern to see this test nearly any shelf text mining tool without removing stop words rst more often than not these methods will fail and their results will be dominated by these low information words stop word lists are a hard threshold solution when a soft threshold tapering is more appropriate rescaling oers such a tapering approach the question then becomes which rescaling to use or alternatively how much tapering do we want we use lq rescaling because it oers a class of choices and integrates well into the gradient descent algorithm with lq rescaling dierent choices of q weight phrases relatively dierently allowing for focus on more common or uncommon phrases at the desire of the researcher overall lower q means generally higher normalization factors z which will change the appropriate c for regularization the main point of concern however is the relative sizes of the weights for rare phrases compared to common phrases in general a rescaling heavily penalizes common phrases while a rescaling does not on the other hand rescaling penalizes rare phrases slightly more than lower choices of q to illustrate see figure here we consider a sequence of phrases that appear once in each of m out of documents the dierent series of weights have been rescaled so a phrase which appears of the time times has the same weight for all choices tf idf rescaling a related strategy for rescaling is tf idf rescaling from information trieval it is typically something like variations exist xij rj with rj log cij ni n dj with dj being the total number of documents containing the phrase j it diers in that it corrects for each document length with the ni we do not do so if documents are roughly the same length this becomes less relevant tf idf also puts more of an extreme dierence between weights for rare and common phrases scaled by the total number of documents for rare and mid range phrases the tf idf rescaling is similar to a lq rescaling with large q figure impact of dierent choices of rescaling here we see dierent rescaling factors z for phrases with a single appearance for each of m documents out of total documents more common phrases are penalized more greatly relative to rare phrases interpretation and negative coecients ccs returns a list of phrases with non zero coecients interpreting these coecients can be quite dicult just as in ols a model based interpretation for k would be that changing feature by would change the prediction by k holding other features xed which given normalization means changing feature by a count of would change the outcome by k zk however given the lack of a well motivated model in our context interpreting the magnitude of these coecients is somewhat dubious nonetheless we still wish to interpret the sign of the coecients positive indicates a feature is associated with our positive class of documents and negative indicates the negative class when the negative group is a baseline however it is not an object of direct interest this is especially the case if it is much larger and more diverse than the positive class in this case the regression is ideally subtracting out baseline characteristics leaving the researcher with what makes the positive class distinct and noteworthy here interpreting negative coecients can be dicult one interpretation would be that such features are conspicuous in their absence unfortunately even when there is a mix of positive and negative features we can still end up with unclear interpretations of the sign due to the holding other features constant aspect of the above for example a negative weight for a feature might be osetting the positive weight for a highly correlated alternate feature and in fact both features may have a positive correlation with the labeling in this case interpreting the negative sign as e conspicuous in its absence is erroneous it is more accurate to say the feature is conspicuous in its absence given its normal association with the second feature this can be hard to communicate one solution is to extend the optimization to consider only the set of positive forcing negative coecients to not exist this is an easy extension of the above algorithm simply rescaling results in different drop the lower bound on the gradient search and truncate any line search update of a k at this is not to say that negative features are useless for example if we allow negative features and nd all the coecients are positive it would suggest that the positive group has a clearer signal than the negative group only phrases found in the positive group are dierentiating the groups this might suggest distinct language use larger vocabulary or specic turns of phrase on the part of the positive group which could be of interest in its own right picking the regularization parameter for most regularized regression settings picking the regularization parameter c is a riously dicult problem in general higher cs lead to fewer features i e more concise summaries low c summaries will be more verbose however an overly low c allows for which in our context means obtaining features that are detected soley due to random uctuations in the appearance patterns of phrases we need to ensure that c is suciently large to mostly prune out such noise classically selection of c is done using methods such as cross validation to optimize prediction accuracy on out of sample predictions as prediction is not our primary focus we look for other methods to select c that enhance the quality or interpretability of the maries generated the lack of appropriateness of prediction accuracy is somewhat motivated by the literature prediction accuracy is for example not the same as model selection as is illustrated by the choice between aic and bic selection methods in regression we present two methods rooted in the goals of ccs to select c the rst is to conduct a permutation test to select a c that gives a statistically signicant summary in that the summary being non empty indicates the presence of systematic dierences in the text between the positively and negatively marked documents the second is to select a minimum c to guarantee the pruning of very rare phrases we discuss how one might select which approach to use in the discussion after the case studies below a permutation test on the summary one might wonder if the phrases returned by ccs are simply due to random chance there are so many dierent phrases it is reasonable to believe some will randomly be associated with any document labeling we can control this with a permutation test this is an exact test and the resulting p value is easy to interpret to test whether it is meaningful to generate a summary at all repeatedly randomize the labeling across the documents regress and nd the corresponding c that zeros out all the features given our random permutation of the labels this gives a null distribution of what c is appropriate if there were no signal finally compare our originally observed c obs to this distribution of fake c s we calculate a value of p pr obs c if p is small we conclude that the needed regularization to zero out our originally observed signal is greater than that for a random signal i e there is a relationship between the labeling and the text similarly if we pick a c that is at the percentile of our permutation distribution we are condent that the resulting summary being non empty is due to the relationship of the labeling and the text and not due to random chance the individual phrases however are not specically tested as being signicant it is possible that they would change for example given mild perturbations to the data theless this test provides a useful minimum bound for the nal c any c lower than this bound could result in a non empty summary purely due to random chance in a similar manner one can check the coherence of nal summaries by generating maries under dierent permutations of the labeling potentially adjusting c with each ation to get similar length summaries for all permutations and creating a list of lists with if humans can then reliably pick out the actual the actual summary randomly inserted summary from the fake ones this is indication that the structure of the summary is not due to random chance this idea is based on assessing the quality of eda visualizations see pruning rare phrases most potential phrases are rare showing up only a handful of times in even very large corpora selecting from such phrases introduces a severe multiple testing problem and we seek to appropriately regularize the regression with c to solve it in particular rare phrases that show up only a few times can be selected if they happen to fall only in the positive set more generally with improper rescaling of features a term that shows up once in the positive examples with a high count and several times in the negative examples with a low count can also be selected this often is contrary to the interpretive goal behind selecting predictors we want phrases that are general summaries informing the researcher of aspects across multiple documents these problems can be partially remedied by proper selection of the tuning parameter c here we investigate minimal c to guarantee that quite rare phrases are dropped we nd such c by investigating so called perfect predictors consider a feature j such that cij if yi and cij for some of the documents where yi for the moment assume we do not have multiple counts in any document this is a perfect predictor predicting r i cij s of the s positive documents these perfect predictors could be used to identify a subset of the positive examples while incurring no loss for the negative examples the only cost of including such a predictor is due to the regularization term c if we set c high enough the cost will be prohibitive and we will not select in fact the cut o of c q with m phrase suces see appendix b for a derivation yi with yi being the prediction for document i without any such hypothetical for this c any perfect predictor of r documents will be pruned for comparison see table which shows for both the case of few positive and many negative examples and the case of roughly equal numbers of positive and negative documents the needed cut os for r this cuto will generally be overly aggressive if other predictors also predict these documents then the gain of including the perfect predictor is potentially less c q r r r r r table needed c to prune rare perfect predictors no rescaling as discussed at the end of section no rescaling is bad for appropriately handling common phrases no rescaling is also bad for appropriately handling rare ones as we can see by its connection to the innity norm and the top row of the table no rescaling of features makes it very dicult to prune perfect predictors singleton predictors as a special case singleton predictors are those that appear only once in the entire corpus and appear for a positive example normally if such a rare phrase appears once in a positive example it can be pruned as described above regardless of q in order to remove singletons that predict for a document with no other predictors we must have c this can be generalized somewhat consider for q if the count of a phrase for a single positive document is s and the count for t negative documents for that same phrase are each the normalizing constant is then z cki t n t s t for the single positive document and xki and xki for the few negative documents this is approximately the same as the singleton phrase circumstance and will therefore be pruned as above proper selection of the tuning parameter is a better approach for pruning than cutting by dropping phrases with low counts ignoring computational issues as it can also prune near singleton phrases with high counts this circumstance indeed arises in a study of the fat cats corpus below the word lion used times for a report involving a plague ridden mountain lion corpse positive example and times in various negative examples was kept as a predictor in the nal summary of disease when c was too low c regularization with cross validation the traditional form of selecting a tuning parameter is via cross validation where some metric of predictive performance is optimized for example in our context we could calculate the predicted labeling of set aside documents and select c such that the average squared distance between predictions and actual is minimized as we will see this tends to give longer lists which can be less interpretable as more important signal can be buried amongst less relevant terms generally predictive performance is not directly a measure of our primary focus the interpretability and signicance of the selected phrases regularization with early stopping and the elastic net the original ifrim et al method includes both a penalty term in the loss function but also regularizes by stopping before full convergence dierent choices of c do aect the resulting model but early stopping is an easy way to obtain a list of specied length quite quickly although if the list is non sensical then this is obviously not a good move ifrim s initial paper in fact has no penalty term in the loss function at all their entire regularization is due to early stopping the relationship of these forms of regularization is unclear early stopping clearly has great computational advantages and there is no need for convergence checks we simply stop when we have found enough features of interest however it is unclear for example whether this approach alone will successfully prune out rare phrases computational comparisons we compare our ccs tool to three other methods in two studies the rst compares running time and general characteristics of the nal word lists generally using default values and recommended approaches for setting tuning parameters the second compares prediction accuracy for the four methods in a third study we also examine the ccs tool under dierent choices of q for our data we use our fat cats corpus and our ecab corpus both of which we describe more fully in the case studies section below the four methods the main comparison method is multinomial inverse regression mnir a text sion method that is primarily designed to be distributed across many cores in order to be able to handle massive data it parallelizes the regression by conducting individual inverse regressions of the counts of phrases onto the feature space which is in our case the binary labeling it is regularized giving a sparse feature vector the recommendation of mnir regarding tuning is to use aic or bic penalization we selected bic because we are more interested in interpreting the covariates than in the quality of prediction and bic is known to be superior for model recovery as compared to aic see e or more generally we used the textir package mnir resulted in very long lists of often more than a thousand words so we truncated the list by taking the words with the top weights for practical use we would advocate greater regularization via e cross validation to restrict the list further we also compare to a classic linear lasso approach see e earlier work shows that the gains from logistic over linear are minimal and the computational expense is large we use the glmnet package selecting the regularization parameter with cross validation on the rmse test error the standard package automatically standardizes the columns so this method uses rescaling generally the lasso lists were short but if they were above words we truncated as above we nally compare to the original n gram method of ifrim et al other than our own there does not seem to be an r package implementing this approach so we replicated their method by using our binary features option and not rescaling the columns for a more direct comparison we select the tuning parameter with our permutation approach they initially advocated early stopping to regularize but in later work and in their package which we extended and modied for our package they introduced direct regularization in the rst two studies we used normalization for our method we select c two ways the permutation approach discussed above and to a xed c to prune perfect predictors for or fewer documents we did not allow gaps in phrases and did not upper bound phrase length for the permutation we permuted times and took the maximum as the c as the c from the permutations do not vary much we ran our trials primarily on the cleaned fat cat dataset stemmed with porter ming via the tm package see section for further details for the latter two methods the data was stored in a at le of cleaned and stemmed text with each line responding to a single document for the rst two methods we generated a document term matrix from this text dropping all terms that appeared fewer than times to keep the matrix manageable this resulted in unigrams bigrams and trigrams there are documents in total we ran the lasso twice rst with unigrams and second with all unigrams through trigrams for mnir we only use unigrams the number of tures generated when we expanded to trigrams was computationally prohibitive on a single computer to obtain labeling we selected a random sample of of the keywords associated with the fat cat reports weighted by the frequency of the keywords s appearance to form the labeling schemes to evaluate for each keyword we dropped any phrases from eration as a feature e phrases with carbon or monoxide for the carbon monoxide keyword were dropped by either passing these words as banned words to our algorithm or dropping the relevant columns from the document term matrix to further understand computational timing we also replicated our rst comparison study on the ecab legal decisions discussed below here we had judges that were part of overlapping subsets of decisions and we compared each judge to baseline this is not our labeling of primary interest in the case study below for the rst computational comparison we ran the four methods on the full data and compared runtimes and characteristics of the nal word lists for the second computation comparison we set aside a random of the data and then predicted the labeling of the set aside data using each method the lasso the ifrim et al and our method all produce nominal predictive scores often overly low due to the massive imbalance of the positive and negative classes to nd a cut point for classication we t a logistic regression model on whatever predictive value came out of the method on the training set and then classied by determining whether the predicted log odds were above or below for the mnir method we rst projected the results of the inverse regression as described in and used the resulting score as the covariate in the logistic regression modeling step we also calculated auc scores using the raw predictive scores for all methods our third investigation was on the impact of dierent choices of q for lq rescaling when using our method we again generated word lists for each of our keywords and calculated average length and average frequency of words for the resulting lists for q and we put no upper bound on phrase length simulations were run on a macbook pro with a ghz intel core processor gb of memory and a solid state drive reproducible r code for all simulations is available on request comparison results for each labeling and each method we calculated dierent statistics on the word lists we also timed the total time to generate a list table has the average of these measures across the labelings a few general trends are evident list characteristics overall the resulting lists are very dierent in character we culated average list length and then for lists truncated at the top terms calculated the mean and median frequency of the words to gauge how common or rare selected phrases were we nally averaged these means and medians across the dierent labeling runs we see the dierent methods are quite dierent in terms of these scores mnir gives very long lists of very specic words this is probably due to the inverse regression which selects all phrases that are relevant without much regard to correlation structure between them the lasso also tends to have longer lists but the average ance of the selected words is on par with the n gram feature rescaled regressions however when the lasso had access to trigrams which can be highly targeted and specic the median frequency plummeted to for pentagrams it went to as expected the ifrim method due to no column rescaling selects very general terms with a median frequency of around for the osha dataset the permutation c values tended to be close to the xed c giving overall word lists that were similar as well for the ecab dataset with longer documents the permutation selected c was much higher and thus the word lists were much shorter due to the greater regularization the resulting words were also typically more general overall it is clear that the practitioner can use dierent methods to get dierent types of lists we believe generally that one wants short lists of phrases and that those phrases should not be overly general runtime runtimes were generally comparable for our osha data but widely dierent for the ecab data we discuss the osha data rst the lasso method even including its cross validation step was quite comparable to the textreg method with respect to time due to its very fast implementation it is also robust to very wide feature matrices note the average time for using all trigrams is the same as for just unigrams the ifrim method is faster than the textreg methods likely due to improved pruning mnir is also quite fast event though it is designed for parallel systems and we ran on a single core generally the computational times to generate summaries are comparable however mnir on the trigrams was not workable as mnir is linear in the number of features the blow up of features was too much of a time increase of course multiple machines and its parallel structure could avoid this we were quite surprised by the time statistics being insensitive to number of terms for the glmnet package to investigate further we calculated a document term matrix for all phrases up to words giving unique terms giving the extra row in table here average runtimes for the lasso increased modestly by about to a mean of seconds the computational times were more spread out for the ecab data see the bottom half of table we now see a substantially increased time from moving to unigrams to trigrams for the lasso and the timing of the textreg methods exploded in investigating this time dierential further we found many selected phrases had or more words furthermore most runs reached the maximum number of iterations before convergence indicating at surfaces this is a weakness of greedy coordinate descent which we discuss further below overall we potentially have due to the use of boilerplate language in legal writing long yet informative phrases that we wish to see in our summaries this could make pre generation of candidate phrases prohibitively expensive the mnir method selected all unigrams each unigram apparently has enough dierence in use across judges to be selected under the bic penalization the times for the lasso and mnir do not include the time to generate the document term matrix which was minutes for grams and minutes for trigrams method ifrim lasso lasso trigrams lasso pentagrams mnir textreg textreg xed c ifrim lasso lasso trigrams mnir textreg textreg xed c time avg sec list length avg word freq avg median table results of comparison study on osha reports top and ecab legal decisions bottom runtime does not include time to generate the document term matrix for the lasso and mnir predictive accuracy results to assess predictive accuracy we used the macro averaged f statistic f calculated for each of the trials and then averaged see table all methods would sometimes score none of the test set documents as positive giving undened f scores these are also indicated on the table figure shows these scores along with the component precision and recall as well as auc scores for those keywords where all methods had dened f the predictive accuracy is comparable across the methods although our text regularization does suer some due to over regularization from the permutation method overall predictive accuracy is low recall in particular tends to run around for all methods although the lasso with trigrams was noticeably superior we also examined auc scores here we again see the cost of textreg s over regularization conservative classication reduces sensitivity greatly lowering the roc and auc scores the auc scores are deceptively high due to the imbalance between positive and negative examples generally the superior performance of the lasso on trigrams indicates that rescaling tures coupled with the richer feature set of multiple words is useful for prediction tasks our methods results indicate over regularization is detrimental to prediction due to primarily diminished ability to predict positive documents as positive shown by the recall scores method mnir lasso lasso trigrams ifrim textreg sd missing table f scores for dierent methods averaged across keywords examined with standard deviations number of keywords with no dened f out of also indicated selection of q results as anticipated dierent values of q produce lists of dierent quality results are on table higher q corresponds to lists with relatively more common words and phrases low q duces lists that tend to have phrases with more words for the q list more than half the phrases were words or longer we also found that the length of the list increased with q for q we had longer lists with more common words for low q the algorithm selected very targeted phrases and not too many of them the regularization parameter c is not comparable across q therefore we recalculated it via the permutation approach for each run and value of q mean values for c are shown on the table for reference generally we nd a rule of thumb worth remembering longer lists tend to have more general terms this can happen by adjusting either c or q see rst simulation and compare dierent c for example figure out of sample classication rates for dierent methods lasso on trigrams is generally the best although there is substantial variability keywords where any of the methods failed to return f scores were dropped charts substantively the same when these keywords included method textreg textreg textreg time avg sec list length phrase length word freq word freq median avg avg avg c avg table word list characteristics of dierent rescaling norms length is length of list mean size is average number of words in the selected phrases frequency refers to phrase occurrence in the corpus mean c is the average regularization value discussion overall our method succeeds in accommodating multiple word phrases while also allowing for an intercept and the rescaling of features as shown these extensions are critical for generating manageable lists with phrases that are not overly common further picking c and q does control both list length and commonality of words on the lists as predicted by our initial discussion in terms of computational time our method did not perform particularly well when compared to the lasso of the glmnet package especially considering that the lasso s overall time includes that of cross validation as our method is similar to lasso regression on a pre constructed phrase matrix diering only in that it uses a hinge loss instead of a squared loss one might naturally ask whether an alternative approach would be to simply generate the full document term matrix and use glmnet we are not entirely convinced as we discuss next llllllllllllllllifrimtextregmnirlassolasso first generating full document term matrices including phrases with wildcards could be computationally tedious especially if we imagine extensions such as optionally included words at the very least it is expensive in both time and memory and grows increasingly so with the number of possible phrases the minutes to generate all grams is well over the times the stored matrix was twice the size of the initial raw text second the hinge loss is dierent from the classic loss and there could be core dierences in overall behavior here this is an area for future investigation third the speed of the glmnet package is partially due to the lars approach where the entire regularization path is computed at once it is arguably also due to the package being particularly well developed and optimized for eciency there may be similar ways to substantially speed up our package and method to make it more competitive for example one potential slow down determined from examining the convergence paths of many runs is that we often see an initial selection of a small list of phrases and then a slow hill climb where with each step a dierent already selected phrase is selected for adjustment this comes from the coordinate descent the optimal gradient path is at an angle and so following it requires small steps in the associated coordinate directions to trace that path unfortunately with each such step the algorithm conducts a full search for the highest gradient across all phrases which is expensive we might instead after each phrase is selected nd the maximum point given the set of all phrases selected at that point then only if a new phrase were introduced would the algorithm have further steps this is eectively the lars approach and is an important area for enhancement furthermore the greedy ascent iteration often involves the intercept ratcheting down as features ratchet up this suggests another possible direction of forcing the intercept to be for corpora with rare positive labeling rates rather than estimating it a intercept corresponds to predicting all documents as negative by default distinct from no intercept which predicts uncertainty as default or allowing the intercept to move which tends to predict the overall base rate as default specifying the intercept value would save an intercept update with each step the optimization problem is then nding a collection of phrases to give positive documents positive weight without aecting the negative documents too heavily regardless for truly massive data especially when the number of documents grows large neither our approach or the lasso will work instead methods that allow for easy parallelization such as mnir will be key another area for future exploration therefore is to determine how to over regularize mnir to get shorter lists which also might induce lists with more common phrases case studies we illustrate the ccs tool with two case studies for the rst case study we also compare the resulting summaries from ccs to the three other methods discussed above an overview of the code to generate these results using our textreg r package available on cran is in the appendix full scripts and data are available on our website for other studies using address is harvard edu lmiratrix software textreg r package similar tools see for example or before presenting our results we discuss data representation data representation and cleaning there are many choices one might make in preparing a corpus for statistical analysis it is common to for example convert text to lowercase and to drop all punctuation we take that approach here although we convert all digits to x to preserve the presence of numbers in case that is informative and convert hyphens to blank spaces so the sequence of hyphenated words would coincide with a non hyphenated similar phrase something not possible with single word analyses most text analysis packages would then convert the raw text into an m p matrix of counts dropping any stop words but because of the greedy coordinate descent algorithm unknown phrase length and the related generation of features on we store the text as raw strings with one string per document there is some controversy as to whether to stem documents which is where the tails of many words are cropped so as to collapse the number of possible words for example clean cleaning and cleaner might all be cropped to this has the disadvantage of making resulting text output somewhat dicult to read especially when considering phrases stemming can also lose textual meaning if the dierent suxes are in fact important in the context it has the advantage particularly for phrases of collapsing several dierent versions of phrases into one we provide stemming as an option but to enhance readability of output append a to the end of all stemmed words and their roots to indicate they have been potentially cropped we also provide tools to wildcard search for stemmed phrases in the original text so as to recover examples of the complete phrases sometimes a given context involves words that are known a or nearly a priori to have no meaning we therefore provide a option for custom made short stop word lists i e a list of banned words that are prohibited from being in any summary phrase generally these lists are built on the y as an iterative process the rst summary generated will often contain words that are immediately recognizable as inconsequential to a researcher with pre existing contextual knowledge even though they are correlated with the labeling the researcher would then drop these words rerun the algorithm and repeat as necessary we do not see any way to avoid this the case studies below illustrate why fat cats our main investigation relies on osha s publicly available summaries of occupational talities and catastrophes fat in the united states from to these summaries describe workplace incidents that have resulted in death or the inpatient talization of three or more workers when such an event occurs an employer must report it to osha in the course of conducting the resulting investigation osha generates a address is gov views c f r recently expanded the list of reportable events to include the loss of an eye amputation or inpatient hospitalization occupational injury and illness reporting requirements naics update and narrative report part of which becomes publicly available and is annotated with any of a set of about keywords to categorize the narrative reports in terms of specic chemicals involved machinery involved body parts aected and other salient features the publicly available records primarily consist of a title a short paragraph summary of the incident along with the date whether the incident involved a fatality and several other covariates we concatenated the title and paragraph description to form the documents these documents tend to be to words long these are the and quantiles with a minimum length of words and a longest report of words after stemming there were unique unigrams word stems of which appeared or more times and appeared or more times to investigate this corpus we can for any keyword generate a labeling of the narrative reports by setting those reports tagged with the keyword as and the remainder as using ccs on this would then summarize the collection of reports marked with a keyword by comparing them to all other reports ideally this would take out words common to these reports e employee or other general work place terms leaving us with phrases that make the identied set stand out we would interpret this summary as a distillation of what is distinct about this category of fat cats as compared to fat cats in general by periodically summarizing reports for each keyword of interest researchers may gain information about emerging hazards and trends in circumstances hopefully the resulting summaries would be faster to read than the individual narratives but still contain hints as to general themes within these narratives as chemical exposure is an area of particular interest for enhanced surveillance and standing we generated a background comparison set of documents by identifying keywords that we deemed to be at least loosely associated with chemical exposure we then dened the chemical family of narratives as all narratives that were labeled with at least one of these keywords this allowed us to compare various categories of narratives within the ited context of this chemical family as well as within the larger context of all other types of narratives changing the background set highlights dierent aspects of what sets apart a marked collection of reports as an overview of the overall number of narratives of dierent topics of interest table shows the appearance pattern of the categories examined we discuss methylene chloride and carbon monoxide here and defer chemical reaction to a supplementary document the table also shows how many narratives involved a fatality methylene chloride as it is our motivating example we rst examined methylene chloride we initially selected a value of c to ensure that we prune all singleton perfect predictors see section there are reports marked with the methylene chloride keyword running ccs on these reports returns two words methylene and chloride as these words are not of interest we immediately added these words to the ban list and reran table displays the resulting summary comparing these narratives to all the other narratives reporting revisions fed reg september c f r methylene chloride carbon monoxide chemical reaction general fatality total chemical table number of narratives with dierent keyword labelings second pair of columns restrict database to only reports related to chemical exposure the summarizer picks up on the coherent theme across these reports of bathroom nishing this example is quite encouraging given our prior knowledge of the dangers of methylene chloride but the utility of ccs in detecting yet unknown patterns remains to be seen if we select c based on the percentile of permutations we obtain c the needed c to result in a null summary is by comparison c obs we conclude that there is a statistically signicant relationship between the text and the keyword beyond the presence of the banned words and that the summary is thus informative the sponding summary for c is quite succinct containing only a bathtub and stripper contained picking c to give statistical signicance appears here to drop informative phrases phrase a bathtub paint stripper stripper contained and reglazing from a bathtub remover contained stripping agent tub head phrase reports tag tag phrase table methylene chloride rescaling against all reports phrase is total occurrence of phrase reports is number of reports containing phrase tag is number of methylene chloride reports containing phrase tag is percent of phrase appearances in methylene chloride reports and phrase is percent of methylene chloride reports containing phrase the summaries do not necessarily capture information in all tagged documents in this case for example six of the methylene chloride reports do not have any of the phrases on table and so are not represented a manual review of these reports revealed that four involved strippers for tile oors and furniture one involved an explosion and one quite terse only referred to methylene chloride gas carbon monoxide we also examined reports relating to carbon monoxide an asphyxiant odorless gas we ran ccs with dierent values of q for lq rescaling to examine the impact of dierent levels of rescaling we compared the co cases to all other cases involving any of a set of keywords predetermined to be related to some sort of chemical exposure i e those narratives marked as members of the chemical family to reduce computational time we limited attention to phrases that appear at least ve times in the corpus results are in table to obtain these results we summarized in an iterative process words such as bon monoxide gas poisoning exposed exposure overexposed hemoglobin ppm levels partspermillion overcome and co were eventually dropped we also removed the more specialized hyperbaric having to do with a ical intervention for co poisoning and cohb an abbreviation for carboxyhemoglobin a molecular complex that hemoglobin and carbon monoxide form in the body all of these words appeared in initial summaries and are due to the technical obvious aspects of co poisoning they do not reveal trends or characteristics of interest and thus obscure the desired results none of these words would have appeared on any conventional stop word list as they are in fact correlated with the category we see no way of automatically removing them the nal results reect several known patterns in co poisoning for example the exhaust from gasoline and propane powered engines are major culprits of these exposures particularly in combination with poorly ventilated enclosed spaces the appearance of the phrase cold room appears to reect incidents in which propane fueled forklifts and oor cleaning devices were the source of carbon monoxide exposure within cold storage areas where ventilation can be poor in investigating hospitalization we found of the co poisoning cases contained were hospitalized versus only of the other chemical related narratives and of the chemical narratives the re department was mentioned in nearly of these narratives versus a baseline of this all may be due to lower rates of fatality with only of the co poisoning cases involving fatalities as compared to for other chemical family reports and for non chemical family reports interestingly dead appears in of the co narratives as compared to in the remainder of the chemical family dierent rescalings give dierent styles of summaries the smaller q and q have very specic phrases e were using a gasoline and their blood that appear only in the positively marked documents larger q give more phrases overall and give phrases that appear at higher rates in both the positive and negative class for example employees with more than appearances appears for q overall infrequent and specic phrases are relatively easy to interpret and the more common phrases less so but their patterns of appearance are striking employees for example appears in of the co narratives versus a baseline of a mere less prisingly enclosed appears of the time versus less than at baseline table contrasts carbon monoxide to all incidents labeled with other chemical related keywords we also compared co cases to the full set of cases in the database that is we summarized the same collection of reports but used a dierent baseline point of comparison results are in table in the appendix they are broadly similar we also analyzed the data using stemming see table in the appendix results are again broadly similar but possibly harder to read stemming collapses phrases which can be helpful but hampers human readability phrase a propane powered their blood gasoline powered concrete saw an x hour the re department measured in a cold operating a propane propane operated were using a gasoline propane powered powered forklifts for fresh the generator was x hour overexposure exhaust generator was blood a gasoline average found were treated the re department hour source of the ventilation enclosed were taken employees employees were were re department a propane were hospitalized dead warehouse reports tagged tagged phrase table dierent summaries of carbon monoxide compared to chemical family narratives finally we compare ccs to the other methods of mnir the lasso and the ifrim et al approach the methods returned summaries of very dierent lengths mnir was lasso ifrim and textreg we had to truncate mnir and lasso to display the lists but we did this by taking the union of the top words of each list and then displaying weights before truncation to see maximal overlap see table with words sorted by frequency of appearance in the corpus we see the lists are quite dierent with mild overlap mnir generally targets rare phrases most of which are not displayed ifrim very general ones mnir restricted to unigrams in this instance due to computational concerns has less overlap than it might otherwise textreg has mid range phrases with a few very rare phrases which are perfect predictors these phrases were not included in the document term matrix due to their rarity so could not be on the lasso or mnir lists many of the phrases have similar meanings for example ifrim has the general versions e for the more specic of the lasso and textreg legal decisions in the context of legal decisions our motivating question is whether we can eciently learn about characteristics of certain types of cases by extracting associated phrases and topics from a corpus of those cases as an exploratory case study we chose to examine publicly available decisions from the employees compensation appeals board ecab which siders appeals to determinations by the oce of workers compensation programs owcp in the u s department of labor dol owcp handles compensation claims from federal mnir lasso ifrim textreg phrase of for their blood vanguard decatur mek twa carbonyl newton qa transient stratton three are fd by by when his was num phrase table dierent summaries of carbon monoxide comparing dierent methods workers injured during the course of employment ebac handles as many as appeals per year within this case law one particular area of interest is how ecab handles compensation claims for so called emotional conditions these cases can be challenging for a number of interesting reasons for example establishing whether an employee s condition was mately caused by workplace conditions requires an analysis of causation that is unique in many ways from that which is appropriate in the context of a physical condition to further probe the potential utility of css in extracting useful information from large bodies of technical text we performed an exploratory analysis on the collection of ecab decisions that relate to both mental health conditions and causality we sought to mine whether automated summaries would reveal meaningful patterns these decisions are publicly available through we examined years to by scraping them from the website address is gov ecab decisions main html we ended up with legal decisions documents generally ranging in length from to words these being the and quartiles and a median length of words the shortest was words and the longest there are unique words of which appear or more times and appear or more times these counts include case identiers and other character strings as words we do not attempt to remove them directly we automatically labeled all of the decisions with two sets of dummy variables one for emotional condition and one for discussion of causality or work relatedness of the injury for each we labeled documents if they contained any of a set of handpicked key phrases once we tuned our collection of key phrases we took a random sample of the positively and negatively marked documents and conducted a manual review the labeling is clearly not perfect as is illustrated in table ideally the ccs method will still be able to produce relevant summaries despite the noise of the missed labels although it is possible that specic types of positive decisions are systematically missed due to the labeling discovery of meaningful summary phrases would nevertheless be suggestive labeling emotional condition no emotional condition total cases causality work relatedness no work relatedness total cases total correct sample estimated positive negative sense spec table manual review of labeling quality for legal decisions column of table shows a rst pass summary of those cases that both involve an emotional condition and revolve around issues of causality we see fairly general terms and some boilerplate language here it is necessary to explore the raw text to discover the contexts for these phrases this is easily done using our package for example one positively marked decision has not every injury or illness that is related to a claimant s employment is compensable in the case of lillian cutler the board explained some distinctions as to the type of employment situations giving rise to a compensable emotional condition under feca c e docket no issued november emphasis added another is to establish that an emotional condition arose in the performance of duty a claimant must submit the following medical evidence establishing that she has an emotional or psychiatric disorder factual evidence identifying employment factors or incidents alleged to have caused or contributed to the condition and rationalized medical opinion evidence establishing that the emotional condition is causally related to the identied compensable employment factors t g ecab emphasis added illness that is cutler xx ecab requirement imposed by the psychiatrist incidents alleged to have caused lillian cutler compensable disorder factor of employment and a factor of employment not covered reaction to anxiety cutler xx ecab xxx xxxx depression lillian cutler xx compensable factor of employment and results from an environment or an administrative or personnel requirement imposed allegations main ecab depression xx many table summary of cases involving an emotional condition and a discussion of causality dierent columns correspond to the number of dropped phrases first column is only emotion and condition rest are adding more and more phrases column b adds column c depression and column d xx for illustration purposes column d includes many other terms as an illustration of stability of ccs consider the other columns of table each column corresponds to dropping more and more terms from consideration note that the transition from the third to fourth column drops one of the case references lillian cutler even though we did not explicitly drop those words and phrases ccs selected phrases are picked in the context of other phrases because we removed xx indicating a case number lillian cutler is no longer selected along with other phrases including parts of this phrase and xx instead we obtain a cluster of phrases showcasing dierent aspects of these cases dropping phrases can only aect a summary if those phrases are in the summary the nal two columns are the same because none of the additional phrases were in the summary from column care must be taken to understand the complex dependencies between phrases thus in the context of ecab decisions the ccs tool provides phrases that ag plate language and case citations to some degree these phrases appear to reect precedent and common statements of law that characterize a given category of cases while our results are exploratory inexact and not particularly revealing in and of themselves they do suggest that a rened ccs tool might one day facilitate the development of automated case content analysis or aid the development of rened legal taxonomies discussion as the above studies illustrate using these tools to understand text is a very dierent and far less precise activity than working to correctly classify text the common problems with machine learning approaches selecting methods selecting tuning parameters are only exacerbated by this uncertain area the researcher is left with many decisions to make and only vague guidance on how to make them with our method two such decisions are prominent what method to use for selection of the regularization parameter c and what method to use for selection of the normalization parameter q we also need to determine how to remove domain specic stop words picking c for selecting c we have several options especially if we include picking a regularization parameter by optimizing predictive performance which option to select is a dicult especially since there is no easy metric of nal quality if one s focus is not prediction the computational investigations shed some light on this problem ideally one would use the maximum of the permutation approach and the rare phrase pruning approach this will guarantee only nding a summary if one should be found and also will discard rare phrases that do not speak of general trends across the positive documents the free test from the permutation selected c of whether the phrases as a whole are in fact signicantly associated with the text is a real boon in our view it moves towards presentation of results that are known to not be entirely noise that being said future work on stability where documents are perturbed to see how the selected phrases change for example is a must furthermore we acknowledge that the above examples suggest that the permutation selected c is severe more severe than from cross validation or similar this means we can lose human meaning as illustrated by the methylene chloride example the richness of the summary was much greater with a slightly reduced c relaxing regularization towards what would be achieved with oriented approaches to achieve longer lists may be informative but other than improved prediction this could undermine the guarantees provided by the above perhaps work on testing individual phrases via false discovery rates could nd a better balance regardless one should always compare the nally chosen c to table to see to what degree it is discarding perfect predictors and to what degree it would leave the remainder to be potentially picked up this provides a human interpretation of the impact of the regularization picking q selecting q is also admittedly dicult by design it gives dierent views of the data from the quite general to the very specic we advocate for exploring a range of values as the best practice in the above case studies for example the full range of terms on the tables provided the most complex and rich story perhaps pooling the lists and exploring these pooled lists would be one way forward we underscore that we view these tools as exploratory the researcher can extract small snippits of text to see if they oer some clue towards a more thorough investigation this is similar in spirit to for example xgobi stop words even though we avoid using stop word lists as a rst pass approach we still needed to generate specialized stop word lists we see no way to have removed these words without human intervention as from a prediction standpoint the removed words are key indicators of a given labeling unfortunately selecting them occludes terms that could enhance human understanding we can see this with the calculated c thresholds as we add words to the ban list the c plummets because we are removing the words that are most correlated with the labeling eventually we could reach a point where we have conditioned out all the connection of the labeling to text this is another potential avenue for exploring such data overall we advocate generating modest length stop word ban lists using substance matter knowledge coupled with rescaling over using generic stop lists that allow milquetoast words through due to their being not obviously wrong conclusions we present a method for comparing sets of documents that is simple sparse and fast we argue that these qualities are important for text analysis especially if it is to be used for surveillance or other exploratory tasks here simple means the summaries can not be too technical in nature for example the presence absence of features is easier to interpret than regression weights we need sparsity as humans are lazy the number of phrases in a summary must be few the faster summaries can be computed the better otherwise exploratory analysis and discovery are bogged down we do not however argue that the results of these tools or in fact any other text analysis tools that we know of can be taken as ultimate proof of any particular substantive theme or meaning summaries can be quite suggestive but researchers would need to investigate further to substantiate those suggestions alternatively secondary analyses such as blind scoring of the key phrases for sentiment could lead to traditional statistical conclusions in these cases ccs should be viewed as a dimension reduction tool providing a targeted small number of informative features for a very complicated form of data on the technical side we have eectively provided an implementation of lasso style gression where the full set of features are dynamically created and the loss is a squared hinge loss rather than a normal quadratic loss this work shows that implementing sparse sion with greedy coordinate oers a viable direction for summarization using phrases rather than words furthermore the approach of dynamically building features shows promise for other customizations such as skipping or dropping words to automatically detect related phrases collapsing them into single features admittedly more work needs to be done to optimize this particular implementation to see exactly how fast it could be currently the algorithm is sub optimal because it does not fully t currently selected features at each iteration that being said compared to methods with a pre computed design matrix it is comparably fast is more exible in the n grams considered and allows for some trickiness such as having gaps in the key phrases i e card words and the enforcement of non negative weights additionally the textreg package is quite natural to use allowing users to avoid calculating the phrase matrix and instead works with raw text it also easily allows for customization of dierent rescaling schemes other than going beyond text analysis these methods also hint at ways of incorporating many interaction terms among features in high dimensional regression phrase features are simply interactions of nearby word features and thus similar bounding methods may exist this is another area for future exploration acknowledgements we would like to thank three anonymous reviewers and an ae for their detailed comments the paper has been much improved by this feedback also enormous thanks to matt taddy for his invaluable and speedy support with his textir package used in our comparison studies this work builds on conversations and ideas discussed in the statnews research in particular thanks to group in uc berkeley led by bin yu and laurent el ghaoui garvesh raskutti for his ideas on the impact of dierent rescaling choices the authors are very grateful for these opportunities and inspirations also thanks to kevin wu for a portion of the code in the r package and to janet ackerman for collaboration on an earlier incarnation of this project and initial data collection appendix a appendix a consists of supplemental tables showing alternate summaries of the case studies discussed above table compares carbon monoxide narratives to all the other narratives table demonstrates a series of summaries on a stemmed corpus reports tagged tagged phrase phrase were using a gasoline propane powered oor their blood hemoglobin in the cold room they were found powered forklifts gasoline powered propane powered overexposure exhaust fumes exhaust calculated generator was employees were treated evacuated the re department ventilation were treated a propane powered blood were hospitalized found employees stratton headaches source of the a gasoline passed out enclosed re department cold hours room table dierent summaries of carbon monoxide when comparing against all other cases phrase for x was the cold xxx the was the cold of the x cold are reports tagged tagged phrase table dierent summaries of carbon monoxide using stemming compared to chemical family appendix b derivations we here show three derivations used in the above work we rst show how to obtain the bound on the gradient second we give an alternate formulation of the loss function which gives a dierent approach for nding the feature with the maximal gradient we then show how to obtain the minimal to ensure perfect predictors are pruned bound on the gradient the gradient for phrase j is dj c cij dj n cij c cij dj now consider all phrases with phrase j as a prex that are currently not in the model i e which currently have k and maximize over the possible gradients a similar argument gives a bound below for negative gradients for vectors a let a b a component wise relationship of ai bi for i m then the set a a cj contains all potential appearance patterns for a phrase with phrase j as a prex we do not wish to calculate what the actual phrases are hence we optimize over this set of potential phrases this results in the optimization problem u max max ai ai za c ai za dj c max c za with za being the lq norm of a w being a m vector of weights with wi and being the inner product because is everywhere nonpositive the rst term in the rst line above is negative the second line follows because setting ai for any document with yi only increases the gradient as doing so will simultaneously drop negative terms and shrink the penalty term is negative because we are examining gradients at if we step in the negative direction as indicated by the rst term the gradient will immediately be shrunk towards the mi which include the intercept are xed constants determined by the current location of our optimization path we are eectively maximizing over an inner product of a and a vector of weights w with wi for i n overall this bound is assessing the maximum possible utility of a hypothetical super phrase which boils down to maximizing weights put on positive examples the normalization za renders this problem dicult we can bound the optimization using the following relationship for q r such that q r this inequality gives u max cos c cos c cos c max max c with being the angle between a and w coupling this with a similar argument for minimization gives the overall bound a note on the elastic net the elastic net is where we penalize our loss function with using ifrim et al s notation ca a p p this regularization tends to keep groups of correlated features rather than picking one it can borrow from the stability of ridge regression it can be potentially useful when many small features have weak signals setting a corresponds to regularization for our problem the gradient search is just changed to a subtraction of ca rather than c for the at zero potential new features the gradients calculated for features already in the model have an extra term of due to the derivative of the second term above alternate gradient formulation by redening we can change the optimization problem to have a lq rescaling term in the penalty this gives dierent bounds on the gradients for super phrases based on a phrase however it also changes the gradients themselves which would change the path of the optimization problem that is dierent features will initially have the largest gradient assuming true convergence the nal solution will be identical however in particular dene j the loss term can then be reexpressed as c n the gradient for phrase j is then dj n czj dj dj dj czj again consider all phrases with phrase j as a prex and maximize over the gradient yielding the optimization problem czj u max max ai c n p ap i as before the second line comes from noticing that setting ai for any document with yi will only increase the gradient due to dropping the negative terms and shrinking the penalty term is still negative because if we step in the negative direction from which is indicated by the rst term the gradient will immediately shrink towards if we consider only phrases that have at least r occurrences in our corpus then we can roughly bound with max k dk cj p this is from maximizing both terms separately for the rst we simply add maximum weight without regard to the normalizing constant for the normalizing constant given a total count of r occurrences the maximum zj would be putting singletons on each of r documents giving the p total similarly bounding from below gives an overall bound of max k dk max cij cij p this bound appears to be less useful than the one presented in the main paper more not rescaling by zj tend to make more common phrases be selected rst as we are not rescaling the rst term allowing it to grow quite large perfect predictors take the count vector for a perfect predictor cj it has r and m r for the regression the count vector cj is q rescaled giving xj cj zj q as zj q q m assume feature cj has been set aside and we have optimized without it we have x with j j zj except for the intercept for our current set of predictions and an overall yi now reintroduce feature cj our loss function when only predicted mean m considering feature cj is then where we have dropped those terms not dependent on j this is convex take the derivative and set equal to to nd the minimum yi yi m m i yi xi xijj xi j q xi j q q q q i i i xi q i j q xi q the s will not be negative and hence we examine the positive case allowing us to drop the sgn term set equal to and solve giving j q r xi i the term in the outer parenthesis is the average prediction for the documents having the perfect predictor cj if for a document i with cik there are not really any other predictive features then if this is true for all of the documents predicted by ck then the above is then yi xi approximately j if some documents are predicted by other features included in the model then the sum will be less and the necessary c for pruning will be reduced rearrange to obtain an approximate cut o for c to drop all perfect predictors that perfectly predict r documents the q case for q we have j r r xi i c c for the positive examples xij r giving prediction of yi r c r c the rst term of j takes our prediction perfectly to and the second term shrinks the coecient away from by half of c predictions from predictors that predict for more documents will be shrunk less than those for fewer the raw coecients will also be larger appendix c using the textreg package our text regression package textreg on cran is a wrapper for an extensively modied version of the code written by georgiana ifrim it is also designed to integrate with the tm package a commonly used r package for dealing with text our package is fully documented but it is research code meaning gaps and errors are possible the author would appreciate notication of anything that is out of order the primary method in this package is the regression call textreg this method takes a corpus and a labeling vector and returns a textreg result object that contains the nal regression result along with diagnostic information that can be of use the somewhat edited function heading along with default values is textreg labeling banned null c lq maxiter verbosity positive only false binary features false no regularization false min support min pattern max pattern gap convergence the main arguments to this method are listed below corpus a vector of strings or a corpus object built out of strings labeling a vector of values where means drop from consideration banned a vector of unigrams words that should not be allowed in any summary phrase c the c tuning parameter for regularization lq the q for the lq rescaling of terms or above is treated as innity maxiter the maximum number of iterations allowed before terminating even under no convergence verbosity means silent larger numbers mean more diagnostic printout positive only only allow positive features other than the intercept useful if there are few positive documents and many negative baseline documents binary features the feature vectors are vectors indicating whether a phrase is in or not in any given document this is compared to vectors of counts of how many times a phrases in a document these feature vectors are lq rescaled regardless no regularization if true then features will not be rescaled which recovers the ifrim et al algorithm min support phrases that do not appear this many times are not considered viable features increasing this number can greatly decrease the running time of the algorithm but it will force the dropping of very rare phrases regardless of rescaling or regularization choice min pattern max pattern minimum and maximum lengths for phrases that are gap number of words that can appear in a gap a phrase can have multiple gaps of this ered length the resulting textreg result object can be printed plotted and explored try in an r console typing rs by itself or plot rs the method reformat textreg gives a nice table see e table of summary statistics for the nal phrases the by side summary table such as table is made by passing a list of textreg result objects to make list table the method calc loss rs gives the nal loss of a result rs and predict rs will return individual document level predictions of the labeling the method rule to matrix rs gives back the n r design matrix for the nal selected r phrases including intercept to pick a tuning parameter one can use cs find threshold c labeling ban words this method returns a r length list of numbers the rst number is the choice of c that will return a null model for the labeling given and the subsequent r numbers constitute our found c values that return a null model under a random permutation of the labeling holding the zeros xed it takes the same parameters as textreg except for maxiter and c be sure to use the same remaining values for both calls so that find threshold c culminates with a c corresponding to the correct model family for exploring text sample fragments phrase labeling corpus is useful see also grab fragments to prole specic phrases possibly even phrases not in the results use make phrase count table one can make cluster plot of how phrases relate with cluster phrases rs or make matrices of co occurrence of phrases using make phrase correlation chart rs all of the above and a bit more is demonstrated and more fully explained in the vignette bathtub demo that comes with the package please read through it for further discussion and ideas references jinzhu jia luke miratrix bin yu brian gawalt laurent el ghaoui luke moore and sophie clavier concise comparative summaries ccs of large text corpora with a human experiment the annals of applied statistics david m blei andrew y ng and michael i jordan latent dirichlet allocation the journal of machine learning research georgiana ifrim gokhan bakir and gerhard weikum fast logistic regression for text categorization with variable length n grams in acm sigkdd international conference on knowledge discovery and data mining pages georgiana ifrim and carsten wiuf bounded coordinate descent for biological sequence classication in high dimensional predictor space in acm sigkdd international conference on knowledge discovery and data mining pages matt taddy multinomial inverse regression for text analysis journal of the american statistical association robert tibshirani iain johnstone b efron and t hastie least angle regression the annals of statistics j grimmer a bayesian hierarchical topic model for political texts measuring pressed agendas in senate press releases political analysis january jonathan chang jordan boyd graber sean gerrish chong wang and david m blei in neural information reading tea leaves how humans interpret topic models processing systems nips jonathan m bischof and edoardo m airoldi capturing semantic content with word frequency and exclusivity in international conference on machine learning inburgh scotland february gerard salton and christopher buckley term weighting approaches in automatic text retrieval information processing management g salton developments in automatic text retrieval science taku kudo and yuji matsumoto a boosting algorithm for classication of structured text in conference on empirical methods in natural language processing pages barcelona spain association for computational lingusitics robert schapire and yoram singer boostexter a boosting based system for text categorization machine learning alexander genkin david d lewis and david madigan large scale bayesian logistic regression for text categorization technometrics tong zhang and frank j oles text categorization based on regularized linear ication methods information retrieval t hastie robert tibshirani and j h friedman the elements of statistical learning springer hui zou and trevor hastie regularization and variable selection via the elastic net journal of the royal statistical society series b statistical methodology f mosteller and d l wallace applied bayesian and classical inference the case of the federalist papers springer verlag e m airoldi a g anderson and s e fienberg who wrote ronald reagan s radio addresses bayesian analysis gary king and will lowe an automated information extraction tool for international conict data with performance as good as human coders a rare events evaluation design international organization september daniel j hopkins and gary king a method of automated nonparametric content analysis for social science american journal of political science c cortes and v vapnik support vector networks machine learning george forman an extensive empirical study of feature selection metrics for text classication the journal of machine learning research s dumais j platt and d heckerman inductive learning algorithms and tions for text categorization in proceedings of the seventh international conference on information and knowledge management acm pages thorsten joachims learning to classify text using support vector machines the springer international series in engineering and computer science springer stuart rose dave engel nick cramer and wendy cowley automatic keyword traction from individual documents in michael w berry and jacob kogan editors text mining applications and theory pages john wiley sons ltd unknown eibe frank gordon w paynter ian h witten carl gutwin and craig g manning domain specic keyphrase extraction in the sixteenth international joint conference on articial intelligence pages california gan kaufmann jilin chen benyu zhang dou shen qiang yang zheng chen and qiansheng cheng diverse topic phrase extraction from text collection in world wide web conference pages edinburgh uk may citeseer sean gerrish and david m blei predicting legislative roll calls from text in the international conference on machine learning pages li wei lee and shyi ming chen new methods for text categorization based on a new feature selection method and a new similarity measure between documents in advances in applied articial intelligence pages advances in applied articial intelligence y yang and i o pendersen a comparative study on feature selection in text tion in international conference on machine learning pages nashville us jacob eisenstein amr ahmed and eric p xing sparse additive generative models of text in international conference on machine learning pages bellevue wa usa february laurent el ghaoui guan cheng li viet an duong vu pham ashok n srivastava and kanishka bhaduri sparse machine learning methods for understanding large text corpora application to flight reports in conference on intelligent data derstanding pages june burt l monroe michael p colaresi and kevin m quinn fightin words lexical ture selection and evaluation for identifying the content of political conict political analysis hui zou the adaptive lasso and its oracle properties journal of the american statistical association tong tong wu and kenneth lange coordinate descent algorithms for lasso penalized regression the annals of applied statistics z q luo and p tseng on the convergence of the coordinate descent method for convex dierentiable minimization journal of optimization theory and applications laurent el ghaoui vivian viallon and tarek rabbani safe feature elimination in sparse supervised learning uc berkeley hadley wickham dianne cook heike hofmann and andreas buja graphical inference for infovis ieee transactions on visualization and computer graphics t p speed and bin yu model selection and prediction normal regression annals of the institute of statistical mathematics jerome friedman trevor hastie and rob tibshirani regularization paths for eralized linear models via coordinate descent journal of statistical software martin f porter an algorithm for sux stripping program david meyer kurt hornik and ingo feinerer text mining infrastructure in r journal of statistical software yiming yang and xin liu a re examination of text categorization methods in ceedings of the annual international acm sigir conference on research and development in information retrieval pages acm andreas buja dianne cook and deborah f swayne interactive high dimensional data visualization journal of computational and graphical statistics
