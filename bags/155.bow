l u j l c s c v v i x r a modeling comprehending and summarizing textual content by vinicius guilherme medeiros leandro krug and jose palazzo moreira de ppgc instituto de informatica ufrgs porto alegre rs brazil vwoloszyn guimmachado wives abstract automatic text summarization strategies have been cessfully employed to digest text collections and extract its essential content usually summaries are generated using textual corpora that belongs to the same domain area where the summary will be used nonetheless there are special cases where it is not found enough tual sources and one possible alternative is to generate a summary from a dierent domain one manner to summarize texts consists in using a graph model this model allows giving more importance to words responding to the main concepts from the target domain found in the summarized text this gives the reader an overview of the main text concepts as well as their relationships however this kind of tion presents a signicant number of repeated terms when compared to human generated summaries in this paper we present an approach to produce graph model extractive summaries of texts meeting the target domain exigences and treating the terms repetition problem to ate the proposition we performed a series of experiments showing that the proposed approach statistically improves the performance of a model based on graph centrality achieving better coverage accuracy and call keywords graph model summarization text modeling graph centrality biased summarization introduction automatic text summarization ats systems play a signicant role by tracting essential content from textual documents this is important given the exponential growth of textual information despite not being one of the newest areas of research there are still open ended summarization issues that pose many challenges to the scientic community one of the examples is when one summary must be generated prioritizing sentences that present terms of another specic domain cross domain example is the dancy problem that occurs when a wrong text modeling leads to a repetition of content supported by capes cnpq and fapergs wolozsyn al the cross domain summarization is a strategy to generate biased summaries which generally favors a subject the need for such bias happens in situations when a summary containing specic aspects must be extracted from general purpose documents for instance if a teacher wants to know better what are the educational aspects of a movie she is hoping to use in her class and to do so she is looking to other peoples comments about that movie another example imagine a person who is shopping a new novel hoping to nd one that also presents for instance historical facts of a city during the story most works on summarization rely on supervised algorithms such as cation and regression however the quality of results produced by supervised algorithms is dependent on the existence of a large domain dependent training dataset one drawback of such strategy is that those datasets are not always available and their construction is labor intense and error prone since documents must be manually annotated to train the algorithms correctly unsupervised models conversely are an interesting strategy for a situation where there are not enough textual sources since it does not need a large ing set for the learning process however a common problem of these models is redundancy it happens when a wrong text modeling can benet from the generation of summaries that repeat the most central sentences or select a set of very similar ones in the documents this causes a gain in accuracy but generates redundant summaries with poor coverage of text aspects to meet the cross domain summarization needs and mitigate the redundancy problem we propose an unsupervised graph based model to generate domain summaries the generated graphs are able to uncover the main topics concepts of a document or a set of documents to do so the summarization algorithm focus on the most relevant central nodes using pre determined domain corpora and nodes relationships in our experiments this combination of cross domain generation avoiding redundancy improves graph based ats system s achieving better coverage precision and recall the contributions of this work are the following it is an unsupervised cross domain summarization it does not depend on specic annotated ing set it address the redundancy problem performing a re ranking of the initial centrality index to improve coverage and decrease redundancy and considering two distinct datasets the results obtained in our experiments were signicantly superior to the baselines analyzed the rest of this paper is organized as follows section presents the ground of the area and related work section details the proposed model section provides a case study and section describes the design of our periments section discusses the results section summarizes our conclusions and presents future research directions background automatic text summarization ats techniques have been successfully ployed on user content to highlight the most relevant information among ments regarding techniques usually employed several works have explored supervised learning strategies to predict text relevance tionally the use of regression algorithms consistently improves the prediction of modeling comprehending and summarizing textual content by graphs fig illustration of graph centrality steps where symbols represent text words helpfulness however these supervised techniques need annotated corpus for the training process which for the most of the cross domains cases is unavailable graph centrality is also widely employed on unsupervised extractive marization systems where a graph representation of documents is used to weight sentences relevance on a set of documents based on that central nodes indicate that the sentence they represent is relevant in the group of uments let s be a set of all sentences extracted from the input a graph representation g v e is created where v s and e is a set of edges that connect pairs v then the score of each node is usually given by an algorithm like pagerank or hits figure depicts the general steps of a summarization system based on graph centrality it builds a similarity graph between pairs of sentences it prunes the graph by removing all edges that do not meet a minimum threshold of similarity it uses pagerank to calculate the centrality scores of each node then a greedy strategy is employed where the centrality index produces a ranking of vertices importance which is used to indicate the ranking of the most relevant sentences to compose the nal summary this a well known strategy used as the basis for many novel unsupervised approaches pagerank computes the centrality of nodes where each edge is ered as a vote to determine the overall centrality score of each node in a graph however as in many types of social networks not all of the relationships are considered of equal importance the premise underlying pagerank is that the importance of a node is measured in terms of both the number and the portance of vertices it is related to the pagerank centrality function is given by p vbu p nv where bu is the set containing all neighborhood of u and nv the number of neighborhoods of however this strategy is normally employed with no restrictions that ensure an empty or a minimal intersection between sentences this lack of restrictions would increase the overall redundancy on these approaches wolozsyn al lexrank which is a popular general purpose extractive summarizer relies on a graph representation of the building a complete graph where each sentence from the document set becomes a node and each edge weight is dened by the value of the cosine similarity between the sentences then the centrality index of each node is computed producing a ranking of vertices based on their importance which indicates the ranking of the most relevant sentences to compose the summary this well known strategy is used as the basis for many recent unsupervised approaches nevertheless these approaches do not take into sideration the repetition problem that causes redundancy of words neither they present a conceptual model to meet the cross domain summarization mands thus next section describes an unsupervised cross domain tion model and a post processing algorithm to reduce repetition and improve the coverage of summaries developed model the developed model structures a given text set in a graph model and uses another specialized text set from another domain to put a bias in the extracted summary as already described sometimes it is necessary to extract a specialized summary from a more general purpose text set the example given before is related to extracting educational aspects from user comments in movies besides the domain bias the model also structures a post processing that treats the problem of sentences repetition in figure it is shown how the cross domain redundancy free summary is extracted by using the graph model since the rst steps are the same of a general graph based summary shown in figure this process starts with the output of the general process a graph where each node have a page rank score that represents how central a determined sentence is figure the initial page rank scores are then recomputed by taking in consideration keywords found on an external corpus such keywords are used as a bias to compute the importance of each sentence the nal specialized summary is based on the centrality score of the sentences weighted by the presence of keywords from the external corpora let s be a set of all sentences extracted from the r user s reviews about a single movie the rst step is to build a graph representation g v e where v s and e is a set of edges that connect pairs v the score of each node that represent a sentence is given by the harmonic mean between its centrality score on the graph given by pagerank and the sum of the cies of its specialized keywords stated in equation the pseudo code of the cross domain re scoring is given in algorithm where g is represented as the adjacency matrix w modeling comprehending and summarizing textual content by graphs fig illustration of the cross domain graph centrality building and the processing to avoid redundancy algorithm cross domain re scoring algorithm s b o input a set of sentences extracted from a general purpose corpora amazon s movies reviews r and a corpora b used as a bias output an extractive biased summary o based on the general purpose corpora w u v idf modied if w u v then w v else for each u s do end for for each u s do end for p p for each u s do end for return o w v end if k sim keyword u b the main steps of the cross domain re scoring algorithm are it builds a similarity graph w between pairs of texts of the same product or subject lines the graph is pruned w by removing all edges that do not meet a minimum similarity threshold given by the parameter lines the best parameter obtained in our experiments is wolozsyn al c using pagerank the centrality scores of each node is calculated line using the educational corpora each sentence is scored according the presence of educational keywords line e the nal importance score of each node is given by the harmonic mean between its centrality score on the graph and the sum of its education keywords frequencies line to get the similarity between the two nodes we dene an adapted metric that is the cosine dierence between two corresponding sentence vectors idf modied y wx y tfw xtfw xidfxi where tfw s is the number of occurrences of the word w in the sentence we employed the approach described by to extract the keywords from the nal corpora the similarity between the sentences and the keywords extracted from the external corpora are given by the following equation sim b wx to reduce the textual redundancy problem it was developed an algorithm that employs a clustering technique to nd groups of sentences from the graph of sentences that are both homogeneous and well separated where entities within the same group should be similar and entities in dierent groups dissimilar then it takes the most central sentence from each group to compose the nal summary while graph centrality chooses the sentences based on their ity our algorithm divides the graph into k groups of sentences and chose the most central sentence from each group figure in the literature we nd work employing clustering paradigms to provide a non redundant multi view of textual data the agglomerative hierarchical clustering method is one of them and it creates a hierarchy tree or dendrogram which can be used for sentence coverage searching purposes conceptually the process of agglomerating documents creates a cluster hierarchy for which the leaf nodes correspond to individual sentences and the internal nodes correspond to merged groups of clusters when two groups are merged a new node is created in this hierarchy tree corresponding to this bigger merged group in our work we employed the complete link hierarchical clustering rithm since it achieves better results on the experiments carried out when compared with other clustering techniques such as k means k medoids and em by default we remove stop words and the remaining terms of the sentence are represented as uni grams weighted by the known term inverse document frequency tf idf the pseudo code for decreasing redundancy is displayed in algorithm where g represents a complete graph obtained from the ats approach based on graph centrality and cross domain re scoring l represents the cluster labels extracted using the function and s the nal solution containing k sentences modeling comprehending and summarizing textual content by graphs algorithm post processing redundancy algorithm g p k s input a complete graph g v e where v are the sentences and e is a set of edges that represents the similarity between sentences p the centrality score of each node k number the sentences to extract output ordered list s of sentences s l k for each k do c c s sort nodes by p s end for return s case study as explained before the goal of the presented approach was to build a graph representation of the main concepts of a document or set of documents this representation works as a cross domain summary avoiding redundancy to do so we selected two application domains one to validate the cross domain summary generation and other to validate the redundancy the cross domain generation was tested in the educational domain and the redundancy control was tested in the news domain three datasets were employed to perform the experiments the rst served as a word thesaurus to implement the educational bias in the cross domain generation and it was collected from an educational website teachwithmovies twm where a set of movies are described by teachers with the goal to use them as learning objects inside a classroom the second dataset is amazon movie reviews amr which provides user comments about a large set of movies since we were interested in movies that appeared in both datasets a lter was applied and we ended up with movies to perform our evaluation the third was used to evaluate the post processing that which treats the redundancy problem is a novel corpus which comprises news texts in brazilian portuguese divided into groups which has been fully employed as gold standard for many recent works on content selection and automatic production of summaries next we describe each dataset with more details teaching with movies the teachwithmovies dataset was collected through a crawler developed by us dierent teachers described the movies on the website but each movie has only one description this was a challenge while collecting the data because the information was not standardized or had associated metadata public available on wolozsyn al however we have noticed that some movies presented common information such as movie description rationale for using the movie movie benets for teaching a subject movie problems and warnings for young watchers and v objectives of using this movie in class the developed crawler extracted such information and we have used the movie description since it contains the greatest amount of educational aspects in the end unique movies and video clips were extracted but after matching with the amazon dataset we could use movies this dataset was used as a gold standard to cross domain summary generation amazon movie reviews the amazon movie reviews was collected with a timespan of more than ten years and consists of proximately millions of reviews that include product and user information ratings and a plain text review in table is shown some statistics about the data table amazon movie reviews statistics dataset statistics number of reviews number of users expert users with reviews number of movies mean number of words per review timespan aug oct cstnews in this dataset each group of news has from to texts on the same topic having in average sentences and words it comprises clusters of news texts ally annotated in dierent ways to discursive organization rhetorical structure theory and cross document structure theory annotations the corpus includes manual multi document summaries one for each cluster of news with pression rate in relation to the longest text the texts are manually annotated with high level of agreement more than of human judges using cohen s kappa coecient which is a statistic to measure inter rater agreement for sication tasks that means the annotation agreement is reliable and similar to that in presented in other works for other languages than portuguese for such reason these human generated summaries were used as a gold standard since this post processing is not language dependent and the redundancy is also a problem observed in dierent languages this corpus was used to evaluate this post processing strategy modeling comprehending and summarizing textual content by graphs experiment design this section presents the experimental setting used to evaluate the cross domain summary generation and the post processing that reduces redundancy it scribes the methods employed as the baselines for comparison the educational plans adopted as gold standard and the metrics applied for evaluation as well as details of the experiment performed to assess the approach first baseline the results obtained from our cross domain summary are compared with trank algorithm textrank was chosen because it is also a graph based ing algorithm and has been widely employed in natural language tools textrank essentially decides the importance of a sentence based on the idea of voting or recommending considering that in this approach each edge represents a vote the higher the number of votes that are cast for a node the higher the portance of the node or sentence in the graph the most important sentences compose the nal summary second baseline centrality based ranking has been successfully on recent works to content tion and automatic production of textual summaries lexrank is a well know ats system based on graph centrality that has been used many times in the literature for comparisons purposes due to its good performance since the post processing strategy aims to reduce redundancy in based approaches we employ lexrank as baseline due it uses only the tence centrality index to the ranking task we used mead s implementation of lexrank which is a publicly available for researching purposes framework for text summarization that provides a set of perl components for the rization of texts written in english as well as in other languages such as chinese evaluation metrics rouge n the evaluation was performed by applying rouge oriented understudy for gisting evaluation which is a metric inspired on the bilingual evaluation understudy bleu specically we used rouge n in the evaluation this version of rouge makes a comparison of n grams between the summary to be evaluated and the gold standard in our case cross domain summaries and twm lesson plans respectively we ated the rst words of the summaries obtained by our approach and the baseline since it corresponds to the median size of the gold standard rouge was chosen because it is one of most used measures in the elds of machine translation and automatic text summarization wolozsyn al redundancy an important aspect related to redundancy is lexical cohesion therefore cohesive links between sentences is a positive component of the mary and it has long been considered a key component in assessing content relevance in text summarization however in some cases it could improve mean redundancy in summaries to show how redundancy would aect a document summarizer we perform a comparison between the baseline and man gold standard summary coverage it is the extent to which all words of the automatic summaries are found in the source documents in other words it is a global score assessing to what extent the candidate summary covers the text given as input results the next subsections present the evaluation results cross domain summaries in this section we present cross domain summaries evaluation regarding the adopted baselines concerning precision recall and score obtained by using rouge the gold standard utilized in the experiments as already stated is the cational description extracted from the twm website table shows the mean precision recall and f score considering both our cross domain strategy and textrank the gold standard used as the baseline the results presented in table show that our strategy outperformed the baseline in all measurements carried out regarding precision the dierences range from to percentage points pp on all rouge n analyzed where n is the size of the n gram used by rouge using wilcoxon statistical test with a signicance level of we veried that our strategy is statistically superior when compared to the baseline regarding recall the dierences are also in favor of our strategy ranging from to pp when compared to the baseline regarding the distribution of rouge s results in fig it is shown a boxplot indicating that our strategy results are not only better in mean but also in terms of lower and upper quartiles minimum and maximal values post processing in this section we will discuss the results obtained in our experiments regarding the adopted baselines in terms of coverage redundancy precision and recall using cstnews redundancy and coverage figures and show that our post processing strategy outperformed the unsupervised baseline generation summaries with less redundancy and more coverage being closer to the human gold standard maries the mean redundancy dierences range from to percentage modeling comprehending and summarizing textual content by graphs table mean of rouge results achieved by the baseline column a and our cross domain strategy column b rouge n column a column b p values f f f fig distribution of rouge results points pp when compared to lexrank in terms of coverage the mean ence is up to pp using a wilcoxon statistical test with a signicance level of we veried that our strategy results are statistically superior both in redundancy and coverage precision and recall figure and show that our strategy also formed the unsupervised baseline in terms of recall and precision obtained using for recall the mean dierences ranging from to pp when compared to lexrank for precision the mean dierences ranging from to pp in all cases using the wilcoxon statistical test with a signicance level of we veried that our strategy results are statistically superior in all cases wolozsyn al fig mean redundancy fig mean coverage fig mean recall fig mean precision conclusion in this paper we presented an approach to generate cross domain summaries based on graphs that are able to represent the main concepts of a document or set of documents the proposed approach also reduces text redundancy in the generated summaries we showed that our approach achieved statistically rior results than textrank a general summary algorithm and lexrank another general summary algorithm the proposed algorithms require no training data which avoids costly and error prone manual training annotations compared to the baselines our proach outperforms the unsupervised techniques in terms of precision and recall statistically reduces redundancy and improves coverage and is easy to plug into any standard graph centrality approach in any domain our periments were performed in two domains the educational and the news one attesting the approach versatility finally it is also important to state that we found out a considerable number of highly helpful sentences with low centrality indexes which lead us to consider the investigation of other techniques to select the most relevant sentences to compose the movies educational description it is also important to rearm the approach source language independence for that reason we consider in the future to extend the evaluation using dierent languages and summaries length modeling comprehending and summarizing textual content by graphs references aggarwal zhai mining text data springer science business media al dhelaan al suhaim sentiment diversication for short review rization in proceedings of the international conference on web intelligence pp acm cardoso jorge pardo al exploring the rhetorical structure theory for multi document summarization in congreso de la sociedad espanola para el procesamiento del lenguaje natural xxxi sociedad espanola para el procesamiento del lenguaje natural sepln cardoso maziero jorge seno di felippo rino nunes pardo cstnews a discourse annotated corpus for single and multi document summarization of news texts in brazilian portuguese in ings of the rst brazilian meeting pp carletta assessing agreement on classication tasks the kappa statistic putational linguistics cheng tran qu relin relatedness and informativeness based ity for entity summarization the semantic web iswc pp condori pardo opinion summarization methods comparing and extending extractive and abstractive approaches expert systems with tions da cunha torres moreno sierra on the development of the rst spanish treebank in proceedings of the linguistic annotation workshop pp association for computational linguistics dias pardo al a discursive grid approach to model local coherence in multi document summaries in annual meeting of the special terest group on discourse and dialogue association for computational linguistics acl erkan radev lexrank graph based lexical centrality as salience in text summarization journal of articial intelligence research ganesan zhai han opinosis a graph based approach to abstractive summarization of highly redundant opinions in proceedings of the national conference on computational linguistics pp association for computational linguistics kleinberg authoritative sources in a hyperlinked environment journal of the acm jacm lin rouge a package for automatic evaluation of summaries in text marization branches out proceedings of the workshop pp maziero hirst pardo al semi supervised never ending ing in rhetorical relation identication in international conference on recent advances in natural language processing bulgarian academy of sciences mcauley leskovec from amateurs to connoisseurs modeling the evolution of user expertise through online reviews in proceedings of the international conference on world wide web pp www international world wide web conferences steering committee republic and canton of geneva switzerland mihalcea tarau textrank bringing order into texts association for putational linguistics nobrega pardo improving content selection for update tion with subtopic enriched sentence ranking functions int comput linguistics appl page brin motwani winograd the pagerank citation ranking bringing order to the web wolozsyn al poibeau saggion piskorski yangarber multi source multilingual information extraction and summarization springer science business media radev allison blair goldensohn blitzer celebi dimitrov drabek hakim lam liu al mead a platform for ment multilingual text summarization saggion poibeau automatic text summarization past present and future in multi source multilingual information extraction and summarization pp springer dos santos ulbrich woloszyn vieira ddc outlier ing medication errors using unsupervised learning ieee journal of biomedical and health informatics thomas beutenmuller de la puente remus bordag exb text summarizer in sigdial conference pp wan co regression for cross language review rating prediction in ings of the annual meeting of the association for computational linguistics volume short papers pp association for computational linguistics soa bulgaria august woloszyn machado palazzo moreira de oliveira krug wives saggion beatnik an algorithm to automatic generation of educational scription of movies in proceedings of the sbie pp recife brazil woloszyn nejdl distrustrank spotting false news domains in ings of the acm conference on web science pp acm woloszyn dos santos wives becker mrr an unsupervised algorithm to rank reviews by relevance in proceedings of the international ference on web intelligence pp acm wu xu li an unsupervised approach to rank product reviews in fuzzy systems and knowledge discovery fskd eighth international conference on vol pp ieee xiong litman automatically predicting peer review helpfulness in ceedings of the annual meeting of the association for computational guistics human language technologies pp association for tional linguistics portland oregon usa anthology yang yan qiu bao semantic analysis and helpfulness tion of text for online product reviews in proceedings of the annual ing of the association for computational linguistics pp association for computational linguistics beijing china july anthology yang duan lai online public opinion hotspot detection and analysis based on short text clustering using string distance journal of beijing university of technology zeng wu modeling the helpful opinion mining of online consumer reviews as a classication problem in proceedings of the ijcnlp workshop on nlp for social media socialnlp pp asian federation of natural guage processing nagoya japan zhai liu xu jia clustering product features for opinion mining in proceedings of the fourth acm international conference on web search and data mining pp acm
