leveraging word embeddings spoken document summarization kuan shih hung liu hsin min wang berlin chen hsin hsi chen institute information science academia sinica taiwan national taiwan normal university taiwan national taiwan university taiwan kychen journey sinica edu edu ntu edu abstract owing rapidly growing multimedia content available internet extractive spoken document summarization purpose automatically selecting set representative sentences spoken document concisely express important theme document active area research experimentation hand word embedding emerged newly favorite research subject excellent performance natural language processing tasks far aware relatively studies investigating use extractive text speech summarization common thread leveraging word embeddings summarization process represent document sentence averaging word embeddings words occurring document sentence intuitively cosine similarity measure employed determine relevance degree pair representations continued efforts improve representation words paper focuses building novel efficient ranking models based general word embedding methods extractive speech summarization experimental effectiveness proposed methods compared existing state art methods results demonstrate index terms spoken document summarization word embedding ranking model introduction owing popularity internet applications rapidly growing content music video broadcast news programs lecture recordings continuously filling daily life obviously speech important sources information multimedia virtue spoken document summarization sds efficiently content listening associated speech summary extractive sds manages select set indicative sentences spoken document according target summarization ratio concatenate form summary wide spectrum extractive sds methods developed far divided categories methods simply based sentence position structure information methods based unsupervised sentence ranking methods based supervised sentence classification category important sentences selected salient parts spoken document introductory concluding parts methods applied specific domains limited document structures unsupervised sentence ranking methods attempt select important sentences based statistical features sentences words sentences human annotations involved popular methods include vector space model vsm latent semantic analysis lsa method markov random walk mrw method maximum marginal relevance mmr method sentence significant score method unigram language model based ulm method lexrank method submodularity based method integer linear programming ilp method statistical features include term word frequency linguistic score recognition confidence measure prosodic information contrast supervised sentence classification methods gaussian mixture model gmm bayesian classifier support vector machine svm conditional random fields crfs usually formulate sentence selection binary classification problem sentence included summary interested readers refer comprehensive reviews new insights major methods developed applied good success wide range text speech summarization tasks different methods explore paper word embedding methods use extractive sds recently demonstrated excellent performance natural language processing tasks relational analogy prediction sentiment analysis sentence completion central idea learn continuously distributed vector methods representations words neural networks probe latent semantic syntactic cues turn induce similarity measures words sentences documents common thread leveraging word embedding methods nlp related tasks represent document query sentence averaging word embeddings corresponding words occurring document query sentence intuitively cosine similarity measure applied determine relevance degree pair representations framework ignores inter dimensional correlation vector representations mitigate deficiency propose novel use triplet learning model enhance estimation similarity degree pair representations addition word embedding methods founded probabilistic objective function probabilistic similarity measure natural choice non probabilistic ones consequently propose new language model based framework incorporates word embedding methods document likelihood measure recapitulate continued tremendous efforts improve representation words paper focuses building novel efficient ranking models general word embedding methods extractive sds review word embedding methods known seminal studies developing word embedding methods presented estimated statistical gram language model formalized forward neural network predicting future words context inducing word embeddings representations product attempt motivated extensions develop similar methods probing latent semantic syntactic regularities representation words representative methods include limited continuous bag words cbow model gram model global vector glove model far aware little work contextualize methods use speech summarization continuous bag words cbow model seeking learn statistical language model cbow model manages obtain dense vector representation embedding word directly structure cbow similar feed forward neural network exception non linear hidden layer removed getting heavy computational burden incurred non linear hidden layer model trained large efficiently retains good performance formally given sequence words objective function cbow maximize log probability wwp window size context words considered central word denotes length training corpus wwp exp exp denotes vector representation word position size vocabulary denotes weighted average vector representations context words concept cbow motivated distributional hypothesis states words similar meanings occur similar contexts suggested look word representation capture context distributions skip gram model contrast cbow model model employs inverse training objective learning word representations simplified feed forward neural network given word sequence objective function maximize following log probability log window size context words central word conditional probability computed exp exp denote word representations words positions implementations cbow hierarchical soft max algorithm negative sampling algorithm training process efficient effective respectively global vector glove model glove model suggests appropriate starting point word representation learning associated ratios occurrence probabilities prediction probabilities precisely glove makes use weighted squares regression aims learning word representations preserving occurrence frequencies pair words log denotes number times words occur pre defined sliding context window monotonic smoothing function modulate impact pair words involved model training denote word representation bias term word respectively analytic comparisons analytic comparisons word embedding methods different model structures learning strategies cbow adopt line learning strategy parameters word representations trained sequentially order training samples change resulting models dramatically contrast glove uses batch learning strategy accumulates statistics entire training updates model parameters second worthy note trained negative implicit explicit relation classic weighted matrix factorization approach major difference glove concentrate rendering word word occurrence matrix weighted matrix factorization usually concerned decomposing word document matrix glove algorithm sampling observations relation word embedding methods matrix factorization bring notion leveraging value decomposition svd method alternative mechanism derive word embeddings paper given training text corpus word word occurrence matrix element aij log frequency times words occur pre defined sliding context window subsequently svd decomposes sub matrices vua orthogonal matrices diagonal matrix finally row vector matrix column vector matrix symmetric matrix designates word embedding specific word vocabulary worthy note svd derive word representations similar spirit latent semantic analysis lsa word word occurrence matrix instead word document occurrence matrix sentence ranking based word embeddings triplet learning model inspired vector space model vsm straightforward way leverage word embedding methods extractive sds represent sentence document summarized averaging vector representations words occurring sentence document swn probability word given word calculated document sentence respective fixed length dense vector representation relevance degree evaluated cosine similarity measure approach ignores inter dimensional correlation vector representations mitigate deficiency cosine similarity measure employ triplet learning model enhance estimation similarity degree pair representations loss generality goal learn similarity function assigns higher similarity scores summary sentences non summary sentences denotes sentence representation form column vector summary sentence representation non summary sentence parametric ranking function linear form follows sdr dimension vector representation applying passive aggressive learning algorithm presented derive similarity function triplets obey similarity function distinguish summary non summary sentences safety margin hinge loss function defined loss obtained applying efficient sequential learning algorithm iteratively triplets sentences ranked descending order similarity measure sentences selected sequenced form summary according target summarization ratio document likelihood measure recent school thought extractive sds employ language modeling approach selection important sentences principal realization use probabilistic generative paradigm ranking sentence document expressed simplest way estimate unigram language model ulm based frequency distinct word occurring maximum likelihood criterion swp swn number times word occurs length obviously major challenge facing approach accurately estimate model parameters sentence stimulated document likelihood measure adopted ulm method word representation methods studied paper construct new word based language model predicting occurrence probability arbitrary word taking cbow example wwp exp exp linearly combine associated word based language models words occurring sentence form composite sentence specific language model express document likelihood measure sdp wwp dwn weighting factor set proportional frequency word occurring sentence subject sentences offering higher document likelihoods selected sequenced form summary according target summarization ratio experimental setup dataset study matbn broadcast news collected academia sinica public television service foundation taiwan november april corpus segmented separate stories transcribed manually story contains speech studio anchor field reporters interviewees subset broadcast news documents compiled november august reserved summarization experiments chose documents test set remaining documents held development set reference summaries generated ranking sentences manual transcript spoken document importance assigning score sentence document reference summaries annotated subjects assessment summarization performance adopted widely rouge metrics experimental results reported obtained calculating scores rouge metrics summarization ratio set text news documents compiled period broadcast news documents estimate related models compared paper cbow glove svd subset hour speech data matbn compiled november december bootstrap acoustic training minimum phone error rate mpe criterion training data selection scheme vocabulary size thousand words average word error rate automatic transcription experimental results outset assess performance levels practiced state art summarization methods extractive sds serve baseline systems paper including based summarization method ulm vector space methods vsm lsa mmr graph based methods mrw lexrank submodularity method integer linear programming ilp method results illustrated table denotes results obtained based manual transcripts spoken documents denotes results speech recognition transcripts contain recognition errors noteworthy observations drawn table graph based methods mrw lexrank competitive perform better vector space methods table summarization results achieved studied state art unsupervised methods table summarization results achieved word embedding methods conjunction cosine similarity measure method ulm vsm lsa mmr mrw lexrank ilp text documents spoken documents rouge rouge vsm lsa mmr case case situation reversed reveals imperfect speech recognition affect graph based methods vector space methods possible reason phenomenon speech recognition errors lead inaccurate similarity measures pair sentences pagerank like procedure graph based methods turn performed based problematic measures potentially leading degraded results second lsa represents sentences spoken document document latent semantic space instead index term word space performs slightly better vsm cases ilp achieve best results case comparable performance methods case finally ulm shows competitive results compared state art methods confirming language modeling applicability approach speech summarization turn investigate utilities state art word embedding methods cbow glove proposed svd method section working conjunction cosine similarity measure speech summarization results shown table results observations art word embedding methods cbow glove disparate model structures learning strategies achieve comparable results cases methods outperform conventional vsm model achieve level performance lsa mmr improved versions vsm perform worse mrw lexrank ilp case surprise proposed svd method outperforms word embedding methods substantial margin case slightly case noted svd method outperforms cbow glove lsa mmr outperforms methods compared table case learning model outperforms set experiments evaluate capability triplet learning model improving measurement similarity applying word embedding methods speech summarization results shown table table observations drawn clear triplet baseline cosine similarity measure table cases indicates triplet learning able improve measurement similarity degree sentence ranking considering inter dimensional correlation similarity measure vector representations beneficial second cbow triplet learning outperforms methods compared table cases note learning resort set documents reference summaries comparison unfair methods table unsupervised ones development set section learn far figured systematic text documents spoken documents rouge rouge table summarization results achieved word embedding methods conjunction triplet learning model text documents spoken documents rouge rouge table summarization results achieved word embedding methods conjunction document likelihood measure text documents spoken documents rouge rouge method cbow glove svd method cbow glove svd method cbow glove svd effective ways incorporate word embeddings existing supervised speech summarization methods leave future work set experiments pair word embedding methods document likelihood measure extractive sds deduced sentence based language models linearly combined ulm computing document likelihood results shown table comparing results word embedding methods paired cosine similarity measure table evident document likelihood measure works pretty vehicle leverage word embedding methods speech summarization notice cbow outperforms word embedding methods cases previously table combined triplet learning svd document superiority svd triplet learning table comparing results state art methods table word embedding methods document likelihood measure competitive cases likelihood measure preserve conclusions future work paper triplet learning model document likelihood measure proposed leverage word embeddings learned word embedding methods speech summarization addition new svd based word embedding method proposed proven efficient effective existing word embedding methods experimental proposed summarization methods comparable state art methods indicating potential new word embedding based speech summarization framework future work explore effective ways enrich representations words integrate extra cues speaker identities prosodic emotional information proposed framework interested investigating represent spoken techniques documents elegant way evidence indexing supports robust references furui fundamental technologies modern speech recognition ieee signal processing magazine ostendorf speech technology information access ieee signal processing magazine lee chen spoken document understanding organization ieee signal processing magazine vol liu hakkani tur speech summarization chapter spoken language understanding systems extracting semantic information speech tur mori eds new york wiley penn zhu critical reassessment evaluation baselines speech summarization proc acl nenkova mckeown automatic summarization foundations trends information retrieval vol mani maybury eds advances automatic text summarization cambridge mit press baxendale machine index technical experiment ibm journal october gong liu generic text summarization relevance measure latent semantic analysis proc sigir wan yang multi document summarization cluster based link analysis proc sigir carbonell goldstein use mmr diversity based reranking reordering documents producing summaries proc sigir furui speech text speech speech summarization spontaneous speech ieee transactions speech audio processing vol mikolov efficient estimation word representations vector space proc iclr pennington glove global vector word representation proc emnlp tang learning sentiment specific word embedding twitter sentiment classification proc acl collobert weston unified architecture natural language processing deep neural networks multitask learning proc icml kageback extractive summarization continuous vector space models proc cvsc qiu learning word representation considering proximity ambiguity proc aaai miller charles contextual correlates semantic similarity language cognitive processes mikolov distributed representations words phrases compositionality proc iclr morin bengio hierarchical probabilistic neural network language model proc aistats mnih kavukcuoglu learning word embeddings efficiently noise contrastive estimation proc nips levy goldberg neural word embedding implicit matrix factorization proc nips chen weighted matrix factorization spoken document retrieval proc icassp afify gaussian mixture language models speech recognition proc icassp crammer online passive aggressive algorithms journal machine learning research erkan radev lexrank graph based lexical centrality salience text summarization journal artificial intelligent research vol chechik large scale online learning image similarity ranking journal machine learning research lin bilmes multi document summarization budgeted maximization submodular functions proc naacl hlt riedhammer long story short global unsupervised models keyphrase based meeting summarization speech communication vol kupiec trainable document summarizer proc sigir zhang fung speech summarization lexical features mandarin broadcast news proc naacl hlt companion volume galley skip chain conditional random field ranking meeting utterances importance proc emnlp bengio neural probabilistic language model journal machine learning research mnih hinton new graphical models statistical language modeling proc icml norouzi hamming distance metric learning proc nips chen probabilistic generative framework extractive broadcast news speech summarization ieee transactions audio speech language processing vol zhai lafferty study smoothing methods language models applied information retrieval proc sigir wang matbn mandarin chinese broadcast news corpus international journal computational linguistics chinese language processing vol lin rouge recall oriented understudy gisting available evaluation isi edu heigold discriminative training automatic speech recognition modeling criteria optimization implementation performance ieee signal processing magazine vol
