leveraging word embeddings spoken document summarization kuan yu shih hung liu hsin min wang berlin chen hsin hsi chen institute information science academia sinica taiwan national taiwan normal university taiwan national taiwan university taiwan kychen journey sinica edu tw edu tw ntu edu tw abstract owing rapidly growing multimedia content available internet extractive spoken document summarization purpose automatically selecting set representative sentences spoken document concisely express important theme document active area research experimentation hand word embedding emerged newly favorite research subject excellent performance natural language processing tasks far aware relatively studies investigating use extractive text speech summarization common thread leveraging word embeddings summarization process represent document sentence averaging word embeddings words occurring document sentence intuitively cosine similarity measure employed determine relevance degree pair representations continued efforts improve representation words paper focuses building novel efficient ranking models based general word embedding methods extractive speech summarization experimental effectiveness proposed methods compared existing state art methods results demonstrate index terms spoken document summarization word embedding ranking model introduction owing popularity internet applications rapidly growing content music video broadcast news programs lecture recordings continuously filling daily life obviously speech important sources information multimedia virtue spoken document summarization sds efficiently content listening associated speech summary extractive sds manages select set indicative sentences spoken document according target summarization ratio concatenate form summary wide spectrum extractive sds methods developed far divided categories methods simply based sentence position structure information methods based unsupervised sentence ranking methods based supervised sentence classification category important sentences selected salient parts spoken document introductory concluding parts methods applied specific domains limited document structures unsupervised sentence ranking methods attempt select important sentences based statistical features sentences words sentences human annotations involved popular methods include vector space model vsm latent semantic analysis lsa method markov random walk mrw method maximum marginal relevance mmr method sentence significant score method unigram language model based ulm method lexrank method submodularity based method integer linear programming ilp method statistical features include term word frequency linguistic score recognition confidence measure prosodic information contrast supervised sentence classification methods gaussian mixture model gmm bayesian classifier bc support vector machine svm conditional random fields crfs usually formulate sentence selection binary classification problem e sentence included summary interested readers refer comprehensive reviews new insights major methods developed applied good success wide range text speech summarization tasks different methods explore paper word embedding methods use extractive sds recently demonstrated excellent performance natural language processing tasks relational analogy prediction sentiment analysis sentence completion central idea learn continuously distributed vector methods representations words neural networks probe latent semantic syntactic cues turn induce similarity measures words sentences documents common thread leveraging word embedding methods nlp related tasks represent document query sentence averaging word embeddings corresponding words occurring document query sentence intuitively cosine similarity measure applied determine relevance degree pair representations framework ignores inter dimensional correlation vector representations mitigate deficiency propose novel use triplet learning model enhance estimation similarity degree pair representations addition word embedding methods founded probabilistic objective function probabilistic similarity measure natural choice non probabilistic ones consequently propose new language model based framework incorporates word embedding methods document likelihood measure recapitulate continued tremendous efforts improve representation words paper focuses building novel efficient ranking models general word embedding methods extractive sds review word embedding methods known seminal studies developing word embedding methods presented estimated statistical n gram language model formalized forward neural network predicting future words context inducing word embeddings representations product attempt motivated extensions develop similar methods probing latent semantic syntactic regularities representation words representative methods include limited continuous bag words cbow model gram sg model global vector glove model far aware little work contextualize methods use speech summarization continuous bag words cbow model seeking learn statistical language model cbow model manages obtain dense vector representation embedding word directly structure cbow similar feed forward neural network exception non linear hidden layer removed getting heavy computational burden incurred non linear hidden layer model trained large efficiently retains good performance formally given sequence words objective function cbow maximize log probability t t t wwp ct w t w t w ct c window size context words considered central word wt t denotes length training corpus t wwp ct w t w t w ct t v exp v w exp v v t w v w w t denotes vector representation word w position t v size vocabulary denotes weighted average vector representations context words wt concept cbow motivated distributional hypothesis states words similar meanings occur similar contexts suggested look word representation capture context distributions skip gram sg model contrast cbow model sg model employs inverse training objective learning word representations simplified feed forward neural network given word sequence objective function sg maximize following log probability t t j jc log wp t j w t c window size context words central word wt conditional probability computed v exp v t j t wp t j t w w exp v w w v t w v denote word representations words positions implementations cbow sg hierarchical soft max algorithm negative sampling algorithm training process efficient effective respectively t global vector glove model glove model suggests appropriate starting point word representation learning associated ratios co occurrence probabilities prediction probabilities precisely glove makes use weighted squares regression aims learning word representations preserving co occurrence frequencies pair words v v j xf ww j v w v w j w w j log x ww j denotes number times words wi wj occur pre defined sliding context window f monotonic smoothing function modulate impact pair words involved model training vw bw denote word representation bias term word w respectively analytic comparisons analytic comparisons word embedding methods different model structures learning strategies cbow sg adopt line learning strategy e parameters word representations trained sequentially order training samples change resulting models dramatically contrast glove uses batch learning strategy e accumulates statistics entire training updates model parameters second worthy note sg trained negative implicit explicit relation classic weighted matrix factorization approach major difference sg glove concentrate rendering word word occurrence matrix weighted matrix factorization usually concerned decomposing word document matrix glove algorithm sampling observations relation word embedding methods matrix factorization bring notion leveraging value decomposition svd method alternative mechanism derive word embeddings paper given training text corpus word word co occurrence matrix element aij log frequency times words wi wj co occur pre defined sliding context window subsequently svd decomposes sub matrices vua t u v orthogonal matrices diagonal matrix finally row vector matrix u column vector matrix vt u v symmetric matrix designates word embedding specific word vocabulary worthy note svd derive word representations similar spirit latent semantic analysis lsa word word co occurrence matrix instead word document co occurrence matrix sentence ranking based word embeddings triplet learning model inspired vector space model vsm straightforward way leverage word embedding methods extractive sds represent sentence si document d summarized averaging vector representations words occurring sentence si document d v s sw swn s v w probability word wj given word wi calculated document d sentence si d respective fixed length dense vector representation relevance degree evaluated cosine similarity measure approach ignores inter dimensional correlation vector representations mitigate deficiency cosine similarity measure employ triplet learning model enhance estimation similarity degree pair representations loss generality goal learn similarity function r assigns higher similarity scores summary sentences non summary sentences e r v v sd r v v sd j denotes sentence representation form column vector summary sentence si representation non summary sentence sj parametric ranking function bi linear form follows v v sdr v t d wv s dimension vector representation applying passive aggressive learning algorithm presented derive similarity function r triplets obey r v v sd r v v sd j similarity function distinguish summary non summary sentences safety margin hinge loss function defined loss v v sd v s j r v v sd r v v sd w obtained applying efficient sequential learning algorithm iteratively triplets w sentences ranked descending order similarity measure sentences selected sequenced form summary according target summarization ratio document likelihood measure recent school thought extractive sds employ language modeling lm approach selection important sentences principal realization use probabilistic generative paradigm ranking sentence s document d expressed simplest way estimate unigram language model ulm based frequency distinct word w occurring s maximum likelihood ml criterion swp swn s s number times word w occurs s length s obviously major challenge facing lm approach accurately estimate model parameters sentence stimulated document likelihood measure adopted ulm method word representation methods studied paper construct new word based language model predicting occurrence probability arbitrary word wj taking cbow example wwp j exp v w j v exp v w l w v vw l w linearly combine associated word based language models words occurring sentence s form composite sentence specific language model s express document likelihood measure sdp dw j w wwp j sw dwn j weighting factor set proportional frequency word wi occurring sentence s subject sentences offering higher document likelihoods selected sequenced form summary according target summarization ratio experimental setup dataset study matbn broadcast news collected academia sinica public television service foundation taiwan november april corpus segmented separate stories transcribed manually story contains speech studio anchor field reporters interviewees subset broadcast news documents compiled november august reserved summarization experiments chose documents test set remaining documents held development set reference summaries generated ranking sentences manual transcript spoken document importance assigning score sentence document reference summaries annotated subjects assessment summarization performance adopted widely rouge metrics experimental results reported obtained calculating scores rouge metrics summarization ratio set text news documents compiled period broadcast news documents estimate related models compared paper cbow sg glove svd subset hour speech data matbn compiled november december bootstrap acoustic training minimum phone error rate mpe criterion training data selection scheme vocabulary size thousand words average word error rate automatic transcription experimental results outset assess performance levels practiced state art summarization methods extractive sds serve baseline systems paper including lm based summarization method e ulm eq vector space methods e vsm lsa mmr graph based methods e mrw lexrank submodularity method sm integer linear programming ilp method results illustrated table td denotes results obtained based manual transcripts spoken documents sd denotes results speech recognition transcripts contain recognition errors noteworthy observations drawn table graph based methods e mrw lexrank competitive perform better vector space methods table summarization results achieved studied state art unsupervised methods table summarization results achieved word embedding methods conjunction cosine similarity measure method ulm vsm lsa mmr mrw lexrank sm ilp text documents td spoken documents sd rouge l rouge l e vsm lsa mmr td case sd case situation reversed reveals imperfect speech recognition affect graph based methods vector space methods possible reason phenomenon speech recognition errors lead inaccurate similarity measures pair sentences pagerank like procedure graph based methods turn performed based problematic measures potentially leading degraded results second lsa represents sentences spoken document document latent semantic space instead index term word space performs slightly better vsm td sd cases sm ilp achieve best results td case comparable performance methods sd case finally ulm shows competitive results compared state art methods confirming language modeling applicability approach speech summarization turn investigate utilities state art word embedding methods e cbow sg glove proposed svd method section working conjunction cosine similarity measure speech summarization results shown table results observations art word embedding methods e cbow sg glove disparate model structures learning strategies achieve comparable results td sd cases methods outperform conventional vsm model achieve level performance lsa mmr improved versions vsm perform worse mrw lexrank sm ilp td case surprise proposed svd method outperforms word embedding methods substantial margin td case slightly sd case noted svd method outperforms cbow sg glove lsa mmr outperforms methods compared table sd case learning model outperforms set experiments evaluate capability triplet learning model improving measurement similarity applying word embedding methods speech summarization results shown table table observations drawn clear triplet baseline cosine similarity measure table cases indicates triplet learning able improve measurement similarity degree sentence ranking considering inter dimensional correlation similarity measure vector representations beneficial second cbow triplet learning outperforms methods compared table td sd cases note learning w eq resort set documents reference summaries comparison unfair methods table unsupervised ones development set c section learn w far figured systematic text documents td spoken documents sd rouge l rouge l table summarization results achieved word embedding methods conjunction triplet learning model text documents td spoken documents sd rouge l rouge l table summarization results achieved word embedding methods conjunction document likelihood measure text documents td spoken documents sd rouge l rouge l method cbow sg glove svd method cbow sg glove svd method cbow sg glove svd effective ways incorporate word embeddings existing supervised speech summarization methods leave future work set experiments pair word embedding methods document likelihood measure extractive sds deduced sentence based language models linearly combined ulm computing document likelihood eq results shown table comparing results word embedding methods paired cosine similarity measure table evident document likelihood measure works pretty vehicle leverage word embedding methods speech summarization notice cbow outperforms word embedding methods td sd cases previously table combined triplet learning svd document superiority svd triplet learning c table comparing results state art methods table word embedding methods document likelihood measure competitive cases likelihood measure preserve conclusions future work paper triplet learning model document likelihood measure proposed leverage word embeddings learned word embedding methods speech summarization addition new svd based word embedding method proposed proven efficient effective existing word embedding methods experimental proposed summarization methods comparable state art methods indicating potential new word embedding based speech summarization framework future work explore effective ways enrich representations words integrate extra cues speaker identities prosodic emotional information proposed framework interested investigating represent spoken techniques documents elegant way evidence indexing supports robust references s furui al fundamental technologies modern speech recognition ieee signal processing magazine pp m ostendorf speech technology information access ieee signal processing magazine pp l s lee b chen spoken document understanding organization ieee signal processing magazine vol pp y liu d hakkani tur speech summarization chapter spoken language understanding systems extracting semantic information speech g tur r d mori eds new york wiley g penn x zhu critical reassessment evaluation baselines speech summarization proc acl pp nenkova k mckeown automatic summarization foundations trends information retrieval vol pp mani m t maybury eds advances automatic text summarization cambridge ma mit press p b baxendale machine index technical experiment ibm journal october y gong x liu generic text summarization relevance measure latent semantic analysis proc sigir pp x wan j yang multi document summarization cluster based link analysis proc sigir pp j carbonell j goldstein use mmr diversity based reranking reordering documents producing summaries proc sigir pp s furui al speech text speech speech summarization spontaneous speech ieee transactions speech audio processing vol pp t mikolov et al efficient estimation word representations vector space proc iclr pp j pennington al glove global vector word representation proc emnlp pp d tang et al learning sentiment specific word embedding twitter sentiment classification proc acl pp r collobert j weston unified architecture natural language processing deep neural networks multitask learning proc icml pp m kageback et al extractive summarization continuous vector space models proc cvsc pp l qiu al learning word representation considering proximity ambiguity proc aaai pp g miller w charles contextual correlates semantic similarity language cognitive processes pp t mikolov al distributed representations words phrases compositionality proc iclr pp f morin y bengio hierarchical probabilistic neural network language model proc aistats pp mnih k kavukcuoglu learning word embeddings efficiently noise contrastive estimation proc nips pp o levy y goldberg neural word embedding implicit matrix factorization proc nips pp k y chen al weighted matrix factorization spoken document retrieval proc icassp pp m afify al gaussian mixture language models speech recognition proc icassp pp k crammer al online passive aggressive algorithms journal machine learning research pp g erkan d r radev lexrank graph based lexical centrality salience text summarization journal artificial intelligent research vol pp g chechik al large scale online learning image similarity ranking journal machine learning research pp h lin j bilmes multi document summarization budgeted maximization submodular functions proc naacl hlt pp k riedhammer et al long story short global unsupervised models keyphrase based meeting summarization speech communication vol pp j kupiec et al trainable document summarizer proc sigir pp j zhang p fung speech summarization lexical features mandarin broadcast news proc naacl hlt companion volume pp m galley skip chain conditional random field ranking meeting utterances importance proc emnlp pp y bengio al neural probabilistic language model journal machine learning research pp mnih g hinton new graphical models statistical language modeling proc icml pp m norouzi et al hamming distance metric learning proc nips pp y t chen al probabilistic generative framework extractive broadcast news speech summarization ieee transactions audio speech language processing vol pp c zhai j lafferty study smoothing methods language models applied information retrieval proc sigir pp h m wang al matbn mandarin chinese broadcast news corpus international journal computational linguistics chinese language processing vol pp c y lin rouge recall oriented understudy gisting available evaluation isi edu g heigold et al discriminative training automatic speech recognition modeling criteria optimization implementation performance ieee signal processing magazine vol pp
