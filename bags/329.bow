multi view sequence sequence models conversational structure abstractive dialogue summarization jiaao chen school interactive computing georgia institute technology edu diyi yang school interactive computing georgia institute technology edu abstract text summarization lenging interesting problems nlp attention paid summarizing structured text like news ports encyclopedia articles summarizing conversations essential human machine interaction tant pieces information scattered utterances different speakers remains relatively investigated work proposes multi view sequence sequence model rst extracting tional structures unstructured daily chats different views represent tions utilizing multi view coder incorporate different views erate dialogue summaries experiments large scale dialogue summarization corpus demonstrated methods signicantly outperformed previous state art els automatic evaluations man judgment discussed specic challenges current approaches faced task publicly released code com multi view introduction live information age cations human human machine increasing exponentially form textual alogues users users agents kester challenging time consuming review content starting tions especially chatting history long gao process ganize interaction activities concise structured data conversation summarization technically socially important existing research efforts text rization focused single speaker uments like news reports nallapati scientic publications nikolov encyclopedia articles liu structured text usually orate core idea person point view information clear graphs sections different structured documents conversations informal bose repetitive sprinkled false starts channeling reconrmations hesitations speaker interruptions sacks salient information scattered chat ing current summarization models hard focus informative utterances conversation table example turns informal words breviations emoticons introduce new forms challenges task summarization calls design development new ods dialogue summarization instead directly applying current document summarization models recent research sation summarization directly deploying existing document summarization models gliwa exploring multi sentence sion shang utilized specic conversational structures refer way utterances organized order conversation meaningful able understandable sacks alogues key factor differentiates dialogues structured documents way language socially things words gether persons conversation dynamic structures organize utterances certain orders conversation meaningful enjoyable understandable sacks exceptions ing topic segmentation liu dialogue acts goo chen key point sequence liu need conversation topic view stage view james hey thinking hannah nice james hannah james sleep miss hoping hannah early work tomorrow greetings today plan openings intention james tomorrow plan tomorrow hannah honest plans tomorrow evening james sat hannah yeah sure available sat pick james hannah sounds good plan saturday discussion pick time conclusion summary james misses hannah agree james pick hannah saturday table example conversation samsum gliwa topic view stage view extracted methods human annotated summary extensive expert annotations discourse chen liu code conversations based topics liu fails capture rich conversation structures dialogues single conversation viewed different perspectives resulting multiple conversational discourse patterns instance table based topics discussed topic view galley liu segmented greetings today plan plan tomorrow plan saturday pick time conversation progression perspective stage view ritter paul althoff dialogue categorized openings intention discussion conclusion coarse perspective global view conversations treated utterance serve segment discrete view models utilized xed topic view conversation joty liu fail capture comprehensive nuanced conversational structures information loss introduced conversation encoder lead larger error cascade decoding stage gaps propose combine multiple diverse views tions order generate precise summaries sum contributions propose utilize rich conversational structures tured views topic view stage view generic views global view discrete view abstractive conversation summarization sign multi view sequence sequence model consists conversation encoder encode ferent views multi view decoder view attention generate dialogue summaries perform experiments large scale sation summarization dataset samsum gliwa demonstrate effectiveness proposed methods conduct thorough error analyses discuss specic challenges current approaches faced task related work document summarization document rization received extensive research attention especially abstractive summarization instance rush introduced use sequence sequence models abstractive text summarization proposed pointer generator network allow copying words source text handle oov issue avoid generating repeated content paulus chen bansal utilized reinforcement learning select correct content needed summarization large scale pre trained language models liu lapata raffel lewis duced improve summarization mance line work explored long document summarization utilizing discourse structures text cohan introducing hierarchical models fabbri modifying tion mechanisms beltagy recent studies looking faithfulness figure model architecture different views conversations rst extracted automatically encoded conversation encoder combined multi view decoder generate summaries conversation encoder view consists blocks encoded separately block representations encoded lstm represent view multi view decoder model decides attention weights different views attend token different views multi view attention document summarization cao zhu order enhance information consistency summaries input including topic segments conversational stages alogue overview utterances design view model dialogue summarization dialogue summarization comes summarization dialogues shang proposed simple multi sentence compression technique summarize meetings zhao zhu introduced turn based hierarchical models encoded turn terance rst aggregated sentation generate summaries studies paid attention utilizing conversational analysis generating dialogue summaries leveraging dialogue acts goo chen key point sequence liu topics liu needed large human annotation dialogue acts key points visual focus goo chen liu utilized topical information conversations liu prior work largely ignored diverse conversational structures dialogues instance reply relations participants mayeld zhu dialogue acts ritter paul conversation stages thoff models utilized xed topic view conversation galley joty fail capture sive nuanced conversational structures information loss introduced versation encoder lead larger error cascade decoding stage gaps pose leverage diverse conversational structures method conversations interpreted different views single view enables model focus specic aspect conversation advantages rich conversation views design multi view sequence sequence model figure rstly extracts different views conversations section encodes generate summaries section conversation view extraction conversation summarization models easily stray sorts information ous speakers utterances especially versations long naturally informative structures form small blocks plicitly extracted long conversations models able understand better ganized way rst extract different views structures conversations topic view conversations structured documents organized topics coarse grained ture honneth instance phone chat possess pattern greetings invitation party details rejection ical perspective explicit view topic help models interpret conversations cisely generate summaries cover important topics combine classic topic segment stage interpretation freq words openings intentions discussions conclusions hey good yeah going time need like think want know time come tomorrow meet thanks great thank sure table frequent words appearing stage interpretations different stages versation followed discussions details nally conclude certain endings table shows example stage view global view discrete view addition aforementioned structured views tions naturally viewed relatively coarse perspective global view nates utterances giant block gliwa discrete view separates utterance distinct block liu chen gliwa multi view sequence sequence model extend generic sequence sequence models encode combine different conversation views better utilize semantic information recent trained models implement base encoders decoders transformer based pre trained model bart lewis note multi view sequence sequence model agnostic bart initialized block token conversation encoder given conversation der specic view blocks rst encoded conversation encoder bart encoder shown figure hidden representations note add special tokens ning block use tokens tations describe block depict different views hidden vectors aggregate information blocks conversation lstm layers hochreiter schmidhuber figure allowed state transitions hmm versation model conversation stages tences encoded representations conversation stages evolve increasing order algorithm choi segments sations based inter sentence similarities recent advanced sentence representations bert reimers gurevych extract topic view specically utterance conversation rst encoded hidden vectors sentence bert conversation divided blocks ctopic block contains consecutive ances topic view described table stage view way things words socially people conversation organizes utterances certain orders meaningful enjoyable understandable sacks althoff example seling conversations found follow common pattern introductions problem exploration problem solving wrap althoff conversation stage view provides high level sketches functions goals different parts conversations help models focus stages key information follow althoff extract stages hidden markov model hmm pose xed ordering stages allow transitions current stage observations hmm model coded representations sentence bert set number hidden stages similar topic view extraction segment sations blocks cstage block contains consecutive utterances interpret inferred stages itatively visualize frequent words appearing stage table found conversations daily chats usually start openings introduce goals focus use hidden state current view denoted represent experiments dataset baselines multi view decoder different views vide different types conversational aspects models learn determine set utterances deserve attention der generate better dialogue summaries result ability strategically combine ent views essential end propose transformer based multi view decoder integrate encoded representations different views generate summaries shown figure input decoder contains previously generated tokens multi view decoder token predicted parameter learned different generic transformer decoder introduce multi view attention layer block multi view attention layer rst cides importance view tanh exp exp randomly initialized context vector parameters avoid attention weights similar views actually encoded similar context utilize sharpening function temperature attention weights behave like hot vector multi head attention performed conversation tokens different views form separately attended results combined based view attention weights continue forward passing training minimize cross entropy loss ing training log specically apply teacher forcing strategy training time inputs previous tokens ground truth test time inputs ous tokens predicted decoder evaluate model large scale dialogue summary dataset samsum gliwa dialogues human written maries data statistics shown table samsum contains messenger like conversations daily topics chit chats arranging meetings discussing events compare multi view sequence sequence model view bart baseline models pointer generator ing gliwa added separators utterance discrete view input pointer generator model dynamicconv news followed gliwa use initialize token embeddings ford added news marization corpus cnn nallapati extra training data fast abs enhanced chen bansal rst selects salient sentences rewrites abstractively sentence level policy gradient methods combined global view gliwa bart generic views lewis utilized bart denoising autoencoder pretraining sequence sequence models gether generic views global view discrete view bart large model default settings model loaded pre trained bert base nli stsb sentence bert representations utterance extracting topic view set window size std coefcient extracting stage view set number hidden states hmm hyper parameters set grid search bart tured views stage topic views set parameters bart generic views com pytorch fairseq details shown section appendix com sentence transformers conversations train dev test participants std mean interval mean turns std interval mean reference length std interval table samsum dataset statistics interval denotes minimum maximum range model views pointer generator dynamicconv fast abs enhanced dynamicconv news bart bart multi view bart discrete global global discrete discrete global stage topic global stage global topic topic stage rouge table rouge scores different models test set results averaged runs meant methods utilized views introduced multi view bart experimented ent view combinations best generic view global view combined structured views stage topic view separately best structured views combined topic stage settings bart encoder decoder kept identical baselines layer lstm encoding sections learning rate section encoder multi view attention set temperature beam search size inference models results figure relations rouge scores number participants turns conversations quantitative results evaluated models standard metric rouge score stemming lin och reported rouge results test set different models shown table pared pointer generator reinforcement learning select important sentences rst fast abs enhanced slightly increased scores adding pre trained embeddings extra documents training data lightweight convolution models dynamicconv news lead ter rouge scores pre trained based model bart generic views rouge scores improved signicantly bart followed bart com pltrdy rouge note different tools ate different rouge scores global outperformed bart discrete especially terms rouge scores segmenting versations blocks structured views stage view topic view boosted mance suggesting extracted conversation structures help conversational encoders capture nuanced informative aspects dialogs performance boost bining generic global view topic conversational stage views partially coarse granularity global view ment structured views contrast utilizing structured views topic view stage view increased rouge scores consistently cating effectiveness synthesizing informative conversation blocks introduced views visualized attention weight distributions figure human evaluation results mean score model shown box plot model analysis discussion highest human annotation scores signicantly higher student test generic crete global view structured stage topic view proved effectiveness combing different views far achieved reasonable tion performance study dialog marization challenging future research advance direction closer look dialogue summarization dataset samsum model generation errors certain lenges existing approaches struggling challenges dialog summarization conduct thorough examination lenges conversation summarization nized categories informal language use conversations especially online contexts ter reddit jackson moulinier tain typos word abbreviations slang cons emojis making hard represented summarized multiple participants shown figure conversations speakers harder summarized require els accurately differentiate language styles content different speakers similar multiple characters issue story summarization zhang multiple turns similar long document summarization xiao carenini conversations utterances contain information processed harder summarized referral coreference people usually fer mention names use coreference messages troduces extra difculty dialogue rization challenge exists reading comprehension chen ment summarization falke repetition interruption information generally scattered sation speakers interrupt stage view topic view best model appendix found contributions topic views slightly prominent compared stage views communicated different structured views complement sharing dialogue tent note gains multi view bart topic stage mainly precision scores recall scores kept comparable gesting proposed model produced fewer irrelevant tokens preserving necessary mation generated summary impact participants turns ized impact essential components conversations number participants turns rouge scores best performing model multi view bart topic view stage view figure number pants turns increases rouge scores decrease dicating difculty conversation rization increased participants involved conversations utterances qualitative human evaluation ducted human annotations evaluate extracted dialogue summaries addition rouge scores similar gliwa asked human annotators amazon mechanical turk rate summary randomly sampled summaries total scale means summary poor extracted irrelevant formation sense means understandable gave concise overview text refers summary tracted relevant information mistakes score summary averaged different annotators intra class correlation indicating erate agreement koo shown figure consistent rouge scores table multi view model achieved mturk generic informal language multiple participants multiple turns referral coreference repetition interruption negations rhetorical role language change challenge table breakdown challenges dialogue marization based analyses sampled versations rouge scores challenge errors missing information redundancy wrong references incorrect reasoning improper gendered pronouns table common error types model pared golden reference sampled tions rouge scores error type error reconrm channeling repeat selves unique discourse challenge logue summarization examined summaries generated performing model compared ground truth maries observed major error types negations rhetorical questions long standing problem nlp eld negation related issues frequent conversations question answer exchanges speakers role language change conversations usually involve speaker role speaker shift questioner answerer requiring summarization model dynamically deal speaker roles associated language rst sonal pronouns randomly sampled test set classied lenge taxonomy conversation category labels aforementioned challenges labeled generic usually marked generic shorter simple structure table presents percentage type challenge category performances best model multi view bart topic view stage view observed referral erence role language change frequent challenges logue summarization task faced expected generic conversations relatively easier marize best model performed relatively worse came repetition interruption multiple turns referral coreference ing intelligent summarization methods tackle challenges missing information content mentioned references missing generated summaries redundancy content occurred generated summaries mentioned references wrong references generated summaries contain information faithful original dialogue associate tions locations wrong speaker incorrect reasoning generated summaries reasoned relations dialogues incorrectly came wrong conclusions improper gendered pronouns summaries improper gendered pronouns misuse gendered pronouns annotated set randomly pled summaries error type taxonomy summary category labels categorized summary belong error types table presents breakdown error types category rouge scores found missing information quent error type indicating current tion models struggled identifying key mation incorrect reasoning percentage worst despite ing minor type improper gendered pronouns severely decrease relatively low rouge scores associated incorrect reasoning wrong erences urged better summarization models ing faithfulness dialogue summarization analyzed set examples shown appendix analysis baselines displayed appendix tract different views future plan tate data explore supervised tation models introduce conversation structures like dialogue acts oya carenini joty hoque tive dialogue summarization acknowledgment like thank anonymous reviewers helpful comments members georgia tech salt group feedback acknowledge support nvidia corporation donation gpu research references tim althoff kevin clark jure leskovec large scale analysis counseling conversations application natural language processing mental health transactions association computational linguistics beltagy matthew peters arman cohan longformer long document transformer ziqiang cao furu wei wenjie sujian faithful original fact aware neural tive summarization aaai danqi chen jason bolton christopher ning thorough examination cnn daily mail reading comprehension task proceedings annual meeting sociation computational linguistics volume long papers pages berlin germany association computational linguistics yen chun chen mohit bansal fast tive summarization reinforce selected sentence rewriting proceedings annual ing association computational tics volume long papers pages bourne australia association computational linguistics freddy choi advances domain pendent linear text segmentation meeting north american chapter association computational linguistics arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang zli goharian discourse aware attention model abstractive summarization long proceedings conference ments north american chapter association computational linguistics human language nologies volume short papers pages new orleans louisiana association tional linguistics figure relations difculties tions errors model relation challenges errors gure relations challenges rors models different types errors correlate different types lenges visualized occurrence heat map figure found model generated good summary generic simple conversations kinds challenges high correlations lead missing information ror wrong references highly associated referral coreference expected references conversations rally increase difculty models associate correct speakers correct actions high relations role language change referral coreference incorrect reasoning indicated interactions multiple participants frequent references easily lead current summarization models reason incorrectly conclusion work proposed multi view sequence model leveraged multiple sational structures topic view stage view generic views global view discrete view generate summaries conversations order strategically combine different views better summary generations propose view sequence sequence model experiments conducted demonstrated effectiveness proposed models terms quantitative qualitative evaluations thorough error ses concluded set challenges current models struggled tate future research conversation summarization lack annotations adopted simple unsupervised segmentation methods alexander fabbri irene tianwei suyi dragomir radev multi news large scale multi document summarization dataset tive hierarchical model proceedings nual meeting association computational linguistics tobias falke christian meyer iryna gurevych concept map based multi document rization concept coreference resolution global importance optimization proceedings eighth international joint conference ral language processing volume long papers pages michel galley kathleen mckeown eric lussier hongyan jing discourse mentation multi party conversation ings annual meeting association computational linguistics pages poro japan association computational tics shen gao xiuying chen zhaochun ren dongyan zhao rui yan standard rization new tasks summarization manifold information bogdan gliwa iwona mochol maciej biesek aleksander wawer samsum corpus human annotated dialogue dataset abstractive summarization proceedings workshop new frontiers summarization pages hong kong china association computational linguistics chih wen goo yun nung chen tive dialogue summarization sentence gated modeling optimized dialogue acts ieee spoken language technology workshop slt sepp hochreiter jurgen schmidhuber neural comput long short term memory axel honneth hans joas social action human nature cup archive peter jackson isabelle moulinier natural language processing online applications text retrieval extraction categorization volume john benjamins publishing shaq joty giuseppe carenini gabriel murray raymond exploiting conversation structure unsupervised topic segmentation emails proceedings conference empirical methods natural language ing pages cambridge association computational linguistics grant kester conversation pieces nity communication modern art univ ifornia press terry koo mae guideline selecting reporting intraclass correlation cients reliability research journal tic medicine mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov luke zettlemoyer bart denoising sequence sequence pre training natural language generation translation comprehension jing aixin sun shaq joty segbot generic neural text segmentation model pointer network ijcai pages jiwei xinlei chen eduard hovy dan jurafsky visualizing understanding neural models nlp proceedings conference north american chapter association computational linguistics human language nologies pages san diego california sociation computational linguistics manling lingyu zhang heng richard radke meeting summaries topic abstractive multi modal meeting summarization proceedings association computational linguistics pages florence italy association tational linguistics annual meeting chin yew lin franz josef och matic evaluation machine translation quality ing longest common subsequence skip bigram statistics proceedings annual ing association computational linguistics page association computational tics chunyi liu peng wang jiang zang jieping automatic dialogue summary generation customer service proceedings acm sigkdd international conference knowledge discovery data mining page new york usa association computing machinery peter liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser noam shazeer generating wikipedia ing long sequences international conference learning representations shaq joty enamul hoque speech act eling written asynchronous conversations task specic embeddings conditional structured proceedings annual models ing association computational linguistics volume long papers pages yang liu mirella lapata text tion pretrained encoders proceedings conference empirical methods ral language processing international joint conference natural language processing emnlp ijcnlp zhengyuan liu nancy chen reading turn turn hierarchical attention architecture ken dialogue comprehension proceedings annual meeting association putational linguistics pages florence italy association computational linguistics nils reimers iryna gurevych bert sentence embeddings siamese networks proceedings conference empirical methods natural language processing international joint conference ral language processing emnlp ijcnlp zhengyuan liu angela sheldon lee nancy chen topic aware generator networks summarizing spoken sations ieee automatic speech recognition understanding workshop asru elijah mayeld david adamson carolyn stein rose hierarchical conversation ture prediction multi party chat proceedings annual meeting special interest group discourse dialogue pages seoul south korea association computational linguistics ramesh nallapati bowen zhou cicero dos santos aglar bing xiang tive text summarization sequence sequence proceedings rnns signll conference computational natural guage learning pages berlin germany association computational linguistics nikola nikolov michael pfeiffer richard hahnloser data driven summarization entic articles corr tatsuro oya giuseppe carenini extractive summarization dialogue act modeling email threads integrated probabilistic approach proceedings annual meeting cial interest group discourse dialogue dial pages michael paul mixed membership markov models unsupervised conversation modeling proceedings joint conference ical methods natural language processing computational natural language learning pages jeju island korea association tational linguistics romain paulus caiming xiong richard socher deep reinforced model abstractive marization international conference ing representations alec radford jeffrey rewon child david luan dario amodei ilya sutskever language models unsupervised multitask learners openai blog alan ritter colin cherry bill dolan pervised modeling twitter conversations man language technologies annual ference north american chapter ation computational linguistics pages los angeles california association tional linguistics alexander rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing pages lisbon portugal association computational linguistics harvey sacks emanuel schegloff gail son simplest systematics tion turn taking conversation studies organization conversational interaction pages elsevier abigail peter liu christopher manning point summarization generator networks proceedings annual meeting association computational guistics volume long papers guokan shang wensi ding zekun zhang toine tixier polykarpos meladianos michalis giannis jean pierre vised abstractive meeting summarization sentence compression budgeted submodular maximization nual meeting association computational linguistics volume long papers pages melbourne australia association tational linguistics proceedings felix angela fan alexei baevski yann dauphin michael auli pay attention lightweight dynamic convolutions tional conference learning representations wen xiao giuseppe carenini extractive summarization long documents combining global local context emnlp ijcnlp weiwei zhang jackie chi kit cheung joel oren generating character descriptions proceedings matic summarization ction aaai conference articial intelligence ume pages colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei peter liu exploring limits transfer learning unied text text zhou zhao haojie pan changjie fan yan liu lin min yang deng cai tive meeting summarization hierarchical tive segmental network learning world wide web conference www page new york usa association computing ery chenguang zhu william hinthorn ruochen qingkai zeng michael zeng xuedong huang meng jiang boosting factual correctness abstractive summarization chenguang zhu ruochen michael zeng dong huang end end abstractive rization meetings henghui zhu feng nan zhiguo wang ramesh pati bing xiang respond conversation structure modeling masked hierarchical transformer model settings load pre trained bert base nli stsb sentence bert representations utterance extracting topic view set window size std coefcient extracting stage view set number hidden states hmm parameters set grid search ing randomly sampled segmented results human bart structured views stage topic views followed parameters bart generic views multi view bart selected different views combine generic view structured view best generic view global view bined structured views stage topic view structured view structured view best single views combined topic stage settings bart encoder decoder kept baseline layer lstm ing sections learning rate section encoder multi view attention set perature beam search size inference models experiments performed tesla memory view attention visualization visualized attention weights distribution stage view topic view best multi view model explore importance stage verses topic figure found topic views prominent stage views tent performances bart topic view bart stage view indicated having com sentence transformers figure attention weights distribution stage view topic view multi view model table index list samples discourse structures topics portant topic stage improve conversation summarization municated different structured views complement sharing dialogue content displayed examples table golden references single view ated summaries combined views ated summaries combined view balance advantages single view generated precise summaries attention weights model learned consistent single view performances supplementary examples model analysis discussion analysis model analysis cussion section paper randomly sampled examples test set samsum reference stage topic james misses hannah agree james pick hannah saturday hannah early tomorrow james pick saturday james hannah saturday stage topic attention weight james pick hannah saturday petra sleepy work today andy nds day boring ezgi working petra needs sleep sleepy ezgi working working ofce today ezgi working petra sleepy wants sleep petra sleepy needs sleep ezgi working ofce table generated summary examples compared references rouge shown summary stage weight topic weight displayed row errors discrete global stage topic multi view missing information redundancy wrong references incorrect reasoning improper gendered pronouns table common error types different models compared golden reference sampled conversations dataset downloaded table provides index list samples table shows error analysis discrete bart global bart stage bart topic bart multi view models observed explicit structures view global view models generated summaries redundancies compared golden ence summaries models easily lost focus massive information introduced certain conversation structures topic view stage view models behaved better terms redundancy incorrect reasoning cated structured views help models better understand conversations view models combined stage view topic view number errors pared single view models suggesting effectiveness combining different views versation summarization org
