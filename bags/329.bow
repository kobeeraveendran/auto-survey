multi view sequence to sequence models with conversational structure for abstractive dialogue summarization jiaao chen school of interactive computing georgia institute of technology edu diyi yang school of interactive computing georgia institute of technology edu abstract text summarization is one of the most lenging and interesting problems in nlp although much attention has been paid to summarizing structured text like news ports or encyclopedia articles summarizing conversations an essential part of human machine interaction where most tant pieces of information are scattered across various utterances of different speakers remains relatively under investigated this work proposes a multi view sequence sequence model by rst extracting tional structures of unstructured daily chats from different views to represent tions and then utilizing a multi view coder to incorporate different views to erate dialogue summaries experiments on a large scale dialogue summarization corpus demonstrated that our methods signicantly outperformed previous state of the art els via both automatic evaluations and man judgment we also discussed specic challenges that current approaches faced with this task we have publicly released our code at com gt multi view introduction we live in an information age where cations between human and human machine are increasing exponentially in the form of textual alogues between users and users agents kester it is challenging and time consuming to review all the content before starting any tions especially when the chatting history becomes very long gao et al how to process and ganize those interaction activities into concise and structured data i e conversation summarization becomes technically and socially important most existing research efforts on text rization have been focused on single speaker uments like news reports nallapati et al see et al scientic publications nikolov et al or encyclopedia articles liu et al where structured text is usually used to orate a core idea in the third person point of view and the information ow is very clear through graphs or sections different from these structured documents conversations are often informal bose and repetitive sprinkled with false starts back channeling reconrmations hesitations speaker interruptions sacks et al and the salient information is scattered in the whole chat ing current summarization models hard to focus on many informative utterances take the conversation in table as an example turns informal words breviations and emoticons all introduce new forms of challenges to the task of summarization this calls for the design and development of new ods for dialogue summarization instead of directly applying current document summarization models there has been some recent research on sation summarization such as directly deploying existing document summarization models gliwa et al and exploring multi sentence sion shang et al however most of them have nt utilized specic conversational structures which refer to the way utterances are organized in order to make the conversation meaningful able and understandable sacks et al in alogues a key factor that differentiates dialogues from structured documents as a way of using language socially of doing things with words gether with other persons the conversation has its own dynamic structures that organize utterances in certain orders to make the conversation meaningful enjoyable and understandable sacks et al although there are a few exceptions such as ing topic segmentation liu et al li et al dialogue acts goo and chen or key point sequence liu et al they either need t c o l c s c v v i x r a conversation topic view stage view james hey i have been thinking about you hannah oh that s nice james what are you up to hannah james i m about to sleep i miss u i was hoping to see you hannah have to get up early for work tomorrow greetings today s plan openings intention james what about tomorrow plan for tomorrow hannah to be honest i have plans for tomorrow evening james oh ok what about sat then hannah yeah sure i am available on sat i ll pick you up at james hannah sounds good see you then plan for saturday discussion pick up time conclusion summary james misses hannah they agree for james to pick hannah up on saturday at table example conversation from samsum gliwa et al with its topic view and stage view extracted by our methods and the human annotated summary extensive expert annotations of discourse and chen liu et al or only code conversations based on their topics liu et al which fails to capture rich conversation structures in dialogues even one single conversation can be viewed from different perspectives resulting in multiple conversational or discourse patterns for instance in table based on what topics were discussed topic view galley et al liu et al li et al it can be segmented into greetings today s plan plan for tomorrow plan for saturday and pick up time from a conversation progression perspective stage view ritter et al paul althoff et al the same dialogue can be categorized into openings intention discussion and conclusion from a coarse perspective global view conversations can be treated as a whole or each utterance can serve as one segment discrete view models that only utilized a xed topic view of the conversation joty et al liu et al may fail to capture its comprehensive and nuanced conversational structures and any amount of information loss introduced by the conversation encoder may lead to larger error cascade in the decoding stage to ll these gaps we propose to combine those multiple diverse views of tions in order to generate more precise summaries to sum up our contributions are we propose to utilize rich conversational structures i e tured views topic view and stage view and the generic views global view and discrete view for abstractive conversation summarization we sign a multi view sequence to sequence model that consists of a conversation encoder to encode ferent views and a multi view decoder with view attention to generate dialogue summaries we perform experiments on a large scale sation summarization dataset samsum gliwa et al and demonstrate the effectiveness of our proposed methods we conduct thorough error analyses and discuss specic challenges that current approaches faced with this task related work document summarization document rization has received extensive research attention especially for abstractive summarization for instance rush et al introduced to use sequence to sequence models for abstractive text summarization see et al proposed a pointer generator network to allow copying words from the source text to handle the oov issue and avoid generating repeated content paulus et al chen and bansal further utilized reinforcement learning to select the correct content needed by summarization large scale pre trained language models liu and lapata raffel et al lewis et al have also been duced to further improve the summarization mance other line of work explored long document summarization by utilizing discourse structures in text cohan et al introducing hierarchical models fabbri et al or modifying tion mechanisms beltagy et al there are also recent studies looking at the faithfulness in figure model architecture different views of conversations are rst extracted automatically and then encoded through the conversation encoder a and combined in the multi view decoder to generate summaries b in the conversation encoder each view consists of blocks is encoded separately and the block s representations si are encoded through lstm to represent the view in the multi view decoder the model decides attention weights over different views and then attend to each token in different views through the multi view attention document summarization cao et al zhu et al in order to enhance the information consistency between summaries and the input including topic segments conversational stages alogue overview and utterances to design a view model for dialogue summarization dialogue summarization when it comes to the summarization of dialogues shang et al proposed a simple multi sentence compression technique to summarize meetings zhao et al zhu et al introduced turn based hierarchical models that encoded each turn of terance rst and then used the aggregated sentation to generate summaries a few studies have also paid attention to utilizing conversational analysis for generating dialogue summaries such as leveraging dialogue acts goo and chen key point sequence liu et al or topics liu et al li et al however they either needed a large amount of human annotation for dialogue acts key points or visual focus goo and chen liu et al li et al or only utilized topical information in conversations li et al liu et al these prior work also largely ignored diverse conversational structures in dialogues for instance reply relations among participants mayeld et al zhu et al dialogue acts ritter et al paul and conversation stages thoff et al models that only utilized a xed topic view of the conversation galley et al joty et al may fail to capture its sive and nuanced conversational structures and any amount of information loss introduced by the versation encoder may lead to larger error cascade in the decoding stage to ll these gaps we pose to leverage diverse conversational structures method conversations can be interpreted from different views and every single view enables the model to focus a specic aspect of the conversation to take advantages of those rich conversation views we design a multi view sequence to sequence model see figure that rstly extracts different views of conversations section and then encodes them to generate summaries section conversation view extraction conversation summarization models may easily stray among all sorts of information across ous speakers and utterances especially when versations become long naturally if informative structures in the form of small blocks can be plicitly extracted from long conversations models may be able to understand them better in a more ganized way thus we rst extract different views of structures from conversations topic view although conversations are often less structured than documents they are mostly organized around topics in a coarse grained ture honneth et al for instance a phone chat could possess a pattern of greetings invitation party details rejection from a ical perspective such explicit view and topic ow could help models interpret conversations more cisely and generate summaries that cover important topics here we combine the classic topic segment stage interpretation top freq words openings intentions discussions conclusions hey hi good yeah going time need like think get want really will know time come tomorrow meet thanks ok see great thank sure table the top frequent words appearing in each stage and the interpretations for different stages versation followed by discussions of the details and nally conclude with certain endings table shows an example of the stage view global view and discrete view in addition to the aforementioned two structured views tions can also be naturally viewed from a relatively coarse perspective i e a global view that nates all utterances into one giant block gliwa et al and a discrete view that separates each utterance into a distinct block liu and chen gliwa et al multi view sequence to sequence model we extend generic sequence to sequence models to encode and combine different conversation views to better utilize semantic information in recent trained models we implement our base encoders and decoders with a transformer based pre trained model bart lewis et al note that our multi view sequence to sequence model is agnostic to bart with which it is initialized xk bk xk i j in a block bk n each token xk conversation encoder given a conversation der a specic view with n blocks ck bk j xk m j is rst encoded through the conversation encoder e e bart encoder as shown in figure into hidden representations m j m j note that we add special tokens xk at the ning of each block and use these tokens tations to describe each block i e sk hk xk hk xk hk to depict different views using hidden vectors we aggregate the information from all blocks in one conversation through lstm layers hochreiter and schmidhuber j sk j n sk figure allowed state transitions for the hmm versation model si are conversation stages oi are tences encoded representations conversation stages evolve in an increasing order from to n algorithm choi that segments sations based on inter sentence similarities with recent advanced sentence representations bert reimers and gurevych to extract the topic view specically each utterance ui in a conversation c um is rst encoded into hidden vectors via sentence bert then the conversation c is divided into blocks ctopic bn through where bi is one block that contains several consecutive ances such as the topic view described in table stage view as a way of doing things with words socially together with other people conversation organizes utterances in certain orders to make it meaningful enjoyable and understandable sacks et al althoff et al for example seling conversations are found to follow a common pattern of introductions problem exploration problem solving wrap up althoff et al such conversation stage view provides high level sketches about the functions or goals of different parts in conversations which could help models focus on the stages with key information we follow althoff et al to extract stages through a hidden markov model hmm we pose a xed ordering on the stages and only allow transitions from the current stage to the next one the observations in the hmm model are the coded representations hi from sentence bert we set the number of hidden stages as similar to the topic view extraction we segment the sations into blocks cstage bn where si is one block that contains several consecutive utterances we interpret the inferred stages itatively and further visualize the top frequent words appearing in each stage in table we found that conversations around daily chats usually start with openings introduce the goals focus of the we use the last hidden state sk current view k denoted as vk n to represent the experiments dataset and baselines multi view decoder different views could vide different types of conversational aspects for models to learn and further determine which set of utterances should deserve more attention in der to generate better dialogue summaries as a result the ability to strategically combine ent views is essential to this end we propose a transformer based multi view decoder to integrate encoded representations from different views and generate summaries as shown in figure the input to the decoder contains previously generated tokens via our multi view decoder d the l th token is predicted via p l c here wp is a parameter to be learned different from generic transformer decoder we introduce a multi view attention layer in each former block multi view attention layer rst cides the importance k of each view vk through uk tanh w vk exp i i exp k where v is a randomly initialized context vector w and b are parameters to avoid the attention weights being too similar to each other as views are actually encoded from a similar context we utilize a sharpening function over k with a temperature i when t the attention t k weights will behave like a one hot vector i t t then the multi head attention is performed over conversation tokens hk i from different views and form ak separately the attended results are further combined based on the view attention weights k and continue forward passing training we minimize the cross entropy loss ing training l log p l c specically we apply the teacher forcing strategy at training time the inputs are previous tokens from the ground truth at test time the inputs are ous tokens predicted by the decoder we evaluate our model on a large scale dialogue summary dataset samsum gliwa et al that has dialogues with human written maries the data statistics are shown in table samsum contains messenger like conversations about daily topics such as chit chats arranging meetings discussing events we compare our multi view sequence to sequence model view bart with several baseline models pointer generator see et al ing gliwa et al we added separators between each utterance discrete view and used it as input for pointer generator model dynamicconv news wu et al we followed gliwa et al to use to initialize token embeddings ford et al we also added news marization corpus cnn dm nallapati et al as extra training data fast abs rl enhanced chen and bansal rst selects salient sentences and then rewrites them abstractively via sentence level policy gradient methods we combined it with the global view gliwa et al bart generic views lewis et al utilized bart a denoising autoencoder for pretraining sequence to sequence models gether with generic views global view and discrete view we used the bart large model with its default settings model we loaded the pre trained bert base nli stsb for sentence bert to get representations for each utterance for extracting the topic view via we set the window size and std coefcient for extracting the stage view we set the number of hidden states in hmm these hyper parameters were set with a grid search the bart tured views stage and topic views used the same set of parameters as bart generic views for com pytorch fairseq details are shown in section a in the appendix com sentence transformers conversations train dev test participants std mean interval mean turns std interval mean reference length std interval table samsum dataset statistics interval denotes the minimum and maximum range model views pointer generator dynamicconv fast abs rl enhanced dynamicconv news bart bart multi view bart discrete global global discrete discrete global stage topic global stage global topic topic stage p r f p r f rouge l p r f table and rouge l scores for different models on the test set results are averaged over three runs meant our methods or utilized views introduced by us multi view bart we experimented with ent view combinations the best generic view global view was combined with two structured views stage and topic view separately the best two structured views are also combined topic stage the settings for bart encoder decoder kept identical as baselines we used a one layer lstm for encoding sections the learning rate for section encoder and multi view attention was set the temperature t was the beam search size during inference for all the models was results figure relations between rouge scores and the number of participants turns in conversations quantitative results we evaluated models with the standard metric rouge score with stemming lin and och and reported and rouge results on the test set for different models were shown in table pared to pointer generator using reinforcement learning to select important sentences rst fast abs rl enhanced slightly increased f scores adding pre trained embeddings or extra documents training data to lightweight convolution models dynamicconv news lead to even ter rouge scores when using pre trained former based model bart with generic views all rouge scores improved signicantly and bart we followed bart and used com pltrdy rouge note that different tools may ate different rouge scores global outperformed bart discrete especially in terms of rouge l f scores segmenting versations into blocks from structured views stage view and topic view further boosted the mance suggesting that our extracted conversation structures help conversational encoders to capture nuanced and informative aspects of dialogs we did not see any performance boost when bining the generic global view with either topic or conversational stage views partially due to that the coarse granularity of global view does not ment structured views well in contrast utilizing both structured views topic view stage view further increased rouge scores consistently cating the effectiveness of synthesizing informative conversation blocks introduced by both views we visualized the attention weight distributions figure human evaluation results the mean score for each model is also shown in the box plot model analysis and discussion the highest human annotation scores signicantly higher via a student t test than either generic crete or global view or structured stage or topic view which further proved the effectiveness of combing different views so far we have achieved a reasonable tion performance to further study why dialog marization is challenging and how future research could advance this direction we take a closer look at this dialogue summarization dataset samsum model generation errors as well as certain lenges that existing approaches are struggling with challenges in dialog summarization we conduct a thorough examination of the lenges in conversation summarization and nized them into categories as below informal language use many conversations especially in online contexts such as ter reddit jackson and moulinier tain typos word abbreviations slang or cons emojis making it hard to be represented and summarized multiple participants as shown in figure conversations with more speakers are harder to be summarized since it may require els to accurately differentiate both language styles and content from different speakers similar to the multiple characters issue in story summarization zhang et al multiple turns similar to long document summarization xiao and carenini conversations with many utterances contain more information to be processed thus harder to be summarized referral and coreference people usually fer to each other mention others names or use coreference in their messages which troduces extra difculty to dialogue rization also a challenge also exists in reading comprehension chen et al and ment summarization falke et al repetition and interruption information is generally scattered through the whole sation and speakers may interrupt each other for the stage view and topic view in our best model see appendix and found contributions of topic views are slightly more prominent compared to stage views this also communicated that the two different structured views can complement each other well though sharing the same dialogue tent note that the gains from multi view bart topic stage are mainly from the precision scores while recall scores are kept comparable gesting that our proposed model produced fewer irrelevant tokens while preserving necessary mation in its generated summary impact of participants and turns we ized the impact of two essential components in conversations the number of participants and turns on rouge scores via our best performing model multi view bart with topic view stage view in figure as the number of pants turns increases rouge scores decrease dicating that the difculty of conversation rization increased with more participants involved in conversations and more utterances qualitative human evaluation we also ducted human annotations to evaluate the extracted dialogue summaries in addition to rouge scores similar to gliwa et al we asked human annotators on amazon mechanical turk to rate each summary randomly sampled summaries in total on the scale of where means that a summary was poor extracted irrelevant formation or did not make sense at all means it was understandable and gave a concise overview of the text and refers to that the summary only tracted only a part of relevant information or made some mistakes the score for each summary was averaged among three different annotators the intra class correlation was indicating erate agreement koo and li as shown in figure consistent with rouge scores in table our multi view model achieved mturk generic informal language multiple participants multiple turns referral coreference repetition interruption negations rhetorical role language change challenge l table the breakdown of challenges in dialogue marization based on our analyses of sampled versations and the rouge scores per challenge errors l other missing information redundancy wrong references incorrect reasoning improper gendered pronouns table the common error types of our model pared to golden reference on sampled tions and the rouge scores per error type error reconrm back channeling or repeat selves a unique discourse challenge for logue summarization we examined summaries generated by our performing model compared to ground truth maries and observed several major error types negations and rhetorical questions as a long standing problem in nlp eld li et al negation related issues are even more frequent in conversations as there are more question answer exchanges between speakers role and language change conversations usually involve more than one speaker and the role of a speaker may shift from a questioner to an answerer requiring the summarization model to dynamically deal with speaker roles and the associated language e rst sonal pronouns we randomly sampled from our test set and classied them using the above lenge taxonomy a conversation might have more than one category labels and if it had none of the aforementioned challenges we labeled it as generic usually the one marked as generic were shorter or had a simple structure table presents the percentage of each type of challenge and per category performances from our best model multi view bart with topic view stage view we observed that referral erence and role language change were the two most frequent challenges that logue summarization task faced as expected generic conversations were relatively easier marize our best model performed relatively worse when it came to repetition interruption multiple turns and referral coreference ing for more intelligent summarization methods to tackle those challenges missing information content mentioned in references is missing in generated summaries redundancy content occurred in generated summaries was not mentioned by references wrong references generated summaries contain information that is not faithful to the original dialogue and associate one s tions locations with a wrong speaker incorrect reasoning generated summaries reasoned relations in dialogues incorrectly thus came to wrong conclusions improper gendered pronouns summaries used improper gendered pronouns e the misuse of gendered pronouns we annotated the same set of randomly pled summaries via the above error type taxonomy a summary might have more than one category labels and we categorized a summary as other if it did not belong to any error types table presents the breakdown of error types and per category rouge scores we found that i missing information was the most quent error type indicating that current tion models struggled with identifying key mation incorrect reasoning had a percentage of with the worst despite of ing a minor type improper gendered pronouns seemed to severely decrease both and the relatively low rouge scores associated with incorrect reasoning and wrong erences urged better summarization models in ing with faithfulness in dialogue summarization full analyzed set of examples are shown in appendix analysis for baselines are displayed in the appendix tract different views in the future we plan to tate some of the data explore supervised tation models li et al and introduce more conversation structures like dialogue acts oya and carenini joty and hoque into tive dialogue summarization acknowledgment we would like to thank the anonymous reviewers for their helpful comments and the members of georgia tech salt group for their feedback we acknowledge the support of nvidia corporation with the donation of gpu used for this research references tim althoff kevin clark and jure leskovec large scale analysis of counseling conversations an application of natural language processing to mental health transactions of the association for computational linguistics iz beltagy matthew e peters and arman cohan longformer the long document transformer ziqiang cao furu wei wenjie li and sujian li faithful to the original fact aware neural tive summarization in aaai danqi chen jason bolton and christopher d the ning a thorough examination of cnn daily mail reading comprehension task in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages berlin germany association for computational linguistics yen chun chen and mohit bansal fast tive summarization with reinforce selected sentence rewriting in proceedings of the annual ing of the association for computational tics volume long papers pages bourne australia association for computational linguistics freddy y y choi advances in domain pendent linear text segmentation in meeting of the north american chapter of the association for computational linguistics arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and zli goharian a discourse aware attention model for abstractive summarization of long in proceedings of the conference of ments the north american chapter of the association for computational linguistics human language nologies volume short papers pages new orleans louisiana association for tional linguistics figure relations between difculties in tions and errors made by our model relation between challenges and errors to gure out relations between challenges and rors made by our models i e how different types of errors correlate with different types of lenges we visualized the co occurrence heat map in figure we found that our model generated good summary for generic simple conversations ii all kinds of challenges had high correlations with or could lead to the missing information ror wrong references were highly associated with referral coreference this was as expected since co references in conversations would rally increase the difculty for models to associate correct speakers with correct actions iv high relations between role language change referral coreference and incorrect reasoning indicated that interactions between multiple participants with frequent co references might easily lead current summarization models to reason incorrectly conclusion in this work we proposed a multi view to sequence model that leveraged multiple sational structures topic view and stage view and generic views global view and discrete view to generate summaries for conversations in order to strategically combine these different views for better summary generations we propose a view sequence to sequence model experiments conducted demonstrated the effectiveness of our proposed models in terms of both quantitative and qualitative evaluations via thorough error ses we concluded a set of challenges that current models struggled with which can further tate future research on conversation summarization due to the lack of annotations we only adopted simple unsupervised segmentation methods to alexander fabbri irene li tianwei she suyi li and dragomir radev multi news a large scale multi document summarization dataset and tive hierarchical model proceedings of the nual meeting of the association for computational linguistics tobias falke christian m meyer and iryna gurevych concept map based multi document rization using concept coreference resolution and global importance optimization in proceedings of the eighth international joint conference on ral language processing volume long papers pages michel galley kathleen r mckeown eric lussier and hongyan jing discourse mentation of multi party conversation in ings of the annual meeting of the association for computational linguistics pages poro japan association for computational tics shen gao xiuying chen zhaochun ren dongyan zhao and rui yan from standard rization to new tasks and beyond summarization with manifold information bogdan gliwa iwona mochol maciej biesek and aleksander wawer samsum corpus a human annotated dialogue dataset for abstractive summarization in proceedings of the workshop on new frontiers in summarization pages hong kong china association for computational linguistics chih wen goo and yun nung chen tive dialogue summarization with sentence gated modeling optimized by dialogue acts ieee spoken language technology workshop slt sepp hochreiter and jurgen schmidhuber neural comput long short term memory axel honneth hans joas al social action and human nature cup archive peter jackson and isabelle moulinier natural language processing for online applications text retrieval extraction and categorization volume john benjamins publishing shaq joty giuseppe carenini gabriel murray and raymond t ng exploiting conversation structure in unsupervised topic segmentation for emails in proceedings of the conference on empirical methods in natural language ing pages cambridge ma association for computational linguistics grant h kester conversation pieces nity and communication in modern art univ of ifornia press terry k koo and mae y li a guideline of selecting and reporting intraclass correlation cients for reliability research journal of tic medicine mike lewis yinhan liu naman goyal jan ghazvininejad abdelrahman mohamed omer levy ves stoyanov and luke zettlemoyer bart denoising sequence to sequence pre training for natural language generation translation and comprehension jing li aixin sun and shaq r joty segbot a generic neural text segmentation model with pointer network in ijcai pages jiwei li xinlei chen eduard hovy and dan jurafsky visualizing and understanding neural models in nlp in proceedings of the conference of the north american chapter of the association for computational linguistics human language nologies pages san diego california sociation for computational linguistics manling li lingyu zhang heng ji and richard j radke keep meeting summaries on topic abstractive multi modal meeting summarization in proceedings of the association for computational linguistics pages florence italy association for tational linguistics the annual meeting of chin yew lin and franz josef och matic evaluation of machine translation quality ing longest common subsequence and skip bigram statistics in proceedings of the annual ing on association for computational linguistics page association for computational tics chunyi liu peng wang jiang xu zang li and jieping ye automatic dialogue summary generation for customer service in proceedings of the acm sigkdd international conference on knowledge discovery data mining page new york ny usa association for computing machinery peter j liu mohammad saleh etienne pot ben goodrich ryan sepassi lukasz kaiser and noam shazeer generating wikipedia by ing long sequences in international conference on learning representations shaq joty and enamul hoque speech act eling of written asynchronous conversations with task specic embeddings and conditional structured in proceedings of the annual models ing of the association for computational linguistics volume long papers pages yang liu and mirella lapata text tion with pretrained encoders proceedings of the conference on empirical methods in ral language processing and the international joint conference on natural language processing emnlp ijcnlp zhengyuan liu and nancy chen reading turn by turn hierarchical attention architecture for ken dialogue comprehension in proceedings of the annual meeting of the association for putational linguistics pages florence italy association for computational linguistics nils reimers and iryna gurevych bert sentence embeddings using siamese networks proceedings of the conference on empirical methods in natural language processing and the international joint conference on ral language processing emnlp ijcnlp zhengyuan liu angela ng sheldon lee ai ti aw and nancy f chen topic aware generator networks for summarizing spoken sations ieee automatic speech recognition and understanding workshop asru elijah mayeld david adamson and carolyn stein rose hierarchical conversation ture prediction in multi party chat in proceedings of the annual meeting of the special interest group on discourse and dialogue pages seoul south korea association for computational linguistics ramesh nallapati bowen zhou cicero dos santos c aglar and bing xiang tive text summarization using sequence to sequence in proceedings of the rnns and beyond signll conference on computational natural guage learning pages berlin germany association for computational linguistics nikola i nikolov michael pfeiffer and richard h r hahnloser data driven summarization of entic articles corr tatsuro oya and giuseppe carenini extractive summarization and dialogue act modeling on email threads an integrated probabilistic approach in proceedings of the annual meeting of the cial interest group on discourse and dialogue dial pages michael j paul mixed membership markov models for unsupervised conversation modeling in proceedings of the joint conference on ical methods in natural language processing and computational natural language learning pages jeju island korea association for tational linguistics romain paulus caiming xiong and richard socher a deep reinforced model for abstractive marization in international conference on ing representations alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskever language models are unsupervised multitask learners openai blog alan ritter colin cherry and bill dolan pervised modeling of twitter conversations in man language technologies the annual ference of the north american chapter of the ation for computational linguistics pages los angeles california association for tional linguistics alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of the tence summarization conference on empirical methods in natural guage processing pages lisbon portugal association for computational linguistics harvey sacks emanuel a schegloff and gail son a simplest systematics for the tion of turn taking for conversation in studies in the organization of conversational interaction pages elsevier abigail see peter j liu and christopher d manning get to the point summarization with generator networks proceedings of the annual meeting of the association for computational guistics volume long papers guokan shang wensi ding zekun zhang toine tixier polykarpos meladianos michalis giannis and jean pierre vised abstractive meeting summarization with sentence compression and budgeted submodular the maximization nual meeting of the association for computational linguistics volume long papers pages melbourne australia association for tational linguistics in proceedings of felix wu angela fan alexei baevski yann dauphin and michael auli pay less attention with in lightweight and dynamic convolutions tional conference on learning representations wen xiao and giuseppe carenini extractive summarization of long documents by combining global and local context in emnlp ijcnlp weiwei zhang jackie chi kit cheung and joel oren generating character descriptions for in proceedings of matic summarization of ction the aaai conference on articial intelligence ume pages colin raffel noam shazeer adam roberts katherine lee sharan narang michael matena yanqi zhou wei li and peter j liu exploring the limits of transfer learning with a unied text to text former zhou zhao haojie pan changjie fan yan liu lin li min yang and deng cai tive meeting summarization via hierarchical tive segmental network learning in the world wide web conference www page new york ny usa association for computing ery chenguang zhu william hinthorn ruochen xu qingkai zeng michael zeng xuedong huang and meng jiang boosting factual correctness of abstractive summarization chenguang zhu ruochen xu michael zeng and dong huang end to end abstractive rization for meetings henghui zhu feng nan zhiguo wang ramesh pati and bing xiang who did they respond to conversation structure modeling using masked hierarchical transformer a model settings we load the pre trained bert base nli stsb for sentence bert to get representations for each utterance when extracting the topic view we set the window size and std coefcient in when extracting the stage view we set the number of hidden states in hmm these parameters were set after a grid search with ing randomly sampled segmented results by human the bart structured views stage and topic views followed the same parameters as bart generic views for multi view bart we selected different views to combine generic view structured view best generic view global view was bined with two structured views stage and topic view structured view structured view best two single views are combined topic stage the settings for bart encoder decoder kept the same as baseline we used a one layer lstm for ing sections the learning rate for section encoder and multi view attention was set the perature t was the beam search size during inference for all the models was experiments were performed on two tesla gb memory b view attention visualization we visualized the attention weights distribution for the stage view and topic view in our best multi view model to explore the importance of stage verses topic in figure we found that the topic views were more prominent than the stage views tent with the performances of bart topic view and bart stage view this indicated that having com sentence transformers figure attention weights distribution for stage view and topic view in the multi view model table a full index list of our samples discourse structures about topics might be more portant while both topic and stage could improve the conversation summarization this also municated that the two different structured views can complement each other well though sharing the same dialogue content we displayed two examples in table with the golden references each single view s ated summaries and the combined views ated summaries the combined view could balance the advantages of each single view and generated more precise summaries and the attention weights the model learned were also consistent with single view s performances c supplementary examples for model analysis and discussion for the analysis in the model analysis and cussion section in our paper we randomly sampled examples from the test set of the samsum reference stage topic james misses hannah they agree for james to pick hannah up on saturday at hannah has to get up early for tomorrow james will pick her up at on saturday james and hannah will see each other on saturday at stage topic attention weight james will pick hannah up on saturday at pm petra is very sleepy at work today andy nds the day boring and ezgi is working petra needs to sleep because she s sleepy ezgi is working nobody is working at the ofce today ezgi is working petra is sleepy and wants to sleep petra is sleepy and needs to sleep ezgi is working at the ofce table some generated summary examples compared to references rouge l is shown after each summary and stage weight topic weight is displayed in the last row errors discrete global stage topic multi view other missing information redundancy wrong references incorrect reasoning improper gendered pronouns table common error types of different models compared to golden reference on sampled conversations dataset which can be downloaded here table provides a full index list of the samples table shows the error analysis for discrete bart global bart stage bart topic and bart multi view models it can be observed that i without any explicit structures view and global view models generated summaries with more redundancies compared to golden ence summaries as models may easily lost focus on massive information once we introduced certain conversation structures such as topic view and stage view models behaved better in terms of redundancy and incorrect reasoning which cated that the structured views could help models to better understand the conversations our view models which combined both stage view and topic view made the least number of errors pared to all single view models suggesting the effectiveness of combining different views for versation summarization org
