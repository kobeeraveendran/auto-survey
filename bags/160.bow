improving abstraction text summarization wojciech krysci nski kth royal institute technology romain paulus salesforce research com caiming xiong salesforce research com richard socher salesforce research com g u l c s c v v x r abstract text summarization aims abstractive shorten long text documents human readable form contains important facts original document level actual abstraction measured novel phrases appear remains low existing source document approaches propose techniques improve level abstraction generated summaries decompose decoder contextual network retrieves relevant parts source document pretrained language model incorporates prior knowledge language generation second propose novelty metric optimized directly policy learning encourage generation novel phrases model achieves results comparable state art models determined rouge scores human evaluations achieving signicantly higher level abstraction measured n gram overlap source document introduction text summarization concerns task pressing long sequence text cise form common approaches summarization extractive dorr et al nallapati et al model extracts salient parts source document tive paulus et al et al model extracts concisely paraphrases important parts document generation focus developing marization model produces increased level abstraction model produces cise summaries copying long sages source document work performed salesforce research high quality summary shorter inal document conveys important extraneous information cally syntactically correct cult gauge correctness summary evaluation metrics summarization models use word overlap ground truth summary form rouge lin scores word overlap metrics capture tive nature high quality human written maries use paraphrases words necessarily appear source document state art abstractive text tion models high word overlap performance tend copy long passages source document directly summary producing summaries tive et al propose general extensions rization models improve level tion summary preserving word lap ground truth summary rst tribution decouples extraction generation responsibilities decoder factoring contextual network language model contextual network sole responsibility extracting compacting source document language model responsible generation concise paraphrases second contribution mixed objective jointly timizes n gram overlap ground truth summary encouraging abstraction combining maximum likelihood tion policy gradient reward policy rouge metric measures word overlap ground truth summary novel abstraction reward encourages generation words source document demonstrate effectiveness tributions encoder decoder summarization article human written summary cnn allay possible concerns boston prosecutors released video friday shooting police ofcer month resulted killing gunman ofcer wounded john moynihan white angelo west gunman shot death ofcers black shooting community leaders predominantly african american neighborhood boston police ofcer john moynihan released hospital video shows man later shot dead police boston opened rst moynihan shot face trafc stop generated summary et al generated summary liu et al boston prosecutors released video friday shooting police ofcer month gunman shot death ofcers black said ofcers forced return placed medically induced coma boston hospital boston prosecutors released video shooting police ofcer month boston marathon bombing video shows west sprang red shot pistol ofcer s face shooting occurred wake summary lm new boston police release video shooting ofcer john moynihan new angelo west prior gun convictions police boston police ofcer john moynihan survived bullet wound medically induced coma boston hospital police ofcer says table summaries generated different models cnn daily mail article highlighted spans indicate phrases tokens copied word word original article model obtains model state art rouge l scores performance comparable state art ods cnn dailymail dataset signicantly outperform previous tive approaches abstraction metrics ble shows comparison summaries ated model previous abstractive els showing copying abstraction model model base model training objective base model follows encoder decoder architecture temporal attention attention proposed paulus et al let e rndemb denote matrix demb sional word embeddings n words source document encoding source ument henc computed bidirectional lstm hochreiter schmidhuber dimension dhid henc bilstm e rndhid decoder uses temporal attention encoded sequence penalizes input tokens previously high attention scores let hdec note decoder state time t temporal t tention context time t ctmp computed t stmp ti qtmp ti tmp ti ctmp t w tmphenc r r ti hdec t qtmp ti qtmp ji tj r n tmp ti henc rdhid ti t set qtmp decoder attends previous states intra attention decoded sequence intra attention context time t cint computed ti t sint ti cint t hdec t w inthdec r sint ti sint tj rdhid hdec decoder generates tokens interpolating selecting words source ment pointer network selecting words xed output vocabulary let zt note ground truth label tth figure network architecture decoder factorized separate contextual language models reference vector composed context vectors ctmp hidden state t textual model hdec fused hidden state language model compute distribution output vocabulary cint t t output word generated selecting output vocabulary opposed source document compute bility decoder generates output vocabulary rt hdec ctmp t zrt bz r cint t t probability selecting word yt xed vocabulary time step t dened likelihood log yt log zt log zt log zt log zt log zt log p zt zt log log zt log log p zt objective function combines maximum likelihood estimation policy learning let m denote length ground truth summary maximum likelihood loss lml computed softmax w genrt bgen lml log m set probability copying word yt source document temporal attention distribution tmp joint probability t generator generating word yt time step t yt policy learning uses rouge l reward function self critical baseline greedy decoding policy rennie et al let ysam denote summary obtained sampling current policy p ygre zgre mary generator choice obtained ily choosing rouge l score summary y model ters policy learning loss r r ysam r ygre lpg e zsam ysam use greedy predictions model according eq baseline variance reduction policy gradient schulman et al lpg r log p zsam t ysam t m nal loss mixture mum likelihood loss policy learning loss weighted hyperparameter l lpg language model fusion decoder essential component base model given source document viously generated summary tokens decoder extracts relevant parts source document pointer network composes paraphrases xed vocabulary ple responsibilities augmenting decoder external language model guage model assumes responsibility generating xed vocabulary allows decoder focus attention extraction position added benet easily ing external knowledge uency domain specic styles pre training language model large scale text corpora architecture language model based merity et al use layer tional lstm weight dropped lstm units let et denote embedding word erated time step t hidden state language model l th layer l t lstmlm hlm hlm l time step t combine hidden state language model lstm layer hlm rt dened eq fashion similar sriram et al let denote element wise plication use gating function output gt lters content language model hidden state ft gt w gt hlm w hlm bfuse hfuse t relu gt replace output distribution language model pgen yt eq pgen yt softmax w genhfuse t abstractive reward order produce abstractive summary model exclusively copy source document particular model needs parse large chunks source document create concise summaries phrases source document encourage behavior pose novelty metric promotes generation novel words dene novel phrase summary source document let ng n denote function computes set unique n grams document xgen generated mary xsrc source document ber words s unnormalized novelty ric n dened fraction unique n grams summary novel n xgen n xgen n ng xsrc xgen prevent model receiving high elty rewards outputting short summaries normalize metric length ratio generated ground truth summaries let xgt denote ground truth summary dene novelty metric rnov xgen n n xgen n incorporate novelty metric reward policy gradient objective eq alongside original rouge l metric encourage model generate summaries overlap human written ground truth summaries incorporate novel words source document r y rourrou ysam novrnov ysam rou nov hyperparameters control weighting reward experiments datasets train model cnn daily mail dataset hermann et al nallapati et al vious works abstractive summarization use anonymized version dataset original article summary texts different formats difcult compare overall rouge scores performance version order compare ous results train evaluate versions dataset anonymized version follow pre processing steps described lapati et al pre processing steps et al text version use named entities source document supervise model use pointer use generator e zt eq training teach model point source document word ground truth summary named entity vocabulary word numerical value source document obtain list named entities hermann et al training details lstms bidirectional encoder dimensional decoder lstm dimensional restrict input vocabulary embedding matrix tokens output decoding layer tokens limit size input articles rst tokens summaries tokens use scheduled sampling bengio et al probability calculating maximum likelihood training loss set n computing novelty reward n nal training loss reinforcement learning set rou nov finally use trigram repetition avoidance heuristic dened paulus et al beam search decoding ensure model output twice trigram given summary ing repetitions novelty baseline create novelty baseline taking outputs base model rl training language model inserting dom words present article summary token probability r baseline intuitively higher centage novel n grams base model puts similar original puts keeping rouge score difference relatively small results language models quantitative analysis dataset version train language model consisting dimensional word bedding layer layer lstm layer having hidden size dimensions input layer output size nal decoding layer shares weights embedding layer inan et al press wolf use dropconnect wan et al hidden hidden connections non monotonically triggered chronous gradient descent optimizer merity et al train language model cnn daily mail ground truth summaries following training validation test splits main experiments obtain validation test perplexity respectively anonymized dataset text dataset language models described section rouge scores novelty scores nal summarization model versions cnn daily mail dataset shown table report l f scores percentage novel grams marked nn n generated summaries n results omitted cases available vious authors include novel n gram scores ground truth summaries parison indicate level abstraction man written summaries model r l ground truth summaries intra attn paulus et al lm anonymized text ground truth summaries pointer gen coverage et al sumgan liu et al rsal pasunuru bansal rl pasunuru bansal lm table comparison rouge novel n gram test results model abstractive summarization models cnn daily mail dataset model outputs signicantly fewer novel n grams human written higher percentage maries novel n grams previous achieves state art tive approaches rouge l performance dataset versions obtains scores close state art results ablation study order evaluate relative impact individual contributions run ablation ies comparing model ablations novelty baseline sults different models validation set anonymized cnn daily mail dataset shown table results base model trained maximum likelihood loss language model coder ml lm higher rouge scores novel unigrams novel bigrams scores base model language model ml ml lm beats novelty baseline metrics training models reinforcement learning rouge reward rouge rouge lm model language model obtains higher scores loses novel unigrams bigrams advantage finally mixed rouge novelty rewards duces higher rouge scores novel unigrams language model indicates combination guage model decoder novelty reward training makes model produce novel unigrams maintaining high rouge scores rouge vs novelty trade order understand correlation rouge novel n gram scores different architectures nd model type gives best trade metrics plot novel unigram scores ve best iterations model type anonymized dataset novel bigram scores separate plot include novelty baseline described tion values r model type indicate pareto tier line plot ben tal illustrating models given type best nation rouge novelty scores plots shown figure plots exist inverse relation rouge novelty scores model types illustrating challenge choosing model performs given nal model lm provides best trade scores compared novel unigrams indicated higher pareto frontier rst plot similarly nal model gives best trade offs scores novel bigrams model lm produces novel model r l ml ml nov baseline r ml lm rouge rouge lm lm table ablation study validation set anonymized cnn daily mail dataset figure rouge novel n grams results anonymized validation set different runs model type lines indicates pareto frontier model type bigrams lower score qualitative evaluation order ensure quality model outputs ask human evaluators rate randomly selected text test summaries giving scores respectively readability relevance given original article include text test outputs et al liu et al comparison uators shown different summaries ing article ing told models generated mean score condence interval model evaluation criterion ported table results model matches relevance score et al liu et al slightly rior terms readability related work text summarization existing summarization approaches usually extractive tive extractive summarization model lects passages input document bines form shorter summary times post processing step ensure nal coherence output neto et al dorr et al filippova altun menares et al nallapati et al extractive models usually robust produce coherent summaries create concise summaries paraphrase source document new phrases abstractive summarization allows model paraphrase source document create cise summaries phrases source document state art abstractive marization models based sequence sequence models attention bahdanau et al extensions model include attention mechanism paulus et al article coverage vector et al prevent repeated phrases output summary different training procedures improve rouge score paulus et al textual model readability relevance pointer gen coverage et al sumgan liu et al lm table mean condence interval human evaluation scores text test outputs individual summaries rated higher score indicating higher quality readability relevance separately entailment pasunuru bansal forcement learning generative ial networks generate natural summaries liu et al datasets train evaluate summarization models gigaword graff cieri duc datasets et al headline generation models rush et al nallapati et al generated summary shorter characters generating longer summaries challenging task pecially abstractive models nallapati et al proposed cnn daily mail dataset hermann et al train models generating longer multi sentence summaries words new york times dataset haus benchmark generation long summaries durrett et al paulus et al training strategies sequential models common approach training models sequence generation maximum likelihood estimation teacher forcing time step model given previous ground truth output dicts current output sequence objective accumulation cross entropy losses time step despite popularity approach quence generation suboptimal exposure bias huszar loss evaluation mismatch wiseman rush goyal et al propose way reduce exposure bias plicitly forcing hidden representations model similar training ence bengio et al wiseman rush propose alternate method poses network test dynamics training reinforcement learning methods sutton barto policy learning sutton et al mitigate mismatch optimization objective evaluation metrics directly optimizing evaluation metrics approach led consistent improvements domains image captioning zhang et al abstractive text summarization paulus et al recent approach training sequential models utilizes generative adversarial networks ing human perceived quality generated puts fedus et al guimaraes et al liu et al models use additional discriminator network distinguishes natural generated output guide tive model outputs akin human written text conclusions introduced new abstractive summarization model uses external language model decoder new reinforcement ing reward encourage summary abstraction periments cnn daily mail dataset model generates summaries abstractive previous approaches maintaining high rouge scores close state art future work closing gap match human levels tion far ahead model terms novel n grams including mechanisms promote paraphrase generation summary generator interesting direction references dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly learning align translate iclr aharon ben tal characterization pareto lexicographic optimal solutions multiple ria decision making theory application pages springer samy bengio oriol vinyals navdeep jaitly noam shazeer scheduled sampling quence prediction recurrent neural networks nips linqing liu yao lu min yang qiang qu jia zhu hongyan li generative adversarial work abstractive text summarization aaai stephen merity nitish shirish keskar richard socher regularizing optimizing lstm guage models iclr carlos colmenares marina litvak amin mantrach fabrizio silvestri heads headline eration sequence prediction abstract feature rich space hlt naacl pages ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments aaai bonnie dorr david zajic richard schwartz hedge trimmer parse trim approach line generation hlt naacl greg durrett taylor berg kirkpatrick dan klein learning based single document tion compression anaphoricity constraints acl william fedus ian j goodfellow andrew m dai maskgan better text generation lling iclr katja filippova yasemin altun ing lack parallel data sentence compression proceedings emnlp pages seer anirudh goyal alex lamb ying zhang saizheng zhang aaron c courville yoshua bengio professor forcing new algorithm ing recurrent networks nips david graff c cieri english gigaword guistic data consortium gabriel lima guimaraes benjamin lengeling pedro luis cunha farias alan aspuru guzik objective reinforced ative adversarial networks organ sequence generation models corr karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend nips sepp hochreiter jurgen schmidhuber long short term memory neural computation ferenc huszar train tive model scheduled sampling likelihood sary corr hakan inan khashayar khosravi richard socher tying word vectors word classiers loss framework language modeling iclr chin yew lin rouge package automatic evaluation summaries proc acl workshop text summarization branches page ramesh nallapati bowen zhou c aglar gulcehre bing xiang al abstractive text rization sequence sequence rnns yond proceedings signll conference putational natural language learning joel larocca neto alex freitas celso aa kaestner automatic text summarization ing machine learning approach brazilian posium articial intelligence pages springer paul hoa dang donna harman duc context inf process manage ramakanth pasunuru mohit bansal reward reinforced summarization saliency entailment corr romain paulus caiming xiong richard socher deep reinforced model abstractive marization iclr press lior wolf output arxiv embedding improve language models preprint steven j rennie etienne marcheret youssef mroueh jarret ross vaibhava goel self critical sequence training image captioning corr alexander m rush sumit chopra jason weston neural attention model abstractive tence summarization proceedings emnlp evan sandhaus new york times annotated corpus linguistic data consortium philadelphia john schulman nicolas heess theophane weber pieter abbeel gradient estimation stochastic computation graphs nips abigail peter j liu christopher d manning point summarization generator networks acl anuroop sriram heewoo jun sanjeev satheesh adam coates cold fusion training models language models corr richard s sutton andrew g barto inforcement learning introduction adaptive computation machine learning mit press richard s sutton david mcallester satinder p singh yishay mansour policy ent methods reinforcement learning tion approximation nips li wan matthew zeiler sixin zhang yann le cun rob fergus regularization neural works dropconnect icml sam wiseman alexander m rush sequence sequence learning beam search timization emnlp li zhang flood sung feng liu tao xiang shaogang gong yongxin yang timothy m hospedales actor critic sequence training image tioning corr
