improving abstraction text summarization wojciech krysci nski kth royal institute technology romain paulus salesforce research com caiming xiong salesforce research com richard socher salesforce research com abstract text summarization aims abstractive shorten long text documents human readable form contains important facts original document level actual abstraction measured novel phrases appear remains low existing source document approaches propose techniques improve level abstraction generated summaries decompose decoder contextual network retrieves relevant parts source document pretrained language model incorporates prior knowledge language generation second propose novelty metric optimized directly policy learning encourage generation novel phrases model achieves results comparable state art models determined rouge scores human evaluations achieving signicantly higher level abstraction measured gram overlap source document introduction text summarization concerns task pressing long sequence text cise form common approaches summarization extractive dorr nallapati model extracts salient parts source document tive paulus model extracts concisely paraphrases important parts document generation focus developing marization model produces increased level abstraction model produces cise summaries copying long sages source document work performed salesforce research high quality summary shorter inal document conveys important extraneous information cally syntactically correct cult gauge correctness summary evaluation metrics summarization models use word overlap ground truth summary form rouge lin scores word overlap metrics capture tive nature high quality human written maries use paraphrases words necessarily appear source document state art abstractive text tion models high word overlap performance tend copy long passages source document directly summary producing summaries tive propose general extensions rization models improve level tion summary preserving word lap ground truth summary rst tribution decouples extraction generation responsibilities decoder factoring contextual network language model contextual network sole responsibility extracting compacting source document language model responsible generation concise paraphrases second contribution mixed objective jointly timizes gram overlap ground truth summary encouraging abstraction combining maximum likelihood tion policy gradient reward policy rouge metric measures word overlap ground truth summary novel abstraction reward encourages generation words source document demonstrate effectiveness tributions encoder decoder summarization article human written summary cnn allay possible concerns boston prosecutors released video friday shooting police ofcer month resulted killing gunman ofcer wounded john moynihan white angelo west gunman shot death ofcers black shooting community leaders predominantly african american neighborhood boston police ofcer john moynihan released hospital video shows man later shot dead police boston opened rst moynihan shot face trafc stop generated summary generated summary liu boston prosecutors released video friday shooting police ofcer month gunman shot death ofcers black said ofcers forced return placed medically induced coma boston hospital boston prosecutors released video shooting police ofcer month boston marathon bombing video shows west sprang red shot pistol ofcer face shooting occurred wake summary new boston police release video shooting ofcer john moynihan new angelo west prior gun convictions police boston police ofcer john moynihan survived bullet wound medically induced coma boston hospital police ofcer says table summaries generated different models cnn daily mail article highlighted spans indicate phrases tokens copied word word original article model obtains model state art rouge scores performance comparable state art ods cnn dailymail dataset signicantly outperform previous tive approaches abstraction metrics ble shows comparison summaries ated model previous abstractive els showing copying abstraction model model base model training objective base model follows encoder decoder architecture temporal attention attention proposed paulus let rndemb denote matrix demb sional word embeddings words source document encoding source ument henc computed bidirectional lstm hochreiter schmidhuber dimension dhid henc bilstm rndhid decoder uses temporal attention encoded sequence penalizes input tokens previously high attention scores let hdec note decoder state time temporal tention context time ctmp computed stmp qtmp tmp ctmp tmphenc hdec qtmp qtmp tmp henc rdhid set qtmp decoder attends previous states intra attention decoded sequence intra attention context time cint computed sint cint hdec inthdec sint sint rdhid hdec decoder generates tokens interpolating selecting words source ment pointer network selecting words xed output vocabulary let note ground truth label tth figure network architecture decoder factorized separate contextual language models reference vector composed context vectors ctmp hidden state textual model hdec fused hidden state language model compute distribution output vocabulary cint output word generated selecting output vocabulary opposed source document compute bility decoder generates output vocabulary hdec ctmp zrt cint probability selecting word xed vocabulary time step dened likelihood log log log log log log log log log log log objective function combines maximum likelihood estimation policy learning let denote length ground truth summary maximum likelihood loss lml computed softmax genrt bgen lml log set probability copying word source document temporal attention distribution tmp joint probability generator generating word time step policy learning uses rouge reward function self critical baseline greedy decoding policy rennie let ysam denote summary obtained sampling current policy ygre zgre mary generator choice obtained ily choosing rouge score summary model ters policy learning loss ysam ygre lpg zsam ysam use greedy predictions model according baseline variance reduction policy gradient schulman lpg log zsam ysam nal loss mixture mum likelihood loss policy learning loss weighted hyperparameter lpg language model fusion decoder essential component base model given source document viously generated summary tokens decoder extracts relevant parts source document pointer network composes paraphrases xed vocabulary ple responsibilities augmenting decoder external language model guage model assumes responsibility generating xed vocabulary allows decoder focus attention extraction position added benet easily ing external knowledge uency domain specic styles pre training language model large scale text corpora architecture language model based merity use layer tional lstm weight dropped lstm units let denote embedding word erated time step hidden state language model layer lstmlm hlm hlm time step combine hidden state language model lstm layer hlm dened fashion similar sriram let denote element wise plication use gating function output lters content language model hidden state hlm hlm bfuse hfuse relu replace output distribution language model pgen pgen softmax genhfuse abstractive reward order produce abstractive summary model exclusively copy source document particular model needs parse large chunks source document create concise summaries phrases source document encourage behavior pose novelty metric promotes generation novel words dene novel phrase summary source document let denote function computes set unique grams document xgen generated mary xsrc source document ber words unnormalized novelty ric dened fraction unique grams summary novel xgen xgen xsrc xgen prevent model receiving high elty rewards outputting short summaries normalize metric length ratio generated ground truth summaries let xgt denote ground truth summary dene novelty metric rnov xgen xgen incorporate novelty metric reward policy gradient objective alongside original rouge metric encourage model generate summaries overlap human written ground truth summaries incorporate novel words source document rourrou ysam novrnov ysam rou nov hyperparameters control weighting reward experiments datasets train model cnn daily mail dataset hermann nallapati vious works abstractive summarization use anonymized version dataset original article summary texts different formats difcult compare overall rouge scores performance version order compare ous results train evaluate versions dataset anonymized version follow pre processing steps described lapati pre processing steps text version use named entities source document supervise model use pointer use generator training teach model point source document word ground truth summary named entity vocabulary word numerical value source document obtain list named entities hermann training details lstms bidirectional encoder dimensional decoder lstm dimensional restrict input vocabulary embedding matrix tokens output decoding layer tokens limit size input articles rst tokens summaries tokens use scheduled sampling bengio probability calculating maximum likelihood training loss set computing novelty reward nal training loss reinforcement learning set rou nov finally use trigram repetition avoidance heuristic dened paulus beam search decoding ensure model output twice trigram given summary ing repetitions novelty baseline create novelty baseline taking outputs base model training language model inserting dom words present article summary token probability baseline intuitively higher centage novel grams base model puts similar original puts keeping rouge score difference relatively small results language models quantitative analysis dataset version train language model consisting dimensional word bedding layer layer lstm layer having hidden size dimensions input layer output size nal decoding layer shares weights embedding layer inan press wolf use dropconnect wan hidden hidden connections non monotonically triggered chronous gradient descent optimizer merity train language model cnn daily mail ground truth summaries following training validation test splits main experiments obtain validation test perplexity respectively anonymized dataset text dataset language models described section rouge scores novelty scores nal summarization model versions cnn daily mail dataset shown table report scores percentage novel grams marked generated summaries results omitted cases available vious authors include novel gram scores ground truth summaries parison indicate level abstraction man written summaries model ground truth summaries intra attn paulus anonymized text ground truth summaries pointer gen coverage sumgan liu rsal pasunuru bansal pasunuru bansal table comparison rouge novel gram test results model abstractive summarization models cnn daily mail dataset model outputs signicantly fewer novel grams human written higher percentage maries novel grams previous achieves state art tive approaches rouge performance dataset versions obtains scores close state art results ablation study order evaluate relative impact individual contributions run ablation ies comparing model ablations novelty baseline sults different models validation set anonymized cnn daily mail dataset shown table results base model trained maximum likelihood loss language model coder higher rouge scores novel unigrams novel bigrams scores base model language model beats novelty baseline metrics training models reinforcement learning rouge reward rouge rouge model language model obtains higher scores loses novel unigrams bigrams advantage finally mixed rouge novelty rewards duces higher rouge scores novel unigrams language model indicates combination guage model decoder novelty reward training makes model produce novel unigrams maintaining high rouge scores rouge novelty trade order understand correlation rouge novel gram scores different architectures model type gives best trade metrics plot novel unigram scores best iterations model type anonymized dataset novel bigram scores separate plot include novelty baseline described tion values model type indicate pareto tier line plot ben tal illustrating models given type best nation rouge novelty scores plots shown figure plots exist inverse relation rouge novelty scores model types illustrating challenge choosing model performs given nal model provides best trade scores compared novel unigrams indicated higher pareto frontier rst plot similarly nal model gives best trade offs scores novel bigrams model produces novel model nov baseline rouge rouge table ablation study validation set anonymized cnn daily mail dataset figure rouge novel grams results anonymized validation set different runs model type lines indicates pareto frontier model type bigrams lower score qualitative evaluation order ensure quality model outputs ask human evaluators rate randomly selected text test summaries giving scores respectively readability relevance given original article include text test outputs liu comparison uators shown different summaries ing article ing told models generated mean score condence interval model evaluation criterion ported table results model matches relevance score liu slightly rior terms readability related work text summarization existing summarization approaches usually extractive tive extractive summarization model lects passages input document bines form shorter summary times post processing step ensure nal coherence output neto dorr filippova altun menares nallapati extractive models usually robust produce coherent summaries create concise summaries paraphrase source document new phrases abstractive summarization allows model paraphrase source document create cise summaries phrases source document state art abstractive marization models based sequence sequence models attention bahdanau extensions model include attention mechanism paulus article coverage vector prevent repeated phrases output summary different training procedures improve rouge score paulus textual model readability relevance pointer gen coverage sumgan liu table mean condence interval human evaluation scores text test outputs individual summaries rated higher score indicating higher quality readability relevance separately entailment pasunuru bansal forcement learning generative ial networks generate natural summaries liu datasets train evaluate summarization models gigaword graff cieri duc datasets headline generation models rush nallapati generated summary shorter characters generating longer summaries challenging task pecially abstractive models nallapati proposed cnn daily mail dataset hermann train models generating longer multi sentence summaries words new york times dataset haus benchmark generation long summaries durrett paulus training strategies sequential models common approach training models sequence generation maximum likelihood estimation teacher forcing time step model given previous ground truth output dicts current output sequence objective accumulation cross entropy losses time step despite popularity approach quence generation suboptimal exposure bias huszar loss evaluation mismatch wiseman rush goyal propose way reduce exposure bias plicitly forcing hidden representations model similar training ence bengio wiseman rush propose alternate method poses network test dynamics training reinforcement learning methods sutton barto policy learning sutton mitigate mismatch optimization objective evaluation metrics directly optimizing evaluation metrics approach led consistent improvements domains image captioning zhang abstractive text summarization paulus recent approach training sequential models utilizes generative adversarial networks ing human perceived quality generated puts fedus guimaraes liu models use additional discriminator network distinguishes natural generated output guide tive model outputs akin human written text conclusions introduced new abstractive summarization model uses external language model decoder new reinforcement ing reward encourage summary abstraction periments cnn daily mail dataset model generates summaries abstractive previous approaches maintaining high rouge scores close state art future work closing gap match human levels tion far ahead model terms novel grams including mechanisms promote paraphrase generation summary generator interesting direction references dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly learning align translate iclr aharon ben tal characterization pareto lexicographic optimal solutions multiple ria decision making theory application pages springer samy bengio oriol vinyals navdeep jaitly noam shazeer scheduled sampling quence prediction recurrent neural networks nips linqing liu yao min yang qiang jia zhu hongyan generative adversarial work abstractive text summarization aaai stephen merity nitish shirish keskar richard socher regularizing optimizing lstm guage models iclr carlos colmenares marina litvak amin mantrach fabrizio silvestri heads headline eration sequence prediction abstract feature rich space hlt naacl pages ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments aaai bonnie dorr david zajic richard schwartz hedge trimmer parse trim approach line generation hlt naacl greg durrett taylor berg kirkpatrick dan klein learning based single document tion compression anaphoricity constraints acl william fedus ian goodfellow andrew dai maskgan better text generation lling iclr katja filippova yasemin altun ing lack parallel data sentence compression proceedings emnlp pages seer anirudh goyal alex lamb ying zhang saizheng zhang aaron courville yoshua bengio professor forcing new algorithm ing recurrent networks nips david graff cieri english gigaword guistic data consortium gabriel lima guimaraes benjamin lengeling pedro luis cunha farias alan aspuru guzik objective reinforced ative adversarial networks organ sequence generation models corr karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend nips sepp hochreiter jurgen schmidhuber long short term memory neural computation ferenc huszar train tive model scheduled sampling likelihood sary corr hakan inan khashayar khosravi richard socher tying word vectors word classiers loss framework language modeling iclr chin yew lin rouge package automatic evaluation summaries proc acl workshop text summarization branches page ramesh nallapati bowen zhou aglar gulcehre bing xiang abstractive text rization sequence sequence rnns yond proceedings signll conference putational natural language learning joel larocca neto alex freitas celso kaestner automatic text summarization ing machine learning approach brazilian posium articial intelligence pages springer paul hoa dang donna harman duc context inf process manage ramakanth pasunuru mohit bansal reward reinforced summarization saliency entailment corr romain paulus caiming xiong richard socher deep reinforced model abstractive marization iclr press lior wolf output arxiv embedding improve language models preprint steven rennie etienne marcheret youssef mroueh jarret ross vaibhava goel self critical sequence training image captioning corr alexander rush sumit chopra jason weston neural attention model abstractive tence summarization proceedings emnlp evan sandhaus new york times annotated corpus linguistic data consortium philadelphia john schulman nicolas heess theophane weber pieter abbeel gradient estimation stochastic computation graphs nips abigail peter liu christopher manning point summarization generator networks acl anuroop sriram heewoo jun sanjeev satheesh adam coates cold fusion training models language models corr richard sutton andrew barto inforcement learning introduction adaptive computation machine learning mit press richard sutton david mcallester satinder singh yishay mansour policy ent methods reinforcement learning tion approximation nips wan matthew zeiler sixin zhang yann cun rob fergus regularization neural works dropconnect icml sam wiseman alexander rush sequence sequence learning beam search timization emnlp zhang flood sung feng liu tao xiang shaogang gong yongxin yang timothy hospedales actor critic sequence training image tioning corr
