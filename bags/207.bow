multimodal abstractive summarization for videos shruti jindrich spandana florian of computer science carnegie mellon university of mathematics and physics charles university amazon ai cmu edu mff cuni cz com cmu edu n u j l c s c v v i x r a abstract in this paper we study abstractive tion for open domain videos unlike the ditional text news summarization the goal is less to compress text information but rather to provide a uent textual summary of mation that has been collected and fused from different source modalities in our case video and audio transcripts or text we show how a multi source sequence to sequence model with hierarchical attention can integrate mation from different modalities into a ent output compare various models trained with different modalities and present pilot periments on the corpus of instructional videos we also propose a new evaluation ric content for abstractive summarization task that measures semantic adequacy rather than uency of the summaries which is ered by metrics like rouge and bleu introduction in recent years with the growing popularity of video sharing platforms there has been a steep rise in the number of user generated instructional videos shared online with the abundance of videos online there has been an increase in demand for efcient ways to search and retrieve relevant videos song et al wang et al otani et al torabi et al many cross modal search applications rely on text associated with the video such as description or title to nd relevant content however often videos do not have text meta data associated with them or the existing ones do not provide clear information of the video tent and fail to capture subtle differences between related videos wang et al we address this by aiming to generate a short text summary of the video that describes the most salient content of the done while sg was at university of edinburgh video our work benets users through better textual information and user experience and video sharing platforms with increased user engagement by retrieving or suggesting relevant videos to users and capturing their attention summarization is a task of producing a shorter version of the content in the document while serving its information and has been studied for both textual documents automatic text tion and visual documents such as images and videos video summarization automatic text marization is a widely studied topic in natural guage processing luhn kupiec et al mani given a text document the task is to generate a textual summary for applications that can assist users to understand large documents most of the work on text summarization has cused on single document summarization for mains such as news rush et al nallapati et al see et al narayan et al and some on multi document summarization stein et al lin and hovy woodsend and lapata cao et al yasunaga et al video summarization is the task of producing a compact version of the video visual summary by encapsulating the most informative parts money and agius lu and grauman gygli et al song et al sah et al multimodal summarization is the combination of textual and visual modalities by summarizing a video document with a text summary that rizes the content of the video multimodal rization is a more recent challenge with no marking datasets yet li et al collected a multimodal corpus of english news videos and articles paired with manually annotated summaries the dataset is small scale and has news articles with audio video and text summaries but there are no human annotated audio transcripts figure dataset example with different modalities cuban breakfast and free cooking video is not mentioned in the transcript and has to be derived from other sources related tasks include image or video captioning and description generation video story generation procedure learning from instructional videos and title generation which focus on events or activities in the video and generating descriptions at ous levels of granularity from single sentence to multiple sentences das et al regneri et al rohrbach et al zeng et al zhou et al zhang et al gella et al a closely related task to ours is video title generation where the task is to describe the most salient event in the video in a compact title that is aimed at capturing users attention zeng et al zhou et al present the youcookii dataset containing instructional videos specically cooking recipes with temporally localized tations for the procedure which could be viewed as a summarization task as well although localized with time alignments between video segments and procedures in this work we study multimodal tion with various methods to summarize the intent of open domain instructional videos stating the clusive and unique features of the video tive of modality we study this task in detail using the new sanabria et al which contains human annotated video summaries for a varied range of topics our models generate ral language descriptions for video content using the transcriptions both user generated and output of automatic speech recognition systems as well as visual features extracted from the video we also introduce a new evaluation metric content that suits this task and present detailed results to understand the task better multimodal abstractive summarization the sanabria et al contains about hours of short instructional videos spanning different domains such as cooking sports indoor outdoor activities music each video is accompanied by a human generated transcript and a to sentence summary is available for ery video written to generate interest in a potential viewer the example in figure shows the transcript scribes instructions in detail while the summary is a high level overview of the entire video ing that the peppers are being cut and that this is a cuban breakfast recipe which is not tioned in the transcript we observe that text and vision modalities both contain complementary formation thereby when fused helps in generating richer and more uent summaries additionally we can also leverage the speech modality by using the output of a speech recognizer as input to a marization model instead of a human annotated transcript the contains videos for training for validation and for testing the average length of transcripts is words and of summaries is words a more general parison of the dataset for summarization as compared with certain common datasets is given in sanabria et al video based summarization we represent videos by features extracted from a pre trained action recognition model a convolutional neural network hara et al today we are going to show you how to make spanish omelet i m going to dice a little bit of peppers here i m not going to use a lot i m going to use very very little a little bit more then this maybe you can use red peppers if you like to get a little bit color in your omelet some people do and some people do t is the way they make there spanish omelets that is what she says i loved it it actually tasted really good you are going to take the onion also and dice it really small you do want big chunks of onion in there cause it is just pops out of the omelet so we are going to dice the up also very very small so we have small pieces of onions and peppers ready to go how to cut peppers to make a spanish omelette get expert tips and advice on making cuban breakfast recipes in this free cooking video summarytranscriptvideo trained to recognize different human actions in the kinetics dataset kay et al these features are dimensional extracted for every non overlapping frames in the video this results in a sequence of feature vectors per video rather than a single global one we use these sequential features in our models described in section dimensional feature vector representing all text a single video speech based summarization we leverage the speech modality by using the outputs from a trained speech recognizer that is trained with other data as inputs to a text summarization model we use the state of the art models for microphone conversational speech recognition pire peddinti et al and eesen miao et al le franc et al the word error rate of these models on the test data is this high error mostly stems from normalization issues in the data for example recognizing and labeling as twenty handling these tively will reduce the word error rates signicantly we accept these as is for this task transfer learning our parallel work sanabria et al demonstrates the use of summarization models trained in this paper for a transfer learning based summarization task on the charades dataset sigurdsson et al that has audio video and text summary caption and question answer pairs modalities similar to the dataset sanabria et al observe that pre training and transfer learning with the dataset led to signicant improvements in unimodal and multimodal tion tasks on the charades dataset summarization models we study various summarization models first we use a recurrent neural network rnn to sequence model sutskever et al consisting of an encoder rnn to encode text or video features with the attention mechanism bahdanau et al and a decoder rnn to generate summaries our second model is a pointer generator pg model vinyals et al glehre et al that has shown strong formance for abstractive summarization nallapati et al see et al as our third model we use hierarchical attention approach of libovick and helcl originally proposed for multimodal machine translation to combine textual and visual video frames resnext features rnn rnn attention attention hier attn w rnn over transcript rnn decoder figure building blocks of the sequence to sequence models gray numbers in brackets indicate which ponents are utilized in which experiments modalities to generate text the model rst putes the context vector independently for each of the input modalities text and video in the next step the context vectors are treated as states of another encoder and a new vector is computed when using a sequence of action features instead of a single averaged vector for a video the rnn layer helps capture context in figure we present the building block of our models evaluation we evaluate the summaries using the standard ric for abstractive summarization rouge l lin and och that measures the longest common sequence between the reference and the generated summary additionally we introduce the content metric that ts the template like structure of the summaries we analyze the most frequently occurring words in the transcription and summary the words in transcript reect the conversational and spontaneous speech while the words in the summaries reect their descriptive nature for amples see table in appendix a content this metric is the score of the content words in the summaries based over a lingual alignment similar to metrics used to ate quality of monolingual alignment sultan et al we use the meteor toolkit banerjee and lavie denkowski and lavie to obtain the alignment then we remove function words and task specic stop words that appear in most of the summaries see appendix a from the ence and the hypothesis the stop words are easy to predict and thus increase the rouge score we treat remaining content words from the reference v i e o r a m e s r e s n e x t e a t u r e s w r n n w o r n n a t t e n t i o n r n n o v e r t r a n s c r i t a t t e n t i o n h i e r a t t n w r n n d e c o e r model no description rouge l content random baseline using language model rule based extractive summary next neighbor summary using extracted sentence from only text only first tokens text only complete transcript text only tokens pg complete transcript text only asr output complete transcript text only action features only video action features rnn video ground truth transcript action with hierarchical attn asr output action with hierarchical attn table rouge l and content for different summarization models random baseline rule based extracted summary nearest neighbor summary different text only pointer generator asr output transcript video only and text and video models model no inf rel coh flu text only video only text and video table human evaluation scores on different sures of informativeness inf relevance rel herence coh fluency flu and the hypothesis as two bags of words and pute the score over the alignment note that the score ignores the uency of output human evaluation in addition to automatic evaluation we perform a human evaluation to derstand the outputs of this task better following the abstractive summarization human annotation work of grusky et al we ask our annotators to label the generated output on a scale of on informativeness relevance coherence and uency we perform this on randomly sampled videos from the test set we evaluate three models two unimodal only video only and one multimodal text and video three workers annotated each video on amazon mechanical turk more details about human evaluation are in the appendix a experiments and results as a baseline we train an rnn language model sutskever et al on all the summaries and randomly sample tokens from it the output tained is uent in english leading to a high rouge score but the content is unrelated which leads to a low content score in table as another baseline we replace the target summary with a rule based extracted summary from the tion itself we used the sentence containing words how to with predicates learn tell show discuss or explain usually the second sentence in the script our nal baseline was a model trained with the summary of the nearest neighbor of each video in the latent dirichlet allocation lda blei et al based topic space as a target this model achieves a similar content score as the based model which shows the similarity of content and further demonstrates the utility of the content score we use the transcript either ground truth script or speech recognition output and the video action features to train various models with ferent combinations of modalities the text only model performs best when using the complete script in the input tokens this is in contrast to prior work with news domain summarization nallapati et al we also observe that pg networks do not perform better than models on this data which could be attributed to the tive nature of our summaries and also the lack of common n gram overlap between input and output which is the important feature of pg networks we also use the automatic transcriptions obtained from a pretrained automatic speech recognizer as input to the summarization model this model achieves difference in content rather than length example presented in table section a shows how the outputs vary conclusions we present several baseline models for ing abstractive text summaries for the open domain videos in data our presented models include a video only summarization model that performs competitively with a text only model in the future we would like to extend this work to generate document multi video summaries and also build end to end models directly from audio in the video instead of text based output from pretrained asr we dene and show the quality of a new metric content for evaluation of the video summaries that are designed as teasers or highlights for ers instead of a condensed version of the input like traditional text summaries acknowledgements this work was mostly conducted at the erick jelinek memorial summer workshop on speech and language hosted and sponsored by johns hopkins university shruti palaskar received funding from facebook and zon grants jindrich libovick received funding from the czech science foundation grant no this work used the extreme science and engineering discovery environment xsede ported by nsf grant and the bridges system supported by nsf award at the pittsburgh supercomputing center references dzmitry bahdanau kyunghyun cho and yoshua neural machine translation by corr bengio jointly learning to align and translate satanjeev banerjee and alon lavie meteor an automatic metric for mt evaluation with improved correlation with human judgments in proceedings of the acl workshop on intrinsic and extrinsic ation measures for machine translation marization pages david m blei andrew y ng and michael i jordan latent dirichlet allocation journal of chine learning research clsp jhu edu figure word distribution in comparison with the man summaries for different unimodal and multimodal models density curves show the length distributions of human annotated and system produced summaries competitive performance with the video only els described below but degrades noticeably than ground truth transcription summarization model this is as expected due to the large margin of asr errors in distant microphone open domain speech recognition we trained two video only models the rst one uses a single mean pooled feature vector tation for the entire video while the second one applies a single layer rnn over the vectors in time note that using only the action features in input reaches almost competitive rouge and content scores compared to the text only model ing the importance of both modalities in this task finally the hierarchical attention model that bines both modalities obtains the highest score in table we report human evaluation scores on our best text only video only and multimodal els in three evaluation measures the multimodal models with the hierarchical attention reach the best scores model hyperparameter settings tion analysis and example outputs for the models described above are available in the appendix in figure we analyze the word distributions of different system generated summaries with the man annotated reference the density curves show that most model outputs are shorter than human notations with the action only model being the shortest as expected interestingly the two different uni modal and multimodal systems with truth text and asr output text features are very similar in length showing that the improvements in rouge l and content scores stem from the of feat truth transcript feat output only ozan caglayan mercedes garca martnez adrien bardet walid aransa fethi bougares and loc rault nmtpy a exible toolkit for advanced neural machine translation systems the prague bulletin of mathematical linguistics chloe hillier will kay joao carreira karen simonyan brian sudheendra zhang narasimhan fabio viola tim green trevor back paul natsev al the kinetics human action video dataset corr ziqiang cao furu wei li dong sujian li and ming zhou ranking with recursive neural works and its application to multi document rization in twenty ninth aaai conference on cial intelligence kyunghyun cho bart van merrienboer caglar cehre dzmitry bahdanau fethi bougares holger schwenk and yoshua bengio learning phrase representations using rnn encoder decoder for statistical machine translation in proceedings of the conference on empirical methods in ral language processing emnlp p das c xu r f doell and j j corso a thousand frames in just a few words lingual scription of videos through latent topics and sparse object stitching in proceedings of ieee conference on computer vision and pattern recognition michael denkowski and alon lavie meteor universal language specic translation evaluation for any target language in proceedings of the ninth workshop on statistical machine translation pages association for computational tics spandana gella mike lewis and marcus rohrbach a dataset for telling the stories of social media videos in proceedings of the conference on empirical methods in natural language processing pages jade goldstein vibhu mittal jaime carbonell and mark kantrowitz multi document in proceedings rization by sentence extraction of the naacl anlp workshop on automatic summarization pages association for putational linguistics max grusky mor naaman and yoav artzi newsroom a dataset of million summaries with diverse extractive strategies corr aglar glehre sungjin ahn ramesh nallapati bowen zhou and yoshua bengio pointing the unknown words in proceedings of the nual meeting of the association for computational linguistics acl volume long papers michael gygli helmut grabner hayko der and luc van gool creating summaries from user videos in european conference on puter vision pages springer kensho hara hirokatsu kataoka and yutaka satoh can spatiotemporal cnns retrace the in proceedings of tory of cnns and imagenet the ieee conference on computer vision and tern recognition cvpr pages diederik p kingma and jimmy ba adam corr a method for stochastic optimization julian kupiec jan pedersen and francine chen a trainable document summarizer in proceedings of the annual international acm sigir ence on research and development in information retrieval pages acm adrien le franc eric riebling julien karadayi w yun camila scaff florian metze and rina cristia the aclew divime an easy in interspeech pages use diarization tool interspeech isca haoran li junnan zhu cong ma jiajun zhang and chengqing zong multi modal tion for asynchronous collection of text image in proceedings of the dio and video ference on empirical methods in natural language processing pages jindrich libovick and jindrich helcl attention strategies for multi source sequence to sequence learning in proceedings of the annual ing of the association for computational linguistics volume short papers pages chin yew lin and eduard hovy from single to multi document summarization in proceedings of annual meeting of the association for tational linguistics pages chin yew lin and franz josef och matic evaluation of machine translation quality ing longest common subsequence and skip bigram statistics in proceedings of the meeting of the association for computational linguistics pages association for computational tics zheng lu and kristen grauman story driven summarization for egocentric video in proceedings of the ieee conference on computer vision and tern recognition pages hans peter luhn the automatic creation of erature abstracts ibm journal of research and velopment inderjeet mani advances in automatic text marization mit press yajie miao mohammad gowayyed and florian metze eesen end to end speech recognition in ing deep rnn models and wfst based decoding automatic speech recognition and understanding asru ieee workshop on pages ieee arthur g money and harry agius video marisation a conceptual framework and survey of the state of the art journal of visual tion and image representation ramesh nallapati bowen zhou cicero dos santos a glar gulehre and bing xiang tive text summarization using sequence to sequence rnns and beyond conll page shashi narayan shay b cohen and mirella lapata ranking sentences for extractive tion with reinforcement learning corr mayu otani yuta nakashima esa rahtu janne heikkil and naokazu yokoya learning joint representations of videos and sentences with web image search in european conference on puter vision pages springer vijayaditya peddinti guoguo chen vimal manohar tom ko daniel povey and sanjeev khudanpur jhu aspire system robust lvcsr with tdnns ivector adaptation and rnn lms in automatic speech recognition and understanding asru ieee workshop on pages ieee michaela regneri marcus rohrbach dominikus zel stefan thater bernt schiele and manfred pinkal grounding action descriptions in videos tacl anna rohrbach marcus rohrbach wei qiu nemarie friedrich manfred pinkal and bernt schiele coherent multi sentence video in scription with variable level of detail tern recognition german conference gcpr pages alexander m rush sumit chopra and jason weston a neural attention model for abstractive in proceedings of the tence summarization conference on empirical methods in natural guage processing pages association for computational linguistics shagan sah sourabh kulhare allison gray hashini venugopalan emily prudhommeaux and raymond ptucha semantic text in applications of computer tion of long videos vision wacv ieee winter conference on pages ieee ramon sanabria ozan caglayan shruti palaskar desmond elliott loc barrault lucia specia and florian metze a large scale dataset for multimodal language understanding in ings of the workshop on visually grounded tion and language vigil nips ramon sanabria shruti palaskar and florian metze cmu sinbad s submission for the avsd challenge in proc dialog system technology challenges workshop at aaai honolulu hawaii usa abigail see peter j liu and christopher d manning get to the point summarization with generator networks in proceedings of the nual meeting of the association for computational linguistics volume long papers pages rico sennrich orhan firat kyunghyun cho dra birch barry haddow julian hitschler marcin junczys dowmunt samuel lubli antonio rio miceli barone jozef mokry and maria nadejde nematus a toolkit for neural machine lation in proceedings of the software tions of the conference of the european ter of the association for computational tics pages association for computational linguistics gunnar a sigurdsson gl varol xiaolong wang ali farhadi ivan laptev and abhinav gupta hollywood in homes crowdsourcing data in european tion for activity understanding ference on computer vision jingkuan song yi yang zi huang heng tao shen and richang hong multiple feature ing for real time large scale near duplicate video in proceedings of the acm retrieval national conference on multimedia pages acm yale song jordi vallmitjana amanda stent and jandro jaimes tvsum summarizing web videos using titles in proceedings of the ieee ference on computer vision and pattern recognition pages md arafat sultan steven bethard and tamara sumner back to basics for monolingual alignment exploiting word similarity and contextual evidence transactions of the association for computational linguistics ilya sutskever james martens and geoffrey e ton generating text with recurrent neural networks in proceedings of the international conference on machine learning pages jmlr org ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neural networks in advances in neural information processing tems pages curran associates inc atousa torabi niket tandon and leonid sigal learning language visual embedding for movie understanding with natural language corr oriol vinyals meire fortunato and navdeep jaitly in advances in neural pointer networks information processing systems pages curran associates inc meng wang richang hong guangda li zheng jun zha shuicheng yan and tat seng chua event driven web video summarization by tag ieee ization and key shot identication tions on multimedia kristian woodsend and mirella lapata multiple aspect summarization using integer linear ming in proceedings of the joint conference on empirical methods in natural language ing and computational natural language learning pages michihiro yasunaga rui zhang kshitijh meelu ayush pareek krishnan srinivasan and dragomir r radev graph based neural multi document summarization in proceedings of the ence on computational natural language learning conll pages kuo hao zeng tseng hung chen juan carlos niebles and min sun generation for user generated videos in european conference on puter vision pages springer jianguo zhang pengcheng zou zhao li yao wan ye liu xiuming pan yu gong and philip s yu product title renement via multi modal arxiv preprint generative adversarial learning luowei zhou chenliang xu and jason j corso towards automatic learning of procedures from web in proceedings of the instructional videos second aaai conference on articial intelligence pages a appendix a experimental setup in all our experiments the text encoder consists of bidirectional layers of the encoder with gated recurrent units gru cho et al and layers of the decoder with conditional gated recurrent units cgru sennrich et al we optimize the models with the adam optimizer kingma and ba with learning rate halved after each epoch when the validation mance does not increase for maximum epochs we restrict the input length to tokens for all experiments except the best text only model in the section experiments and results we use ulary the most frequently occurring words which showed best results in our experiments largely outperforming models using subword based vocabularies we ran all experiments with the nmtpytorch toolkit caglayan et al set words transcript the to and you a it that of is i going we in your this s so on summary in a this to free the video and learn from on with how tips for of expert an table most frequently occurring words in script and summaries a frequent words in transcripts and summaries table shows the frequent words in transcripts input and summaries output the words in transcripts reect conversational and spontaneous speech while words in the summary reect their descriptive nature a output examples from different models table shows example outputs from our different text only and text and video models the text only model produces a uent output which is close to the reference the action features with the rnn model which sees no text in the input produces an in domain y tying and shing abstractive summary that involves more details like ment which is missing from the text based models but is relevant the action features without rnn model belongs to the relevant domain but contains fewer details the nearest neighbor model is lated to knot tying but not related to shing the scores for each of these models reect their respective properties the random baseline output shows the output of sampling from the random guage model based baseline although it is a uent output the content is incorrect observing other outputs of the model we noticed that although dictions were usually uent leading to high scores there is scope to improve them by predicting all details from the ground truth summary like the subtle selling point phrases or by using the visual features in a different adaptation model a attention analysis figure shows an analysis of the attention tributions using the hierarchical attention model in an example video of painting the vertical axis denotes the output summary of the model and the horizontal axis denotes the input time steps from the transcript we observe less attention in the rst no model r l c output reference watch and learn how to tie thread to a hook to help with y tying as explained by out expert in this free how to video on y tying tips and techniques ground truth text action feat text only truth asr output tion feat learn from our expert how to attach thread to y shing for y shing in this free how to video on y tying tips and techniques learn from our expert how to tie a thread for y shing in this free how to video on y tying tips and techniques learn how to tie a y knot for y shing in this free how to video on y tying tips and techniques asr output learn tips and techniques for y shing in this free shing video on techniques for and making y shing nymphs action features rnn learn about the equipment needed for y tying as well as other y shing tips from our expert in this free how to video on y tying tips and techniques action only features learn from our expert how to do a double half hitch knot in this free video clip about how to use y shing next neighbor use a sheep shank knot to shorten a long piece of rope learn how to tie sheep shank knots for shortening rope in this free knot tying video from an eagle scout random baseline learn tips on how to play the bass drum beat variation on the guitar in this free video clip on music theory and guitar lesson table example outputs of ground truth text and video with hierarchical attention text only with truth text only with asr output asr output text andv video with hierarchical attention action features with rnn and action features only models compared with the reference the topic based next neighbor and random baseline arranged in the order of best to worst summary in this table figure visualizing attention over video features part of the video where the speaker is introducing the task and preparing the brush in the middle half the camera focuses on the close up of brush strokes with hand to which the model pays higher attention over consecutive frames towards the end the close up does not contain the hand but only the paper and brush where the model again pays less attention which could be due to unrecognized cutcuttalking and preparing the brushclose up of brushstrokes handblack frames at the end close up of brushstrokes no hand tions in the close up there are black frames in the very end of the video where the model learns not to pay any attention in the middle of the video there are two places with a cut in the video when the camera shifts angle the model has learned to identify these areas and uses it effectively from this particular example we see the model using both modalities very effectively in this task of the summarization of open domain videos a human evaluation details to understand the outputs generated for this task better we ask workers on amazon mechanical turk to compare outputs of unimodal and modal models with the ground truth summary and assign a score between lowest and highest for four metrics informativeness relevance herence and uency of generated summary the annotators were shown the ground truth summary and a candidate summary without knowledge of the type of modality used to generate it each ample was annotated by three workers annotation was restricted to english speaking countries annotators participated in this task
