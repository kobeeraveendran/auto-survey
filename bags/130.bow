query focused abstractive summarization incorporating query relevance multi document coverage summary length constraints models tal baumel dept computer science ben gurion university beer sheva israel bgu matan eyal dept computer science ben gurion university beer sheva israel bgu michael elhadad dept computer science ben gurion university beer sheva israel bgu abstract query focused summarization qfs addressed extractive ods methods produce text suffers low coherence vestigate abstractive methods applied qfs overcome tions recent developments neural attention based sequence sequence models led state art results task stractive generic single document tion models trained end end method large amounts training data address aspects stractive summarization applicable qfs training data porate query relevance pre trained stractive model existing abstractive models trained single document ting design iterated method embed abstractive models multi document requirement qfs abstractive els adapt trained generate text specic length words aim generating output different size words design way adapt target size generated summaries given size ratio compare method relevance sensitive attention qfs tractive baselines ways combine abstractive models duc qfs datasets demonstrate solid improvements rouge performance introduction query focused summarization qfs task rst introduced duc dang task provides set queries paired vant document collections collection ing topic expected output short mary answering query according data documents current state art methods task daume iii fisher roark feigenblat extractive produced summary set sentences tracted document set extractive methods tend produce ent summaries manually crafted ones examples weaknesses extractive methods include unresolved anaphora unreadable sentence ordering lack cohesiveness text problem extractive methods lack ability extract salient information long sentence including salient tion included sentence system committed sentence sentence extracted documented tractive algorithms haghighi vanderwende nallapati tend prefer longer sentences reasons weaknesses extractive summarization methods hard quantify illustrate high probability achieving incoherent text applying tractive methods qfs assume tence understood text starts connective phrase identify matching closed set tives breaks reference chain sentences non proper denite noun phrase noun refers noun phrase preceding sentence identied reference chains ing core nlp lee percent sentences duc passed tions mentioned lower sentences duc understood context data risks producing low ence text great incentive test abstractive summarization methods task qfs work aim adapting abstractive single document summarization methods handle qfs task tecture attention mechanism adopted abstractive approaches tematically explore stage query evance benecial qfs abstractive process experiment method build summary iterative process extraction abstraction pairs batches vant content multiple documents abstracted sequence coherent segments text compare system extractive methods combinations trained abstractive model relevance ing multiple document input evaluate proposed model called relevance sensitive stractive qfs rsa qfs traditional duc datasets experiments demonstrate tial abstractive qfs models solid rouge gains baselines previous work extractive methods current state art methods task qfs duc dataset categorized unsupervised methods small scale supervised methods unsupervised methods search set tences optimizes gain function cross entropy summarizer ces feigenblat optimizes relevance length straint method achieves current state art rouge scores duc datasets small scale supervised methods use small datasets usually previous duc datasets learn representation dataset resentation optimize gain function recent ample approach docrebuild trains neural network set sentences minimize original document reconstruction error method uses duc learn word representations obtains results slightly lower ces extractive methods suffer herence problems mentioned sequence sequence models abstractive summarization figure comparison output unmodied model model rsa qfs qfs data sample unmodied summary lacks coherence relevant input query rst obstacles face ing data available training end end qfs way similar recently single document generic abstractive tion existing abstractive models dle multiple documents input clude explicit query relevance criterion computation salient summary content isting abstractive models trained duce short summaries regardless tion density input document reasons direct application existing state art abstractive model qfs data sample produces inappropriate output fig hypothesize existing trained tive model encapsulates reusable linguistic edge leverage qfs task investigate ways augment pre trained single document abstractive model explicit modeling query relevance ability handle multiple input documents adjust length produced summary accordingly validate hypothesis work sequence sequence abstractive methods generation emerged practical tools point successful attempts abstractive rization task generic single document summarization rush nallapati paulus based sequence sequence proach attention mechanism bahdanau models include following ponents encoder neural network transforms list words list dense vector representations dense representations aim capture word context encoders commonly implemented word embedding layer followed recurrent neural network rnn long short term memory lstm component hochreiter schmidhuber gated recurrent units gru chung decoder neural network generates word summary conditioned sentation prex generated text dense context vector representing input quence decoder commonly implemented rnn fully connected layer mension output matching size cabulary softmax layer turns vector distribution vocabulary attention mechanism neural network mines importance encoded word decoding step maps variable length list encoded words representations size context representation attention anism commonly implemented multiple levels fully connected layers calculate unnormalized attention weight word input softmax layer normalize weights training models abstractive gle document summarization sible availability large scale datasets gigaword graff cieri new york times dataset sandhaus cnn daily news hermann contain pairs source text short text examples example cnn daily mail corpus automatically curated matching articles summary created site editor dataset cludes documents cnn uments daily mail average size abstract corpus words size input documents words contrast average abstract length duc qfs dataset words large scale dataset currently able qfs task duc settings hypothesize models trained datasets capture linguistic capability combine small windows coherent sentences concise phrases accordingly objective adapt pre trained generic abstractive tion architecture complex task qfs recent work abstractive qfs summarization nema attempt solve issue missing training data introducing new dataset abstractive qfs based debatepedia dataset introduced different duc qfs datasets summaries sented debate key points single short sentence average words summary words duc data input texts short snippets text average words duc reach words distinct size differences duc dia datasets compare methods rectly work focus adapting cic architecture pointer generator erage mechanism network qfs task model achieves state art rouge lin readability scores single document generic abstractive marization task pointer generator coverage mechanism network includes nicant modications pointer network erage mechanisms adheres general encoder decoder attention architecture present proposed modication context generic architecture dling relevance orthogonal processing rare words switch generator erage mechanism ability avoid redundancy experiments network query relevance adopt approach qfs formulated baumel qfs task split stages relevance model determines tent passages source documents relevant input query generic methods incorporating relevance attention models discussed lack large scale dataset similar qfs task presented duc prevents attempting end end solution learns generate relevant mary documents query order overcome obstacle split problem tasks relevance model abstractive model takes relevance count relevance introduced isting attention model different ways filter input include sentences high relevance score pass ltered model generation time test method baseline inject relevance score pre trained model given document query calculate relevance sentence query pre processing step use relevance additional input network relevance model predicts relevance sentence given query project relevance score tences words sentence obtain word level relevance score decoding step abstractive model multiply unnormalized attention score word calculated model pre computed relevance score illustrated fig unmodied model adapted unnormalized tention word step calculated wsst battn battn trained ters encoder output word decoder state step attention scores later normalized softmax function model multiply word relevance score normalization reli reli relevance score combines sentence relevance lexical vance predicted relevance ranking model words sentence given relevance score discuss figure stage query focused summarization scheme rization method applied combine relevant passages coherent summary relevance model identies redundant ordered passages information retrieval methods summarization model selects salient tent removes redundancy organizes target summary schematic approach illustrated figure method achieves good rouge results simple extractive tion methods sum haghighi derwende relevance model high quality accordingly order adapt abstractive ods qfs rst baseline consider consists ltering input documents according vance pass ltered relevant passages abstractive model hypothesize approach adapt abstractive ods input generated tering process different type documents abstractive model trained structured coherent ument abstractive models rely critically sequential structure input decision generation time method aims preserving document structure infusing relevance abstractive model decoding paper consider simple relevance models attempt optimize compare relevance measures based unigram overlap query sentences encodings cosine distance tween query sentences upper bound impact good relevance model consider oracle relevance model compare sentences gold maries word count cosine measure focus assess mechanism pose order combine relevance abstractive capabilities capable producing uent evant summaries given good relevance model document setdocument setdocument setdocument setqueryretrieverelevant passagesgenericsummarizationsummary figure illustration rsa qfs architecture relv ector vector length ith element relevance score ith input word relv ector calculated advance input range relevance models imented relevance scores brated model scheme adapted model able irrelevant sentences generation time beneting context information encoding time contrast ing baseline encoder fed relevance sentences hypothesize proposed scheme encoder produce better representation input documents filtered baseline regime trained important note train model original parameters baseline encoder decoder attention model unchanged calibrating relevance score unlike normalization methods softmax function sensitive scale values scale input lower variance softmax output similarly low figure variance softmax output low single word receives normalized attention model unable focus single word attention models use softmax normalize tention weights important scale multiplying relevance scores calibrated attention scores address issue multiplied cosine similarity scores order increase scale applying softmax figure demonstration scale sensitivity softmax function gures illustrate softmax operation samples uniform tion left sampled range right normalization scale modication nicant impact reported rouge mance adapting abstractive models multi document summarization long output summarization datasets mail cnn include single document input short summary words duc quires words need adapt trained abstractive model handle document scenario produce longer output possible solution use extractive summarization method generate input apply abstractive method method handle multiple documents input suffers problems crease recall unlikely abstractive method introduce relevant information cluded input suffer stractive model bias short output directly encourage abstractive model ate longer text cover content instead use following simple eager gorithm produce summaries multiple uments control length output rst sort input documents overall idf cosine similarity query iteratively marize documents till budget words achieved avoid redundancy lter generated sentences generated summary half words cluded current summary algorithm ignores document structure topic progression uses simplistic model redundancy leave future work parison baseline algorithm algorithm iterative version documents sort output summary new summary document documents summary rsa word sentence summary summary sent budget return output summary end summary sentence output sentence end end end phisticated models content redundancy course experiments goals experiments compare rsa qfs baseline input uments ltered according relevance test method incorporate relevance attention mechanism single ment input produces readable relevant output measure impact quality ent relevance models output rsa qfs single document input evaluate iterative version rsa qfs state art extractive qfs method ces evaluation tested scenarios qfs track data duc datasets dang hoa pared rsa qfs debatepedia dataset spite differences sizes discussed use rouge metrics performance isons evaluate separately incorporation relevance model pre trained abstractive model single document ablation study test iterative algorithm handle tiple input documents second round iments rst round experiments compare abstractive baselines longest input document qfs topic set perimented relevant document obtained lower rouge performance comparisons use rouge metrics rouge measures recall longest common substrings ing rsa qfs extractive method use rouge usually reported extractive method performance rouge values obtained document ablation study expected lower competitive qfs results main reasons use reference duc ence summaries modications erence summaries created manually cover topic set contrast ablation study read single document best mary generate lack coverage pointer generator abstractive model trained generate words summary duc datasets summaries contain words reported rouge performance indicates trends detect generated short summaries manage capture relevance abstractive baselines compare rsa qfs following lines blackbox run document pointer generator abstractive model modications weak baseline indicates method improve qfs performance abstractive method completely ignores query filtered ltered half document tences selecting ones highest evance score maintained original ing sentences ltered document input pointer generator stractive model relevance score count shared words betwen query sentence list relevance models tested model relevance vided best results ltered baseline relevance sensitive attention rsa qfs method main contribution work tested method following vance score functions word count query given sentence simple count word overlap rsa tfidf generated idf sentation entire document set topic aggregated sentence scores cosine similarity query sentence idf vectors rsa tfidf rsa use model mikolov pre trained google news dataset relevance measured sine similarity summed representation vector word query tence words appear pre trained model vocabulary ignored results given table expected blackbox method ignores query pletely performs poorly surprisingly observe filtered model lter input document according word count relevance model apply abstractive model behave better box unmodied model contrast rsa qfs improve signicantly improvements nicant filtered pipeline processing exactly input material filtered method indicates way porate relevance attention mechanism effective directly adjusting input representation word count relevance model achieves highest rouge scores compared relevance models datasets performs ltered baseline large based method close tently condence interval word count method speculate fact cabulary words ignored fact duc queries tend verbose need expansion explain fact improve word count model based method performed poorly presume fact rouge eliminate stop words frequent words evaluation extractive baselines experiments compare rsa qfs method extended iterative gorithm consume multiple documents query exact duc conditions duce summaries comparable existing extractive methods compare ces current state art extractive algorithm qfs results table compare relevance models default settings pointer generator evaluation qfs word count model identied best performing ablation study oracle model oracle model pute relevance input sentence ing reference models instead query gives theoretical upper bound potential benet sophisticated retrieval ranking methods observe rsa qfs competitive state art extractive methods forms metric oracle baseline shows sophisticated relevance method potential improve performance nicant way current tive model evaluation debatepedia dataset debatepedia qfs dataset nema evaluate method lstm based diversity attention trained end end debatepedia dataset model compare rouge recall results provided original paper result table suggest method outperforms model trained actual dataset noted model yielded summaries times longer required achieved low rouge cision compare precision score provided original research comparison indicates datasets directly comparable completely ferent domain abstractive capability lated model provides readable realistic summaries analysis output abstractiveness order test model truly abstractive instead simply copying relevant fragments batim input documents counted sentences summary generated model word count similarity tion substrings original text found average tences copied original document average word edit distance generated sentence similar tence edits tested duc marized iterative rsa word count method observed generated sentences single document blackbox filtered rsa word count rsa tfidf rsa table incorporating relevance single longest document input multi document ces iterative rsa word count iterative rsa oracle table iterative rsa qfs extractive methods recall rouge debatepedia rsa word count table results debatepedia qfs dataset signicantly different source sentences introduce new content words generated words present source ments measures indicate good level abstractiveness remains challenge measure abstractiveness interpretable quantitative manner cursory reading erated summaries feels literal assessed readability reading maries generated best performing methods rsa word count example seen fig oracle based iterative method found summaries produced gle document variant maintained readability unmodied model notice erage mechanism affected cation sentences repeated summaries model produced compared original abstractive model iterative version suffer repeated sentences dismissed algorithm suffer lack coherence sentences indicating better discourse model required simple eager iterative model improved ence requires better evaluation metrics rouge metrics produced summaries methods code required produce available talbaumel rsasummarization conclusion work present rsa qfs novel method incorporating relevance neural models attention mechanism stractive summarization qfs task additional training rsa qfs signicantly improves rouge scores qfs task compared unmodied models steps ltered qfs scheme preserving ability output summary method relevance score functions compared method state art tive methods showed produces competitive rouge scores qfs task simple relevance models simple iterative model account multiple input documents ideal oracle relevance model method achieves high rouge results pared extractive methods study frames future work document abstractive summarization need design quantitative measures abstractiveness formulation involved ing summary given input documents summary coherence overcome known itations rouge evaluation applied non extractive methods vance models remain key aspect tion gap oracle practical vance models indicates potential improvement models heeyoung lee angel chang yves peirsman nathanael chambers mihai surdeanu dan jurafsky deterministic coreference tion based entity centric precision ranked rules computational linguistics chin yew lin rouge package matic evaluation summaries text tion branches proceedings shop barcelona spain volume shulei zhi hong deng yunlun yang unsupervised multi document summarization framework based neural document model coling pages tomas mikolov ilya sutskever kai chen greg rado jeff dean distributed tions words phrases advances neural information processing ity systems pages ramesh nallapati feifei zhai bowen zhou summarunner recurrent neural network based quence model extractive summarization ments ramesh nallapati bowen zhou caglar gulcehre bing xiang abstractive text rization sequence sequence rnns yond arxiv preprint preksha nema mitesh khapra anirban laha balaraman ravindran diversity driven tion model query based abstractive tion arxiv preprint romain paulus caiming xiong richard socher deep reinforced model abstractive summarization corr org alexander rush sumit chopra jason neural attention model arxiv preprint ston stractive sentence summarization evan sandhaus new york times annotated corpus linguistic data consortium philadelphia abigail peter liu christopher manning point summarization generator networks corr org references dzmitry bahdanau kyunghyun cho yoshua neural machine translation corr org bengio jointly learning align translate tal baumel raphael cohen michael elhadad topic concentration query focused marization datasets aaai pages junyoung chung caglar gulcehre kyunghyun cho yoshua bengio empirical evaluation gated recurrent neural networks sequence ing arxiv preprint hoa trang dang overview duc ceedings document understanding conference volume pages hal daume iii bayesian query focused rization corr org guy feigenblat haggai roitman odellia boni david konopnicki unsupervised query focused multi document summarization proceedings ing cross entropy method international acm sigir ence research development tion retrieval acm new york usa sigir pages seeger fisher brian roark query focused summarization supervised sentence ranking proceedings skewed word distributions document understanding conference new york usa david graff cieri english gigaword pus linguistic data consortium aria haghighi lucy vanderwende ing content models multi document proceedings human language tion nologies annual conference north american chapter association tational linguistics association computational linguistics pages karl moritz hermann tomas kocisky edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend advances ral information processing systems nips org hoa overview duc document understanding conference sepp hochreiter jurgen schmidhuber neural computation long short term memory
