windowing models abstractive summarization long texts leon sch florian nico goran data web science group university mannheim inovex gmbh leon com uni mannheim abstract neural summarization models suffer xed size input limitation text length passes model maximal number tokens document content bly summary relevant gets truncated dently summarizing windows maximal size disallows information tween windows leads incoherent maries propose windowing models neural abstractive summarization ily long texts extend sequence sequence model augmented pointer erator network allowing encoder slide different windows input ment sharing decoder retaining state different input windows explore windowing variants static dowing precomputes number tokens decoder generate window based training statistics namic windowing decoder learns emit token signals encoder shift window empirical results render els effective intended use case marizing long texts relevant content bound document beginning background motivation extractive summarization selects copies relevant source phrases sentences summary abstractive summarization aims capture source meaning generate summaries necessarily containing portions source texts nenkova mckeown holding promise producing summaries like human created ones state art ral models nallapati paulus tan makino extend standard sequence sequence architecture ing recurrent rnn bahdanau transformer based vaswani coder decoder components extend standard model generator network net providing model extractive capabilities allowing choose generating token copying source text tokens tan propose archical model introduces additional based attention mechanism serves model interactions encoded sentence tations paulus incorporate reward expectation based reinforcement learning mixed training objective steer model predicting globally meaningful sequences respect long document summarization likyilmaz distribute encoding task multiple collaborating encoder agents cohan propose hierarchical encoder captures document discourse structure attentive discourse aware decoder erates summary requires predened discourse structure designed specic texts scientic publications despite multiple encoders operating different document segments models limit maximal ument length inference work address prominent limitation neural models summarize texts longer maximal input length set model training inference documents longer tokens truncated renders potentially summary relevant truncated content inaccessible model propose novel models based windowing source text sequentially shift encoder attention ent windows source text decoder shared windows preserving semantic formation previous window decoding investigate windowing strategies static windowing model swm precomputes state input windows sharing coder input windows allows semantic information adjacent windows holds promise retaining summary ence decoding step attend window representations decoder hidden state attention query obtain conditioned window encoding coding step jhj attention jtw weight computed softmax normalized value dot product encoded decoder state decoder puts embedding feed forward projection concatenation attended input sentation hidden state parameters output probability distribution training vocabulary simply computed applying softmax function vector dot product values computed pretrained word embeddings augment base model pointer generator network net allowing decoder choose step generating token training vocabulary copying token source document generation probability based context vector decoder hidden state decoder input pgen bptr rdemb bptr rameters output probability word extended vocabulary union source text words interpolates generation copying distributions pgen species net augmented model operates window tokens need specify transition window source text static windowing model static windowing model precomputes number tokens decoder needs generate input window let equally sized source windows determined use following function termine importance weight window figure high level illustration windowing model long document summarization based training corpus statistics ber tokens decoder generate source window dynamic windowing model dwm rst heuristically based mantic similarity source text summary sentences inject special window shift tokens training reference summaries let decoder learn emit window shift tokens generation signaling window shift erating special token conceptually allows dwm model summarize arbitrarily long texts inference evaluation wikihow pus koupaee wang long texts distribution summary relevant tent renders windowing models effective windowing models figure contains high level depiction windowing model start based model recurrent components bahdanau maps sequence xtx output sequence yty bidirectional lstm lstm encoder produces contextualized representations input token decoder state initialized concatenation end states encoder lstms apply attention mechanism similar luong instead learning local attention span source text position limit model xed size input training attend window tokens sequentially slide window long text way decoder learns model transitions content windows allowing summarize arbitrarily long documents inference window size stride step divide source text tokens overlapping dows use decoder retaining experimented transformer vaswani encoder decoder obtained weaker performance pad shorter tokens rameters dening shape summary bution windows unnormalized weights converted probabilities softmax function compute expected summary length given document based document length training corpus statistics let set documents set respective reference summaries training corpus compute expected summary length new document length covers training documents length covers reference summaries number tokens decoder generate window simply product normalized weight dynamic windowing model swm relies document summary lengths training corpus number summary tokens decoded window content dynamic windowing model dwm aims exible allowing decoder dynamically signal special token ration current window shift decoder needs learn emit window shift token want end end trainable model need inject window shift tokens reference summaries training corpus achieve heuristically computing semantic similarity scores source text sentences reference summary sentences simplicity obtain sentence embedding sum respective word embeddings compute cosine similarity tween sentence embeddings reference summary sentence tify similar source document sentence determine respective window way example early dows receive larger weights later windows acknowledge rudimentary method computing semantic similarity sentences intend experiment advanced sentence embedding els accurate sentence similarity measures kusner conneau devlin niak inter alia subsequent work sentence map reference summary sentence source window order windows assigned summary sentences necessarily sequential reference summary sentences model lows sequential window shifts rst window order sequential replacing breaking windows accumulated maximums inject window shift tokens summary sentences different assigned source windows window assignment inject rst second mary sentence fourth sentence inference input window shifted decoder outputs token evaluation data evaluate windowing models benchmark datasets cnn dailymail news corpus created nallapati question answering dataset hermann wikihow corpus koupaee wang news place relevant formation beginning called lead body principle standard models truncate long documents likely perform cnn dailymail evaluation wikihow dataset construction bias summary relevant information evenly tributed texts experimental setup use negative log likelihood objective optimize models maximizing rouge performance opment sets use batch level beam search decoder beam size unlike standard beam search decrease end summary token eos predicted longer incomplete partial hypotheses completed beams prevail terms length normalized log probability set hidden state sizes encoder lstms decoder lstm employ adam timizer kingma word representations use pretrained dim fasttext embeddings frequent window cases map sentence containing window com model stan swm dwm table results cnn dailymail test set maries tokens stan trained size input tokens swm dwm trained tokens windows tokens stride model stan dwm stan swm dwm table results wikihow dataset swm baselines compare different variants swm dwm standard net model stan xed size input commonly employed baseline simply copies rst document sentences summary results discussion table contains sults cnn dailymail dataset ingly simple baseline outperforms stan static dynamic windowing models cnn dailymail ments summary relevant content found beginning document ability process windows benet swm dwm setting virtually summary relevant content later windows table display results dataset bound ate windowing models distribution summary relevant content source documents wikihow dataset windowing models swm dwm generally edge standard net model stan xed size input stan matches windows size dowing models larger input size stan performs comparably dwm window size notably dwm advantage able process longer overall lowering stan comparing swm dwm windows figure summary wikipedia page lionel messi tokens produced dwm trained cnn dailymail tokens colors spond different source text windows decoder attended generation size windowing models clearly prevail renders windowing models approriate solution summarization documents following properties hold document length massively surpasses maximal number tokens feed xed input size model summary relevant information present document beginning swm outperform dwm tice swm summarize arbitrarily long texts inference despite transitioning content windows swm adapts summary lengths seen training corpus generates eos token early inference long texts contrast learning emit window transitions dynamic windowing model truly generate summaries arbitrarily long texts inference time regardless observed lengths training document respective reference summaries figure depicts summary long document tokens produced dws model trained order magnitude shorter documents tokens conclusion neural summarization models length source texts training based age source document length training set forcing documents longer threshold truncated inference work proposed windowing summarization models allow process arbitrarily long documents inference taking account source text models effective summarizing long texts evenly distributed summary relevant content length constraint neural text proceedings annual meeting tion association computational linguistics pages ramesh nallapati bing xiang bowen zhou sequence sequence rnns text summarization proceedings iclr workshop track ani nenkova kathleen mckeown tomatic summarization foundations trends information retrieval romain paulus caiming xiong richard socher deep reinforced model abstractive marization proceedings iclr abigail peter liu christopher manning point summarization generator networks proceedings nual meeting association computational linguistics volume long papers pages jiwei tan xiaojun wan jianguo xiao abstractive document summarization proceedings based attentional neural model annual meeting association computational linguistics volume long papers pages association computational linguistics ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez lukasz kaiser illia polosukhin attention need proceedings neurips yongjian weijia jia tianyi liu wenmian yang improving abstractive document marization salient information modeling proceedings annual meeting ciation computational linguistics pages vitalii zhelezniak aleksandar savkov april shen francesco moramarco jack flann nils merla settle average max fuzzy sets max pooled word vectors proceedings iclr references dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly proceedings learning align translate iclr asli celikyilmaz antoine bosselut xiaodong yejin choi deep communicating agents proceedings abstractive summarization conference north american chapter association computational linguistics man language technologies volume long pers pages arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang nazli goharian discourse aware attention model abstractive summarization long documents proceedings conference north american chapter association tional linguistics human language technologies volume short papers pages alexis conneau douwe kiela holger schwenk loc barrault antoine bordes supervised learning universal sentence representations natural language inference data proceedings conference empirical methods ral language processing pages jacob devlin ming wei chang kenton lee kristina toutanova bert pre training deep bidirectional transformers language standing proceedings conference north american chapter association computational linguistics human language nologies volume long short papers pages karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read comprehend advances neural information processing systems pages diederik kingma jimmy adam method stochastic optimization iclr mahnaz koupaee william yang wang ihow large scale text summarization dataset corr matt kusner sun nicholas kolkin kilian weinberger word embeddings ument distances international conference chine learning pages minh thang luong hieu pham christopher manning effective approaches based neural machine translation proceedings emnlp pages takuya makino tomoya iwakura hiroya takamura manabu okumura global optimization
