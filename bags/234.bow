topic aware pointer generator networks summarizing spoken conversations zhengyuan liu angela ng sheldon lee ai ti aw nancy f institute infocomm research singapore changi general hospital singapore t c o l c s c v v x r abstract lack publicly available resources conversation summarization received far attention text marization purpose conversations exchange information interlocutors key tion certain topic scattered spanned multiple utterances turns different speakers phenomenon pronounced spoken tions speech characteristics backchanneling false starts interrupt topical ow topic diffusion intra utterance topic drift common human human conversations linguistic characteristics dialogue topics sentence level tive summarization approaches spoken documents suited summarizing conversations pointer generator works effectively demonstrated strength integrating extractive abstractive capabilities neural ing text summarization best knowledge date adopted summarizing conversations work propose topic aware architecture ploit inherent hierarchical structure conversations adapt pointer generator model approach nicantly outperforms competitive baselines achieves efcient learning outcomes attains robust mance index terms dialogue summarization neural works attention mechanism conversation technology introduction automatic summarization condenses lengthy materials shorter versions focuses essential tion overall meaning summaries enable users browse digest information effectively research supported funding digital health tute infocomm research science engineering research council project work conducted ing resources human language technology unit thank r e banchs l f dharo p krishnaswamy h lim f suhaimi s ramasamy w l chow ng h c oh s c tong changi general hospital insightful discussions efciently especially useful digital era formation overload summarization attracted research attention years humans generate summaries usually rst comprehend entire content extract concatenate keywords salient tences obtain succinct readable results distill rephrase important information gives rise main paradigms automatic summarization extractive abstractive recently sophisticated neural architectures representation learning linguistic elements large scale available corpora driven approaches progress paradigms work text summarization focuses documents news academic articles contrary scarcity publicly available corpora ground truth summaries speech summarization task human human spoken conversations received far attention high industry demand potential applications different domains different passages human human spoken versations dynamic ow information exchange informal verbose repetitive sprinkled false starts backchanneling reconrmations tions speaker interruptions key information certain topic sub sentence utterance level scattered spanned multiple utterances turns different speakers leading lower information density diffuse topic coverage utterance topic drifts spoken characteristics pose technical challenges sentence level extractive approaches inevitably include unnecessary spans words generated summaries pointer generator networks neural sequence sequence design produce summaries word level extraction abstractive generation propose exploit advantages tackle tioned challenges conversations structured passages inherently nized dialogue topics coarse grained structure topic segmentation shown useful dialogue based information retrieval prior ses investigations inspire augment pointer generator networks topic level attention mechanism gantly attend underlying interrupted topical ow human human conversations dialogue setting work based real world scenario nurses discuss symptom information post discharge patients follow status conversation summarization task targeted matically generating notes describe symptoms patients experiencing proposed topic aware generator framework exploits inherent hierarchical ture dialogues able address technical lenges spoken dialogue summarization empirical results outperform competitive baseline architectures signicantly achieving efcient robust learning outcomes related work speech text summarization traditional approaches studied extractive methods utilizing rule based statistical graph based algorithms ous linguistic features like lexical similarity semantic structure discourse relation recently end end neural approaches widely adopted text marization capability exibility scalability neural extractive models adopt sentence labeling ing strategies semantic vector representations sequential context modeling abstractive tasks sequence sequence models use neural decoder ate informative readable summaries word word methods proposed improvement attention mechanism helps decoder concentrate priate parts source content pointer mechanism effective handling vocabulary words coverage mechanism reduce generative repetitions extractive abstractive models proposed obtain better results text summarization focuses passages news articles academic publications speech summarization investigated monologues broadcasts lectures multi party dialogues meetings recently neural modeling approaches adopted goo et al sequence sequence model write headlines meetings liu et al hierarchical extractive model summarize logue spoken documents paper propose aware pointer generator architecture hierarchical context modeling generate summary notes spoken conversations conversation corpus setup sampled dialogues training set distinct dialogues validation set according section training validation sets constructed simulated data test set derived turn dialogues took place nurses patients healthcare setting topic segmentation section ground truth summary construction section ducted subsets nurse patient dialogue data corpus inspired pilot set conversations took place clinical setting nurses inquire symptoms patients linguistic structures tic syntactic discourse pragmatic levels cally abstracted conversations construct plates automatically simulating multi turn dialogues informal spontaneous styles spoken interactions interlocutor interruption backchanneling hesitation false starts repetition topic drift preserved ure example team linguistically trained personnel rened stantiated corrected simulated dialogues enriching verbal expressions considering paraphrasing ent regional english speaking styles american british asian word usage sentence patterns validating logical correctness considering dialogues natural disobeying common sense verifying clinical content consulting certied nurses sations cover topics symptoms e headache cough topic segmentation dialogue analysis change topic corresponds change cognitive attention acknowledged acted speakers usually related content themes work specify dialogue topics according symptoms nurse patient conversations figure shows example different topic segments spans different utterances speakers note types spoken characteristics e false start break topical congruence rule based lexical algorithm detect boundaries dialogue topics labels respectively added topic segment position indices segment labels section human verication conducted ensure quality control ground truth summaries goal conversation summarization task obtain concise description characterizing different attributes specied symptom particular clinical scenario summary notes preferred represented tured format facilitate indexing searching retrieving extracting variety downstream workow applications e decision support medical triage paraphrases particular symptom represented entity e shortness breath breathlessness resented symptom breathlessness attentive architecture attentive model similar adds attention layer vanilla network lter unnecessary contextual information decoding step sequence encoding given document input hot word representation embedding layer converts vector representations look operation embedding matrix obtaining vector sequence v vn vi rd d embedding dimension bi directional long short term memory bi lstm layer encode vi forward backward temporal hi respectively concatenate pendencies hidden representation hn d hidden dimension size hi hi hi hi hi hi sequence decoding attention decoder layer unidirectional lstm generating words step step decoding step t receives word embedding previous token calculates decoder state st attention scoring conducted concatenation st battn wattn battn trainable parameters concatenation operation attention scores viewed importance input content guiding decoder concentrate appropriate positions context erating word attention scores produce weighted pooling encoded hidden states xed size representation read source step context vector hc t hc t concatenated decoder state st fed dense layers produce distribution vocabulary pt vocab w st w w vocab probability distribution words vocabulary generate decoded tokens trainable parameters pt pointer generator networks pointer generator networks variant ture adding pointer network aside ing words xed vocabulary model able copy words pointing source content bypassing vocabulary issues pointer generator model quence encoding representation h attention distribution context vector hc t calculated section scribe pointer generator model achieves word level extraction abstractive generation fig dialogue example multi turn conversation nurse patient symptom discussion colored spans ances topic spoken characteristics preserved represented bold font generated ground truth mary colored spans indicate corresponding topics given dialogue example summary format shown figure symptom listed separately corresponding attributes frequency symptom severity symptom symptom mentioned conversations mentioned included summary signal symptom e cough discussion nurse patient summary tom represented cough recorded key information dialogue e headache night bit human verication conducted ensure quality control approach sequence sequence model encoder ceives token sequence content xn length n decoder outputs token sequence mary y ym length m decoder ates words step step previously generated words encoded provide contextual information time step task learn function parameter set maximizes probability generate readable ful output text section describe baselines attentive model pointer generator network demonstrate integrate topic level attention neural mechanisms fig architecture proposed topic aware pointer generator network pointer mechanism directly copy words source content attentive distribution equation garded copy probability input sequence ically token position max probability tracted output pointer generator switching decoding step switching probability determine generate token xed vocabulary copy source pgen step t calculated context vector ht c decoder input decoder state st gen pt c st bptr wptr bptr trainable parameters moid function pt gen soft switch choose copying word input sequence tion distribution generating word lary distribution pt vocab equation sample extend vocabulary unique words input content obtain following probability distribution extended vocabulary tseg tseg score topic level attention obtain topic aware context representation delineate integrate topic aware attention baseline models figure topic level states obtain representations hseg topic segments collecting hidden states h tion topic level segment position indices tseg tseg k topic segment number dialogue content output bi directional lstm collect states forward backward directions concatenate initial decode state baseline models den state sequence encoding representation initial state fed decoder denote pooling topic level states hseg topic aware contextual representation decoding time step t calculate topic level attention use aware context vectors guide ne grained level prediction topic level attention scores culated dense layers softmax normalization aseg t seg e bseg output pt gen pt pt gen wi w multiply attention score topic level states obtain topic aware context vector contrary vanilla model restricted xed vocabulary ability copy generate words primary advantages pointer generator sign improvement vanilla age mechanism avoids repetitions decoding details found h t hsegaseg t pointing distribution equation ulary distribution equation inuenced aware contextual representation nal output produced equation topic aware attentive architecture exploit topical structure dialogue modeling erarchical architecture attention layer introduced h t st battn pt vocab w st t h model attn attn ta pg net pg net ta proposed precision recall precision recall rouge l precision recall table evaluation results baselines proposed model experiments training setup experiments conducted nurse patient versation corpus described section implemented attentive attn pointer generator work pg net baselines topic aware attentive attn model control proposed aware pointer generator model pg cross entropy measure loss diction ground truth time step t negative log likelihood target word yt dened losst overall loss sum time steps teacher forcing strategy applied training input previous word ground truth test time input previous word predicted decoder setup encoder decoder sion word embeddings hidden states set adopted pre trained word embedding glove vocabulary words segment labels initiated dom vectors embedding weight sharing strategy applied sharing embedding matrix wemb coder decoder sharing signicantly reduced eter size boosted performance reusing semantic syntactic information embedding space learning rate xed batch size set adopted gradient clipping maximum gradient norm adam algorithm stochastic optimization vocabulary size k limited source contents tokens decoding length tokens adopted early stop egy validation training epoch testing set beam search size empirical evaluations evaluation effectiveness summarization performance measured rouge l scores shown table baselines pointer generator model tains higher performance attentive model fig evaluation results learning efciency itative analysis section shows certain tion generated tokens directly copied source content demonstrating effectiveness pointer mechanism task topic aware attentive modeling baseline models obtain signicant ment proposed pg achieves best mance gains prominent precision scores table indicating proposed approach generates fewer unnecessary tokens preserving key information generated summary evaluation ii learning efciency evaluate learning efciency models recorded loss values training shown figure rst batches iterations loss generator decreases faster attentive adding topic level attention attentive pointer generator improved posed pg performs best having demonstrated strength pointer generator works attentive model focus mer following experiments evaluation iii performance model robustness spoken conversations verbose low information density scattered topics central main dialogue theme especially speakers chit chat distracted task oriented discussions evaluate ios adopted model independent addsent randomly extracted sentences squad inserted fig switching pointer generator tokens higher switching probability darker color generated vocabulary visualizing topic level attention scores darker shade higher scores model pg net pg net ta rouge l table scores lengthy sample evaluation eted values denote absolute decrease model performance section topically coherent segments average length augmented test set increased shown table topic level attention helps generator model robust lengthy samples evaluation iv low resource training limited training data major pain point dialogue based tasks time consuming intensive collect annotate natural dialogues scale expect model perform better low resource scenarios advantage inherently hierarchical dialogue structure induction bias ducted experiments range training sizes shown figure proposed pg lead steeper learning curves outperforms baseline visualization analysis section probe deeper proposed neural architecture examine innerworkings pointer generator switches topic level tion interacts decoded sequence fig evaluation results low resource training entities punctuation tokens generated cabulary lexicon generation behavior resonates rationale constructing ground truth summary section enabling model normalize symptom entities handle vocabulary words symptom tributes topic level attention scoring proposed framework conducts topic aware contextual modeling illustrate topic level attention scores aseg equation shown figure summary decoding process dialogue topics step model concentrates topic segment observe smooth topic transition attention layer aligns topical ow dialogue content topic level modeling help improve marization performance ltering nonessential details word level modeling layers conclusion pointer generator switching illustrate probability pointer generator ing pgen equation indicates probability words generated vocabulary posed model summarizes dialogue summary example produced dialogue topics shown figure attribute information symptom segment tags directly copied source content symptom work automatically summarized spoken dialogues nurse patient conversations presented tive efcient neural architecture integrates topic level attention mechanism pointer generator networks utilizing hierarchical structure dialogues demonstrated proposed model signicantly outperforms competitive baselines obtains efcient learning outcomes robust lengthy dialogue samples performs limited training data references hongyan jing kathleen r mckeown position human written summary sentences ceedings annual international acm sigir conference research development tion retrieval new york ny usa sigir pp acm ramesh nallapati feifei zhai bowen zhou marunner recurrent neural network based sequence model extractive summarization documents thirty aaai conference articial intelligence ramesh nallapati bowen zhou cicero dos santos c aglar gulcehre bing xiang abstractive text summarization sequence sequence rnns proceedings signll ference computational natural language learning berlin germany aug pp association computational linguistics t liu s liu b chen hierarchical neural summarization framework spoken documents icassp ieee international conference acoustics speech signal processing icassp pp karl moritz hermann tomas kocisky edward stette lasse espeholt kay mustafa suleyman phil blunsom teaching machines read hend advances neural information processing systems pp abigail peter j liu christopher d manning point summarization pointer generator networks proceedings annual meeting association computational linguistics ume long papers vancouver canada july pp association computational tics ilya sutskever oriol vinyals quoc v le quence sequence learning neural networks advances neural information processing systems pp boufaden guy lapalme yoshua bengio topic segmentation rst stage dialog based mation extraction natural language processing pacic rim symposium citeseer kai hong ani nenkova improving estimation word importance news multi document rization proceedings conference european chapter association computational linguistics pp gunes erkan dragomir r radev lexrank based lexical centrality salience text tion j artif int res vol pp dec horacio saggion thierry poibeau automatic text summarization past present future source multilingual information extraction rization pp springer noemie elhadad m y kan judith l klavans kathleen r mckeown customization unied framework summarizing medical literature cial intelligence medicine vol pp t hirao m nishino y yoshida j suzuki n yasuda m nagata summarizing document trimming discourse tree ieee acm transactions audio speech language processing vol pp nov justin jian zhang ho yin chan pascale fung proving lecture speech summarization rhetorical ieee workshop automatic information speech recognition understanding asru ieee pp chih wen goo yun nung chen abstractive logue summarization sentence gated modeling timized dialogue acts ieee spoken guage technology workshop slt ieee pp harvey sacks emanuel schegloff gail jefferson simplest systematics organization turn ing conversation studies organization conversational interaction pp elsevier chris kedzie kathleen mckeown hal daume iii content selection deep learning models rization proceedings conference pirical methods natural language processing sels belgium oct pp ation computational linguistics shashi narayan shay b cohen mirella lapata ranking sentences extractive summarization proceedings reinforcement learning conference north american chapter sociation computational linguistics human guage technologies volume long papers new leans louisiana june pp tion computational linguistics jeffrey pennington richard socher christopher manning glove global vectors word tion proceedings conference cal methods natural language processing emnlp doha qatar oct pp association computational linguistics sepp hochreiter jurgen schmidhuber long term memory neural computation vol pp sumit chopra michael auli alexander m rush abstractive sentence summarization attentive current neural networks proceedings conference north american chapter sociation computational linguistics human guage technologies san diego california june pp association computational linguistics romain paulus caiming xiong richard socher deep reinforced model abstractive tion proceedings international ence learning representations wan ting hsu chieh kai lin ming ying lee kerui min jing tang min sun unied model tractive abstractive summarization tency loss proceedings annual meeting association computational linguistics ume long papers melbourne australia july pp association computational tics sebastian gehrmann yuntian deng alexander rush abstractive summarization ceedings conference empirical methods natural language processing brussels belgium oct pp association putational linguistics alexander m rush sumit chopra jason weston neural attention model abstractive sentence proceedings conference marization empirical methods natural language processing lisbon portugal sept pp association computational linguistics sameer maskey julia hirschberg comparing ical acoustic prosodic structural discourse features speech summarization ninth european ence speech communication technology shasha xie yang liu improving supervised ing meeting summarization sampling gression computer speech language vol pp zhengyuan liu hazel lim nur farah ain suhaimi shao chuen tong sharon ong angela ng sheldon lee michael r macdonald savitha ramasamy tra krishnaswamy wai leng chow nancy f chen fast prototyping dialogue comprehension system nurse patient conversations symptom monitoring proceedings conference north ican chapter association computational guistics human language technologies minneapolis june pp association computational linguistics pararth shah dilek hakkani tur bing liu gokhan tur bootstrapping neural conversational agent dialogue self play crowdsourcing line proceedings ment learning ence north american chapter association computational linguistics human language nologies volume industry papers new orleans louisiana june pp association putational linguistics mike schuster kuldip k paliwal bidirectional current neural networks ieee transactions signal processing vol pp thang luong hieu pham christopher d manning effective approaches attention based neural machine proceedings conference translation empirical methods natural language processing lisbon portugal sept pp tion computational linguistics oriol vinyals meire fortunato navdeep jaitly advances neural pointer networks tion processing systems c cortes n d lawrence d d lee m sugiyama r garnett eds pp curran associates inc diederik p kingma jimmy ba adam method stochastic optimization proceedings international conference learning representations chin yew lin rouge package automatic uation summaries text summarization branches barcelona spain july pp tion computational linguistics robin jia percy liang adversarial examples evaluating reading comprehension systems ceedings conference empirical methods natural language processing copenhagen mark sept pp association putational linguistics
