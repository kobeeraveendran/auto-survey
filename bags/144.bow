r a l c s c v v i x r a data driven summarization of scientic articles nikola nikolov michael pfeiffer richard hahnloser institute of neuroinformatics university of zurich and eth zurich switzerland niniko pfeiffer abstract data driven approaches to sequence to sequence modelling have been successfully applied to short text summarization of news articles such models are typically trained on input summary pairs consisting of only a single or a few sentences partially due to limited ability of multi sentence training data here we propose to use scientic articles as a new milestone for text summarization large scale training data come almost for free with two types of high quality summaries at different levels the title and the abstract we generate two novel multi sentence summarization datasets from scientic articles and test the suitability of a wide range of existing extractive and abstractive neural network based summarization approaches our analysis demonstrates that scientic papers are suitable for data driven text summarization our results could serve as valuable benchmarks for scaling sequence to sequence models to very long sequences introduction the goal of automatic text summarization is to produce a shorter informative version of an input text while tractive summarization only consists of selecting important sentences from the input abstractive summarization erates content without explicitly re using whole sentences nenkova al text summarization is an area with much promise in today s age of information overow in the domain of scientic literature the rate of publications grows exponentially hunter and cohen which calls for efcient automatic summarization tools recent state of the art summarization methods learn to summarize in a data driven way relying on large tions of input summary training examples the majority of previous work focused on short summarization of news articles such as to generate a title rush al lapati et al one major challenge is to scale these methods to process long input output sequence pairs rently availability of large scale high quality training data is scarce in this paper we explore the suitability of scientic nal articles as a new benchmark for data driven text marization the typical well structured format of scientic papers makes them an interesting challenge and provides plenty of freely available training data because every ticle comes with a summary in the form of its abstract and in even more compressed form its title we make a st step towards summarization of whole scientic cles by composing two novel large datasets for scientic summarization title abstract pairs title gen composed of million papers in the biomedical domain and body pairs abstract gen composed of the second dataset is particularly challenging because it is tended for summarizing the full body of the paper in terms of the abstract the lengths of input output sequences are substantially longer than what has been considered so far in previous research see table we evaluate a range of existing state of the art approaches on these datasets extractive approaches based on word beddings as well as word subword and character level datasets are available at including ninikolov data driven summarization versions with and without preprocessing encoder decoder models that use recurrent as well as volutional modules we perform a quantitative and tive analysis of the models outputs background extractive summarization given an input document consisting of ts sentences sss sts the goal of extractive summarization is to select the k most salient sentences as the output mary extractive summarization typically involves a tence representation module e that represents each put sentence si in a common space as ri as a vector of real numbers as well as a ranking module score that weights the salience wi of each sentence a typical approach to unsupervised extractive summarization is to implement wi as the similarity between ri and a document representation or a document centroid rd radev et al alternatively one can pute wi as the sentence centrality which is an based measure of sentence importance erkan and radev in this work we propose two simple unsupervised lines for extractive summarization both of which rely on word embeddings mikolov et al the rst emb represents each sentence in the input document as the weighted sum of its constituent word embeddings similar to rossiello al ri ti xsi xsi where is the embedding of word is an tional weighting function that weighs the importance of a word and ti is a normalization factor as a weighing function we use the term frequency inverse ument frequency tf idf score similar to brokos et al each sentence embedding ri can then be ranked by computing its cosine similarity ri to a document centroid rd computed similarly as ri the summary sists of the top k sentences with embeddings most similar to the document embedding the second baseline rwmd rank ranks the salience of a sentence in terms of its similarity to all the other sentences in the document all similarities are stored in an sentence similarity matrix w we use the relaxed word mover s distance rwmd to compute this matrix ner et al wij sj sj si sj min xsi where si and sj are two sentences and is the euclidean distance between the embeddings of words and in the sentences to rank the sentences we apply the graph based method from the lexrank system erkan and radev lexrank represents the input as a highly connected graph in which vertices represent sentences and edges between sentences are assigned weights equal to their similarity from w the centrality of a sentence is then computed using the pagerank algorithm page et al abstractive summarization given an input sequence of tx words xxx coming from a xed length input vocabulary vx of size kx the goal of abstractive summarization is to produce a densed sequence of ty summary words yyy yty from a summarization vocabulary vy of size ky where tx ty abstractive summarization is a structured diction problem that can be solved by learning a tic mapping for the summary yyy given the input sequence xxx dietterich et al xxx ty i the encoder decoder architecture is a recently proposed general framework for structured prediction cho et al in which the distribution arg maxyyy is learned using two neural networks an encoder network e which produces intermediate representations of the put and a decoder language modelling network d which generates the target summary the decoder is conditioned on a context vector c which is recomputed from the coded representation at each decoding step the decoder was rst implemented using recurrent neural works rnns sutskever et al cho et al that process the input sequentially recent studies have shown that convolutional neural networks cnns lecun et al can outperform rnns in sequence transduction tasks kalchbrenner al gehring et al unlike rnns cnns can be efciently implemented on parallel gpu hardware this advantage is particularly important when working with very long input and output sequences such as whole paragraphs or documents cnns create archical representations over the input in which lower ers operate on nearby elements and higher layers implement increasing levels of abstraction in this work we investigate the performance of three isting systems that operate on different levels of sequence lstm is a recurrent long short granularity the rst term memory lstm encoder decoder model sutskever et al with an attention mechanism bahdanau et al that operates on the word level processing the put sequentially the second system fconv is a tional encoder decoder model from gehring et al fconv works on the subword level and segments words into smaller units using the byte pair encoding scheme using subword units improves the generation quality when ing with rare or unknown words sennrich et al the third system is a character level encoder decoder model from lee et al that models xxx and yyy as dividual characters with no explicit segmentation between tokens rst builds representations of groups of acters in the input using a series of convolutional layers it then applies a recurrent encoder decoder similar to the lstm system scientic articles previous research on summarization of scientic articles has focused almost exclusively on extractive methods nenkova al in lloret et al the authors develop an unsupervised system for abstract generation of biomedical papers that rst selects relevant content from the body following which it performs an abstractive formation fusion step more recently kim et al consider the problem of supervised generation of level summaries for each paragraph of the introduction of a paper they construct a training dataset of computer ence papers from arxiv selecting the most informative tence as the summary of each paragraph using the jaccard similarity thus their target summary is fully contained in the input in collins et al they develop a pervised extractive summarization framework which they apply to a dataset of computer science papers to the best of our knowledge our work is the rst on abstractive title generation of scientic articles and is the rst to sider supervised generation of the absctract directly from the full body of the paper the datasets we utilize here are also substantially larger than in previous work on scientic summarization scientic articles are potentially more challenging to marize than news articles because of their compact plicit discourse style biber and gray while the events described by news headlines frequently recur in lated articles a scientic title focuses on the unique bution that sets a paper apart from previous research teufel and moens furthermore while the rst two tences of a news article are typically sufciently tive to generate its headline nallapati al teufel and moens the rst sentences of the abstract or troduction of a paper typically contain background mation on the research topic constructing a good scientic title thus requires understanding and integrating concepts from multiple sentences of the abstract datasets to investigate the performance of encoder decoder neural networks as generative models of scientic text we structed two novel datasets for scientic summarization table statistics mean and standard deviation of the two scientic summarization datasets ken sentence counts are computed with nltk title gen and abstract gen title gen token count sentence count sent token count overlap repeat size tr val test abstract title abstract gen token count sentence count sent token count overlap repeat size tr val test body abstract for title gen we used whereas for gen we used the pubmed open access medline contains scientic metadata in xml format of lion papers in the biomedical domain whereas the pubmed open access subset contains metadata and full text of million papers we processed the xml to pair the abstract of a per to its title title gen dataset or the full body gen skipping any gures tables or section headings in the body we then apply several preprocessing steps from the moses statistical machine translation including tokenization and conversion to lowercase any urls were removed all numbers replaced with and any pairs with abstract lengths not in the range of tokens title lengths not within tokens and body lengths not within tokens were excluded the overlap y is the fraction of unique output summary tokens y that overlap with an input token excluding punctuation and stop words as can be seen in table the overlaps are large in our datasets indicating frequent reuse of words the repeat is the average overlap of each sentence si in a text with the remainder of the text where si denotes the complement of sentence repeat measures the redundancy of content within a text a high value indicates frequent repetition of content whereas in abstracts there are only moderate els of repetition in the bodies the repetition rates are much higher possibly because concepts and ideas are reiterated in multiple sections of the paper i si evaluation set up we evaluated the performance of several state of the art proaches on our scientic summarization datasets the tractive systems we consider are lead lexrank tdf emb and rwmd rank the lead baseline returns the rst sentence of the abstract for title gen or the rst sentences of the body for abstract gen lexrank erkan and radev is a graph based centrality approach frequently used as a baseline in the literature emb tdf uses sentence to select the most salient sentences from the input openftlist parser use use the best performing model from chiu et al which is trained on pubmed and medline while rwmd rank uses the relaxed word mover s distance as described in section oracle estimates an upper bound for the extractive summarization task by nding the most similar sentence in the input document for each tence in the original summary we use the relaxed word mover s distance to compute the output of the oracle the abstractive systems we consider are lstm fconv and described in section for lstm we set the put output vocabularies to use two lstm layers of hidden units each and word embedding of dimension we found no improvement from additionally ing the size of this model for and fconv we use the default hyper parameters that come with the public mentations provided by the authors of the systems the title gen lstm and fconv were trained for and epochs respectively until convergence we were unable to train lstm and on abstract gen cause of the very high memory and time requirements ciated with the recurrent layers in these models we found fconv to be much more efcient to train and we succeeded in training a default model for epochs for title gen we used beam search with beam size while for abstract gen we found a beam size of to perform better quantitative evaluation in tables and we evaluate our approaches using the rouge metric lin which is a recall based metric frequently used for summarization and meteor denkowski and lavie which is a precision based metric for machine translation overlap can be interpreted as the tendency of the model to directly copy input tent instead of generating novel correct or incorrect words whereas repeat measures a model s tendency to repeat self which is a frequent issue with encoder decoder models suzuki and nagata on title generation rwmd rank achieved the best mance in terms of selecting a sentence as the title in all the abstractive systems signicantly outperformed the extractive systems as well as the extractive oracle and fconv performed much better than lstm with a very high rate of overlap the rouge performance of and fconv is similar despite the difference of a few points in favour of fconv that model is evaluated on a level ground truth le where we observe a slight increase of rouge points on average due to the conversion the baseline remained on abstract generation tough to beat in terms of rouge and only the extractive systems managed to surpass it by a small margin all tractive systems achieved similar results with rwmd rank table metric results for the title gen dataset r l represent the l metrics r l meteor token count overlap model oracle lexrank emb tdf rwmd rank lstm fconv table metric results for the abstract gen dataset r l represent the l metrics model oracle lexrank emb tdf rwmd rank fconv repeat r l meteor token count overlap having a minor edge while the abstractive fconv performed poorly even though it performed best in terms of teor we observed a much higher repeat rate in the put summaries than the observed average in the inal abstracts table as revealed by the large repeat standard deviation for fconv some examples are affected by very frequent repetitions qualitative evaluation in tables and we present two shortened inputs from our title gen and abstract gen test sets along with original and system generated summaries in figure we show a histogram of the locations of input sentences that estimates which locations were most preferred on average when ducing a summary we observe a large variation in the sentence locations lected by the extractive systems on title gen figure with the rst sentence having high importance based on our inspection it is rare that a sentence from the abstract will match the title exactly the title is also typically shorter than an average sentence from the abstract table a good title seems to require the selection combination and paraphrasing of suitable parts from multiple sentences as also shown by the original titles in our examples many of the titles generated by the abstractive systems sound ful and at rst glance can pass for a title of a scientic per the abstractive models are good at discerning tant from unimportant content in the abstract at extracting long phrases or sometimes whole sentences and at tively combining the information to generate a title lstm is more prone to generate novel words whereas and fconv mostly rely on direct copying of content from the abstract as also indicated by their overlap scores closer inspection of the titles reveals occasional subtle takes for example in the rst example in table the fconv model incorrectly selected and duced which was investigated in the previous work of the authors and is not the main focus of the article the model also copied the incorrect genus mouse instead of rat sometimes the generated titles sound too general and fail to communicate the specics of the paper in the second example all models produced a model of basal ganglia missing to include the keyword reinforcement learning a model of reinforcement learning in the basal ganglia these mistakes highlight the complexity of the task and show that there is still much room for further improvement as shown in figure the introductory and concluding sections are often highly relevant for abstract generation however relevant content is spread across the entire paper interestingly in the example in table there is a wide range of content that was selected by the extractive systems with little overlap across systems for instance rwmd rank overlaps with oracle by sentences and only by sentence with emb tdf the outputs of the abstractive fconv tem on abstract generation are poor in quality and many of the generated abstracts lack coherent structure and content ow there is also frequent repetition of entire sentences as shown by the last sentences produced by fconv in table fconv also appears to only use the rst sentences of the paper to construct the abstract figure conclusion we evaluated a range of extractive and abstractive ral network based summarization approaches on two novel datasets constructed from scientic journal articles while the results for title generation are promising the models struggled with generating the abstract this difculty lights the necessity for developing novel models capable of efciently dealing with long input and output sequences while at the same time preserving the quality of generated sentences we hope that our datasets will promote more work in this area a direction to explore in future work is hybrid extractive abstractive end to end approaches that jointly select content and then paraphrase it to produce a summary c figure sentence selection normalized histograms computed on the test set showing the input locations that were most preferred on average by the systems on title gen a and abstract gen b c for we normalize the sentence locations by the length of each paper to get a better uniform view there is a large variation in the length of a paper as shown in table for the abstractive systems we search for the closest sentences in the input using relaxed word mover s distance see section table examples from the test set of title gen the outputs of the extractive systems are highlighted as oracle tdf emb rwmd rank for the abstractive systems we manually highlighted the text of the concepts that are relevant for the task errors are highlighted in red example giridharan et al abstract amyloid neurotoxicity is a major pathological mechanism of alzheimers disease ad our previous studies have demonstrated that schisandrin b sch b an antioxidant lignan from schisandra chinensis could protect mouse brain against and cisplatin induced neuronal dysfunction in the present study we examined the protective effect of sch b against intracerebroventricular a induced neuronal dysfunction in rat cortex and explored the potential mechanism of its action our results showed that days co administration of sch b significantly improved the ioral performance of a rats in step through test at the same time sch b attenuated a induced increases in oxidative and nitrosative stresses the aforementioned effects of sch b suggest its protective role against a induced neurotoxicity through intervention in the negative cycle of rage mediated a accumulation during ad patho physiology original title schisandrin b ameliorates icv infused amyloid induced oxidative stress and neuronal dysfunction through inhibiting rage nf b mapk and up regulating hsp beclin expression lstm schisandrin b an antioxidant lignan from schisandra chinensis protects against amyloid neurotoxicity schisandra chinensis b protects against intracerebroventricular infused amyloid induced neuronal dysfunction in rat cortex fconf schisandrin b protects mouse brain against and induced neurotoxicity in rats example fee abstract in its simplest formulation reinforcement learning is based on the idea that if an action taken in a particular context is followed by a favorable outcome then in the same context the tendency to produce that action should be strengthened or reinforced recent experiments in the songbird suggest that vocal related bg circuitry receives two functionally distinct excitatory inputs the other is an efference copy of motor commands from a separate cortical brain region that generates vocal variability during learning based on these ndings i propose here a general model of vertebrate bg function that combines context information with a distinct motor efference copy signal the model makes testable predictions about the anatomical and functional properties of hypothesized context and efference copy inputs to the striatum from both thalamic and cortical sources original title oculomotor learning revisited a model of reinforcement learning in the basal ganglia incorporating an efference copy of motor actions lstm a model of basal ganglia function a general model of vertebrate basal ganglia function fconf a model of basal ganglia function in the songbird acknowledgments we thank the reviewers for their useful comments and nvidia for the donation of a titan x graphics card references bahdanau cho and bengio neural chine translation by jointly learning to align and late arxiv preprint biber and gray challenging stereotypes about academic writing complexity elaboration plicitness journal of english for academic purposes brokos malakasiotis and androutsopoulos using centroids of word embeddings and word mover s distance for biomedical document retrieval in question answering arxiv preprint chiu crichton korhonen and pyysalo how to train good word embeddings for ical nlp proceedings of page cho van merrienboer gulcehre danau bougares schwenk and gio learning phrase representations using rnn encoder decoder for statistical machine translation arxiv preprint cho courville and bengio ing multimedia content using attention based encoder decoder networks arxiv preprint collins augenstein and riedel a vised approach to extractive summarisation of scientic table two examples from the test set of abstract gen the outputs of the extractive systems are highlighted as tdf emb and rank whereas gray denotes overlap between the two in bold we mark the content that was selected by the fconv system next page in full and in underline we mark the selection of the oracle example pyysalo al body in recent years there has been a significant shift in focus in biomedical information extraction from simple pairwise relations representing associations such as protein protein interactions ppi toward representations that capture typed structured associations of arbitrary numbers of entities in specic roles frequently termed event extraction much of this work draws on the genia event corpus this resource served also as the source for the annotations in the rst collaborative evaluation of biomedical event extraction methods the bionlp shared task on event extraction bionlp st as well as for the genia subtask of the second task in the series another recent trend in the domain is a move toward the application of extraction methods to the full scale of the existing literature with results for various targets covering the entire pubmed literature database of nearly million citations being made available as event extraction methods initially developed to target the set of events dened in the genia bionlp st corpora are now being applied at pubmed scale it makes sense to ask how much of the full spectrum of gene protein associations found there they can maximally cover by contrast we will assume that associations not appearing in this data can not be extracted as the overwhelming majority of current event extraction methods are based on supervised machine learning or hand crafted rules written with reference to the annotated data it reasonable to assume as a rst approximation that their coverage of associations not appearing in that data is zero in this study we seek to characterize the full range of associations of specic genes proteins described in the literature and estimate what coverage of these associations event extraction systems relying on currently available resources can it is necessary not only to have an inventory of concepts that largely maximally achieve to address these questions covers the ways in which genes proteins can be associated but also to be able to estimate the relative frequency with which these concepts are used to express gene protein associations in the literature here as we are interested in particular in texts describing associations between two or more gene protein related entities we apply a focused selection picking only those individual sentences in which two or more mentions co occur while this excludes associations in which the entities occur in different sentences their relative frequency is expected to be low for example in the bionlp st data all event participants occurred within a single sentence in of the targeted biomolecular event statements here we follow the assumption that when two entities are stated to be associated in some way the most important words expressing their association will typically be found on the shortest dependency path connecting the two entities cf the shortest path hypothesis of bunescu and mooney the specic dependency representation table shows the words most frequently occurring on these paths this list again suggests an increased focus on words relating to gene protein associations expression is the most frequent word on the paths and binding appears in the top ranked words finally to make this pair data consistent with the tps event spans tokenization and other features we aligned the entity annotations of the two corpora this processing was applied to the bionlp st training set creating a corpus of entity pairs of which were marked as expressing an association positive evaluation we rst evaluated each of the word rankings discussed in the section on identication of gene protein associations by comparing the ranked lists of words against the set of single words marked as trigger expressions in the bionlp st development data to evaluate the capability of the presented approach to identify new expressions of gene protein associations we next performed a manual study of candidate words for stating gene protein associations using the e w ranking we then selected the words ranked highest by e w that were not known grouped by normalized and lemmatized form and added for reference examples of frequent shortest dependency paths on which any of these words appear see example in table if static relations and experimental observations and manipulations are excluded as arguably not in scope for event extraction this estimate suggests that currently available resources for event extraction cover over of all events involving gene protein entities in pubmed sion we found that out of all gene protein associations in pubmed currently existing resources for event extraction are lacking in coverage of a number of event types such as dissociation many relatively rare though biologically important protein post translational modications as well as some high level process types involving genes proteins such as apoptosis this suggests that for practical applications it may be important to consider also this class of associations while these results are highly encouraging it must be noted that the approach to identifying gene protein associations considered here is limited in a number of ways it excludes associations stated across sentence boundaries and ones for which the shortest path hypothesis does not hold does not treat multi word expressions as wholes ignores ambiguity in implicitly assuming a single sense for each word and only rectly includes associations stated between exactly two entities the approach is also fundamentally limited to associations expressed through specic words and thus blind to part of relations implied by statements such as binding site sions we have presented an approach to discovering expressions of gene protein associations from pubmed based on named entity co occurrences shortest dependency paths and an unlexicalized classier to identify likely statements of gene protein associations drawing on the automatically created full pubmed annotations of the turku pubmed scale tps corpus and using the shared task data to dene positive and negative examples of association statements we distilled an initial set of over million protein mentions into a set of unique unlexicalized paths estimated likely to express gene protein associations these paths were then used to rank all words in pubmed by the expected number of times they are predicted to express such associations and candidate association expressing words not appearing in the shared task data evaluated manually study of these candidates suggested new event classes for the genia ontology and indicated that the majority of statements of gene protein associations not covered by currently available resources are not statements of biomolecular events but rather statements of static relations or experimental manipulation it could thus be assumed that the event types and the specic statements annotated in genia would have only modest coverage of all gene protein association types and statements in pubmed et al body the same site is exceedingly rare with less both alopecia areata aa and vitiligo are autoimmune diseases and their than ve cases being reported in the example original abstract background event extraction following the genia event corpus and bionlp shared task models has been a considerable focus of recent work in biomedical information extraction this work includes efforts applying event extraction methods to the entire pubmed literature database far beyond the narrow subdomains of biomedicine for which annotated resources for extraction method development are available results in the present study our aim is to estimate the coverage of all statements of gene protein associations in pubmed that existing resources for event extraction can provide we base our analysis on a recently released automatically annotated for gene protein entities and syntactic analyses covering the entire pubmed and use named entity co occurrence shortest dependency paths and an unlexicalized classier to identify likely statements of gene protein ciations a set of high frequency high likelihood association statements are then manually analyzed with reference to the genia ontology conclusions we present a rst estimate of the overall coverage of gene protein associations provided by existing resources for event extraction our results suggest that for event type associations this coverage may be over we also identify several biologically signicant associations of genes and proteins that are not addressed by these resources suggesting directions for further extension of extraction coverage example fconv background in recent years there has been a signicant shift in focus in biomedical information extraction from simple pairwise relations representing associations such as protein protein interactions ppi toward representations that capture typed structured associations of arbitrary numbers of entities in specic roles frequently termed event extraction as event extraction methods are based on supervised machine learning or hand crafted rules written with reference to the annotated data it is necessary not only to have an inventory of concepts that largely covers the ways in which genes proteins can be associated but also to be able to estimate the relative frequency with which these concepts are used to express gene protein associations results we apply a focused selection picking king only those individual sentences in which at least one named entity has been tagged we apply a focused selection picking only those individual sentences in which at least one named entity has been tagged we apply a focused selection picking only those individual sentences in which at least one named entity has been tagged we apply a probabilistic approach example kumar coexistence in the same patient is not uncommon as vitiligo has been reported to occur in of patients of aa and is about times more common in patients with aa than in the general population their ization over we present a case of a year old male child who had vitiligo and later developed aa over the existing lesions of vitiligo over face and scalp and have attempted to elucidate the current understanding of mechanisms of coexistence of these two diseases a year old boy presented to the skin outpatient department with history of depigmented areas on the scalp face neck arms and legs for years he also gave a history of development of patchy loss of hair over some of these lesions for years there was no previous history of any trauma or medications family history was not relevant on examination there were depigmented macules over the scalp forehead eyebrows eyebrows perioral preauricular regions neck elbows hands feet shins nose chin hands knees and feet patches of hair loss were seen limited to some of these depigmented areas over the vertex and occipital region of the scalp and eyebrows figure other body areas were not affected by patchy hair loss clinically the diagnosis of vitiligo with aa was made additionally the basal layer of the epidermis was almost devoid of pigment figure conrming the diagnosis of vitiligo over the same site both aa and vitiligo are clubbed under the spectrum of autoimmune ders our case lends support to the hypothesis that aa and vitiligo share a common pathogenic pathway including autoimmune antigens response against some common antigens like those derived from the bulb melanocytes melanocytes derived released during vitiligo pathogenesis could act as auto antigens not only for vitiligo but also for aa and autoimmune th cells against them could also trigger a response against the hair follicle melanocytes thus pre disposing to aa text omitted example original abstract both alopecia areata aa and vitiligo are autoimmune diseases and their coexistence in the same patient is not uncommon as vitiligo has been reported to occur in of patients of aa we present a case of a year old male child who had vitiligo and later developed aa over the existing lesions of vitiligo over face and scalp and have attempted to elucidate the current understanding of mechanisms of coexistence of these two diseases our case lends support to the hypothesis that aa and vitiligo share a common pathogenic pathway including autoimmune response against some common antigens like those derived from the bulb melanocytes stimulation of proinammatory t cell mediated immunological response or inactivation of a suppressor t cell mediated response could be the common underlying phenomenon however the striking rarity of colocalization of these two diseases has led to the recent debate over current understanding of their pathogenesis and whether this association is merely a coincidence as both aa and vitiligo are frequent and chronic dermatological disorders it is of utmost importance to gain more understanding into their pathogenic mechanisms so that more denitive treatment modalities may be devised and the quality of life of these patients can be improved example fconv alopecia areata aa and vitiligo are autoimmune diseases and their coexistence in the same patient is not uncommon as vitiligo has been reported to occur in of patients of aa and is about times more common in patients with aa than in the general population we present a case of a year old male child who had vitiligo and later developed aa over the scalp forehead eyebrows eyebrows perioral preauricular regions nose and chin depigmented macules over the scalp forehead eyebrows eyebrows perioral preauricular regions nose and chin depigmented macules over the scalp forehead eyebrows periorbital perioral preauricular regions nose and chin depigmented macules over the scalp forehead eyebrows periorbital perioral preauricular regions nose and chin depigmented macules over the scalp forehead eyebrows periorbital however peptide abstractive text summarization using sequence to sequence rnns and beyond arxiv preprint nenkova maskey and liu automatic summarization in proceedings of the annual ing of the association for computational linguistics tutorial abstracts of acl page association for computational linguistics page brin motwani and winograd the pagerank citation ranking bringing order to the web technical report stanford infolab pyysalo ohta and tsujii an analysis of gene protein associations at pubmed scale journal of biomedical semantics rossiello basile and semeraro radev jing stys and tam centroid based summarization of multiple documents information processing management centroid based text summarization through tionality of word embeddings multiling page rush chopra and weston a neural attention model for abstractive sentence summarization arxiv preprint sennrich haddow and birch neural machine translation of rare words with subword units arxiv preprint sutskever vinyals and le sequence to sequence learning with neural networks in advances in neural information processing systems pages suzuki and nagata cutting off redundant repeating generations for neural abstractive tion in proceedings of the conference of the pean chapter of the association for computational guistics volume short papers volume pages teufel and moens summarizing scientic articles experiments with relevance and rhetorical tus computational linguistics papers arxiv preprint denkowski and lavie meteor universal language specic translation evaluation for any target language in proceedings of the eacl workshop on statistical machine translation dietterich domingos getoor muggleton and tadepalli structured machine learning the next ten years machine learning erkan and radev lexrank graph based lexical centrality as salience in text summarization nal of articial intelligence research fee oculomotor learning revisited a model of reinforcement learning in the basal ganglia rating an efference copy of motor actions frontiers in neural circuits gehring auli grangier yarats and dauphin convolutional sequence to quence learning arxiv preprint giridharan thandavarayan arumugam mizuno nawa suzuki ko namurthy watanabe and konishi schisandrin b ameliorates icv infused amyloid induced oxidative stress and neuronal dysfunction through hibiting rage nf b mapk and up regulating hsp beclin expression plos one hunter and cohen biomedical language processing what s beyond pubmed molecular cell kalchbrenner espeholt simonyan van den time corr oord graves and kavukcuoglu neural machine translation in linear kim singh and lee wards abstraction from extraction multiple timescale gated recurrent unit for summarization arxiv preprint kumar mittal and mahajan tion of vitiligo and alopecia areata coincidence or sequence international journal of trichology kusner sun kolkin weinberger al from word embeddings to document tances in icml volume pages lecun bottou bengio and haffner gradient based learning applied to document tion proceedings of the ieee lee cho and hofmann fully level neural machine translation without explicit tation arxiv preprint lin rouge a package for automatic tion of summaries in text summarization branches out proceedings of the workshop volume lloret roma ferri and palomar compendium a text summarization system for ating abstracts of research papers data knowledge engineering mikolov chen corrado and dean efcient estimation of word representations in vector space arxiv preprint nallapati zhou c and xiang
