learning syntactic and dynamic selective encoding for document summarization haiyang xu yahao he kun han junwen chen and xiangang li ai labs didi chuxing co ltd beijing china email xuhaiyangsnow heyahao kunhan chenjunwen com r a m l c s c v v i x r a abstract text summarization aims to generate a headline or a short summary consisting of the major information of the source text recent studies employ the sequence to sequence framework to encode the input with a neural network and generate abstractive summary however most studies feed the encoder with the semantic word embedding but ignore the syntactic information of the text further although previous studies proposed the selective gate to control the information ow from the encoder to the decoder it is static during the decoding and can not differentiate the information based on the decoder states in this paper we propose a novel neural architecture for document summarization our approach has the following contributions rst we incorporate syntactic information such as constituency parsing trees into the encoding sequence to learn both the semantic and syntactic information from the document resulting in more accurate summary we propose a dynamic gate network to select the salient information based on the context of the decoder state which is essential to document summarization the proposed model has been evaluated on cnn daily mail summarization datasets and the experimental results show that the proposed approach outperforms baseline approaches index terms summarization parse tree dynamic selective gate syntactic attention i introduction text summarization is a very challenging task of natural guage processing nlp and information retrieval existing proaches for text summarization are categorized into two major types extractive and abstractive extractive methods produce summaries by extracting sentences or tokens from the source text which can produce the grammatically correct summaries and preserve the meaning of the original text however these approaches heavily rely on the text in the original documents and the extracted sentences may contain redundant information or be lack of readability on the contrary abstractive methods produce the summaries by generating new sentences or tokens which do not necessarily appear in the source text however abstractive approaches are more difcult in practice because they need to address many nlp problems including document understanding semantic representation and natural language generation which are harder than sentence extraction the recent neural sequence to sequence proach has achieved tremendous success in many nlp tasks such as machine translation dialogue systems the essence of based text summarization methods is an encoder decoder framework which rst encodes a input sentence to a low dimensional representation and then fig the example of text summarization in cnn dataset the colored text show the source text corresponding maries generated by human written original with attention and the proposed approach respectively decodes the abstract representation into a output sequence as the extension of methods attention based models encode a input sequence to a context vector using attention mechanism and dynamically calculate the attentive probability distribution at each generation step similar to machine translation some researchers have plied neural model to abstractive text summarization however there is a signicant difference between two tasks in machine translation one aims to capture all the semantic details from the source text while in text rization one only focuses on the salient text information and it is critical to utilize only the key information in the source text rather than the whole text furthermore the original with attention method does not learn the syntactic structure of the source text which is important to text summarization in figure a piece of original text is shown at the top the human written summary is shown in the next box as the target the third box shows a model generated summary using the original with attention method baseline the green text in the target and the red text in the baseline show the summary corresponding to the blue text in the original text as shown in the gure the baseline model incorrectly summarizes that he says he will become the citizens mainly because it is not able to capture the internal syntactic structure of the original text e applicants is a noun phase and applied to ceremony is an attributive clause for it to address these problems we propose a novel syntactic and dynamic selective encoding method for document tion we incorporate structured linguistic information such as parsing trees to learn more effective sentence representation by serializing parsing trees into a encoder sequence as in in this way the encoding sequence contains both the semantic words and the syntactic parsing symbols information both of which are fed into the decoder for summary generation in addition a document may contains hundreds of words and it is hard to directly encode the key information from the whole source text a selective gate was proposed in previous study to lter out the secondary information however the salient information varies in different decoding stage so it is better to select the salient information based on the context of decoder states therefore for each decoding step we take a dynamic selective gate network to control the salient mation ow according to the document information current decoder state and the previous gated encoder state in this way our approach can learn better representation of the sentences and select the salient information from the long input sequence for summary generation as an example we show the summary generated by our approach in figure where the text correctly summarize the original sentences from the document for reference figure shows the constituency parsing tree for the source sentence and figure shows the change of the states of the dynamic selective gates in a input sentence we also conduct experiments on two large scale cnn daily mail datasets and the experimental results show that our model achieves superiority over baseline tion models we organize the paper as follows sec ii introduces the related work sec iii describes our proposed method in sec iv we present the experiment settings and illustrate experimental results we conclude the paper in sec v ii related work in general there are two broad approaches to automatic text summarization extractive and abstractive extractive methods work by selecting important sentences or passages in the original text and reproducing them as summary in contrast abstractive summarization techniques generate new shorter texts that seldom consist of the identical tences from the document banko et al apply statistical models of the term selection and term ordering processes to produce short summaries bonnie and dorr implement a system using a combination of linguistically motivated sentence compression technique other notable methods for abstractive summarization include using discriminative tree tree transduction model and quasi synchronous grammar approach utilizing both context free parses and dependency parses recently researchers have started utilizing deep learning framework in extractive and abstractive summarization for extractive methods nallapati et al use recurrent neural networks rnns to read the article and get the representations of the sentences and select important sentences yasunaga al combine rnns with graph convolutional networks cnns to compute the salience of each sentence narayan et al propose a framework composed of a hierarchical encoder based on cnns and an attention based extractor with attention over external information more works are published recently on abstractive methods rush et al rstly apply modern neural networks to text summarization by using a local attention based model to generate word conditioned on the input sentence a bunch of work have been proposed to extend this approach which achieving further improvements in performance chopra et al use a similar tional attention based encoder and replace the decoder with a conditional rnns nallapati et al apply encoder decoder rnns framework with hierarchical attention and feature rich embedding vector tan et al propose graph based attention mechanism to summarize the salient information of document however the above neural models can not emit unseen words since the vocabulary is xed at training time in order to solve this problem the point network and the copynet have been proposed to allow both copying words from the original text and generating words from a xed vocabulary hsu et al combine the strength of extractive and stractive summarization and propose an inconsistency loss zhou et al extend general encoder decoder framework with a selective gate network which helps improve encoding effectiveness and release the burden of the decoder our work has several signicant improvements comparing with previous studies first to incorporate syntactic tion previous works only use unstructured linguistic mation such as part of speech pos tags and named entity in this work we utilize a structured syntactic parsing tree to learn a more effective context vector which improves the performance of word prediction and alleviate the repetition problem second to choose the salient information previous works employ a selective gate network which is static during the decoding stage we improve the gate network and let the states of the gate dynamically adjust according to the context of the decoder states which is essential to document summarization iii methodology in this section we describe the proposed model the architecture of the syntactic and dynamic selective syntactic encoding model is shown in figure which consists of the syntactic sequence encoder the dynamic selective gates and the pointer generator network with syntactic attention decoder fig overall architecture of the proposed syntactic and dynamic selective encoding model the parsing tree of each sentence is serialized and fed into the encoder to help attain syntactic meanings in jth decoding stage the decoder benets from the dynamic selective gate to drop out trivial words as well as attention mechanism inuenced by the syntactic vector a syntactic sequence encoder previous studies usually treat a document as a sequence of words but ignore the syntactic structure of document to leverage the syntactic knowledge we design a syntactic sequence encoder to learn document representations a document d is denoted as a sequence of sentences q d qn where n is the number of sentences in the document for each sentence ql we apply a syntactic parser to generate a parsing tree and then adopt a traversal to serialize the parsing tree to a sequence of tokens ql ekl where kl is the number of tokens in the serialized parsing tree note that the token is not necessarily a word in a parsing tree a leaf node represents a word while a non leaf represents a parsing symbol including either a phrase label or a pos tag then for a document we concatenate all the serialized parsing trees into a long sequence em here m is the total number of the tokens in all parsing trees m kl to model the sequential information we rst use an embedding vector xi to represent the token ei which can be either a word or a symbol in the parsing tree then we employ a bidirectional long short term memory bilstm to encoder the sequence information i m i m h h h i denote the hidden state of the forward where lstm and the backward lstm respectively the whole representation of ith token is the concatenation of the hidden states from both directions hi h h i and h i to model the syntactic information we apply the the hidden states corresponding to the pooling over all parsing symbols to produce the syntactic vector ds max s where s is the set of all parsing symbols in the document as shown in figure the syntactic sequence encoder takes the serialized parsing tree as the input the bilstm compute the hidden states for both the words e and the parsing symbols e as input the word hidden states are used for further computation while the hidden states of the parsing symbols are max pooled to generate a syntactic vector b dynamic selective gates as we discussed in sec i for document summarization not all the information in the source should be fed into the decoder and it is more important to only select the salient information and remove the unnecessary information from the input herein we propose a novel dynamic selective gate to model the generation process of the salient information we use a parameterized gate network to select the useful information for the summary generation the gate state takes as input from both the state of the source and the previous state of the generated target as well as a low dimensional representation of the whole document dh which is a concatenation of the last state of the forward lstm and the rst state of the backward lstm dh h m h specically for the ith encoder step and jth decoder step the state of the dynamic selective gate gj i is calculated as g dh g sj g bg gj i hj i gj i hi where sj is the state of the jth step from the decoder lstm which will be discussed in the next subsection is the gated hidden state of the encoder denotes the sigmoid function and denotes element wise multiplication when j equals gj i is set to vector wg ug vg bg are trainable parameters note that previous study has utilized the selective gate to control the information ow but the gate state only depends on the hidden states of the source text and the selective gate is static during the whole decoding stage but the proposed dynamic selective gate depends on both the encoder and the decoder states suggesting that the gate only open to the information which is useful for the current target output rather than the whole target outputs this is critical to document summarization because the length of the document is long and a static gate may select much irrelevant information from the source at every decoding step we will show the effectiveness of the dynamic selective gate in sec iv e c pointer generator network with syntactic attention coder we use the recent proposed pointer generator network for decoding which allows either copying words from the original text via pointers or generating new words based on the source vocabulary to handle the oov problem specically the attention strength between the ith source step and the jth target step is calculated by the current decoder state sj the current gated encoder hidden state hj i and the document syntactic vector the context vector cj is calculated by the attention weighted summation of the gated encoding hidden states hj i a ds a sj ba ej a i cj aj i hj i i where wa ua va ba are trainable parameters an lstm takes as input from the word embedding vector of the previous generated word the previous context vector and the previous decoder hidden state to compute the new decoder state sj and then the current context vector cj and the current decoder hidden state sj are fed into two linear layers and predicts the probability for each word w in the vocabulary using the softmax function pvoc j w v v cj bw bv where wv uv bw bv are trainable parameters further a pointer generator network produces the switch probability pgen to decide whether generates a word by pvoc or copies a word from the original source text pgen is calculated the decoder state sj and the from the context vector cj decoder word yj the nal probability with the word w is calculated based on pgen and the attention distribution pgen cj pgen p sj yj bp aj i w where wp up vp bp are trainable parameters d model training to train the model we use the negative log likelihood function log as the loss for each document we further adopt the coverage loss from see et al aiming to handle the repetition problem in text summarization the coverage loss at the decoding step j corresponding to the encoding step i is the summation of attention distributions over all previous decoding step aj i at i the nal loss function at the decoding step j is log i aj i i iv experiments in this section we describe the experiment details including datasets implementation details baselines and the results a datasets we conduct experiments on cnn daily dataset which comprises multi sentence summaries and has been widely used in automatic text summarization we use released to obtain the same version of the the data which has training pairs validation pairs and test pairs the source documents have words spanning sentences on an average while the summaries consist of words and sentences the dataset is released in two versions one is anonymized version which has been processed to replace each named entity and the other is the original version consisting of actual entity names in this work we use the original text since it requires no pre processing and is more challenging because anonymized dataset replaces named entities with unique identier which always are out of vocabulary in the following experiments all the models are trained and tested with three different datasets separately including cnn corpus daily mail corpus and the combination of cnn and daily mail corpus table i shows the detail statistics information of experiment datasets b implementation for all experiments we use words of the source ulary and stanford constituency to get the syntactic information of the sentences in the corpora which includes phrase labels and pos tags our model takes dimensional hidden states dimensional word embedding vectors and use adagrad with learning rate and initialize com abisee pointer generator com abisee cnn dailymail stanford edu software srparser html data set cnn daily cnn daily avgdocsents avgdocwords avgsumwords avgsumsents table i data statistics for cnn and cnn daily mail datasets avgdocsents is the average sentences number of original documents and avgdocwords is the average sentences length of original documents avgsumsents is the average sentences number of summaries and avgdocwords is the average sentences length of summaries the accumulator value with this was found to work best among stochastic gradient descent adadelta momentum adam and rmsprop in addition we set the maximum length of sentence on source side to on target side for training and testing to and respectively to both decode fast and get better results we set the beam size to in our experiments furthermore we added the coverage mechanism in loss function with coverage loss weighted to c baselines we compare our proposed model with several state the art automatic text summarization systems and techniques consisting of extractive and abstractive methods is a standard extractive baseline which generates summary simply by selecting the leading three tences from source document nn se utilizes encoder decoder framework which learns the representation of source though encoder and classies sentences of document by decoder applies encoder decoder rnn tive framework with hierarchical attention and rich embedding vector treats extractive summarization as a sequence classication problem where a binary decision has been made on each sentence about whether or not it should be included in the summary summarunner is also an extractive model is trained directly on the ilar to summarunner but abstractive summaries we use a framework based on uni gru with non hierarchical attention as our baseline model distraction is an extension of model with distract mechanism to traverse between ferent content of a document to better grasp the overall meaning for summarization graph based model proposes a novel abstractive graph based attention mechanism in the work which aims to nd salient content from the original document deeprl proposes a unied framework combining and rl into to improve the quality of summary pointer improves the standard model with a hybrid pointer generator which can not only produce novel words but also copy words from the source text selectivegate proposes the encoder decoder work based on a static selective gate network which helps improve encoding effectiveness and release the burden of the decoder d experimental results we adopt the widely used by pyrouge for evaluation metric it measures the similarity of the output summary and the standard reference by computing ping n gram such as unigram bigram and longest common subsequence in the following experiments we adopt unigram bigram and rouge l longest common subsequence for evaluation it can be observed from tables ii and iii that the posed approach achieves the best performance on the two datasets our best model outperforms all baseline extractive and abstractive models on and l compared with abstractive graph based rl based and leverages the structural summarunner model our model information of document and improves the pointer network with syntactic attention to copy relevant words in semantic and structural aspect from the original text to handle oov problems while graph based rl based and summarunner take the anonymized data which has replaced model all named entity with to alleviate oov problems thermore unlike graph based rl based and summarunner model we do not pretrain the word embedding vectors method rl distraction graph based model po o po our syntax our selective our table ii comparison results on the cnn test set tively using the full length variants of rouge baseline model results with mark are taken from the corresponding papers all our rouge scores have a condence interval of at most as reported by the ofcial rouge script we also compare in detail with two similar methods in table ii for pointer generator with coverage po model results of baselines are incomplete on sub dataset because some other researchers chose to report results on only one sub dataset python org pypi method rl summarunn summarunner graph based model deeprl pointer our model table iii comparison results on the cnn daily mail test set using the full length variants of rouge baseline model results with mark are taken from the corresponding papers represents vocabulary size of and represents vocabulary size of all our rouge scores have a condence interval of at most as reported by the ofcial rouge we show that with the help of structural information and dynamic selective gate the scores of our best model performs the best over the po model on evaluation metrics and rouge l for static selectivegate model we conduct two experiments with and without po due to its original paper focusing on short text summarization which does not use mechanism to alleviate oov and word repetition problems the result demonstrates that the static selectivegate improves the performance of po model and the dynamic selectivegate can further improve the rouge scores of static selectivegate model by selecting current important information for decoding in every time step further to study the different impacts of source syntax and dynamic selective gate on the performance of the proposed model we conduct ablation experiments on the cnn dataset where we train the model with the source syntax encoding only and the dynamic selective gate only respectively as shown in the last three rows in table ii that either the source syntax encoding or the dynamic selective gate can improve the performance compared with selectivegate approach combining both approach leads to further improvement which achieves the best results as shown in the last row in the table l rl table iv comparison results with different document lengths on the cnn dataset respectively using the full length variants of rouge all our rouge scores have a condence interval of at most as reported by the ofcial rouge in addition to study the impact of the lengths of document on the performance of the proposed model we conduct experiments on the cnn test sets between and table iv clearly shows that the performance of the proposed approach is stable across different lengths of document e example analysis in this subsection we use an example to show the tiveness of the syntactic encoding and the dynamic selective mechanism we choose the same example introduced in figure figure shows the constituency parsing tree of the sentence the baseline model in figure generates wrong summary cause it is not able to model the syntactic structure e applicants is a noun phase and applied to ceremony is an attributive clause in our approach the bilstm encoder takes as input from the serialized parsing tree and each word token is surrounded by the parsing symbols intuitively if two consecutive words do not belong to the same syntactic subtree more parsing symbols will be inserted between them in the bilstm encoder and it will be less likely that these two symbols have strong connection as shown at the top in figure the generated summary of our model correctly conveys the summary of the source text fig a parsing tree corresponding to the blue text in figure the dashed box shows that applied to ceremony is an attributive clause for applicants the upper box shows the our generated summary which correctly summarize the original document for the dynamic selective gate we use a method in to visualize it the method denes a highly non linear function to measure the contribution of the source word wi gated by gj i in the jth generation step as shown in figure the dynamic selective mechanism can select the most important information from the original document in every decoding step for example at decoding step the selective gate lters out some nonsensical words e the is he and selects current important words e jedlicka politician to help the following attention to generate the most important word e jedlicka furthermore figure also shows that the word out of source vocabulary can also be generated e vit jedlicka but the weight of the selected words will decrease in the next decoding steps indicating that our model can address the oov problem and the word repetition problem fig visualization of the dynamic selective gates the gates dynamically adjust the states in different decoding steps darkness of the blocks indicates the openness of the gates v conclusions and future work in this work we propose a novel document summarization model which takes its input from a serialized parsing tree which enables the encoder to learn the inherent syntactic structure from each sentence in the document further we pose a dynamic selective gate to control the information ow from the encoder to the decoder this mechanism dynamically control the salient information based on the context of the decoder state which is essential to document summarization the experimental results on two long text datasets cnn daily mail show the advantage of our model over several baseline approaches in this work we use the traversal to generate the serialized parsing tree which is not able to contain all structure information from the tree we will consider better tion to model the hierarchical structure of the parsing tree on the other hand the proposed dynamic selective gate only apply to the word tokens in the source we may consider integrating both the word tokens and the parsing symbol to produce better the information ow from the source to the target references i sutskever o vinyals and q v le sequence to sequence learning with neural networks in proceedings of the conference on neural information processing systems pp d bahdanau k cho and y bengio neural machine translation by jointly learning to align and translate arxiv preprint t luong h pham and c d manning effective proaches to attention based neural machine translation in proceedings of the conference on empirical methods in natural language processing pp i v serban a sordoni y bengio a courville and j pineau building end to end dialogue systems ing generative hierarchical neural network models in proceedings of the aaai conference on articial intelligence aaai press pp a bordes y boureau and j weston ing end to end goal oriented dialog arxiv preprint j gu z lu h li and v o li incorporating copying mechanism in sequence to sequence learning arxiv preprint r nallapati b zhou c dos santos c gulcehre and b xiang abstractive text summarization using sequence to sequence rnns and beyond in proceedings of the signll conference on computational ural language learning pp a see p j liu and c d manning get to the point summarization with pointer generator networks in ceedings of the annual meeting of the association for computational linguistics volume long papers vol pp j tan x wan and j xiao abstractive document summarization with a graph based attentional neural model in proceedings of the annual meeting of the association for computational linguistics volume long papers vol pp j li d xiong z tu m zhu m zhang and g zhou modeling source syntax for neural machine translation in proceedings of the annual meeting of the ciation for computational linguistics volume long papers vol pp q zhou n yang f wei and m zhou selective encoding for abstractive sentence summarization in proceedings of the annual meeting of the ciation for computational linguistics volume long papers vol pp k wong m wu and w li extractive rization using supervised and semi supervised learning in proceedings of the international conference on computational linguistics volume association for computational linguistics pp d wang s zhu t li and y gong multi document summarization using sentence based topic models in proceedings of the annual meeting of the ciation for computational linguistics association for computational linguistics pp a celikyilmaz and d hakkani tur a hybrid archical model for multi document summarization in proceedings of the annual meeting of the ation for computational linguistics association for computational linguistics pp r m alguliev r m aliguliyev m s hajirahimova and c a mehdiyev mcmr maximum coverage and minimum redundant text summarization model expert systems with applications vol no pp g erkan and d r radev lexrank graph based cal centrality as salience in text summarization journal of articial intelligence research vol pp r m alguliev r m aliguliyev and n r isazade multiple documents summarization based on ary optimization algorithm expert systems with cations vol no pp summarization using inconsistency loss in proceedings of the annual meeting of the association for putational linguistics volume long papers a graves and j schmidhuber framewise phoneme classication with bidirectional lstm and other neural network architectures neural networks vol no pp j cheng and m lapata neural summarization by tracting sentences and words in proceedings of the annual meeting of the association for computational linguistics volume long papers vol pp q chen x zhu z ling s wei and h jiang distraction based neural networks for modeling ments in proceedings of the international joint conference on articial intelligence aaai press pp r paulus c xiong and r socher a deep reinforced model for abstractive summarization arxiv preprint c lin rouge a package for automatic evaluation of summaries text summarization branches out j li x chen e hovy and d jurafsky visualizing and understanding neural models in nlp arxiv preprint m banko v o mittal and m j witbrock headline generation based on statistical translation in ings of the annual meeting on association for putational linguistics association for computational linguistics pp d z bonnie and b dorr bbn umd at ary in proceedings of the document ing conference duc at conference of the north american chapter of the association for computational linguistics human language technologies citeseer t cohn and m lapata sentence compression beyond word deletion in proceedings of the international conference on computational linguistics volume sociation for computational linguistics pp k woodsend y feng and m lapata generation with quasi synchronous grammar in proceedings of the conference on empirical methods in natural language processing association for computational linguistics pp r nallapati f zhai and b zhou summarunner a recurrent neural network based sequence model for extractive summarization of documents in proceedings of the aaai conference on articial intelligence pp m yasunaga r zhang k meelu a pareek k srinivasan and d radev graph based summarization arxiv preprint ral multi document s narayan r cardenas n papasarantopoulos s b cohen m lapata j yu and y chang document modeling with external attention for sentence extraction in proceedings of the annual meeting of the ciation for computational linguistics volume long papers vol pp a m rush s chopra and j weston a neural attention model for abstractive sentence summarization in proceedings of the conference on empirical methods in natural language processing pp s chopra m auli and a m rush abstractive sentence summarization with attentive recurrent neural networks in proceedings of the annual conference of the north american chapter of the association for computational linguistics human language gies pp o vinyals m fortunato and n jaitly pointer works in proceedings of the twenty ninth conference on neural information processing systems pp w hsu c lin m lee k min j tang and m sun a unied model for extractive and abstractive
