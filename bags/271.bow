learning syntactic dynamic selective encoding document summarization haiyang yahao kun han junwen chen xiangang labs didi chuxing ltd beijing china email xuhaiyangsnow heyahao kunhan chenjunwen com abstract text summarization aims generate headline short summary consisting major information source text recent studies employ sequence sequence framework encode input neural network generate abstractive summary studies feed encoder semantic word embedding ignore syntactic information text previous studies proposed selective gate control information encoder decoder static decoding differentiate information based decoder states paper propose novel neural architecture document summarization approach following contributions rst incorporate syntactic information constituency parsing trees encoding sequence learn semantic syntactic information document resulting accurate summary propose dynamic gate network select salient information based context decoder state essential document summarization proposed model evaluated cnn daily mail summarization datasets experimental results proposed approach outperforms baseline approaches index terms summarization parse tree dynamic selective gate syntactic attention introduction text summarization challenging task natural guage processing nlp information retrieval existing proaches text summarization categorized major types extractive abstractive extractive methods produce summaries extracting sentences tokens source text produce grammatically correct summaries preserve meaning original text approaches heavily rely text original documents extracted sentences contain redundant information lack readability contrary abstractive methods produce summaries generating new sentences tokens necessarily appear source text abstractive approaches difcult practice need address nlp problems including document understanding semantic representation natural language generation harder sentence extraction recent neural sequence sequence proach achieved tremendous success nlp tasks machine translation dialogue systems essence based text summarization methods encoder decoder framework rst encodes input sentence low dimensional representation fig example text summarization cnn dataset colored text source text corresponding maries generated human written original attention proposed approach respectively decodes abstract representation output sequence extension methods attention based models encode input sequence context vector attention mechanism dynamically calculate attentive probability distribution generation step similar machine translation researchers plied neural model abstractive text summarization signicant difference tasks machine translation aims capture semantic details source text text rization focuses salient text information critical utilize key information source text text furthermore original attention method learn syntactic structure source text important text summarization figure piece original text shown human written summary shown box target box shows model generated summary original attention method baseline green text target red text baseline summary corresponding blue text original text shown gure baseline model incorrectly summarizes says citizens mainly able capture internal syntactic structure original text applicants noun phase applied ceremony attributive clause address problems propose novel syntactic dynamic selective encoding method document tion incorporate structured linguistic information parsing trees learn effective sentence representation serializing parsing trees encoder sequence way encoding sequence contains semantic words syntactic parsing symbols information fed decoder summary generation addition document contains hundreds words hard directly encode key information source text selective gate proposed previous study lter secondary information salient information varies different decoding stage better select salient information based context decoder states decoding step dynamic selective gate network control salient mation according document information current decoder state previous gated encoder state way approach learn better representation sentences select salient information long input sequence summary generation example summary generated approach figure text correctly summarize original sentences document reference figure shows constituency parsing tree source sentence figure shows change states dynamic selective gates input sentence conduct experiments large scale cnn daily mail datasets experimental results model achieves superiority baseline tion models organize paper follows sec introduces related work sec iii describes proposed method sec present experiment settings illustrate experimental results conclude paper sec related work general broad approaches automatic text summarization extractive abstractive extractive methods work selecting important sentences passages original text reproducing summary contrast abstractive summarization techniques generate new shorter texts seldom consist identical tences document banko apply statistical models term selection term ordering processes produce short summaries bonnie dorr implement system combination linguistically motivated sentence compression technique notable methods abstractive summarization include discriminative tree tree transduction model quasi synchronous grammar approach utilizing context free parses dependency parses recently researchers started utilizing deep learning framework extractive abstractive summarization extractive methods nallapati use recurrent neural networks rnns read article representations sentences select important sentences yasunaga combine rnns graph convolutional networks cnns compute salience sentence narayan propose framework composed hierarchical encoder based cnns attention based extractor attention external information works published recently abstractive methods rush rstly apply modern neural networks text summarization local attention based model generate word conditioned input sentence bunch work proposed extend approach achieving improvements performance chopra use similar tional attention based encoder replace decoder conditional rnns nallapati apply encoder decoder rnns framework hierarchical attention feature rich embedding vector tan propose graph based attention mechanism summarize salient information document neural models emit unseen words vocabulary xed training time order solve problem point network copynet proposed allow copying words original text generating words xed vocabulary hsu combine strength extractive stractive summarization propose inconsistency loss zhou extend general encoder decoder framework selective gate network helps improve encoding effectiveness release burden decoder work signicant improvements comparing previous studies incorporate syntactic tion previous works use unstructured linguistic mation speech pos tags named entity work utilize structured syntactic parsing tree learn effective context vector improves performance word prediction alleviate repetition problem second choose salient information previous works employ selective gate network static decoding stage improve gate network let states gate dynamically adjust according context decoder states essential document summarization iii methodology section describe proposed model architecture syntactic dynamic selective syntactic encoding model shown figure consists syntactic sequence encoder dynamic selective gates pointer generator network syntactic attention decoder fig overall architecture proposed syntactic dynamic selective encoding model parsing tree sentence serialized fed encoder help attain syntactic meanings jth decoding stage decoder benets dynamic selective gate drop trivial words attention mechanism inuenced syntactic vector syntactic sequence encoder previous studies usually treat document sequence words ignore syntactic structure document leverage syntactic knowledge design syntactic sequence encoder learn document representations document denoted sequence sentences number sentences document sentence apply syntactic parser generate parsing tree adopt traversal serialize parsing tree sequence tokens ekl number tokens serialized parsing tree note token necessarily word parsing tree leaf node represents word non leaf represents parsing symbol including phrase label pos tag document concatenate serialized parsing trees long sequence total number tokens parsing trees model sequential information rst use embedding vector represent token word symbol parsing tree employ bidirectional long short term memory bilstm encoder sequence information denote hidden state forward lstm backward lstm respectively representation ith token concatenation hidden states directions model syntactic information apply hidden states corresponding pooling parsing symbols produce syntactic vector max set parsing symbols document shown figure syntactic sequence encoder takes serialized parsing tree input bilstm compute hidden states words parsing symbols input word hidden states computation hidden states parsing symbols max pooled generate syntactic vector dynamic selective gates discussed sec document summarization information source fed decoder important select salient information remove unnecessary information input propose novel dynamic selective gate model generation process salient information use parameterized gate network select useful information summary generation gate state takes input state source previous state generated target low dimensional representation document concatenation state forward lstm rst state backward lstm specically ith encoder step jth decoder step state dynamic selective gate calculated state jth step decoder lstm discussed subsection gated hidden state encoder denotes sigmoid function denotes element wise multiplication equals set vector trainable parameters note previous study utilized selective gate control information gate state depends hidden states source text selective gate static decoding stage proposed dynamic selective gate depends encoder decoder states suggesting gate open information useful current target output target outputs critical document summarization length document long static gate select irrelevant information source decoding step effectiveness dynamic selective gate sec pointer generator network syntactic attention coder use recent proposed pointer generator network decoding allows copying words original text pointers generating new words based source vocabulary handle oov problem specically attention strength ith source step jth target step calculated current decoder state current gated encoder hidden state document syntactic vector context vector calculated attention weighted summation gated encoding hidden states trainable parameters lstm takes input word embedding vector previous generated word previous context vector previous decoder hidden state compute new decoder state current context vector current decoder hidden state fed linear layers predicts probability word vocabulary softmax function pvoc trainable parameters pointer generator network produces switch probability pgen decide generates word pvoc copies word original source text pgen calculated decoder state context vector decoder word nal probability word calculated based pgen attention distribution pgen pgen trainable parameters model training train model use negative log likelihood function log loss document adopt coverage loss aiming handle repetition problem text summarization coverage loss decoding step corresponding encoding step summation attention distributions previous decoding step nal loss function decoding step log experiments section describe experiment details including datasets implementation details baselines results datasets conduct experiments cnn daily dataset comprises multi sentence summaries widely automatic text summarization use released obtain version data training pairs validation pairs test pairs source documents words spanning sentences average summaries consist words sentences dataset released versions anonymized version processed replace named entity original version consisting actual entity names work use original text requires pre processing challenging anonymized dataset replaces named entities unique identier vocabulary following experiments models trained tested different datasets separately including cnn corpus daily mail corpus combination cnn daily mail corpus table shows detail statistics information experiment datasets implementation experiments use words source ulary stanford constituency syntactic information sentences corpora includes phrase labels pos tags model takes dimensional hidden states dimensional word embedding vectors use adagrad learning rate initialize com abisee pointer generator com abisee cnn dailymail stanford edu software srparser html data set cnn daily cnn daily avgdocsents avgdocwords avgsumwords avgsumsents table data statistics cnn cnn daily mail datasets avgdocsents average sentences number original documents avgdocwords average sentences length original documents avgsumsents average sentences number summaries avgdocwords average sentences length summaries accumulator value found work best stochastic gradient descent adadelta momentum adam rmsprop addition set maximum length sentence source target training testing respectively decode fast better results set beam size experiments furthermore added coverage mechanism loss function coverage loss weighted baselines compare proposed model state art automatic text summarization systems techniques consisting extractive abstractive methods standard extractive baseline generates summary simply selecting leading tences source document utilizes encoder decoder framework learns representation source encoder classies sentences document decoder applies encoder decoder rnn tive framework hierarchical attention rich embedding vector treats extractive summarization sequence classication problem binary decision sentence included summary summarunner extractive model trained directly ilar summarunner abstractive summaries use framework based uni gru non hierarchical attention baseline model distraction extension model distract mechanism traverse ferent content document better grasp overall meaning summarization graph based model proposes novel abstractive graph based attention mechanism work aims salient content original document deeprl proposes unied framework combining improve quality summary pointer improves standard model hybrid pointer generator produce novel words copy words source text selectivegate proposes encoder decoder work based static selective gate network helps improve encoding effectiveness release burden decoder experimental results adopt widely pyrouge evaluation metric measures similarity output summary standard reference computing ping gram unigram bigram longest common subsequence following experiments adopt unigram bigram rouge longest common subsequence evaluation observed tables iii posed approach achieves best performance datasets best model outperforms baseline extractive abstractive models compared abstractive graph based based leverages structural summarunner model model information document improves pointer network syntactic attention copy relevant words semantic structural aspect original text handle oov problems graph based based summarunner anonymized data replaced model named entity alleviate oov problems thermore unlike graph based based summarunner model pretrain word embedding vectors method distraction graph based model syntax selective table comparison results cnn test set tively length variants rouge baseline model results mark taken corresponding papers rouge scores condence interval reported ofcial rouge script compare detail similar methods table pointer generator coverage model results baselines incomplete sub dataset researchers chose report results sub dataset python org pypi method summarunn summarunner graph based model deeprl pointer model table iii comparison results cnn daily mail test set length variants rouge baseline model results mark taken corresponding papers represents vocabulary size represents vocabulary size rouge scores condence interval reported ofcial rouge help structural information dynamic selective gate scores best model performs best model evaluation metrics rouge static selectivegate model conduct experiments original paper focusing short text summarization use mechanism alleviate oov word repetition problems result demonstrates static selectivegate improves performance model dynamic selectivegate improve rouge scores static selectivegate model selecting current important information decoding time step study different impacts source syntax dynamic selective gate performance proposed model conduct ablation experiments cnn dataset train model source syntax encoding dynamic selective gate respectively shown rows table source syntax encoding dynamic selective gate improve performance compared selectivegate approach combining approach leads improvement achieves best results shown row table table comparison results different document lengths cnn dataset respectively length variants rouge rouge scores condence interval reported ofcial rouge addition study impact lengths document performance proposed model conduct experiments cnn test sets table clearly shows performance proposed approach stable different lengths document example analysis subsection use example tiveness syntactic encoding dynamic selective mechanism choose example introduced figure figure shows constituency parsing tree sentence baseline model figure generates wrong summary cause able model syntactic structure applicants noun phase applied ceremony attributive clause approach bilstm encoder takes input serialized parsing tree word token surrounded parsing symbols intuitively consecutive words belong syntactic subtree parsing symbols inserted bilstm encoder likely symbols strong connection shown figure generated summary model correctly conveys summary source text fig parsing tree corresponding blue text figure dashed box shows applied ceremony attributive clause applicants upper box shows generated summary correctly summarize original document dynamic selective gate use method visualize method denes highly non linear function measure contribution source word gated jth generation step shown figure dynamic selective mechanism select important information original document decoding step example decoding step selective gate lters nonsensical words selects current important words jedlicka politician help following attention generate important word jedlicka furthermore figure shows word source vocabulary generated vit jedlicka weight selected words decrease decoding steps indicating model address oov problem word repetition problem fig visualization dynamic selective gates gates dynamically adjust states different decoding steps darkness blocks indicates openness gates conclusions future work work propose novel document summarization model takes input serialized parsing tree enables encoder learn inherent syntactic structure sentence document pose dynamic selective gate control information encoder decoder mechanism dynamically control salient information based context decoder state essential document summarization experimental results long text datasets cnn daily mail advantage model baseline approaches work use traversal generate serialized parsing tree able contain structure information tree consider better tion model hierarchical structure parsing tree hand proposed dynamic selective gate apply word tokens source consider integrating word tokens parsing symbol produce better information source target references sutskever vinyals sequence sequence learning neural networks proceedings conference neural information processing systems bahdanau cho bengio neural machine translation jointly learning align translate arxiv preprint luong pham manning effective proaches attention based neural machine translation proceedings conference empirical methods natural language processing serban sordoni bengio courville pineau building end end dialogue systems ing generative hierarchical neural network models proceedings aaai conference articial intelligence aaai press bordes boureau weston ing end end goal oriented dialog arxiv preprint incorporating copying mechanism sequence sequence learning arxiv preprint nallapati zhou dos santos gulcehre xiang abstractive text summarization sequence sequence rnns proceedings signll conference computational ural language learning liu manning point summarization pointer generator networks ceedings annual meeting association computational linguistics volume long papers vol tan wan xiao abstractive document summarization graph based attentional neural model proceedings annual meeting association computational linguistics volume long papers vol xiong zhu zhang zhou modeling source syntax neural machine translation proceedings annual meeting ciation computational linguistics volume long papers vol zhou yang wei zhou selective encoding abstractive sentence summarization proceedings annual meeting ciation computational linguistics volume long papers vol wong extractive rization supervised semi supervised learning proceedings international conference computational linguistics volume association computational linguistics wang zhu gong multi document summarization sentence based topic models proceedings annual meeting ciation computational linguistics association computational linguistics celikyilmaz hakkani tur hybrid archical model multi document summarization proceedings annual meeting ation computational linguistics association computational linguistics alguliev aliguliyev hajirahimova mehdiyev mcmr maximum coverage minimum redundant text summarization model expert systems applications vol erkan radev lexrank graph based cal centrality salience text summarization journal articial intelligence research vol alguliev aliguliyev isazade multiple documents summarization based ary optimization algorithm expert systems cations vol summarization inconsistency loss proceedings annual meeting association putational linguistics volume long papers graves schmidhuber framewise phoneme classication bidirectional lstm neural network architectures neural networks vol cheng lapata neural summarization tracting sentences words proceedings annual meeting association computational linguistics volume long papers vol chen zhu ling wei jiang distraction based neural networks modeling ments proceedings international joint conference articial intelligence aaai press paulus xiong socher deep reinforced model abstractive summarization arxiv preprint lin rouge package automatic evaluation summaries text summarization branches chen hovy jurafsky visualizing understanding neural models nlp arxiv preprint banko mittal witbrock headline generation based statistical translation ings annual meeting association putational linguistics association computational linguistics bonnie dorr bbn umd ary proceedings document ing conference duc conference north american chapter association computational linguistics human language technologies citeseer cohn lapata sentence compression word deletion proceedings international conference computational linguistics volume sociation computational linguistics woodsend feng lapata generation quasi synchronous grammar proceedings conference empirical methods natural language processing association computational linguistics nallapati zhai zhou summarunner recurrent neural network based sequence model extractive summarization documents proceedings aaai conference articial intelligence yasunaga zhang meelu pareek srinivasan radev graph based summarization arxiv preprint ral multi document narayan cardenas papasarantopoulos cohen lapata chang document modeling external attention sentence extraction proceedings annual meeting ciation computational linguistics volume long papers vol rush chopra weston neural attention model abstractive sentence summarization proceedings conference empirical methods natural language processing chopra auli rush abstractive sentence summarization attentive recurrent neural networks proceedings annual conference north american chapter association computational linguistics human language gies vinyals fortunato jaitly pointer works proceedings ninth conference neural information processing systems hsu lin lee min tang sun unied model extractive abstractive
