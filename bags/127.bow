query based abstractive summarization neural networks johan hasselqvist ist niklas helmertz mikael kgebck department computer science engineering chalmers university technology abstract paper present model erating summaries text documents respect query known based summarization adapt isting dataset news article summaries task train pointer generator model dataset ated summaries evaluated ing similarity reference summaries results neural network marization model similar existing ral network models abstractive rization constructed use queries produce targeted summaries introduction creating short summaries documents spect query applications example search engines help inform users relevant results ing summary automatically difcult paper problem fully solved neural network model task presented specically model designed brief commonly single sentence summaries tion useful user performed search search engine set documents returned concise maries displayed search results giving quick overview document related search query commonly search engines today text surrounding occurrence search query document displayed summary example extractive summarization produces summary contains parts original document signicant difference model present generates abstractive summary type summary lows rephrasing words ily present original document comparable human written summary tial summarizing documents concise way possible extractive mary making easier reader stand relationship document query automatic text summarization search topic years general goal concisely represent important formation documents previous work summarization extractive methods nenkova mckeown mogren commonly individual sentences tracted composed form summary gives sentences grammatically rect source document inherently limited reproduce written summaries general abstractive marization particular closely related ural language generation able reach human level performance require ing summaries level understanding context documents produce results comparable human written ones important progress neural work models generating text sequence sequence sutskever machine translation way mapping varying length input text varying length text applicable machine tion summarization recent years progress neural network models text summarization similar lems examples sequence sequence models non query based abstractive rization rush nallapati neural network models additionally generating image captions thy fei fei form mary question answering problems hermann tan inspired progress designed model query based summarization neural works main contributions work includes model query based abstractive rization presented section dataset query based abstractive summarization ated adapting existing dataset originally question answering described section quantitative evaluation formance proposed model compared extractive baseline uninformed abstractive model presented section qualitative analysis generated summaries related work early work evaluating methods tractive query based summarization presented goldstein queries use short queries average words similar length types queries experiments sis work work otterbacher recent work query based summarization wang parse trees sentence compression described pure extractive summarization later stages thesis work nema propose neural network model query based abstractive summarization ilarities model present dataset use smaller average ument length number documents tionally types queries different use complete questions opposed single entity queries task question answering produce answer question posed natural language task general problems expressed question answering lem summarizing respect query instance expressed summary document respect query query answer question gle complete sentence especially close types query based summaries considered thesis otterbacher present model biased lexrank use form question answering extractive query based summarization answers generate sentences makes similar task query based summarization mann present neural network els question answering training create large dataset cnn daily mail news articles adapt dataset based summarization detailed chapter kumar introduce dynamic memory networks reached state art performance variety nlp tasks draw inspiration use question ule incorporate query information model general abstractive summarization differs query based summarization document summarized respect query pati build machine tion model bahdanau erate general abstractive summaries multiple datasets including cnn daily mail dataset hermann additions model include pointer generator anism glehre allows model copy words source document propose similar model lar pointer generator mechanism outperforms nallapati slightly different sion cnn daily mail dataset making result strictly comparable porate coverage avoiding titions output background following sections terms cepts paper explained named entity recognition information extraction class tasks volve extracting structured information uments example task named entity recognition classication parts text different categories persons locations category example sentence mathematician jeff paris visited city paris jeff paris tated person paris location gated recurrent units gated recurrent unit gru type current neural network rnn designed alleviate vanishing exploding gradient lem hochreiter bengio hinders original rnn capturing long term dependencies gru similar lar long short term memory lstm model simpler computationally intensive achieving comparable results tasks chung kumar tire gru architecture described mulas vectors input time step output scaling vectors tended regulate information let described gates ments vector intended carry data elements generated network tanh activation function denote entire gru update step word embeddings given vocabulary encode word uniquely hot encoding gives vector length word cabulary mapped uniquely dimension value dimensions vector transformed ding word multiplying ding matrix wemb dimensionality demb demb word embedding ity commonly hyperparameter neural network models intention embeddings ture characteristics words giving useful vector representations instance related words football soccer pected close vector space methods generating word embeddings mikolov glove attention problems found cial use rnn states nal xed size hidden state attention mechanism allowing model access information decoding process letting identify vant parts input use encoder hidden state locations technique successfully machine translation danau image captioning model propose sequence sequence model attention pointer mechanism making pointer generator model input problem document query sequences words passed document coder query encoder respectively coders outputs passed attentive decoder generates summary coders decoder use rnns grus occurrence gru subscript formulas following sections arate weights biases entire model picted figure different components variables gure explained detail section document encoder document encoder processes input ment generating state input word representation context word use bidirectional rnn schuster paliwal encoder context ter contribute representation bahdanau ing good results similar task related text comprehension combined rnn hidden state time step intermediate states forward reader backward reader tively computed gru doc gru doc vocabulary word word reversed input document input word embedding initial states zero vectors concatenation combined state twice dimensionality state directional encoder document encoder state figure overview model illustrates connections parts model xed decoder time step containing labeled boxes correspond different rnns intended visualize ways output word selected pointer generator mechanism left right respectively dimensionality denoted ddoc word bedding dimensionality demb query encoder query encoder responsible creating xed size internal representation input query unlike document encoder query encoder unidirectional rnn encoder queries relatively short compared ments use nal state resent query rnn state updated according query word input query length query initial state zero vector query encoder state dimensionality denoted dque decoder decoder unidirectional rnn ing summary input document ing nal state input encoder utilizes soft attention combination query pointer mechanism generator similar bahdanau query embedding fed input decoder time step similar answering module question answering model presented kumar use rnn encoded question representation input decoder time step model rnn state updated ing hnd nal document encoder state number input words corresponds special token initial time step previous word predicted context vector time step attention mechanism dened subsequently predicted output word time step generator nism pointer mechanism dened sequently word embeddings encoder intention inclusion input grudec decoder ability tune structure output sequence eventually output concerning query ample query location decoder output words leading appropriate sion location generator outputs word subset vocabulary vgen time step selection output words distribution words vgen computed softmax pgen index uniquely mapped word vgen ztj dened subsequently dening probability gen word ygen arg max wvgen highest probability ygen select output softmax probability gen gen gen pends ztj output linear mations decoder state context vector gen dened gen gen gen gen rdgen gen trainable hyperparameters dgen ality hidden layer main function layer reduce dimensionality input reducing computation time nal layer size model soft attention mechanism based bahdanau machine translation result attention mechanism context vector produced time step computed tihi eti document encoder hidden state index score function dened att batt watt weight matrix vatt rdatt vector batt bias vector trained rest network query included model focus attention query words appropriate pointer mechanism general issue generator mechanism limited frequent words infrequent words generated model needs learn output names different ones occurrences training data training model generate correctly problematic way solve issues allow model directly copy word document output summary point additionally viewed input text secondary output vocabulary addition vgen pointer mechanism adds switch pptr decoder time step model computed output linear mation fed sigmoid activation function pptr bptr vptr bptr vectors trained rest work pptr word copied input generator output copied input tth decoder word determined attention distribution cally time step select word index arg max document tention highest yptr word dened nal output yptr ygen pptr training loss log pptr model trained use pointer mechanism supervised manner additional training input xptr ther pointer mechanism set tth word summary training dene loss function lptr pptr xptr training generator mechanism loss generator softmax layer lgen length target summary vgen tth word target summary plying xptr excludes addition loss pointer mechanism set introduce form supervised attention pointer mechanism set output word introducing loss function latt log dex input document point log gen xptr xptr nal loss function sum different losses normalized length computed lgen latt lptr generating summaries summaries considered complete cial eos token generated maximum output length reached potential summaries explored beam search time steps pointer mechanism partial summaries prioritized probabilities generator stead partial summaries different abilities created word chosen table highlights cnn article titled airline quality report sorts duds dynamos hawaiian airlines lands time performance airline quality rankings report looks largest airlines expressjet american airlines worst time performance virgin america best baggage handling southwest lowest complaint rate table statistics dataset val training doc doc query pairs doc query sum avg words doc avg words query avg words sum test pointer mechanism difcult justify hope reasonable bility time steps pointer mechanism preventing summaries pointer mechanism prioritized slight deviation presented section pointer mechanism attended word output unk preted model actual word converted index vocabulary viewed processing step dataset dataset constructed paper based hermann consist ment query answer triples cnn daily mail news articles included published news article number human written highlights summarize different aspects article table shows example lights single article construct ument query answer considering named tity highlight unknown making light cloze style question taylor answer entity unknown example document cloze style question answer seen table propose cnn daily mail dataset query based abstractive summarization light summary document entities highlight queries occurrence entity highlight construct query summary triple query based tion table shows sample document cloze style question compared sponding query summary pair constructed entity mentioned multiple highlights consider multiple target references document query pair contrast hermann translate entities tiers use minimal preprocessing form tokenization lowercasing mix articles dnn daily mail mann keeps separate cided train model mix cnn daily mail articles proportion ing reserved validation test sets articles included validation test set determined randomly equal probability article statistics resulting dataset seen table dataset reproduced script available experiments experiments conducted rst measure model uses information query section second compares model extractive baseline section beam width maximum output length query dependence determine incorporating query ets model compare proposed model query corrupted instead evaluating generated summary document query reference maries query evaluate reference summaries query query offset query highest reference summaries rst query idea score lower normal evaluation model use additional information query com querysum data table reference summaries normal evaluation compared offset queries query normal offset queries ble shows example document generated summaries evaluated query dependence evaluation worth mention reference summaries ent queries original highlight reference summary multiple queries cases query appropriate summary model beneted query offset evaluation extractive baseline baseline compare results simple extractive summary designed specically dataset thesis work baseline mary constructed selecting rst sentence document containing query stricting length document sentence found document tain query rst sentence document instead occur dataset frequently additionally observe average length baseline sentences cnn daily mail dataset commonly greater reference summaries average number words baseline summaries reference summaries possible gain higher rouge score fewer number words query occurrence selected form complete sentence evaluation metric results evaluated different rics provided rouge recall oriented derstudy gisting evaluation lin defacto standard evaluation method automatic summarization rouge scores grams grams tively rouge rouge complex metrics detailed lin training details vocabulary input text contains frequent words training set generator vocabulary vgen consist frequent words smaller lary generator pointer nism word embeddings vocabulary words initialized dimensional glove trained wikipedia gigaword word glove ding initialize word embedding pling dimension univariate normal butions means standard deviations entire collection glove embeddings training test time limit document length rst words reduce computation time loss minimized sgd based adam optimizer kingma mini batches samples averaged loss samples batch mini batches remained epochs order trained randomized tween epoch experiments run single nvidia tesla memory took hours train model implemented tensorflow abadi complete source available hyperparameters experiments reported table extensive rameter tuning performed instead examined hyperparameters similar els nallapati table hyperparameter conguration hyperparameter word embedding size document encoder size query encoder size decoder size attention hidden size generator hidden size value demb ddoc dque ddec datt dgen glove zip stanford edu projects com helmertz querysum table example document query pair query netix table rouge scores evaluated models model query sentence model offset queries document cnn united states named germany captain jurgen klinsmann new national coach day sacking bob bradley bradley took coach january relieved duties thursday soccer federation president sunil gulati conrmed statement friday replacement appointed query united states reference jurgen klinsmann named coach united states national output klinsmann appointed new coach united states results results experiments summarised table result query dence evaluation offset queries described section rouge scores goes statistical signicance according rouge reported condence intervals queries offset indicates model benets information provided queries observe model score lower baseline model denote rst query sentence described section noted baseline expected strong given nature dataset analysis observe attention time step appears highly focused words document example output summary seen table figure shows attention distribution time erated summary observation attention focused beginning documents certainly stances entities selected far documents bias partly decision point rst occurrences tities noted goldstein beginning news articles table example document query pair document president barack obama sided internet activists monday urging federal nications commission draft new rules classify broadband net regulate like lic utility end result tie hands internet service providers want cut special deals vices like netix youtube hulu amazon push streaming content fast lane ordinary icans access reference obama vision bar providers like izon comcast cutting deals hulu netix amazon streaming content delivered online fast lanes output obama chief executive netix refused allow users access service table example document query pair document february breakthrough belarus verdict italy expected veto headline cnn student news friday query cnn student news roll reference page comment chance mentioned cnn student news teacher student age older request tion cnn student news roll output page comment chance mentioned cnn student news teacher student age older summarizes article examining output summaries model strongly match topic input documents rarely succeed generating summaries ing actually stated article table shows example output fairly ically correct truthful respect article observe model manages learn dataset samples actual summaries described section notices repeated articles generated mary shown table example terestingly model manages literally repeat reference summary maximum output length limit frequently repetitions phrases extreme example seen figure model appears stuck ing begin summary additionally observe repetition observed attention distribution problem seen nallapati tion temporal attention sankaran model alleviating issue repetitions propose coverage solve issue running experiments suspected difcult pointer mechanism sequentially point words longer entities fully example summary certainty selecting sequence entity words seen figure compared reference summaries generally shorter average number words output summaries dataset average noted beam search commonly favors shorter summaries propose addition length normalization reducing tendency menting measure improve results model comparison nallapati rouge scores low use different version dataset lights combined form single sentence summary similar models results general summarization task train model output summary document completely ent target summaries different queries queries smaller conclusion designed model query based stractive summarization evaluated adapted dataset redesigned query based summarization overall performance model outperform tive baseline shown rate query utilize information create focused summaries references martn abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat ian goodfellow andrew harp geoffrey irving michael isard yangqing jia rafal icz lukasz kaiser manjunath kudlur josh enberg dan man rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal war paul tucker vincent vanhoucke vijay van fernanda vigas oriol vinyals pete warden martin wattenberg martin wicke yuan aoqiang zheng tensorflow large scale chine learning heterogeneous systems software available tensorflow org dzmitry bahdanau kyunghyun cho yoshua gio neural machine translation jointly international learning align translate ference learning representations iclr yoshua bengio patrice simard paolo frasconi learning long term dependencies ent descent difcult ieee transactions neural networks junyoung chung aglar glehre kyunghyun cho yoshua bengio empirical evaluation gated recurrent neural networks sequence ing arxiv prints jade goldstein mark kantrowitz vibhu mittal jaime carbonell summarizing text ments sentence selection evaluation metrics proceedings annual international acm sigir conference research opment information retrieval acm new york usa sigir pages aglar glehre sungjin ahn ramesh nallapati bowen zhou yoshua bengio pointing proceedings unknown words annual meeting association tional linguistics volume long papers ciation computational linguistics berlin pages karl moritz hermann toms kocisk edward grefenstette lasse espeholt kay mustafa leyman phil blunsom teaching chines read comprehend advances ral information processing systems pages sepp hochreiter untersuchungen chen neuronalen netzen thesis diploma sis institut informatik lehrstuhl prof brauer technische universitt mnchen andrej karpathy fei fei deep semantic alignments generating image tions ieee conference computer vision pattern recognition cvpr diederik kingma jimmy adam international method stochastic optimization conference learning representations iclr ankit kumar ozan irsoy peter ondruska mohit iyyer james bradbury ishaan gulrajani victor zhong romain paulus richard socher ask dynamic memory networks natural language processing maria florina kilian weinberger editors proceedings international conference machine learning pmlr new york new york usa ume proceedings machine learning search pages chin yew lin rouge package matic evaluation summaries stan szpakowicz marie francine moens editor text summarization branches proceedings shop association computational linguistics barcelona spain pages tomas mikolov ilya sutskever kai chen greg rado jeffrey dean distributed tations words phrases ality proceedings international ference neural information processing systems curran associates inc usa pages olof mogren mikael kgebck devdatt hashi extractive summarization ing multiple similarities ranlp pages ramesh nallapati bowen zhou ccero nogueira dos santos aglar glehre bing xiang abstractive text summarization sequence proceedings sequence rnns signll conference computational natural language learning conll berlin germany august pages preksha nema mitesh khapra anirban laha balaraman ravindran diversity driven tention model query based abstractive rization arxiv prints ani nenkova kathleen mckeown vey text summarization techniques springer boston pages jahna otterbacher gunes erkan dragomir radev biased lexrank passage retrieval ing random walks question based priors mation processing management jeffrey pennington richard socher pher manning glove global vectors word representation empirical methods ural language processing emnlp pages alexander rush sumit chopra jason weston neural attention model abstractive proceedings tence summarization conference empirical methods natural guage processing association computational linguistics lisbon portugal pages baskaran sankaran haitao yaser onaizan abe ittycheriah temporal attention model arxiv prints neural machine translation mike schuster kuldip paliwal tional recurrent neural networks ieee transactions signal processing abigail peter liu christopher ning point summarization pointer generator networks arxiv prints ilya sutskever oriol vinyals quoc sequence sequence learning neural proceedings international works conference neural information processing tems mit press cambridge usa pages ming tan bing xiang bowen zhou based deep learning models non factoid answer selection arxiv prints wilson taylor cloze procedure new tool measuring readability journalism bulletin wang hema raghavan vittorio castelli radu rian claire cardie sentence pression based framework query focused document summarization acl yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan cao qin gao klaus macherey jeff klingner apurva shah melvin johnson xiaobing liu lukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wei wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes jeffrey dean google neural machine translation system bridging gap human machine translation arxiv prints kelvin jimmy ryan kiros kyunghyun cho aaron courville ruslan salakhudinov rich zemel yoshua bengio attend tell neural image caption generation visual international conference machine tention learning pages supplemental material dataset example record dataset shown table organize dataset triples hierarchically document query reference documents queries numbered numerically starting references numbered alphabetically starting document queries reference summaries order shufed document query reference ids matching format expected pyrouge attention visualisations figure visualization attention distribution summary table generated words document shown horizontal axis left right limited number document words shown vertical axis shows output words token darker cell higher attention position figure visualization attention distribution output summary document query pair generated query australia format figure figure visualization attention distribution output summary document query pair test set generated query fools horses format figure ellipsis signies parts attention distribution skipped justadayaftersackingbobbradley bradley klinsmannappointedasthenewcoachofunitedstatesasanychildoftheeightieswilltellyou neonisastapleofanyafter darkcelebration buttwofoodiesfrommelbournehavetakentheirloveofitonestepfurthertocreateasweettreatthatlightsupthenight stevefelice stevefelice glennstorey stevefelice glennstorey stevefelice andifyouheardhimspeak hiscockneyaccentwouldbeasbroadastheriverthames thisisderekhockley thewheeler dealerwhodavidjasonhasrevealedhetookashisinspirationwhenplayingdelboyinonlyfoolsandhorses andifyouheardhimspeak hiscockneyaccentwouldbeasbro hewasinspiredbyarealeastendofonlyfoolsandhorses table example dataset samples ated document query pair method compared hermann style questions entity corresponding swer replaced document cnn vice president walter mondale released mayo clinic saturday admitted inuenza hospital spokeswoman kelley luckstein said treated cold symptoms released today said mondale diagnosed went hospital routine checkup following fever president jimmy carter said friday bed right moment looking forward come home carter said speech nobel peace prize forum minneapolis said tell everybody mondale underwent treatment mayo clinic rochester minnesota vice dent served carter later ran president lost ronald reagan history naming woman rep geraldine ferraro new york running mate lawyer senator minnesota wife joan mondale died year highlight walter mondale released mayo clinic day hospital spokeswoman said cloze style question walter mondale released saturday pital spokeswoman said cloze style answer mayo clinic query mayo clinic target summary walter mondale released mayo clinic day hospital spokeswoman said
