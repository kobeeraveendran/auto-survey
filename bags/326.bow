extracting summary knowledge graphs long documents zeqiu wu rik koncel kedziorski mari ostendorf hannaneh hajishirzi university washington seattle wa usa kedzior ostendor edu p e s l c s c v v x r abstract knowledge graphs capture entities relations long documents facilitate reasoning downstream applications extracting compact knowledge graphs ing salient entities relations important lenging understanding summarizing long documents introduce new text graph task predicting rized knowledge graphs long documents develop dataset document graph pairs automatic human annotations develop strong baselines task based graph learning text summarization provide quantitative qualitative studies effect introduction knowledge graphs popular representations tant entities relationships compact interpretable knowledge graphs facilitate human data analysis empower memory dependent knowledge based tions makes ideal modeling content documents document level information extraction captures relations distant sentences construct knowledge graphs documents jia wong poon yao et al techniques focus tracting entities relations document long dense documents scientic papers hundreds thousands poses new challenge determine important entities paper key relationships automatic summarization liu lapata sunaga et al addresses problem identifying salient information document introduces tional challenge discourse structuring tive case text generation summarizing entities relations directly rst step decouple mixed burdens models help assure factual correctness summary line recent trends evaluation summarization wang cho lewis durmus diab zhang et al work introduce task extracting scientic document compact knowledge graph sents important information figure illustrates situation large knowledge graph extracted document portion entities relations terize main ideas colored nodes thick edges figure introduce task extract summary edge graph long document e scientic paper example dataset target mary graph contain entities relations salient included abstract entities relations shown light grey found abstract removed omit entity tion types simplicity rest play minor role task emphasizes ing salient subgraph support task dataset scientic document graph pairs integrates matic human annotations existing knowledge sources scientic domain outline evaluation paradigm balances accuracy redundancy admitting variability textual reference entity develop investigate competitive baselines based text summarization graph learning models compare simple frequency based methods vide analysis tradeoffs general lenges posed proposed dataset example serve missing entities entity coreference errors predicted graphs large impact relation racy hope task data facilitate search future models better capture lenging important textual relations abstract tree adjoining grammars extended domain locality wide coverage lexicalized grammars english lexsys xtag exploit edol main papertree adjoining grammarsedolxtaglexsyslexicalized grammars english background related work information extraction ie ie work focuses extracting entities relational facts single sentence zhang et al stanovsky et al recent work addresses document level ie aims capture relations distant sentences jain et al jia wong poon yao et al ll pre dened metadata table entities hou et al jia wong poon yao et al hou et al formulate task classifying relation type pair truth entities expressed document assume existence ground truth entities extract level relations directly text work complements trend applying ie long document understanding addressing need focused compact knowledge representations closest work jain et al separately explore idea identifying salient entities related experimental sults weak supervision provided papers code dataset contrast focus identifying salient entities paper based weak supervision abstract framing generalizes wider variety uments domains supports diverse tasks including multi document summarization building scientic edge bases entity salience task requires models identify salience relations challenging text summarization document summarization models create summaries tifying important sentences documents lapati zhai zhou narayan cohen lapata decoder generate abstractive summaries rush chopra weston celikyilmaz et al text summarization tasks liu lapata yasunaga et al share objective distilling crucial information documents mix objective goal producing uent natural language text argue summarizing entities relations directly rst step decouple mixed burdens els help models check factual correctness summary advantages benet text tion tasks rely long document understanding resentation generation grounded long text increasing number recent works wang cho lewis durmus diab zhang et al proposed automatically evaluating summarization models applying information extraction question answering models match entities relations generated reference summaries newly proposed measures found higher correlation human ments standard measures recent applications large pretrained language models ribeiro et al kale promise generating uent accurate text edge graphs highlighting need identifying correct code paperswithcode com underlying knowledge representations addition rized knowledge graphs multiple documents naturally merged collapsing shared entity nodes bring richer information summarized structures easily leveraged facilitate downstream tasks line work closely related graph based summarization leverages graph tures documents facilitate summary generation erkan radev tan wan xiao sunaga et al huang wu wang xu et al works try leverage graphs capture lations sentences discourse units wang et al incorporate graphs entities extracted sentence level ie systems considering entity lation salience graph summarization general graph summarization work categorized according goal optimizing memory computational resources needed processing ing analysis liu et al knowledge graph tion safavi et al node estimation park et al relevant work normally huge knowledge graph pruning based given query document context unlike works task quires model handle different document contexts inference document contain completely different knowledge graph unique entities work similar falke gurevych collect small corpus data instances concept map annotations openie tuples summarizing sets documents choose science specic annotation scheme provides structure target scientic document understanding proposed idea summarized knowledge graphs applied documents domain focus scientic papers work existing works derstanding scientic documents include limited information extraction luan et al wadden et al summarization cohan et al collins stein riedel yasunaga et al fact tion wadden et al citation generation luu et al xing fan wan recently attention search scientic domain nlp community grown wang et al esteva et al urgent need mitigating global pandemic task formulation introduce text graph task extracting succinct structured knowledge graph contains salient entities relations document specically summarized knowledge graph meet ditions contains important entities document includes relations entities crucial understanding main ideas text salient entity represented single node graph conditions evaluated entity salience relation salience entity duplication rate respectively evaluation details section dataset section long documents scientic papers salient entities relations dened included paper abstracts figure shows example task entities relations appear abstract included summary knowledge graph grey moved mentioned paper necessary describe understand main idea paper formally dene problem given document d pre dened entity type set tv relation type set tr predict summarized knowledge graph g v e vi v represents salient entity entity type ti tv mentioned d vi vj rk ij e represents important edge vi vj relation type rk ij tr note multiple edges vi vj ij rl rk ij l vi consists cluster ni string names co referent entity mentions mni dataset scigraphsumm construct text graph dataset corpus tic papers textual data consists roughly puter science research papers taken corpus million text research papers abstracts lo et al leverage abstracts create summarized edge graphs papers abstracts effectively contain summarized information documents existing human annotation information extraction ie systems enable constructing relational graphs stracts expense difculty annotation access small number human annotated summary graphs use judge system performance data human test set training development weakly supervised approach automatically extracted summary graphs use scientic ie system extract summary graphs abstracts pair papers entity relation graphs extracted papers divide graph paper ples train dev automatic test sets dev set parameter tuning purposes auto test set compare systems randomly sampling examples observe extracted target entities abstracts automatic test set ful section similar tem performance trends observed human test set automatic test set table gives statistics number size textual documents summary graphs data splits data collection graph construction details described sections follow automatic test set larger human test set reduces problems noise automatic annotation examples doc tokens graph entities graph relations train dev auto test human test table statistics data split manual summarized graph annotation leverage scierc scientic ie luan et al human labeled test data scierc consists expert annotated paper abstracts labeled entities types task method metric material scientic term generic co reference relations types pare conjunction evaluate feature hyponym construct knowledge graphs annotations collapsing coreferent mentions single node linking nodes annotated relations abstracts scierc texts able order guarantee information richness discard pairs annotated graphs fewer predicted relations ltering human test set consists knowledge graphs text documents automatic summarized graph annotation facilitate model training model development weakly supervised setting automatically create target knowledge graphs remaining papers stracts leverage state art scientic ie system extracts entities relations co references simultaneously wadden et al train instead use pretrained model scierc processing modeling steps work construct knowledge graphs ie output collapsing coreferences create entities associate list coreferential text mentions ie relations tions edges corresponding entities graph sample ltering applied automatic graph construction task designed build summarized graph directly document order perform task models use specic information extraction tools build graph provide edge graphs constructed documents dataset reproducibility encourage future ploration graph learning models task process document text overlapping token windows reduce computation memory consecutive chunks having overlapped sentence serve cross sentence co references collapse additionally discard pairs abstracts longer tokens rare avoid memory limitations discard sentences longer tokens guarantee overlapped sentence consecutive chunks fewer sentences discarded tial mentions previous steps collapse ence clusters different windows matching unique non generic mentions single graph node generic entity mention string excluding pronouns determiners token unigram inverse document frequency idf training data higher empirically chosen threshold selected high precision identifying generic mentions generic entity mention clustered model predicts coreferent entity mention evaluation metrics goal evaluate correctness predicted mary knowledge graph compared ground truth mary graph rst align entity nodes predicted graph nodes target graph entity ment step measure qualities predicted graphs entity salience relation salience duplication rate der relaxed alignment condition described entity alignment human test set found annotated entity tions exact string match main paper text analysis showed cases paraphrasing hyphenation differences typos caused ocr parsers process papers pdf format example domain monolingual pus abstract domain monolingual corpus paper equivalent exact match hyphen difference addition assume specic information extraction tools similar sue exact matching occur potentially different entity mention names extracted different models exact string match yield good alignment instead use relaxed alignment method found reasonably accurate evaluation issue aligning entities graphs entity referred multiple strings entity node represents cluster co referent tity mentions align predicted node target node cluster mention types nd maximum similarity possible pairs similarity score target entity vi j mnj predicted entity vj mni j calculated mt j j max s t employ gestalt pattern matching ratcliff metzener calculate string similarity based mon substrings predicted node aligned node gives highest similarity score subject minimum score selected set relaxed exact alignments manually inspected acceptable manually inspected samples fall ing categories paraphrases target nodes consider laxed alignment examples good differences involve typo hyphen item order paraphrases example log linear linear interpolation versus linear log linear interpolation different specicity level consider aligned entities different specicity level relevant ments example speaker s intention prediction ules versus intention prediction modules alignments error consider entities ing aligned distinct meanings bad ments example dimensional analog sorting versus dimensional notion sorting human auto test sets applying relaxed alignment graph nodes target nodes increases percentage aligned target nodes exact matching salience duplication measures calculate entity salience align predicted node target graph node collapsing multiple dicted nodes map target entity single node calculating precision recall scores words multiple predicted entities aligned target entity counted calculating scores metrics computed matching ignoring entity types typed vs untyped evaluation entity entity type adopt dominating type mentions entity type process penalize predicted graphs multiple nodes aligned single target node calculate duplication rate average number dicted nodes aligned target node based entity alignment target relation edge vi vj aligned predicted relation vk vl sponding nodes align e vi aligns vk vj aligns vl evaluate relation salience based relation ments allowing multiple relation types pair entities report precision recall scores relation prediction considering relation type direction matching typed vs untyped evaluation evaluating relation type direction merge relations multiple entity pair single edge baseline models develop baseline models graph tion problem text summarization model tracts summary sentences extract entities relations rst builds document graph applies graph learning model graph pruning text text graph ttg model rst produces text summary ment text extractive summarizer bertsumext liu lapata subsequently uses entities tions text summary form summary knowledge graph train original model dataset placing pre trained bert scibert beltagy lo cohan increase sequence length entities relations pear text summary summarized graph graph graph g model predicts summary subgraph graph extracted described section formulate subgraph selection node classication problem encode graph gat velickovic et al use resulting node representations binary salience prediction original gat node vi embedded able feature vector contextualized multi headed tention graph neighbors n vi graph tion layer graph attention layer vertex vi neighborhood n vi contextualized vi vi ijwv vj jn vi ij zn vi vi vi rh contextualized original vector representations vi wv wk wq rhh model parameters ij attention weights computed vertex representations formulation extended multi head attention layered non linearities produce graph attention network original gat consider different tion types neighboring vertices incorporate lation types model use separate heads ferent relation types tr head corresponding relation type r tr attend vi vj vi vj connected edge label r e vi vj r e different relation types dataset use heads gat sentations heads concatenated transformed non linearity model layers node embedding layer use features bed entity node vi number mentions ument ni section d entity s rst appearance document frequent entity type mentions predicted ti pooled representation scibert beltagy lo cohan longest mention string zi encode node follows vi nin wssi wtti wes zi n rh learnable unit feature vector ni rns ti rnt hot vectors si ti respectively ns nt number unique tion ids node entity types dataset s zi rhe hidden representation rst token nal layer scibert ws rhns wt rhnt rhhe trained model parameters following node embedding layer contextualize node representation gat layers pass node nal binary classication layer predict salience supervise training model apply laxed alignment method section align graph entities target graph entities graph entities aligned treated positive examples ers negative finally use negative log likelihood loss function positive labeled salient nodes negative sampling ratio training include graph relations predicted tities output summary graph leave better relation prediction future study implementaion details manually tune rameters gat based dev set entity performance curve versus training steps x model rameters e vector dimension number layers batch size parameters tuned learning rate dropout rate negative sampling ratio manually change parameter value serve performance instability dev set rst training steps average number tuning trials parameter fewer times finally set negative pling ratio dropout rate learning rate experiments g model use adam timizer netune scibert base model gat run experiment single titan rtx select model checkpoint based typed relation score performance valid set experiments evaluated systems compare ttg g models section frequency based baselines pagerank pr k authorized entities document graph highest pagerank scores page et al edge weight initialized number relation mentions entity pair topk freq tkf k frequent entities k selected els average number graph nodes aligned target nodes training set cases relations document graphs selected ties added generate predicted summary graph report performance gold entity ge vides performance upper bound relying tities relations extracted text ge picks graph node est similarity score target node lower old inclusion predicted graph includes relations found graph entities quantitative results table shows precision recall scores typed typed entity relation prediction entity duplication rates automatic test set similar evaluation trends untyped typed evaluation lower scores type matching required pected g ttg outperform baselines ent p ent r ent rel p rel r rel untyped typed ent p ent r ent rel p rel r rel e dup pr tkf ttg g ge table results untyped typed entity relation evaluation p r precision recall entity duplication rate e dup auto test set scores entity duplication rate note entity duplication rates expected untyped typed evaluation entity alignment considers string matching ent p ent r ent rel p rel r rel untyped typed ent p ent r ent rel p rel r rel e dup pr tkf ttg g ge table results untyped typed entity relation evaluation p r precision recall entity duplication rate e dup human test set scores entity duplication rate human test ttg g ttg g auto test task method metric material scientic term generic table entity relaxed match entity type entity relation evaluations particular g sistently performs best scores ttg higher precision evaluating entity duplication rate ttg consistently performs best systems performances far ge shows signicant room improvement task future table gives results human test set trends observed automatic test set hold human test set exception g shows better formance entities effective typed relations argue automatic test set reasonable extra set test systems development qualitative analysis looked handful abstracts human test set analyze scoring criteria proposed task target graphs hand annotated identify important entities salient entities stract eliminating generic nodes e method non essential scientic term nodes occasional duplicated nodes cases added entities gold reference identied automatic algorithms deemed appropriate entities identied automatic algorithms aligned reduced target set untyped entities recall higher reduced set g pagerank suggesting algorithms better capturing salient entities errors entity types involved clear cases trends precision reduced graph consistent automatic scoring noticed aligned predicted nodes contain unrelated entities coreference errors ie systems explains low relation scores impact missing inserted entities investigate causes tkf performing poorly pared g ttg specic analyze quency good indicator salient entities cases rst calculate average length mention string names predicted salient entities nd tkf average length ttg g average furthermore nd tant entities tend long string names especially paper authors start introducing specic tasks els entities tend split smaller segments later parts paper detailed explanations smaller segments tend mentioned frequently predicted tkf comprehensive qualify salient entities ample key entity bayesian semi supervised chinese word segmentation model detected salient g ttg tkf predicts chinese words word segmentation closest salient entities ple tkf predicts kl systems salient entity paper gold entity kl like knowledge representation systems predicted g ttg noted table sizes target graphs pers different human automatic test sets observe ttg produces graphs similar size papers test sets nodes edges contrast g produces graphs different sizes different ment lengths averaging nodes edges human test set nodes edges automatic test set documents longer target graphs bigger observe interesting trends sections entities rst appear ge entities appear rst tion paper time auto human test sets rst mention middle sections nal sections tively numbers consistent test sets observation highlights fact extracting main idea paper needs understanding partly sequence length limitation bertsumext ttg extremely biased entities rst section g vulnerable bias fails include entities later paper sections summaries table shows entity score based relaxed match different entity type calculate scores comparing subgraphs predicted target graphs contain entities certain type entities type ric hardest predict correlates fact metric entities likely appear rst section paper vs overall human test set possible reason metric frequent salient entity type salient entities type metric auto human test set respectively figure sample output g ttg figure shows example target metric entity rst appearing second section paper correctly predicted g missed ttg red incorrect relation edges relying graph relations limits relation prediction performance evidenced tables ge gives low relation prediction scores absence gold target relations document graphs conclusions future work described new text graph task ing summary knowledge graphs text documents including standard preprocessed open access dataset evaluation techniques facilitate research investigated graph classication text summarization techniques task detailed qualities analysis relation salience prediction challenging task extracting summary knowledge graphs important investigation leveraging document level ie graph learning techniques interesting direction explore models merge entity nodes better lead lower entity duplication rate improved relation accuracy major shortcoming gat model consider context entity mention document incorporating tual information entity mentions ing research direction references beltagy lo k cohan scibert trained language model scientic text emnlp celikyilmaz bosselut x choi y deep communicating agents abstractive tion naacl cohan dernoncourt f kim d s bui t kim s chang w goharian n discourse aware tention model abstractive summarization long uments naacl collins e augenstein riedel s vised approach extractive summarisation scientic papers conll durmus e h diab m feqa question answering evaluation framework faithfulness ment abstractive summarization acl erkan g radev d r lexrank graph based lexical centrality salience text summarization journal articial intelligence research esteva kale paulus r hashimotoa k yin w radev d socher r co search information retrieval semantic search arxiv tion answering abstractive summarization falke t gurevych bringing structure summaries crowdsourcing benchmark corpus cept maps emnlp hou y jochim c gleize m bonin f ganguly d identication tasks datasets evaluation metrics numeric scores scientic leaderboards tion acl huang l wu l wang l knowledge augmented abstractive summarization driven cloze reward acl si corpusspeaker independent recognitionspeaker adaptationword error ratesi corpusspeaker independent recognitionspeaker adaptationword error ratesi corpusspeaker independent recognitionspeaker adaptationtarget velickovic p cucurull g casanova romero p bengio y graph attention networks iclr wadden d lin s lo k wang l l zuylen m v han hajishirzi h fact fiction verifying scientic claims arxiv wadden d wennberg u luan y hajishirzi h entity relation event extraction alized span representations emnlp wang cho k lewis m asking swering questions evaluate factual consistency summaries acl wang d liu p zheng y qiu x huang x heterogeneous graph neural networks extractive ument summarization acl wang l l lo k chandrasekhar y reas r yang j burdick d eide d funk k katsis y kinney r li y liu z merrill w mooney p murdick d rishi d sheehan j shen z stilson b wade wang k wang n x r wilhelm c xie b raymond d weld d s etzioni o kohlmeier s acl nlp covid open research dataset workshop xing x fan x wan x automatic generation citation texts scholarly papers pilot study acl xu j gan z cheng y liu j aware neural extractive text summarization acl yao y ye d li p han x lin y liu z liu z huang l zhou j sun m docred scale document level relation extraction dataset acl yasunaga m kasai j zhang r fabbri r li friedman d radev d r scisummnet large annotated corpus content impact models scientic paper summarization citation networks aaai zhang y merck d tsai e b manning c d glotz c p optimizing factual correctness summary study summarizing radiology reports acl zhang y zhong v chen d angeli g manning c d position aware attention supervised data improve slot filling emnlp jain s zuylen m v hajishirzi h beltagy scirex challenge dataset document level tion extraction acl jia r wong c poon h document level n ary relation extraction multiscale representation learning naacl hlt kale m text text pre training data text tasks arxiv liu y lapata m text summarization trained encoders emnlp liu y safavi t dighe koutra d graph summarization methods applications survey acm computing surveys lo k wang l l neumann m kinney r weld d s semantic scholar open research corpus acl luan y l ostendorf m hajishirzi h multi task identication entities relations erence scientic knowledge graph construction emnlp luu k koncel kedziorski r lo k cachola arxiv smith n citation text generation nallapati r zhai f zhou b summarunner recurrent neural network based sequence model tractive summarization documents aaai narayan s cohen s b lapata m ing sentences extractive summarization ment learning naacl hlt page l brin s motwani r winograd t pagerank citation ranking bringing order web technical report stanford infolab park n kan dong x l zhao t faloutsos c estimating node importance knowledge graphs graph neural networks kdd ratcliff j w metzener d e pattern matching gestalt approach dr dobb s journal ribeiro l f r schmitt m schutze h gurevych investigating pretrained language models text generation arxiv rush m chopra s weston j neural attention model abstractive sentence summarization emnlp safavi t belth c faber l mottin d muller e koutra d personalized knowledge graph rization cloud pocket icdm stanovsky g michael j zettlemoyer l dagan supervised open information extraction naacl tan j wan x xiao j abstractive ment summarization graph based attentional ral model acl
