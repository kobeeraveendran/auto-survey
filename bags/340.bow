klearn background knowledge inference from summarization data maxime peyrard epfl maxime ch robert west epfl robert t c o l c s c v v i x r a abstract the goal of text summarization is to press documents to the relevant information while excluding background information ready known to the receiver so far rization researchers have given considerably more attention to relevance than to background knowledge in contrast this work puts ground knowledge in the foreground ing on the realization that the choices made by human summarizers and annotators contain implicit information about their background knowledge we develop and compare niques for inferring background knowledge from summarization data based on this work we dene summary scoring functions that explicitly model background knowledge and show that these scoring functions t man judgments signicantly better than lines we illustrate some of the many tential applications of our framework first we provide insights into human information importance priors second we demonstrate that averaging the background knowledge of multiple potentially biased annotators or pora greatly improves summary scoring formance finally we discuss potential cations of our framework beyond tion introduction summarization is the process of identifying the most important information pieces in a document for humans this process is heavily guided by ground knowledge which encompasses tions about the task and priors about what kind of information is important mani despite its fundamental role background edge has received little attention from the rization community existing approaches largely focus on the relevance aspect which enforces ilarity between the generated summaries and the source documents peyrard figure a summary s results from the tion of the background knowledge k and the source document d following peyrard s is lar to d relevance measured by a small but also brings new information compared to ground knowledge informativeness measured by a large we can infer the unobserved k from the choices unexplained by the relevance criteria in previous work background knowledge has usually been modeled by simple aggregation of large background corpora for instance using tfidf sparck jones one may ize background knowledge as the set of words with a large document frequency in background corpora however the assumption that frequently cussed topics reect what is on average known does not necessarily hold for example sense information is often not even discussed liu and singh also information present in background texts has already gone through the portance lter of humans e writers and ers in general a particular difculty preventing the development of proper background knowledge models is its latent nature we can only hope to infer it from proxy signals besides there is at present no principled way to compare and evaluate background knowledge models s should be similar small relevance should be dierent large informativeness in this work we put the background knowledge in the foreground and propose to infer it from marization data indeed choices made by human summarizers and human annotators provide plicit information about their background edge we build upon a recent theoretical model of information selection peyrard which tulates that information selected in the summary results from desiderata low redundancy the mary contain diverse information high relevance the summary is representative of the document and high informativeness the summary adds new information on top of the background knowledge the tension between these elements is encoded in a summary scoring function k that explicitly depends on the background knowledge k as trated by fig the latent k can then be inferred from the residual differences in information tion that are not explained by relevance and dancy for example the black information unit in fig is not selected in the summary despite being very prominent in the source document intuitively this is explained if this unit is already known by the receiver to leverage this implicit signal we view k as a latent parameter learned to best the observed summarization data contributions we develop algorithms for ring k in two settings when only pairs of ments and reference summaries pairs are observed sec and when pairs of document and summaries are enriched with human judgments sec in sec we evaluate our inferred ks with spect to how well the induced scoring function k correlates with human judgments our proposed algorithms signicantly surpass previous baselines by large margins in sec we give a geometrical perpespective on the framework and show that a clear geometrical structure emerges from real summarization data the ability to infer interpretable importance ors in a data driven way has many applications some of which we explore in sec sec tatively reveals which topics emerge as known and unkown in the tted priors moreover we can fer k based on different subsets of the data by training on the data of one annotator we get a prior specic to this annotator similarly one can nd domain specic k s by training on different datasets this is explored in sec where we alyze annotators and different summarization datasets yielding interesting insights ing several potentially biased annotator specic or domain specic k s results in systematic alization gains finally we discuss future work and tial applications beyond summarization in sec our code is available at epfl dlab klearn related work the modeling of background knowledge has ceived little attention by the summarization munity although the problem of identifying tent words was already encountered in some of the earliest work on summarization luhn a simple and effective solution came from the eld of information retrieval using techniques such as tfidf on background corpora sparck jones similarly dunning proposed the likelihood ratio test to identify highly descriptive words these techniques are known to be useful for news summarization harabagiu and lacatusu later approaches include heuristics to tify summary worthy bigrams riedhammer et al also hong and nenkova proposed a supervised model for predicting whether a word will appear in a summary or not using a large set of features including global indicators from the new york times corpus which can then serve as a prior of word importance conroy et al proposed to model ground knowledge by aggregating a large random set of news articles delort and alfonseca used bayesian topic models to ensure the extraction of informative summaries finally louis vestigated background knowledge for update marization with bayesian surprise these ideas have been generalized in an abstract model of importance peyrard discussed in the next section background this work builds upon the abstract model duced by peyrard whose relevant aspects we briey present here let t be a text and a function mapping a text to its semantic representation of the following form pt pt n the semantic representation is a probability tribution p over so called semantic units jn many different text representation techniques can be chosen e topic models with topics as mantic units or a properly renormalized semantic vector space with the dimensions as semantic units in the summarization setting the source ment d and the summary s are represented by ability distributions over the semantic units pd and ps similarly k the background knowledge is represented as a distribution pk over semantic units intuitively j is high whenever j is known a summary scoring d or simply since the document d is never ambiguous can be derived from simple requirements d k where red captures the redundancy in the summary via the entropy h rel reects the relevance of the summary via the kullback leibler kl divergence between the summary and the document a good summary is expected to be similar to the original document i e the kl divergence should be low finally inf models the informativeness of the summary via the kl divergence between the summary and the latent background knowledge k the summary should bring new information i e the kl divergence should be high in this work we x the klearn framework as laid out in our framework texts are viewed as distributions over a choice of semantic units jn we aim to infer a general k as the bution over these units that best explains rization data we consider two types of data with and without human judgments inferring k without human judgments assume we have access to a dataset xi of pairs of documents di and their associated summaries si di si under the assumption that the si are good summaries e generated by humans we infer the background knowledge k that best explains the observation of these summaries deed if these summaries are good we assume that information has been selected to minimize dancy maximize relevance and maximize tiveness use k and pk interchangeably when there is no guity direct score maximization a straightforward proach is to determine the k that maximizes the k score of the observed summaries formally this corresponds to maximizing the function xi where acts as a regularization term ing k to remain similar to a predened distribution p here p can serve as a prior about what k should be the factor controls the emphasis put on the regularization a rst natural choice for the prior p can be the uniform distribution u over semantic units in this case we show in appendix b that maximizing eq yields the following simple solution for k j j si with the choice note that j is always positive as expected this solution is fairly itive as it simply counts the prominence of each semantic unit in human written summaries and siders the ones often selected as interesting i e as having low values in the background knowledge we denote this technique as to indicate the maximum score with uniform prior surprisingly it does not involve documents whereas intuitively k should be a function of both the summaries and documents however if such a simplistic model works well it could be applied to broader ios where the documents may not even be fully observed alternatively we can choose the prior p to be the source documents di then as shown in appendix b the solution becomes j j j si here a conservative choice for to ensure the j itivity of j is min j this model j is also intuitive as the resulting value of j would be higher if j is prominent in the document but not selected in the summary this is for ple the case for the black semantic unit in fig furthermore choosing d as the prior implies ing the documents as the only knowledge available and makes a minimal prior commitment as to what k should be we denote this approach as probabilistic model when directly maximizing the score of observed summaries there is no antee that the scores of other unobserved maries remain low a principled way to address this issue is to formulate a probabilistic model over the observations xi di si s where the partition function is computed over the set of all possible summaries of ment di in practice we draw random summaries as negative samples to estimate the partition tion negative samples for each positive then k is learned to maximize the likelihood of the data via gradient descent to enforce the constraint of k being a probability tribution we parametrize k as the softmax of a vector k of scalars the vector k is trained with mini batch gradient descent to mize the negative log likelihood of the observed data this approach is denoted as pm inferring k with human judgments next we assume a dataset annotated with man judgments observations come in the form si di hi where hi is a human assessment of how good si is as a summary of di we can use this extra information to enforce high scoring low scoring summaries to also have a high low k scores regression as a rst solution we propose sion with the goal of minimizing the difference between the predicted k and the corresponding human scores on the training set more formally the task is to minimize the following loss a xi where a is a scaling parameter to put k and hi on a comparable range to train k with gradient descent we again parametrize k as the softmax of a vector of scalars sec we denote this approach as hreg preference learning in practice regression fers from annotation inconsistencies in particular the human scores for some documents might be on average higher than for other documents which easily confuses the regression preference learning pl is robust to these issues by learning the ative ordering induced by the human scores gao al pl can be formulated as a binary sication task maystre where the input is a pair of data points si di hi s j d j h j and the output is a binary ag indicating whether si is better than s j i e hi h j j h j i j where is the logistic sigmoid function and l can be for example the binary cross entropy again we perform mini batch gradient descent to train k we denote this approach as hpl comparison of approaches to compare the usefulness of various k s we need a way to evaluate them fortunately there is a natural evaluation setup i plug k into k the summary scoring function described by eq use the induced k to score summaries si and compute the agreement with human scores hi to distinguish between the algorithms duced in sec we adopt the following naming convention for scoring functions if the background knowledge k was computed using algorithm a we denote the corresponding scoring function by a e hpl is the scoring function where k was ferred by hpl data we use two datasets from the text ysis conference tac shared task and they contain and ics respectively each topic was summarized by about systems and humans all system maries and human written summaries were ually evaluated by nist assessors for readability content selection with pyramid nenkova and sonneau and overall responsiveness dang and owczarzak in this evaluation we focus on the pyramid score as the framework is built to model the content selection aspect semantic units as in previous work peyrard we use words as semantic units in sec we also experiment with topic models however different choices of text representations can be ily plugged in the proposed methods words have the advantage of being simple and directly rable to existing baselines nist nist kendall mr baselines lr icsi idf u without human judgments ours pm with human judgments ours hpl best training data optimal hpl table comparison of background knowledge based on how well the induced k correlates with humans kendall s higher is better and how far written summaries are ranked compared to system maries mr lower is better the improvements of and hpl over the baselines are signicant paired t test p baselines for reference we report the summary scoring functions of several baselines lexrank lr erkan and radev is a graph based approach whose summary scoring function is the average centrality of sentences in the summary icsi gillick and favre scores summaries based on their coverage of frequent bigrams from the source documents and haghighi and vanderwende measure vergences between the distribution of words in the summary and in the sources js divergence is a symmetrized and smoothed version of kl gence additionally we report the performance of choosing the uniform distribution for k denoted u and an idf baseline where k is built from the document frequency computed using the english wikipedia denoted as idf for reference we report the performance of training and evaluating hpl on all data denoted as optimal this sures the ability of hpl to t the training data results table reports the fold validation averaged over all topics in both and the rst column reports the kendall s correlation between humans and the various mary scoring functions the second column reports the mean rank mr of reference summaries among figure multi dimensional scaling projection of uments summaries and k inferred by hpl the clidean distance in the projection approximates to kl divergence in the original space the geometrical tuition that summaries documents and k should form a line with documents in the middle is simultaneously respected for different randomly selected topics from tac datasets all summaries produced in the shared tasks when ranked according to the summary scoring functions thus lower mr is better first note that even techniques that do not rely on human judgments can signicantly outperform previous baselines the results of are ticularly strong with large improvements despite the simplicity of the algorithm indeed and have a time complexity of where n is the number of topics and run much faster than any other algorithm seconds on a single cpu to infer k from a tac dataset despite being more principled pm does not outperform improvements over baseline are also obtained by hpl which leverages the ne grained tion of human judgments however even without beneting from supervision performs larly to hpl without signicant difference also as expected the preference learning setup hpl is stronger and more robust than the regression setup hreg which does not signicantly outperform the uniform baseline u therefore we use hpl when human judgments are available and when only summary pairs are available a geometric view previously see fig we mentioned that a good k corresponds to a distribution such that the mary s is different from k is large but still similar to the document d is small furthermore the regularization term in eq with p d enforcing small makes minimal commitment as to what k should look like i e no a priori information except the documents is assumed viewing these distributions as points in clidean space the optimal arrangement for s d and k is on a line with d in between s and k since human written summaries s and documents d are given inferring k intuitively consists in ing the point in high dimensional space matching this property for all document summary pairs interestingly we can easily test whether this ometrical structure appears in real data with our inferred k to do so we perform a simultaneous multi dimensional scaling mds embedding of documents di human written summaries si and k in this space two distributions are close to each other if their kl divergence is low we plot such an embedding in fig for randomly chosen topics from and k inferred by hpl we deed observe documents summaries and k nicely aligned such that the summaries are close to their documents but far away from k this nding also holds for k inferred by these observations are important for two sons they show that general framework troduced in fig is an appropriate model of the summarization data for any given topic the erence summaries are arranged on one side of the document they deviate from the document in a systematic way that is explained by the repulsive tion of the background knowledge human written summaries contain information from the document but not from the background knowledge which puts them on the border of the space our models can be seen to infer an appropriate background knowledge that is common to a wide spectrum of topics as shown by the fact that k occupies the central point in the embedding of fig applications we now investigate some applications arising from our framework as k is easily interpretable we explore which units receive high or low scores one can also use different subsets or aggregations of training data here we look into annotator specic k s and domain specic k s known unknown said also like say told one kill liberty new nation announcement investigation table example of words known and unknown according to the best k inferred by hpl a word j is known unknown according to k when j is high low qualitative analysis to understand what is considered as known j is high or unknown j is low we t our best model hpl using all tac data for two choices of semantic units i words and lda topics trained on the english wikipedia topics in table we report the top known and known words frequent but uninformative words like said or also are considered known and thus undesired in the summary on the contrary known words are low frequency specic words that summarization systems systematically failed to extract although they were important according to humans we emphasize that the inferred ground knowledge encodes different information than a standard idf we provide a detailed ison between k and idf in appendix e when using a text representation given by a topic model trained on wikipedia we obtain the lowing top most known topics described by words government election party united state litical minister president book published work new wrote life novel air aircraft ship navy army service training well ight known topics appeared suit the following are identied as the top series show episode tv lm season card player chess game played hand team university research college science sor research degree published topics related to military and politics receive higher scores in k given that these topics tend to be the most frequent in news datasets k trained with man annotations learns to penalize systems tting on the frequency signal within source uments on the contrary series games and figure multi dimensional scaling projections of a annotators and domains the euclidean distance in the projected space represents kl divergence in the original space the disk size is proportional to how well the k performs on the full tac datasets as uated by the correlation kendall s between the duced k and human judgments versity topics receive low scores and should be extracted more often by systems to improve their agreement with humans inferring and domain specic background knowledge within the tac datasets the annotations are also tagged with an annotator id it is thus possible to infer a background knowledge specic to each notator by applying our algorithms on the subset of annotations performed by the respective annotator in and combined annotators are identied resulting in different k s instead of analyzing only news datasets with human annotations like tac we can infer ground knowledge from any summarization dataset from any domain as long as document summary pairs are observed to illustrate this we consider a large collection of datasets covering domains such as news legal documents product reviews pedia articles these do not contain human annotations so we employ our algorithm to infer a k specic to each dataset the detailed scription of these datasets is given in appendix c structure of differences to visualize the ences between annotators we embed them in using mds with two annotators being close if their k are similar in fig a each annotator is a dot whose size is proportional to how well its k generalizes to the rest of the tac datasets as uated by the correlation kendall s between the induced k and human judgments the same dure is applied to domains and is depicted in fig b figure correlation with human judgments on tac datasets news domain resulting from averaging annotator specic k s and domain specic k s distance to the optimal k computed by running hpl on the full tac datasets news datasets appear at the center of all domains meaning that the news domain can be seen as an average of the peripheral non news domains thermore the k s trained on different news datasets are close to each other indicating a good level of intra domain transfer and unsurprisingly news datasets also exhibit the best transfer performance on tac improvements due to averaging based on vious observations we make the hypothesis that averaging different annotator specic k can lead to better correlation with human judgments on the unseen part of the tac dataset similarly news domains generalize better than other domains we hypothesized that averaging domains may also sult in improved correlations with humans in the news domain in fig we report the improvements in relation with human judgments on tac news main resulting from averaging an increasing ber of annotators or domains the error bars sent condence intervals arising from ing a different subset to compute the average as we see increasing the number of annotators aged results in clear and signicant improvements since the error bars are small which annotators are newsdomains figure several k distributions visualized as density in glove embedding space included in the averaging has little impact on the results source documents as illustrated by initial results in appendix d similarly averaging different domains also sults in signicant improvements in particular averaging several non news domains gives better generalization to the news domain furthermore fig shows in the glove et al embedding space the k s resulting from averaging a all annotators k s ferred by hpl all news datasets k s inferred by and c all non news datasets k s ferred by in comparison to the optimal k learned with hpl trained on all data from tac datasets to produce these visualizations we form a density estimation of the k s in the projection of word embeddings all averaged k s tend to be similar to the mal k it indicates that only one prior produces strong results on the news datasets and it can be obtained by averaging many biased but different k s this is further conrmed by fig where the distance to the optimal k measured in terms of kl divergence signicantly decreases when more annotators are averaged conclusion we focus on the often ignored background edge for summarization and infer it from implicit signals from human summarizers and annotators we introduced and evaluated different approaches observing strong abilities to t the data the newly gained ability to infer interpretable priors on importance in a data driven way has many potential applications for example we can scribe which topics should be extracted more quently by systems to improve their agreement with humans using pretrained priors also helps systems to reduce overtting on the frequency signal within that the y axis has been normalized to put the ent divergences on a comparable scale an important application made possible by this framework is to infer k on any meaningful subset of the data in particular we learned specic k s which yielded interesting insights some annotators exhibit large differences from the others and averaging several potentially biased k s results in generalization improvements we also inferred k s from different summarization datasets and also found increased performance on the news domain when averaging k s from diverse domains for future work different choices of semantic units can be explored e learning k directly in the embedding space also we xed to get comparable results across methods but cluding them as learnable parameters could provide further performance boosts investigating how to infuse the tted priors into summarization systems is another promising direction more generally inferring k from a sense task like summarization can provide insights about general human importance priors inferring such priors has applications beyond tion as the framework can model any information selection task acknowledgments we gratefully acknowledge partial support from facebook google microsoft the swiss national science foundation grant and the european union grant tailor references jean carletta simone ashby sebastien bourban mike flynn mael guillemot thomas hain jaroslav kadlec vasilis karaiskos wessel kraaij melissa kronenthal guillaume lathoud mike lincoln iain mccowan wilfried post agnes lisowska dennis reidsma and pierre wellner the in ami meeting corpus a pre announcement proceedings of the second international conference on machine learning for multimodal interaction page berlin heidelberg springer verlag filippo galgani paul compton and achim hoffmann citation based summarisation of legal texts in pricai trends in articial intelligence pages berlin heidelberg springer berlin heidelberg jianpeng cheng and mirella lapata neural summarization by extracting sentences and words in proceedings of the annual meeting of the sociation for computational linguistics volume long papers pages association for putational linguistics arman cohan franck dernoncourt doo soon kim trung bui seokhwan kim walter chang and zli goharian a discourse aware attention model for abstractive summarization of long uments in proceedings of the conference of the north american chapter of the association for computational linguistics human language nologies volume short papers pages new orleans louisiana association for tional linguistics john m conroy judith d schlesinger and dianne p topic focused multi document oleary summarization using an approximate oracle score in proceedings of the coling acl main ference poster sessions pages sydney australia association for computational tics hoa trang dang and karolina owczarzak overview of the tac update summarization task in proceedings of the text analysis conference tac workshop notebook papers and results pages hoa trang dang and karolina owczarzak overview of the tac update summarization task in proceedings of the first text analysis ference tac pages hoa trang dang and karolina owczarzak overview of the tac summarization track in proceedings of the text analysis conference tac pages hoa trang dang and karolina owczarzak overview of the tac summarization track in proceedings of the first text analysis conference tac pages jean yves delort and enrique alfonseca alsum a topic model based approach for update summarization in proceedings of the ence of the european chapter of the association for computational linguistics pages ted dunning accurate methods for the tics of surprise and coincidence computational guistics kavita ganesan chengxiang zhai and jiawei han opinosis a graph based approach to stractive summarization of highly redundant ions in proceedings of the international ference on computational linguistics coling pages yang gao christian m meyer and iryna gurevych april interactively learning to summarise by combining active preference learning and forcement learning in proceedings of the ference on empirical methods in natural language processing pages brussels belgium association for computational linguistics dan gillick and benoit favre a scalable global in proceedings of the model for summarization workshop on integer linear programming for ural language processing pages boulder colorado association for computational tics aria haghighi and lucy vanderwende ing content models for multi document tion in proceedings of human language gies the annual conference of the north american chapter of the association for tional linguistics pages sanda harabagiu and finley lacatusu topic themes for multi document summarization in ceedings of the annual international acm gir conference on research and development in information retrieval pages karl moritz hermann tomas kocisky edward stette lasse espeholt will kay mustafa suleyman and phil blunsom teaching machines to read and comprehend in advances in neural formation processing systems pages kai hong and ani nenkova improving the estimation of word importance for news in proceedings of the document summarization conference of the european chapter of the sociation for computational linguistics pages gothenburg sweden association for tational linguistics manoel horta ribeiro kristina gligoric and robert west message distortion in information in the world wide web conference page cades new york ny usa association for computing machinery gnes erkan and dragomir r radev lexrank graph based lexical centrality as salience in text journal of articial intelligence summarization research pages byeongchang kim hyunwoo kim and gunhee kim abstractive summarization of reddit posts in with multi level memory networks ings of the conference of the north american chapter of the association for computational guistics human language technologies volume long and short papers pages neapolis minnesota association for computational linguistics mahnaz koupaee and william yang wang ihow a large scale text summarization dataset corr chin yew lin rouge a package for in text matic evaluation of summaries rization branches out proceedings of the workshop pages barcelona spain tion for computational linguistics hugo liu and push singh conceptnet a practical commonsense reasoning tool kit bt technology journal annie louis a bayesian method to rate background knowledge during automatic text summarization in proceedings of the annual meeting of the association for computational guistics volume short papers pages baltimore maryland hans peter luhn the automatic creation of literature abstracts ibm journal of research velopment inderjeet mani advances in automatic text marization mit press cambridge ma usa lucas maystre efcient learning from isons epfl lausanne ryan mcdonald a study of global inference algorithms in multi document summarization in proceedings of the european conference on formation retrieval research pages shashi narayan shay b cohen and mirella lapata do nt give me the details just the summary topic aware convolutional neural networks for treme summarization in proceedings of the conference on empirical methods in natural guage processing pages brussels gium association for computational linguistics ani nenkova and rebecca passonneau ing content selection in summarization the in proceedings of the human mid method guage technology conference of the north can chapter of the association for computational linguistics hlt naacl pages boston massachusetts usa romain paulus caiming xiong and richard socher a deep reinforced model for abstractive summarization corr jeffrey pennington richard socher and christopher manning glove global vectors for word in proceedings of the representation ference on empirical methods in natural language processing emnlp pages doha qatar association for computational linguistics maxime peyrard a simple theoretical model of importance for summarization in proceedings of the annual meeting of the association for putational linguistics pages florence italy association for computational linguistics maxime peyrard and judith eckle kohler a general optimization framework for document summarization using genetic in proceedings of rithms and swarm intelligence the international conference on computational linguistics coling pages avinesh p v s maxime peyrard and christian m meyer live blog corpus for summarization in proceedings of the international ence on language resources and evaluation pages korbinian riedhammer benoit favre and dilek hakkani tr long story short global unsupervised models for keyphrase based speech communication ing summarization evan sandhaus the new york times annotated corpus linguistic data consortium philadelphia abigail see peter j liu and christopher d ning get to the point summarization in proceedings with pointer generator networks of the annual meeting of the association for computational linguistics volume long papers pages vancouver canada association for computational linguistics karen sparck jones a statistical interpretation of term specicity and its application in retrieval journal of documentation wei zhao maxime peyrard fei liu yang gao tian m meyer and steffen eger moverscore text generation evaluating with contextualized beddings and earth mover distance in proceedings of the conference on empirical methods in natural language processing and the tional joint conference on natural language cessing emnlp ijcnlp pages hong kong china association for computational guistics markus zopf maxime peyrard and judith kohler the next step for multi document summarization a heterogeneous multi genre pus built with a novel construction approach in proceedings of the international conference on computational linguistics technical papers pages figure visualization of each annotator s k based on projection of glove word embedding figure visualization of each domain s k based on projection of glove word embedding a visualization of k for each annotator and each domain we produce visualizations in the embedding space with the same procedure as in fig fig depicts the annotators and fig depicts the domains it is interesting to observe much more diversity ing from the domains and the domain specic k s are more spread out in the semantic space this reects the greater topic diversity discussed in ferent domains in contrast each annotator s k is inferred based on the tac datasets which are in the same domain news b derivation of approaches the direct score maximization model consists in maximizing l we use lagrange multipliers with the constraint that k is a valid distribution first with p u the uniform and we have the following derivatives j d j dl j j j j j j j setting the lagrange derivative to yields j j where is the normalizing constant in particular when j j note that choosing ensures that for all we have second we consider the case p d the document and u changes with every document summary pair and l becomes l d s d s l j then only the the derivative concerning is modied and becomes d j j j sumlegalreports which gives the following solution after setting the lagrange derivative to the ami corpus carletta et al is a dard product review summarization dataset j j j here it is not clear that j is positive for ery units to avoid such issue notice that we can choose min j j j c datasets the summarization track at the text analysis ference tac was a direct continuation of the duc series in particular the main tasks of dang and owczarzak and dang and owczarzak were renements of the pilot update summarization task of duc a dataset of topics was released as part of the edition and new topics were created in and became standard benchmark datasets the new york times annotated corpus haus counts as one of the largest rization datasets currently available it contains nearly million carefully selected articles from the new york times each with summaries written by humans also the cnn daily mail dataset hermann et al has been decisive in the recent opment of neural abstractive summarization see et al paulus et al cheng and lapata it contains cnn and daily mail articles together with bullet point summaries zopf et al also viewed the quality wikipedia featured articles as summaries for which potential sources were automatically searched on the web p v s al recently crawled the blog archives from the bbc and the guardian gether with some bullet point summaries reporting the main developments of the event covered to evaluate their opinion oriented tion system ganesan et al constructed the opinosis dataset it contains articles discussing the features of commercial products e ipod s battery life furthermore we consider the large pubmed taset cohan et al a collection of scientic publications the reddit dataset kim et al has been collected on popular sub reddits koupaee and wang automatically crawled the wikihow website using the reported bullet points as summaries the xsum dataset narayan et al is a large collection of news articles with a focus on abstractive summaries to measure the effect of information distortion in summarization cascades of scientic results horta ribeiro et al collected manual maries of various lengths we also included the legalreport dataset et al where the task is to summarize legal documents d extracting summaries example once k is specied the summary scoring tion k can be used to extract summaries for extractive summarization this is an optimal set selection problem mcdonald tunately k is not linear and can not be optimized with integer linear programming it is also not modular and can not be optimized with the greedy algorithm for submodularity we have to rely on generic optimization techniques which do not make any assumption about the objective function and can approximately optimize any arbitrary function we use the genetic algorithm proposed by peyrard and eckle kohler which creates and tively optimizes summaries over time we denote as k gen the summarization system mately solving the subset selection problem we compare systems when k is inferred by when k is inferred by hpl and when k is the form distribution for reference we report the standard summarization baselines described in the previous section the summaries are evaluated with automatic evaluation metrics call with stopwords removed lin and a recent bert based evaluation metric mover zhao et al the results reported in table are encouraging since the systems based on the learned priors outperform the uniform prior they also perform well in comparison to baselines the inferred prior can benet systems by preventing them from overtting on the frequency signal com genetic swarm mds employ a fold cross validation setup dataset creation man input type genre summary length size topics doc topic scisumm cnn daily mail nyt corpus opinosis liveblogs hmds pubmed xsum reddit ami wikihow legalreports mdic m m a a a m a m a a a m a a m mds mds mds sds sds mds temporal mds sds sds sds mds sds sds cascade news news sci news news review snippets heter sci news heter meeting heter legal sci varying k k k k k k k table description of datasets used in the experiments visualization of optimal k renormalized idf and their absolute difference one bar for word in the support words table comparison of summarization systems based on maximizing the summary scoring function k duced by different background knowledge mover mover baselines lr icsi kl greedy js gen ours u gen gen hpl gen e comparison idf vs optimal k to verify that our inferred k contains different formation from id we compare idf and our mal k see sec to be comparable idf weights need to be malized as the idf weights of known unknown words would be low high whereas pk would be high low thus we compute c j for each word where c j in fig we represent the full distributions over all words in the support of k and show the absolute difference with renormalized idf weights furthermore fig is a scatter plot where each dot represent a word and the coordinates are its idf and k weights the low correlation between the two indicates that k learns a different signal than idf scatter plot where each dot is a word and the coordinates are its probability in k and its renomalized idf optimal krenormalized idfsabsolute differenceoptimal k inferred by hplidfs
