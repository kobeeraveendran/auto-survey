neural attention model abstractive sentence summarization alexander m rush facebook ai research harvard seas harvard edu sumit chopra facebook ai research com jason weston facebook ai research com p e s l c s c v v x r abstract summarization based text extraction inherently limited generation style stractive methods proven work propose ing build fully data driven approach tive sentence summarization method utilizes local attention based model generates word summary ditioned input sentence model structurally simple ily trained end end scales large training data model shows signicant performance gains shared task compared strong baselines introduction summarization important challenge ral language understanding aim produce condensed representation input text captures core meaning original successful summarization systems utilize tive approaches crop stitch portions text produce condensed contrast abstractive summarization sion tempts produce summary aspects appear original focus task sentence level marization work task looked deletion based sentence compression techniques knight marcu studies human summarizers common apply operations condensing paraphrasing ization reordering jing past work modeled abstractive summarization lem linguistically inspired constraints dorr et al zajic et al tactic transformations input text cohn figure example output attention based rization abs system heatmap represents soft ment input right generated summary columns represent distribution input generating word lapata woodsend et al proaches described detail section instead explore fully data driven approach generating abstractive summaries inspired recent success neural machine translation combine neural language model textual input encoder encoder modeled attention based encoder bahdanau et al learns latent soft alignment input text help inform summary shown figure crucially encoder generation model trained jointly sentence summarization task model scribed detail section model corporates beam search decoder tional features model extractive elements aspects discussed sections approach summarization attention based summarization abs rates linguistic structure comparable stractive summarization approaches easily input sentence article russian defense minister ivanov called sunday creation joint combating global terrorism output generated headline russia calls joint terrorism joint figure example input sentence generated summary score generating terrorism based context yc input note summary generated abstractive makes possible generalize russian defense minister russia paraphrase combating addition compressing dropping creation jing survey editing operations scale train large data system makes assumptions lary generated summary trained directly document summary pair allows train summarization model headline generation corpus article pairs gigaword graff et al consisting million articles example tion given figure discuss details task section test effectiveness approach run extensive comparisons multiple tive extractive baselines including traditional integer linear syntax based systems constrained systems information retrieval style approaches statistical phrase based chine translation section describes results experiments approach outperforms machine translation system trained large scale dataset yields large improvement highest scoring system competition background begin dening sentence summarization task given input sentence goal duce condensed summary let input sist sequence m words xm ing xed vocabulary v size v represent word indicator vector xi m sentences sequence indicators x set ble inputs furthermore dene notation j indicate sub sequence elements j k summarizer takes input outputs shortened sentence y length n m assume words summary come vocabulary v output contrast large scale sentence compression tems like filippova altun require tonic aligned compressions sequence yn note contrast related tasks like machine translation sume output length n xed system knows length summary fore generation problem consider summaries erating set y possible sentences length n e y y yi indicator system abstractive tries nd optimal sequence set y dene arg max yy y scoring function x y r contrast fully extractive sentence transfers words input arg max m n mn related problem sentence compression concentrates deleting words input arg max m n mi mn abstractive summarization poses cult generation challenge lack hard straints gives system freedom tion allows t wider range ing data work focus factored scoring tions s account xed window previous words y yc n evaluation actually number bytes output capped detail given section literature inconsistent formal denition distinction systems self described abstractive extractive denition dene yc window size c particular consider probability summary given input y log write conditional log log yc n markov assumption length context size c assume yi special start symbol scoring function mind main focus modelling local conditional distribution yc section denes parameterization distribution section return question generation factored models section introduce modied factored scoring function model distribution interest yc conditional language model based sentence past work summarization compression noisy channel approach split independently estimate language model conditional summarization model banko et al knight marcu daume iii marcu e arg max y log arg max log y estimated separately instead follow work neural machine translation directly parameterize original distribution neural network network tains neural probabilistic language model encoder acts conditional marization model neural language model core parameterization language model estimating contextual probability word language model adapted standard feed forward neural network guage model nnlm particularly class nnlms described bengio et al model yc yc v h u e yc w enc p p c g yc x x f figure network diagram nnlm decoder additional encoder element network diagram attention based encoder parameters e u v w e rdv word embedding matrix u v rv h w rv h weight d size word embeddings h hidden layer size h black box function enc contextual encoder term turns vector size h representing input current context consider possible ants described subsequently figure gives schematic representation decoder ture encoders note encoder term represents standard language model incorporating enc training elements jointly cially incorporate input text tion discuss possible tions encoder bag words encoder basic model simply uses bag words input sentence embedded size h ignoring ties original order relationships neighboring words write model yc p m m x input embedding matrix f rhv new parameter encoder uniform distribution input words yc eyi h weight matrices u v w responding bias term readability omit terms paper summarization model capture relative importance words distinguish tent words stop words embellishments potentially model learn combine words inherently limited senting contiguous phrases convolutional encoder address modelling issues bag words sider deep convolutional encoder input sentence architecture improves bag words model allowing local interactions words requiring text yc encoding input utilize standard time delay neural network tdnn architecture alternating ral convolution layers max pooling layers j max xl l xl j xl g rdv embedding text p new weight matrix rameter mapping context embedding input embedding q smoothing dow model shown figure informally think model simply replacing uniform distribution bag words learned soft alignment p summary figure shows ple distribution p summary ated soft alignment weight smoothed version input x structing representation instance current context aligns position words xiq highly weighted encoder nnlm model seen stripped version attention based neural machine translation model l xl training f word embedding matrix consists set lters layer l eq temporal tion layer eq consists element temporal max pooling layer pointwise non linearity nal output eq max time layer x half size x simplicity assume convolution padded boundaries m greater dimensions dened attention based encoder tional encoder richer capacity bag words required produce single resentation entire input sentence lar issue machine translation inspired bahdanau et al instead utilize attention based contextual encoder constructs representation based generation context note exploit context actually use simple model similar bag words yc c p x fxm gyi xi xi q q lack generation constraints makes sible train model arbitrary input output pairs dened local tional model yc estimate parameters minimize negative likelihood set summaries dene ing set consisting j input summary pairs negative likelihood conveniently term token summary log j j n log yc minimize nll mini batch stochastic gradient descent details described section explicit compared bahdanau et al model uses nnlm instead target lstm source windowed averaging instead source directional rnn weighted dot product alignment instead alignment mlp dependent gold standard contexts alternative use predicted context structured reenforcement learning style objective generating summaries extension extractive tuning return problem generating maries recall eq goal nd y arg max yc n yy unlike phrase based machine translation inference np hard actually tractable ory compute y explicit hard alignment constraint viterbi decoding plied requires v c time nd exact solution practice v large difcult alternative approach approximate arg max strictly greedy deterministic decoder compromise exact greedy coding use beam search decoder rithm maintains vocabulary v limiting k potential hypotheses position summary standard approach neural mt models danau et al sutskever et al luong et al beam search algorithm shown modied feed forward model algorithm beam search input parameters beam size k input output approx k best summaries s v abstractive n generate hypotheses n y hypothesis recombination n h n s t yc c filter k max k arg max yh end return yc attention based model effective generating summaries miss important aspect seen human generated particular abstractive model references capacity nd extractive word matches necessary example transferring unseen proper noun phrases input lar issues observed neural lation models particularly terms translating rare words luong et al address issue experiment tuning small set additional features abstractive extractive tendency tem modifying scoring function directly estimate probability summary log linear model standard machine translation yc n weight vector f ture function finding best summary distribution corresponds maximizing factored scoring function s yc n yc yc isfy eq function dened combine local conditional probability tional indicator featrues yc log yc xjk k xjk k j yi xk viterbi beam search algorithm simpler beam search phrase based mt explicit constraint source word exactly need maintain bit set ply left right generating words beam search algorithm requires v time computational perspective round beam search dominated computing yc k hypotheses computed mini batch tice greatly reduces factor k features correspond indicators gram bigram trigram match input reordering input words note ting gives model identical standard abs training main neural model x tune parameters follow tical machine translation setup use error rate training mert tune rization metric tuning data och tuning step identical phrase based machine translation baseline related work abstractive sentence summarization ditionally connected task headline ation work similar early work banko et al developed statistical machine translation inspired approach task corpus headline article pairs extend neural summarization approach model opposed count based noisy channel model training model larger scale k compared million articles lowing fully abstractive decoding task standardized competitions et al topiary system zajic et al performed best task described detail section point interested ers duc web page nist list systems entered shared task recently cohn lapata compression method allows bitrary transformations extract tree duction rules aligned parsed texts learn weights transfomations max margin learning algorithm woodsend et al pose quasi synchronous grammar approach lizing context free parses dependency parses produce legible summaries approaches differ rectly use syntax input output sentences system results tempted train system dataset train scale addition banko et al work statistical machine translation directly abstractive summary wubben et al utilize moses directly method text simplication recently filippova altun developed strictly extractive system trained atively large corpora k sentences title pairs focus extractive pression sentences transformed series heuristics words monotonic alignment system require ment step instead uses text directly neural mt work closely related cent work neural network language models nnlm work neural machine tion core model nnlm based bengio et al recently papers models machine translation kalchbrenner blunsom cho et al sutskever et al model closely related attention based model bahdanau et al explicitly nds soft alignment tween current position input source models utilize recurrent neural works rnns generation opposed forward models hope incorporate lm future work experimental setup experiment attention based sentence summarization model task headline eration section describe corpora task baseline methods pare implementation details proach data set standard sentence summarization evaluation set associated shared tasks et al data task consists news cles new york times associated press wire services paired different human generated reference summaries ally headlines capped bytes data set evaluation similarly sized data set available task expectation summary roughly words based text complete cle use rst tence data set available request nist gov data html shared task systems entered evaluated variants oriented rouge metric lin recall evaluation unbiased length systems cut characters bonus given shorter summaries unlike bleu interpolates n gram matches versions rouge different match lengths duc evaluation uses unigrams bigrams rouge l longest common substring report addition standard ation report evaluation single ence headline generation randomly subset gigaword evaluation closer task model trained allows use bigger evaluation set clude code release evaluation tune systems generate output average title length training data tasks utilize annotated gigaword data set graff et al napoles et al consists standard gigaword preprocessed stanford corenlp tools manning et al model uses annotations tokenization sentence tion baselines use parsing tagging gigaword contains million news articles sourced tic international news services decades training set pair headline article rst sentence create summary pair model theory trained pair gigaword contains rious headline article pairs prune training based following heuristic lters non stop words common title contain byline ous editing marks title tion mark colon applying lters training set consists roughly j million title article pairs apply minimal ing step ptb tokenization lower casing placing digit characters replacing word types seen times unk remove articles time period duc evaluation release complete input training vocabulary consists million word tokens k unique word types average sentence size words headline vocabulary consists million kens k word types average title length words note signicantly shorter duc summaries average overlapping word types headline input rst characters input baselines variety approaches sentence summarization problem report broad set headline generation baselines task include fix baseline simply returns rst characters input headline report winning system shared task topiary zajic et al topiary merges compression system motivated transformations input dorr et al unsupervised topic detection utd algorithm appends key phrases article compressed output woodsend et al described report results duc dataset duc task includes set manual summaries performed human summarizers summarizing half test data sentences yielding references sentence report average inter annotater agreement score erence reference best human evaluator scores include baselines cess training data system rst sentence compression baseline press clarke lapata model uses syntactic structure original sentence language model trained line data produce compressed output syntax language model combined set linguistic constraints decoding formed ilp solver control memorizing titles training implement information retrieval baseline ir baseline indexes training set gives title article highest match input manning et al finally use phrase based statistical chine translation system trained gigaword produce summaries koehn et al improve baseline task augment phrase table deletion rules mapping article word include tional deletion feature rules allow innite distortion limit itly tune model mert target byte capped rouge score opposed standard bleu based tuning unfortunately ing issue non trivial modify lation decoder produce xed length outputs tune system produce roughly pected length model rouge l ext rouge l gigaword ir prefix compress topiary abs reference table experimental results main summary tasks rouge metrics baseline models described detail section report percentage tokens summary appear input gigaword ext implementation training use mini batch stochastic gradient descent minimize negative log likelihood use learning rate split learning rate half validation log likelihood improve epoch training performed shufed mini batches size minibatches grouped input length epoch renormalize embedding tables hinton et al based validation set set parameters d h c l q implementation uses torch numerical framework openly available data pipeline cially training performed gpus intractable require approximations wise processing mini batches d h requires seconds best tion accuracy reached epochs data requires days training additionally described section apply mert tuning step training data step use z mert zaidan refer main model abs tuned model results main results presented table run experiments uation data set sentences references bytes systems randomly held gigaword test set sentences reference rst note baselines compress ir relatively poorly datasets indicating having article information guage model information sufcient task prefix baseline actually performs prisingly makes sense given earlier observed overlap article summary abs perform better topiary particularly rouge l duc model scores best tasks signicantly better based default rouge condence level topiary metrics duc rouge l gigaword note additional extractive features bias system taining input words useful underlying metric consider ablations model gorithm structure table shows experiments model encoders iments look perplexity system language model validation data trols variable inference tuning nnlm language model encoder gives gain standard n gram language model including bag words encoder reduces perplexity number lutional encoder attention based encoder reduce perplexity attention giving value consider model decoding ablations main summary model shown table experiments compare bow encoding models compare beam search greedy ing restricting system plete extractive features biggest pact powerful encoder tion versus bow beam search generate summaries abstractive nature system helps rouge pure tractive generation effective model encoder perplexity kn smoothed gram feed forward nnlm bag word convolutional tdnn attention based abs table perplexity results gigaword validation set comparing language models end summarization models encoders dened section decoder model cons greedy beam beam beam abs bow abs ext abs r l table rouge scores development data versions inference greedy beam scribed section ext purely extractive version system eq finally consider example summaries shown figure despite improving line scores model far human formance task generally models good picking key words input names places models reorder words syntactically incorrect ways instance sentence models wrong subject abs uses interesting wording instance new nz pm election sentence lead attachment mistakes russian oil giant chevron tence conclusion presented neural attention based model abstractive summarization based recent velopments neural machine translation combine probabilistic model tion algorithm produces accurate tive summaries step like improve grammaticality maries data driven way scale system generate paragraph level summaries pose additional challenges terms cient alignment consistency generation references dzmitry bahdanau kyunghyun cho yoshua neural machine translation corr bengio jointly learning align translate detained iranian american academic accused acting national security released tehran prison hefty bail posted p judiciary ofcial said tuesday g iranian american academic held tehran released bail detained iranian american academic released jail posting bail detained iranian american academic released prison hefty bail ministers european union mediterranean neighbors gathered heavy security monday unprecedented conference economic political cooperation g european mediterranean ministers gather landmark conference julie bradford mediterranean neighbors gather unprecedented conference heavy security mediterranean neighbors gather heavy security dented conference death toll school collapse haitian shanty town rose rescue workers uncovered classroom dead students teacher ofcials said saturday g toll rises haiti school unk ofcial death toll haiti school accident rises death toll haiti school dead students australian foreign minister stephen smith sunday congratulated new zealand s new prime minister elect john key praised ousted leader helen clark gutsy respected politician g time caught nz s gutsy clark says australian fm australian foreign minister congratulates new nz pm election australian foreign minister congratulates smith new zealand leader drunken south african fans hurled racist abuse country s rugby sevens coach team eliminated weekend s hong kong tournament reports said tuesday g rugby union racist taunts mar hong kong sevens report south african fans hurl racist taunts rugby sevens south african fans racist abuse rugby sevens tournament christian conservatives kingmakers presidential elections success getting pick elected political observers g christian conservatives power diminished ahead vote christian conservatives success election christian conservatives presidential elections white house thursday warned iran possible new sanctions un nuclear watchdog reported tehran begun sensitive nuclear work key site deance un resolutions g warns iran step backward nuclear issue iran warns possible new sanctions nuclear work un nuclear watchdog warns iran possible new sanctions thousands kashmiris chanting pro pakistan slogans sunday attended rally welcome hardline separatist leader underwent cancer treatment mumbai g thousands attend rally kashmir hardliner thousands rally support hardline kashmiri separatist leader thousands kashmiris rally welcome cancer treatment explosion iraq s restive northeastern province diyala killed soldiers wounded military reported monday g soldiers killed iraq blast december toll soldiers killed restive northeast province explosion restive northeastern province kills soldiers russian world nikolay davydenko fth drawal injury illness sydney international wednesday retiring second round match foot injury g tennis davydenko pulls sydney injury davydenko pulls sydney international foot injury russian world davydenko retires sydney international russia s gas oil giant gazprom oil major chevron set joint venture based resource rich northwestern siberia interfax news agency reported thursday quoting gazprom ofcials g gazprom chevron set joint venture russian oil giant chevron set siberia joint venture russia s gazprom set joint venture siberia figure example sentence summaries produced gaword input abs g true headline michele banko vibhu o mittal michael j brock headline generation based tical translation proceedings annual meeting association computational tics pages association computational linguistics yoshua bengio rejean ducharme pascal vincent christian janvin neural probabilistic guage model journal machine learning search kyunghyun cho bart van merrienboer c aglar gulcehre dzmitry bahdanau fethi bougares ger schwenk yoshua bengio learning phrase representations rnn encoder decoder statistical machine translation proceedings emnlp pages james clarke mirella lapata global ference sentence compression integer linear programming approach journal articial gence research pages trevor cohn mirella lapata sentence compression word deletion proceedings international conference tional linguistics volume pages ciation computational linguistics hal daume iii daniel marcu channel model document compression ceedings annual meeting association computational linguistics pages sociation computational linguistics bonnie dorr david zajic richard schwartz hedge trimmer parse trim approach headline generation proceedings naacl text summarization workshop volume pages association computational guistics katja filippova yasemin altun ing lack parallel data sentence compression emnlp pages david graff junbo kong ke chen kazuaki maeda english gigaword linguistic data consortium philadelphia geoffrey e hinton nitish srivastava alex krizhevsky ilya sutskever ruslan improving neural networks dinov preventing co adaptation feature detectors corr hongyan jing hidden markov modeling decompose human written summaries tional linguistics nal kalchbrenner phil blunsom recurrent emnlp pages continuous translation models kevin knight daniel marcu tion sentence extraction probabilistic proach sentence compression articial gence philipp koehn hieu hoang alexandra birch chris callison burch marcello federico nicola bertoldi brooke cowan wade shen christine moran richard zens al moses open source toolkit statistical machine translation ceedings annual meeting acl interactive poster demonstration sessions pages association computational tics chin yew lin rouge package automatic text summarization evaluation summaries branches proceedings shop pages thang luong ilya sutskever quoc v le oriol vinyals wojciech zaremba ing rare word problem neural machine proceedings annual lation ing association computational tics pages christopher d manning prabhakar raghavan introduction hinrich schutze tion retrieval volume cambridge university press cambridge christopher d manning mihai surdeanu john bauer jenny finkel steven j bethard david closky stanford corenlp natural proceedings guage processing toolkit annual meeting association tional linguistics system demonstrations pages courtney napoles matthew gormley benjamin van durme annotated gigaword ceedings joint workshop automatic edge base construction web scale knowledge extraction pages association tational linguistics franz josef och minimum error rate training statistical machine translation proceedings annual meeting association tational linguistics volume pages sociation computational linguistics paul hoa dang donna harman duc context information processing management ilya sutskever oriol vinyals quoc vv le sequence sequence learning neural works advances neural information ing systems pages kristian woodsend yansong feng mirella lapata generation quasi synchronous grammar proceedings conference empirical methods natural language processing pages association computational linguistics sander wubben antal van den bosch emiel krahmer sentence simplication lingual machine translation proceedings annual meeting association tational linguistics long papers volume pages association computational tics omar zaidan z mert fully congurable open source tool minimum error rate training machine translation systems prague bulletin mathematical linguistics david zajic bonnie dorr richard schwartz bbn umd topiary ceedings hlt naacl document standing workshop boston pages
