abstraction extraction multiple timescale gated recurrent unit summarization minsoo kim school electronics engineering kyungpook national university daegu south korea com moirangthem dennis singh school electronics engineering kyungpook national university daegu south korea com minho lee school electronics engineering kyungpook national university daegu south korea com abstract work introduce temporal erarchies sequence sequence model tackle problem abstractive summarization scientic ticles proposed multiple timescale model gated recurrent unit gru implemented decoder setting better deal presence multiple compositionalities larger texts proposed model pared conventional rnn decoder results demonstrate model trains faster shows performance gains results temporal hierarchies help improve ability models capture compositionalities better presence highly complex tural hierarchies introduction related works summarization extensively researched past decades jones nenkova offer excellent overviews eld broadly summarization methods categorized extractive approaches abstractive approaches hahn mani based type computational task tive summarization selection problem abstractive summarization requires deeper mantic discourse understanding text novel text generation process extractive summarization focus past abstractive summarization remains challenge recently sequence sequence current neural networks rnns seen wide application number tasks rnn encoder decoders cho bahdanau combine representation learning coder language modeling decoder perform mappings sequences similarly cent works proposed cast summarization mapping problem input sequence summary sequence recent successes rush nallapati shown rnn encoder decoder performs markably summarizing short text approaches offer fully data driven tion semantic discourse understanding text generation presents promising way ward abstractive summarization extrapolating methodology tasks marization scientic article trivial number practical theoretical concerns arise simply train rnn encoder decoders entire articles memory capacity rent gpus scientic articles long processed rnns moving sentences sentences paragraphs introduces additional levels positionality richer discourse structure improve conventional rnn decoder better capture deep learning approaches depend heavily good quality scale datasets collecting source summary data pairs difcult datasets scarce outside newswire domain paper present rst ate step end end abstractive rization scientic articles aim tend based summarization larger text complex summarization task dress issues propose paragraph wise summarization system trained paragraph salient sentence pairs use term frequency inverse document frequency idf luhn jones scores tract salient sentence paragraph introduce novel model multiple timescale gated recurrent unit mtgru adds temporal hierarchy component serves dle multiple levels compositionality inspired analogous concept temporal erarchical organization found human brain implemented modulating different layers multilayer rnn different timescales yamashita tani demonstrate model capable understanding tics multi sentence source text knowing important rst essary step abstractive summarization build new dataset computer science articles arxiv org extracting tions latex source les tions decomposed paragraphs graph acting natural unit discourse finally concatenate generated summary paragraph create non expert summary article introduction evaluate sults actual abstract model capable summarizing multiple tences salient unseen data ther supporting larger view summarization mapping task demonstrate mtgru model satises major quirements abstractive summarization tem report mtgru bility reducing training time signicantly pared conventional rnn encoder decoder paper structured follows section scribes proposed model detail section report results experiments generated summary samples section analyze results model comment future work proposed model section discuss background related model describe detail newly veloped architecture application rization figure gated recurrent unit background principle compositionality denes meaning conveyed linguistic expression function syntactic combination stituent units words meaning sentence determined way words combined multi sentence text sentence level compositionality way tences combined ditional function add meaning overall text dealing larger texts compositionality sentence graph levels considered order ture text meaning completely approach plored recent literature create dedicated chitectures hierarchical fashion capture sequent levels compositionality nallapati build dedicated word sentence level rnn architectures capture compositionality different levels text units leading improvements performance architectural modications rnn encoder decoder suffer drawback major increase ing time memory usage pose alternative enhancement ture improve performance overhead draw inspiration roscience shown tional differentiation occurs naturally human brain giving rise temporal hierarchies meunier botvinick documented neurons hierarchically nize layers different tion rates stimuli quintessential example phenomenon auditory system syllable level information short time window integrated word level information longer time window previous works applied concept rnns movement tracking paine tani speech ztut time constant added activation mtgru shown control timescale gru cell larger meaning slower cell outputs makes cell focus slow features dynamic sequence input proposed mtgru model illustrated fig conventional gru special case mtgru attempt organize layers different timescales shows learning algorithm derived mtgru according dened forward cess propagation time rules error cell outputs time current gradient cell outputs different timescale constants set layer larger means slower context units denes default input timescale based hypothesis later layers learn features operate slower timescales set larger layers application question word sequences analyzed rnn sess information operates different poral hierarchies case tinuous audio signals received human tory system hypothesize word level clause level sentence level positionalities strong candidates light multiple timescale modication functions way explicitly guide layer neural network facilitate learning features erating increasingly slower timescales figure proposed multiple timescale gated current unit nition heinrich multiple timescale gated recurrent unit proposed multiple timescale gated rent unit mtgru model applies ral hierarchy concept problem text summarization framework rnn encoder decoder previous works mashita tani multiple timescale recurrent neural network mtrnn ployed temporal hierarchy motion prediction mtrnn prone problems present rnn difculty ing long term dependencies vanishing ent problem hochreiter long short term memory network hochreiter utilizes complex gating architecture aid learning long term dependencies shown perform better rnn tasks long term temporal dependencies machine translation sutskever gated recurrent unit gru cho proven comparable lstm chung similar complex gating architecture requires memory dard gru architecture shown fig summarization involves tentially long range temporal dependencies model applies temporal hierarchy gru apply timescale constant end gru essentially adding constant gating unit modulates mixture past current hidden states reset gate update gate candidate activation computed similarly original gru shown rnn type layers hidden units gru mtgru table network parameters model sponding subsequent levels compositional hierarchy summarization apply newly proposed multiple timescale model summarization build new dataset academic articles collect latex source les articles mains arxiv preprint server extracting introductions abstracts decompose introduction paragraphs pair paragraph salient sentence summary target summaries erated widely adopted idf scoring fig shows structure summarization model dataset contains rich compositionality longer text sequences increasing complexity summarization problem temporal archy function biggest impact plex compositional hierarchies exist input data multiple timescale concept play bigger role context compared previous summarization tasks rush model mtgru trained paragraphs targets generated maries introduction evaluated abstracts collected articles chose abstracts gold summaries ally contain important discourse structures goal related works methods results making good baseline summaries test fectiveness proposed method compare conventional rnn encoder decoder terms training speed performance experiments results trained models rst model ing conventional gru rnn encoder coder second model newly posed mtgru models trained hyperparamenter settings optimal conguration existing hardware pability following sutskever inputs divided multiple buckets gru figure paragraph level approach tion steps mtgru rnn gru train perplexity test perplexity table training results models gru models consist layers hidden units models longer input sequence sizes hidden units size ber layers limited embedding size networks timescale constant layer set respectively models trained summary pairs source text paragraphs extracted introduction academic ticles targets salient tence extracted paragraphs idf scores comparison training speed models fig shows plot training curve train perplexity reaches models trained nvidia force gtx titan gpus takes roughly days days respectively test greedy decoding generate likely output given source introduction evaluation adopt recall oriented understudy gisting evaluation rouge rics lin proposed lin hovy rouge recall oriented measure score tem summaries proven strong correlation human evaluations measures evaluation metric recall rouge precision score table rouge scores gru model summarymtgru model introductionparagraph nsummary nslow context unitsfast context unitsslowest context unitsslower context units evaluation metric recall rouge precision score table rouge scores mtgru model figure example output summary extracted targets discussion future work rouge scores obtained tion model gru mtgru multiple timescale concept improves mance conventional model presence highly complex architectural archies major advantage increase training speed epoch sample summary shown fig strates model successfully generalized difcult task summarizing large graph line salient summary setting timescale parameters low yamashita tani gradually increase layers higher layers slower context units experiment multiple settings pare training performance shown fig set tively nal model adopted experiment described previous section comparatively slower context ers fast slow text layers shown comparison ing performance superior remaining justies selection timescale settings results experiment provide evidence organizational process akin functional differentiation occurs rnn language tasks mtgru able train faster conventional gru epoch believe mtrgu expedites type tional differentiation process ring rnn explicitly guiding ers multiple timescales temporal hierarchical organization occurs gradually figure comparison training speed gru mtgru gram recall candidate summary gold summaries work gold summary abstract ticle rouge score calculated given rouge report performance models performance evaluation models trained steps training perplexity gru mtgru shown table step chosen early stopping point step test perplexity gru model rouge scores calculated trained networks shown table table gru gru models respectively sample summary generated mtgru model shown fig figure example generated summary mtgru number speed comparisonmtgrugruinput text input introduction paper generated summarization topic explored challenge text semantic recently unk neural networks emerged success wide range practical particular need use new way evaluate important questions use concept define temporal hierarchy sentence context demonstrate model outperforms conventional unk system significantly lead section evaluate experimental results model evaluate results section paper structured follows section describes related works section describes data collection processing steps section describes proposed models detail section report results experiments sample generated summaries section analyze results models section describes data collection models experimental results section report results experiments sample generated summaries mtgru output summaryinputtf idf extracted summary shown implicitly previous works rush nallapati explicit work choice data consisting paragraph salient secondly results indicate sentence pairs probabilistic language models solve task novel word generation tion setting meeting key criteria abstractive summarization bengio originally demonstrated probabilistic language models achieve better generalization lar words fact ity function smooth function word bedding vectors similar words trained similar embedding vectors small change features induces small change dicted probability makes strong case rnn language models best available lution abstractive summarization necessary generate novel sentences ample fig rst summary shows model generates word explored present paper furthermore results suggest given abstractive targets model train fully abstractive tion system future hope explore zational effect mtgru different tasks temporal hierarchies arise investigating ways effectively optimize timescale constant finally work fully abstractive end end tion system multi paragraph text utilizing abstractive target potentially generated help abstract articles conclusion paper demonstrated ity mtgru multi paragraph text summarization task model fullls mental requirement abstractive summarization deep semantic understanding text tance identication method draws researched phenomenon human brain implemented hierarchical architectural complexity additional memory quirements training application task capturing tional hierarchies text summarization gru shows ability enhance learning figure comparison training performance multiple time constants fig comparison erated summary input paragraph tracted summary seen example model successfully extracted key mation multiple sentences reproduces single line summary tem trained extractive summary abstraction entire paragraph ble generalization capability model objective maximizes joint probability target sequence ditioned source sequence marization model trained source extracted salient sentence target pairs objective viewed consisting subgoals correctly perform saliency nding importance traction order identify salient tent generate precise order sentence target fact training observe optimization rst subgoal achieved second subgoal second subgoal fully achieved overtting curs training set generalization bility model attributable fact model expected learn multiple points saliency given paragraph input gle salient section corresponding single tence training examples seen explains results fig obtained model believe work meaningful plications abstractive summarization going forward results conrm possible train encoder decoder model perform saliency identication need refer external corpus test time number multiple speed reducing training time signicantly future hope extend work fully abstractive end end summarization system multi paragraph text acknowledgment research supported basic science research program national search foundation funded ministry science ict future industrial strategic technology development gram funded ministry trade industry energy motie korea references bahdanau dzmitry bahdanau kyunghyun cho yoshua bengio neural machine translation jointly learning align translate corr bengio yoshua bengio rejean ducharme pascal vincent christian jauvin journal ral probabilistic language model chine learning research matthew botvinick tilevel structure behaviour brain model fuster hierarchy philosophical actions royal society biological sciences september cho kyunghyun cho bart van boer aglar gulcehre fethi bougares holger schwenk yoshua bengio ing phrase representations rnn decoder statistical machine translation corr chung junyoung chung aglar gulcehre kyunghyun cho yoshua bengio pirical evaluation gated recurrent neural networks sequence modeling corr hahn udo hahn inderjeet mani challenges automatic summarization computer november heinrich stefan heinrich cornelius ber stefan wermter articial neural networks machine learning icann international conference articial ral networks lausanne switzerland september proceedings chapter tive learning linguistic hierarchy multiple timescale recurrent neural network pages springer berlin heidelberg berlin hochreiter sepp hochreiter yoshua gio paolo frasconi gradient recurrent nets difculty learning long term dependencies kolen kremer tors field guide dynamical recurrent networks ieee press karen sparck jones statistical interpretation term specicity application retrieval journal documentation karen sparck jones automatic summarising state art information cessing management international journal jiwei minh thang luong dan hierarchical neural corr paragraphs documents jurafsky coder lin chin yew lin eduard hovy automatic evaluation summaries proceedings gram occurrence statistics conference north american ter association computational linguistics human language technology volume pages association computational linguistics chin yew lin rouge package automatic evaluation summaries text rization branches proceedings workshop volume luhn automatic creation literature abstracts ibm res dev april meunier lambiotte nito ersche bullmore erarchical modularity human brain functional works arxiv prints april nallapati ramesh nallapati bing xiang bowen zhou sequence sequence rnns text summarization international ference learning representations workshop track iclr nenkova ani nenkova sameer maskey yang liu automatic summarization proceedings annual meeting association computational linguistics tutorial abstracts acl hlt pages stroudsburg usa association tional linguistics paine rainer paine jun tani motor primitive sequence organization hierarchical recurrent neural work neural networks new developments self organizing systems rush alexander rush sumit chopra jason weston neural attention model sentence summarization proceedings conference empirical methods natural language processing pages association computational linguistics lisbon portugal sutskever ilya sutskever oriol vinyals sequence sequence quoc learning neural networks ghahramani welling cortes lawrence weinberger editors advances neural tion processing systems pages ran associates inc yamashita yuichi yamashita jun tani emergence functional hierarchy multiple timescale neural network model humanoid robot experiment plos comput biol
