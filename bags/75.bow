towards abstraction from extraction multiple timescale gated recurrent unit for summarization minsoo kim school of electronics engineering kyungpook national university daegu south korea com moirangthem dennis singh school of electronics engineering kyungpook national university daegu south korea com minho lee school of electronics engineering kyungpook national university daegu south korea com abstract in this work we introduce temporal erarchies to the sequence to sequence model to tackle the problem of abstractive summarization of scientic ticles the proposed multiple timescale model of the gated recurrent unit gru is implemented in the decoder setting to better deal with the presence of multiple compositionalities in larger texts the proposed model is pared to the conventional rnn decoder and the results demonstrate that our model trains faster and shows ca nt performance gains the results also show that the temporal hierarchies help improve the ability of models to capture compositionalities better without the presence of highly complex tural hierarchies introduction and related works summarization has been extensively researched over the past several decades jones and nenkova et al offer excellent overviews of the eld broadly summarization methods can be categorized into extractive approaches and abstractive approaches hahn and mani based on the type of computational task tive summarization is a selection problem while abstractive summarization requires a deeper mantic and discourse understanding of the text as well as a novel text generation process extractive summarization has been the focus in the past but abstractive summarization remains a challenge recently sequence to sequence current neural networks rnns have seen wide application in a number of tasks such rnn encoder decoders cho et al bahdanau et al combine a representation learning coder and a language modeling decoder to perform mappings between two sequences similarly cent works have proposed to cast summarization as a mapping problem between an input sequence and a summary sequence recent successes such as rush et al nallapati et al have shown that the rnn encoder decoder performs markably well in summarizing short text such approaches offer a fully data driven tion to both semantic and discourse understanding and text generation while presents a promising way ward for abstractive summarization extrapolating the methodology to other tasks such as the marization of a scientic article is not trivial a number of practical and theoretical concerns arise we can not simply train rnn encoder decoders on entire articles for the memory capacity of rent gpus scientic articles are too long to be processed whole via rnns moving from one or two sentences to several sentences or several paragraphs introduces additional levels of positionality and richer discourse structure how can we improve the conventional rnn decoder to better capture these deep learning approaches depend heavily on good quality scale datasets collecting source summary data pairs is difcult and datasets are scarce outside of the newswire domain in this paper we present a rst ate step towards end to end abstractive rization of scientic articles our aim is to tend based summarization to larger text with a more complex summarization task to l u j l c s c v v i x r a dress each of the issues above we propose a paragraph wise summarization system which is trained via paragraph salient sentence pairs we use term frequency inverse document frequency tf idf luhn jones scores to tract a salient sentence from each paragraph we introduce a novel model multiple timescale gated recurrent unit mtgru which adds a temporal hierarchy component that serves to dle multiple levels of compositionality this is inspired by an analogous concept of temporal erarchical organization found in the human brain and is implemented by modulating different layers of the multilayer rnn with different timescales yamashita and tani we demonstrate that our model is capable of understanding the tics of a multi sentence source text and knowing what is important about it which is the rst essary step towards abstractive summarization we build a new dataset of computer science cs articles from arxiv org extracting their tions from the latex source les the tions are decomposed into paragraphs each graph acting as a natural unit of discourse finally we concatenate the generated summary of each paragraph to create a non expert summary of the article s introduction and evaluate our sults against the actual abstract we show that our model is capable of summarizing multiple tences to its most salient part on unseen data ther supporting the larger view of summarization as a mapping task we demonstrate that our mtgru model satises some of the major quirements of an abstractive summarization tem we also report that mtgru has the bility of reducing training time signicantly pared to the conventional rnn encoder decoder the paper is structured as follows section scribes the proposed model in detail in section we report the results of our experiments and show the generated summary samples in section we analyze the results of our model and comment on future work proposed model in this section we discuss the background related to our model and describe in detail the newly veloped architecture and its application to rization figure a gated recurrent unit background the principle of compositionality denes the meaning conveyed by a linguistic expression as a function of the syntactic combination of its stituent units in other words the meaning of a sentence is determined by the way its words are combined with each other in multi sentence text sentence level compositionality the way tences are combined with one another is an ditional function which will add meaning to the overall text when dealing with such larger texts compositionality at the sentence and even graph levels should be considered in order to ture the text meaning completely an approach plored in recent literature is to create dedicated chitectures in a hierarchical fashion to capture sequent levels of compositionality li et al and nallapati et al build dedicated word and sentence level rnn architectures to capture compositionality at different levels of text units leading to improvements in performance however architectural modications to the rnn encoder decoder such as these suffer from the drawback of a major increase in both ing time and memory usage therefore we pose an alternative enhancement to the ture that will improve performance with no such overhead we draw our inspiration from roscience where it has been shown that tional differentiation occurs naturally in the human brain giving rise to temporal hierarchies meunier et al botvinick it has been well documented that neurons can hierarchically nize themselves into layers with different tion rates to stimuli the quintessential example of this phenomenon is the auditory system in which syllable level information in a short time window is integrated into word level information over a longer time window and so on previous works have applied this concept to rnns in movement tracking paine and tani and speech rt zt ht ztut the time constant added to the activation ht of the mtgru is shown in eq is used to control the timescale of each gru cell larger meaning slower cell outputs but it makes the cell focus on the slow features of a dynamic sequence input the proposed mtgru model is illustrated in fig the conventional gru will be a special case of mtgru where where no attempt is made to organize layers into different timescales ut e e ht e ht zt rt e ht zt e ht zt e ht eq shows the learning algorithm derived for the mtgru according to the dened forward cess and the back propagation through time rules is the error of the cell outputs at time t e and e is the current gradient of the cell outputs ht different timescale constants are set for each layer where larger means slower context units and denes the default or the input timescale based on our hypothesis that later layers should learn features that operate over slower timescales we set larger as we go up the layers in this application the question is whether the word sequences being analyzed by the rnn sess information that operates over different poral hierarchies as they do in the case of the tinuous audio signals received by the human tory system we hypothesize that they do and that word level clause level and sentence level positionalities are strong candidates in this light the multiple timescale modication functions as a way to explicitly guide each layer of the neural network to facilitate the learning of features erating over increasingly slower timescales figure proposed multiple timescale gated current unit nition heinrich et al multiple timescale gated recurrent unit our proposed multiple timescale gated rent unit mtgru model applies the ral hierarchy concept to the problem of text summarization in the framework of the rnn encoder decoder previous works such as mashita and tani s multiple timescale recurrent neural network mtrnn have ployed temporal hierarchy in motion prediction however mtrnn is prone to the same problems present in the rnn such as difculty in ing long term dependencies and vanishing ent problem hochreiter et al long short term memory network hochreiter et al utilizes a complex gating architecture to aid the learning of long term dependencies and has been shown to perform much better than the rnn in tasks with long term temporal dependencies such as machine translation sutskever et al gated recurrent unit gru cho et al which has been proven to be comparable to lstm chung et al has a similar complex gating architecture but requires less memory the dard gru architecture is shown in fig because summarization involves tentially many long range temporal dependencies our model applies temporal hierarchy to the gru we apply a timescale constant at the end of a gru essentially adding another constant gating unit that modulates the mixture of past and current hidden states the reset gate rt update gate zt and the candidate activation ut are computed similarly to that of the original gru as shown in eq rnn type layers hidden units gru mtgru table network parameters for each model sponding to subsequent levels in the compositional hierarchy summarization to apply our newly proposed multiple timescale model to summarization we build a new dataset of academic articles we collect latex source les of articles in the cs cl cv lg ne mains from the arxiv preprint server extracting their introductions and abstracts we decompose the introduction into paragraphs and pair each paragraph with its most salient sentence as the get summary these target summaries are erated using the widely adopted tf idf scoring fig shows the structure of our summarization model our dataset contains rich compositionality and longer text sequences increasing the complexity of the summarization problem the temporal archy function has the biggest impact when plex compositional hierarchies exist in the input data hence the multiple timescale concept will play a bigger role in our context compared to previous summarization tasks such as rush et al the model using mtgru is trained using these paragraphs and their targets the generated maries of each introduction is evaluated using the abstracts of the collected articles we chose the abstracts as gold summaries because they ally contain important discourse structures such as goal related works methods and results making them good baseline summaries to test the fectiveness of the proposed method we compare it with the conventional rnn encoder decoder in terms of training speed and performance experiments and results we trained two models the rst model ing the conventional gru in the rnn encoder coder and the second model using the newly posed mtgru both models are trained using the same hyperparamenter settings with the optimal conguration which ts our existing hardware pability following sutskever et al the inputs are divided into multiple buckets both gru and figure paragraph level approach to tion steps mtgru rnn gru train perplexity test perplexity table training results of the models gru models consist of layers and hidden units as our models take longer input and get sequence sizes the hidden units size and ber of layers are limited an embedding size of was used for both networks the timescale constant for each layer is set to respectively the models are trained on summary pairs the source text are the paragraphs extracted from the introduction of academic ticles and the targets are the most salient tence extracted from the paragraphs using tf idf scores for comparison of the training speed of the models fig shows the plot of the training curve until the train perplexity reaches both of the models are trained using nvidia ge force gtx titan x gpus which takes roughly days and days respectively during test greedy decoding was used to generate the most likely output given a source introduction for evaluation we adopt the recall oriented understudy for gisting evaluation rouge rics lin proposed by lin and hovy rouge is a recall oriented measure to score tem summaries which is proven to have a strong correlation with human evaluations it measures evaluation metric recall rouge l precision f score table rouge scores of gru model summarymtgru model introductionparagraph nsummary nslow context unitsfast context unitsslowest context unitsslower context units evaluation metric recall rouge l precision f score table rouge scores of mtgru model figure an example of the output summary vs the extracted targets discussion and future work the rouge scores obtained for the tion model using gru and mtgru show that the multiple timescale concept improves the mance of the conventional model without the presence of highly complex architectural archies another major advantage is the increase in training speed by as much as epoch over the sample summary shown in fig strates that the model has successfully generalized on the difcult task of summarizing a large graph into a one line salient summary in setting the timescale parameters we low yamashita and tani we gradually increase as we go up the layers such that higher layers have slower context units moreover we experiment with multiple settings of and pare the training performance as shown in fig the of and are set as and tively is the nal model adopted in our experiment described in the previous section has comparatively slower context ers and has two fast and two slow text layers as shown in the comparison the ing performance of is superior to the remaining two which justies our selection of the timescale settings the results of our experiment provide evidence that an organizational process akin to functional differentiation occurs in the rnn in language tasks the mtgru is able to train faster than the conventional gru by as much as epoch we believe that the mtrgu expedites a type of tional differentiation process that is already ring in the rnn by explicitly guiding the ers into multiple timescales where otherwise this temporal hierarchical organization occurs more gradually figure comparison of training speed between gru and mtgru the n gram recall between the candidate summary and gold summaries in this work we only have one gold summary which is the abstract of an ticle thus the rouge score is calculated as given in li et al and rouge l are used to report the performance of the models for the performance evaluation both the models are trained up to steps where the training perplexity of gru and mtgru are shown in table this step was chosen as the early stopping point as at this step we get the test perplexity of the gru model the rouge scores calculated using these trained networks are shown in table and table for the gru and gru models respectively a sample summary generated by the mtgru model is shown in fig figure an example of the generated summary with mtgru number of speed comparisonmtgrugruinput text the input is the introduction of this paper generated summarization has been the topic explored as a challenge of text semantic recently unk neural networks have emerged as a success in wide range of practical in particular we need to use a new way to evaluate three important questions into the we use a concept to define the temporal hierarchy of each sentence in the context of we demonstrate that our model outperforms a conventional unk system and significantly lead to in section we evaluate the experimental results on our model and evaluate our results in section the paper is structured as follows section describes the related works section describes the data collection and processing steps section describes the proposed models in detail in section we report the results of our experiments and show the sample generated summaries in section we analyze the results of our models section describes the data collection models and the experimental results in section we report the results of our experiments and show the sample generated summaries mtgru output summaryinputtf idf extracted summary has already been shown implicitly in previous works such as rush et al nallapati et al but is made explicit in our work due to our choice of data consisting of paragraph salient secondly our results indicate sentence pairs that probabilistic language models can solve the task of novel word generation in the tion setting meeting a key criteria of abstractive summarization bengio et al originally demonstrated that probabilistic language models can achieve much better generalization over lar words this is due to the fact that the ity function is a smooth function of the word bedding vectors since similar words are trained to have similar embedding vectors a small change in the features induces a small change in the dicted probability this makes a strong case for rnn language models as the best available lution for abstractive summarization where it is necessary to generate novel sentences for ample in fig the rst summary shows that our model generates the word explored which is not present in the paper furthermore our results suggest that if given abstractive targets the same model could train a fully abstractive tion system in the future we hope to explore the zational effect of the mtgru in different tasks where temporal hierarchies can arise as well as investigating ways to effectively optimize the timescale constant finally we will work to move towards a fully abstractive end to end tion system of multi paragraph text by utilizing a more abstractive target which can potentially be generated with the help of the abstract from the articles conclusion in this paper we have demonstrated the ity of the mtgru in the multi paragraph text summarization task our model fullls a mental requirement of abstractive summarization deep semantic understanding of text and tance identication the method draws from a well researched phenomenon in the human brain and can be implemented without any hierarchical architectural complexity or additional memory quirements during training although we show its application to the task of capturing tional hierarchies in text summarization only gru also shows the ability to enhance the learning figure comparison of training performance between multiple time constants in fig we show the comparison of a erated summary of the input paragraph to an tracted summary as seen in the example our model has successfully extracted the key mation from multiple sentences and reproduces it into a single line summary while the tem was trained only on the extractive summary the abstraction of the entire paragraph is ble because of the generalization capability of our model the objective maximizes the joint probability of the target sequence ditioned on the source sequence when a marization model is trained on source extracted salient sentence target pairs the objective can be viewed as consisting of two subgoals one is to correctly perform saliency nding importance traction in order to identify the most salient tent and the other is to generate the precise order of the sentence target in fact during training we observe that the optimization of the rst subgoal is achieved before the second subgoal the second subgoal is fully achieved only when overtting curs on the training set the generalization bility of the model is attributable to the fact that the model is expected to learn multiple points of saliency per given paragraph input not only a gle salient section corresponding to a single tence as many training examples are seen this explains how the results such as those in fig can be obtained from this model we believe our work has some meaningful plications for abstractive summarization going forward first our results conrm that it is possible to train an encoder decoder model to perform saliency identication without the need to refer to an external corpus at test time this number of of multiple speed thereby reducing training time signicantly in the future we hope to extend our work to a fully abstractive end to end summarization system of multi paragraph text acknowledgment this research was supported by basic science research program through the national search foundation of funded by the ministry of science ict and future and by the industrial strategic technology development gram funded by the ministry of trade industry and energy motie korea references bahdanau et al dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation by jointly learning to align and translate corr bengio et al yoshua bengio rejean ducharme pascal vincent and christian jauvin a journal of ral probabilistic language model chine learning research matthew m botvinick tilevel structure in behaviour and in the brain a model of fuster hierarchy philosophical actions of the royal society b biological sciences september cho et al kyunghyun cho bart van boer c aglar gulcehre fethi bougares holger schwenk and yoshua bengio ing phrase representations using rnn decoder for statistical machine translation corr chung et al junyoung chung c aglar gulcehre kyunghyun cho and yoshua bengio pirical evaluation of gated recurrent neural networks on sequence modeling corr hahn and udo hahn and inderjeet mani the challenges of automatic summarization computer november heinrich et al stefan heinrich cornelius ber and stefan wermter articial neural networks and machine learning icann international conference on articial ral networks lausanne switzerland september proceedings part i chapter tive learning of linguistic hierarchy in a multiple timescale recurrent neural network pages springer berlin heidelberg berlin hochreiter al sepp hochreiter yoshua gio and paolo frasconi gradient ow in recurrent nets the difculty of learning long term dependencies in j kolen and s kremer tors field guide to dynamical recurrent networks ieee press karen sparck jones a statistical interpretation of term specicity and its application in retrieval journal of documentation karen sparck jones automatic summarising the state of the art information cessing and management an international journal et al jiwei li minh thang luong and dan a hierarchical neural corr for paragraphs and documents jurafsky coder lin and chin yew lin and eduard hovy automatic evaluation of summaries using in proceedings of gram co occurrence statistics the conference of the north american ter of the association for computational linguistics on human language technology volume pages association for computational linguistics chin yew lin rouge a package for automatic evaluation of summaries in text rization branches out proceedings of the workshop volume h p luhn the automatic creation of literature abstracts ibm j res dev april et al d meunier r lambiotte a nito k d ersche and e t bullmore erarchical modularity in human brain functional works arxiv e prints april nallapati et al ramesh nallapati bing xiang and bowen zhou sequence to sequence rnns for text summarization international ference on learning representations workshop track iclr nenkova et al ani nenkova sameer maskey and yang liu automatic summarization in proceedings of the annual meeting of the association for computational linguistics tutorial abstracts of acl hlt pages stroudsburg pa usa association for tional linguistics paine and rainer w paine and jun tani motor primitive and sequence organization in a hierarchical recurrent neural work neural networks new developments in self organizing systems rush et al alexander m rush sumit chopra and jason weston a neural attention model for sentence summarization in proceedings of the conference on empirical methods in natural language processing pages association for computational linguistics lisbon portugal sutskever et al ilya sutskever oriol vinyals sequence to sequence and quoc v le learning with neural networks in z ghahramani m welling c cortes n d lawrence and k q weinberger editors advances in neural tion processing systems pages ran associates inc yamashita and yuichi yamashita and jun tani emergence of functional hierarchy in a multiple timescale neural network model a humanoid robot experiment plos comput biol
