BENGALI TEXT SUMMARIZATION BY SENTENCE Sarkar Science Engineering summarization is a process to produce an abstract or a summary by selecting portion of the information from one or more In an automatic text a text is given to the computer and the computer returns a less redundant extract or abstract of the original Many techniques have developed for summarizing English a very few attempts have been for Bengali text This paper presents a method for Bengali text which extracts important sentences from a Bengali document to a Bengali Text Sentence Indian Languages INTRODUCTION information overload on the World Wide Web is becoming a for an increasingly large number of web To reduce this information automatic text summarization can be an indispensable The or summaries can be used as the document surrogates in place of the original In another the summaries can help the reader to get a quick overview an entire Another important issue related to the information explosion on internet is the problem that many documents with the same or similar topics are This kind of data duplication problem increases the necessity for effective In the following are the important reasons in of automatic text A summary or abstract saves reading time A summary or an abstract facilitate document selection and literature searches improves document indexing efficiency Machine generated summary is free from bias Customized summaries can be useful in systems where they personalized The use of automatic or summarization by commercial abstract may allow them to scale the number of published texts they can to a summarization process can be one or more text When only one is the it is called single document text summarization and when the is a group of related text it is called can also categorize the text summarization based on the type of users the summary intended User focused summaries are tailored to the of a particular user or group of users and generic summaries are aimed at broad readership community on the nature of a summary can be categorized as an abstract and An extract is a summary consisting of a number of salient text units selected the An abstract is a which represents the subject matter of an with the text which are generated by reformulating the salient units from an An abstract may contain some text which are not present the input on information content of the it can be categorized as informative indicative The indicative summary presents an indication about an purpose and approach to the user for selecting the article for summary covers all salient information in the document at some level of that it will contain information about all the different aspects such as results and conclusions For an of a research article is more informative than its main objective of the work presented in this paper is to generate an extract from a We have followed a simple and approach to single document text summarization because the sophisticated summarization requires resources for deeper semantic Bengali is a resource language and NLP language research activities on have recently been In our work presented in this we have the impact of thematic term feature and position feature on Bengali text To our no generic text summarization system for Bengali available for comparison to our we have compared the proposed to the LEAD baseline which was defined for single document text task in past two DUC conferences DUC and DUC LEAD considers the first n words of an input article as a where n is a summary work is different from the work on Bengali opinion summarization presented in and because we mainly focus on generic text for section we present a brief survey on single document text summarization in The proposed summarization method has been presented in section section we describe the summary evaluation method and experimental A SURVEY ON SINGLE DOCUMENT TEXT SUMMARIZATION IN DOMAIN this we present a brief survey on single document text summarization for Although new research on text summarization in English domain has been many years most works on text summarization today still rely on sentence to form previous works on extractive summarization use two major ranking sentences based on their scores which are computed by combining few or all of features such as term frequency positional information and cue phrases Lin and Hovy and few top ranked sentences to form an The very first work on text summarization by Luhn computes salient sentences based on frequency of times a word occurs in a and phrase subsequent research has developed sophisticated summarization methods on various new the work presented by Edmundson is still today as the foundation for extraction based presented a straightforward method of sentence extraction using first and last sentences of the document each Lin Hovy claimed that as the discourse structures change over the domains the the position method can not be as simple as in defined an optimal policy of locating the likely positions of in the computes the score of a sentence based on many such as similarity to the similarity to the first sentence of the position of the sentence in the sentence length applied a machine learning approach to text They a summarizer using a Bayesian classifier to combine features from a corpus scientific articles and their presented a sentence extraction method that exploits the semantic between sentences in the The feature they used in this work may be as a cohesion Text cohesion and Hasan refers to relations between word or referring determine how tightly connected the text In this text is by a graph in which each node represents a paragraph in a document and edges are labeled with the similarity score between two The paragraph is connected to many other paragraphs with a similarity above a predefined is considered as the bushy The paragraph representing the is considered as a salient and Elhadad described a summarization approach that used lexical method to compute the salience of a Cohesion and is a method for sticking together different parts of the Lexical is the simplest form of Lexical Cohesion links the different parts of text through semantically related ellipsis and cohesion also involves relations such as hypernymy relations such as The of lexical chain was introduced in and They characterized chain as a sequence of related words that spans a topical unit of In other lexical chain is basically lexical cohesion that occurs between two terms and sequences of related Barzilay and Elhadad used a WordNet to construct the lexical work in and considered the fact that the probability of of a sentence in an extract depends on whether the previous sentence had been as well and applied hidden Markov models in sentence extraction applied maximum entropy model to decide whether a will be included in a summary or He assumed no feature features he considered word sentence sentence features whether sentence follows the to creating an automatic generation of abstract is harder and the requires deeper approaches which exploit semantic properties in the of an abstract from a document is relatively harder since it representation of text units or in a reformulation two or more text units and rendering the new representation in natural approaches have used template based information information and In information extraction based predefined template are filled with the desired pieces of information extracted by the summarization and An automated technique has been presented in to build a corpus representing the used by humans so that such a corpus can then be used to train an automated True abstraction needs more sophisticated process that requires generation can be viewed as generation of very short summary less that represents the relevant points contained in a A summary is a kind of the indicative Banko presented approach that uses some statistical methods to generate headline like Markov based headline generation has been presented in Dorr and et developed the Hedge Trimmer that uses a based to generate In this the first sentence of a document is using a parser and then the parsed sentence is compressed to form a headline eliminating the unimportant constituents of the sentence using a set of motivated et a headline generation combines the version of the lead sentence and a set of topic descriptors generated from corpus to form a The sentence is compressed using the approach similar the approach in et and the topic A number of for creating abstracts have been conceptualized without much emphasis the issue that a true abstract may contain some information not contained in the Creating such an abstract requires external information of some kind such knowledge base Since resources of this kind are difficult abstractive summarization has not progressed beyond the PROPOSED SUMMARIZATION METHOD proposed summarization method is extraction It has three major sentence ranking summary Preprocessing preprocessing step includes stemming and breaking the input in to a collection of For stop word we have used the list downloadable from the website of Forum for Information Evaluation Stemming a word is split into its stem and The design of a stemmer is and requires some significant linguistic expertise in the typical simple stemmer algorithm involves removing suffixes using a list of while a more complex one would use morphological knowledge to a stem from the Since Bengali is a highly inflectional is necessary while computing frequency of a our we use a lightweight stemmer for Bengali that strips the suffixes using a suffix on a using the algorithm similar to that Hindi and Sentence Ranking an input document is formatted and the document is broken into a of sentences and the sentences are ranked based on two important term and The thematic terms are the terms which are related to the main theme a We define the thematic terms are the terms whose TFIDF values are than a predefined The TFIDF value of a term is measured by the of TF and where TF is the number of times a word in a document and IDF is Inverse Document The IDF of a word is on a corpus using the where of in the corpus and df indicates the number of in which a word The score of a sentence k is computed based on the of the sentence to the set of thematic terms in a The similarity of a k to the set of thematic terms in a document is computed as the sum of the values of the thematic terms contained in the sentence is a TFIDF value of a thematic term w in a sentence k and Sk is the of the sentence crucial issue is to determine the TFIDF threshold value based on which we can on whether a term is a thematic term or In experimental we will how this threshold value has been adjusted for the best The positional score of a sentence is computed in such a way that the sentence of a document gets the highest score and the last sentence gets the lowest The positional value for the sentence k is computed using following We consider length of a sentence as a feature because we observe that a sentence is too but it occurs in the beginning paragraph of a document it is selected due to its positional On the other if a sentence is it is sometimes selected due to the fact that it contains many we the sentences which are too short or too Parameters for Sentence We compute the score of a sentence the linear combination of the normalized values of thematic term based score Sk positional score Pk if the sentence is not too long or too If a sentence is too or too it is assigned a score of The final score of a sentence k values of   LL cutoff on the sentence length and LU cutoff the sentence length are obtained by tuning them for the best results on a subset documents randomly selected from our In the experimental we will in detail how the values of these parameters are Summary Generation summary is produced after ranking the sentences based on their scores and selecting ranked when the value of K is set by the To increase the of the the sentences in the summary are reordered based on their in the original for the sentence which occurs first in the text will appear first in the EXPERIMENTS AND RESULTS test our summarization we collected Bengali documents from the daily Ananda Bazar The documents are typed and saved in text files using For each document in our we consider only reference summary for Evaluation of a system generated summary is by comparing it to the reference Evaluation is very difficult to determine whether a summary is good or The summary methods can be broadly categorized as human evaluation methods and evaluation A human evaluation is done by summaries with summaries by human According to some predefined the judges assign a score in a scale to each summary under Quantitative scores are given to the based on the different qualitative features such as information The main problems with human evaluation the evaluation process tedious it suffers from the lack of Two human judges may not agree each On the other automatic evaluation is consistent with a The automatic evaluations may lack the linguistic and emotional perspective that a human Hence although automatic is not perfect compared to the human it is popular primarily the evaluation process is quick even if summaries to be evaluated are large in Since automatic evaluation is performed by a it follows a fixed logic always produces the same result on a given Since automatic evaluation are free from human it provides a consistent way of comparing the summarization several past Document Understanding Conferences organized by NIST National Institute of Standards and single document text systems for English have been In DUC and DUC single document summarization task was to generate a summary of fixed length as words A baseline called LEAD baseline was defined in LEAD baseline considers the first n words of an input article as a where n is a predefined summary DUC single document text summarization task where there was a fixed length for each we believe that a generic summary of a document be longer or shorter than a summary of another we assume that the of a system generated summary should be equal to that of the corresponding but the different model summaries may not be equal in adopted an automatic summary evaluation metric for comparing to reference When we compare a system generated summary to reference we ensure that they would be of the same We have used unigram overlap method stated in for evaluating the system Unigram overlap between a system generated summary and a summary is computed as based Recall is the length of the reference summary and indicates the maximum number of unigrams in the system summary S and the reference summary of reference summaries is a laborious In our we have only one reference summary for evaluating each system generated Experiments and Results   and choosing appropriate threshold For the best    used in equation would be set At the same an appropriate threshold value for selecting the thematic terms in subsection be For tuning these we build a training data set by collection of selecting pairs from pairs in our we set the value of   to since   is the weight of the positional feature which observed by us as a feature producing better results than the thematic term set the value of   to for all the experimental cases presented in this tuning the value of we set the TFIDF threshold value to and conduct with the different values of   that ranges from to To obtain the values of we step between to by The figure shows summarization curve with respect to different values of   on the training Average Recall score when TFIDF threshold value is set to and   is to figure shows that when the value of   is set to which is a relatively smaller the better result is depending on TFIDF threshold value we decide on whether a term is the term or an appropriate threshold value should be determined to improve summarization For this after fixing the value of   to we the TFIDF threshold figure shows the summarization performance curve with different TFIDF Average Recall score TFIDF threshold when   is set to and is set to figure shows that the best result is achieved when TFIDF threshold value is set any value between and We set TFIDF threshold value to because at this average recall score transits from a lower value to the best fixing the value of   to and the TFIDF threshold value to we adjust the cutoff and the upper cutoff on the sentence Table shows the results on set with different values of the upper cutoff on sentence Recall Results on training set with different values of the upper cutoff on results on training set with different values of lower cutoff on sentence length shown in Table Recall Score Results on training set with different values of lower cutoff on sentence and table show that the best results are obtained when LU is set to any value and and LL is set to We set the value of LU to and the value of LL when we run the system on the test We pairs in our corpus and considered this subset as a training set for the values of several parameters discussed After setting the parameters the values learnt from the training we test our system on the entire collection of From each of a summary of n words is where is the length of the reference summary of the corresponding A system summary is compared to a reference summary and the unigram based recall is computed using the equation The average recall score is obtained by the recall scores obtained on all the documents in the of shows the performance of the proposed system on the test data our no generic text summarization system for Bengali is available for to our we have compared the proposed method to the LEAD LEAD baseline considers the first n words of an input article as a n is a predefined summary Table shows the comparisons of our to the LEAD Unigram based Recall Score System baseline Comparison of the proposed system to LEAD baseline shows that the proposed method outperforms the LEAD The of generic summarization systems in the past DUC conferences DUC DUC proves that it is very hard to beat LEAD baseline on the news The following is an article taken from the Bengali daily newspaper Bazar p t m o    o   ei p   o   sp st o   e o nt ei      i   u o   m o t    a   t   eo e   o nt e d m   ei ps    o o is the reference summary for the article mentioned    p st following is the summary generated by the proposed system for the news p t m o    ei p   o   sp st Conclusion paper discusses a single document text summarization method for Many have been developed for summarizing English a very few have been made for Bengali text performance of the proposed system may further be improved by improving exploring more number of features and applying learning for effective feature more than one reference summaries are used for evaluating each system but in our we have used only one reference summary for In we will consider more than one reference summaries summary Bengali Opinion A Lightweight Stemmer for In the of EACL Hedge A to headline In Proceedings of the Summarization Workshop and Document Understanding Conference The identification of important concepts in highly technical In the proceedings of the International on Research and Development in Information Retrieval C Y and Identifying Topics by In proceedings of the Applied Natural Language Processing New New Association for Computational summarization of Journal of Information Processing and Volume Issue MEAD A platform for multidocument text In Proceedings of the International on Language Resources and Evaluation at In Association for North American Chapter of of Linguistics Workshop on Document Automatic Headline Generation for In Workshop on Automatic A Lexical Database for Communications of the for Computing Machinery Automatic text structuring Processing and Journal of New methods in automatic Journal of the for Computing The automatic creation of literature IBM Journal of Automatic Volume of Natural language John Benjamins Publishing Using hidden Markov modeling to decompose Computational The decomposition of summary In the Proceedings of International Conference on and Development in Information University of pages A trainable document In of Research and Development in Information pp Text summarization via hidden Markov models pivoted QR matrix University of Lexical cohesion computed by thesaural relations as an of the structure of Computational Cohesion in K Cohesion in English Language Witbrock Headline generation based on statistical In Proceedings of the Annual Meeting of the Association for Linguistics Hong Using maximum entropy for sentence In Proceedings of Workshop on Automatic Volume Annual Meeting of the Association for Computational index for technical literature An IBM of Research and pages Using Lexical Chains for Text In of the Workshop on Intelligent Scalable Text